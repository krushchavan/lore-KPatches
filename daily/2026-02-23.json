{
  "date": "2026-02-23",
  "report_file": "2026-02-23.html",
  "status": "complete",
  "last_updated": "2026-02-26 01:19 UTC",
  "llm_backends": [],
  "generation_time_seconds": 162.09488558769226,
  "developer_reports": [
    {
      "name": "Alexandre Ghiti",
      "primary_email": "alexghiti@rivosinc.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Boris Burkov",
      "primary_email": "boris@bur.io",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Dmitry Ilvokhin",
      "primary_email": "d@ilvokhin.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 4/4] mm: add tracepoints for zone lock",
          "message_id": "1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Add tracepoint instrumentation to zone lock acquire/release operations via the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a lightweight inline helper checks whether the tracepoint is enabled and calls into an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 3/4] mm: convert compaction to zone lock wrappers",
          "message_id": "3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Compaction uses compact_lock_irqsave(), which currently operates on a raw spinlock_t pointer so that it can be used for both zone->lock and lru_lock. Since zone lock operations are now wrapped, compact_lock_irqsave() can no longer operate directly on a spinlock_t when the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The structure carries a lock type enum and a union holding either a zone pointer or a raw spinlock_t pointer, and dispatches to the appropriate lock/unlock helper.\n\nNo functional change intended.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Cheatham, Benjamin",
              "summary": "Nit: You could remove the helpers above and just do the calls directly in this function, though it would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay since they have the __acquires() annotations. You don't need the return statement here (and you shouldn't be returning a value at all). It may be cleaner to just do an if-else statement here instead. I would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you didn't change it due to location but I would argue it isn't really relevant to what's being added in this patch and fits better in the last.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "nits"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> Compaction uses compact_lock_irqsave(), which currently operates\n> on a raw spinlock_t pointer so that it can be used for both\n> zone->lock and lru_lock. Since zone lock operations are now wrapped,\n> compact_lock_irqsave() can no longer operate directly on a spinlock_t\n> when the lock belongs to a zone.\n> \n> Introduce struct compact_lock to abstract the underlying lock type. The\n> structure carries a lock type enum and a union holding either a zone\n> pointer or a raw spinlock_t pointer, and dispatches to the appropriate\n> lock/unlock helper.\n> \n> No functional change intended.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> ---\n>  mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n>  1 file changed, 89 insertions(+), 19 deletions(-)\n> \n> diff --git a/mm/compaction.c b/mm/compaction.c\n> index 1e8f8eca318c..1b000d2b95b2 100644\n> --- a/mm/compaction.c\n> +++ b/mm/compaction.c\n> @@ -24,6 +24,7 @@\n>  #include <linux/page_owner.h>\n>  #include <linux/psi.h>\n>  #include <linux/cpuset.h>\n> +#include <linux/zone_lock.h>\n>  #include \"internal.h\"\n>  \n>  #ifdef CONFIG_COMPACTION\n> @@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n>  }\n>  #endif /* CONFIG_COMPACTION */\n>  \n> +enum compact_lock_type {\n> +\tCOMPACT_LOCK_ZONE,\n> +\tCOMPACT_LOCK_RAW_SPINLOCK,\n> +};\n> +\n> +struct compact_lock {\n> +\tenum compact_lock_type type;\n> +\tunion {\n> +\t\tstruct zone *zone;\n> +\t\tspinlock_t *lock; /* Reference to lru lock */\n> +\t};\n> +};\n> +\n> +static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n> +\t\t\t\t\t    unsigned long *flags)\n> +{\n> +\treturn zone_trylock_irqsave(zone, *flags);\n> +}\n> +\n> +static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n> +\t\t\t\t\t   unsigned long *flags)\n> +{\n> +\treturn spin_trylock_irqsave(lock, *flags);\n> +}\n> +\n> +static bool compact_do_trylock_irqsave(struct compact_lock lock,\n> +\t\t\t\t       unsigned long *flags)\n> +{\n> +\tif (lock.type == COMPACT_LOCK_ZONE)\n> +\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n> +\n> +\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n> +}\n\nNit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n> +\n> +static void compact_do_zone_lock_irqsave(struct zone *zone,\n> +\t\t\t\t\t unsigned long *flags)\n> +__acquires(zone->lock)\n> +{\n> +\tzone_lock_irqsave(zone, *flags);\n> +}\n> +\n> +static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n> +\t\t\t\t\tunsigned long *flags)\n> +__acquires(lock)\n> +{\n> +\tspin_lock_irqsave(lock, *flags);\n> +}\n> +\n> +static void compact_do_lock_irqsave(struct compact_lock lock,\n> +\t\t\t\t    unsigned long *flags)\n> +{\n> +\tif (lock.type == COMPACT_LOCK_ZONE) {\n> +\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n> +\t\treturn;\n> +\t}\n> +\n> +\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n> +}\n> +\n>  /*\n>   * Compaction requires the taking of some coarse locks that are potentially\n>   * very heavily contended. For async compaction, trylock and record if the\n> @@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n>   *\n>   * Always returns true which makes it easier to track lock state in callers.\n>   */\n> -static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n> -\t\t\t\t\t\tstruct compact_control *cc)\n> -\t__acquires(lock)\n> +static bool compact_lock_irqsave(struct compact_lock lock,\n> +\t\t\t\t unsigned long *flags,\n> +\t\t\t\t struct compact_control *cc)\n>  {\n>  \t/* Track if the lock is contended in async mode */\n>  \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n> -\t\tif (spin_trylock_irqsave(lock, *flags))\n> +\t\tif (compact_do_trylock_irqsave(lock, flags))\n>  \t\t\treturn true;\n>  \n>  \t\tcc->contended = true;\n>  \t}\n>  \n> -\tspin_lock_irqsave(lock, *flags);\n> +\tcompact_do_lock_irqsave(lock, flags);\n>  \treturn true;\n>  }\n>  \n> @@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n>   * Returns true if compaction should abort due to fatal signal pending.\n>   * Returns false when compaction can continue.\n>   */\n> -static bool compact_unlock_should_abort(spinlock_t *lock,\n> -\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n> +static bool compact_unlock_should_abort(struct zone *zone,\n> +\t\t\t\t\tunsigned long flags,\n> +\t\t\t\t\tbool *locked,\n> +\t\t\t\t\tstruct compact_control *cc)\n>  {\n>  \tif (*locked) {\n> -\t\tspin_unlock_irqrestore(lock, flags);\n> +\t\tzone_unlock_irqrestore(zone, flags);\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.\n\n>  \t\t*locked = false;\n>  \t}\n>  \n> @@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n>  \t\t * contention, to give chance to IRQs. Abort if fatal signal\n>  \t\t * pending.\n>  \t\t */\n> -\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n> -\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n> -\t\t\t\t\t\t\t\t&locked, cc))\n> +\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n> +\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n>  \t\t\tbreak;\n>  \n>  \t\tnr_scanned++;\n> @@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n>  \n>  \t\t/* If we already hold the lock, we can skip some rechecking. */\n>  \t\tif (!locked) {\n> -\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n> -\t\t\t\t\t\t\t\t&flags, cc);\n> +\t\t\tstruct compact_lock zol = {\n> +\t\t\t\t.type = COMPACT_LOCK_ZONE,\n> +\t\t\t\t.zone = cc->zone,\n> +\t\t\t};\n> +\n> +\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n>  \n>  \t\t\t/* Recheck this is a buddy page under lock */\n>  \t\t\tif (!PageBuddy(page))\n> @@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n>  \t}\n>  \n>  \tif (locked)\n> -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> +\t\tzone_unlock_irqrestore(cc->zone, flags);\n>  \n>  \t/*\n>  \t * Be careful to not go outside of the pageblock.\n> @@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n>  \n>  \t\t/* If we already hold the lock, we can skip some rechecking */\n>  \t\tif (lruvec != locked) {\n> +\t\t\tstruct compact_lock zol = {\n> +\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n> +\t\t\t\t.lock = &lruvec->lru_lock,\n> +\t\t\t};\n> +\n>  \t\t\tif (locked)\n>  \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n>  \n> -\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n> +\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n>  \t\t\tlocked = lruvec;\n>  \n>  \t\t\tlruvec_memcg_debug(lruvec, folio);\n> @@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n>  \t\tif (!area->nr_free)\n>  \t\t\tcontinue;\n>  \n> -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> +\t\tzone_lock_irqsave(cc->zone, flags);\n>  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n>  \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n>  \t\t\tunsigned long pfn;\n> @@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n>  \t\t\t}\n>  \t\t}\n>  \n> -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> +\t\tzone_unlock_irqrestore(cc->zone, flags);\n>  \n>  \t\t/* Skip fast search if enough freepages isolated */\n>  \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n> @@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n>  \t\tif (!area->nr_free)\n>  \t\t\tcontinue;\n>  \n> -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> +\t\tzone_lock_irqsave(cc->zone, flags);\n>  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n>  \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n>  \t\t\tunsigned long free_pfn;\n> @@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n>  \t\t\t\tbreak;\n>  \t\t\t}\n>  \t\t}\n> -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> +\t\tzone_unlock_irqrestore(cc->zone, flags);\n>  \t}\n>  \n>  \tcc->total_migrate_scanned += nr_scanned;\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Yes, I agree, there is no much value in this wrappers, will remove them, Yes, agree, will fix in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "nits"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:10:05PM -0600, Cheatham, Benjamin wrote:\n> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> > Compaction uses compact_lock_irqsave(), which currently operates\n> > on a raw spinlock_t pointer so that it can be used for both\n> > zone->lock and lru_lock. Since zone lock operations are now wrapped,\n> > compact_lock_irqsave() can no longer operate directly on a spinlock_t\n> > when the lock belongs to a zone.\n> > \n> > Introduce struct compact_lock to abstract the underlying lock type. The\n> > structure carries a lock type enum and a union holding either a zone\n> > pointer or a raw spinlock_t pointer, and dispatches to the appropriate\n> > lock/unlock helper.\n> > \n> > No functional change intended.\n> > \n> > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > ---\n> >  mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n> >  1 file changed, 89 insertions(+), 19 deletions(-)\n> > \n> > diff --git a/mm/compaction.c b/mm/compaction.c\n> > index 1e8f8eca318c..1b000d2b95b2 100644\n> > --- a/mm/compaction.c\n> > +++ b/mm/compaction.c\n> > @@ -24,6 +24,7 @@\n> >  #include <linux/page_owner.h>\n> >  #include <linux/psi.h>\n> >  #include <linux/cpuset.h>\n> > +#include <linux/zone_lock.h>\n> >  #include \"internal.h\"\n> >  \n> >  #ifdef CONFIG_COMPACTION\n> > @@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n> >  }\n> >  #endif /* CONFIG_COMPACTION */\n> >  \n> > +enum compact_lock_type {\n> > +\tCOMPACT_LOCK_ZONE,\n> > +\tCOMPACT_LOCK_RAW_SPINLOCK,\n> > +};\n> > +\n> > +struct compact_lock {\n> > +\tenum compact_lock_type type;\n> > +\tunion {\n> > +\t\tstruct zone *zone;\n> > +\t\tspinlock_t *lock; /* Reference to lru lock */\n> > +\t};\n> > +};\n> > +\n> > +static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n> > +\t\t\t\t\t    unsigned long *flags)\n> > +{\n> > +\treturn zone_trylock_irqsave(zone, *flags);\n> > +}\n> > +\n> > +static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n> > +\t\t\t\t\t   unsigned long *flags)\n> > +{\n> > +\treturn spin_trylock_irqsave(lock, *flags);\n> > +}\n> > +\n> > +static bool compact_do_trylock_irqsave(struct compact_lock lock,\n> > +\t\t\t\t       unsigned long *flags)\n> > +{\n> > +\tif (lock.type == COMPACT_LOCK_ZONE)\n> > +\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n> > +\n> > +\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n> > +}\n> \n> Nit: You could remove the helpers above and just do the calls directly in this function, though\n> it would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\n> since they have the __acquires() annotations.\n\nYes, I agree, there is no much value in this wrappers, will remove them,\nthanks!\n\n> > +\n> > +static void compact_do_zone_lock_irqsave(struct zone *zone,\n> > +\t\t\t\t\t unsigned long *flags)\n> > +__acquires(zone->lock)\n> > +{\n> > +\tzone_lock_irqsave(zone, *flags);\n> > +}\n> > +\n> > +static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n> > +\t\t\t\t\tunsigned long *flags)\n> > +__acquires(lock)\n> > +{\n> > +\tspin_lock_irqsave(lock, *flags);\n> > +}\n> > +\n> > +static void compact_do_lock_irqsave(struct compact_lock lock,\n> > +\t\t\t\t    unsigned long *flags)\n> > +{\n> > +\tif (lock.type == COMPACT_LOCK_ZONE) {\n> > +\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n> > +\t\treturn;\n> > +\t}\n> > +\n> > +\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n> \n> You don't need the return statement here (and you shouldn't be returning a value at all).\n\nYes, agree, will fix in v2.\n\n> \n> It may be cleaner to just do an if-else statement here instead.\n> \n> > +}\n> > +\n> >  /*\n> >   * Compaction requires the taking of some coarse locks that are potentially\n> >   * very heavily contended. For async compaction, trylock and record if the\n> > @@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n> >   *\n> >   * Always returns true which makes it easier to track lock state in callers.\n> >   */\n> > -static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n> > -\t\t\t\t\t\tstruct compact_control *cc)\n> > -\t__acquires(lock)\n> > +static bool compact_lock_irqsave(struct compact_lock lock,\n> > +\t\t\t\t unsigned long *flags,\n> > +\t\t\t\t struct compact_control *cc)\n> >  {\n> >  \t/* Track if the lock is contended in async mode */\n> >  \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n> > -\t\tif (spin_trylock_irqsave(lock, *flags))\n> > +\t\tif (compact_do_trylock_irqsave(lock, flags))\n> >  \t\t\treturn true;\n> >  \n> >  \t\tcc->contended = true;\n> >  \t}\n> >  \n> > -\tspin_lock_irqsave(lock, *flags);\n> > +\tcompact_do_lock_irqsave(lock, flags);\n> >  \treturn true;\n> >  }\n> >  \n> > @@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n> >   * Returns true if compaction should abort due to fatal signal pending.\n> >   * Returns false when compaction can continue.\n> >   */\n> > -static bool compact_unlock_should_abort(spinlock_t *lock,\n> > -\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n> > +static bool compact_unlock_should_abort(struct zone *zone,\n> > +\t\t\t\t\tunsigned long flags,\n> > +\t\t\t\t\tbool *locked,\n> > +\t\t\t\t\tstruct compact_control *cc)\n> >  {\n> >  \tif (*locked) {\n> > -\t\tspin_unlock_irqrestore(lock, flags);\n> > +\t\tzone_unlock_irqrestore(zone, flags);\n> \n> I would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\n> didn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\n> and fits better in the last.\n\nThanks for the suggestion. Totally makes sense to me, will do in v2 as well.\n\n> \n> >  \t\t*locked = false;\n> >  \t}\n> >  \n> > @@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n> >  \t\t * contention, to give chance to IRQs. Abort if fatal signal\n> >  \t\t * pending.\n> >  \t\t */\n> > -\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n> > -\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n> > -\t\t\t\t\t\t\t\t&locked, cc))\n> > +\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n> > +\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n> >  \t\t\tbreak;\n> >  \n> >  \t\tnr_scanned++;\n> > @@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n> >  \n> >  \t\t/* If we already hold the lock, we can skip some rechecking. */\n> >  \t\tif (!locked) {\n> > -\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n> > -\t\t\t\t\t\t\t\t&flags, cc);\n> > +\t\t\tstruct compact_lock zol = {\n> > +\t\t\t\t.type = COMPACT_LOCK_ZONE,\n> > +\t\t\t\t.zone = cc->zone,\n> > +\t\t\t};\n> > +\n> > +\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n> >  \n> >  \t\t\t/* Recheck this is a buddy page under lock */\n> >  \t\t\tif (!PageBuddy(page))\n> > @@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n> >  \t}\n> >  \n> >  \tif (locked)\n> > -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> > +\t\tzone_unlock_irqrestore(cc->zone, flags);\n> >  \n> >  \t/*\n> >  \t * Be careful to not go outside of the pageblock.\n> > @@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n> >  \n> >  \t\t/* If we already hold the lock, we can skip some rechecking */\n> >  \t\tif (lruvec != locked) {\n> > +\t\t\tstruct compact_lock zol = {\n> > +\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n> > +\t\t\t\t.lock = &lruvec->lru_lock,\n> > +\t\t\t};\n> > +\n> >  \t\t\tif (locked)\n> >  \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n> >  \n> > -\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n> > +\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n> >  \t\t\tlocked = lruvec;\n> >  \n> >  \t\t\tlruvec_memcg_debug(lruvec, folio);\n> > @@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n> >  \t\tif (!area->nr_free)\n> >  \t\t\tcontinue;\n> >  \n> > -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> > +\t\tzone_lock_irqsave(cc->zone, flags);\n> >  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n> >  \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n> >  \t\t\tunsigned long pfn;\n> > @@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n> >  \t\t\t}\n> >  \t\t}\n> >  \n> > -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> > +\t\tzone_unlock_irqrestore(cc->zone, flags);\n> >  \n> >  \t\t/* Skip fast search if enough freepages isolated */\n> >  \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n> > @@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n> >  \t\tif (!area->nr_free)\n> >  \t\t\tcontinue;\n> >  \n> > -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> > +\t\tzone_lock_irqsave(cc->zone, flags);\n> >  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n> >  \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n> >  \t\t\tunsigned long free_pfn;\n> > @@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n> >  \t\t\t\tbreak;\n> >  \t\t\t}\n> >  \t\t}\n> > -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> > +\t\tzone_unlock_irqrestore(cc->zone, flags);\n> >  \t}\n> >  \n> >  \tcc->total_migrate_scanned += nr_scanned;\n> \n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/4] mm: zone lock tracepoint instrumentation",
          "message_id": "cover.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/cover.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Zone lock contention can significantly impact allocation and reclaim latency, as it is a central synchronization point in the page allocator and reclaim paths. Improved visibility into its behavior is therefore important for diagnosing performance issues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable zone lock contention. Deeper analysis of lock holders and waiters is currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints cover the slow path, they do not provide sufficient visibility into lock hold times. In particular, the lack of a release-side event makes it difficult to identify long lock holders and correlate them with waiters. As a result, distinguishing between short bursts of contention and pathological long hold times requires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to zone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock hold time measurements without affecting the fast path when tracing is disabled.\n\nThe series is structured as follows:\n\n1. Introduce zone lock wrappers. 2. Mechanically convert zone lock users to the wrappers. 3. Convert compaction to use the wrappers (requires minor restructuring of compact_lock_irqsave()). 4. Add zone lock tracepoints.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Cheatham, Benjamin",
              "summary": "I think you can improve the flow of this series if reorder as follows: 1. Introduce zone lock wrappers 4. Add zone lock tracepoints 2. Mechanically convert zone lock users to the wrappers 3. Convert compaction to use the wrappers... and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in patch 1 by the time they get to patch 4.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> Zone lock contention can significantly impact allocation and\n> reclaim latency, as it is a central synchronization point in\n> the page allocator and reclaim paths. Improved visibility into\n> its behavior is therefore important for diagnosing performance\n> issues in memory-intensive workloads.\n> \n> On some production workloads at Meta, we have observed noticeable\n> zone lock contention. Deeper analysis of lock holders and waiters\n> is currently difficult with existing instrumentation.\n> \n> While generic lock contention_begin/contention_end tracepoints\n> cover the slow path, they do not provide sufficient visibility\n> into lock hold times. In particular, the lack of a release-side\n> event makes it difficult to identify long lock holders and\n> correlate them with waiters. As a result, distinguishing between\n> short bursts of contention and pathological long hold times\n> requires additional instrumentation.\n> \n> This patch series adds dedicated tracepoint instrumentation to\n> zone lock, following the existing mmap_lock tracing model.\n> \n> The goal is to enable detailed holder/waiter analysis and lock\n> hold time measurements without affecting the fast path when\n> tracing is disabled.\n> \n> The series is structured as follows:\n> \n>   1. Introduce zone lock wrappers.\n>   2. Mechanically convert zone lock users to the wrappers.\n>   3. Convert compaction to use the wrappers (requires minor\n>      restructuring of compact_lock_irqsave()).\n>   4. Add zone lock tracepoints.\n\nI think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "I don't think this suggestion will make anything better. This just seems like a different taste. If I make a suggestion, I would request to squash (1) and (2) i.e. patch containing wrappers and their use together but that is just my taste and would be a nit. The series ordering is good as is.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:\n> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> > Zone lock contention can significantly impact allocation and\n> > reclaim latency, as it is a central synchronization point in\n> > the page allocator and reclaim paths. Improved visibility into\n> > its behavior is therefore important for diagnosing performance\n> > issues in memory-intensive workloads.\n> > \n> > On some production workloads at Meta, we have observed noticeable\n> > zone lock contention. Deeper analysis of lock holders and waiters\n> > is currently difficult with existing instrumentation.\n> > \n> > While generic lock contention_begin/contention_end tracepoints\n> > cover the slow path, they do not provide sufficient visibility\n> > into lock hold times. In particular, the lack of a release-side\n> > event makes it difficult to identify long lock holders and\n> > correlate them with waiters. As a result, distinguishing between\n> > short bursts of contention and pathological long hold times\n> > requires additional instrumentation.\n> > \n> > This patch series adds dedicated tracepoint instrumentation to\n> > zone lock, following the existing mmap_lock tracing model.\n> > \n> > The goal is to enable detailed holder/waiter analysis and lock\n> > hold time measurements without affecting the fast path when\n> > tracing is disabled.\n> > \n> > The series is structured as follows:\n> > \n> >   1. Introduce zone lock wrappers.\n> >   2. Mechanically convert zone lock users to the wrappers.\n> >   3. Convert compaction to use the wrappers (requires minor\n> >      restructuring of compact_lock_irqsave()).\n> >   4. Add zone lock tracepoints.\n> \n> I think you can improve the flow of this series if reorder as follows:\n> \t1. Introduce zone lock wrappers\n> \t4. Add zone lock tracepoints\n> \t2. Mechanically convert zone lock users to the wrappers\n> \t3. Convert compaction to use the wrappers...\n> \n> and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\n> wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\n> patch 1 by the time they get to patch 4.\n\nI don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "I structured the series intentionally to keep all behavior-preserving refactoring separate from the actual instrumentation change. In particular, I had to split the conversion into two patches to separate the purely mechanical changes from the compaction restructuring. With the current order, tracepoints addition remains a single, atomic functional change on top of a fully converted tree. This keeps the instrumentation isolated from the refactoring and with an intention to make bisection and review of the behavioral change easier. Reordering as suggested would mix instrumentation with intermediate refactoring states, which I'd prefer to avoid. I hope this reasoning makes sense, but I'm happy to discuss if there are strong objections.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:\n> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> > Zone lock contention can significantly impact allocation and\n> > reclaim latency, as it is a central synchronization point in\n> > the page allocator and reclaim paths. Improved visibility into\n> > its behavior is therefore important for diagnosing performance\n> > issues in memory-intensive workloads.\n> > \n> > On some production workloads at Meta, we have observed noticeable\n> > zone lock contention. Deeper analysis of lock holders and waiters\n> > is currently difficult with existing instrumentation.\n> > \n> > While generic lock contention_begin/contention_end tracepoints\n> > cover the slow path, they do not provide sufficient visibility\n> > into lock hold times. In particular, the lack of a release-side\n> > event makes it difficult to identify long lock holders and\n> > correlate them with waiters. As a result, distinguishing between\n> > short bursts of contention and pathological long hold times\n> > requires additional instrumentation.\n> > \n> > This patch series adds dedicated tracepoint instrumentation to\n> > zone lock, following the existing mmap_lock tracing model.\n> > \n> > The goal is to enable detailed holder/waiter analysis and lock\n> > hold time measurements without affecting the fast path when\n> > tracing is disabled.\n> > \n> > The series is structured as follows:\n> > \n> >   1. Introduce zone lock wrappers.\n> >   2. Mechanically convert zone lock users to the wrappers.\n> >   3. Convert compaction to use the wrappers (requires minor\n> >      restructuring of compact_lock_irqsave()).\n> >   4. Add zone lock tracepoints.\n> \n> I think you can improve the flow of this series if reorder as follows:\n> \t1. Introduce zone lock wrappers\n> \t4. Add zone lock tracepoints\n> \t2. Mechanically convert zone lock users to the wrappers\n> \t3. Convert compaction to use the wrappers...\n> \n> and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\n> wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\n> patch 1 by the time they get to patch 4.\n\nHi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.\n\n> \n> Thanks,\n> Ben\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "No that's fine, I figured as much. I just wasn't sure that was more important to you than what (I thought) was a better reading order for the series.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "\n\nOn 2/23/2026 10:46 AM, Dmitry Ilvokhin wrote:\n> [You don't often get email from d@ilvokhin.com. Learn why this is important at https://aka.ms/LearnAboutSenderIdentification ]\n> \n> On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:\n>> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n>>> Zone lock contention can significantly impact allocation and\n>>> reclaim latency, as it is a central synchronization point in\n>>> the page allocator and reclaim paths. Improved visibility into\n>>> its behavior is therefore important for diagnosing performance\n>>> issues in memory-intensive workloads.\n>>>\n>>> On some production workloads at Meta, we have observed noticeable\n>>> zone lock contention. Deeper analysis of lock holders and waiters\n>>> is currently difficult with existing instrumentation.\n>>>\n>>> While generic lock contention_begin/contention_end tracepoints\n>>> cover the slow path, they do not provide sufficient visibility\n>>> into lock hold times. In particular, the lack of a release-side\n>>> event makes it difficult to identify long lock holders and\n>>> correlate them with waiters. As a result, distinguishing between\n>>> short bursts of contention and pathological long hold times\n>>> requires additional instrumentation.\n>>>\n>>> This patch series adds dedicated tracepoint instrumentation to\n>>> zone lock, following the existing mmap_lock tracing model.\n>>>\n>>> The goal is to enable detailed holder/waiter analysis and lock\n>>> hold time measurements without affecting the fast path when\n>>> tracing is disabled.\n>>>\n>>> The series is structured as follows:\n>>>\n>>>   1. Introduce zone lock wrappers.\n>>>   2. Mechanically convert zone lock users to the wrappers.\n>>>   3. Convert compaction to use the wrappers (requires minor\n>>>      restructuring of compact_lock_irqsave()).\n>>>   4. Add zone lock tracepoints.\n>>\n>> I think you can improve the flow of this series if reorder as follows:\n>>       1. Introduce zone lock wrappers\n>>       4. Add zone lock tracepoints\n>>       2. Mechanically convert zone lock users to the wrappers\n>>       3. Convert compaction to use the wrappers...\n>>\n>> and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\n>> wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\n>> patch 1 by the time they get to patch 4.\n> \n> Hi Ben,\n> \n> Thanks for the suggestion.\n> \n> I structured the series intentionally to keep all behavior-preserving\n> refactoring separate from the actual instrumentation change.\n> \n> In particular, I had to split the conversion into two patches to\n> separate the purely mechanical changes from the compaction\n> restructuring. With the current order, tracepoints addition remains a\n> single, atomic functional change on top of a fully converted tree. This\n> keeps the instrumentation isolated from the refactoring and with an\n> intention to make bisection and review of the behavioral change easier.\n> \n> Reordering as suggested would mix instrumentation with intermediate\n> refactoring states, which I'd prefer to avoid.\n> \n> I hope this reasoning makes sense, but I'm happy to discuss if there are\n> strong objections.\n\nNo that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen\n\n> \n>>\n>> Thanks,\n>> Ben\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 2/4] mm: convert zone lock users to wrappers",
          "message_id": "7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Replace direct zone lock acquire/release operations with the newly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change intended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be handled separately in the following patch due to additional non-trivial modifications.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Wed, Feb 11, 2026 at 03:22:14PM +0000, Dmitry Ilvokhin wrote:\n> Replace direct zone lock acquire/release operations with the\n> newly introduced wrappers.\n> \n> The changes are purely mechanical substitutions. No functional change\n> intended. Locking semantics and ordering remain unchanged.\n> \n> The compaction path is left unchanged for now and will be\n> handled separately in the following patch due to additional\n> non-trivial modifications.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Add thin wrappers around zone lock acquire/release operations. This prepares the code for future tracepoint instrumentation without modifying individual call sites.\n\nCentralizing zone lock operations behind wrappers allows future instrumentation or debugging hooks to be added without touching all users.\n\nNo functional change intended. The wrappers are introduced in preparation for subsequent patches and are not yet used.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Any reason you used macros for above two and inlined functions for remaining?",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> Add thin wrappers around zone lock acquire/release operations. This\n> prepares the code for future tracepoint instrumentation without\n> modifying individual call sites.\n> \n> Centralizing zone lock operations behind wrappers allows future\n> instrumentation or debugging hooks to be added without touching\n> all users.\n> \n> No functional change intended. The wrappers are introduced in\n> preparation for subsequent patches and are not yet used.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> ---\n>  MAINTAINERS               |  1 +\n>  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n>  2 files changed, 39 insertions(+)\n>  create mode 100644 include/linux/zone_lock.h\n> \n> diff --git a/MAINTAINERS b/MAINTAINERS\n> index b4088f7290be..680c9ae02d7e 100644\n> --- a/MAINTAINERS\n> +++ b/MAINTAINERS\n> @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n>  F:\tinclude/linux/ptdump.h\n>  F:\tinclude/linux/vmpressure.h\n>  F:\tinclude/linux/vmstat.h\n> +F:\tinclude/linux/zone_lock.h\n>  F:\tkernel/fork.c\n>  F:\tmm/Kconfig\n>  F:\tmm/debug.c\n> diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> new file mode 100644\n> index 000000000000..c531e26280e6\n> --- /dev/null\n> +++ b/include/linux/zone_lock.h\n> @@ -0,0 +1,38 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +#ifndef _LINUX_ZONE_LOCK_H\n> +#define _LINUX_ZONE_LOCK_H\n> +\n> +#include <linux/mmzone.h>\n> +#include <linux/spinlock.h>\n> +\n> +static inline void zone_lock_init(struct zone *zone)\n> +{\n> +\tspin_lock_init(&zone->lock);\n> +}\n> +\n> +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> +do {\t\t\t\t\t\t\t\t\\\n> +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> +} while (0)\n> +\n> +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> +({\t\t\t\t\t\t\t\t\\\n> +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> +})\n\nAny reason you used macros for above two and inlined functions for remaining?\n\n> +\n> +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n> +{\n> +\tspin_unlock_irqrestore(&zone->lock, flags);\n> +}\n> +\n> +static inline void zone_lock_irq(struct zone *zone)\n> +{\n> +\tspin_lock_irq(&zone->lock);\n> +}\n> +\n> +static inline void zone_unlock_irq(struct zone *zone)\n> +{\n> +\tspin_unlock_irq(&zone->lock);\n> +}\n> +\n> +#endif /* _LINUX_ZONE_LOCK_H */\n> -- \n> 2.47.3\n> \n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The reason for using macros in those two cases is that they need to modify the flags variable passed by the caller, just like spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same convention here. If we used normal inline functions instead, we would need to pass a pointer to flags, which would change the call sites and diverge from the existing *_irqsave() locking pattern. There is also a difference between zone_lock_irqsave() and zone_trylock_irqsave() implementations: the former is implemented as a do { } while (0) macro since it does not return a value, while the latter uses a GCC extension in order to return the trylock result. This matches spin_lock_* convention as well.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > Add thin wrappers around zone lock acquire/release operations. This\n> > prepares the code for future tracepoint instrumentation without\n> > modifying individual call sites.\n> > \n> > Centralizing zone lock operations behind wrappers allows future\n> > instrumentation or debugging hooks to be added without touching\n> > all users.\n> > \n> > No functional change intended. The wrappers are introduced in\n> > preparation for subsequent patches and are not yet used.\n> > \n> > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > ---\n> >  MAINTAINERS               |  1 +\n> >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> >  2 files changed, 39 insertions(+)\n> >  create mode 100644 include/linux/zone_lock.h\n> > \n> > diff --git a/MAINTAINERS b/MAINTAINERS\n> > index b4088f7290be..680c9ae02d7e 100644\n> > --- a/MAINTAINERS\n> > +++ b/MAINTAINERS\n> > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> >  F:\tinclude/linux/ptdump.h\n> >  F:\tinclude/linux/vmpressure.h\n> >  F:\tinclude/linux/vmstat.h\n> > +F:\tinclude/linux/zone_lock.h\n> >  F:\tkernel/fork.c\n> >  F:\tmm/Kconfig\n> >  F:\tmm/debug.c\n> > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > new file mode 100644\n> > index 000000000000..c531e26280e6\n> > --- /dev/null\n> > +++ b/include/linux/zone_lock.h\n> > @@ -0,0 +1,38 @@\n> > +/* SPDX-License-Identifier: GPL-2.0 */\n> > +#ifndef _LINUX_ZONE_LOCK_H\n> > +#define _LINUX_ZONE_LOCK_H\n> > +\n> > +#include <linux/mmzone.h>\n> > +#include <linux/spinlock.h>\n> > +\n> > +static inline void zone_lock_init(struct zone *zone)\n> > +{\n> > +\tspin_lock_init(&zone->lock);\n> > +}\n> > +\n> > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > +do {\t\t\t\t\t\t\t\t\\\n> > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > +} while (0)\n> > +\n> > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > +({\t\t\t\t\t\t\t\t\\\n> > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > +})\n> \n> Any reason you used macros for above two and inlined functions for remaining?\n>\n\nThe reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.\n\n> > +\n> > +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n> > +{\n> > +\tspin_unlock_irqrestore(&zone->lock, flags);\n> > +}\n> > +\n> > +static inline void zone_lock_irq(struct zone *zone)\n> > +{\n> > +\tspin_lock_irq(&zone->lock);\n> > +}\n> > +\n> > +static inline void zone_unlock_irq(struct zone *zone)\n> > +{\n> > +\tspin_unlock_irq(&zone->lock);\n> > +}\n> > +\n> > +#endif /* _LINUX_ZONE_LOCK_H */\n> > -- \n> > 2.47.3\n> > \n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Cool, thanks for the explanation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 03:18:04PM +0000, Dmitry Ilvokhin wrote:\n> On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> > On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > > Add thin wrappers around zone lock acquire/release operations. This\n> > > prepares the code for future tracepoint instrumentation without\n> > > modifying individual call sites.\n> > > \n> > > Centralizing zone lock operations behind wrappers allows future\n> > > instrumentation or debugging hooks to be added without touching\n> > > all users.\n> > > \n> > > No functional change intended. The wrappers are introduced in\n> > > preparation for subsequent patches and are not yet used.\n> > > \n> > > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > > ---\n> > >  MAINTAINERS               |  1 +\n> > >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> > >  2 files changed, 39 insertions(+)\n> > >  create mode 100644 include/linux/zone_lock.h\n> > > \n> > > diff --git a/MAINTAINERS b/MAINTAINERS\n> > > index b4088f7290be..680c9ae02d7e 100644\n> > > --- a/MAINTAINERS\n> > > +++ b/MAINTAINERS\n> > > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> > >  F:\tinclude/linux/ptdump.h\n> > >  F:\tinclude/linux/vmpressure.h\n> > >  F:\tinclude/linux/vmstat.h\n> > > +F:\tinclude/linux/zone_lock.h\n> > >  F:\tkernel/fork.c\n> > >  F:\tmm/Kconfig\n> > >  F:\tmm/debug.c\n> > > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > > new file mode 100644\n> > > index 000000000000..c531e26280e6\n> > > --- /dev/null\n> > > +++ b/include/linux/zone_lock.h\n> > > @@ -0,0 +1,38 @@\n> > > +/* SPDX-License-Identifier: GPL-2.0 */\n> > > +#ifndef _LINUX_ZONE_LOCK_H\n> > > +#define _LINUX_ZONE_LOCK_H\n> > > +\n> > > +#include <linux/mmzone.h>\n> > > +#include <linux/spinlock.h>\n> > > +\n> > > +static inline void zone_lock_init(struct zone *zone)\n> > > +{\n> > > +\tspin_lock_init(&zone->lock);\n> > > +}\n> > > +\n> > > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > > +do {\t\t\t\t\t\t\t\t\\\n> > > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +} while (0)\n> > > +\n> > > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > > +({\t\t\t\t\t\t\t\t\\\n> > > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +})\n> > \n> > Any reason you used macros for above two and inlined functions for remaining?\n> >\n> \n> The reason for using macros in those two cases is that they need to\n> modify the flags variable passed by the caller, just like\n> spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\n> convention here.\n> \n> If we used normal inline functions instead, we would need to pass a\n> pointer to flags, which would change the call sites and diverge from the\n> existing *_irqsave() locking pattern.\n> \n> There is also a difference between zone_lock_irqsave() and\n> zone_trylock_irqsave() implementations: the former is implemented as a\n> do { } while (0) macro since it does not return a value, while the\n> latter uses a GCC extension in order to return the trylock result. This\n> matches spin_lock_* convention as well.\n> \n\nCool, thanks for the explanation.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> Add thin wrappers around zone lock acquire/release operations. This\n> prepares the code for future tracepoint instrumentation without\n> modifying individual call sites.\n> \n> Centralizing zone lock operations behind wrappers allows future\n> instrumentation or debugging hooks to be added without touching\n> all users.\n> \n> No functional change intended. The wrappers are introduced in\n> preparation for subsequent patches and are not yet used.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Steven Rostedt",
              "summary": "Have you thought about adding guards as well. It could make the code simpler: (Not tested) #include <linux/cleanup.h> [..] DEFINE_LOCK_GUARD_1(zonelock_irqsave, struct zone *, zone_lock_irqsave(_T->lock, _T->flags), zone_unlock_irqrestore(_T->lock, _T->flags), unsigned long flags) DECLARE_LOCK_GUARD_1_ATTRS(zonelock_irqsave, __acquires(_T), __releases(*(struct zone ***)_T)) #define class_zonelock_irqsave_constructor(_T) WITH_LOCK_GUARD_1_ATTRS(zonelock_irqsave, _T) DEFINE_LOCK_GUARD_1(zonelock_irq, struct zone *, zone_lock_irq(_T->lock), zone_unlock_irq(_T->lock)) DECLARE_LOCK_GUARD_1_ATTRS(zonelock_irq, __acquires(_T), __releases(*(struct zone ***)_T)) #define...",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Wed, 11 Feb 2026 15:22:13 +0000\nDmitry Ilvokhin <d@ilvokhin.com> wrote:\n\n\n> diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> new file mode 100644\n> index 000000000000..c531e26280e6\n> --- /dev/null\n> +++ b/include/linux/zone_lock.h\n> @@ -0,0 +1,38 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +#ifndef _LINUX_ZONE_LOCK_H\n> +#define _LINUX_ZONE_LOCK_H\n> +\n> +#include <linux/mmzone.h>\n> +#include <linux/spinlock.h>\n> +\n> +static inline void zone_lock_init(struct zone *zone)\n> +{\n> +\tspin_lock_init(&zone->lock);\n> +}\n> +\n> +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> +do {\t\t\t\t\t\t\t\t\\\n> +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> +} while (0)\n> +\n> +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> +({\t\t\t\t\t\t\t\t\\\n> +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> +})\n> +\n> +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n> +{\n> +\tspin_unlock_irqrestore(&zone->lock, flags);\n> +}\n> +\n> +static inline void zone_lock_irq(struct zone *zone)\n> +{\n> +\tspin_lock_irq(&zone->lock);\n> +}\n> +\n> +static inline void zone_unlock_irq(struct zone *zone)\n> +{\n> +\tspin_unlock_irq(&zone->lock);\n> +}\n> +\n> +#endif /* _LINUX_ZONE_LOCK_H */\n\nHave you thought about adding guards as well. It could make the code simpler:\n\n  (Not tested)\n\n#include <linux/cleanup.h>\n[..]\n\nDEFINE_LOCK_GUARD_1(zonelock_irqsave, struct zone *,\n\t\t    zone_lock_irqsave(_T->lock, _T->flags),\n\t\t    zone_unlock_irqrestore(_T->lock, _T->flags),\n\t\t    unsigned long flags)\nDECLARE_LOCK_GUARD_1_ATTRS(zonelock_irqsave, __acquires(_T), __releases(*(struct zone ***)_T))\n#define class_zonelock_irqsave_constructor(_T) WITH_LOCK_GUARD_1_ATTRS(zonelock_irqsave, _T)\n\nDEFINE_LOCK_GUARD_1(zonelock_irq, struct zone *,\n\t\t    zone_lock_irq(_T->lock),\n\t\t    zone_unlock_irq(_T->lock))\nDECLARE_LOCK_GUARD_1_ATTRS(zonelock_irq, __acquires(_T), __releases(*(struct zone ***)_T))\n#define class_zonelock_irq_constructor(_T) WITH_LOCK_GUARD_1_ATTRS(zonelock_irq, _T)\n\nThen you could even remove the \"flags\" variables from the C code, and some goto unlocks.\n\n-- Steve\n\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 0/4] mm: zone lock tracepoint instrumentation",
          "message_id": "aZyEctoThn0anlz8@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aZyEctoThn0anlz8@shell.ilvokhin.com/",
          "date": "2026-02-23T16:52:49Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Cheatham, Benjamin",
              "summary": "No that's fine, I figured as much. I just wasn't sure that was more important to you than what (I thought) was a better reading order for the series.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "\n\nOn 2/23/2026 10:46 AM, Dmitry Ilvokhin wrote:\n> [You don't often get email from d@ilvokhin.com. Learn why this is important at https://aka.ms/LearnAboutSenderIdentification ]\n> \n> On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:\n>> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n>>> Zone lock contention can significantly impact allocation and\n>>> reclaim latency, as it is a central synchronization point in\n>>> the page allocator and reclaim paths. Improved visibility into\n>>> its behavior is therefore important for diagnosing performance\n>>> issues in memory-intensive workloads.\n>>>\n>>> On some production workloads at Meta, we have observed noticeable\n>>> zone lock contention. Deeper analysis of lock holders and waiters\n>>> is currently difficult with existing instrumentation.\n>>>\n>>> While generic lock contention_begin/contention_end tracepoints\n>>> cover the slow path, they do not provide sufficient visibility\n>>> into lock hold times. In particular, the lack of a release-side\n>>> event makes it difficult to identify long lock holders and\n>>> correlate them with waiters. As a result, distinguishing between\n>>> short bursts of contention and pathological long hold times\n>>> requires additional instrumentation.\n>>>\n>>> This patch series adds dedicated tracepoint instrumentation to\n>>> zone lock, following the existing mmap_lock tracing model.\n>>>\n>>> The goal is to enable detailed holder/waiter analysis and lock\n>>> hold time measurements without affecting the fast path when\n>>> tracing is disabled.\n>>>\n>>> The series is structured as follows:\n>>>\n>>>   1. Introduce zone lock wrappers.\n>>>   2. Mechanically convert zone lock users to the wrappers.\n>>>   3. Convert compaction to use the wrappers (requires minor\n>>>      restructuring of compact_lock_irqsave()).\n>>>   4. Add zone lock tracepoints.\n>>\n>> I think you can improve the flow of this series if reorder as follows:\n>>       1. Introduce zone lock wrappers\n>>       4. Add zone lock tracepoints\n>>       2. Mechanically convert zone lock users to the wrappers\n>>       3. Convert compaction to use the wrappers...\n>>\n>> and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\n>> wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\n>> patch 1 by the time they get to patch 4.\n> \n> Hi Ben,\n> \n> Thanks for the suggestion.\n> \n> I structured the series intentionally to keep all behavior-preserving\n> refactoring separate from the actual instrumentation change.\n> \n> In particular, I had to split the conversion into two patches to\n> separate the purely mechanical changes from the compaction\n> restructuring. With the current order, tracepoints addition remains a\n> single, atomic functional change on top of a fully converted tree. This\n> keeps the instrumentation isolated from the refactoring and with an\n> intention to make bisection and review of the behavioral change easier.\n> \n> Reordering as suggested would mix instrumentation with intermediate\n> refactoring states, which I'd prefer to avoid.\n> \n> I hope this reasoning makes sense, but I'm happy to discuss if there are\n> strong objections.\n\nNo that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen\n\n> \n>>\n>> Thanks,\n>> Ben\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Gregory Price",
      "primary_email": "gourry@gourry.net",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[RFC PATCH v4 27/27] cxl: add cxl_compression PCI driver",
          "message_id": "20260222084842.1824063-28-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260222084842.1824063-28-gourry@gourry.net/",
          "date": "2026-02-22T08:50:38Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-22",
          "patch_summary": "Add a generic CXL type-3 driver for compressed memory controllers.\n\nThe driver provides an alternative PCI binding that converts CXL RAM regions to private-node sysram and registers them with the CRAM subsystem for transparent demotion/promotion.\n\nProbe flow: 1. cxl_pci_type3_probe_init() for standard CXL device setup 2. Discover/convert auto-RAM regions or create a RAM region 3. Convert to private-node sysram via devm_cxl_add_sysram() 4. Register with CRAM via cram_register_private_node()\n\nPage flush pipeline: When a CRAM folio is freed, the CRAM free_folio   callback buffers it into a per-CPU RCU-protected flush buffer to offload the operation.\n\nA periodic kthread swaps the per-CPU buffers under RCU, then sends batched Sanitize-Zero commands so the device can zero pages.\n\nA flush_record bitmap tracks in-flight pages to avoid re-buffering on the second free_folio entry after folio_put().\n\nOverflow from full buffers is handled by a per-CPU workqueue fallback.\n\nWatermark interrupts: MSI-X vector 12 - delivers \"Low\" watermark interrupts MSI-X vector 13 - delivers \"High\" watermark interrupts This adjusts CRAM pressure: Low  - increases pressure. High - reduces pressure.",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/2] cxl/region: fix region leak when attach_target fails in cxl_add_to_region",
          "message_id": "20260221043013.1420169-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260221043013.1420169-1-gourry@gourry.net/",
          "date": "2026-02-21T04:30:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-21",
          "patch_summary": "cxl_add_to_region() ignores the return value of attach_target().  When attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO), the auto-discovered region remains registered with its HPA resource consumed but never reaches COMMIT state.  Subsequent region creation attempts fail with -ENOSPC because the HPA range is already reserved.\n\nTrack whether this call to cxl_add_to_region() created the region, and call drop_region() on attach_target() failure to unregister it and release the HPA resource.  Pre-existing regions are left alone since other endpoints may already be attached.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "BAH - disregard this patch, it uses drop_region which is introduced by Alejandro here: https://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 11:30:12PM -0500, Gregory Price wrote:\n> cxl_add_to_region() ignores the return value of attach_target().  When\n> attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO),\n> the auto-discovered region remains registered with its HPA resource\n> consumed but never reaches COMMIT state.  Subsequent region creation\n> attempts fail with -ENOSPC because the HPA range is already reserved.\n> \n> Track whether this call to cxl_add_to_region() created the region, and\n> call drop_region() on attach_target() failure to unregister it and\n> release the HPA resource.  Pre-existing regions are left alone since\n> other endpoints may already be attached.\n> \n> Signed-off-by: Gregory Price <gourry@gourry.net>\n\nBAH - disregard this patch, it uses drop_region which is introduced by\nAlejandro here:\n\nhttps://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/\n\n",
              "reply_to": "",
              "message_date": "2026-02-21",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Alison Schofield",
              "summary": "I see you dropping this, perhaps just for the moment, because the drop_region() you wanted to use is not available yet. This looks a lot like https://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/ cxl/region: Unregister auto-created region when assembly fails When auto-created region assembly fails the region remains registered but disabled. The region continues to reserve its memory resource, preventing DAX from registering the memory. Unregister the region on assembly failure to release the resource. And the review comments on that one, or at least on that thread in general, was to leave all the broken things in place. I didn't agree with that, and hope to see this version move ahead when you have the drop_region you need.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 11:30:12PM -0500, Gregory Price wrote:\n> cxl_add_to_region() ignores the return value of attach_target().  When\n> attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO),\n> the auto-discovered region remains registered with its HPA resource\n> consumed but never reaches COMMIT state.  Subsequent region creation\n> attempts fail with -ENOSPC because the HPA range is already reserved.\n> \n> Track whether this call to cxl_add_to_region() created the region, and\n> call drop_region() on attach_target() failure to unregister it and\n> release the HPA resource.  Pre-existing regions are left alone since\n> other endpoints may already be attached.\n\nI see you dropping this, perhaps just for the moment, because\nthe drop_region() you wanted to use is not available yet.\n\nThis looks a lot like \n\thttps://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/\n\tcxl/region: Unregister auto-created region when assembly fails\n\tWhen auto-created region assembly fails the region remains registered\n\tbut disabled. The region continues to reserve its memory resource,\n\tpreventing DAX from registering the memory.\n\tUnregister the region on assembly failure to release the resource.\n\nAnd the review comments on that one, or at least on that thread in\ngeneral, was to leave all the broken things in place.\nI didn't agree with that, and hope to see this version move ahead\nwhen you have the drop_region you need.\n\n-- Alison\n\n\n\n\n\n\n> \n> Signed-off-by: Gregory Price <gourry@gourry.net>\n> ---\n>  drivers/cxl/core/region.c | 15 ++++++++++++---\n>  1 file changed, 12 insertions(+), 3 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 385be9cb44cd..276046d49f88 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -3923,6 +3923,7 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n>  \tstruct cxl_region_context ctx;\n>  \tstruct cxl_region_params *p;\n>  \tbool attach = false;\n> +\tbool newly_created = false;\n>  \tint rc;\n>  \n>  \tctx = (struct cxl_region_context) {\n> @@ -3946,15 +3947,23 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n>  \tmutex_lock(&cxlrd->range_lock);\n>  \tstruct cxl_region *cxlr __free(put_cxl_region) =\n>  \t\tcxl_find_region_by_range(cxlrd, &ctx.hpa_range);\n> -\tif (!cxlr)\n> +\tif (!cxlr) {\n>  \t\tcxlr = construct_region(cxlrd, &ctx);\n> +\t\tnewly_created = !IS_ERR(cxlr);\n> +\t}\n>  \tmutex_unlock(&cxlrd->range_lock);\n>  \n>  \trc = PTR_ERR_OR_ZERO(cxlr);\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> -\tattach_target(cxlr, cxled, -1, TASK_UNINTERRUPTIBLE);\n> +\trc = attach_target(cxlr, cxled, -1, TASK_UNINTERRUPTIBLE);\n> +\tif (rc) {\n> +\t\t/* If endpoint was just created, tear it down to release HPA */\n> +\t\tif (newly_created)\n> +\t\t\tdrop_region(cxlrd, cxlr);\n> +\t\treturn rc;\n> +\t}\n>  \n>  \tscoped_guard(rwsem_read, &cxl_rwsem.region) {\n>  \t\tp = &cxlr->params;\n> @@ -3972,7 +3981,7 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n>  \t\t\t\tp->res);\n>  \t}\n>  \n> -\treturn rc;\n> +\treturn 0;\n>  }\n>  EXPORT_SYMBOL_NS_GPL(cxl_add_to_region, \"CXL\");\n>  \n> -- \n> 2.47.3\n> \n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Yeah it's not a particularly useful cleanup in the current infrastructure because nothing actually uses this pattern (yet). The important note here is the difference between auto-regions and manually created regions.  For auto-regions, you might have another endpoint show up looking for the partially created region - and then just go off and create it anyway because it thinks it was first. But in my driver, i'm explicitly converting these auto-regions into other things, and if that fails it causes *all other* region creation to fail - even if it wasn't actually dependent on that original region. This is only an issue if you have two devices unbind/bind cycling at the same time - i.e. echo 0000:d0:00.00 > cxl_pci/unbind echo 0000:e0:00.00 > cxl_pci/unbind echo 0000:d0:00.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 11:48:42AM -0800, Alison Schofield wrote:\n> On Fri, Feb 20, 2026 at 11:30:12PM -0500, Gregory Price wrote:\n> > cxl_add_to_region() ignores the return value of attach_target().  When\n> > attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO),\n> > the auto-discovered region remains registered with its HPA resource\n> > consumed but never reaches COMMIT state.  Subsequent region creation\n> > attempts fail with -ENOSPC because the HPA range is already reserved.\n> > \n> > Track whether this call to cxl_add_to_region() created the region, and\n> > call drop_region() on attach_target() failure to unregister it and\n> > release the HPA resource.  Pre-existing regions are left alone since\n> > other endpoints may already be attached.\n> \n> I see you dropping this, perhaps just for the moment, because\n> the drop_region() you wanted to use is not available yet.\n> \n\nYeah it's not a particularly useful cleanup in the current\ninfrastructure because nothing actually uses this pattern (yet).\n\n> This looks a lot like \n> \thttps://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/\n> \tcxl/region: Unregister auto-created region when assembly fails\n> \tWhen auto-created region assembly fails the region remains registered\n> \tbut disabled. The region continues to reserve its memory resource,\n> \tpreventing DAX from registering the memory.\n> \tUnregister the region on assembly failure to release the resource.\n> \n> And the review comments on that one, or at least on that thread in\n> general, was to leave all the broken things in place.\n> I didn't agree with that, and hope to see this version move ahead\n> when you have the drop_region you need.\n> \n> \n\nThe important note here is the difference between auto-regions and\nmanually created regions.  For auto-regions, you might have another\nendpoint show up looking for the partially created region - and then\njust go off and create it anyway because it thinks it was first.\n\nBut in my driver, i'm explicitly converting these auto-regions into\nother things, and if that fails it causes *all other* region creation to\nfail - even if it wasn't actually dependent on that original region.\n\nThis is only an issue if you have two devices unbind/bind cycling at\nthe same time - i.e.\n\n   echo 0000:d0:00.00 > cxl_pci/unbind\n   echo 0000:e0:00.00 > cxl_pci/unbind\n   echo 0000:d0:00.00 > mydriver/bind\n   echo 0000:e0:00.00 > mydriver/bind\n\nIf the platform has pre-programmed and locked the decoders, and one of\nthe two devices fails to probe and leaves a hanging partially\ncreated region, the other device will fail too.\n\nIt's a pretty narrow failure scenario.\n\n~Gregory\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Alison Schofield",
              "summary": "That's by design, and that'll eventually fail too. But - is see how your case is different. Thanks for the explanation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 03:15:16PM -0500, Gregory Price wrote:\n> On Mon, Feb 23, 2026 at 11:48:42AM -0800, Alison Schofield wrote:\n> > On Fri, Feb 20, 2026 at 11:30:12PM -0500, Gregory Price wrote:\n> > > cxl_add_to_region() ignores the return value of attach_target().  When\n> > > attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO),\n> > > the auto-discovered region remains registered with its HPA resource\n> > > consumed but never reaches COMMIT state.  Subsequent region creation\n> > > attempts fail with -ENOSPC because the HPA range is already reserved.\n> > > \n> > > Track whether this call to cxl_add_to_region() created the region, and\n> > > call drop_region() on attach_target() failure to unregister it and\n> > > release the HPA resource.  Pre-existing regions are left alone since\n> > > other endpoints may already be attached.\n> > \n> > I see you dropping this, perhaps just for the moment, because\n> > the drop_region() you wanted to use is not available yet.\n> > \n> \n> Yeah it's not a particularly useful cleanup in the current\n> infrastructure because nothing actually uses this pattern (yet).\n> \n> > This looks a lot like \n> > \thttps://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/\n> > \tcxl/region: Unregister auto-created region when assembly fails\n> > \tWhen auto-created region assembly fails the region remains registered\n> > \tbut disabled. The region continues to reserve its memory resource,\n> > \tpreventing DAX from registering the memory.\n> > \tUnregister the region on assembly failure to release the resource.\n> > \n> > And the review comments on that one, or at least on that thread in\n> > general, was to leave all the broken things in place.\n> > I didn't agree with that, and hope to see this version move ahead\n> > when you have the drop_region you need.\n> > \n> > \n> \n> The important note here is the difference between auto-regions and\n> manually created regions.  For auto-regions, you might have another\n> endpoint show up looking for the partially created region - and then\n> just go off and create it anyway because it thinks it was first.\n\nThat's by design, and that'll eventually fail too.\n\nBut - is see how your case is different. Thanks for the explanation.\n\n> \n> But in my driver, i'm explicitly converting these auto-regions into\n> other things, and if that fails it causes *all other* region creation to\n> fail - even if it wasn't actually dependent on that original region.\n> \n> This is only an issue if you have two devices unbind/bind cycling at\n> the same time - i.e.\n> \n>    echo 0000:d0:00.00 > cxl_pci/unbind\n>    echo 0000:e0:00.00 > cxl_pci/unbind\n>    echo 0000:d0:00.00 > mydriver/bind\n>    echo 0000:e0:00.00 > mydriver/bind\n> \n> If the platform has pre-programmed and locked the decoders, and one of\n> the two devices fails to probe and leaves a hanging partially\n> created region, the other device will fail too.\n> \n> It's a pretty narrow failure scenario.\n> \n> ~Gregory\n> \n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Feel free to add it to this series. I have started to send individual series as you know but the part changing the region creation will require more work than the already sent. About this fix, it looks good to me, although I have to admit I'm a bit lost after following the discussion Allison points to. If we want to keep the state of failure for forensics, not sure if the debugging/tracing or default error info in this case will be enough. In any case:",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\nOn 2/21/26 05:17, Gregory Price wrote:\n> On Fri, Feb 20, 2026 at 11:30:12PM -0500, Gregory Price wrote:\n>> cxl_add_to_region() ignores the return value of attach_target().  When\n>> attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO),\n>> the auto-discovered region remains registered with its HPA resource\n>> consumed but never reaches COMMIT state.  Subsequent region creation\n>> attempts fail with -ENOSPC because the HPA range is already reserved.\n>>\n>> Track whether this call to cxl_add_to_region() created the region, and\n>> call drop_region() on attach_target() failure to unregister it and\n>> release the HPA resource.  Pre-existing regions are left alone since\n>> other endpoints may already be attached.\n>>\n>> Signed-off-by: Gregory Price <gourry@gourry.net>\n> BAH - disregard this patch, it uses drop_region which is introduced by\n> Alejandro here:\n>\n> https://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/\n>\nFeel free to add it to this series. I have started to send individual \nseries as you know but the part changing the region creation will \nrequire more work than the already sent.\n\nAbout this fix, it looks good to me, although I have to admit I'm a bit \nlost after following the discussion Allison points to. If we want to \nkeep the state of failure for forensics, not sure if the \ndebugging/tracing or default error info in this case will be enough.\n\nIn any case:\n\nReviewed-by: Alejandro Lucero <alucerop@amd.com>\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Yeah i don't quite follow the want to keep the objects around, it seems to cause more issues than it solves - but then i also don't think this is going to be a particularly common problem",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 04:15:33PM +0000, Alejandro Lucero Palau wrote:\n> \n> On 2/21/26 05:17, Gregory Price wrote:\n> > On Fri, Feb 20, 2026 at 11:30:12PM -0500, Gregory Price wrote:\n> > > cxl_add_to_region() ignores the return value of attach_target().  When\n> > > attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO),\n> > > the auto-discovered region remains registered with its HPA resource\n> > > consumed but never reaches COMMIT state.  Subsequent region creation\n> > > attempts fail with -ENOSPC because the HPA range is already reserved.\n> > > \n> > > Track whether this call to cxl_add_to_region() created the region, and\n> > > call drop_region() on attach_target() failure to unregister it and\n> > > release the HPA resource.  Pre-existing regions are left alone since\n> > > other endpoints may already be attached.\n> > > \n> > > Signed-off-by: Gregory Price <gourry@gourry.net>\n> > BAH - disregard this patch, it uses drop_region which is introduced by\n> > Alejandro here:\n> > \n> > https://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/\n> > \n> Feel free to add it to this series. I have started to send individual series\n> as you know but the part changing the region creation will require more work\n> than the already sent.\n> \n> About this fix, it looks good to me, although I have to admit I'm a bit lost\n> after following the discussion Allison points to. If we want to keep the\n> state of failure for forensics, not sure if the debugging/tracing or default\n> error info in this case will be enough.\n> \n> In any case:\n> \n> Reviewed-by: Alejandro Lucero <alucerop@amd.com>\n> \n\nYeah i don't quite follow the want to keep the objects around, it seems\nto cause more issues than it solves - but then i also don't think this\nis going to be a particularly common problem\n\n~Gregory\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] cxl/core: fix test_bit misuse with CXL_DECODER_F_ bitmask flags",
          "message_id": "20260221021810.1390342-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260221021810.1390342-1-gourry@gourry.net/",
          "date": "2026-02-21T02:18:16Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-21",
          "patch_summary": "CXL_DECODER_F_LOCK (BIT(4) = 16) and CXL_DECODER_F_NORMALIZED_ADDRESSING (BIT(6) = 64) are bit masks, but three call sites pass them to test_bit() which expects a bit number.\n\nReplace test_bit() with direct bitmask tests, consistent with every other use of these flags.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Cheatham, Benjamin",
              "summary": "Alison sent out a patch [1] two weeks ago for this. I suspect you found this bug independently, so I figured I should point it out. Otherwise, I would add a Reported-by (or some other tag) with her name. [1]: https://lore.kernel.org/linux-cxl/20260206181404.1025991-1-alison.schofield@intel.com/",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/20/2026 8:18 PM, Gregory Price wrote:\n> CXL_DECODER_F_LOCK (BIT(4) = 16) and CXL_DECODER_F_NORMALIZED_ADDRESSING\n> (BIT(6) = 64) are bit masks, but three call sites pass them to test_bit()\n> which expects a bit number.\n> \n> Replace test_bit() with direct bitmask tests, consistent with every other\n> use of these flags.\n> \n> Fixes: 2230c4bdc412 (\"cxl: Add handling of locked CXL decoder\")\n> Signed-off-by: Gregory Price <gourry@gourry.net>\n\nAlison sent out a patch [1] two weeks ago for this. I suspect you found this bug\nindependently, so I figured I should point it out. Otherwise, I would add a Reported-by (or some\nother tag) with her name.\n\nThanks,\nBen\n\n[1]: https://lore.kernel.org/linux-cxl/20260206181404.1025991-1-alison.schofield@intel.com/\n> ---\n>  drivers/cxl/core/hdm.c    | 2 +-\n>  drivers/cxl/core/region.c | 4 ++--\n>  2 files changed, 3 insertions(+), 3 deletions(-)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index b2db5967f5c0..c8fcb9d9aa3d 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -904,7 +904,7 @@ static void cxl_decoder_reset(struct cxl_decoder *cxld)\n>  \tif ((cxld->flags & CXL_DECODER_F_ENABLE) == 0)\n>  \t\treturn;\n>  \n> -\tif (test_bit(CXL_DECODER_F_LOCK, &cxld->flags))\n> +\tif (cxld->flags & CXL_DECODER_F_LOCK)\n>  \t\treturn;\n>  \n>  \tif (port->commit_end == id)\n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index bd4c4a4a27da..385be9cb44cd 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -1100,12 +1100,12 @@ static int cxl_rr_assign_decoder(struct cxl_port *port, struct cxl_region *cxlr,\n>  static void cxl_region_setup_flags(struct cxl_region *cxlr,\n>  \t\t\t\t   struct cxl_decoder *cxld)\n>  {\n> -\tif (test_bit(CXL_DECODER_F_LOCK, &cxld->flags)) {\n> +\tif (cxld->flags & CXL_DECODER_F_LOCK) {\n>  \t\tset_bit(CXL_REGION_F_LOCK, &cxlr->flags);\n>  \t\tclear_bit(CXL_REGION_F_NEEDS_RESET, &cxlr->flags);\n>  \t}\n>  \n> -\tif (test_bit(CXL_DECODER_F_NORMALIZED_ADDRESSING, &cxld->flags))\n> +\tif (cxld->flags & CXL_DECODER_F_NORMALIZED_ADDRESSING)\n>  \t\tset_bit(CXL_REGION_F_NORMALIZED_ADDRESSING, &cxlr->flags);\n>  }\n>  \n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Ah, yeah, missed this, and did find independently when testing unbinds. Wasn't on cxl/next so I thought it hadn't been found yet. Cool, thanks! ~Gregory",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 11:33:13AM -0600, Cheatham, Benjamin wrote:\n> On 2/20/2026 8:18 PM, Gregory Price wrote:\n> > CXL_DECODER_F_LOCK (BIT(4) = 16) and CXL_DECODER_F_NORMALIZED_ADDRESSING\n> > (BIT(6) = 64) are bit masks, but three call sites pass them to test_bit()\n> > which expects a bit number.\n> > \n> > Replace test_bit() with direct bitmask tests, consistent with every other\n> > use of these flags.\n> > \n> > Fixes: 2230c4bdc412 (\"cxl: Add handling of locked CXL decoder\")\n> > Signed-off-by: Gregory Price <gourry@gourry.net>\n> \n> Alison sent out a patch [1] two weeks ago for this. I suspect you found this bug\n> independently, so I figured I should point it out. Otherwise, I would add a Reported-by (or some\n> other tag) with her name.\n> \n> Thanks,\n> Ben\n> \n> [1]: https://lore.kernel.org/linux-cxl/20260206181404.1025991-1-alison.schofield@intel.com/\n\nAh, yeah, missed this, and did find independently when testing unbinds.\n\nWasn't on cxl/next so I thought it hadn't been found yet.\n\nCool, thanks!\n~Gregory\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dave Jiang",
              "summary": "Yeah waiting on 7.0-rc1 for cxl/fixes. I also asked her to split the patches into 2 fixes. But if you don't mind go add your review tag that'd be great! :)",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "\n\nOn 2/23/26 10:45 AM, Gregory Price wrote:\n> On Mon, Feb 23, 2026 at 11:33:13AM -0600, Cheatham, Benjamin wrote:\n>> On 2/20/2026 8:18 PM, Gregory Price wrote:\n>>> CXL_DECODER_F_LOCK (BIT(4) = 16) and CXL_DECODER_F_NORMALIZED_ADDRESSING\n>>> (BIT(6) = 64) are bit masks, but three call sites pass them to test_bit()\n>>> which expects a bit number.\n>>>\n>>> Replace test_bit() with direct bitmask tests, consistent with every other\n>>> use of these flags.\n>>>\n>>> Fixes: 2230c4bdc412 (\"cxl: Add handling of locked CXL decoder\")\n>>> Signed-off-by: Gregory Price <gourry@gourry.net>\n>>\n>> Alison sent out a patch [1] two weeks ago for this. I suspect you found this bug\n>> independently, so I figured I should point it out. Otherwise, I would add a Reported-by (or some\n>> other tag) with her name.\n>>\n>> Thanks,\n>> Ben\n>>\n>> [1]: https://lore.kernel.org/linux-cxl/20260206181404.1025991-1-alison.schofield@intel.com/\n> \n> Ah, yeah, missed this, and did find independently when testing unbinds.\n> \n> Wasn't on cxl/next so I thought it hadn't been found yet.\n\nYeah waiting on 7.0-rc1 for cxl/fixes. I also asked her to split the patches into 2 fixes. But if you don't mind go add your review tag that'd be great! :) \n\n> \n> Cool, thanks!\n> ~Gregory\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3] cxl/memdev: fix deadlock in cxl_memdev_autoremove() on attach failure",
          "message_id": "20260211192228.2148713-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260211192228.2148713-1-gourry@gourry.net/",
          "date": "2026-02-11T19:22:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "cxl_memdev_autoremove() takes device_lock(&cxlmd->dev) via guard(device) and then calls cxl_memdev_unregister() when the attach callback was provided but cxl_mem_probe() failed to bind.\n\ncxl_memdev_unregister() calls cdev_device_del() device_del() bus_remove_device() device_release_driver()\n\nThis path is reached when a driver uses the @attach parameter to devm_cxl_add_memdev() and the CXL topology fails to enumerate (e.g. DVSEC range registers decode outside platform-defined CXL ranges, causing the endpoint port probe to fail).\n\nAdd cxl_memdev_attach_failed() to set the scope of the check correctly.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Davidlohr Bueso",
              "summary": "I preferred the \"if (!cxl_memdev_did_attach())\" but the below still reads nicely.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Wed, 11 Feb 2026, Gregory Price wrote:\n\n>cxl_memdev_autoremove() takes device_lock(&cxlmd->dev) via guard(device)\n>and then calls cxl_memdev_unregister() when the attach callback was\n>provided but cxl_mem_probe() failed to bind.\n>\n>cxl_memdev_unregister() calls\n>  cdev_device_del()\n>    device_del()\n>      bus_remove_device()\n>        device_release_driver()\n>\n>This path is reached when a driver uses the @attach parameter to\n>devm_cxl_add_memdev() and the CXL topology fails to enumerate (e.g.\n>DVSEC range registers decode outside platform-defined CXL ranges,\n>causing the endpoint port probe to fail).\n>\n>Add cxl_memdev_attach_failed() to set the scope of the check correctly.\n\nI preferred the \"if (!cxl_memdev_did_attach())\" but the below still\nreads nicely.\n\nReviewed-by: Davidlohr Bueso <dave@stgolabs.net>\n",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "dan.j.williams",
              "summary": "Looks good, This was a kreview find? Would it make sense to do something like \"Reported-by: kreview-<git sha>\"?",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Gregory Price wrote:\n> cxl_memdev_autoremove() takes device_lock(&cxlmd->dev) via guard(device)\n> and then calls cxl_memdev_unregister() when the attach callback was\n> provided but cxl_mem_probe() failed to bind.\n> \n> cxl_memdev_unregister() calls\n>   cdev_device_del()\n>     device_del()\n>       bus_remove_device()\n>         device_release_driver()\n> \n> This path is reached when a driver uses the @attach parameter to\n> devm_cxl_add_memdev() and the CXL topology fails to enumerate (e.g.\n> DVSEC range registers decode outside platform-defined CXL ranges,\n> causing the endpoint port probe to fail).\n> \n> Add cxl_memdev_attach_failed() to set the scope of the check correctly.\n> \n> Fixes: 29317f8dc6ed (\"cxl/mem: Introduce cxl_memdev_attach for CXL-dependent operation\")\n> Signed-off-by: Gregory Price <gourry@gourry.net>\n\nLooks good,\n\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\n\nThis was a kreview find? Would it make sense to do something like\n\"Reported-by: kreview-<git sha>\"?\n",
              "reply_to": "",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Technically it's a combination of Claude using kreview, but I think this is reasonable since Claude falls over on some of this without the prompts. If that's something we want:",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Thu, Feb 12, 2026 at 07:16:41PM -0800, dan.j.williams@intel.com wrote:\n> Gregory Price wrote:\n> > cxl_memdev_autoremove() takes device_lock(&cxlmd->dev) via guard(device)\n> > and then calls cxl_memdev_unregister() when the attach callback was\n> > provided but cxl_mem_probe() failed to bind.\n> > \n> > cxl_memdev_unregister() calls\n> >   cdev_device_del()\n> >     device_del()\n> >       bus_remove_device()\n> >         device_release_driver()\n> > \n> > This path is reached when a driver uses the @attach parameter to\n> > devm_cxl_add_memdev() and the CXL topology fails to enumerate (e.g.\n> > DVSEC range registers decode outside platform-defined CXL ranges,\n> > causing the endpoint port probe to fail).\n> > \n> > Add cxl_memdev_attach_failed() to set the scope of the check correctly.\n> > \n> > Fixes: 29317f8dc6ed (\"cxl/mem: Introduce cxl_memdev_attach for CXL-dependent operation\")\n> > Signed-off-by: Gregory Price <gourry@gourry.net>\n> \n> Looks good,\n> \n> Reviewed-by: Dan Williams <dan.j.williams@intel.com>\n> \n> This was a kreview find? Would it make sense to do something like\n> \"Reported-by: kreview-<git sha>\"?\n\nTechnically it's a combination of Claude using kreview, but I think this\nis reasonable since Claude falls over on some of this without the\nprompts.\n\nIf that's something we want:\n\nReported-by: kreview-c94b85d6d2\n",
              "reply_to": "",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dave Jiang",
              "summary": "Applied to cxl/fixes 318c58852e68",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "applied"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "\n\nOn 2/11/26 12:22 PM, Gregory Price wrote:\n> cxl_memdev_autoremove() takes device_lock(&cxlmd->dev) via guard(device)\n> and then calls cxl_memdev_unregister() when the attach callback was\n> provided but cxl_mem_probe() failed to bind.\n> \n> cxl_memdev_unregister() calls\n>   cdev_device_del()\n>     device_del()\n>       bus_remove_device()\n>         device_release_driver()\n> \n> This path is reached when a driver uses the @attach parameter to\n> devm_cxl_add_memdev() and the CXL topology fails to enumerate (e.g.\n> DVSEC range registers decode outside platform-defined CXL ranges,\n> causing the endpoint port probe to fail).\n> \n> Add cxl_memdev_attach_failed() to set the scope of the check correctly.\n> \n> Fixes: 29317f8dc6ed (\"cxl/mem: Introduce cxl_memdev_attach for CXL-dependent operation\")\n> Signed-off-by: Gregory Price <gourry@gourry.net>\n\nApplied to cxl/fixes\n318c58852e68\n\n> ---\n>  drivers/cxl/core/memdev.c | 13 +++++++++----\n>  1 file changed, 9 insertions(+), 4 deletions(-)\n> \n> diff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\n> index af3d0cc65138..25ca4443e4f7 100644\n> --- a/drivers/cxl/core/memdev.c\n> +++ b/drivers/cxl/core/memdev.c\n> @@ -1089,10 +1089,8 @@ static int cxlmd_add(struct cxl_memdev *cxlmd, struct cxl_dev_state *cxlds)\n>  DEFINE_FREE(put_cxlmd, struct cxl_memdev *,\n>  \t    if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> -static struct cxl_memdev *cxl_memdev_autoremove(struct cxl_memdev *cxlmd)\n> +static bool cxl_memdev_attach_failed(struct cxl_memdev *cxlmd)\n>  {\n> -\tint rc;\n> -\n>  \t/*\n>  \t * If @attach is provided fail if the driver is not attached upon\n>  \t * return. Note that failure here could be the result of a race to\n> @@ -1100,7 +1098,14 @@ static struct cxl_memdev *cxl_memdev_autoremove(struct cxl_memdev *cxlmd)\n>  \t * succeeded and then cxl_mem unbound before the lock is acquired.\n>  \t */\n>  \tguard(device)(&cxlmd->dev);\n> -\tif (cxlmd->attach && !cxlmd->dev.driver) {\n> +\treturn (cxlmd->attach && !cxlmd->dev.driver);\n> +}\n> +\n> +static struct cxl_memdev *cxl_memdev_autoremove(struct cxl_memdev *cxlmd)\n> +{\n> +\tint rc;\n> +\n> +\tif (cxl_memdev_attach_failed(cxlmd)) {\n>  \t\tcxl_memdev_unregister(cxlmd);\n>  \t\treturn ERR_PTR(-ENXIO);\n>  \t}\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v3 2/2] cxl: Fix race of nvdimm_bus object when creating nvdimm objects",
          "message_id": "aZzSMwc2evqS8uBc@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZzSMwc2evqS8uBc@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T22:18:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/2] cxl/region: fix region leak when attach_target fails in cxl_add_to_region",
          "message_id": "aZy1VGindEm-NbFn@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZy1VGindEm-NbFn@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T20:15:20Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Alison Schofield",
              "summary": "That's by design, and that'll eventually fail too. But - is see how your case is different. Thanks for the explanation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 03:15:16PM -0500, Gregory Price wrote:\n> On Mon, Feb 23, 2026 at 11:48:42AM -0800, Alison Schofield wrote:\n> > On Fri, Feb 20, 2026 at 11:30:12PM -0500, Gregory Price wrote:\n> > > cxl_add_to_region() ignores the return value of attach_target().  When\n> > > attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO),\n> > > the auto-discovered region remains registered with its HPA resource\n> > > consumed but never reaches COMMIT state.  Subsequent region creation\n> > > attempts fail with -ENOSPC because the HPA range is already reserved.\n> > > \n> > > Track whether this call to cxl_add_to_region() created the region, and\n> > > call drop_region() on attach_target() failure to unregister it and\n> > > release the HPA resource.  Pre-existing regions are left alone since\n> > > other endpoints may already be attached.\n> > \n> > I see you dropping this, perhaps just for the moment, because\n> > the drop_region() you wanted to use is not available yet.\n> > \n> \n> Yeah it's not a particularly useful cleanup in the current\n> infrastructure because nothing actually uses this pattern (yet).\n> \n> > This looks a lot like \n> > \thttps://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/\n> > \tcxl/region: Unregister auto-created region when assembly fails\n> > \tWhen auto-created region assembly fails the region remains registered\n> > \tbut disabled. The region continues to reserve its memory resource,\n> > \tpreventing DAX from registering the memory.\n> > \tUnregister the region on assembly failure to release the resource.\n> > \n> > And the review comments on that one, or at least on that thread in\n> > general, was to leave all the broken things in place.\n> > I didn't agree with that, and hope to see this version move ahead\n> > when you have the drop_region you need.\n> > \n> > \n> \n> The important note here is the difference between auto-regions and\n> manually created regions.  For auto-regions, you might have another\n> endpoint show up looking for the partially created region - and then\n> just go off and create it anyway because it thinks it was first.\n\nThat's by design, and that'll eventually fail too.\n\nBut - is see how your case is different. Thanks for the explanation.\n\n> \n> But in my driver, i'm explicitly converting these auto-regions into\n> other things, and if that fails it causes *all other* region creation to\n> fail - even if it wasn't actually dependent on that original region.\n> \n> This is only an issue if you have two devices unbind/bind cycling at\n> the same time - i.e.\n> \n>    echo 0000:d0:00.00 > cxl_pci/unbind\n>    echo 0000:e0:00.00 > cxl_pci/unbind\n>    echo 0000:d0:00.00 > mydriver/bind\n>    echo 0000:e0:00.00 > mydriver/bind\n> \n> If the platform has pre-programmed and locked the decoders, and one of\n> the two devices fails to probe and leaves a hanging partially\n> created region, the other device will fail too.\n> \n> It's a pretty narrow failure scenario.\n> \n> ~Gregory\n> \n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] cxl/core: fix test_bit misuse with CXL_DECODER_F_ bitmask flags",
          "message_id": "aZySU-tcjVvYcb23@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZySU-tcjVvYcb23@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T17:45:58Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Dave Jiang",
              "summary": "Yeah waiting on 7.0-rc1 for cxl/fixes. I also asked her to split the patches into 2 fixes. But if you don't mind go add your review tag that'd be great! :)",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "\n\nOn 2/23/26 10:45 AM, Gregory Price wrote:\n> On Mon, Feb 23, 2026 at 11:33:13AM -0600, Cheatham, Benjamin wrote:\n>> On 2/20/2026 8:18 PM, Gregory Price wrote:\n>>> CXL_DECODER_F_LOCK (BIT(4) = 16) and CXL_DECODER_F_NORMALIZED_ADDRESSING\n>>> (BIT(6) = 64) are bit masks, but three call sites pass them to test_bit()\n>>> which expects a bit number.\n>>>\n>>> Replace test_bit() with direct bitmask tests, consistent with every other\n>>> use of these flags.\n>>>\n>>> Fixes: 2230c4bdc412 (\"cxl: Add handling of locked CXL decoder\")\n>>> Signed-off-by: Gregory Price <gourry@gourry.net>\n>>\n>> Alison sent out a patch [1] two weeks ago for this. I suspect you found this bug\n>> independently, so I figured I should point it out. Otherwise, I would add a Reported-by (or some\n>> other tag) with her name.\n>>\n>> Thanks,\n>> Ben\n>>\n>> [1]: https://lore.kernel.org/linux-cxl/20260206181404.1025991-1-alison.schofield@intel.com/\n> \n> Ah, yeah, missed this, and did find independently when testing unbinds.\n> \n> Wasn't on cxl/next so I thought it hadn't been found yet.\n\nYeah waiting on 7.0-rc1 for cxl/fixes. I also asked her to split the patches into 2 fixes. But if you don't mind go add your review tag that'd be great! :) \n\n> \n> Cool, thanks!\n> ~Gregory\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC][RFC PATCH v4 00/27] Private Memory Nodes (w/ Compressed RAM)",
          "message_id": "aZx7hsVNU0XOCCiG@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZx7hsVNU0XOCCiG@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T16:08:42Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH v5 00/10] mm: Hot page tracking and promotion infrastructure",
          "message_id": "aZxsBifRchLn2m42@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZxsBifRchLn2m42@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T15:02:36Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Bharata Rao",
              "summary": "For pghot-default, with target_nid alternating between the available toptier nodes 0 and 1, the numbers catch up with pghot-precise and base NUMAB2 case as seen below: ================================ Time in seconds         4337.98 Mop/s total             90217.86 pgpromote_success       42170085 pgpromote_candidate     0 pgpromote_candidate_nrl 42171963 pgdemote_kswapd         0 numa_pte_updates        42338538 numa_hint_faults        42185662 ================================",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 23-Feb-26 8:32 PM, Gregory Price wrote:\n> On Mon, Feb 23, 2026 at 07:57:39PM +0530, Bharata B Rao wrote:\n>>\n>> Time in seconds - Lower is better\n>> Mop/s total - Higher is better\n>> =====================================================================================\n>>                         Base            Base            pghot-default\n>> pghot-precise\n>>                         NUMAB0          NUMAB2          NUMAB2          NUMAB2\n>> =====================================================================================\n>> Time in seconds         7349.86         4422.50         6219.71         4113.56\n>> Mop/s total             53247.66        88493.630       62923.030       95139.810\n>>\n>> pgpromote_success       0               42181834        248503390       41955718\n>> pgpromote_candidate     0               0               577086192       0\n>> pgpromote_candidate_nrl 0               42181834        29410329        41956171\n>> pgdemote_kswapd         0               0               216489010       0\n>> numa_pte_updates        0               42252749        607470975       42037882\n>> numa_hint_faults        0               42183772        606540729       41968150\n>> =====================================================================================\n>>\n>> - In the base case, the benchmark numbers improve significantly due to hot page\n>>   promotion.\n>> - Though the benchmark runs for hundreds of minutes, the pages get promoted\n>>   within the first few mins.\n>> - pghot-precise is able to match the base case numbers.\n>> - The benchmark suffers in pghot-default case due to promotion being limited\n>>   to the default NID (0) only. This leads to excessive PTE updates, hint faults,\n>>   demotion and promotion churn.\n> \n> Wow, this really seems to justify the extra memory usage.\n> \n> Is it possible for you to change pghot-default to move the page to a\n> random (or round-robin) node on the top tier instead of NID(0) by default?\n> \n> At least then pghot-default would be correct 1/N % of the time (in theory).\n> I'd be curious to see how close it gets to NUMAB2 with that.\n\nFor pghot-default, with target_nid alternating between the available\ntoptier nodes 0 and 1, the numbers catch up with pghot-precise and base\nNUMAB2 case as seen below:\n================================\nTime in seconds         4337.98\nMop/s total             90217.86\n\npgpromote_success       42170085\npgpromote_candidate     0\npgpromote_candidate_nrl 42171963\npgdemote_kswapd         0\nnuma_pte_updates        42338538\nnuma_hint_faults        42185662\n================================\n\nRegards,\nBharata.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Fascinating! Thank you for the quick follow up. I wonder if this was a lucky run, it almost seems *too* perfect.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 05:25:13PM +0530, Bharata B Rao wrote:\n> >> Time in seconds - Lower is better\n> >> Mop/s total - Higher is better\n> >> =====================================================================================\n> >>                         Base            Base            pghot-default\n> >> pghot-precise\n> >>                         NUMAB0          NUMAB2          NUMAB2          NUMAB2\n> >> =====================================================================================\n> >> Time in seconds         7349.86         4422.50         6219.71         4113.56\n> >> Mop/s total             53247.66        88493.630       62923.030       95139.810\n> >>\n> >> pgpromote_success       0               42181834        248503390       41955718\n> >> pgpromote_candidate     0               0               577086192       0\n> >> pgpromote_candidate_nrl 0               42181834        29410329        41956171\n> >> pgdemote_kswapd         0               0               216489010       0\n> >> numa_pte_updates        0               42252749        607470975       42037882\n> >> numa_hint_faults        0               42183772        606540729       41968150\n> >> =====================================================================================\n> \n> For pghot-default, with target_nid alternating between the available\n> toptier nodes 0 and 1, the numbers catch up with pghot-precise and base\n> NUMAB2 case as seen below:\n> ================================\n> Time in seconds         4337.98\n> Mop/s total             90217.86\n> \n> pgpromote_success       42170085\n> pgpromote_candidate     0\n> pgpromote_candidate_nrl 42171963\n> pgdemote_kswapd         0\n> numa_pte_updates        42338538\n> numa_hint_faults        42185662\n> ================================\n> \n\nFascinating! Thank you for the quick follow up.\n\nI wonder if this was a lucky run, it almost seems *too* perfect.\n\n~Gregory\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Bharata Rao",
              "summary": "It consistently performs that way. Here are the numbers from another run: ================================ Time in seconds         4329.22 Mop/s total             90400.27 pgpromote_success       41967282 pgpromote_candidate     0 pgpromote_candidate_nrl 41968339 pgdemote_kswapd         0 numa_pte_updates        42253854 numa_hint_faults        42019449 ================================ grep -E \"pgpromote|pgdemote\" /sys/devices/system/node/node0/vmstat pgpromote_success 20996597 pgpromote_candidate 0 pgpromote_candidate_nrl 41968339 (*) pgdemote_kswapd 0...",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 24-Feb-26 9:00 PM, Gregory Price wrote:\n>>\n>> For pghot-default, with target_nid alternating between the available\n>> toptier nodes 0 and 1, the numbers catch up with pghot-precise and base\n>> NUMAB2 case as seen below:\n>> ================================\n>> Time in seconds         4337.98\n>> Mop/s total             90217.86\n>>\n>> pgpromote_success       42170085\n>> pgpromote_candidate     0\n>> pgpromote_candidate_nrl 42171963\n>> pgdemote_kswapd         0\n>> numa_pte_updates        42338538\n>> numa_hint_faults        42185662\n>> ================================\n>>\n> \n> Fascinating! Thank you for the quick follow up.\n> \n> I wonder if this was a lucky run, it almost seems *too* perfect.\n\nIt consistently performs that way. Here are the numbers from another\nrun:\n\n================================\nTime in seconds         4329.22\nMop/s total             90400.27\n\npgpromote_success       41967282\npgpromote_candidate     0\npgpromote_candidate_nrl 41968339\npgdemote_kswapd         0\nnuma_pte_updates        42253854\nnuma_hint_faults        42019449\n================================\n\ngrep -E \"pgpromote|pgdemote\" /sys/devices/system/node/node0/vmstat\npgpromote_success 20996597\npgpromote_candidate 0\npgpromote_candidate_nrl 41968339 (*)\npgdemote_kswapd 0\npgdemote_direct 0\npgdemote_khugepaged 0\npgdemote_proactive 0\n\ngrep -E \"pgpromote|pgdemote\" /sys/devices/system/node/node1/vmstat\npgpromote_success 20970685\npgpromote_candidate 0\npgpromote_candidate_nrl 0\npgdemote_kswapd 0\npgdemote_direct 0\npgdemote_khugepaged 0\npgdemote_proactive 0\n\n\n(*) The round-robin b/n nodes 0 and 1 happens after this metric is\nattributed to the original default target_nid. Hence nrl metric\ngets populated for node 0 only.\n\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC][RFC PATCH v4 00/27] Private Memory Nodes (w/ Compressed RAM)",
          "message_id": "aZxqP7J1kOClQUPQ@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZxqP7J1kOClQUPQ@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T14:55:07Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "scratch all this - existing hooks exist for exactly this purpose: can_change_[pte|pmd]_writable() Surprised I missed this. I can clean this up to remove it from the page table walks.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 09:54:55AM -0500, Gregory Price wrote:\n> On Mon, Feb 23, 2026 at 02:07:15PM +0100, David Hildenbrand (Arm) wrote:\n> > \n> > I'm concerned about adding more special-casing (similar to what we already\n> > added for ZONE_DEVICE) all over the place.\n> > \n> > Like the whole folio_managed_() stuff in mprotect.c\n> > \n> > Having that said, sounds like a reasonable topic to discuss.\n> > \n> \n> Another option would be to add the hook to vma_wants_writenotify()\n> instead of the page table code - and mask MM_CP_TRY_CHANGE_WRITABLE.\n> \n\nscratch all this - existing hooks exist for exactly this purpose:\n\n\tcan_change_[pte|pmd]_writable()\n\nSurprised I missed this.\n\nI can clean this up to remove it from the page table walks.\n\nStill valid to question whether we want this, but at least the hook\nlives with other write-protect hooks now.\n\n~Gregory\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v2 2/2] cxl/region: Test CXL_DECODER_F_NORMALIZED_ADDRESSING as a bitmask",
          "message_id": "aZzGURaa8aHKqreA@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZzGURaa8aHKqreA@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T21:27:48Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v2 1/2] cxl: Test CXL_DECODER_F_LOCK as a bitmask",
          "message_id": "aZzGNiMPuU-Jphou@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZzGNiMPuU-Jphou@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T21:27:21Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH] cxl: Test decoder flags as bitmasks",
          "message_id": "aZy0EtERdCpGn4gF@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZy0EtERdCpGn4gF@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T20:09:57Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Jeff Layton",
      "primary_email": "jlayton@kernel.org",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 0/4] sunrpc: cache infrastructure scalability improvements",
          "message_id": "20260223-sunrpc-cache-v2-0-91fc827c4d33@kernel.org",
          "url": "https://lore.kernel.org/all/20260223-sunrpc-cache-v2-0-91fc827c4d33@kernel.org/",
          "date": "2026-02-23T17:10:17Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The first patch fixes a pre-existing bug that Neil spotted during the review of v1. The next two patches convert the global spinlock and waitqueue to be per-cache_detail instead.\n\nThe last patch splits up the cache_detail->queue into two lists: one to hold cache_readers and one for cache_requests. This simplifies the code, and the new sequence number that helps the readers track position may help with implementing netlink upcalls.\n\nPlease consider these for v7.1.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Chuck Lever",
              "summary": "From: Chuck Lever <chuck.lever@oracle.com> Applied to nfsd-testing, replacing v1. Thanks! [1/4] sunrpc: fix cache_request leak in cache_release commit: dad5f78046759eb5c95970198eb9865550eb6227 [2/4] sunrpc: convert queue_lock from global spinlock to per-cache-detail lock commit: c94ad34b7ecd5928cf3fdb6ea4fcf6ef55765e97 [3/4] sunrpc: convert queue_wait from global to...",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "applied"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nOn Mon, 23 Feb 2026 12:09:57 -0500, Jeff Layton wrote:\n> The first patch fixes a pre-existing bug that Neil spotted during the\n> review of v1. The next two patches convert the global spinlock and\n> waitqueue to be per-cache_detail instead.\n> \n> The last patch splits up the cache_detail->queue into two lists: one to\n> hold cache_readers and one for cache_requests. This simplifies the code,\n> and the new sequence number that helps the readers track position may\n> help with implementing netlink upcalls.\n> \n> [...]\n\nApplied to nfsd-testing, replacing v1. Thanks!\n\n[1/4] sunrpc: fix cache_request leak in cache_release\n      commit: dad5f78046759eb5c95970198eb9865550eb6227\n[2/4] sunrpc: convert queue_lock from global spinlock to per-cache-detail lock\n      commit: c94ad34b7ecd5928cf3fdb6ea4fcf6ef55765e97\n[3/4] sunrpc: convert queue_wait from global to per-cache-detail waitqueue\n      commit: 951696964e9c370a5f91d5e3e136d39aa08d912c\n[4/4] sunrpc: split cache_detail queue into request and reader lists\n      commit: 3557b9c71039b2435b383fc57283a0b847b40144\n\n--\nChuck Lever\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/3] sunrpc: cache infrastructure scalability improvements",
          "message_id": "20260220-sunrpc-cache-v1-0-47d04014c245@kernel.org",
          "url": "https://lore.kernel.org/all/20260220-sunrpc-cache-v1-0-47d04014c245@kernel.org/",
          "date": "2026-02-20T12:26:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-20",
          "patch_summary": "I've been working on trying to retrofit a netlink upcall into the sunrpc cache infrastructure. While crawling over that code, I noticed that it relies on both a global spinlock and waitqueue. The first two patches convert those to be per-cache_detail instead.\n\nThe last patch splits up the cache_detail->queue into two lists: one to hold cache_readers and one for cache_requests. This simplifies the code, and the new sequence number that helps the readers track position may help with implementing netlink upcalls.\n\nPlease consider these for v7.1.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Chuck Lever",
              "summary": "From: Chuck Lever <chuck.lever@oracle.com> Applied to nfsd-testing, thanks! [1/3] sunrpc: convert queue_lock from global spinlock to per-cache_detail lock commit: 8da8f32e9a2702259cdf97e2f8f492ef9c79db65 [2/3] sunrpc: convert queue_wait from global to per-cache_detail waitqueue commit: 802261d8b58dd2f41a52a0c92776e0fb45619efe [3/3] sunrpc: split cache_detail...",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "applied"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nOn Fri, 20 Feb 2026 07:26:02 -0500, Jeff Layton wrote:\n> I've been working on trying to retrofit a netlink upcall into the sunrpc\n> cache infrastructure. While crawling over that code, I noticed that it\n> relies on both a global spinlock and waitqueue. The first two patches\n> convert those to be per-cache_detail instead.\n> \n> The last patch splits up the cache_detail->queue into two lists: one to\n> hold cache_readers and one for cache_requests. This simplifies the code,\n> and the new sequence number that helps the readers track position may\n> help with implementing netlink upcalls.\n> \n> [...]\n\nApplied to nfsd-testing, thanks!\n\n[1/3] sunrpc: convert queue_lock from global spinlock to per-cache_detail lock\n      commit: 8da8f32e9a2702259cdf97e2f8f492ef9c79db65\n[2/3] sunrpc: convert queue_wait from global to per-cache_detail waitqueue\n      commit: 802261d8b58dd2f41a52a0c92776e0fb45619efe\n[3/3] sunrpc: split cache_detail queue into request and reader lists\n      commit: 0eb3d9dc71ada02909e4dfe9cb54e703ec717ed4\n\n--\nChuck Lever\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] Add support for empty path in openat and openat2 syscalls",
          "message_id": "44a2111e33631d78aded73e4b79908db6237227f.camel@kernel.org",
          "url": "https://lore.kernel.org/all/44a2111e33631d78aded73e4b79908db6237227f.camel@kernel.org/",
          "date": "2026-02-23T15:28:27Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Christian Brauner",
              "summary": "I do like restricting it to openat2() as well.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 10:28:24AM -0500, Jeff Layton wrote:\n> On Mon, 2026-02-23 at 16:16 +0100, Jori Koolstra wrote:\n> > To get an operable version of an O_PATH file descriptors, it is possible\n> > to use openat(fd, \".\", O_DIRECTORY) for directories, but other files\n> > currently require going through open(\"/proc/<pid>/fd/<nr>\") which\n> > depends on a functioning procfs.\n> > \n> > This patch adds the O_EMPTY_PATH flag to openat and openat2. If passed\n> > LOOKUP_EMPTY is set at path resolve time.\n> > \n> \n> This sounds valuable, but there was recent discussion around the\n> O_REGULAR flag that said that we shouldn't be adding new flags to older\n> syscalls [1]. Should this only be an OPENAT2_* flag instead?\n> \n> [1]: https://lore.kernel.org/linux-fsdevel/20260129-siebzehn-adler-efe74ff8f1a9@brauner/\n\nI do like restricting it to openat2() as well.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Jori Koolstra",
              "summary": "So would you want to filter the O_EMPTY_PATH flag from openat(), or maybe add a RESOLVE_EMPTY flag to the resolve options?",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "\n> Op 24-02-2026 11:10 CET schreef Christian Brauner <brauner@kernel.org>:\n> \n>  \n> On Mon, Feb 23, 2026 at 10:28:24AM -0500, Jeff Layton wrote:\n> > On Mon, 2026-02-23 at 16:16 +0100, Jori Koolstra wrote:\n> > > To get an operable version of an O_PATH file descriptors, it is possible\n> > > to use openat(fd, \".\", O_DIRECTORY) for directories, but other files\n> > > currently require going through open(\"/proc/<pid>/fd/<nr>\") which\n> > > depends on a functioning procfs.\n> > > \n> > > This patch adds the O_EMPTY_PATH flag to openat and openat2. If passed\n> > > LOOKUP_EMPTY is set at path resolve time.\n> > > \n> > \n> > This sounds valuable, but there was recent discussion around the\n> > O_REGULAR flag that said that we shouldn't be adding new flags to older\n> > syscalls [1]. Should this only be an OPENAT2_* flag instead?\n> > \n> > [1]: https://lore.kernel.org/linux-fsdevel/20260129-siebzehn-adler-efe74ff8f1a9@brauner/\n> \n> I do like restricting it to openat2() as well.\n\nSo would you want to filter the O_EMPTY_PATH flag from openat(), or maybe add\na RESOLVE_EMPTY flag to the resolve options?\n\nThanks,\nJori.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Christian Brauner",
              "summary": "No, add a OPENAT2_EMPTY_PATH in the upper 32 bit of the 64-bit flag argument for struct open_how. Then it cannot be used in openat(). But let's wait a day or so to see whether we have someone that really wants to extend this to openat() as well...",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 02:35:00PM +0100, Jori Koolstra wrote:\n> \n> > Op 24-02-2026 11:10 CET schreef Christian Brauner <brauner@kernel.org>:\n> > \n> >  \n> > On Mon, Feb 23, 2026 at 10:28:24AM -0500, Jeff Layton wrote:\n> > > On Mon, 2026-02-23 at 16:16 +0100, Jori Koolstra wrote:\n> > > > To get an operable version of an O_PATH file descriptors, it is possible\n> > > > to use openat(fd, \".\", O_DIRECTORY) for directories, but other files\n> > > > currently require going through open(\"/proc/<pid>/fd/<nr>\") which\n> > > > depends on a functioning procfs.\n> > > > \n> > > > This patch adds the O_EMPTY_PATH flag to openat and openat2. If passed\n> > > > LOOKUP_EMPTY is set at path resolve time.\n> > > > \n> > > \n> > > This sounds valuable, but there was recent discussion around the\n> > > O_REGULAR flag that said that we shouldn't be adding new flags to older\n> > > syscalls [1]. Should this only be an OPENAT2_* flag instead?\n> > > \n> > > [1]: https://lore.kernel.org/linux-fsdevel/20260129-siebzehn-adler-efe74ff8f1a9@brauner/\n> > \n> > I do like restricting it to openat2() as well.\n> \n> So would you want to filter the O_EMPTY_PATH flag from openat(), or maybe add\n> a RESOLVE_EMPTY flag to the resolve options?\n\nNo, add a OPENAT2_EMPTY_PATH in the upper 32 bit of the 64-bit flag\nargument for struct open_how. Then it cannot be used in openat(). But\nlet's wait a day or so to see whether we have someone that really wants\nto extend this to openat() as well...\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/1] NFSD: Expose callback statistics in /proc/net/rpc/nfsd",
          "message_id": "84bbbe173485c6cbd0af9169e55717be0aa0e367.camel@kernel.org",
          "url": "https://lore.kernel.org/all/84bbbe173485c6cbd0af9169e55717be0aa0e367.camel@kernel.org/",
          "date": "2026-02-23T14:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 3/3] sunrpc: split cache_detail queue into request and reader lists",
          "message_id": "7c0e019cbf7371bdf47bd7a7c48df132fc5b87fd.camel@kernel.org",
          "url": "https://lore.kernel.org/all/7c0e019cbf7371bdf47bd7a7c48df132fc5b87fd.camel@kernel.org/",
          "date": "2026-02-23T14:08:00Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joanne Koong",
      "primary_email": "joannelkoong@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v1 0/1] iomap: don't mark folio uptodate if read IO has bytes pending",
          "message_id": "20260219003911.344478-1-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260219003911.344478-1-joannelkoong@gmail.com/",
          "date": "2026-02-19T00:41:04Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-19",
          "patch_summary": "This is a fix for this scenario:",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v1 11/11] io_uring/cmd: set selected buffer index in __io_uring_cmd_done()",
          "message_id": "20260210002852.1394504-12-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260210002852.1394504-12-joannelkoong@gmail.com/",
          "date": "2026-02-10T00:31:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-10",
          "patch_summary": "When uring_cmd operations select a buffer, the completion queue entry should indicate which buffer was selected.\n\nSet IORING_CQE_F_BUFFER on the completed entry and encode the buffer index if a buffer was selected.\n\nThis will be needed for fuse, which needs to relay to userspace which selected buffer contains the data.",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v1 1/1] iomap: don't mark folio uptodate if read IO has bytes pending",
          "message_id": "CAJnrk1Zk1hHCoC4xaY_KT0m_04CQ=pO6j3e1tGrdj7LTf5BHsA@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1Zk1hHCoC4xaY_KT0m_04CQ=pO6j3e1tGrdj7LTf5BHsA@mail.gmail.com/",
          "date": "2026-02-23T23:53:28Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Christoph Hellwig",
              "summary": "Yes.  I've been thinking about that on and off, but unfortunately so far I've not come up with a good idea how to merge the code.  Doing so would be very useful for many reasons. The problem with that isn't really async vs sync; ->read_folio clearly shows you you turn underlying asynchronous logic into a synchronous call. It's really about the range logic, where the writer preparation might want to only read the head and the tail segments of a folio.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 03:53:15PM -0800, Joanne Koong wrote:\n> > There are three ways that iomap can be reading into the pagecache:\n> > a) async ->readahead,\n> > b) synchronous ->read_folio (page faults), and\n> \n> b) is async as well. The code for b) and a) are exactly the same (the\n> logic in iomap_read_folio_iter())\n\nYes.\n\n> > This is confusing to me.  It would be more straightforward (I think) if\n> > we just did it for all cases instead of adding more conditionals.  IOWs,\n> > how hard would it be to consolidate the read code so that there's one\n> > function that iomap calls when it has filled out part of a folio.  Is\n> > that possible, even though we shouldn't be calling folio_end_read during\n> > a pagecache write?\n> \n> imo, I don't think the synchronous ->read_folio_range() for buffered\n> writes should be consolidated with the async read logic.\n\nYes.  I've been thinking about that on and off, but unfortunately so far\nI've not come up with a good idea how to merge the code.  Doing so would\nbe very useful for many reasons.\n\nThe problem with that isn't really async vs sync; ->read_folio clearly\nshows you you turn underlying asynchronous logic into a synchronous call.\nIt's really about the range logic, where the writer preparation might\nwant to only read the head and the tail segments of a folio.\n\nBut if we can merge that into the main implementation and have a single\ncore implementation we'd be much better off.\n\nAnyone looking for a \"little\" project? :)\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Johannes Weiner",
      "primary_email": "hannes@cmpxchg.org",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 1/2] mm: vmalloc: streamline vmalloc memory accounting",
          "message_id": "20260223160147.3792777-1-hannes@cmpxchg.org",
          "url": "https://lore.kernel.org/all/20260223160147.3792777-1-hannes@cmpxchg.org/",
          "date": "2026-02-23T16:01:51Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Use a vmstat counter instead of a custom, open-coded atomic. This has the added benefit of making the data available per-node, and prepares for cleaning up the memcg accounting as well.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Vishal (Oracle)",
              "summary": "Gave Reviewed-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Mon, Feb 23, 2026 at 11:01:06AM -0500, Johannes Weiner wrote:\n> Use a vmstat counter instead of a custom, open-coded atomic. This has\n> the added benefit of making the data available per-node, and prepares\n> for cleaning up the memcg accounting as well.\n> \n> Acked-by: Shakeel Butt <shakeel.butt@linux.dev>\n> Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>\n\nReviewed-by: Vishal Moola (Oracle) <vishal.moola@gmail.com>\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/2] mm: vmalloc: streamline vmalloc memory accounting",
          "message_id": "20260220191035.3703800-1-hannes@cmpxchg.org",
          "url": "https://lore.kernel.org/all/20260220191035.3703800-1-hannes@cmpxchg.org/",
          "date": "2026-02-20T19:10:37Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-20",
          "patch_summary": "Use a vmstat counter instead of a custom, open-coded atomic. This has the added benefit of making the data available per-node, and prepares for cleaning up the memcg accounting as well.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "mod_node_page_state() takes 'struct pglist_data *pgdat', you need to use page_pgdat(page) as first param. Same here. With above fixes, you can add:",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n[...]\n>  static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)\n>  {\n>  \tstruct rb_node *n = root->rb_node;\n> @@ -3463,11 +3457,11 @@ void vfree(const void *addr)\n>  \t\t * High-order allocs for huge vmallocs are split, so\n>  \t\t * can be freed as an array of order-0 allocations\n>  \t\t */\n> +\t\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> +\t\t\tdec_node_page_state(page, NR_VMALLOC);\n>  \t\t__free_page(page);\n>  \t\tcond_resched();\n>  \t}\n> -\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> -\t\tatomic_long_sub(vm->nr_pages, &nr_vmalloc_pages);\n>  \tkvfree(vm->pages);\n>  \tkfree(vm);\n>  }\n> @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\t\tcontinue;\n>  \t\t}\n>  \n> +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n\nmod_node_page_state() takes 'struct pglist_data *pgdat', you need to use\npage_pgdat(page) as first param.\n\n> +\n>  \t\tsplit_page(page, large_order);\n>  \t\tfor (i = 0; i < (1U << large_order); i++)\n>  \t\t\tpages[nr_allocated + i] = page + i;\n> @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \tif (!order) {\n>  \t\twhile (nr_allocated < nr_pages) {\n>  \t\t\tunsigned int nr, nr_pages_request;\n> +\t\t\tint i;\n>  \n>  \t\t\t/*\n>  \t\t\t * A maximum allowed request is hard-coded and is 100\n> @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\t\t\t\t\t\tnr_pages_request,\n>  \t\t\t\t\t\t\tpages + nr_allocated);\n>  \n> +\t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n> +\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n> +\n>  \t\t\tnr_allocated += nr;\n>  \n>  \t\t\t/*\n> @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\tif (unlikely(!page))\n>  \t\t\tbreak;\n>  \n> +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n\nSame here.\n\nWith above fixes, you can add:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Uladzislau Rezki",
              "summary": "Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()? Or mod_node_page_state in first place should be invoked on high-order page before split(to avoid of looping over small pages afterword)? I mean it would be good to place to the one solid place. If it is possible of course.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> Use a vmstat counter instead of a custom, open-coded atomic. This has\n> the added benefit of making the data available per-node, and prepares\n> for cleaning up the memcg accounting as well.\n> \n> Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>\n> ---\n>  fs/proc/meminfo.c       |  3 ++-\n>  include/linux/mmzone.h  |  1 +\n>  include/linux/vmalloc.h |  3 ---\n>  mm/vmalloc.c            | 19 ++++++++++---------\n>  mm/vmstat.c             |  1 +\n>  5 files changed, 14 insertions(+), 13 deletions(-)\n> \n> diff --git a/fs/proc/meminfo.c b/fs/proc/meminfo.c\n> index a458f1e112fd..549793f44726 100644\n> --- a/fs/proc/meminfo.c\n> +++ b/fs/proc/meminfo.c\n> @@ -126,7 +126,8 @@ static int meminfo_proc_show(struct seq_file *m, void *v)\n>  \tshow_val_kb(m, \"Committed_AS:   \", committed);\n>  \tseq_printf(m, \"VmallocTotal:   %8lu kB\\n\",\n>  \t\t   (unsigned long)VMALLOC_TOTAL >> 10);\n> -\tshow_val_kb(m, \"VmallocUsed:    \", vmalloc_nr_pages());\n> +\tshow_val_kb(m, \"VmallocUsed:    \",\n> +\t\t    global_node_page_state(NR_VMALLOC));\n>  \tshow_val_kb(m, \"VmallocChunk:   \", 0ul);\n>  \tshow_val_kb(m, \"Percpu:         \", pcpu_nr_pages());\n>  \n> diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> index fc5d6c88d2f0..64df797d45c6 100644\n> --- a/include/linux/mmzone.h\n> +++ b/include/linux/mmzone.h\n> @@ -220,6 +220,7 @@ enum node_stat_item {\n>  \tNR_KERNEL_MISC_RECLAIMABLE,\t/* reclaimable non-slab kernel pages */\n>  \tNR_FOLL_PIN_ACQUIRED,\t/* via: pin_user_page(), gup flag: FOLL_PIN */\n>  \tNR_FOLL_PIN_RELEASED,\t/* pages returned via unpin_user_page() */\n> +\tNR_VMALLOC,\n>  \tNR_KERNEL_STACK_KB,\t/* measured in KiB */\n>  #if IS_ENABLED(CONFIG_SHADOW_CALL_STACK)\n>  \tNR_KERNEL_SCS_KB,\t/* measured in KiB */\n> diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h\n> index e8e94f90d686..3b02c0c6b371 100644\n> --- a/include/linux/vmalloc.h\n> +++ b/include/linux/vmalloc.h\n> @@ -286,8 +286,6 @@ int unregister_vmap_purge_notifier(struct notifier_block *nb);\n>  #ifdef CONFIG_MMU\n>  #define VMALLOC_TOTAL (VMALLOC_END - VMALLOC_START)\n>  \n> -unsigned long vmalloc_nr_pages(void);\n> -\n>  int vm_area_map_pages(struct vm_struct *area, unsigned long start,\n>  \t\t      unsigned long end, struct page **pages);\n>  void vm_area_unmap_pages(struct vm_struct *area, unsigned long start,\n> @@ -304,7 +302,6 @@ static inline void set_vm_flush_reset_perms(void *addr)\n>  #else  /* !CONFIG_MMU */\n>  #define VMALLOC_TOTAL 0UL\n>  \n> -static inline unsigned long vmalloc_nr_pages(void) { return 0; }\n>  static inline void set_vm_flush_reset_perms(void *addr) {}\n>  #endif /* CONFIG_MMU */\n>  \n> diff --git a/mm/vmalloc.c b/mm/vmalloc.c\n> index e286c2d2068c..a49a46de9c4f 100644\n> --- a/mm/vmalloc.c\n> +++ b/mm/vmalloc.c\n> @@ -1063,14 +1063,8 @@ static BLOCKING_NOTIFIER_HEAD(vmap_notify_list);\n>  static void drain_vmap_area_work(struct work_struct *work);\n>  static DECLARE_WORK(drain_vmap_work, drain_vmap_area_work);\n>  \n> -static __cacheline_aligned_in_smp atomic_long_t nr_vmalloc_pages;\n>  static __cacheline_aligned_in_smp atomic_long_t vmap_lazy_nr;\n>  \n> -unsigned long vmalloc_nr_pages(void)\n> -{\n> -\treturn atomic_long_read(&nr_vmalloc_pages);\n> -}\n> -\n>  static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)\n>  {\n>  \tstruct rb_node *n = root->rb_node;\n> @@ -3463,11 +3457,11 @@ void vfree(const void *addr)\n>  \t\t * High-order allocs for huge vmallocs are split, so\n>  \t\t * can be freed as an array of order-0 allocations\n>  \t\t */\n> +\t\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> +\t\t\tdec_node_page_state(page, NR_VMALLOC);\n>  \t\t__free_page(page);\n>  \t\tcond_resched();\n>  \t}\n> -\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> -\t\tatomic_long_sub(vm->nr_pages, &nr_vmalloc_pages);\n>  \tkvfree(vm->pages);\n>  \tkfree(vm);\n>  }\n> @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\t\tcontinue;\n>  \t\t}\n>  \n> +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> +\n>  \t\tsplit_page(page, large_order);\n>  \t\tfor (i = 0; i < (1U << large_order); i++)\n>  \t\t\tpages[nr_allocated + i] = page + i;\n> @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \tif (!order) {\n>  \t\twhile (nr_allocated < nr_pages) {\n>  \t\t\tunsigned int nr, nr_pages_request;\n> +\t\t\tint i;\n>  \n>  \t\t\t/*\n>  \t\t\t * A maximum allowed request is hard-coded and is 100\n> @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\t\t\t\t\t\tnr_pages_request,\n>  \t\t\t\t\t\t\tpages + nr_allocated);\n>  \n> +\t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n> +\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n> +\n>  \t\t\tnr_allocated += nr;\n>  \n>  \t\t\t/*\n> @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\tif (unlikely(!page))\n>  \t\t\tbreak;\n>  \n> +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n> +\n>  \t\t/*\nCan we move *_node_page_stat() to the end of the vm_area_alloc_pages()?\n\nOr mod_node_page_state in first place should be invoked on high-order\npage before split(to avoid of looping over small pages afterword)?\n\nI mean it would be good to place to the one solid place. If it is possible\nof course.\n\n--\nUladzislau Rezk\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Johannes Weiner (author)",
              "summary": "Good catch, my apologies. Serves me right for not compiling incrementally.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 02:09:28PM -0800, Shakeel Butt wrote:\n> On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> [...]\n> >  static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)\n> >  {\n> >  \tstruct rb_node *n = root->rb_node;\n> > @@ -3463,11 +3457,11 @@ void vfree(const void *addr)\n> >  \t\t * High-order allocs for huge vmallocs are split, so\n> >  \t\t * can be freed as an array of order-0 allocations\n> >  \t\t */\n> > +\t\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> > +\t\t\tdec_node_page_state(page, NR_VMALLOC);\n> >  \t\t__free_page(page);\n> >  \t\tcond_resched();\n> >  \t}\n> > -\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> > -\t\tatomic_long_sub(vm->nr_pages, &nr_vmalloc_pages);\n> >  \tkvfree(vm->pages);\n> >  \tkfree(vm);\n> >  }\n> > @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \t\t\tcontinue;\n> >  \t\t}\n> >  \n> > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> \n> mod_node_page_state() takes 'struct pglist_data *pgdat', you need to use\n> page_pgdat(page) as first param.\n\nGood catch, my apologies. Serves me right for not compiling\nincrementally.\n\n> With above fixes, you can add:\n> \n> Acked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\nThanks! I'll send out v2.\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Johannes Weiner (author)",
              "summary": "Note that the top one in the fast path IS called before the split. We're accounting in the same step size as the page allocator can give us. In the fallback paths (bulk allocator, and one-by-one loop), the issue is that the individual pages could be coming from different nodes, so they need to bump different counters. One possible solution would be to remember the last node and accumulate until it differs, then flush: fallback_loop() { page = alloc_pages(); nid = page_to_nid(page); if (nid != last_nid) { if (node_count) { mod_node_page_state(...); node_count = 0; } last_nid = nid; } } if (node_count) mod_node_page_state(...); But it IS the slow path, and these are fairly cheap per-cpu counters. Especially compared to the cost of calling into the allocator.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 04:30:32PM +0100, Uladzislau Rezki wrote:\n> On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> > @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \t\t\tcontinue;\n> >  \t\t}\n> >  \n> > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> > +\n> >  \t\tsplit_page(page, large_order);\n> >  \t\tfor (i = 0; i < (1U << large_order); i++)\n> >  \t\t\tpages[nr_allocated + i] = page + i;\n> > @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \tif (!order) {\n> >  \t\twhile (nr_allocated < nr_pages) {\n> >  \t\t\tunsigned int nr, nr_pages_request;\n> > +\t\t\tint i;\n> >  \n> >  \t\t\t/*\n> >  \t\t\t * A maximum allowed request is hard-coded and is 100\n> > @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \t\t\t\t\t\t\tnr_pages_request,\n> >  \t\t\t\t\t\t\tpages + nr_allocated);\n> >  \n> > +\t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n> > +\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n> > +\n> >  \t\t\tnr_allocated += nr;\n> >  \n> >  \t\t\t/*\n> > @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \t\tif (unlikely(!page))\n> >  \t\t\tbreak;\n> >  \n> > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n> > +\n> >  \t\t/*\n> Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()?\n> \n> Or mod_node_page_state in first place should be invoked on high-order\n> page before split(to avoid of looping over small pages afterword)?\n> \n> I mean it would be good to place to the one solid place. If it is possible\n> of course.\n\nNote that the top one in the fast path IS called before the\nsplit. We're accounting in the same step size as the page allocator\ncan give us.\n\nIn the fallback paths (bulk allocator, and one-by-one loop), the issue\nis that the individual pages could be coming from different nodes, so\nthey need to bump different counters. One possible solution would be\nto remember the last node and accumulate until it differs, then flush:\n\nfallback_loop() {\n\tpage = alloc_pages();\n\tnid = page_to_nid(page);\n\tif (nid != last_nid) {\n\t\tif (node_count) {\n\t\t\tmod_node_page_state(...);\n\t\t\tnode_count = 0;\n\t\t}\n\t\tlast_nid = nid;\n\t}\n}\n\nif (node_count)\n\tmod_node_page_state(...);\n\nBut it IS the slow path, and these are fairly cheap per-cpu\ncounters. Especially compared to the cost of calling into the\nallocator. So I'm not sure it's worth it... What do you think?\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Uladzislau Rezki",
              "summary": "I see. I agree it is easier to keep original solution. I see that Andrew took it, but just in case:",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Mon, Feb 23, 2026 at 03:19:20PM -0500, Johannes Weiner wrote:\n> On Mon, Feb 23, 2026 at 04:30:32PM +0100, Uladzislau Rezki wrote:\n> > On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> > > @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \t\t\tcontinue;\n> > >  \t\t}\n> > >  \n> > > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> > > +\n> > >  \t\tsplit_page(page, large_order);\n> > >  \t\tfor (i = 0; i < (1U << large_order); i++)\n> > >  \t\t\tpages[nr_allocated + i] = page + i;\n> > > @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \tif (!order) {\n> > >  \t\twhile (nr_allocated < nr_pages) {\n> > >  \t\t\tunsigned int nr, nr_pages_request;\n> > > +\t\t\tint i;\n> > >  \n> > >  \t\t\t/*\n> > >  \t\t\t * A maximum allowed request is hard-coded and is 100\n> > > @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \t\t\t\t\t\t\tnr_pages_request,\n> > >  \t\t\t\t\t\t\tpages + nr_allocated);\n> > >  \n> > > +\t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n> > > +\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n> > > +\n> > >  \t\t\tnr_allocated += nr;\n> > >  \n> > >  \t\t\t/*\n> > > @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \t\tif (unlikely(!page))\n> > >  \t\t\tbreak;\n> > >  \n> > > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n> > > +\n> > >  \t\t/*\n> > Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()?\n> > \n> > Or mod_node_page_state in first place should be invoked on high-order\n> > page before split(to avoid of looping over small pages afterword)?\n> > \n> > I mean it would be good to place to the one solid place. If it is possible\n> > of course.\n> \n> Note that the top one in the fast path IS called before the\n> split. We're accounting in the same step size as the page allocator\n> can give us.\n> \n> In the fallback paths (bulk allocator, and one-by-one loop), the issue\n> is that the individual pages could be coming from different nodes, so\n> they need to bump different counters. One possible solution would be\n> to remember the last node and accumulate until it differs, then flush:\n> \n> fallback_loop() {\n> \tpage = alloc_pages();\n> \tnid = page_to_nid(page);\n> \tif (nid != last_nid) {\n> \t\tif (node_count) {\n> \t\t\tmod_node_page_state(...);\n> \t\t\tnode_count = 0;\n> \t\t}\n> \t\tlast_nid = nid;\n> \t}\n> }\n> \n> if (node_count)\n> \tmod_node_page_state(...);\n> \n> But it IS the slow path, and these are fairly cheap per-cpu\n> counters. Especially compared to the cost of calling into the\n> allocator. So I'm not sure it's worth it... What do you think?\n>\nI see. I agree it is easier to keep original solution. I see that\nAndrew took it, but just in case:\n\nReviewed-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\n\n--\nUladzislau Rezki\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/2] mm: vmalloc: streamline vmalloc memory accounting",
          "message_id": "aZy2SHbXi6qdGS0a@cmpxchg.org",
          "url": "https://lore.kernel.org/all/aZy2SHbXi6qdGS0a@cmpxchg.org/",
          "date": "2026-02-23T20:19:25Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Uladzislau Rezki",
              "summary": "I see. I agree it is easier to keep original solution. I see that Andrew took it, but just in case:",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Mon, Feb 23, 2026 at 03:19:20PM -0500, Johannes Weiner wrote:\n> On Mon, Feb 23, 2026 at 04:30:32PM +0100, Uladzislau Rezki wrote:\n> > On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> > > @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \t\t\tcontinue;\n> > >  \t\t}\n> > >  \n> > > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> > > +\n> > >  \t\tsplit_page(page, large_order);\n> > >  \t\tfor (i = 0; i < (1U << large_order); i++)\n> > >  \t\t\tpages[nr_allocated + i] = page + i;\n> > > @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \tif (!order) {\n> > >  \t\twhile (nr_allocated < nr_pages) {\n> > >  \t\t\tunsigned int nr, nr_pages_request;\n> > > +\t\t\tint i;\n> > >  \n> > >  \t\t\t/*\n> > >  \t\t\t * A maximum allowed request is hard-coded and is 100\n> > > @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \t\t\t\t\t\t\tnr_pages_request,\n> > >  \t\t\t\t\t\t\tpages + nr_allocated);\n> > >  \n> > > +\t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n> > > +\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n> > > +\n> > >  \t\t\tnr_allocated += nr;\n> > >  \n> > >  \t\t\t/*\n> > > @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \t\tif (unlikely(!page))\n> > >  \t\t\tbreak;\n> > >  \n> > > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n> > > +\n> > >  \t\t/*\n> > Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()?\n> > \n> > Or mod_node_page_state in first place should be invoked on high-order\n> > page before split(to avoid of looping over small pages afterword)?\n> > \n> > I mean it would be good to place to the one solid place. If it is possible\n> > of course.\n> \n> Note that the top one in the fast path IS called before the\n> split. We're accounting in the same step size as the page allocator\n> can give us.\n> \n> In the fallback paths (bulk allocator, and one-by-one loop), the issue\n> is that the individual pages could be coming from different nodes, so\n> they need to bump different counters. One possible solution would be\n> to remember the last node and accumulate until it differs, then flush:\n> \n> fallback_loop() {\n> \tpage = alloc_pages();\n> \tnid = page_to_nid(page);\n> \tif (nid != last_nid) {\n> \t\tif (node_count) {\n> \t\t\tmod_node_page_state(...);\n> \t\t\tnode_count = 0;\n> \t\t}\n> \t\tlast_nid = nid;\n> \t}\n> }\n> \n> if (node_count)\n> \tmod_node_page_state(...);\n> \n> But it IS the slow path, and these are fairly cheap per-cpu\n> counters. Especially compared to the cost of calling into the\n> allocator. So I'm not sure it's worth it... What do you think?\n>\nI see. I agree it is easier to keep original solution. I see that\nAndrew took it, but just in case:\n\nReviewed-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\n\n--\nUladzislau Rezki\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH RFC 00/15] mm, swap: swap table phase IV with dynamic ghost swapfile",
          "message_id": "aZyFxKGXc8J6PIij@cmpxchg.org",
          "url": "https://lore.kernel.org/all/aZyFxKGXc8J6PIij@cmpxchg.org/",
          "date": "2026-02-23T16:52:25Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Kairui Song",
              "summary": "No, only one entry in the ghost swapfile (xswap or virtual swap file, anyway it's just a name). The one in the physical swap is a reverse mapping entry, it tells which slot in the ghost swapfile is pointing to the physical slot, so swapoff / migration of the physical slot can be done in O(1) time. So, zero duplicate of any data. I am talking about disk swap here, not zswap. Swapoff of a physical entry just loads the swap data in the virtual slot according to the reverse mapping entry. It can be dynamic (just si->max += 2M on every cluster allocation since it's really just a number now). Can be hidden, and can have an infinite size. That's just an interface design that can be flexibly changed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 1:00AM Johannes Weiner <hannes@cmpxchg.org> wrote:\n>\n> On Fri, Feb 20, 2026 at 07:42:01AM +0800, Kairui Song via B4 Relay wrote:\n> > - 8 bytes per slot memory usage, when using only plain swap.\n> >   - And the memory usage can be reduced to 3 or only 1 byte.\n> > - 16 bytes per slot memory usage, when using ghost / virtual zswap.\n> >   - Zswap can just use ci_dyn->virtual_table to free up it's content\n> >     completely.\n> >   - And the memory usage can be reduced to 11 or 8 bytes using the same\n> >     code above.\n> >   - 24 bytes only if including reverse mapping is in use.\n>\n> That seems to tie us pretty permanently to duplicate metadata.\n>\n> For every page that was written to disk through zswap, we have an\n> entry in the ghost swapfile, and an entry in the backend swapfile, no?\n\nNo, only one entry in the ghost swapfile (xswap or virtual swap file,\nanyway it's just a name). The one in the physical swap is a reverse\nmapping entry, it tells which slot in the ghost swapfile is pointing\nto the physical slot, so swapoff / migration of the physical slot can\nbe done in O(1) time.\n\nSo, zero duplicate of any data.\n\n>\n> > - Minimal code review or maintenance burden. All layers are using the exact\n> >   same infrastructure for metadata / allocation / synchronization, making\n> >   all API and conventions consistent and easy to maintain.\n> > - Writeback, migration and compaction are easily supportable since both\n> >   reverse mapping and reallocation are prepared. We just need a\n> >   folio_realloc_swap to allocate new entries for the existing entry, and\n> >   fill the swap table with a reserve map entry.\n> > - Fast swapoff: Just read into ghost / virtual swap cache.\n>\n> Can we get this for disk swap as well? ;)\n>\n> Zswap swapoff is already fairly fast, albeit CPU intense. It's the\n> scattered IO that makes swapoff on disks so terrible.\n\nI am talking about disk swap here, not zswap. Swapoff of a physical\nentry just loads the swap data in the virtual slot according to the\nreverse mapping entry.\n\n> > free -m\n> >                total        used        free      shared  buff/cache   available\n> > Mem:            1465         250         927           1         356        1215\n> > Swap:       15269887           0    15269887\n>\n> I'm not a fan of this. This makes free(1) output kind of useless, and\n> very misleading. The swap space presented here has nothing to do with\n> actual swap capacity, and the actual disk swap capacity is obscured.\n>\n> And how would a user choose this size? How would a distribution?\n\nIt can be dynamic (just si->max += 2M on every cluster allocation\nsince it's really just a number now). Can be hidden, and can have an\ninfinite size. That's just an interface design that can be flexibly\nchanged.\n\nFor example if we just set this to a super large value and hide it, it\nwill look identical to vss from userspace perspect, but stay optional\nand zero overhead for existing ZRAM or plain swap users.\n\n> The only limit is compression ratio, and you don't know this in\n> advance. This restriction seems pretty arbitrary and avoidable.\n\nJust as a reference: In practice we limit our ZRAM setup to 1/4 or 1:1\nof the total RAM to avoid the machine goto endless reclaim and never\ngo OOM.\n\nBut we can also have an infinite size ZSWAP now, with this series.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH RFC 08/15] mm, swap: store and check memcg info in the swap table",
          "message_id": "aZyCJ6pH4hey-ZoU@cmpxchg.org",
          "url": "https://lore.kernel.org/all/aZyCJ6pH4hey-ZoU@cmpxchg.org/",
          "date": "2026-02-23T16:37:00Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Kairui Song",
              "summary": "I'm just not sure how much of a real problem this is. The refault challenge change was made in commit b910718a948a which was before anon shadow was introduced. And shadows could get reclaimed, especially when under pressure (and we could be doing that again by reclaiming full_clusters with swap tables). And MGLRU simply ignores the target_memcg here yet it performs surprisingly well with multiple memcg setups. And I did find a comment in workingset.c saying the kernel used to activate all pages, which is also fine. And that commit also mentioned the active list shrinking, but anon active list gets shrinked just fine without refault feedback in shrink_lruvec under can_age_anon_pages. So in this RFC I just be a bit aggressive and changed it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 12:46AM Johannes Weiner <hannes@cmpxchg.org> wrote:\n>\n> On Fri, Feb 20, 2026 at 07:42:09AM +0800, Kairui Song via B4 Relay wrote:\n> > From: Kairui Song <kasong@tencent.com>\n> >\n> > To prepare for merging the swap_cgroup_ctrl into the swap table, store\n> > the memcg info in the swap table on swapout.\n> >\n> > This is done by using the existing shadow format.\n> >\n> > Note this also changes the refault counting at the nearest online memcg\n> > level:\n> >\n> > Unlike file folios, anon folios are mostly exclusive to one mem cgroup,\n> > and each cgroup is likely to have different characteristics.\n>\n> This is not correct.\n>\n> As much as I like the idea of storing the swap_cgroup association\n> inside the shadow entry, the refault evaluation needs to happen at the\n> level that drove eviction.\n>\n> Consider a workload that is split into cgroups purely for accounting,\n> not for setting different limits:\n>\n> workload (limit domain)\n> `- component A\n> `- component B\n>\n> This means the two components must compete freely, and it must behave\n> as if there is only one LRU. When pages get reclaimed in a round-robin\n> fashion, both A and B get aged at the same pace. Likewise, when pages\n> in A refault, they must challenge the *combined* workingset of both A\n> and B, not just the local pages.\n>\n> Otherwise, you risk retaining stale workingset in one subgroup while\n> the other one is thrashing. This breaks userspace expectations.\n>\n\nHi Johannes, thanks for pointing this out.\n\nI'm just not sure how much of a real problem this is. The refault\nchallenge change was made in commit b910718a948a which was before anon\nshadow was introduced. And shadows could get reclaimed, especially\nwhen under pressure (and we could be doing that again by reclaiming\nfull_clusters with swap tables). And MGLRU simply ignores the\ntarget_memcg here yet it performs surprisingly well with multiple\nmemcg setups. And I did find a comment in workingset.c saying the\nkernel used to activate all pages, which is also fine. And that commit\nalso mentioned the active list shrinking, but anon active list gets\nshrinked just fine without refault feedback in shrink_lruvec under\ncan_age_anon_pages.\n\nSo in this RFC I just be a bit aggressive and changed it. I can do\nsome tests with different memory size setup.\n\nIf we are not OK with it, then just use a ci->memcg_table then we are\nfine, everything is still dynamic but single slot usage could be a bit\nhigher, 8 bytes to 10 bytes: and maybe find a way later to make\nci->memcg_table NULL and shrink back to 8 bytes with, e.g. MGLRU and\nbalance the memcg with things like aging feed back maybe (the later\npart is just idea but seems doable?).\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Johannes Weiner (author)",
              "summary": "*if inactive anon is empty, as part of the second chance logic Please try to understand *why* this code is the way it is before throwing it all out. It was driven by real production problems. The fact that some workloads don't care is not prove that many don't hurt if you break this. Anon refault detection was added for that reason: Once you have swap, you facilitate anon workingsets that exceed memory capacity. At that point, cache replacement strategies apply. Scan resistance matters. With fast modern compression and flash swap, the anon set alone can be larger than memory capacity.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 04:34:00PM +0800, Kairui Song wrote:\n> On Tue, Feb 24, 2026 at 12:46\\u202fAM Johannes Weiner <hannes@cmpxchg.org> wrote:\n> >\n> > On Fri, Feb 20, 2026 at 07:42:09AM +0800, Kairui Song via B4 Relay wrote:\n> > > From: Kairui Song <kasong@tencent.com>\n> > >\n> > > To prepare for merging the swap_cgroup_ctrl into the swap table, store\n> > > the memcg info in the swap table on swapout.\n> > >\n> > > This is done by using the existing shadow format.\n> > >\n> > > Note this also changes the refault counting at the nearest online memcg\n> > > level:\n> > >\n> > > Unlike file folios, anon folios are mostly exclusive to one mem cgroup,\n> > > and each cgroup is likely to have different characteristics.\n> >\n> > This is not correct.\n> >\n> > As much as I like the idea of storing the swap_cgroup association\n> > inside the shadow entry, the refault evaluation needs to happen at the\n> > level that drove eviction.\n> >\n> > Consider a workload that is split into cgroups purely for accounting,\n> > not for setting different limits:\n> >\n> > workload (limit domain)\n> > `- component A\n> > `- component B\n> >\n> > This means the two components must compete freely, and it must behave\n> > as if there is only one LRU. When pages get reclaimed in a round-robin\n> > fashion, both A and B get aged at the same pace. Likewise, when pages\n> > in A refault, they must challenge the *combined* workingset of both A\n> > and B, not just the local pages.\n> >\n> > Otherwise, you risk retaining stale workingset in one subgroup while\n> > the other one is thrashing. This breaks userspace expectations.\n> >\n> \n> Hi Johannes, thanks for pointing this out.\n> \n> I'm just not sure how much of a real problem this is. The refault\n> challenge change was made in commit b910718a948a which was before anon\n> shadow was introduced. And shadows could get reclaimed, especially\n> when under pressure (and we could be doing that again by reclaiming\n> full_clusters with swap tables). And MGLRU simply ignores the\n> target_memcg here yet it performs surprisingly well with multiple\n> memcg setups. And I did find a comment in workingset.c saying the\n> kernel used to activate all pages, which is also fine. And that commit\n> also mentioned the active list shrinking, but anon active list gets\n> shrinked just fine without refault feedback in shrink_lruvec under\n> can_age_anon_pages.\n\n                    *if inactive anon is empty, as part of the second\n                     chance logic\n\nPlease try to understand *why* this code is the way it is before\nthrowing it all out. It was driven by real production problems. The\nfact that some workloads don't care is not prove that many don't hurt\nif you break this.\n\nAnon refault detection was added for that reason: Once you have swap,\nyou facilitate anon workingsets that exceed memory capacity. At that\npoint, cache replacement strategies apply. Scan resistance matters.\n\nWith fast modern compression and flash swap, the anon set alone can be\nlarger than memory capacity. Everything that\n6a3ed2123a78de22a9e2b2855068a8d89f8e14f4 says about file cache starts\napplying to anonymous pages: you don't want to throw out the hot anon\nworkingset just because somebody is doing a one-off burst scan through\na larger set of cold, swapped out pages.\n\nLike I said in the LSFMM thread, there is no difference between anon\nand file. There didn't use to be historically. The LRU lists were\nsplit mechanically because noswap systems became common (lots of RAM +\nrotational drives = sad swap) and there was no point in scanning/aging\nanonymous memory if there is no swap space.\n\nBut no reasonable argument has been put forth why anon should be aged\ncompletely differently than file when you DO have swap.\n\nThere is more explanation of Why for the cgroup behavior in the cover\nletter portion of 53138cea7f398d2cdd0fa22adeec7e16093e1ebd.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH RFC 06/15] memcg, swap: reparent the swap entry on swapin if swapout cgroup is dead",
          "message_id": "aZx-zFmQmC0zoWKs@cmpxchg.org",
          "url": "https://lore.kernel.org/all/aZx-zFmQmC0zoWKs@cmpxchg.org/",
          "date": "2026-02-23T16:22:42Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/2] mm: vmalloc: streamline vmalloc memory accounting",
          "message_id": "aZx5M2WYMK7pKhC1@cmpxchg.org",
          "url": "https://lore.kernel.org/all/aZx5M2WYMK7pKhC1@cmpxchg.org/",
          "date": "2026-02-23T15:58:49Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joshua Hahn",
      "primary_email": "joshua.hahnjy@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[RFC PATCH 0/6] mm/memcontrol: Make memcg limits tier-aware",
          "message_id": "20260223223830.586018-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260223223830.586018-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-24T00:19:15Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Memory cgroups provide an interface that allow multiple workloads on a host to co-exist, and establish both weak and strong memory isolation guarantees. For large servers and small embedded systems alike, memcgs provide an effective way to provide a baseline quality of service for protected workloads.\n\nThis works, because for the most part, all memory is equal (except for zram / zswap). Restricting a cgroup's memory footprint restricts how much it can hurt other workloads competing for memory. Likewise, setting memory.low or memory.min limits can provide weak and strong guarantees to the performance of a cgroup.\n\nHowever, on systems with tiered memory (e.g. CXL / compressed memory), the quality of service guarantees that memcg limits enforced become less effective, as memcg has no awareness of the physical location of its charged memory. In other words, a workload that is well-behaved within its memcg limits may still be hurting the performance of other well-behaving workloads on the system by hogging more than its \"fair share\" of toptier memory.\n\nIntroduce tier-aware memcg limits, which scale memory.low/high to reflect the ratio of toptier:total memory the cgroup has access.\n\nTake the following scenario as an example: On a host with 3:1 toptier:lowtier, say 150G toptier, and 50Glowtier, setting a cgroup's limits to: memory.min:  15G memory.low:  20G memory.high: 40G memory.max:  50G\n\nWill be enforced at the toptier as: memory.min:          15G memory.toptier_low:  15G (20 * 150/200) memory.toptier_high: 30G (40 * 150/200) memory.max:          50G\n\nLet's say that there are 4 such cgroups on the host. Previously, it would be possible for 3 hosts to completely take over all of DRAM, while one cgroup could only access the lowtier memory. In the perspective of a tier-agnostic memcg limit enforcement, the three cgroups are all well-behaved, consuming within their memory limits.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Michal Hocko",
              "summary": "This assumes that the active workingset size of all workloads doesn't fit into the top tier right? Otherwise promotions would make sure to that we have the most active memory in the top tier. Is this typical in real life configurations? Or do you intend to limit memory consumption on particular tier even without an external pressure? Let's spend some more time with the interface first. You seem to be focusing only on the top tier with this interface, right? Is this really the right way to go long term? What makes you believe that we do not really hit the same issue with other tiers as well? Also do we want/need to duplicate all the limits for each/top tier? What is the reasoning for the switch to be runtime sysctl rather than boot-time or cgroup mount option?",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon 23-02-26 14:38:23, Joshua Hahn wrote:\n> Memory cgroups provide an interface that allow multiple workloads on a\n> host to co-exist, and establish both weak and strong memory isolation\n> guarantees. For large servers and small embedded systems alike, memcgs\n> provide an effective way to provide a baseline quality of service for\n> protected workloads.\n> \n> This works, because for the most part, all memory is equal (except for\n> zram / zswap). Restricting a cgroup's memory footprint restricts how\n> much it can hurt other workloads competing for memory. Likewise, setting\n> memory.low or memory.min limits can provide weak and strong guarantees\n> to the performance of a cgroup.\n> \n> However, on systems with tiered memory (e.g. CXL / compressed memory),\n> the quality of service guarantees that memcg limits enforced become less\n> effective, as memcg has no awareness of the physical location of its\n> charged memory. In other words, a workload that is well-behaved within\n> its memcg limits may still be hurting the performance of other\n> well-behaving workloads on the system by hogging more than its\n> \"fair share\" of toptier memory.\n\nThis assumes that the active workingset size of all workloads doesn't\nfit into the top tier right? Otherwise promotions would make sure to\nthat we have the most active memory in the top tier. Is this typical in\nreal life configurations?\n\nOr do you intend to limit memory consumption on particular tier even\nwithout an external pressure?\n\n> Introduce tier-aware memcg limits, which scale memory.low/high to\n> reflect the ratio of toptier:total memory the cgroup has access.\n> \n> Take the following scenario as an example:\n> On a host with 3:1 toptier:lowtier, say 150G toptier, and 50Glowtier,\n> setting a cgroup's limits to:\n> \tmemory.min:  15G\n> \tmemory.low:  20G\n> \tmemory.high: 40G\n> \tmemory.max:  50G\n> \n> Will be enforced at the toptier as:\n> \tmemory.min:          15G\n> \tmemory.toptier_low:  15G (20 * 150/200)\n> \tmemory.toptier_high: 30G (40 * 150/200)\n> \tmemory.max:          50G\n\nLet's spend some more time with the interface first. You seem to be\nfocusing only on the top tier with this interface, right? Is this really the\nright way to go long term? What makes you believe that we do not really\nhit the same issue with other tiers as well? Also do we want/need to\nduplicate all the limits for each/top tier? What is the reasoning for\nthe switch to be runtime sysctl rather than boot-time or cgroup mount\noption?\n\nI will likely have more questions but these are immediate ones after\nreading the cover. Please note I haven't really looked at the\nimplementation yet. I really want to understand usecases and interface\nfirst.\n-- \nMichal Hocko\nSUSE Labs\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "I hope that you are doing well! Thank you for taking the time to review my work and leaving your thoughts. I wanted to note that I hope to bring this discussion to LSFMMBPF as well, to discuss what the scope of the project should be, what usecases there are (as I will note below), how to make this scalable and sustainable for the future, etc. I'll send out a topic proposal later today. I had separated the series from the proposal because I imagined that this series would go through many versions, so it would be helpful to have the topic as a unified place for pre-conference discussions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "Hello Michal,\n\nI hope that you are doing well! Thank you for taking the time to review my\nwork and leaving your thoughts.\n\nI wanted to note that I hope to bring this discussion to LSFMMBPF as well,\nto discuss what the scope of the project should be, what usecases there\nare (as I will note below), how to make this scalable and sustainable\nfor the future, etc. I'll send out a topic proposal later today. I had\nseparated the series from the proposal because I imagined that this\nseries would go through many versions, so it would be helpful to have\nthe topic as a unified place for pre-conference discussions.\n\n> > Memory cgroups provide an interface that allow multiple workloads on a\n> > host to co-exist, and establish both weak and strong memory isolation\n> > guarantees. For large servers and small embedded systems alike, memcgs\n> > provide an effective way to provide a baseline quality of service for\n> > protected workloads.\n> > \n> > This works, because for the most part, all memory is equal (except for\n> > zram / zswap). Restricting a cgroup's memory footprint restricts how\n> > much it can hurt other workloads competing for memory. Likewise, setting\n> > memory.low or memory.min limits can provide weak and strong guarantees\n> > to the performance of a cgroup.\n> > \n> > However, on systems with tiered memory (e.g. CXL / compressed memory),\n> > the quality of service guarantees that memcg limits enforced become less\n> > effective, as memcg has no awareness of the physical location of its\n> > charged memory. In other words, a workload that is well-behaved within\n> > its memcg limits may still be hurting the performance of other\n> > well-behaving workloads on the system by hogging more than its\n> > \"fair share\" of toptier memory.\n\nI will split up your questions to answer them individually:\n\n> This assumes that the active workingset size of all workloads doesn't\n> fit into the top tier right?\n\nYes, for the scenario above, a workload that is violating its fair share\nof toptier memory mostly hurts other workloads if the aggregate working\nset size of all workloads exceeds the size of toptier memory.\n\n> Otherwise promotions would make sure to that we have the most active\n> memory in the top tier.\n\nThis is true. And for a lot of usecases, this is 100% the right thing to do.\nHowever, with this patch I want to encourage a different perspective,\nwhich is to think about things in a per-workload perspective, and not a\nper-system perspective.\n\nHaving hot memory in high tiers and cold memory in low tiers is only\nlogical, since we increase the system's throughput and make the most\noptimal choices for latency. However, what about systems that care about\nobjectives other than simply maximizing throughput?\n\nIn the original cover letter I offered an example of VM hosting services\nthat care less about maximizing host-wide throughput, but more on ensuring\na bottomline performance guarantee for all workloads running on the system.\nFor the users on these services, they don't care that the host their VM is\nrunning on is maximizing throughput; rather, they care that their VM meets\nthe performance guarantees that their provider promised. If there is no\nway to know or enforce which tier of memory their workload lands on, either\nthe bottomline guarantee becomes very underestimated, or users must deal\nwith a high variance in performance.\n\nHere's another example: Let's say there is a host with multiple workloads,\neach serving queries for a database. The host would like to guarantee the\nlowest maximum latency possible, while maximizing the total throughput\nof the system. Once again in this situation, without tier-aware memcg\nlimits the host can maximize throughput, but can only make severely\nunderestimated promises on the bottom line.\n\n> Is this typical in real life configurations?\n\nI would say so. I think that the two examples above are realistic\nscenarios that cloud providers and hyperscalers might face on tiered systems.\n\n> Or do you intend to limit memory consumption on particular tier even\n> without an external pressure?\n\nThis is a great question, and one that I hope to discuss at LSFMMBPF\nto see how people expect an interface like this to work.\n\nOver the past few weeks, I have been discussing this idea during the\nLinux Memory Hotness and Promotion biweekly calls with Gregory Price [1].\nOne of the proposals that we made there (but did not include in this\nseries) is the idea of \"fixed\" vs. \"opportunistic\" reclaim.\n\nFixed mode is what we have here -- start limiting toptier usage whenever\na workload goes above its fair slice of toptier.\nOpportunistic mode would allow workloads to use more toptier memory than\nits fair share, but only be restricted when toptier is pressured.\n\nWhat do you think about these two options? For the stated goal of this\nseries, which is to help maximize the bottom line for workloads, fair\nshare seemed to make sense. Implementing opportunistic mode changes\non top of this work would most likely just be another sysctl.\n\n> > Introduce tier-aware memcg limits, which scale memory.low/high to\n> > reflect the ratio of toptier:total memory the cgroup has access.\n> > \n> > Take the following scenario as an example:\n> > On a host with 3:1 toptier:lowtier, say 150G toptier, and 50Glowtier,\n> > setting a cgroup's limits to:\n> > \tmemory.min:  15G\n> > \tmemory.low:  20G\n> > \tmemory.high: 40G\n> > \tmemory.max:  50G\n> > \n> > Will be enforced at the toptier as:\n> > \tmemory.min:          15G\n> > \tmemory.toptier_low:  15G (20 * 150/200)\n> > \tmemory.toptier_high: 30G (40 * 150/200)\n> > \tmemory.max:          50G\n\nI will split up the following points to answer them individually as well:\n\n> Let's spend some more time with the interface first.\n\nThat sounds good with me, my goal was to bring this out as an RFC patchset\nso folks could look at the code and understand the motivation, and then send\nout the LSFMMBPF topic proposal. In retrospect I think I should have done\nit in the opposite order. I'm sorry if this caused any confusion.\n\n> You seem to be focusing only on the top tier with this interface, right?\n> Is this really the right way to go long term? What makes you believe that\n> we do not really hit the same issue with other tiers as well?\n\nYes, that's right. I'm not sure if this is the right way to go long-term\n(say, past the next 5 years). My thinking was that I can stick with doing\nthis for toptier vs. non-toptier memory for now, and deal with having\n3+ tiers in the future, when we start to have systems with that many tiers.\nAFAICT two-tiered systems are still ~relatively new, and I don't think\nthere are a lot of genuine usecases for enforcing mid-tier memory limits\nas of now. Of course, I would be excited to learn about these usecases\nand work this patchset to support them as well if anybody has them.\n\n> Also do we want/need to duplicate all the limits for each/top tier?\n\nSorry, I'm not sure that I completely understood this question. Are you\nreferring to the case where we have multiple nodes in the toptier?\nIf so, then all of those nodes are treated the same, and don't have\nunique limits. Or are you referring to the case where we have multiple\ntiers in the toptier? If so, I hope the answer above can answer this too.\n\n> What is the reasoning for the switch to be runtime sysctl rather than\n> boot-time or cgroup mount option?\n\nGood point : -) I don't think cgroup mount options are a good idea,\nsince this would mean that we can have a set of cgroups self-policing\ntheir toptier usage, while another cgroup allocates memory unrestricted.\nThis would punish the self-policing cgroup and we would lose the benefit\nof having a bottomline performance guarantee.\n\n> I will likely have more questions but these are immediate ones after\n> reading the cover. Please note I haven't really looked at the\n> implementation yet. I really want to understand usecases and interface\n> first.\n\nThat sounds good to me, thank you again for reviewing this work!\nI hope you have a great day : -)\nJoshua\n\n[1] https://lore.kernel.org/linux-mm/c8bc2dce-d4ec-c16e-8df4-2624c48cfc06@google.com/\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Gregory Price",
              "summary": "Just injecting a few points here (disclosure: I have been in the development loop for this feature) Yes / No.  This makes the assumption that you always want this. Barring a minimum Quality of Service mechanism (as Joshua explains) this reduces the usefulness of a secondary tier of memory. Services will just prefer not to be deployed to these kinds of machines because the performance variance is too high.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 08:13:56AM -0800, Joshua Hahn wrote:\n> ... snip ...\n\nJust injecting a few points here\n(disclosure: I have been in the development loop for this feature)\n\n> \n> > Otherwise promotions would make sure to that we have the most active\n> > memory in the top tier.\n> \n\nYes / No.  This makes the assumption that you always want this.\n\nBarring a minimum Quality of Service mechanism (as Joshua explains)\nthis reduces the usefulness of a secondary tier of memory.\n\nServices will just prefer not to be deployed to these kinds of\nmachines because the performance variance is too high.\n\n> \n> > Is this typical in real life configurations?\n> \n> I would say so. I think that the two examples above are realistic\n> scenarios that cloud providers and hyperscalers might face on tiered systems.\n> \n\nThe answer is unequivocally yes.\n\nLacking tier-awareness is actually a huge blocker for deploying mixed\nworkloads on large, dense memory systems with multiple tiers (2+).\n\nTechnically we're already at 4-ish tiers: DDR, CXL, ZSWAP, SWAP.\n\nWe have zswap/swap controls in cgroups already, we just lack that same\ncontrol for coherent memory tiers.  This tries to use the existing nobs\n(max/high/low/min) to do what they already do - just proportionally.\n\n~Gregory\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Kaiyang Zhao",
              "summary": "recently released a preprint paper on arXiv that includes case studies with a few of Meta's production workloads using a prototype version of the patches. The results confirmed that co-colocated workloads can have working set sizes exceeding the limited top-tier memory capacity given today's server memory shapes and workload stacking settings, causing contention of top-tier...",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 01:49:21PM -0500, Gregory Price wrote:\n> \n> > \n> > > Is this typical in real life configurations?\n> > \n> > I would say so. I think that the two examples above are realistic\n> > scenarios that cloud providers and hyperscalers might face on tiered systems.\n> > \n> \n> The answer is unequivocally yes.\n> \n> Lacking tier-awareness is actually a huge blocker for deploying mixed\n> workloads on large, dense memory systems with multiple tiers (2+).\n\nHello! I'm the author of the RFC in 2024. Just want to add that we've\nrecently released a preprint paper on arXiv that includes case studies\nwith a few of Meta's production workloads using a prototype version of\nthe patches.\n\nThe results confirmed that co-colocated workloads can have working set\nsizes exceeding the limited top-tier memory capacity given today's\nserver memory shapes and workload stacking settings, causing contention\nof top-tier memory. Workloads see significant variations in tail\nlatency and throughput depending on the share of top-tier tier memory\nthey get, which this patch set will alleviate.\n\nBest,\nKaiyang\n\n[1] https://arxiv.org/pdf/2602.08800\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "JP Kobryn",
      "primary_email": "inwardvessel@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v5] mm: move pgscan, pgsteal, pgrefill to node stats",
          "message_id": "20260219235846.161910-1-jp.kobryn@linux.dev",
          "url": "https://lore.kernel.org/all/20260219235846.161910-1-jp.kobryn@linux.dev/",
          "date": "2026-02-19T23:59:27Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-19",
          "patch_summary": "There are situations where reclaim kicks in on a system with free memory. One possible cause is a NUMA imbalance scenario where one or more nodes are under pressure. It would help if we could easily identify such nodes.\n\nMove the pgscan, pgsteal, and pgrefill counters from vm_event_item to node_stat_item to provide per-node reclaim visibility. With these counters as node stats, the values are now displayed in the per-node section of /proc/zoneinfo, which allows for quick identification of the affected nodes.\n\n/proc/vmstat continues to report the same counters, aggregated across all nodes. But the ordering of these items within the readout changes as they move from the vm events section to the node stats section.\n\nMemcg accounting of these counters is preserved. The relocated counters remain visible in memory.stat alongside the existing aggregate pgscan and pgsteal counters.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Zi Yan",
              "summary": "Gave Acked-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On 19 Feb 2026, at 18:58, JP Kobryn (Meta) wrote:\n\n> There are situations where reclaim kicks in on a system with free memory.\n> One possible cause is a NUMA imbalance scenario where one or more nodes are\n> under pressure. It would help if we could easily identify such nodes.\n>\n> Move the pgscan, pgsteal, and pgrefill counters from vm_event_item to\n> node_stat_item to provide per-node reclaim visibility. With these counters\n> as node stats, the values are now displayed in the per-node section of\n> /proc/zoneinfo, which allows for quick identification of the affected\n> nodes.\n>\n> /proc/vmstat continues to report the same counters, aggregated across all\n> nodes. But the ordering of these items within the readout changes as they\n> move from the vm events section to the node stats section.\n>\n> Memcg accounting of these counters is preserved. The relocated counters\n> remain visible in memory.stat alongside the existing aggregate pgscan and\n> pgsteal counters.\n>\n> However, this change affects how the global counters are accumulated.\n> Previously, the global event count update was gated on !cgroup_reclaim(),\n> excluding memcg-based reclaim from /proc/vmstat. Now that\n> mod_lruvec_state() is being used to update the counters, the global\n> counters will include all reclaim. This is consistent with how pgdemote\n> counters are already tracked.\n>\n> Finally, the virtio_balloon driver is updated to use\n> global_node_page_state() to fetch the counters, as they are no longer\n> accessible through the vm_events array.\n>\n> Signed-off-by: JP Kobryn <jp.kobryn@linux.dev>\n> Suggested-by: Johannes Weiner <hannes@cmpxchg.org>\n> Acked-by: Michael S. Tsirkin <mst@redhat.com>\n> Reviewed-by: Vlastimil Babka (SUSE) <vbabka@kernel.org>\n> ---\n> v5:\n> \t- rebase onto mm/mm-new\n>\n> v4: https://lore.kernel.org/linux-mm/20260219171124.19053-1-jp.kobryn@linux.dev/\n> \t- remove unused memcg var from scan_folios()\n>\n> v3: https://lore.kernel.org/linux-mm/20260218222652.108411-1-jp.kobryn@linux.dev/\n> \t- additionally move PGREFILL to node stats\n>\n> v2: https://lore.kernel.org/linux-mm/20260218032941.225439-1-jp.kobryn@linux.dev/\n> \t- update commit message\n> \t- add entries to memory_stats array\n> \t- add switch cases in memcg_page_state_output_unit()\n>\n> v1: https://lore.kernel.org/linux-mm/20260212045109.255391-3-inwardvessel@gmail.com/\n>\n>  drivers/virtio/virtio_balloon.c |  8 ++---\n>  include/linux/mmzone.h          | 13 ++++++++\n>  include/linux/vm_event_item.h   | 13 --------\n>  mm/memcontrol.c                 | 56 +++++++++++++++++++++++----------\n>  mm/vmscan.c                     | 39 ++++++++---------------\n>  mm/vmstat.c                     | 26 +++++++--------\n>  6 files changed, 82 insertions(+), 73 deletions(-)\n>\n\nAcked-by: Zi Yan <ziy@nvidia.com>\n\nBest Regards,\nYan, Zi\n",
              "reply_to": "",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Gave Acked-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Thu, Feb 19, 2026 at 03:58:46PM -0800, JP Kobryn (Meta) wrote:\n> There are situations where reclaim kicks in on a system with free memory.\n> One possible cause is a NUMA imbalance scenario where one or more nodes are\n> under pressure. It would help if we could easily identify such nodes.\n> \n> Move the pgscan, pgsteal, and pgrefill counters from vm_event_item to\n> node_stat_item to provide per-node reclaim visibility. With these counters\n> as node stats, the values are now displayed in the per-node section of\n> /proc/zoneinfo, which allows for quick identification of the affected\n> nodes.\n> \n> /proc/vmstat continues to report the same counters, aggregated across all\n> nodes. But the ordering of these items within the readout changes as they\n> move from the vm events section to the node stats section.\n> \n> Memcg accounting of these counters is preserved. The relocated counters\n> remain visible in memory.stat alongside the existing aggregate pgscan and\n> pgsteal counters.\n> \n> However, this change affects how the global counters are accumulated.\n> Previously, the global event count update was gated on !cgroup_reclaim(),\n> excluding memcg-based reclaim from /proc/vmstat. Now that\n> mod_lruvec_state() is being used to update the counters, the global\n> counters will include all reclaim. This is consistent with how pgdemote\n> counters are already tracked.\n> \n> Finally, the virtio_balloon driver is updated to use\n> global_node_page_state() to fetch the counters, as they are no longer\n> accessible through the vm_events array.\n> \n> Signed-off-by: JP Kobryn <jp.kobryn@linux.dev>\n> Suggested-by: Johannes Weiner <hannes@cmpxchg.org>\n> Acked-by: Michael S. Tsirkin <mst@redhat.com>\n> Reviewed-by: Vlastimil Babka (SUSE) <vbabka@kernel.org>\n\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Yeah this difference always confused me.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Thu, Feb 19, 2026 at 03:58:46PM -0800, JP Kobryn (Meta) wrote:\n> There are situations where reclaim kicks in on a system with free memory.\n> One possible cause is a NUMA imbalance scenario where one or more nodes are\n> under pressure. It would help if we could easily identify such nodes.\n> \n> Move the pgscan, pgsteal, and pgrefill counters from vm_event_item to\n> node_stat_item to provide per-node reclaim visibility. With these counters\n> as node stats, the values are now displayed in the per-node section of\n> /proc/zoneinfo, which allows for quick identification of the affected\n> nodes.\n> \n> /proc/vmstat continues to report the same counters, aggregated across all\n> nodes. But the ordering of these items within the readout changes as they\n> move from the vm events section to the node stats section.\n> \n> Memcg accounting of these counters is preserved. The relocated counters\n> remain visible in memory.stat alongside the existing aggregate pgscan and\n> pgsteal counters.\n> \n> However, this change affects how the global counters are accumulated.\n> Previously, the global event count update was gated on !cgroup_reclaim(),\n> excluding memcg-based reclaim from /proc/vmstat. Now that\n> mod_lruvec_state() is being used to update the counters, the global\n> counters will include all reclaim. This is consistent with how pgdemote\n> counters are already tracked.\n\nYeah this difference always confused me.\n\n> \n> Finally, the virtio_balloon driver is updated to use\n> global_node_page_state() to fetch the counters, as they are no longer\n> accessible through the vm_events array.\n> \n> Signed-off-by: JP Kobryn <jp.kobryn@linux.dev>\n> Suggested-by: Johannes Weiner <hannes@cmpxchg.org>\n> Acked-by: Michael S. Tsirkin <mst@redhat.com>\n> Reviewed-by: Vlastimil Babka (SUSE) <vbabka@kernel.org>\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Michal Hocko",
              "summary": "Gave Acked-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Thu 19-02-26 15:58:46, JP Kobryn (Meta) wrote:\n> There are situations where reclaim kicks in on a system with free memory.\n> One possible cause is a NUMA imbalance scenario where one or more nodes are\n> under pressure. It would help if we could easily identify such nodes.\n> \n> Move the pgscan, pgsteal, and pgrefill counters from vm_event_item to\n> node_stat_item to provide per-node reclaim visibility. With these counters\n> as node stats, the values are now displayed in the per-node section of\n> /proc/zoneinfo, which allows for quick identification of the affected\n> nodes.\n> \n> /proc/vmstat continues to report the same counters, aggregated across all\n> nodes. But the ordering of these items within the readout changes as they\n> move from the vm events section to the node stats section.\n> \n> Memcg accounting of these counters is preserved. The relocated counters\n> remain visible in memory.stat alongside the existing aggregate pgscan and\n> pgsteal counters.\n> \n> However, this change affects how the global counters are accumulated.\n> Previously, the global event count update was gated on !cgroup_reclaim(),\n> excluding memcg-based reclaim from /proc/vmstat. Now that\n> mod_lruvec_state() is being used to update the counters, the global\n> counters will include all reclaim. This is consistent with how pgdemote\n> counters are already tracked.\n> \n> Finally, the virtio_balloon driver is updated to use\n> global_node_page_state() to fetch the counters, as they are no longer\n> accessible through the vm_events array.\n> \n> Signed-off-by: JP Kobryn <jp.kobryn@linux.dev>\n> Suggested-by: Johannes Weiner <hannes@cmpxchg.org>\n> Acked-by: Michael S. Tsirkin <mst@redhat.com>\n> Reviewed-by: Vlastimil Babka (SUSE) <vbabka@kernel.org>\n\nAcked-by: Michal Hocko <mhocko@suse.com>\nThanks\n\n> ---\n> v5:\n> \t- rebase onto mm/mm-new\n> \n> v4: https://lore.kernel.org/linux-mm/20260219171124.19053-1-jp.kobryn@linux.dev/\n> \t- remove unused memcg var from scan_folios()\n> \n> v3: https://lore.kernel.org/linux-mm/20260218222652.108411-1-jp.kobryn@linux.dev/\n> \t- additionally move PGREFILL to node stats\n> \n> v2: https://lore.kernel.org/linux-mm/20260218032941.225439-1-jp.kobryn@linux.dev/\n> \t- update commit message\n> \t- add entries to memory_stats array\n> \t- add switch cases in memcg_page_state_output_unit()\n> \n> v1: https://lore.kernel.org/linux-mm/20260212045109.255391-3-inwardvessel@gmail.com/\n> \n>  drivers/virtio/virtio_balloon.c |  8 ++---\n>  include/linux/mmzone.h          | 13 ++++++++\n>  include/linux/vm_event_item.h   | 13 --------\n>  mm/memcontrol.c                 | 56 +++++++++++++++++++++++----------\n>  mm/vmscan.c                     | 39 ++++++++---------------\n>  mm/vmstat.c                     | 26 +++++++--------\n>  6 files changed, 82 insertions(+), 73 deletions(-)\n> \n> diff --git a/drivers/virtio/virtio_balloon.c b/drivers/virtio/virtio_balloon.c\n> index 4e549abe59ff..ab945532ceef 100644\n> --- a/drivers/virtio/virtio_balloon.c\n> +++ b/drivers/virtio/virtio_balloon.c\n> @@ -369,13 +369,13 @@ static inline unsigned int update_balloon_vm_stats(struct virtio_balloon *vb)\n>  \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_ALLOC_STALL, stall);\n>  \n>  \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_ASYNC_SCAN,\n> -\t\t    pages_to_bytes(events[PGSCAN_KSWAPD]));\n> +\t\t    pages_to_bytes(global_node_page_state(PGSCAN_KSWAPD)));\n>  \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_DIRECT_SCAN,\n> -\t\t    pages_to_bytes(events[PGSCAN_DIRECT]));\n> +\t\t    pages_to_bytes(global_node_page_state(PGSCAN_DIRECT)));\n>  \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_ASYNC_RECLAIM,\n> -\t\t    pages_to_bytes(events[PGSTEAL_KSWAPD]));\n> +\t\t    pages_to_bytes(global_node_page_state(PGSTEAL_KSWAPD)));\n>  \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_DIRECT_RECLAIM,\n> -\t\t    pages_to_bytes(events[PGSTEAL_DIRECT]));\n> +\t\t    pages_to_bytes(global_node_page_state(PGSTEAL_DIRECT)));\n>  \n>  #ifdef CONFIG_HUGETLB_PAGE\n>  \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_HTLB_PGALLOC,\n> diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> index 3e51190a55e4..546bca95ca40 100644\n> --- a/include/linux/mmzone.h\n> +++ b/include/linux/mmzone.h\n> @@ -255,6 +255,19 @@ enum node_stat_item {\n>  \tPGDEMOTE_DIRECT,\n>  \tPGDEMOTE_KHUGEPAGED,\n>  \tPGDEMOTE_PROACTIVE,\n> +\tPGSTEAL_KSWAPD,\n> +\tPGSTEAL_DIRECT,\n> +\tPGSTEAL_KHUGEPAGED,\n> +\tPGSTEAL_PROACTIVE,\n> +\tPGSTEAL_ANON,\n> +\tPGSTEAL_FILE,\n> +\tPGSCAN_KSWAPD,\n> +\tPGSCAN_DIRECT,\n> +\tPGSCAN_KHUGEPAGED,\n> +\tPGSCAN_PROACTIVE,\n> +\tPGSCAN_ANON,\n> +\tPGSCAN_FILE,\n> +\tPGREFILL,\n>  #ifdef CONFIG_HUGETLB_PAGE\n>  \tNR_HUGETLB,\n>  #endif\n> diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\n> index 22a139f82d75..03fe95f5a020 100644\n> --- a/include/linux/vm_event_item.h\n> +++ b/include/linux/vm_event_item.h\n> @@ -38,21 +38,8 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n>  \t\tPGFREE, PGACTIVATE, PGDEACTIVATE, PGLAZYFREE,\n>  \t\tPGFAULT, PGMAJFAULT,\n>  \t\tPGLAZYFREED,\n> -\t\tPGREFILL,\n>  \t\tPGREUSE,\n> -\t\tPGSTEAL_KSWAPD,\n> -\t\tPGSTEAL_DIRECT,\n> -\t\tPGSTEAL_KHUGEPAGED,\n> -\t\tPGSTEAL_PROACTIVE,\n> -\t\tPGSCAN_KSWAPD,\n> -\t\tPGSCAN_DIRECT,\n> -\t\tPGSCAN_KHUGEPAGED,\n> -\t\tPGSCAN_PROACTIVE,\n>  \t\tPGSCAN_DIRECT_THROTTLE,\n> -\t\tPGSCAN_ANON,\n> -\t\tPGSCAN_FILE,\n> -\t\tPGSTEAL_ANON,\n> -\t\tPGSTEAL_FILE,\n>  #ifdef CONFIG_NUMA\n>  \t\tPGSCAN_ZONE_RECLAIM_SUCCESS,\n>  \t\tPGSCAN_ZONE_RECLAIM_FAILED,\n> diff --git a/mm/memcontrol.c b/mm/memcontrol.c\n> index 6fb9c999347b..0d834c47706f 100644\n> --- a/mm/memcontrol.c\n> +++ b/mm/memcontrol.c\n> @@ -331,6 +331,19 @@ static const unsigned int memcg_node_stat_items[] = {\n>  \tPGDEMOTE_DIRECT,\n>  \tPGDEMOTE_KHUGEPAGED,\n>  \tPGDEMOTE_PROACTIVE,\n> +\tPGSTEAL_KSWAPD,\n> +\tPGSTEAL_DIRECT,\n> +\tPGSTEAL_KHUGEPAGED,\n> +\tPGSTEAL_PROACTIVE,\n> +\tPGSTEAL_ANON,\n> +\tPGSTEAL_FILE,\n> +\tPGSCAN_KSWAPD,\n> +\tPGSCAN_DIRECT,\n> +\tPGSCAN_KHUGEPAGED,\n> +\tPGSCAN_PROACTIVE,\n> +\tPGSCAN_ANON,\n> +\tPGSCAN_FILE,\n> +\tPGREFILL,\n>  #ifdef CONFIG_HUGETLB_PAGE\n>  \tNR_HUGETLB,\n>  #endif\n> @@ -444,17 +457,8 @@ static const unsigned int memcg_vm_event_stat[] = {\n>  #endif\n>  \tPSWPIN,\n>  \tPSWPOUT,\n> -\tPGSCAN_KSWAPD,\n> -\tPGSCAN_DIRECT,\n> -\tPGSCAN_KHUGEPAGED,\n> -\tPGSCAN_PROACTIVE,\n> -\tPGSTEAL_KSWAPD,\n> -\tPGSTEAL_DIRECT,\n> -\tPGSTEAL_KHUGEPAGED,\n> -\tPGSTEAL_PROACTIVE,\n>  \tPGFAULT,\n>  \tPGMAJFAULT,\n> -\tPGREFILL,\n>  \tPGACTIVATE,\n>  \tPGDEACTIVATE,\n>  \tPGLAZYFREE,\n> @@ -1401,6 +1405,15 @@ static const struct memory_stat memory_stats[] = {\n>  \t{ \"pgdemote_direct\",\t\tPGDEMOTE_DIRECT\t\t},\n>  \t{ \"pgdemote_khugepaged\",\tPGDEMOTE_KHUGEPAGED\t},\n>  \t{ \"pgdemote_proactive\",\t\tPGDEMOTE_PROACTIVE\t},\n> +\t{ \"pgsteal_kswapd\",\t\tPGSTEAL_KSWAPD\t\t},\n> +\t{ \"pgsteal_direct\",\t\tPGSTEAL_DIRECT\t\t},\n> +\t{ \"pgsteal_khugepaged\",\t\tPGSTEAL_KHUGEPAGED\t},\n> +\t{ \"pgsteal_proactive\",\t\tPGSTEAL_PROACTIVE\t},\n> +\t{ \"pgscan_kswapd\",\t\tPGSCAN_KSWAPD\t\t},\n> +\t{ \"pgscan_direct\",\t\tPGSCAN_DIRECT\t\t},\n> +\t{ \"pgscan_khugepaged\",\t\tPGSCAN_KHUGEPAGED\t},\n> +\t{ \"pgscan_proactive\",\t\tPGSCAN_PROACTIVE\t},\n> +\t{ \"pgrefill\",\t\t\tPGREFILL\t\t},\n>  #ifdef CONFIG_NUMA_BALANCING\n>  \t{ \"pgpromote_success\",\t\tPGPROMOTE_SUCCESS\t},\n>  #endif\n> @@ -1444,6 +1457,15 @@ static int memcg_page_state_output_unit(int item)\n>  \tcase PGDEMOTE_DIRECT:\n>  \tcase PGDEMOTE_KHUGEPAGED:\n>  \tcase PGDEMOTE_PROACTIVE:\n> +\tcase PGSTEAL_KSWAPD:\n> +\tcase PGSTEAL_DIRECT:\n> +\tcase PGSTEAL_KHUGEPAGED:\n> +\tcase PGSTEAL_PROACTIVE:\n> +\tcase PGSCAN_KSWAPD:\n> +\tcase PGSCAN_DIRECT:\n> +\tcase PGSCAN_KHUGEPAGED:\n> +\tcase PGSCAN_PROACTIVE:\n> +\tcase PGREFILL:\n>  #ifdef CONFIG_NUMA_BALANCING\n>  \tcase PGPROMOTE_SUCCESS:\n>  #endif\n> @@ -1562,15 +1584,15 @@ static void memcg_stat_format(struct mem_cgroup *memcg, struct seq_buf *s)\n>  \n>  \t/* Accumulated memory events */\n>  \tmemcg_seq_buf_print_stat(s, NULL, \"pgscan\", ' ',\n> -\t\t\t\t memcg_events(memcg, PGSCAN_KSWAPD) +\n> -\t\t\t\t memcg_events(memcg, PGSCAN_DIRECT) +\n> -\t\t\t\t memcg_events(memcg, PGSCAN_PROACTIVE) +\n> -\t\t\t\t memcg_events(memcg, PGSCAN_KHUGEPAGED));\n> +\t\t\t\t memcg_page_state(memcg, PGSCAN_KSWAPD) +\n> +\t\t\t\t memcg_page_state(memcg, PGSCAN_DIRECT) +\n> +\t\t\t\t memcg_page_state(memcg, PGSCAN_PROACTIVE) +\n> +\t\t\t\t memcg_page_state(memcg, PGSCAN_KHUGEPAGED));\n>  \tmemcg_seq_buf_print_stat(s, NULL, \"pgsteal\", ' ',\n> -\t\t\t\t memcg_events(memcg, PGSTEAL_KSWAPD) +\n> -\t\t\t\t memcg_events(memcg, PGSTEAL_DIRECT) +\n> -\t\t\t\t memcg_events(memcg, PGSTEAL_PROACTIVE) +\n> -\t\t\t\t memcg_events(memcg, PGSTEAL_KHUGEPAGED));\n> +\t\t\t\t memcg_page_state(memcg, PGSTEAL_KSWAPD) +\n> +\t\t\t\t memcg_page_state(memcg, PGSTEAL_DIRECT) +\n> +\t\t\t\t memcg_page_state(memcg, PGSTEAL_PROACTIVE) +\n> +\t\t\t\t memcg_page_state(memcg, PGSTEAL_KHUGEPAGED));\n>  \n>  \tfor (i = 0; i < ARRAY_SIZE(memcg_vm_event_stat); i++) {\n>  #ifdef CONFIG_MEMCG_V1\n> diff --git a/mm/vmscan.c b/mm/vmscan.c\n> index 5fa6e6bd6540..c3dc7c7befac 100644\n> --- a/mm/vmscan.c\n> +++ b/mm/vmscan.c\n> @@ -1984,7 +1984,7 @@ static unsigned long shrink_inactive_list(unsigned long nr_to_scan,\n>  \tunsigned long nr_taken;\n>  \tstruct reclaim_stat stat;\n>  \tbool file = is_file_lru(lru);\n> -\tenum vm_event_item item;\n> +\tenum node_stat_item item;\n>  \tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n>  \tbool stalled = false;\n>  \n> @@ -2010,10 +2010,8 @@ static unsigned long shrink_inactive_list(unsigned long nr_to_scan,\n>  \n>  \t__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, nr_taken);\n>  \titem = PGSCAN_KSWAPD + reclaimer_offset(sc);\n> -\tif (!cgroup_reclaim(sc))\n> -\t\t__count_vm_events(item, nr_scanned);\n> -\tcount_memcg_events(lruvec_memcg(lruvec), item, nr_scanned);\n> -\t__count_vm_events(PGSCAN_ANON + file, nr_scanned);\n> +\tmod_lruvec_state(lruvec, item, nr_scanned);\n> +\tmod_lruvec_state(lruvec, PGSCAN_ANON + file, nr_scanned);\n>  \n>  \tspin_unlock_irq(&lruvec->lru_lock);\n>  \n> @@ -2030,10 +2028,8 @@ static unsigned long shrink_inactive_list(unsigned long nr_to_scan,\n>  \t\t\t\t\tstat.nr_demoted);\n>  \t__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);\n>  \titem = PGSTEAL_KSWAPD + reclaimer_offset(sc);\n> -\tif (!cgroup_reclaim(sc))\n> -\t\t__count_vm_events(item, nr_reclaimed);\n> -\tcount_memcg_events(lruvec_memcg(lruvec), item, nr_reclaimed);\n> -\t__count_vm_events(PGSTEAL_ANON + file, nr_reclaimed);\n> +\tmod_lruvec_state(lruvec, item, nr_reclaimed);\n> +\tmod_lruvec_state(lruvec, PGSTEAL_ANON + file, nr_reclaimed);\n>  \n>  \tlru_note_cost_unlock_irq(lruvec, file, stat.nr_pageout,\n>  \t\t\t\t\tnr_scanned - nr_reclaimed);\n> @@ -2120,9 +2116,7 @@ static void shrink_active_list(unsigned long nr_to_scan,\n>  \n>  \t__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, nr_taken);\n>  \n> -\tif (!cgroup_reclaim(sc))\n> -\t\t__count_vm_events(PGREFILL, nr_scanned);\n> -\tcount_memcg_events(lruvec_memcg(lruvec), PGREFILL, nr_scanned);\n> +\tmod_lruvec_state(lruvec, PGREFILL, nr_scanned);\n>  \n>  \tspin_unlock_irq(&lruvec->lru_lock);\n>  \n> @@ -4537,7 +4531,7 @@ static int scan_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n>  {\n>  \tint i;\n>  \tint gen;\n> -\tenum vm_event_item item;\n> +\tenum node_stat_item item;\n>  \tint sorted = 0;\n>  \tint scanned = 0;\n>  \tint isolated = 0;\n> @@ -4545,7 +4539,6 @@ static int scan_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n>  \tint scan_batch = min(nr_to_scan, MAX_LRU_BATCH);\n>  \tint remaining = scan_batch;\n>  \tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n> -\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n>  \n>  \tVM_WARN_ON_ONCE(!list_empty(list));\n>  \n> @@ -4596,13 +4589,9 @@ static int scan_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n>  \t}\n>  \n>  \titem = PGSCAN_KSWAPD + reclaimer_offset(sc);\n> -\tif (!cgroup_reclaim(sc)) {\n> -\t\t__count_vm_events(item, isolated);\n> -\t\t__count_vm_events(PGREFILL, sorted);\n> -\t}\n> -\tcount_memcg_events(memcg, item, isolated);\n> -\tcount_memcg_events(memcg, PGREFILL, sorted);\n> -\t__count_vm_events(PGSCAN_ANON + type, isolated);\n> +\tmod_lruvec_state(lruvec, item, isolated);\n> +\tmod_lruvec_state(lruvec, PGREFILL, sorted);\n> +\tmod_lruvec_state(lruvec, PGSCAN_ANON + type, isolated);\n>  \ttrace_mm_vmscan_lru_isolate(sc->reclaim_idx, sc->order, scan_batch,\n>  \t\t\t\tscanned, skipped, isolated,\n>  \t\t\t\ttype ? LRU_INACTIVE_FILE : LRU_INACTIVE_ANON);\n> @@ -4705,7 +4694,7 @@ static int evict_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n>  \tLIST_HEAD(clean);\n>  \tstruct folio *folio;\n>  \tstruct folio *next;\n> -\tenum vm_event_item item;\n> +\tenum node_stat_item item;\n>  \tstruct reclaim_stat stat;\n>  \tstruct lru_gen_mm_walk *walk;\n>  \tbool skip_retry = false;\n> @@ -4769,10 +4758,8 @@ static int evict_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n>  \t\t\t\t\tstat.nr_demoted);\n>  \n>  \titem = PGSTEAL_KSWAPD + reclaimer_offset(sc);\n> -\tif (!cgroup_reclaim(sc))\n> -\t\t__count_vm_events(item, reclaimed);\n> -\tcount_memcg_events(memcg, item, reclaimed);\n> -\t__count_vm_events(PGSTEAL_ANON + type, reclaimed);\n> +\tmod_lruvec_state(lruvec, item, reclaimed);\n> +\tmod_lruvec_state(lruvec, PGSTEAL_ANON + type, reclaimed);\n>  \n>  \tspin_unlock_irq(&lruvec->lru_lock);\n>  \n> diff --git a/mm/vmstat.c b/mm/vmstat.c\n> index 86b14b0f77b5..44bbb7752f11 100644\n> --- a/mm/vmstat.c\n> +++ b/mm/vmstat.c\n> @@ -1276,6 +1276,19 @@ const char * const vmstat_text[] = {\n>  \t[I(PGDEMOTE_DIRECT)]\t\t\t= \"pgdemote_direct\",\n>  \t[I(PGDEMOTE_KHUGEPAGED)]\t\t= \"pgdemote_khugepaged\",\n>  \t[I(PGDEMOTE_PROACTIVE)]\t\t\t= \"pgdemote_proactive\",\n> +\t[I(PGSTEAL_KSWAPD)]\t\t\t= \"pgsteal_kswapd\",\n> +\t[I(PGSTEAL_DIRECT)]\t\t\t= \"pgsteal_direct\",\n> +\t[I(PGSTEAL_KHUGEPAGED)]\t\t\t= \"pgsteal_khugepaged\",\n> +\t[I(PGSTEAL_PROACTIVE)]\t\t\t= \"pgsteal_proactive\",\n> +\t[I(PGSTEAL_ANON)]\t\t\t= \"pgsteal_anon\",\n> +\t[I(PGSTEAL_FILE)]\t\t\t= \"pgsteal_file\",\n> +\t[I(PGSCAN_KSWAPD)]\t\t\t= \"pgscan_kswapd\",\n> +\t[I(PGSCAN_DIRECT)]\t\t\t= \"pgscan_direct\",\n> +\t[I(PGSCAN_KHUGEPAGED)]\t\t\t= \"pgscan_khugepaged\",\n> +\t[I(PGSCAN_PROACTIVE)]\t\t\t= \"pgscan_proactive\",\n> +\t[I(PGSCAN_ANON)]\t\t\t= \"pgscan_anon\",\n> +\t[I(PGSCAN_FILE)]\t\t\t= \"pgscan_file\",\n> +\t[I(PGREFILL)]\t\t\t\t= \"pgrefill\",\n>  #ifdef CONFIG_HUGETLB_PAGE\n>  \t[I(NR_HUGETLB)]\t\t\t\t= \"nr_hugetlb\",\n>  #endif\n> @@ -1318,21 +1331,8 @@ const char * const vmstat_text[] = {\n>  \t[I(PGMAJFAULT)]\t\t\t\t= \"pgmajfault\",\n>  \t[I(PGLAZYFREED)]\t\t\t= \"pglazyfreed\",\n>  \n> -\t[I(PGREFILL)]\t\t\t\t= \"pgrefill\",\n>  \t[I(PGREUSE)]\t\t\t\t= \"pgreuse\",\n> -\t[I(PGSTEAL_KSWAPD)]\t\t\t= \"pgsteal_kswapd\",\n> -\t[I(PGSTEAL_DIRECT)]\t\t\t= \"pgsteal_direct\",\n> -\t[I(PGSTEAL_KHUGEPAGED)]\t\t\t= \"pgsteal_khugepaged\",\n> -\t[I(PGSTEAL_PROACTIVE)]\t\t\t= \"pgsteal_proactive\",\n> -\t[I(PGSCAN_KSWAPD)]\t\t\t= \"pgscan_kswapd\",\n> -\t[I(PGSCAN_DIRECT)]\t\t\t= \"pgscan_direct\",\n> -\t[I(PGSCAN_KHUGEPAGED)]\t\t\t= \"pgscan_khugepaged\",\n> -\t[I(PGSCAN_PROACTIVE)]\t\t\t= \"pgscan_proactive\",\n>  \t[I(PGSCAN_DIRECT_THROTTLE)]\t\t= \"pgscan_direct_throttle\",\n> -\t[I(PGSCAN_ANON)]\t\t\t= \"pgscan_anon\",\n> -\t[I(PGSCAN_FILE)]\t\t\t= \"pgscan_file\",\n> -\t[I(PGSTEAL_ANON)]\t\t\t= \"pgsteal_anon\",\n> -\t[I(PGSTEAL_FILE)]\t\t\t= \"pgsteal_file\",\n>  \n>  #ifdef CONFIG_NUMA\n>  \t[I(PGSCAN_ZONE_RECLAIM_SUCCESS)]\t= \"zone_reclaim_success\",\n> -- \n> 2.47.3\n\n-- \nMichal Hocko\nSUSE Labs\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Kiryl Shutsemau",
      "primary_email": "kas@kernel.org",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86",
          "message_id": "aZx1EOLutZd1XrPP@thinkstation",
          "url": "https://lore.kernel.org/all/aZx1EOLutZd1XrPP@thinkstation/",
          "date": "2026-02-23T15:45:53Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "David (Arm)",
              "summary": "Single mapcount, single anon-exclusive flag. Completely different story :P",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": ">>\n>> Just thinking about VMAs spanning partial pages makes me shiver. Or A\n>> single page spanning multiple VMAs.\n> \n> Hate to break it to you, but we have it now upstream :P\n> \n> THP can span multiple VMAs. And can be partially mapped.\n\nSingle mapcount, single anon-exclusive flag.\n\nCompletely different story :P\n\n-- \nCheers,\n\nDavid\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCHv6 08/17] mm: Make page_zonenum() use head page",
          "message_id": "aZxn34ebvSKFCWth@thinkstation",
          "url": "https://lore.kernel.org/all/aZxn34ebvSKFCWth@thinkstation/",
          "date": "2026-02-23T14:52:52Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "I gave it a try, but stumbled on a problem. We need to know the zone in hugetlb_vmemmap_init_early(), but zones are not yet defined. hugetlb_vmemmap_init_early() is called from within sparse_init(), but span of zones is defined in free_area_init() after sparse_init(). Any ideas, how get past this? :/",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 16, 2026 at 11:30:22AM +0000, Kiryl Shutsemau wrote:\n> On Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:\n> > On 2/9/26 12:52, Kiryl Shutsemau wrote:\n> > > On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> > >> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > >> > With the upcoming changes to HVO, a single page of tail struct pages\n> > >> > will be shared across all huge pages of the same order on a node. Since\n> > >> > huge pages on the same node may belong to different zones, the zone\n> > >> > information stored in shared tail page flags would be incorrect.\n> > >> > \n> > >> > Always fetch zone information from the head page, which has unique and\n> > >> > correct zone flags for each compound page.\n> > >> > \n> > >> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > >> > Acked-by: Zi Yan <ziy@nvidia.com>\n> > >> > ---\n> > >> >   include/linux/mmzone.h | 1 +\n> > >> >   1 file changed, 1 insertion(+)\n> > >> > \n> > >> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> > >> > index be8ce40b5638..192143b5cdc0 100644\n> > >> > --- a/include/linux/mmzone.h\n> > >> > +++ b/include/linux/mmzone.h\n> > >> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> > >> >   static inline enum zone_type page_zonenum(const struct page *page)\n> > >> >   {\n> > >> > +\tpage = compound_head(page);\n> > >> >   \treturn memdesc_zonenum(page->flags);\n> > >> \n> > >> We end up calling page_zonenum() without holding a reference.\n> > >> \n> > >> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> > >> see concurrent page freeing etc.\n> > >> \n> > >> However, this change implies that we now perform a compound page lookup for\n> > >> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> > >> including for pageblock access and page freeing].\n> > >> \n> > >> That's a nasty compromise for making HVO better? :)\n> > >> \n> > >> We should likely limit that special casing to kernels that really rquire it\n> > >> (HVO).\n> > > \n> > > I will add compound_info_has_mask() check.\n> > \n> > Not thrilled by this indeed. Would it be a problem to have the shared tail\n> > pages per node+zone instead of just per node?\n> \n> I thought it would be overkill. It likely is going to be unused for most\n> nodes. But sure, move it to per-zone.\n\nI gave it a try, but stumbled on a problem.\n\nWe need to know the zone in hugetlb_vmemmap_init_early(), but zones are\nnot yet defined.\n\nhugetlb_vmemmap_init_early() is called from within sparse_init(), but\nspan of zones is defined in free_area_init() after sparse_init().\n\nAny ideas, how get past this? :/\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86",
          "message_id": "aZxEKdXTXoI0BZYJ@thinkstation",
          "url": "https://lore.kernel.org/all/aZxEKdXTXoI0BZYJ@thinkstation/",
          "date": "2026-02-23T12:16:12Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86",
          "message_id": "aZw1DlKHaWvgOtm_@thinkstation",
          "url": "https://lore.kernel.org/all/aZw1DlKHaWvgOtm_@thinkstation/",
          "date": "2026-02-23T11:13:38Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "David (Arm)",
              "summary": "Even in well controlled environments you would run in a hyperscaler?",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/23/26 12:13, Kiryl Shutsemau wrote:\n> On Mon, Feb 23, 2026 at 12:04:10PM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/20/26 20:33, Kalesh Singh wrote:\n>>> On Fri, Feb 20, 2026 at 8:30\\u202fAM David Hildenbrand (Arm)\n>>> <david@kernel.org> wrote:\n>>>\n>>> I think most issues will stem from linkers setting the default ELF\n>>> segment alignment (max-page-size) for x86 to 4096. So those ELFs will\n>>> not load correctly or at all on the larger emulated granularity.\n>>\n>> Right, I assume that they will have to be thought about that, and possibly,\n>> some binaries/libraries recompiled.\n> \n> I think backward compatibility is important and I believe we can get\n> there without ABI break. And optimize from there.\n> \n> BTW, x86-64 SysV ABI allows for 64k page size:\n> \n> \tSystems are permitted to use any power-of-two page size between\n> \t4KB and 64KB, inclusive.\n> \n> But it doesn't work in practice.\n\nEven in well controlled environments you would run in a hyperscaler?\n\n-- \nCheers,\n\nDavid\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "I have not invested much time into investigating this. I intentionally targeted compatible version assuming it will be better received by upstream. I want it to be usable outside specially cured userspace. 64k might not be good fit for a desktop, but 16k can be a different story.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 12:27:33PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/23/26 12:13, Kiryl Shutsemau wrote:\n> > On Mon, Feb 23, 2026 at 12:04:10PM +0100, David Hildenbrand (Arm) wrote:\n> > > On 2/20/26 20:33, Kalesh Singh wrote:\n> > > > On Fri, Feb 20, 2026 at 8:30\\u202fAM David Hildenbrand (Arm)\n> > > > <david@kernel.org> wrote:\n> > > > \n> > > > I think most issues will stem from linkers setting the default ELF\n> > > > segment alignment (max-page-size) for x86 to 4096. So those ELFs will\n> > > > not load correctly or at all on the larger emulated granularity.\n> > > \n> > > Right, I assume that they will have to be thought about that, and possibly,\n> > > some binaries/libraries recompiled.\n> > \n> > I think backward compatibility is important and I believe we can get\n> > there without ABI break. And optimize from there.\n> > \n> > BTW, x86-64 SysV ABI allows for 64k page size:\n> > \n> > \tSystems are permitted to use any power-of-two page size between\n> > \t4KB and 64KB, inclusive.\n> > \n> > But it doesn't work in practice.\n> \n> Even in well controlled environments you would run in a hyperscaler?\n\nI have not invested much time into investigating this.\n\nI intentionally targeted compatible version assuming it will be better\nreceived by upstream. I want it to be usable outside specially cured\nuserspace. 64k might not be good fit for a desktop, but 16k can be a\ndifferent story.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dave Hansen",
              "summary": "I think what Kirill is trying to say is that \"it breaks userspace\". ;) A hyperscaler (or other \"embedded\" environment) might be willing or able to go fix up userspace breakage. I would suspect our high frequency trading friends would be all over this if it shaved a microsecond off their receive times. The more important question is what it breaks and how badly it breaks things.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/23/26 03:27, David Hildenbrand (Arm) wrote:\n...\n>> BTW, x86-64 SysV ABI allows for 64k page size:\n>>\n>> Systems are permitted to use any power-of-two page size between\n>> 4KB and 64KB, inclusive.\n>>\n>> But it doesn't work in practice.\n> \n> Even in well controlled environments you would run in a hyperscaler?\n\nI think what Kirill is trying to say is that \"it breaks userspace\". ;)\n\nA hyperscaler (or other \"embedded\" environment) might be willing or able\nto go fix up userspace breakage. I would suspect our high frequency\ntrading friends would be all over this if it shaved a microsecond off\ntheir receive times.\n\nThe more important question is what it breaks and how badly it breaks\nthings. 5-level paging, for instance, broke some JITs that historically\nused the new (>48) upper virtual address bits for metadata. The gains\nfrom 5-level paging were big enough and the userspace breakage was\nconfined and fixable enough that 5-level paging was viable.\n\nI'm not sure which side a larger base page side will fall on, though. Is\nit going to be an out-of-tree hack that a few folks use, or will it be\nmore like 5-level paging and be good enough that it goes into mainline?\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "David (Arm)",
              "summary": "Yes. Probably similar to Intel proposing an actual 64k page size. Expected. :) Just thinking about VMAs spanning partial pages makes me shiver. Or A single page spanning multiple VMAs. I haven't seen the code yet, but I am certain that I will not like it. I'm happy to be proven wrong :)",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/23/26 16:14, Dave Hansen wrote:\n> On 2/23/26 03:27, David Hildenbrand (Arm) wrote:\n> ...\n>>> BTW, x86-64 SysV ABI allows for 64k page size:\n>>>\n>>>  Systems are permitted to use any power-of-two page size between\n>>>  4KB and 64KB, inclusive.\n>>>\n>>> But it doesn't work in practice.\n>>\n>> Even in well controlled environments you would run in a hyperscaler?\n> \n> I think what Kirill is trying to say is that \"it breaks userspace\". ;)\n\nYes. Probably similar to Intel proposing an actual 64k page size. \nExpected. :)\n\n> \n> A hyperscaler (or other \"embedded\" environment) might be willing or able\n> to go fix up userspace breakage. I would suspect our high frequency\n> trading friends would be all over this if it shaved a microsecond off\n> their receive times.\n> \n> The more important question is what it breaks and how badly it breaks\n> things. 5-level paging, for instance, broke some JITs that historically\n> used the new (>48) upper virtual address bits for metadata. The gains\n> from 5-level paging were big enough and the userspace breakage was\n> confined and fixable enough that 5-level paging was viable.\n> \n> I'm not sure which side a larger base page side will fall on, though. Is\n> it going to be an out-of-tree hack that a few folks use, or will it be\n> more like 5-level paging and be good enough that it goes into mainline?\n\nJust thinking about VMAs spanning partial pages makes me shiver. Or A \nsingle page spanning multiple VMAs.\n\nI haven't seen the code yet, but I am certain that I will not like it.\n\nI'm happy to be proven wrong :)\n\n-- \nCheers,\n\nDavid\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Hate to break it to you, but we have it now upstream :P THP can span multiple VMAs. And can be partially mapped. The only new thing is that we allow this for order-0 page now. And you cannot realistically recover wasted memory -- no deferred split. I will do my best, but no promises :)",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 04:31:56PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/23/26 16:14, Dave Hansen wrote:\n> > On 2/23/26 03:27, David Hildenbrand (Arm) wrote:\n> > ...\n> > > > BTW, x86-64 SysV ABI allows for 64k page size:\n> > > > \n> > > >  Systems are permitted to use any power-of-two page size between\n> > > >  4KB and 64KB, inclusive.\n> > > > \n> > > > But it doesn't work in practice.\n> > > \n> > > Even in well controlled environments you would run in a hyperscaler?\n> > \n> > I think what Kirill is trying to say is that \"it breaks userspace\". ;)\n> \n> Yes. Probably similar to Intel proposing an actual 64k page size. Expected.\n> :)\n> \n> > \n> > A hyperscaler (or other \"embedded\" environment) might be willing or able\n> > to go fix up userspace breakage. I would suspect our high frequency\n> > trading friends would be all over this if it shaved a microsecond off\n> > their receive times.\n> > \n> > The more important question is what it breaks and how badly it breaks\n> > things. 5-level paging, for instance, broke some JITs that historically\n> > used the new (>48) upper virtual address bits for metadata. The gains\n> > from 5-level paging were big enough and the userspace breakage was\n> > confined and fixable enough that 5-level paging was viable.\n> > \n> > I'm not sure which side a larger base page side will fall on, though. Is\n> > it going to be an out-of-tree hack that a few folks use, or will it be\n> > more like 5-level paging and be good enough that it goes into mainline?\n> \n> Just thinking about VMAs spanning partial pages makes me shiver. Or A\n> single page spanning multiple VMAs.\n\nHate to break it to you, but we have it now upstream :P\n\nTHP can span multiple VMAs. And can be partially mapped.\n\nThe only new thing is that we allow this for order-0 page now. And you\ncannot realistically recover wasted memory -- no deferred split.\n\n> I haven't seen the code yet, but I am certain that I will not like it.\n> \n> I'm happy to be proven wrong :)\n\nI will do my best, but no promises :)\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "David (Arm)",
              "summary": "Single mapcount, single anon-exclusive flag. Completely different story :P",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": ">>\n>> Just thinking about VMAs spanning partial pages makes me shiver. Or A\n>> single page spanning multiple VMAs.\n> \n> Hate to break it to you, but we have it now upstream :P\n> \n> THP can span multiple VMAs. And can be partially mapped.\n\nSingle mapcount, single anon-exclusive flag.\n\nCompletely different story :P\n\n-- \nCheers,\n\nDavid\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Lorenzo Stoakes",
              "summary": "Yeah agree, we're not doing this. It's already a nightmare to deal with per-page anonexclusive vs. per-folio pretty much everything else, and we shouldn't have allowed that to be a thing, but now we have to live with it. If the code tries to implement anything that even resembles some sub-base-page metadata then that's just not something that's going to land. Handling VMA vs. folio state coherently is _already_ painful and difficult. Piling on more complexity because we theoretically could feels rather along the lines of 'let's just keep adding features and not worrying about where we end up', which is I think a bit of an anti-pattern in the kernel in general. Sure me also if I'm missing something, but what's discussed here is... worrying.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 04:31:56PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/23/26 16:14, Dave Hansen wrote:\n> > On 2/23/26 03:27, David Hildenbrand (Arm) wrote:\n> > ...\n> > > > BTW, x86-64 SysV ABI allows for 64k page size:\n> > > >\n> > > >  Systems are permitted to use any power-of-two page size between\n> > > >  4KB and 64KB, inclusive.\n> > > >\n> > > > But it doesn't work in practice.\n> > >\n> > > Even in well controlled environments you would run in a hyperscaler?\n> >\n> > I think what Kirill is trying to say is that \"it breaks userspace\". ;)\n>\n> Yes. Probably similar to Intel proposing an actual 64k page size. Expected.\n> :)\n>\n> >\n> > A hyperscaler (or other \"embedded\" environment) might be willing or able\n> > to go fix up userspace breakage. I would suspect our high frequency\n> > trading friends would be all over this if it shaved a microsecond off\n> > their receive times.\n> >\n> > The more important question is what it breaks and how badly it breaks\n> > things. 5-level paging, for instance, broke some JITs that historically\n> > used the new (>48) upper virtual address bits for metadata. The gains\n> > from 5-level paging were big enough and the userspace breakage was\n> > confined and fixable enough that 5-level paging was viable.\n> >\n> > I'm not sure which side a larger base page side will fall on, though. Is\n> > it going to be an out-of-tree hack that a few folks use, or will it be\n> > more like 5-level paging and be good enough that it goes into mainline?\n>\n> Just thinking about VMAs spanning partial pages makes me shiver. Or A single\n> page spanning multiple VMAs.\n\nYeah agree, we're not doing this.\n\nIt's already a nightmare to deal with per-page anonexclusive vs. per-folio\npretty much everything else, and we shouldn't have allowed that to be a thing,\nbut now we have to live with it.\n\n>\n> I haven't seen the code yet, but I am certain that I will not like it.\n\nIf the code tries to implement anything that even resembles some sub-base-page\nmetadata then that's just not something that's going to land.\n\nHandling VMA vs. folio state coherently is _already_ painful and difficult.\n\nPiling on more complexity because we theoretically could feels rather along the\nlines of 'let's just keep adding features and not worrying about where we end\nup', which is I think a bit of an anti-pattern in the kernel in general.\n\n>\n> I'm happy to be proven wrong :)\n\nSure me also if I'm missing something, but what's discussed here is... worrying.\n\n>\n> --\n> Cheers,\n>\n> David\n\nThanks, Lorenzo\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "David Laight",
              "summary": "With a 4k physical page what stops you dynamically splitting the 64k a 'struct page' references into 16 4k pages (using an extra dynamically allocated structure)? I'm not thinking it would happen that often, but it would solve the problem of 4k aligned .data and (probably) mmap() of small files.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, 23 Feb 2026 07:14:39 -0800\nDave Hansen <dave.hansen@intel.com> wrote:\n\n> On 2/23/26 03:27, David Hildenbrand (Arm) wrote:\n> ...\n> >> BTW, x86-64 SysV ABI allows for 64k page size:\n> >>\n> >> Systems are permitted to use any power-of-two page size between\n> >> 4KB and 64KB, inclusive.\n> >>\n> >> But it doesn't work in practice.  \n> > \n> > Even in well controlled environments you would run in a hyperscaler?  \n> \n> I think what Kirill is trying to say is that \"it breaks userspace\". ;)\n\nWith a 4k physical page what stops you dynamically splitting the 64k a\n'struct page' references into 16 4k pages (using an extra dynamically\nallocated structure)?\nI'm not thinking it would happen that often, but it would solve the\nproblem of 4k aligned .data and (probably) mmap() of small files.\n\nIf the cpu supports TLB coalescing there could easily be a net gain\nusing 64k pages for most of a program binary.\n\n\tDavid\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Leo Martins",
      "primary_email": "loemra.dev@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Mark Harmstone",
      "primary_email": "mark@harmstone.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix error messages in btrfs_check_features()",
          "message_id": "20260218111346.31243-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260218111346.31243-1-mark@harmstone.com/",
          "date": "2026-02-18T11:14:00Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-18",
          "patch_summary": "Commit d7f67ac9 introduced a regression when it comes to handling unsupported incompat or compat_ro flags. Beforehand we only printed the flags that we didn't recognize, afterwards we printed them all, which is less useful. Fix the error handling so it behaves like it used to.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Qu Wenruo",
              "summary": "\\u5728 2026/2/18 21:43, Mark Harmstone \\u5199\\u9053:",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\n\\u5728 2026/2/18 21:43, Mark Harmstone \\u5199\\u9053:\n> Commit d7f67ac9 introduced a regression when it comes to handling\n> unsupported incompat or compat_ro flags. Beforehand we only printed the\n> flags that we didn't recognize, afterwards we printed them all, which is\n> less useful. Fix the error handling so it behaves like it used to.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: d7f67ac9a928 (\"btrfs: relax block-group-tree feature dependency checks\")\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\n\nThanks,\nQu\n\n> ---\n>   fs/btrfs/disk-io.c | 6 +++---\n>   1 file changed, 3 insertions(+), 3 deletions(-)\n> \n> diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c\n> index f39008591631..7478d1c50cca 100644\n> --- a/fs/btrfs/disk-io.c\n> +++ b/fs/btrfs/disk-io.c\n> @@ -3176,7 +3176,7 @@ int btrfs_check_features(struct btrfs_fs_info *fs_info, bool is_rw_mount)\n>   \tif (incompat & ~BTRFS_FEATURE_INCOMPAT_SUPP) {\n>   \t\tbtrfs_err(fs_info,\n>   \t\t\"cannot mount because of unknown incompat features (0x%llx)\",\n> -\t\t    incompat);\n> +\t\t    incompat & ~BTRFS_FEATURE_INCOMPAT_SUPP);\n>   \t\treturn -EINVAL;\n>   \t}\n>   \n> @@ -3208,7 +3208,7 @@ int btrfs_check_features(struct btrfs_fs_info *fs_info, bool is_rw_mount)\n>   \tif (compat_ro_unsupp && is_rw_mount) {\n>   \t\tbtrfs_err(fs_info,\n>   \t\"cannot mount read-write because of unknown compat_ro features (0x%llx)\",\n> -\t\t       compat_ro);\n> +\t\t       compat_ro_unsupp);\n>   \t\treturn -EINVAL;\n>   \t}\n>   \n> @@ -3221,7 +3221,7 @@ int btrfs_check_features(struct btrfs_fs_info *fs_info, bool is_rw_mount)\n>   \t    !btrfs_test_opt(fs_info, NOLOGREPLAY)) {\n>   \t\tbtrfs_err(fs_info,\n>   \"cannot replay dirty log with unsupported compat_ro features (0x%llx), try rescue=nologreplay\",\n> -\t\t\t  compat_ro);\n> +\t\t\t  compat_ro_unsupp);\n>   \t\treturn -EINVAL;\n>   \t}\n>   \n\n",
              "reply_to": "",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "David Sterba",
              "summary": "Please use full commit reference in changelog text at least for the first occurence, the format is the same as for the Fixes: line. Thanks.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Wed, Feb 18, 2026 at 11:13:40AM +0000, Mark Harmstone wrote:\n> Commit d7f67ac9\n\nPlease use full commit reference in changelog text at least for the\nfirst occurence, the format is the same as for the Fixes: line. Thanks.\n\n> introduced a regression when it comes to handling\n> unsupported incompat or compat_ro flags. Beforehand we only printed the\n> flags that we didn't recognize, afterwards we printed them all, which is\n> less useful. Fix the error handling so it behaves like it used to.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: d7f67ac9a928 (\"btrfs: relax block-group-tree feature dependency checks\")\n> ---\n>  fs/btrfs/disk-io.c | 6 +++---\n>  1 file changed, 3 insertions(+), 3 deletions(-)\n> \n> diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c\n> index f39008591631..7478d1c50cca 100644\n> --- a/fs/btrfs/disk-io.c\n> +++ b/fs/btrfs/disk-io.c\n> @@ -3176,7 +3176,7 @@ int btrfs_check_features(struct btrfs_fs_info *fs_info, bool is_rw_mount)\n>  \tif (incompat & ~BTRFS_FEATURE_INCOMPAT_SUPP) {\n>  \t\tbtrfs_err(fs_info,\n>  \t\t\"cannot mount because of unknown incompat features (0x%llx)\",\n> -\t\t    incompat);\n> +\t\t    incompat & ~BTRFS_FEATURE_INCOMPAT_SUPP);\n>  \t\treturn -EINVAL;\n>  \t}\n>  \n> @@ -3208,7 +3208,7 @@ int btrfs_check_features(struct btrfs_fs_info *fs_info, bool is_rw_mount)\n>  \tif (compat_ro_unsupp && is_rw_mount) {\n>  \t\tbtrfs_err(fs_info,\n>  \t\"cannot mount read-write because of unknown compat_ro features (0x%llx)\",\n> -\t\t       compat_ro);\n> +\t\t       compat_ro_unsupp);\n>  \t\treturn -EINVAL;\n>  \t}\n>  \n> @@ -3221,7 +3221,7 @@ int btrfs_check_features(struct btrfs_fs_info *fs_info, bool is_rw_mount)\n>  \t    !btrfs_test_opt(fs_info, NOLOGREPLAY)) {\n>  \t\tbtrfs_err(fs_info,\n>  \"cannot replay dirty log with unsupported compat_ro features (0x%llx), try rescue=nologreplay\",\n> -\t\t\t  compat_ro);\n> +\t\t\t  compat_ro_unsupp);\n>  \t\treturn -EINVAL;\n>  \t}\n>  \n> -- \n> 2.52.0\n> \n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "David Sterba",
              "summary": "Also please configure your git so the hashes are at least 12 characters, I've been using 14 and I think others do too. This is 'core.abbrev' config.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Wed, Feb 18, 2026 at 11:13:40AM +0000, Mark Harmstone wrote:\n> Commit d7f67ac9\n\nAlso please configure your git so the hashes are at least 12 characters,\nI've been using 14 and I think others do too. This is 'core.abbrev'\nconfig.\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Nhat Pham",
      "primary_email": "nphamcs@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] Swap status and roadmap discussion",
          "message_id": "CAKEwX=O4ishgvhhZ1ssgbDUQewFamkyFT-uCpEWecWfe8SzwGg@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAKEwX=O4ishgvhhZ1ssgbDUQewFamkyFT-uCpEWecWfe8SzwGg@mail.gmail.com/",
          "date": "2026-02-23T18:38:52Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Yosry Ahmed",
              "summary": "I think we should actually revisit the need for a reverse mapping to begin with. For swapoff, we can probably scan the virtual swap space looking for entries that belong to the backend being swapped off. Not as efficient as a reverse map, but still better than the status quo of scanning page tables.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "> > - Is 64 bits really needed for reverse mapping? For the context, reverse\n> >   mapping here is a swap entry recorded in a lower / physical device\n> >   pointing to the ghost / virtual device.\n>\n> I think you can compact this a bit. Swap space itself is not fully 64\n> bits right?\n>\n> Just not sure if the juice is worth the squeeze to save a couple of\n> bits here and there, especially if the reverse mapping is already\n> dynamic :)\n\nI think we should actually revisit the need for a reverse mapping to\nbegin with. For swapoff, we can probably scan the virtual swap space\nlooking for entries that belong to the backend being swapped off. Not\nas efficient as a reverse map, but still better than the status quo of\nscanning page tables. I don't think optimizing for swapoff is worth\nthe consistent overhead.\n\nThe other use cases are probably cluster readahead and swapcache-only\nreclaim, and I think both of these can also be revisited.\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Yosry Ahmed",
              "summary": "Yeah I am not against having a reverse map in general (regardless of what design we end up having), I just think it has to be justified. With the current code, the use cases are not that important imo, so we can potentially drop it. If new features like migration and compaction require a reverse map, it can be tied to them, and depending on the use cases, we could even make the reverse mapping optional depending on whether these features are enabled or not -- at least in theory. Yeah it's not necessarily related, I was just mentioning that these are the obvious current users of the reverse map, but I don't think we should keep the reverse map for them. It kinda makes sense in theory, but ultimately it should be whatever makes the numbers look good for the largest amount of workloads.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 7:45PM Kairui Song <ryncsn@gmail.com> wrote:\n>\n> On Tue, Feb 24, 2026 at 2:55AM Yosry Ahmed <yosry@kernel.org> wrote:\n> >\n> > > > - Is 64 bits really needed for reverse mapping? For the context, reverse\n> > > >   mapping here is a swap entry recorded in a lower / physical device\n> > > >   pointing to the ghost / virtual device.\n> > >\n> > > I think you can compact this a bit. Swap space itself is not fully 64\n> > > bits right?\n> > >\n> > > Just not sure if the juice is worth the squeeze to save a couple of\n> > > bits here and there, especially if the reverse mapping is already\n> > > dynamic :)\n> >\n>\n> Hi, thanks for the comment.\n>\n> > I think we should actually revisit the need for a reverse mapping to\n> > begin with. For swapoff, we can probably scan the virtual swap space\n> > looking for entries that belong to the backend being swapped off. Not\n> > as efficient as a reverse map, but still better than the status quo of\n> > scanning page tables. I don't think optimizing for swapoff is worth\n> > the consistent overhead.\n>\n> Right, I don't really think swapoff is worth that much effort too. But\n> there are still ideas like migration and compaction, which could\n> really make use of a proper reverse map.\n\nYeah I am not against having a reverse map in general (regardless of\nwhat design we end up having), I just think it has to be justified.\nWith the current code, the use cases are not that important imo, so we\ncan potentially drop it.\n\nIf new features like migration and compaction require a reverse map,\nit can be tied to them, and depending on the use cases, we could even\nmake the reverse mapping optional depending on whether these features\nare enabled or not -- at least in theory.\n\n>\n> >\n> > The other use cases are probably cluster readahead and swapcache-only\n> > reclaim, and I think both of these can also be revisited.\n>\n> Agree, readahead and swap cache reclaim do need some revisit... Not\n> related to the revert map idea though.\n\nYeah it's not necessarily related, I was just mentioning that these\nare the obvious current users of the reverse map, but I don't think we\nshould keep the reverse map for them.\n\n>\n> I'm thinking if we can make the swap cache completely lazy and never\n> reclaim it proactively for non-RAM swap. And for RAM based swap\n> (zswap / ZRAM), do the opposite, always ensure swap cache is\n> reclaimed after use.\n\nIt kinda makes sense in theory, but ultimately it should be whatever\nmakes the numbers look good for the largest amount of workloads.\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Kairui Song",
              "summary": "Right, I don't really think swapoff is worth that much effort too. But there are still ideas like migration and compaction, which could really make use of a proper reverse map. Agree, readahead and swap cache reclaim do need some revisit... Not related to the revert map idea though. I'm thinking if we can make the swap cache completely lazy and never reclaim it proactively for non-RAM swap. And for RAM based swap (zswap / ZRAM), do the opposite, always ensure swap cache is reclaimed after use.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 2:55AM Yosry Ahmed <yosry@kernel.org> wrote:\n>\n> > > - Is 64 bits really needed for reverse mapping? For the context, reverse\n> > >   mapping here is a swap entry recorded in a lower / physical device\n> > >   pointing to the ghost / virtual device.\n> >\n> > I think you can compact this a bit. Swap space itself is not fully 64\n> > bits right?\n> >\n> > Just not sure if the juice is worth the squeeze to save a couple of\n> > bits here and there, especially if the reverse mapping is already\n> > dynamic :)\n>\n\nHi, thanks for the comment.\n\n> I think we should actually revisit the need for a reverse mapping to\n> begin with. For swapoff, we can probably scan the virtual swap space\n> looking for entries that belong to the backend being swapped off. Not\n> as efficient as a reverse map, but still better than the status quo of\n> scanning page tables. I don't think optimizing for swapoff is worth\n> the consistent overhead.\n\nRight, I don't really think swapoff is worth that much effort too. But\nthere are still ideas like migration and compaction, which could\nreally make use of a proper reverse map.\n\n>\n> The other use cases are probably cluster readahead and swapcache-only\n> reclaim, and I think both of these can also be revisited.\n\nAgree, readahead and swap cache reclaim do need some revisit... Not\nrelated to the revert map idea though.\n\nI'm thinking if we can make the swap cache completely lazy and never\nreclaim it proactively for non-RAM swap. And for RAM based swap\n(zswap / ZRAM), do the opposite, always ensure swap cache is\nreclaimed after use.\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH RFC 00/15] mm, swap: swap table phase IV with dynamic ghost swapfile",
          "message_id": "CAKEwX=OaDKQwanaYm=Mt+mWAKjaqXPdiScF6NB=TZYx1B-Xo8w@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAKEwX=OaDKQwanaYm=Mt+mWAKjaqXPdiScF6NB=TZYx1B-Xo8w@mail.gmail.com/",
          "date": "2026-02-23T18:22:37Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Kairui Song",
              "summary": "It's already doing that, storing metadata at the top layer, only a reverse mapping in the lower layer. So none of these issues are still there. Don't worry, I do remember that conversation and kept that in mind :) I left that part undone kind of on purpose, since it's only RFC, and in hope that there could be collaboration. And the dynamic allocator is only ~200 LOC now. Other parts of this series are not only for virtual swap. For example the unified folio alloc for swapin, which gives us 15% performance gain in real workloads, can still get merged and benifit all of us without involving the virtual swap or memcg part. And meanwhile, with the later patches, we don't have to re-implement the whole infrastructure to have a virtual table.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "merged"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 2:22AM Nhat Pham <nphamcs@gmail.com> wrote:\n>\n> On Thu, Feb 19, 2026 at 3:42PM Kairui Song via B4 Relay\n> <devnull+kasong.tencent.com@kernel.org> wrote:\n> > Huge thanks to Chris Li for the layered swap table and ghost swapfile\n> > idea, without whom the work here can't be archived. Also, thanks to Nhat\n> > for pushing and suggesting using an Xarray for the swapfile [11] for\n> > dynamic size. I was originally planning to use a dynamic cluster\n> > array, which requires a bit more adaptation, cleanup, and convention\n> > changes. But during the discussion there, I got the inspiration that\n> > Xarray can be used as the intermediate step, making this approach\n> > doable with minimal changes. Just keep using it in the future, it\n> > might not hurt too, as Xarray is only limited to ghost / virtual\n> > files, so plain swaps won't have any extra overhead for lookup or high\n> > risk of swapout allocation failure.\n>\n> Thanks for your effort. Dynamic swap space is a very important\n> consideration anyone deploying compressed swapping backend on large\n> memory systems in general. And yeah, I think using a radix tree/xarray\n> is easiest out-of-the-box solution for this - thanks for citing me :P\n\nThanks for the discussion :)\n\n>\n> I still have some confusion and concerns though. Johannes already made\n> some good points - I'll just add some thoughts from my point of view,\n> having gone back and forth with virtual swap designs:\n>\n> 1. At which layer should the metadata (swap count, swap cgroup, etc.) live?\n>\n> I remember that in your LSFMMBPF presentation (time flies), your\n> proposal was to store a redirection entry in the top layer, and keep\n> all the metadata at the bottom (i.e backend) layer? This has problems\n> - for once, you might not know suitable backend at swap allocation\n> time, but only at writeout time. For e.g, in certain zswap setups, we\n> reject the incompressible page and cycle it back to the active LRU, so\n> we have no space in zswap layer to store swap entry metadata (note\n> that at this point the swap entry cannot be freed, because we have\n> already unmapped the page from the PTEs (and would require a page\n> table walk to undo this a la swapoff). Similarly, when we\n> exclusive-load a page from zswap, we invalidate the zswap metadata\n> struct, so we will no longer have a space for the swap entry metadata.\n>\n> The zero-filled (or same-filled) swap entry case is an even more\n> egregious example :) It really shouldn't be a state in any backend -\n> it should be a completely independent backend.\n>\n> The only design that makes sense is to store metadata in the top layer\n> as well. It's what I'm doing for my virtual swap patch series, but if\n> we're pursuing this opt-in swapfile direction we are going to\n> duplicate metadata :)\n\nIt's already doing that, storing metadata at the top layer, only a\nreverse mapping in the lower layer.\n\nSo none of these issues are still there. Don't worry, I do remember\nthat conversation and kept that in mind :)\n\n> > And if you consider these ops are too complex to set up and maintain, we\n> > can then only allow one ghost / virtual file, make it infinitely large,\n> > and be the default one and top tier, then it achieves the identical thing\n> > to virtual swap space, but with much fewer LOC changed and being runtime\n> > optional.\n>\n> 2. I think the \"fewer LOC changed\" claim here is misleading ;)\n>\n> A lot of the behaviors that is required in a virtual swap setup is\n> missing from this patch series. You are essentially just implementing\n> a swapfile with a dynamic allocator. You still need a bunch more logic\n> to support a proper multi-tier virtual swap setup - just on top of my\n> mind:\n\nI left that part undone kind of on purpose, since it's only RFC, and\nin hope that there could be collaboration.\n\nAnd the dynamic allocator is only ~200 LOC now. Other parts of this\nseries are not only for virtual swap. For example the unified folio\nalloc for swapin, which gives us 15% performance gain in real\nworkloads, can still get merged and benifit all of us without\ninvolving the virtual swap or memcg part.\n\nAnd meanwhile, with the later patches, we don't have to re-implement\nthe whole infrastructure to have a virtual table. And future plans\nlike data compaction should benifit every layer naturally (same\ninfra).\n\n> a. Charging: virtual swap usage not be charged the same as the\n> physical swap usage, especially when you have a zswap + disk swap\n> setup, powered by virtual swap. For once, I don't believe in sizing\n> virtual swap, but also a latency-sensitive cgroup allowe to use only\n> zswap (backed by virtual swap) is using and competing for resources\n> very differently from a cgroup whose memory is incompressible and only\n> allowed to use disk swap.\n\nAh, now as you mention it, I see in the beginning of this series I\nadded: \"Swap table P4 is stable and good to merge if we are OK with a\nfew memcg reparent behavior (there is also a solution if we don't)\".\nThe \"other solution\" also fits your different charge idea here. Just\nhave a ci->memcg_table, then each layer can have their own charge\ndesign, and the shadow is still only used for refault check. That\ngives us 10 bytes per slot overhead though, but still lower than\nbefore and stays completely dynamic.\n\nAlso, no duplicated memcg, since the upper layer and lower layer\nshould be charged differently. If they don't, then just let\nci->memcg_table stay NULL.\n\n>\n> b. Backend decision making and efficient backend transfer - as you\n> said, \"folio_realloc_swap\" is yet to be implemented :) And as I\n> mention earlier, we CANNOT determine swap backend before PTE unmap\n\nAnd we are not doing that at all. folio_alloc_swap happens before\nunmap, but realloc happens after that. VSS does the same thing.\n\n> time, because backend suitability is content-dependent. You will have\n> to add extra logic to handle this nuanced swap allocation behavior.\n>\n> c. Virtual swap freeing - it requires more work, as you have to free\n> both the virtual swap entry itself, as well as digging into the\n> physical backend layer.\n>\n> d. Swapoff - now you have to both page tables and virtual swap table.\n\nSwapoff is actually easy here... If it sees a reverse map slot, read\ninto the upper layer. Else goto the old logic. Then it's done. If\nghost swap is the layer with highest priority, then every slot is a\nreverse map slot.\n\n>\n> By the time you implement all of this, I think it will be MORE\n> complex, especially since you want to maintain BOTH the new setup and\n> the old non-virtual swap setup. You'll have to litter the codes with a\n> bunch of ifs (or ifdefs) to check - hey do we have a virtual swapfile?\n> Hey is this a virtual swap slot? Etc. Etc. everywhere, from the PTE\n> infra (zapping, page fault, etc.), to cgroup infra, to physical swap\n> architecture.\n\nIt is using the same infrastructure, which means a lot of things are\nreused and unified. Isn't that a good sign? And again we don't need to\nre-implement the whole infra.\n\nAnd if you need multiple layers then there will be more \"if\"s and\noverhead however you implement it. But with unified infra, each layer\ncan stay optional. And checking \"si->flags & GHOST / VIRTUAL\" really\nshouldn't be costly or trouble some at all, compared to a mandatory\nlayer with layers of Xarray walk.\n\nAnd we can move, maintain the virt part in a separate place.\n\n> Comparing this line of work by itself with the vswap series, which\n> already comes with all of these included, is a bit apples-to-oranges\n> (and especially with the fact that vswap simplifies logic and removes\n> LoCs in a lot of places too, such as in swapoff. The delta LoC is only\n> 300-400 IIRC?).\n\nOne thing I want to highlight here is that the old swapoff really\nshouldn't just die. That gives us no chance to clear up the swap cache\nat all (vss holding swap data in RAM is also just swap cache). Pages\nstill in swap cache means minor page faults will still trigger. If the\nworkload is opaque but we knows a high load of traffic is coming and\nwants to get rid of any performance bottleneck by reading all folios\ninto the right place, swapoff gives the guarantee that no anon fault\nwill be ever triggered, that happens a lot in multiple tenant cloud\nenvironments, and these workload are opaque so madvise doesn't apply.\n\n> > The size of the swapfile (si->max) is now just a number, which could be\n> > changeable at runtime if we have a proper idea how to expose that and\n> > might need some audit of a few remaining users. But right now, we can\n> > already easily have a huge swap device with no overhead, for example:\n> >\n> > free -m\n> >                total        used        free      shared  buff/cache   available\n> > Mem:            1465         250         927           1         356        1215\n> > Swap:       15269887           0    15269887\n> >\n>\n> 3. I don't think we should expose virtual swap state to users (in this\n> case, in the swapfile summary view i.e in free). It is just confusing,\n> as it poorly reflects the physical state (be it compressed memory\n> footprint, or actual disk usage). We obviously should expose a bunch\n> of sysfs debug counters for troubleshootings, but for average users,\n> it should be all transparent.\n\nUsing sysfs can also be a choice, that's really just a demonstration\ninterface. But I do think it's worse if the user has no idea what is\nactually going on.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The point is not that it's hard to do. That's the whole sale pitch of vswap - once you have it all the use case is neatly facilitated ;) I'm just pointing out that \"minimal LoC\" is not exactly fair here, as we still have (in my estimate) quite a sizable amount of work. I somewhat agree with Johannes that the problem is quite academic in nature here, but I will think more about it. I think the users should know that virtual swap is enabled or not, and some diagnostics stats - allocated, used, rejected/failure etc. But from users perspective, the other traditional swapfile states don't seem that useful, and might give users misconceptions. When you see swapfile stats, you know that you are occupying a limited physical resource, and how much of it is left.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "merged"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 7:35PM Kairui Song <ryncsn@gmail.com> wrote:\n>\n> On Tue, Feb 24, 2026 at 2:22AM Nhat Pham <nphamcs@gmail.com> wrote:\n> >\n> > On Thu, Feb 19, 2026 at 3:42PM Kairui Song via B4 Relay\n> > <devnull+kasong.tencent.com@kernel.org> wrote:\n> > > Huge thanks to Chris Li for the layered swap table and ghost swapfile\n> > > idea, without whom the work here can't be archived. Also, thanks to Nhat\n> > > for pushing and suggesting using an Xarray for the swapfile [11] for\n> > > dynamic size. I was originally planning to use a dynamic cluster\n> > > array, which requires a bit more adaptation, cleanup, and convention\n> > > changes. But during the discussion there, I got the inspiration that\n> > > Xarray can be used as the intermediate step, making this approach\n> > > doable with minimal changes. Just keep using it in the future, it\n> > > might not hurt too, as Xarray is only limited to ghost / virtual\n> > > files, so plain swaps won't have any extra overhead for lookup or high\n> > > risk of swapout allocation failure.\n> >\n> > Thanks for your effort. Dynamic swap space is a very important\n> > consideration anyone deploying compressed swapping backend on large\n> > memory systems in general. And yeah, I think using a radix tree/xarray\n> > is easiest out-of-the-box solution for this - thanks for citing me :P\n>\n> Thanks for the discussion :)\n>\n> >\n> > I still have some confusion and concerns though. Johannes already made\n> > some good points - I'll just add some thoughts from my point of view,\n> > having gone back and forth with virtual swap designs:\n> >\n> > 1. At which layer should the metadata (swap count, swap cgroup, etc.) live?\n> >\n> > I remember that in your LSFMMBPF presentation (time flies), your\n> > proposal was to store a redirection entry in the top layer, and keep\n> > all the metadata at the bottom (i.e backend) layer? This has problems\n> > - for once, you might not know suitable backend at swap allocation\n> > time, but only at writeout time. For e.g, in certain zswap setups, we\n> > reject the incompressible page and cycle it back to the active LRU, so\n> > we have no space in zswap layer to store swap entry metadata (note\n> > that at this point the swap entry cannot be freed, because we have\n> > already unmapped the page from the PTEs (and would require a page\n> > table walk to undo this a la swapoff). Similarly, when we\n> > exclusive-load a page from zswap, we invalidate the zswap metadata\n> > struct, so we will no longer have a space for the swap entry metadata.\n> >\n> > The zero-filled (or same-filled) swap entry case is an even more\n> > egregious example :) It really shouldn't be a state in any backend -\n> > it should be a completely independent backend.\n> >\n> > The only design that makes sense is to store metadata in the top layer\n> > as well. It's what I'm doing for my virtual swap patch series, but if\n> > we're pursuing this opt-in swapfile direction we are going to\n> > duplicate metadata :)\n>\n> It's already doing that, storing metadata at the top layer, only a\n> reverse mapping in the lower layer.\n>\n> So none of these issues are still there. Don't worry, I do remember\n> that conversation and kept that in mind :)\n>\n> > > And if you consider these ops are too complex to set up and maintain, we\n> > > can then only allow one ghost / virtual file, make it infinitely large,\n> > > and be the default one and top tier, then it achieves the identical thing\n> > > to virtual swap space, but with much fewer LOC changed and being runtime\n> > > optional.\n> >\n> > 2. I think the \"fewer LOC changed\" claim here is misleading ;)\n> >\n> > A lot of the behaviors that is required in a virtual swap setup is\n> > missing from this patch series. You are essentially just implementing\n> > a swapfile with a dynamic allocator. You still need a bunch more logic\n> > to support a proper multi-tier virtual swap setup - just on top of my\n> > mind:\n>\n> I left that part undone kind of on purpose, since it's only RFC, and\n> in hope that there could be collaboration.\n>\n> And the dynamic allocator is only ~200 LOC now. Other parts of this\n> series are not only for virtual swap. For example the unified folio\n> alloc for swapin, which gives us 15% performance gain in real\n> workloads, can still get merged and benifit all of us without\n> involving the virtual swap or memcg part.\n>\n> And meanwhile, with the later patches, we don't have to re-implement\n> the whole infrastructure to have a virtual table. And future plans\n> like data compaction should benifit every layer naturally (same\n> infra).\n>\n> > a. Charging: virtual swap usage not be charged the same as the\n> > physical swap usage, especially when you have a zswap + disk swap\n> > setup, powered by virtual swap. For once, I don't believe in sizing\n> > virtual swap, but also a latency-sensitive cgroup allowe to use only\n> > zswap (backed by virtual swap) is using and competing for resources\n> > very differently from a cgroup whose memory is incompressible and only\n> > allowed to use disk swap.\n>\n> Ah, now as you mention it, I see in the beginning of this series I\n> added: \"Swap table P4 is stable and good to merge if we are OK with a\n> few memcg reparent behavior (there is also a solution if we don't)\".\n> The \"other solution\" also fits your different charge idea here. Just\n> have a ci->memcg_table, then each layer can have their own charge\n> design, and the shadow is still only used for refault check. That\n> gives us 10 bytes per slot overhead though, but still lower than\n> before and stays completely dynamic.\n>\n> Also, no duplicated memcg, since the upper layer and lower layer\n> should be charged differently. If they don't, then just let\n> ci->memcg_table stay NULL.\n>\n> >\n> > b. Backend decision making and efficient backend transfer - as you\n> > said, \"folio_realloc_swap\" is yet to be implemented :) And as I\n> > mention earlier, we CANNOT determine swap backend before PTE unmap\n>\n> And we are not doing that at all. folio_alloc_swap happens before\n> unmap, but realloc happens after that. VSS does the same thing.\n>\n> > time, because backend suitability is content-dependent. You will have\n> > to add extra logic to handle this nuanced swap allocation behavior.\n> >\n> > c. Virtual swap freeing - it requires more work, as you have to free\n> > both the virtual swap entry itself, as well as digging into the\n> > physical backend layer.\n> >\n> > d. Swapoff - now you have to both page tables and virtual swap table.\n>\n> Swapoff is actually easy here... If it sees a reverse map slot, read\n> into the upper layer. Else goto the old logic. Then it's done. If\n> ghost swap is the layer with highest priority, then every slot is a\n> reverse map slot.\n>\n> >\n> > By the time you implement all of this, I think it will be MORE\n> > complex, especially since you want to maintain BOTH the new setup and\n> > the old non-virtual swap setup. You'll have to litter the codes with a\n> > bunch of ifs (or ifdefs) to check - hey do we have a virtual swapfile?\n> > Hey is this a virtual swap slot? Etc. Etc. everywhere, from the PTE\n> > infra (zapping, page fault, etc.), to cgroup infra, to physical swap\n> > architecture.\n>\n> It is using the same infrastructure, which means a lot of things are\n> reused and unified. Isn't that a good sign? And again we don't need to\n> re-implement the whole infra.\n>\n> And if you need multiple layers then there will be more \"if\"s and\n> overhead however you implement it. But with unified infra, each layer\n> can stay optional. And checking \"si->flags & GHOST / VIRTUAL\" really\n> shouldn't be costly or trouble some at all, compared to a mandatory\n> layer with layers of Xarray walk.\n>\n> And we can move, maintain the virt part in a separate place.\n\nThe point is not that it's hard to do. That's the whole sale pitch of\nvswap - once you have it all the use case is neatly facilitated ;)\n\nI'm just pointing out that \"minimal LoC\" is not exactly fair here, as\nwe still have (in my estimate) quite a sizable amount of work.\n\n>\n> > Comparing this line of work by itself with the vswap series, which\n> > already comes with all of these included, is a bit apples-to-oranges\n> > (and especially with the fact that vswap simplifies logic and removes\n> > LoCs in a lot of places too, such as in swapoff. The delta LoC is only\n> > 300-400 IIRC?).\n>\n> One thing I want to highlight here is that the old swapoff really\n> shouldn't just die. That gives us no chance to clear up the swap cache\n> at all (vss holding swap data in RAM is also just swap cache). Pages\n> still in swap cache means minor page faults will still trigger. If the\n> workload is opaque but we knows a high load of traffic is coming and\n> wants to get rid of any performance bottleneck by reading all folios\n> into the right place, swapoff gives the guarantee that no anon fault\n> will be ever triggered, that happens a lot in multiple tenant cloud\n> environments, and these workload are opaque so madvise doesn't apply.\n\nI somewhat agree with Johannes that the problem is quite academic in\nnature here, but I will think more about it.\n\n>\n> > > The size of the swapfile (si->max) is now just a number, which could be\n> > > changeable at runtime if we have a proper idea how to expose that and\n> > > might need some audit of a few remaining users. But right now, we can\n> > > already easily have a huge swap device with no overhead, for example:\n> > >\n> > > free -m\n> > >                total        used        free      shared  buff/cache   available\n> > > Mem:            1465         250         927           1         356        1215\n> > > Swap:       15269887           0    15269887\n> > >\n> >\n> > 3. I don't think we should expose virtual swap state to users (in this\n> > case, in the swapfile summary view i.e in free). It is just confusing,\n> > as it poorly reflects the physical state (be it compressed memory\n> > footprint, or actual disk usage). We obviously should expose a bunch\n> > of sysfs debug counters for troubleshootings, but for average users,\n> > it should be all transparent.\n>\n> Using sysfs can also be a choice, that's really just a demonstration\n> interface. But I do think it's worse if the user has no idea what is\n> actually going on.\n\nI think the users should know that virtual swap is enabled or not, and\nsome diagnostics stats - allocated, used, rejected/failure etc.\n\nBut from users perspective, the other traditional swapfile states\ndon't seem that useful, and might give users misconceptions. When you\nsee swapfile stats, you know that you are occupying a limited physical\nresource, and how much of it is left. I don't think there's even a\ngood reason to statically size virtual swap space - it's just a\nfacility to enable use cases, not an actual resource in the same way\nas memory, or disk drive, and is dynamic (on-demand) in nature.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Rik van Riel",
      "primary_email": "riel@surriel.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Shakeel Butt",
      "primary_email": "shakeel.butt@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "aZzWNM06gNYKDoQW@linux.dev",
          "url": "https://lore.kernel.org/all/aZzWNM06gNYKDoQW@linux.dev/",
          "date": "2026-02-23T22:36:12Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "The reason for using macros in those two cases is that they need to modify the flags variable passed by the caller, just like spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same convention here. If we used normal inline functions instead, we would need to pass a pointer to flags, which would change the call sites and diverge from the existing *_irqsave() locking pattern. There is also a difference between zone_lock_irqsave() and zone_trylock_irqsave() implementations: the former is implemented as a do { } while (0) macro since it does not return a value, while the latter uses a GCC extension in order to return the trylock result. This matches spin_lock_* convention as well.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > Add thin wrappers around zone lock acquire/release operations. This\n> > prepares the code for future tracepoint instrumentation without\n> > modifying individual call sites.\n> > \n> > Centralizing zone lock operations behind wrappers allows future\n> > instrumentation or debugging hooks to be added without touching\n> > all users.\n> > \n> > No functional change intended. The wrappers are introduced in\n> > preparation for subsequent patches and are not yet used.\n> > \n> > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > ---\n> >  MAINTAINERS               |  1 +\n> >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> >  2 files changed, 39 insertions(+)\n> >  create mode 100644 include/linux/zone_lock.h\n> > \n> > diff --git a/MAINTAINERS b/MAINTAINERS\n> > index b4088f7290be..680c9ae02d7e 100644\n> > --- a/MAINTAINERS\n> > +++ b/MAINTAINERS\n> > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> >  F:\tinclude/linux/ptdump.h\n> >  F:\tinclude/linux/vmpressure.h\n> >  F:\tinclude/linux/vmstat.h\n> > +F:\tinclude/linux/zone_lock.h\n> >  F:\tkernel/fork.c\n> >  F:\tmm/Kconfig\n> >  F:\tmm/debug.c\n> > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > new file mode 100644\n> > index 000000000000..c531e26280e6\n> > --- /dev/null\n> > +++ b/include/linux/zone_lock.h\n> > @@ -0,0 +1,38 @@\n> > +/* SPDX-License-Identifier: GPL-2.0 */\n> > +#ifndef _LINUX_ZONE_LOCK_H\n> > +#define _LINUX_ZONE_LOCK_H\n> > +\n> > +#include <linux/mmzone.h>\n> > +#include <linux/spinlock.h>\n> > +\n> > +static inline void zone_lock_init(struct zone *zone)\n> > +{\n> > +\tspin_lock_init(&zone->lock);\n> > +}\n> > +\n> > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > +do {\t\t\t\t\t\t\t\t\\\n> > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > +} while (0)\n> > +\n> > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > +({\t\t\t\t\t\t\t\t\\\n> > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > +})\n> \n> Any reason you used macros for above two and inlined functions for remaining?\n>\n\nThe reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.\n\n> > +\n> > +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n> > +{\n> > +\tspin_unlock_irqrestore(&zone->lock, flags);\n> > +}\n> > +\n> > +static inline void zone_lock_irq(struct zone *zone)\n> > +{\n> > +\tspin_lock_irq(&zone->lock);\n> > +}\n> > +\n> > +static inline void zone_unlock_irq(struct zone *zone)\n> > +{\n> > +\tspin_unlock_irq(&zone->lock);\n> > +}\n> > +\n> > +#endif /* _LINUX_ZONE_LOCK_H */\n> > -- \n> > 2.47.3\n> > \n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt (author)",
              "summary": "Cool, thanks for the explanation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 03:18:04PM +0000, Dmitry Ilvokhin wrote:\n> On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> > On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > > Add thin wrappers around zone lock acquire/release operations. This\n> > > prepares the code for future tracepoint instrumentation without\n> > > modifying individual call sites.\n> > > \n> > > Centralizing zone lock operations behind wrappers allows future\n> > > instrumentation or debugging hooks to be added without touching\n> > > all users.\n> > > \n> > > No functional change intended. The wrappers are introduced in\n> > > preparation for subsequent patches and are not yet used.\n> > > \n> > > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > > ---\n> > >  MAINTAINERS               |  1 +\n> > >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> > >  2 files changed, 39 insertions(+)\n> > >  create mode 100644 include/linux/zone_lock.h\n> > > \n> > > diff --git a/MAINTAINERS b/MAINTAINERS\n> > > index b4088f7290be..680c9ae02d7e 100644\n> > > --- a/MAINTAINERS\n> > > +++ b/MAINTAINERS\n> > > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> > >  F:\tinclude/linux/ptdump.h\n> > >  F:\tinclude/linux/vmpressure.h\n> > >  F:\tinclude/linux/vmstat.h\n> > > +F:\tinclude/linux/zone_lock.h\n> > >  F:\tkernel/fork.c\n> > >  F:\tmm/Kconfig\n> > >  F:\tmm/debug.c\n> > > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > > new file mode 100644\n> > > index 000000000000..c531e26280e6\n> > > --- /dev/null\n> > > +++ b/include/linux/zone_lock.h\n> > > @@ -0,0 +1,38 @@\n> > > +/* SPDX-License-Identifier: GPL-2.0 */\n> > > +#ifndef _LINUX_ZONE_LOCK_H\n> > > +#define _LINUX_ZONE_LOCK_H\n> > > +\n> > > +#include <linux/mmzone.h>\n> > > +#include <linux/spinlock.h>\n> > > +\n> > > +static inline void zone_lock_init(struct zone *zone)\n> > > +{\n> > > +\tspin_lock_init(&zone->lock);\n> > > +}\n> > > +\n> > > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > > +do {\t\t\t\t\t\t\t\t\\\n> > > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +} while (0)\n> > > +\n> > > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > > +({\t\t\t\t\t\t\t\t\\\n> > > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +})\n> > \n> > Any reason you used macros for above two and inlined functions for remaining?\n> >\n> \n> The reason for using macros in those two cases is that they need to\n> modify the flags variable passed by the caller, just like\n> spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\n> convention here.\n> \n> If we used normal inline functions instead, we would need to pass a\n> pointer to flags, which would change the call sites and diverge from the\n> existing *_irqsave() locking pattern.\n> \n> There is also a difference between zone_lock_irqsave() and\n> zone_trylock_irqsave() implementations: the former is implemented as a\n> do { } while (0) macro since it does not return a value, while the\n> latter uses a GCC extension in order to return the trylock result. This\n> matches spin_lock_* convention as well.\n> \n\nCool, thanks for the explanation.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v7 3/3] mm: vmscan: add PIDs to vmscan tracepoints",
          "message_id": "aZys_tvQinYNNpOk@linux.dev",
          "url": "https://lore.kernel.org/all/aZys_tvQinYNNpOk@linux.dev/",
          "date": "2026-02-23T19:42:53Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH v2 0/5] mm/swap, memcg: Introduce swap tiers for cgroup based swap control",
          "message_id": "aZvX0HZy1PDylL8A@linux.dev",
          "url": "https://lore.kernel.org/all/aZvX0HZy1PDylL8A@linux.dev/",
          "date": "2026-02-23T05:56:24Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 2/4] mm: convert zone lock users to wrappers",
          "message_id": "aZzicQS19G_WeL-J@linux.dev",
          "url": "https://lore.kernel.org/all/aZzicQS19G_WeL-J@linux.dev/",
          "date": "2026-02-23T23:28:23Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH] mm/slab: initialize slab->stride early to avoid memory ordering issues",
          "message_id": "aZy3O2qcULFDoDU1@linux.dev",
          "url": "https://lore.kernel.org/all/aZy3O2qcULFDoDU1@linux.dev/",
          "date": "2026-02-23T20:23:58Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v7 2/3] mm: vmscan: add cgroup IDs to vmscan tracepoints",
          "message_id": "aZyrkRDJpHh8ZnCW@linux.dev",
          "url": "https://lore.kernel.org/all/aZyrkRDJpHh8ZnCW@linux.dev/",
          "date": "2026-02-23T19:34:22Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v7 1/3] tracing: Add __event_in_*irq() helpers",
          "message_id": "aZyrZ5c4dk_eshGM@linux.dev",
          "url": "https://lore.kernel.org/all/aZyrZ5c4dk_eshGM@linux.dev/",
          "date": "2026-02-23T19:33:32Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Usama Arif",
      "primary_email": "usama.arif@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v1 01/11] relay: zero page->private when freeing pages",
          "message_id": "20260223144507.3065618-1-usama.arif@linux.dev",
          "url": "https://lore.kernel.org/all/20260223144507.3065618-1-usama.arif@linux.dev/",
          "date": "2026-02-23T14:45:28Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    }
  ]
}