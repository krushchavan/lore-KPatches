{
  "date": "2026-02-23",
  "report_file": "2026-02-23_ollama_llama3.1-8b.html",
  "status": "in_progress",
  "llm_backends": [
    [
      "ollama",
      "llama3.1:8b"
    ]
  ],
  "generation_time_seconds": 0.0,
  "developer_reports": [
    {
      "name": "Alexandre Ghiti",
      "primary_email": "alexghiti@rivosinc.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Boris Burkov",
      "primary_email": "boris@bur.io",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Dmitry Ilvokhin",
      "primary_email": "d@ilvokhin.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 4/4] mm: add tracepoints for zone lock",
          "message_id": "1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch series introduces a new abstraction for zone lock operations, adding thin wrappers around acquire and release operations. The goal is to prepare the code for future tracepoint instrumentation without modifying individual call sites. This allows for easier addition of debugging hooks or instrumentation in the future. The first patch introduces the zone lock wrappers, while subsequent patches add tracepoints for zone lock events.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about direct zone lock acquire/release operations being replaced with the newly introduced wrappers. The author confirms that the changes are purely mechanical substitutions and no functional change is intended, but notes that the compaction path will be handled separately in a following patch due to additional non-trivial modifications.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about the lack of visibility into zone lock behavior, specifically the difficulty in distinguishing between short bursts of contention and pathological long hold times. They explain that their patch series adds dedicated tracepoint instrumentation to enable detailed holder/waiter analysis and lock hold time measurements without affecting the fast path when tracing is disabled. The author also mentions a minor restructuring required for compaction changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "cover.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the compaction code needing to handle both zone locks and raw spinlocks, explained that they introduced a new struct compact_lock to abstract the underlying lock type, and confirmed no functional change is intended.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical issue",
                "explained their solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the need to drop the per-vswap spinlock before calling try_to_unmap() in the swapoff path, acknowledged that this is necessary for correctness and agreed to restructure the code in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to improve flow, specifically introducing zone lock wrappers and tracepoints together before mechanically converting users to the wrappers",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "06b2a2b6-d5c8-4522-8e22-10616f887846@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing zone lock helper functions and making direct calls in this function, citing parity with compact helpers as a reason\n\nReviewer noted that the function should not return a value and suggested replacing it with an if-else statement.\n\nReviewer Cheatham suggested moving zone lock wrapper changes that don't use compact_* functions to a later patch, arguing they are not relevant to the current patch and fit better in the final patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "suggested changes",
                "requested changes",
                "requested_reordering"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "74fc1f28-b77e-4b9c-b208-51babae9d18e@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to squash patches (1) and (2), stating it's just a matter of personal taste, but also mentioned that the series ordering is good as is",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "personal opinion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "aZjg6PWn_xhZV7Nb@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged a suggestion from reviewer Cheatham about reordering patches in the series, explained that they intentionally kept instrumentation separate from refactoring, and stated their preference to keep the current order.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged suggestion",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "aZyEctoThn0anlz8@shell.ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the patch's ordering of tracepoints is not as important as he initially thought, and expressed gratitude towards the submitter.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "gratitude",
                "revised opinion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "c13e340e-74f5-4a66-8fa0-d307ee5ea0eb@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "aZzicQS19G_WeL-J@linux.dev",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 3/4] mm: convert compaction to zone lock wrappers",
          "message_id": "3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch introduces zone lock wrappers, which are thin layers around the existing zone lock acquire and release operations. The goal is to prepare the code for future tracepoint instrumentation without modifying individual call sites. Centralizing zone lock operations behind these wrappers allows for easier addition of debugging hooks or instrumentation in the future. No functional changes are intended by this patch, as it only introduces the wrappers in preparation for subsequent patches.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about replacing direct zone lock acquire/release operations with the newly introduced wrappers in the compaction path, stating that the changes are purely mechanical substitutions and no functional change is intended.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no functional change",
                "mechanical substitutions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about the compaction changes required to use zone lock wrappers, specifically that compact_lock_irqsave() needs to be abstracted away from raw spinlock_t. The author chose a small tagged struct to handle both zone and LRU locks uniformly and asks for feedback on alternative approaches.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question",
                "request for feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "cover.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the compaction code needing to handle both zone locks and raw spinlocks, introduced a new struct compact_lock to abstract the underlying lock type, and explained that no functional change is intended.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no functional change",
                "new abstraction"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock wrappers, explaining that the implementation follows the mmap_lock tracepoint pattern and that the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to introduce zone lock wrappers and tracepoints together, before mechanically converting users to the wrappers, to improve understanding and reduce complexity.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "06b2a2b6-d5c8-4522-8e22-10616f887846@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason\n\nreviewer pointed out that the function should not return a value and suggested replacing it with an if-else statement\n\nReviewer Cheatham, Benjamin suggested moving zone lock wrapper changes that don't use compact_* functions to a later patch, arguing they are not relevant to the current patch and fit better in the final patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested change",
                "suggested reordering"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "74fc1f28-b77e-4b9c-b208-51babae9d18e@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to convert compaction to zone lock wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2) together instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "nit"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "aZjg6PWn_xhZV7Nb@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a suggestion from Benjamin Cheatham to reorder the patch series to separate instrumentation changes from refactoring. The author explains that they intentionally structured the series to keep refactoring and instrumentation changes separate, allowing for easier bisection and review of behavioral changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying explanation",
                "open to discussion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "aZyEctoThn0anlz8@shell.ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer Cheatham, Benjamin expressed no significant technical concerns about the patch, but instead mentioned that he had a different preference for the reading order of the series.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no significant technical concerns"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "c13e340e-74f5-4a66-8fa0-d307ee5ea0eb@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "aZzicQS19G_WeL-J@linux.dev",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/4] mm: zone lock tracepoint instrumentation",
          "message_id": "cover.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/cover.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations, allowing for future tracepoint instrumentation without modifying individual call sites. The wrappers are added to prepare the code for subsequent patches that will add tracing functionality. This change is a preparatory step and does not introduce any functional changes at this time.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about replacing direct zone lock acquire/release operations with the newly introduced wrappers. The author confirmed that the changes are purely mechanical substitutions and no functional change is intended, but noted that the compaction path will be handled separately in a following patch due to additional non-trivial modifications.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about the need for additional instrumentation to measure lock hold times and distinguish between short bursts of contention and pathological long hold times. They explain that generic lock tracepoints do not provide sufficient visibility into lock hold times, particularly on the release side, making it difficult to identify long lock holders and correlate them with waiters. The author confirms that their patch series adds dedicated tracepoint instrumentation to zone lock, following the existing mmap_lock tracing model, and that the goal is to enable detailed holder/waiter analysis and lock hold time measurements without affecting the fast path when tracing is disabled.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged need for additional instrumentation",
                "explained limitations of generic lock tracepoints"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "cover.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the compaction code needing to handle both zone locks and raw spinlocks, introduced an enum and struct to abstract the underlying lock type, and confirmed that no functional change is intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "confirmed no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of tracepoint instrumentation on the fast path, explaining that the implementation follows the mmap_lock tracepoint pattern and that helpers compile to empty inline stubs when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to improve flow, recommending that zone lock wrappers and tracepoints be introduced together in a single patch before converting users to the new wrappers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "06b2a2b6-d5c8-4522-8e22-10616f887846@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason\n\nReviewer pointed out that the function should not return a value and suggested replacing it with an if-else statement.\n\nReviewer Cheatham suggested moving wrapper changes not using compact_* to the last patch, arguing they are not essential to the current patch and fit better in the final one.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested change",
                "minor suggestion"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "74fc1f28-b77e-4b9c-b208-51babae9d18e@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to squash patches (1) and (2), stating that it's just a matter of personal taste, but suggested an alternative ordering where the wrappers and their use are combined in a single patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "personal opinion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "aZjg6PWn_xhZV7Nb@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author addressed a concern about the patch series structure and refactoring order, explaining that he intentionally split the conversion into two patches to keep instrumentation isolated from intermediate refactoring states.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "aZyEctoThn0anlz8@shell.ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer Cheatham expressed uncertainty about prioritizing zone lock wrappers over his suggested reading order for the patch series.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "lack of clear objection"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "c13e340e-74f5-4a66-8fa0-d307ee5ea0eb@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "aZzicQS19G_WeL-J@linux.dev",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 2/4] mm: convert zone lock users to wrappers",
          "message_id": "7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch converts zone lock users to wrappers, which are thin layers around the original zone lock acquire and release operations. The goal is to prepare for future tracepoint instrumentation without modifying individual call sites. Centralizing zone lock operations behind these wrappers allows for easier addition of debugging hooks or instrumentation in the future.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about direct zone lock acquire/release operations being replaced with the newly introduced wrappers in the patch. The author confirms that the changes are purely mechanical substitutions, no functional change intended, and locking semantics and ordering remain unchanged.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about the need for deeper analysis of zone lock contention, specifically identifying long lock holders and correlating them with waiters. They explain that generic lock tracepoints are insufficient and propose adding dedicated tracepoint instrumentation to the zone lock, following the existing mmap_lock tracing model. The author mentions that they have observed noticeable zone lock contention on production workloads at Meta and want to enable detailed holder/waiter analysis without affecting the fast path when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging a concern",
                "proposing an alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "cover.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern that the zone lock wrappers would break compact_lock_irqsave() by introducing a new struct, compact_lock, to abstract the underlying lock type and dispatch to the appropriate lock/unlock helper.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "introduced a new abstraction",
                "no functional change intended"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged a potential issue with the swapoff path needing to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series of patches to improve flow, proposing a sequence where zone lock wrappers are introduced and tracepoints added together before mechanically converting users to the wrappers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "06b2a2b6-d5c8-4522-8e22-10616f887846@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing zone lock helper functions and making direct calls in this function, citing parity with compact helpers as a reason.\n\nreviewer pointed out that the function should not return a value and suggested replacing it with an if-else statement\n\nReviewer Cheatham suggested moving wrapper changes not using compact_* to a later patch, arguing they are not directly related to the current patch's additions and would fit better in the final patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested change",
                "opinion on code organization"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "74fc1f28-b77e-4b9c-b208-51babae9d18e@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to convert zone lock users to wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2) together instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "personal preference"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "aZjg6PWn_xhZV7Nb@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about the order of patches in the series, specifically that the instrumentation change should be kept separate from the refactoring. The author explains that they intentionally structured the series to keep the behavior-preserving changes separate and that reordering as suggested would mix instrumentation with intermediate refactoring states.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "neutral explanation",
                "request for discussion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "aZyEctoThn0anlz8@shell.ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer Cheatham expressed no technical concerns, but mentioned he wasn't sure if the patch's priority aligned with his suggested reading order for the series.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no technical issues raised"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "c13e340e-74f5-4a66-8fa0-d307ee5ea0eb@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "aZzicQS19G_WeL-J@linux.dev",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations in the Linux kernel. The goal is to prepare for future tracepoint instrumentation without modifying individual call sites. Centralizing zone lock operations behind these wrappers allows for easier addition of debugging hooks or other instrumentation without affecting all users. No functional changes are intended, as this patch is a preparatory step for subsequent patches.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about direct zone lock acquire/release operations being replaced with the newly introduced wrappers. The author confirms that the changes are purely mechanical substitutions, no functional change intended, and locking semantics and ordering remain unchanged.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "no functional change",
                "locking semantics and ordering remain unchanged"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about the lack of visibility into zone lock contention, particularly in production workloads at Meta. They explain that generic tracepoints are insufficient and propose adding dedicated tracepoint instrumentation to the zone lock, following the existing mmap_lock tracing model. The author also mentions minor restructuring required for compaction changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging a concern",
                "proposing an alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "cover.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the compaction code needing to handle both zone locks and raw spinlocks, introduced a new struct compact_lock to abstract the underlying lock type, and explained that no functional change is intended.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "introduced a new abstraction",
                "explained no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock acquire/release operations, explaining that they followed the mmap_lock tracepoint pattern and ensured the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to introduce zone lock wrappers and tracepoints together, before mechanically converting users to the wrappers",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "06b2a2b6-d5c8-4522-8e22-10616f887846@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason\n\nReviewer noted that the zone_lock_irqsave() macro should not return a value and suggested replacing it with an if-else statement for better clarity.\n\nReviewer Cheatham suggested moving the zone lock wrapper changes, which are not yet used, to a later patch, arguing they don't directly relate to the functionality being added in this patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested change"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "74fc1f28-b77e-4b9c-b208-51babae9d18e@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to introduce zone lock wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2), but acknowledged the series ordering is good.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "nit"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "aZjg6PWn_xhZV7Nb@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author addressed a concern about the patch series structure, explaining that he intentionally split behavior-preserving refactoring from instrumentation changes to keep them separate and make review easier.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "aZyEctoThn0anlz8@shell.ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that they were unsure if the zone lock wrappers were more important to the submitter than their own suggested reading order for the patch series",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "lack of clear objection"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "c13e340e-74f5-4a66-8fa0-d307ee5ea0eb@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "aZzicQS19G_WeL-J@linux.dev",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 0/4] mm: zone lock tracepoint instrumentation",
          "message_id": "aZyEctoThn0anlz8@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aZyEctoThn0anlz8@shell.ilvokhin.com/",
          "date": "2026-02-23T16:52:49Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about direct zone lock acquire/release operations being replaced with the newly introduced wrappers. They confirmed that the changes are purely mechanical substitutions and no functional change is intended, but noted that the compaction path will be handled separately in a following patch due to additional non-trivial modifications.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "clarified"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged that the existing generic lock contention tracepoints are insufficient for diagnosing performance issues and explained that their patch series adds dedicated tracepoint instrumentation to provide detailed holder/waiter analysis and lock hold time measurements.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a limitation",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "cover.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern that the zone lock wrappers would break compact_lock_irqsave() by introducing struct compact_lock to abstract the underlying lock type, which carries a lock type enum and a union holding either a zone pointer or a raw spinlock_t pointer. The structure dispatches to the appropriate lock/unlock helper based on the lock type.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "clarification or explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of tracepoint instrumentation on the fast path, explaining that when CONFIG_TRACING is disabled, the helpers compile to empty inline stubs and do not affect the fast path.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to improve flow, recommending that zone lock wrappers and tracepoints be introduced together in a single patch before mechanically converting users to the wrappers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "06b2a2b6-d5c8-4522-8e22-10616f887846@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason.\n\nReviewer noted that the function should not return a value and suggested replacing it with an if-else statement for clarity\n\nReviewer suggested moving wrapper changes that don't use compact_* to the last patch, arguing they aren't relevant to this patch and fit better there.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested change"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "74fc1f28-b77e-4b9c-b208-51babae9d18e@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggested change to squash patches (1) and (2), stating it's just a matter of personal taste, but also mentioned that the current series ordering is good as is.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "personal preference"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "aZjg6PWn_xhZV7Nb@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged a suggestion from reviewer Cheatham about reordering the patch series, explained that they intentionally kept refactoring and instrumentation separate, and stated that reordering would mix instrumentation with intermediate refactoring states.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged suggestion",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "aZyEctoThn0anlz8@shell.ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer Cheatham expressed no technical concerns, but instead mentioned a preference for a different reading order for the patch series.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "c13e340e-74f5-4a66-8fa0-d307ee5ea0eb@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "aZzicQS19G_WeL-J@linux.dev",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed Shakeel Butt's concern about using macros for zone lock operations, explaining that it is necessary to modify the flags variable passed by the caller and maintain consistency with existing locking patterns.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "explained reasoning",
                "provided justification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-24",
              "message_id": "aZ3BLKzhIIZvkbwL@shell.ilvokhin.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-24",
              "message_id": "aZ3EQRZ1XRLsGlzX@linux.dev",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged that the zone lock wrappers introduced in patch 1/4 have little value and agreed to remove them.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledgment",
                "agreement"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, I agree, there is no much value in this wrappers, will remove them,\nthanks!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-24",
              "message_id": "aZ3I0ADTAdCN6UmN@shell.ilvokhin.com",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Gregory Price",
      "primary_email": "gourry@gourry.net",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[RFC PATCH v4 27/27] cxl: add cxl_compression PCI driver",
          "message_id": "20260222084842.1824063-28-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260222084842.1824063-28-gourry@gourry.net/",
          "date": "2026-02-22T08:50:38Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-22",
          "patch_summary": "This patch introduces a new PCI driver called cxl_compression, which is part of a larger series that enables Compressed RAM (CRAM) services. CRAM allows memory to be managed by the buddy allocator but excluded from normal allocations, providing isolation and flexibility for device-specific use cases. The patch adds support for compressed memory nodes, enabling features like migration, mempolicy, demotion, and more. It also includes a QEMU device that can inject high/low interrupts for testing purposes. This series aims to simplify the management of private memory nodes and provide an end-to-end CRAM service.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that the N_MEMORY_PRIVATE state is mutually exclusive with N_MEMORY, and explained that this is intentional to prevent general consumption of memory marked as Specific Purpose or Reserved by device drivers.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY nodes are intended to contain general System RAM. Today, some\ndevice drivers hotplug their memory (marked Specific Purpose or Reserved)\nto get access to mm/ services, but don't intend it for general consumption.\n\nCreate N_MEMORY_PRIVATE for memory nodes whose memory is not intended for\ngeneral consumption. This state is mutually exclusive with N_MEMORY.\n\nAdd the node_private infrastructure for N_MEMORY_PRIVATE nodes:\n\n  - struct node_private: Per-node container stored in NODE_DATA(nid),\n    holding driver callbacks (ops), owner, and refcount.\n\n  - struct node_private_ops: Initial structure with void *reserved\n    placeholder and flags field.  Callbacks will be added by subsequent\n    commits as each consumer is wired up.\n\n  - folio_is_private_node() / page_is_private_node(): check if a\n    folio/page resides on a private node.\n\n  - folio_node_private_ops() / node_private_flags(): retrieve the ops\n    vtable or flags for a folio's node.\n\n  - Registration API: node_private_register()/unregister() for drivers\n    to register callbacks for private nodes. Only one driver callback\n    can be registered per node - attempting to register different ops\n    returns -EBUSY.\n\n  - sysfs attribute exposing N_MEMORY_PRIVATE node state.\n\nZonelist construction changes for private nodes are deferred to a\nsubsequent commit.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 197 ++++++++++++++++++++++++++++++++\n include/linux/mmzone.h       |   4 +\n include/linux/node_private.h | 210 +++++++++++++++++++++++++++++++++++\n include/linux/nodemask.h     |   1 +\n 4 files changed, 412 insertions(+)\n create mode 100644 include/linux/node_private.h\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 00cf4532f121..646dc48a23b5 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -22,6 +22,7 @@\n #include <linux/swap.h>\n #include <linux/slab.h>\n #include <linux/memblock.h>\n+#include <linux/node_private.h>\n \n static const struct bus_type node_subsys = {\n \t.name = \"node\",\n@@ -861,6 +862,198 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n \t\t\t   (void *)&nid, register_mem_block_under_node_hotplug);\n \treturn;\n }\n+\n+static DEFINE_MUTEX(node_private_lock);\n+static bool node_private_initialized;\n+\n+/**\n+ * node_private_register - Register a private node\n+ * @nid: Node identifier\n+ * @np: The node_private structure (driver-allocated, driver-owned)\n+ *\n+ * Register a driver for a private node. Only one driver can register\n+ * per node. If another driver has already registered (with different np),\n+ * -EBUSY is returned. Re-registration with the same np is allowed.\n+ *\n+ * The driver owns the node_private memory and must ensure it remains valid\n+ * until refcount reaches 0 after node_private_unregister().\n+ *\n+ * Returns 0 on success, negative errno on failure.\n+ */\n+int node_private_register(int nid, struct node_private *np)\n+{\n+\tstruct node_private *existing;\n+\tpg_data_t *pgdat;\n+\tint ret = 0;\n+\n+\tif (!np || !node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tif (!node_private_initialized)\n+\t\treturn -ENODEV;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\t/* N_MEMORY_PRIVATE and N_MEMORY are mutually exclusive */\n+\tif (node_state(nid, N_MEMORY)) {\n+\t\tret = -EBUSY;\n+\t\tgoto out;\n+\t}\n+\n+\tpgdat = NODE_DATA(nid);\n+\texisting = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t\t     lockdep_is_held(&node_private_lock));\n+\n+\t/* Only one source my register this node */\n+\tif (existing) {\n+\t\tif (existing != np) {\n+\t\t\tret = -EBUSY;\n+\t\t\tgoto out;\n+\t\t}\n+\t\tgoto out;\n+\t}\n+\n+\trefcount_set(&np->refcount, 1);\n+\tinit_completion(&np->released);\n+\n+\trcu_assign_pointer(pgdat->node_private, np);\n+\tpgdat->private = true;\n+\n+out:\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_register);\n+\n+/**\n+ * node_private_set_ops - Set service callbacks on a registered private node\n+ * @nid: Node identifier\n+ * @ops: Service callbacks and flags (driver-owned, must outlive registration)\n+ *\n+ * Validates flag dependencies and sets the ops on the node's node_private.\n+ * The node must already be registered via node_private_register().\n+ *\n+ * Returns 0 on success, -EINVAL for invalid flag combinations,\n+ * -ENODEV if no node_private is registered on @nid.\n+ */\n+int node_private_set_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!ops)\n+\t\treturn -EINVAL;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse\n+\t\tnp->ops = ops;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_set_ops);\n+\n+/**\n+ * node_private_clear_ops - Clear service callbacks from a private node\n+ * @nid: Node identifier\n+ * @ops: Expected ops pointer (must match current ops)\n+ *\n+ * Clears the ops only if @ops matches the currently registered ops,\n+ * preventing one service from accidentally clearing another's callbacks.\n+ *\n+ * Returns 0 on success, -ENODEV if no node_private is registered,\n+ * -EINVAL if @ops does not match.\n+ */\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse if (np->ops != ops)\n+\t\tret = -EINVAL;\n+\telse\n+\t\tnp->ops = NULL;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_clear_ops);\n+\n+/**\n+ * node_private_unregister - Unregister a private node\n+ * @nid: Node identifier\n+ *\n+ * Unregister the driver from a private node. Only succeeds if all memory\n+ * has been offlined and the node is no longer N_MEMORY_PRIVATE.\n+ * When successful, drops the refcount to 0 indicating the driver can\n+ * free its context.\n+ *\n+ * N_MEMORY_PRIVATE state is cleared by offline_pages() when the last\n+ * memory is offlined, not by this function.\n+ *\n+ * Return: 0 if unregistered, -EBUSY if N_MEMORY_PRIVATE is still set\n+ * (other memory blocks remain on this node).\n+ */\n+int node_private_unregister(int nid)\n+{\n+\tstruct node_private *np;\n+\tpg_data_t *pgdat;\n+\n+\tif (!node_possible(nid))\n+\t\treturn 0;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\tpgdat = NODE_DATA(nid);\n+\tnp = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Only unregister if all memory is offline and N_MEMORY_PRIVATE is\n+\t * cleared. N_MEMORY_PRIVATE is cleared by offline_pages() when the\n+\t * last memory block is offlined.\n+\t */\n+\tif (node_state(nid, N_MEMORY_PRIVATE)) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn -EBUSY;\n+\t}\n+\n+\trcu_assign_pointer(pgdat->node_private, NULL);\n+\tpgdat->private = false;\n+\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\n+\tsynchronize_rcu();\n+\n+\tif (!refcount_dec_and_test(&np->refcount))\n+\t\twait_for_completion(&np->released);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(node_private_unregister);\n+\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n /**\n@@ -959,6 +1152,7 @@ static struct node_attr node_state_attr[] = {\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n \t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\n \t\t\t\t\t   N_GENERIC_INITIATOR),\n@@ -972,6 +1166,7 @@ static struct attribute *node_state_attrs[] = {\n \t&node_state_attr[N_HIGH_MEMORY].attr.attr,\n #endif\n \t&node_state_attr[N_MEMORY].attr.attr,\n+\t&node_state_attr[N_MEMORY_PRIVATE].attr.attr,\n \t&node_state_attr[N_CPU].attr.attr,\n \t&node_state_attr[N_GENERIC_INITIATOR].attr.attr,\n \tNULL\n@@ -1007,5 +1202,7 @@ void __init node_dev_init(void)\n \t\t\tpanic(\"%s() failed to add node: %d\\n\", __func__, ret);\n \t}\n \n+\tnode_private_initialized = true;\n+\n \tregister_memory_blocks_under_nodes();\n }\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex b01cb1e49896..992eb1c5a2c6 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -25,6 +25,8 @@\n #include <linux/zswap.h>\n #include <asm/page.h>\n \n+struct node_private;\n+\n /* Free memory management - zoned buddy allocator.  */\n #ifndef CONFIG_ARCH_FORCE_MAX_ORDER\n #define MAX_PAGE_ORDER 10\n@@ -1514,6 +1516,8 @@ typedef struct pglist_data {\n \tatomic_long_t\t\tvm_stat[NR_VM_NODE_STAT_ITEMS];\n #ifdef CONFIG_NUMA\n \tstruct memory_tier __rcu *memtier;\n+\tstruct node_private __rcu *node_private;\n+\tbool private;\n #endif\n #ifdef CONFIG_MEMORY_FAILURE\n \tstruct memory_failure_stats mf_stats;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nnew file mode 100644\nindex 000000000000..6a70ec39d569\n--- /dev/null\n+++ b/include/linux/node_private.h\n@@ -0,0 +1,210 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_NODE_PRIVATE_H\n+#define _LINUX_NODE_PRIVATE_H\n+\n+#include <linux/completion.h>\n+#include <linux/mm.h>\n+#include <linux/nodemask.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+\n+struct page;\n+struct vm_area_struct;\n+struct vm_fault;\n+\n+/**\n+ * struct node_private_ops - Callbacks for private node services\n+ *\n+ * Services register these callbacks to intercept MM operations that affect\n+ * their private nodes.\n+ *\n+ * Flag bits control which MM subsystems may operate on folios on this node.\n+ *\n+ * The pgdat->node_private pointer is RCU-protected.  Callbacks fall into\n+ * three categories based on their calling context:\n+ *\n+ * Folio-referenced callbacks (RCU released before callback):\n+ *   The caller holds a reference to a folio on the private node, which\n+ *   pins the node's memory online and prevents node_private teardown.\n+ *\n+ * Refcounted callbacks (RCU released before callback):\n+ *   The caller has no folio on the private node (e.g., folios are on a\n+ *   source node being migrated TO this node).  A temporary refcount is\n+ *   taken on node_private under rcu_read_lock to keep the structure (and\n+ *   the service module) alive across the callback.  node_private_unregister\n+ *   waits for all temporary references to drain before returning.\n+ *\n+ * Non-folio callbacks (rcu_read_lock held during callback):\n+ *   No folio reference exists, so rcu_read_lock is held across the\n+ *   callback to prevent node_private from being freed.\n+ *   These callbacks MUST NOT sleep.\n+ *\n+ * @flags: Operation exclusion flags (NP_OPS_* constants).\n+ *\n+ */\n+struct node_private_ops {\n+\tunsigned long flags;\n+};\n+\n+/**\n+ * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n+ *\n+ * This structure is allocated by the driver and passed to node_private_register().\n+ * The driver owns the memory and must ensure it remains valid until after\n+ * node_private_unregister() returns with the reference count dropped to 0.\n+ *\n+ * @owner: Opaque driver identifier\n+ * @refcount: Reference count (1 = registered; temporary refs for non-folio\n+ *\t\tcallbacks that may sleep; 0 = fully released)\n+ * @released: Signaled when refcount drops to 0; unregister waits on this\n+ * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ */\n+struct node_private {\n+\tvoid *owner;\n+\trefcount_t refcount;\n+\tstruct completion released;\n+\tconst struct node_private_ops *ops;\n+};\n+\n+#ifdef CONFIG_NUMA\n+\n+#include <linux/mmzone.h>\n+\n+/**\n+ * folio_is_private_node - Check if folio is on an N_MEMORY_PRIVATE node\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio resides on a private node.\n+ */\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn node_state(folio_nid(folio), N_MEMORY_PRIVATE);\n+}\n+\n+/**\n+ * page_is_private_node - Check if page is on an N_MEMORY_PRIVATE node\n+ * @page: The page to check\n+ *\n+ * Returns true if the page resides on a private node.\n+ */\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\tconst struct node_private_ops *ops;\n+\tstruct node_private *np;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(folio_nid(folio))->node_private);\n+\tops = np ? np->ops : NULL;\n+\trcu_read_unlock();\n+\n+\treturn ops;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\tstruct node_private *np;\n+\tunsigned long flags;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tflags = (np && np->ops) ? np->ops->flags : 0;\n+\trcu_read_unlock();\n+\n+\treturn flags;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn node_private_flags(folio_nid(f)) & flag;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn node_private_flags(nid) & flag;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn node_private_flags(zone_to_nid(z)) & flag;\n+}\n+\n+#else /* !CONFIG_NUMA */\n+\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn false;\n+}\n+\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn false;\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\treturn NULL;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+#endif /* CONFIG_NUMA */\n+\n+#if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\n+\n+int node_private_register(int nid, struct node_private *np);\n+int node_private_unregister(int nid);\n+int node_private_set_ops(int nid, const struct node_private_ops *ops);\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n+\n+#else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n+\n+static inline int node_private_register(int nid, struct node_private *np)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_unregister(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline int node_private_set_ops(int nid,\n+\t\t\t\t       const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_clear_ops(int nid,\n+\t\t\t\t\t const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+#endif /* CONFIG_NUMA && CONFIG_MEMORY_HOTPLUG */\n+\n+#endif /* _LINUX_NODE_PRIVATE_H */\ndiff --git a/include/linux/nodemask.h b/include/linux/nodemask.h\nindex bd38648c998d..c9bcfd5a9a06 100644\n--- a/include/linux/nodemask.h\n+++ b/include/linux/nodemask.h\n@@ -391,6 +391,7 @@ enum node_states {\n \tN_HIGH_MEMORY = N_NORMAL_MEMORY,\n #endif\n \tN_MEMORY,\t\t/* The node has memory(regular, high, movable) */\n+\tN_MEMORY_PRIVATE,\t/* The node's memory is private */\n \tN_CPU,\t\t/* The node has one or more cpus */\n \tN_GENERIC_INITIATOR,\t/* The node has one or more Generic Initiators */\n \tNR_NODE_STATES\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-2-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about allocations landing on private nodes without explicit permission by introducing __GFP_PRIVATE and updating various functions to filter out N_MEMORY_PRIVATE nodes unless this flag is set.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY_PRIVATE nodes hold device-managed memory that should not be\nused for general allocations. Without a gating mechanism, any allocation\ncould land on a private node if it appears in the task's mems_allowed.\n\nIntroduce __GFP_PRIVATE that explicitly opts in to allocation from\nN_MEMORY_PRIVATE nodes.\n\nAdd the GFP_PRIVATE compound mask (__GFP_PRIVATE | __GFP_THISNODE)\nfor callers that explicitly target private nodes to help prevent\nfallback allocations from DRAM.\n\nUpdate cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE\nnodes unless __GFP_PRIVATE is set.\n\nIn interrupt context, only N_MEMORY nodes are valid.\n\nUpdate cpuset_handle_hotplug() to include N_MEMORY_PRIVATE nodes in\nthe effective mems set, allowing cgroup-level control over private\nnode access.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/gfp_types.h      | 15 +++++++++++++--\n include/trace/events/mmflags.h |  4 ++--\n kernel/cgroup/cpuset.c         | 32 ++++++++++++++++++++++++++++----\n 3 files changed, 43 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/gfp_types.h b/include/linux/gfp_types.h\nindex 3de43b12209e..ac375f9a0fc2 100644\n--- a/include/linux/gfp_types.h\n+++ b/include/linux/gfp_types.h\n@@ -33,7 +33,7 @@ enum {\n \t___GFP_IO_BIT,\n \t___GFP_FS_BIT,\n \t___GFP_ZERO_BIT,\n-\t___GFP_UNUSED_BIT,\t/* 0x200u unused */\n+\t___GFP_PRIVATE_BIT,\n \t___GFP_DIRECT_RECLAIM_BIT,\n \t___GFP_KSWAPD_RECLAIM_BIT,\n \t___GFP_WRITE_BIT,\n@@ -69,7 +69,7 @@ enum {\n #define ___GFP_IO\t\tBIT(___GFP_IO_BIT)\n #define ___GFP_FS\t\tBIT(___GFP_FS_BIT)\n #define ___GFP_ZERO\t\tBIT(___GFP_ZERO_BIT)\n-/* 0x200u unused */\n+#define ___GFP_PRIVATE\t\tBIT(___GFP_PRIVATE_BIT)\n #define ___GFP_DIRECT_RECLAIM\tBIT(___GFP_DIRECT_RECLAIM_BIT)\n #define ___GFP_KSWAPD_RECLAIM\tBIT(___GFP_KSWAPD_RECLAIM_BIT)\n #define ___GFP_WRITE\t\tBIT(___GFP_WRITE_BIT)\n@@ -139,6 +139,11 @@ enum {\n  * %__GFP_ACCOUNT causes the allocation to be accounted to kmemcg.\n  *\n  * %__GFP_NO_OBJ_EXT causes slab allocation to have no object extension.\n+ *\n+ * %__GFP_PRIVATE allows allocation from N_MEMORY_PRIVATE nodes (e.g., compressed\n+ * memory, accelerator memory). Without this flag, allocations are restricted\n+ * to N_MEMORY nodes only. Used by migration/demotion paths when explicitly\n+ * targeting private nodes.\n  */\n #define __GFP_RECLAIMABLE ((__force gfp_t)___GFP_RECLAIMABLE)\n #define __GFP_WRITE\t((__force gfp_t)___GFP_WRITE)\n@@ -146,6 +151,7 @@ enum {\n #define __GFP_THISNODE\t((__force gfp_t)___GFP_THISNODE)\n #define __GFP_ACCOUNT\t((__force gfp_t)___GFP_ACCOUNT)\n #define __GFP_NO_OBJ_EXT   ((__force gfp_t)___GFP_NO_OBJ_EXT)\n+#define __GFP_PRIVATE\t((__force gfp_t)___GFP_PRIVATE)\n \n /**\n  * DOC: Watermark modifiers\n@@ -367,6 +373,10 @@ enum {\n  * available and will not wake kswapd/kcompactd on failure. The _LIGHT\n  * version does not attempt reclaim/compaction at all and is by default used\n  * in page fault path, while the non-light is used by khugepaged.\n+ *\n+ * %GFP_PRIVATE adds %__GFP_THISNODE by default to prevent any fallback\n+ * allocations to other nodes, given that the caller was already attempting\n+ * to access driver-managed memory explicitly.\n  */\n #define GFP_ATOMIC\t(__GFP_HIGH|__GFP_KSWAPD_RECLAIM)\n #define GFP_KERNEL\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n@@ -382,5 +392,6 @@ enum {\n #define GFP_TRANSHUGE_LIGHT\t((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \\\n \t\t\t __GFP_NOMEMALLOC | __GFP_NOWARN) & ~__GFP_RECLAIM)\n #define GFP_TRANSHUGE\t(GFP_TRANSHUGE_LIGHT | __GFP_DIRECT_RECLAIM)\n+#define GFP_PRIVATE\t(__GFP_PRIVATE | __GFP_THISNODE)\n \n #endif /* __LINUX_GFP_TYPES_H */\ndiff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h\nindex a6e5a44c9b42..f042cd848451 100644\n--- a/include/trace/events/mmflags.h\n+++ b/include/trace/events/mmflags.h\n@@ -37,7 +37,8 @@\n \tTRACE_GFP_EM(HARDWALL)\t\t\t\\\n \tTRACE_GFP_EM(THISNODE)\t\t\t\\\n \tTRACE_GFP_EM(ACCOUNT)\t\t\t\\\n-\tTRACE_GFP_EM(ZEROTAGS)\n+\tTRACE_GFP_EM(ZEROTAGS)\t\t\t\\\n+\tTRACE_GFP_EM(PRIVATE)\n \n #ifdef CONFIG_KASAN_HW_TAGS\n # define TRACE_GFP_FLAGS_KASAN\t\t\t\\\n@@ -73,7 +74,6 @@\n TRACE_GFP_FLAGS\n \n /* Just in case these are ever used */\n-TRACE_DEFINE_ENUM(___GFP_UNUSED_BIT);\n TRACE_DEFINE_ENUM(___GFP_LAST_BIT);\n \n #define gfpflag_string(flag) {(__force unsigned long)flag, #flag}\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 473aa9261e16..1a597f0c7c6c 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -444,21 +444,32 @@ static void guarantee_active_cpus(struct task_struct *tsk,\n }\n \n /*\n- * Return in *pmask the portion of a cpusets's mems_allowed that\n+ * Return in *pmask the portion of a cpuset's mems_allowed that\n  * are online, with memory.  If none are online with memory, walk\n  * up the cpuset hierarchy until we find one that does have some\n  * online mems.  The top cpuset always has some mems online.\n  *\n  * One way or another, we guarantee to return some non-empty subset\n- * of node_states[N_MEMORY].\n+ * of node_states[N_MEMORY].  N_MEMORY_PRIVATE nodes from the\n+ * original cpuset are preserved, but only N_MEMORY nodes are\n+ * pulled from ancestors.\n  *\n  * Call with callback_lock or cpuset_mutex held.\n  */\n static void guarantee_online_mems(struct cpuset *cs, nodemask_t *pmask)\n {\n+\tstruct cpuset *orig_cs = cs;\n+\tint nid;\n+\n \twhile (!nodes_intersects(cs->effective_mems, node_states[N_MEMORY]))\n \t\tcs = parent_cs(cs);\n+\n \tnodes_and(*pmask, cs->effective_mems, node_states[N_MEMORY]);\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_isset(nid, orig_cs->effective_mems))\n+\t\t\tnode_set(nid, *pmask);\n+\t}\n }\n \n /**\n@@ -4075,7 +4086,9 @@ static void cpuset_handle_hotplug(void)\n \n \t/* fetch the available cpus/mems and find out which changed how */\n \tcpumask_copy(&new_cpus, cpu_active_mask);\n-\tnew_mems = node_states[N_MEMORY];\n+\n+\t/* Include N_MEMORY_PRIVATE so cpuset controls access the same way */\n+\tnodes_or(new_mems, node_states[N_MEMORY], node_states[N_MEMORY_PRIVATE]);\n \n \t/*\n \t * If subpartitions_cpus is populated, it is likely that the check\n@@ -4488,10 +4501,21 @@ bool cpuset_node_allowed(struct cgroup *cgroup, int nid)\n  * __alloc_pages() will include all nodes.  If the slab allocator\n  * is passed an offline node, it will fall back to the local node.\n  * See kmem_cache_alloc_node().\n+ *\n+ *\n+ * Private nodes aren't eligible for these allocations, so skip them.\n+ * guarantee_online_mems guaranttes at least one N_MEMORY node is set.\n  */\n static int cpuset_spread_node(int *rotor)\n {\n-\treturn *rotor = next_node_in(*rotor, current->mems_allowed);\n+\tint node;\n+\n+\tdo {\n+\t\tnode = next_node_in(*rotor, current->mems_allowed);\n+\t\t*rotor = node;\n+\t} while (node_state(node, N_MEMORY_PRIVATE));\n+\n+\treturn node;\n }\n \n /**\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-3-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern that the current implementation of cpuset filtering in mm/ does not account for N_MEMORY_PRIVATE nodes on systems without cpusets, leading to private-node zones leaking into allocation paths. The author added a new helper function numa_zone_allowed() which consolidates zone filtering and replaced open-coded patterns in mm/ with this new helper.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Various locations in mm/ open-code cpuset filtering with:\n\n  cpusets_enabled() && ALLOC_CPUSET && !__cpuset_zone_allowed()\n\nThis pattern does not account for N_MEMORY_PRIVATE nodes on systems\nwithout cpusets, so private-node zones can leak into allocation\npaths that should only see general-purpose memory.\n\nAdd numa_zone_allowed() which consolidates zone filtering. It checks\ncpuset membership when cpusets are enabled, and otherwise gates\nN_MEMORY_PRIVATE zones behind __GFP_PRIVATE globally.\n\nReplace the open-coded patterns in mm/ with the new helper.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/compaction.c |  6 ++----\n mm/hugetlb.c    |  2 +-\n mm/internal.h   |  7 +++++++\n mm/page_alloc.c | 31 ++++++++++++++++++++-----------\n mm/slub.c       |  3 ++-\n 5 files changed, 32 insertions(+), 17 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..6a65145b03d8 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -2829,10 +2829,8 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tenum compact_result status;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 51273baec9e5..f2b914ab5910 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -1353,7 +1353,7 @@ static struct folio *dequeue_hugetlb_folio_nodemask(struct hstate *h, gfp_t gfp_\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {\n \t\tstruct folio *folio;\n \n-\t\tif (!cpuset_zone_allowed(zone, gfp_mask))\n+\t\tif (!numa_zone_alloc_allowed(ALLOC_CPUSET, zone, gfp_mask))\n \t\t\tcontinue;\n \t\t/*\n \t\t * no need to ask again on the same node. Pool is node rather than\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 23ee14790227..97023748e6a9 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t      gfp_t gfp_mask);\n #else\n #define node_reclaim_mode 0\n \n@@ -1218,6 +1220,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t\t     gfp_t gfp_mask)\n+{\n+\treturn true;\n+}\n #endif\n \n static inline bool node_reclaim_enabled(void)\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 2facee0805da..47f2619d3840 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3690,6 +3690,21 @@ static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n \treturn node_distance(zone_to_nid(local_zone), zone_to_nid(zone)) <=\n \t\t\t\tnode_reclaim_distance;\n }\n+\n+/* Returns true if allocation from this zone is permitted */\n+bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone, gfp_t gfp_mask)\n+{\n+\t/* Gate N_MEMORY_PRIVATE zones behind __GFP_PRIVATE */\n+\tif (!(gfp_mask & __GFP_PRIVATE) &&\n+\t    node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn false;\n+\n+\t/* If cpusets is being used, check mems_allowed */\n+\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET))\n+\t\treturn cpuset_zone_allowed(zone, gfp_mask);\n+\n+\treturn true;\n+}\n #else\t/* CONFIG_NUMA */\n static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n {\n@@ -3781,10 +3796,8 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\tstruct page *page;\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \t\t/*\n \t\t * When allocating a page cache page for writing, we\n \t\t * want to get it from a node that is within its dirty\n@@ -4585,10 +4598,8 @@ should_reclaim_retry(gfp_t gfp_mask, unsigned order,\n \t\tunsigned long min_wmark = min_wmark_pages(zone);\n \t\tbool wmark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tavailable = reclaimable = zone_reclaimable_pages(zone);\n \t\tavailable += zone_page_state_snapshot(zone, NR_FREE_PAGES);\n@@ -5084,10 +5095,8 @@ unsigned long alloc_pages_bulk_noprof(gfp_t gfp, int preferred_nid,\n \tfor_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&\n-\t\t    !__cpuset_zone_allowed(zone, gfp)) {\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp))\n \t\t\tcontinue;\n-\t\t}\n \n \t\tif (nr_online_nodes > 1 && zone != zonelist_zone(ac.preferred_zoneref) &&\n \t\t    zone_to_nid(zone) != zonelist_node_idx(ac.preferred_zoneref)) {\ndiff --git a/mm/slub.c b/mm/slub.c\nindex 861592ac5425..e4bd6ede81d1 100644\n--- a/mm/slub.c\n+++ b/mm/slub.c\n@@ -3595,7 +3595,8 @@ static struct slab *get_any_partial(struct kmem_cache *s,\n \n \t\t\tn = get_node(s, zone_to_nid(zone));\n \n-\t\t\tif (n && cpuset_zone_allowed(zone, pc->flags) &&\n+\t\t\tif (n && numa_zone_alloc_allowed(ALLOC_CPUSET, zone,\n+\t\t\t\t\t\t   pc->flags) &&\n \t\t\t\t\tn->nr_partial > s->min_partial) {\n \t\t\t\tslab = get_partial_node(s, n, pc);\n \t\t\t\tif (slab) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-4-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about N_MEMORY fallback lists including N_MEMORY_PRIVATE nodes, explaining that this would allow allocation from them in some scenarios and cause iterations over ineligible nodes. The author confirmed that private node primary fallback lists do include N_MEMORY nodes for kernel allocations to fall back to DRAM when __GFP_PRIVATE is not set.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a technical concern",
                "provided an explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY fallback lists should not include N_MEMORY_PRIVATE nodes, at\nworst this would allow allocation from them in some scenarios, and at\nbest it causes iterations over nodes that aren't eligible.\n\nPrivate node primary fallback lists do include N_MEMORY nodes so\nkernel/slab allocations made on behalf of the private node can\nfall back to DRAM when __GFP_PRIVATE is not set.\n\nThe nofallback list contains only the node's own zones, restricting\n__GFP_THISNODE allocations to the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/page_alloc.c | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 47f2619d3840..5a1b35421d78 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5683,6 +5683,26 @@ static void build_zonelists(pg_data_t *pgdat)\n \tlocal_node = pgdat->node_id;\n \tprev_node = local_node;\n \n+\t/*\n+\t * Private nodes need N_MEMORY nodes as fallback for kernel allocations\n+\t * (e.g., slab objects allocated on behalf of this node).\n+\t */\n+\tif (node_state(local_node, N_MEMORY_PRIVATE)) {\n+\t\tnode_order[nr_nodes++] = local_node;\n+\t\tnode_set(local_node, used_mask);\n+\n+\t\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0)\n+\t\t\tnode_order[nr_nodes++] = node;\n+\n+\t\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n+\t\tbuild_thisnode_zonelists(pgdat);\n+\t\tpr_info(\"Fallback order for Node %d (private):\", local_node);\n+\t\tfor (node = 0; node < nr_nodes; node++)\n+\t\t\tpr_cont(\" %d\", node_order[node]);\n+\t\tpr_cont(\"\\n\");\n+\t\treturn;\n+\t}\n+\n \tmemset(node_order, 0, sizeof(node_order));\n \twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-5-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the need for a unified predicate to exclude N_MEMORY_PRIVATE and ZONE_DEVICE folios from MM operations, proposing the addition of folio_is_private_managed() as a replacement for folio_is_zone_device(). The author explained that this new function will return true for both types of folios and preserve existing behavior when NUMA is disabled.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "proposed patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Multiple mm/ subsystems already skip operations for ZONE_DEVICE folios,\nand N_MEMORY_PRIVATE folios share the checkpoints for ZONE_DEVICE pages.\n\nAdd folio_is_private_managed() as a unified predicate that returns true\nfor folios on N_MEMORY_PRIVATE nodes or in ZONE_DEVICE.\n\nThis predicate replaces folio_is_zone_device at skip sites where both\nfolio types should be excluded from an MM operation.\n\nAt some locations, explicit zone_device vs private_node checks are more\nappropriate when the operations between the two fundamentally differ.\n\nThe !CONFIG_NUMA stubs fall through to folio_is_zone_device() only,\npreserving existing behavior when NUMA is disabled.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 6a70ec39d569..7687a4cf990c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -92,6 +92,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio) || folio_is_private_node(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n@@ -146,6 +156,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn false;\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-6-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about mlocking private node folios by pointing out that the existing folio_is_zone_device check is sufficient and can be extended to handle private nodes, indicating no need for further revision.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged",
                "agreed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nmlocked.  The existing folio_is_zone_device check is already correctly\nplaced to handle this - simply extend it for private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/mlock.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/mlock.c b/mm/mlock.c\nindex 2f699c3497a5..c56159253e45 100644\n--- a/mm/mlock.c\n+++ b/mm/mlock.c\n@@ -25,6 +25,7 @@\n #include <linux/memcontrol.h>\n #include <linux/mm_inline.h>\n #include <linux/secretmem.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -366,7 +367,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (is_huge_zero_pmd(*pmd))\n \t\t\tgoto out;\n \t\tfolio = pmd_folio(*pmd);\n-\t\tif (folio_is_zone_device(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)))\n \t\t\tgoto out;\n \t\tif (vma->vm_flags & VM_LOCKED)\n \t\t\tmlock_folio(folio);\n@@ -386,7 +387,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (!pte_present(ptent))\n \t\t\tcontinue;\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\tstep = folio_mlock_step(folio, pte, addr, end);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-7-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern that madvise and pageout operations should not interfere with device driver memory management for private node folios, agreeing to extend the zone_device check to cover private nodes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nsubjectto madvise cold/pageout/free operations that would interfere\nwith the driver's memory management.\n\nExtend the existing zone_device check to cover private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/madvise.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/madvise.c b/mm/madvise.c\nindex b617b1be0f53..3aac105e840b 100644\n--- a/mm/madvise.c\n+++ b/mm/madvise.c\n@@ -32,6 +32,7 @@\n #include <linux/leafops.h>\n #include <linux/shmem_fs.h>\n #include <linux/mmu_notifier.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlb.h>\n \n@@ -475,7 +476,7 @@ static int madvise_cold_or_pageout_pte_range(pmd_t *pmd,\n \t\t\tcontinue;\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n@@ -704,7 +705,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,\n \t\t}\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-8-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about KSM merging interfering with the driver's memory lifecycle management by default. They agreed to extend existing zone_device checks in get_mergeable_page and ksm_next_page_pmd_entry to cover private node folios, indicating that a fix is planned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a need for change",
                "agreed to implement a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not participate in KSM merging by default.\nThe driver manages the memory lifecycle and KSM's page sharing can\ninterfere with driver operations.\n\nExtend the existing zone_device checks in get_mergeable_page and\nksm_next_page_pmd_entry to cover private node folios as well.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/ksm.c | 9 ++++++---\n 1 file changed, 6 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/ksm.c b/mm/ksm.c\nindex 2d89a7c8b4eb..c48e95a6fff9 100644\n--- a/mm/ksm.c\n+++ b/mm/ksm.c\n@@ -40,6 +40,7 @@\n #include <linux/oom.h>\n #include <linux/numa.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include \"internal.h\"\n@@ -808,7 +809,7 @@ static struct page *get_mergeable_page(struct ksm_rmap_item *rmap_item)\n \n \tfolio = folio_walk_start(&fw, vma, addr, 0);\n \tif (folio) {\n-\t\tif (!folio_is_zone_device(folio) &&\n+\t\tif (!folio_is_private_managed(folio) &&\n \t\t    folio_test_anon(folio)) {\n \t\t\tfolio_get(folio);\n \t\t\tpage = fw.page;\n@@ -2521,7 +2522,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\t\tgoto not_found_unlock;\n \t\t\tfolio = page_folio(page);\n \n-\t\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t\t    !folio_test_anon(folio))\n \t\t\t\tgoto not_found_unlock;\n \n \t\t\tpage += ((addr & (PMD_SIZE - 1)) >> PAGE_SHIFT);\n@@ -2545,7 +2547,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\tcontinue;\n \t\tfolio = page_folio(page);\n \n-\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t    !folio_test_anon(folio))\n \t\t\tcontinue;\n \t\tgoto found_unlock;\n \t}\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-9-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the interaction between khugepaged and private nodes during collapse operations. They acknowledged that this is an issue for private nodes, specifically due to potential LRU inversion and lack of migration support. To handle this, they plan to add a check similar to zone_device, which may be revisited if some private node services report explicit support for collapse.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged issue",
                "planned fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "A collapse operation allocates a new large folio and migrates the\nsmaller folios into it.  This is an issue for private nodes:\n\n  1. The private node service may not support migration\n  2. Collapse may promotes pages from the private node to a local node,\n     which may result in an LRU inversion that defeats memory tiering.\n\nHandle this just like zone_device for now.\n\nIt may be possible to support this later for some private node services\nthat report explicit support for collapse (and migration).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/khugepaged.c | 7 ++++---\n 1 file changed, 4 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/khugepaged.c b/mm/khugepaged.c\nindex 97d1b2824386..36f6bc5da53c 100644\n--- a/mm/khugepaged.c\n+++ b/mm/khugepaged.c\n@@ -21,6 +21,7 @@\n #include <linux/shmem_fs.h>\n #include <linux/dax.h>\n #include <linux/ksm.h>\n+#include <linux/node_private.h>\n #include <linux/pgalloc.h>\n \n #include <asm/tlb.h>\n@@ -571,7 +572,7 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,\n \t\t\tgoto out;\n \t\t}\n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out;\n \t\t}\n@@ -1323,7 +1324,7 @@ static int hpage_collapse_scan_pmd(struct mm_struct *mm,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out_unmap;\n \t\t}\n@@ -1575,7 +1576,7 @@ int collapse_pte_mapped_thp(struct mm_struct *mm, unsigned long addr,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, ptent);\n-\t\tif (WARN_ON_ONCE(page && is_zone_device_page(page)))\n+\t\tif (WARN_ON_ONCE(page && page_is_private_managed(page)))\n \t\t\tpage = NULL;\n \t\t/*\n \t\t * Note that uprobe, debugger, or MAP_PRIVATE may change the\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-10-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about cleanup when a folio's refcount drops to zero, explaining that the service may need to perform cleanup before the page returns to the buddy allocator. They added a new function `folio_managed_on_free()` to wrap both zone_device and private node semantics for this operation since they are the same.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical issue",
                "provided an explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a folio's refcount drops to zero, the service may need to perform\ncleanup before the page returns to the buddy allocator (e.g. zeroing\npages to scrub stale compressed data / release compression ratio).\n\nAdd folio_managed_on_free() to wrap both zone_device and private node\nsemantics for this operation since they are the same.\n\nOne difference between zone_device and private node folios:\n  - private nodes may choose to either take a reference and return true\n    (\"handled\"), or return false to return it back to the buddy.\n\n  - zone_device returns the page to the buddy (always returns true)\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 30 ++++++++++++++++++++++++++++++\n mm/swap.c                    | 21 ++++++++++-----------\n 3 files changed, 46 insertions(+), 11 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7687a4cf990c..09ea7c4cb13c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -39,10 +39,16 @@ struct vm_fault;\n  *   callback to prevent node_private from being freed.\n  *   These callbacks MUST NOT sleep.\n  *\n+ * @free_folio: Called when a folio refcount drops to 0\n+ *   [folio-referenced callback]\n+ *   Returns: true if handled (skip return to buddy)\n+ *            false if no op (return to buddy)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n+\tbool (*free_folio)(struct folio *folio);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 97023748e6a9..658da41cdb8e 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1412,6 +1412,36 @@ int numa_migrate_check(struct folio *folio, struct vm_fault *vmf,\n void free_zone_device_folio(struct folio *folio);\n int migrate_device_coherent_folio(struct folio *folio);\n \n+/**\n+ * folio_managed_on_free - Notify managed-memory service that folio\n+ *                         refcount reached zero.\n+ * @folio: the folio being freed\n+ *\n+ * Returns true if the folio is fully handled (zone_device -- caller\n+ * must return immediately).  Returns false if the callback ran but\n+ * the folio should continue through the normal free path\n+ * (private_node -- pages go back to buddy).\n+ *\n+ * Returns false for normal folios (no-op).\n+ */\n+static inline bool folio_managed_on_free(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio)) {\n+\t\tfree_zone_device_folio(folio);\n+\t\treturn true;\n+\t}\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->free_folio) {\n+\t\t\tif (ops->free_folio(folio))\n+\t\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/swap.c b/mm/swap.c\nindex 2260dcd2775e..dca306e1ae6d 100644\n--- a/mm/swap.c\n+++ b/mm/swap.c\n@@ -37,6 +37,7 @@\n #include <linux/page_idle.h>\n #include <linux/local_lock.h>\n #include <linux/buffer_head.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -96,10 +97,9 @@ static void page_cache_release(struct folio *folio)\n \n void __folio_put(struct folio *folio)\n {\n-\tif (unlikely(folio_is_zone_device(folio))) {\n-\t\tfree_zone_device_folio(folio);\n-\t\treturn;\n-\t}\n+\tif (unlikely(folio_is_private_managed(folio)))\n+\t\tif (folio_managed_on_free(folio))\n+\t\t\treturn;\n \n \tif (folio_test_hugetlb(folio)) {\n \t\tfree_huge_folio(folio);\n@@ -961,19 +961,18 @@ void folios_put_refs(struct folio_batch *folios, unsigned int *refs)\n \t\tif (is_huge_zero_folio(folio))\n \t\t\tcontinue;\n \n-\t\tif (folio_is_zone_device(folio)) {\n+\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n+\t\t\tcontinue;\n+\n+\t\tif (unlikely(folio_is_private_managed(folio))) {\n \t\t\tif (lruvec) {\n \t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n \t\t\t\tlruvec = NULL;\n \t\t\t}\n-\t\t\tif (folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\t\tfree_zone_device_folio(folio);\n-\t\t\tcontinue;\n+\t\t\tif (folio_managed_on_free(folio))\n+\t\t\t\tcontinue;\n \t\t}\n \n-\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\tcontinue;\n-\n \t\t/* hugetlb has its own memcg */\n \t\tif (folio_test_hugetlb(folio)) {\n \t\t\tif (lruvec) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-11-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about notifying private node services when a THP folio is split by adding an optional callback to the ops struct and updating __folio_split() to call this new callback. The patch adds code to handle zone_device and private node callbacks in a consolidated manner.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "addressed_concern",
                "provided_explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private node services may need to update internal metadata when\na THP folio is split.  ZONE_DEVICE already has a split callback via\npgmap->ops; private nodes can provide the same capability.\n\nJust like zone_device, some private node services may want to know\nabout a folio being split.  Add this optional callback to the ops\nstruct and add a wrapper for zone_device and private node callback\ndispatch to be consolidated.\n\nWire this into __folio_split() where the zone_device check was made.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 33 +++++++++++++++++++++++++++++++++\n mm/huge_memory.c             |  6 ++++--\n 2 files changed, 37 insertions(+), 2 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 09ea7c4cb13c..f9dd2d25c8a5 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -3,6 +3,7 @@\n #define _LINUX_NODE_PRIVATE_H\n \n #include <linux/completion.h>\n+#include <linux/memremap.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -44,11 +45,19 @@ struct vm_fault;\n  *   Returns: true if handled (skip return to buddy)\n  *            false if no op (return to buddy)\n  *\n+ * @folio_split: Notification that a folio on this private node is being split.\n+ *    [folio-referenced callback]\n+ *     Called from the folio split path via folio_managed_split_cb().\n+ *     @folio is the original folio; @new_folio is the newly created folio,\n+ *     or NULL when called for the final (original) folio after all sub-folios\n+ *     have been split off.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n+\tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n \tunsigned long flags;\n };\n \n@@ -150,6 +159,24 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn node_private_flags(zone_to_nid(z)) & flag;\n }\n \n+static inline void node_private_split_cb(struct folio *folio,\n+\t\t\t\t\t struct folio *new_folio)\n+{\n+\tconst struct node_private_ops *ops = folio_node_private_ops(folio);\n+\n+\tif (ops && ops->folio_split)\n+\t\tops->folio_split(folio, new_folio);\n+}\n+\n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+\telse if (folio_is_private_node(original_folio))\n+\t\tnode_private_split_cb(original_folio, new_folio);\n+}\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -198,6 +225,12 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn false;\n }\n \n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+}\n #endif /* CONFIG_NUMA */\n \n #if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 40cf59301c21..2ecae494291a 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -24,6 +24,7 @@\n #include <linux/freezer.h>\n #include <linux/mman.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/debugfs.h>\n #include <linux/migrate.h>\n@@ -3850,7 +3851,7 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \n \t\t\tnext = folio_next(new_folio);\n \n-\t\t\tzone_device_private_split_cb(folio, new_folio);\n+\t\t\tfolio_managed_split_cb(folio, new_folio);\n \n \t\t\tfolio_ref_unfreeze(new_folio,\n \t\t\t\t\t   folio_cache_ref_count(new_folio) + 1);\n@@ -3889,7 +3890,8 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \t\t\tfolio_put_refs(new_folio, nr_pages);\n \t\t}\n \n-\t\tzone_device_private_split_cb(folio, NULL);\n+\t\tfolio_managed_split_cb(folio, NULL);\n+\n \t\t/*\n \t\t * Unfreeze @folio only after all page cache entries, which\n \t\t * used to point to it, have been updated with new folios.\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-12-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about user-driven migration between regular and private nodes, explaining that ZONE_DEVICE always rejects it but private nodes should be able to opt in. They added the NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper to support this feature.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services may want to support user-driven migration\n(migrate_pages syscall, mbind) to allow data movement between regular\nand private nodes.\n\nZONE_DEVICE always rejects user migration, but private nodes should\nbe able to opt in.\n\nAdd NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper that\ndispatches migration requests.  Private nodes can either set the flag\nand provide a custom migrate_to callback for driver-managed migration.\n\nIn migrate_to_node(), allows GFP_PRIVATE when the destination node\nsupports NP_OPS_MIGRATION, enabling migrate_pages syscall to target\nprivate nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |   4 ++\n include/linux/migrate.h      |  10 +++\n include/linux/node_private.h | 122 +++++++++++++++++++++++++++++++++++\n mm/damon/paddr.c             |   3 +\n mm/internal.h                |  24 +++++++\n mm/mempolicy.c               |  10 +--\n mm/migrate.c                 |  49 ++++++++++----\n mm/rmap.c                    |   4 +-\n 8 files changed, 206 insertions(+), 20 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 646dc48a23b5..e587f5781135 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -949,6 +949,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \tif (!node_possible(nid))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MIGRATION) &&\n+\t    (!ops->migrate_to || !ops->folio_migrate))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 26ca00c325d9..7b2da3875ff2 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -71,6 +71,9 @@ void folio_migrate_flags(struct folio *newfolio, struct folio *folio);\n int folio_migrate_mapping(struct address_space *mapping,\n \t\tstruct folio *newfolio, struct folio *folio, int extra_count);\n int set_movable_ops(const struct movable_operations *ops, enum pagetype type);\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason);\n \n #else\n \n@@ -96,6 +99,13 @@ static inline int set_movable_ops(const struct movable_operations *ops, enum pag\n {\n \treturn -ENOSYS;\n }\n+static inline int migrate_folios_to_node(struct list_head *folios,\n+\t\t\t\t\t\t  int nid,\n+\t\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t\t  enum migrate_reason reason)\n+{\n+\treturn -ENOSYS;\n+}\n \n #endif /* CONFIG_MIGRATION */\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex f9dd2d25c8a5..0c5be1ee6e60 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -4,6 +4,7 @@\n \n #include <linux/completion.h>\n #include <linux/memremap.h>\n+#include <linux/migrate_mode.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -52,15 +53,40 @@ struct vm_fault;\n  *     or NULL when called for the final (original) folio after all sub-folios\n  *     have been split off.\n  *\n+ * @migrate_to: Migrate folios TO this node.\n+ *\t[refcounted callback]\n+ *\tReturns: 0 on full success, >0 = number of folios that failed to\n+ *\t\t migrate, <0 = error.  Matches migrate_pages() semantics.\n+ *\t\t @nr_succeeded is set to the number of successfully migrated\n+ *\t\t folios (may be NULL if caller doesn't need it).\n+ *\n+ * @folio_migrate: Post-migration notification that a folio on this private node\n+ *    changed physical location (on the same node or a different node).\n+ *    [folio-referenced callback]\n+ *     Called from migrate_folio_move() after data has been copied but before\n+ *     migration entries are replaced with real PTEs.  Both @src and @dst are\n+ *     locked.  Faults block in migration_entry_wait() until\n+ *     remove_migration_ptes() runs, so the service can safely update\n+ *     PFN-based metadata (compression tables, device page tables, DMA\n+ *     mappings, etc.) before any access through the page tables.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n \tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n+\tint (*migrate_to)(struct list_head *folios, int nid,\n+\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t  unsigned int *nr_succeeded);\n+\tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tunsigned long flags;\n };\n \n+/* Allow user/kernel migration; requires migrate_to and folio_migrate */\n+#define NP_OPS_MIGRATION\t\tBIT(0)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\n@@ -177,6 +203,81 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n \t\tnode_private_split_cb(original_folio, new_folio);\n }\n \n+#ifdef CONFIG_MEMORY_HOTPLUG\n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn -ENOENT;\n+\treturn node_private_has_flag(folio_nid(folio), NP_OPS_MIGRATION) ?\n+\t       folio_nid(folio) : -ENOENT;\n+}\n+\n+/**\n+ * folio_managed_allows_migrate - Check if a managed folio supports migration\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio can be migrated.  For zone_device folios, only\n+ * device_private and device_coherent support migration.  For private node\n+ * folios, migration requires NP_OPS_MIGRATION.  Normal folios always\n+ * return true.\n+ */\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\tif (folio_is_private_node(folio))\n+\t\treturn folio_private_flags(folio, NP_OPS_MIGRATION);\n+\treturn true;\n+}\n+\n+/**\n+ * node_private_migrate_to - Attempt service-specific migration to a private node\n+ * @folios: list of folios to migrate (may sleep)\n+ * @nid: target node\n+ * @mode: migration mode (MIGRATE_ASYNC, MIGRATE_SYNC, etc.)\n+ * @reason: migration reason (MR_DEMOTION, MR_SYSCALL, etc.)\n+ * @nr_succeeded: optional output for number of successfully migrated folios\n+ *\n+ * If @nid is an N_MEMORY_PRIVATE node with a migrate_to callback,\n+ * invokes the callback and returns the result with migrate_pages()\n+ * semantics (0 = full success, >0 = failure count, <0 = error).\n+ * Returns -ENODEV if the node is not private or the service is being\n+ * torn down.\n+ *\n+ * The source folios are on other nodes, so they do not pin the target\n+ * node's node_private.  A temporary refcount is taken under rcu_read_lock\n+ * to keep node_private (and the service module) alive across the callback.\n+ */\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\tint (*fn)(struct list_head *, int, enum migrate_mode,\n+\t\t  enum migrate_reason, unsigned int *);\n+\tstruct node_private *np;\n+\tint ret;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (!np || !np->ops || !np->ops->migrate_to ||\n+\t    !refcount_inc_not_zero(&np->refcount)) {\n+\t\trcu_read_unlock();\n+\t\treturn -ENODEV;\n+\t}\n+\tfn = np->ops->migrate_to;\n+\trcu_read_unlock();\n+\n+\tret = fn(folios, nid, mode, reason, nr_succeeded);\n+\n+\tif (refcount_dec_and_test(&np->refcount))\n+\t\tcomplete(&np->released);\n+\n+\treturn ret;\n+}\n+#endif /* CONFIG_MEMORY_HOTPLUG */\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -242,6 +343,27 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\treturn -ENOENT;\n+}\n+\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\treturn true;\n+}\n+\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\treturn -ENODEV;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/damon/paddr.c b/mm/damon/paddr.c\nindex 07a8aead439e..532b8e2c62b0 100644\n--- a/mm/damon/paddr.c\n+++ b/mm/damon/paddr.c\n@@ -277,6 +277,9 @@ static unsigned long damon_pa_migrate(struct damon_region *r,\n \t\telse\n \t\t\t*sz_filter_passed += folio_size(folio) / addr_unit;\n \n+\t\tif (!folio_managed_allows_migrate(folio))\n+\t\t\tgoto put_folio;\n+\n \t\tif (!folio_isolate_lru(folio))\n \t\t\tgoto put_folio;\n \t\tlist_add(&folio->lru, &folio_list);\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 658da41cdb8e..6ab4679fe943 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1442,6 +1442,30 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/**\n+ * folio_managed_migrate_notify - Notify service that a folio changed location\n+ * @src: the old folio (about to be freed)\n+ * @dst: the new folio (data already copied, migration entries still in place)\n+ *\n+ * Called from migrate_folio_move() after data has been copied but before\n+ * remove_migration_ptes() installs real PTEs pointing to @dst.  While\n+ * migration entries are in place, faults block in migration_entry_wait(),\n+ * so the service can safely update PFN-based metadata before any access\n+ * through the page tables.  Both @src and @dst are locked.\n+ */\n+static inline void folio_managed_migrate_notify(struct folio *src,\n+\t\t\t\t\t\tstruct folio *dst)\n+{\n+\tconst struct node_private_ops *ops;\n+\n+\tif (!folio_is_private_node(src))\n+\t\treturn;\n+\n+\tops = folio_node_private_ops(src);\n+\tif (ops && ops->folio_migrate)\n+\t\tops->folio_migrate(src, dst);\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 68a98ba57882..2b0f9762d171 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -111,6 +111,7 @@\n #include <linux/mmu_notifier.h>\n #include <linux/printk.h>\n #include <linux/leafops.h>\n+#include <linux/node_private.h>\n #include <linux/gcd.h>\n \n #include <asm/tlbflush.h>\n@@ -1282,11 +1283,6 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tLIST_HEAD(pagelist);\n \tlong nr_failed;\n \tlong err = 0;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = dest,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n \tnodes_clear(nmask);\n \tnode_set(source, nmask);\n@@ -1311,8 +1307,8 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tmmap_read_unlock(mm);\n \n \tif (!list_empty(&pagelist)) {\n-\t\terr = migrate_pages(&pagelist, alloc_migration_target, NULL,\n-\t\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\t\terr = migrate_folios_to_node(&pagelist, dest, MIGRATE_SYNC,\n+\t\t\t\t\t     MR_SYSCALL);\n \t\tif (err)\n \t\t\tputback_movable_pages(&pagelist);\n \t}\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 5169f9717f60..a54d4af04df3 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -43,6 +43,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/memory-tiers.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1387,6 +1388,8 @@ static int migrate_folio_move(free_folio_t put_new_folio, unsigned long private,\n \tif (old_page_state & PAGE_WAS_MLOCKED)\n \t\tlru_add_drain();\n \n+\tfolio_managed_migrate_notify(src, dst);\n+\n \tif (old_page_state & PAGE_WAS_MAPPED)\n \t\tremove_migration_ptes(src, dst, 0);\n \n@@ -2165,6 +2168,7 @@ int migrate_pages(struct list_head *from, new_folio_t get_new_folio,\n \n \treturn rc_gather;\n }\n+EXPORT_SYMBOL_GPL(migrate_pages);\n \n struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n {\n@@ -2204,6 +2208,31 @@ struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n \n \treturn __folio_alloc(gfp_mask, order, nid, mtc->nmask);\n }\n+EXPORT_SYMBOL_GPL(alloc_migration_target);\n+\n+static int __migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, NULL);\n+}\n+\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason)\n+{\n+\tif (node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_private_migrate_to(folios, nid, mode,\n+\t\t\t\t\t       reason, NULL);\n+\treturn __migrate_folios_to_node(folios, nid, mode, reason);\n+}\n \n #ifdef CONFIG_NUMA\n \n@@ -2221,14 +2250,8 @@ static int store_status(int __user *status, int start, int value, int nr)\n static int do_move_pages_to_node(struct list_head *pagelist, int node)\n {\n \tint err;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = node,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n-\terr = migrate_pages(pagelist, alloc_migration_target, NULL,\n-\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\terr = migrate_folios_to_node(pagelist, node, MIGRATE_SYNC, MR_SYSCALL);\n \tif (err)\n \t\tputback_movable_pages(pagelist);\n \treturn err;\n@@ -2240,7 +2263,7 @@ static int __add_folio_for_migration(struct folio *folio, int node,\n \tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\treturn -EFAULT;\n \n-\tif (folio_is_zone_device(folio))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn -ENOENT;\n \n \tif (folio_nid(folio) == node)\n@@ -2364,7 +2387,8 @@ static int do_pages_move(struct mm_struct *mm, nodemask_t task_nodes,\n \t\terr = -ENODEV;\n \t\tif (node < 0 || node >= MAX_NUMNODES)\n \t\t\tgoto out_flush;\n-\t\tif (!node_state(node, N_MEMORY))\n+\t\tif (!node_state(node, N_MEMORY) &&\n+\t\t    !node_state(node, N_MEMORY_PRIVATE))\n \t\t\tgoto out_flush;\n \n \t\terr = -EACCES;\n@@ -2449,8 +2473,8 @@ static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n \t\tif (folio) {\n \t\t\tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\t\t\terr = -EFAULT;\n-\t\t\telse if (folio_is_zone_device(folio))\n-\t\t\t\terr = -ENOENT;\n+\t\t\telse if (unlikely(folio_is_private_managed(folio)))\n+\t\t\t\terr = folio_managed_allows_user_migrate(folio);\n \t\t\telse\n \t\t\t\terr = folio_nid(folio);\n \t\t\tfolio_walk_end(&fw, vma);\n@@ -2660,6 +2684,9 @@ int migrate_misplaced_folio_prepare(struct folio *folio,\n \tint nr_pages = folio_nr_pages(folio);\n \tpg_data_t *pgdat = NODE_DATA(node);\n \n+\tif (!folio_managed_allows_migrate(folio))\n+\t\treturn -ENOENT;\n+\n \tif (folio_is_file_lru(folio)) {\n \t\t/*\n \t\t * Do not migrate file folios that are mapped in multiple\ndiff --git a/mm/rmap.c b/mm/rmap.c\nindex f955f02d570e..805f9ceb82f3 100644\n--- a/mm/rmap.c\n+++ b/mm/rmap.c\n@@ -72,6 +72,7 @@\n #include <linux/backing-dev.h>\n #include <linux/page_idle.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/userfaultfd_k.h>\n #include <linux/mm_inline.h>\n #include <linux/oom.h>\n@@ -2616,8 +2617,7 @@ void try_to_migrate(struct folio *folio, enum ttu_flags flags)\n \t\t\t\t\tTTU_SYNC | TTU_BATCH_FLUSH)))\n \t\treturn;\n \n-\tif (folio_is_zone_device(folio) &&\n-\t    (!folio_is_device_private(folio) && !folio_is_device_coherent(folio)))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn;\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-13-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about allowing userland to directly allocate from private nodes via set_mempolicy() and mbind(), but not wanting those nodes as normal allocable system memory in the fallback lists. The author added a flag NP_OPS_MEMPOLICY requiring NP_OPS_MIGRATION, updated sysfs 'has_memory' attribute, and modified mempolicy migration sites to use __GFP_PRIVATE.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private nodes want userland to directly allocate from the node\nvia set_mempolicy() and mbind() - but don't want that node as normal\nallocable system memory in the fallback lists.\n\nAdd NP_OPS_MEMPOLICY flag requiring NP_OPS_MIGRATION (since mbind can\ndrive migrations).  Only allow private nodes in policy nodemasks if\nall private nodes in the mask support NP_OPS_MEMPOLICY. This prevents\n__GFP_PRIVATE from unlocking nodes without NP_OPS_MEMPOLICY support.\n\nAdd __GFP_PRIVATE to mempolicy migration sites so moves to opted-in\nprivate nodes succeed.\n\nUpdate the sysfs \"has_memory\" attribute to include N_MEMORY_PRIVATE\nnodes with NP_OPS_MEMPOLICY set, allowing existing numactl userland\ntools to work without modification.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c            | 22 +++++++++++++-\n include/linux/node_private.h   | 40 +++++++++++++++++++++++++\n include/uapi/linux/mempolicy.h |  1 +\n mm/mempolicy.c                 | 54 ++++++++++++++++++++++++++++++----\n mm/page_alloc.c                |  5 ++++\n 5 files changed, 116 insertions(+), 6 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex e587f5781135..c08b5a948779 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -953,6 +953,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (!ops->migrate_to || !ops->folio_migrate))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\n@@ -1145,6 +1149,21 @@ static ssize_t show_node_state(struct device *dev,\n \t\t\t  nodemask_pr_args(&node_states[na->state]));\n }\n \n+/* has_memory includes N_MEMORY + N_MEMORY_PRIVATE that support mempolicy. */\n+static ssize_t show_has_memory(struct device *dev,\n+\t\t\t       struct device_attribute *attr, char *buf)\n+{\n+\tnodemask_t mask = node_states[N_MEMORY];\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_set(nid, mask);\n+\t}\n+\n+\treturn sysfs_emit(buf, \"%*pbl\\n\", nodemask_pr_args(&mask));\n+}\n+\n #define _NODE_ATTR(name, state) \\\n \t{ __ATTR(name, 0444, show_node_state, NULL), state }\n \n@@ -1155,7 +1174,8 @@ static struct node_attr node_state_attr[] = {\n #ifdef CONFIG_HIGHMEM\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n-\t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY] = { __ATTR(has_memory, 0444, show_has_memory, NULL),\n+\t\t       N_MEMORY },\n \t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 0c5be1ee6e60..e9b58afa366b 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -86,6 +86,8 @@ struct node_private_ops {\n \n /* Allow user/kernel migration; requires migrate_to and folio_migrate */\n #define NP_OPS_MIGRATION\t\tBIT(0)\n+/* Allow mempolicy-directed allocation and mbind migration to this node */\n+#define NP_OPS_MEMPOLICY\t\tBIT(1)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -276,6 +278,34 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \n \treturn ret;\n }\n+\n+static inline bool node_mpol_eligible(int nid)\n+{\n+\tbool ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_state(nid, N_MEMORY);\n+\n+\trcu_read_lock();\n+\tret = node_private_has_flag(nid, NP_OPS_MEMPOLICY);\n+\trcu_read_unlock();\n+\treturn ret;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\tint nid;\n+\tbool eligible = false;\n+\n+\tfor_each_node_mask(nid, *nodes) {\n+\t\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\t\tcontinue;\n+\t\tif (!node_mpol_eligible(nid))\n+\t\t\treturn false;\n+\t\teligible = true;\n+\t}\n+\treturn eligible;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -364,6 +394,16 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \treturn -ENODEV;\n }\n \n+static inline bool node_mpol_eligible(int nid)\n+{\n+\treturn false;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/include/uapi/linux/mempolicy.h b/include/uapi/linux/mempolicy.h\nindex 8fbbe613611a..b606eae983c8 100644\n--- a/include/uapi/linux/mempolicy.h\n+++ b/include/uapi/linux/mempolicy.h\n@@ -64,6 +64,7 @@ enum {\n #define MPOL_F_SHARED  (1 << 0)\t/* identify shared policies */\n #define MPOL_F_MOF\t(1 << 3) /* this policy wants migrate on fault */\n #define MPOL_F_MORON\t(1 << 4) /* Migrate On protnone Reference On Node */\n+#define MPOL_F_PRIVATE\t(1 << 5) /* policy targets private node; use __GFP_PRIVATE */\n \n /*\n  * Enabling zone reclaim means the page allocator will attempt to fulfill\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 2b0f9762d171..8ac014950e88 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -406,8 +406,6 @@ static int mpol_new_preferred(struct mempolicy *pol, const nodemask_t *nodes)\n static int mpol_set_nodemask(struct mempolicy *pol,\n \t\t     const nodemask_t *nodes, struct nodemask_scratch *nsc)\n {\n-\tint ret;\n-\n \t/*\n \t * Default (pol==NULL) resp. local memory policies are not a\n \t * subject of any remapping. They also do not need any special\n@@ -416,9 +414,12 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \tif (!pol || pol->mode == MPOL_LOCAL)\n \t\treturn 0;\n \n-\t/* Check N_MEMORY */\n+\t/* Check N_MEMORY and N_MEMORY_PRIVATE*/\n \tnodes_and(nsc->mask1,\n \t\t  cpuset_current_mems_allowed, node_states[N_MEMORY]);\n+\tnodes_and(nsc->mask2, cpuset_current_mems_allowed,\n+\t\t  node_states[N_MEMORY_PRIVATE]);\n+\tnodes_or(nsc->mask1, nsc->mask1, nsc->mask2);\n \n \tVM_BUG_ON(!nodes);\n \n@@ -432,8 +433,13 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \telse\n \t\tpol->w.cpuset_mems_allowed = cpuset_current_mems_allowed;\n \n-\tret = mpol_ops[pol->mode].create(pol, &nsc->mask2);\n-\treturn ret;\n+\t/* All private nodes in the mask must have NP_OPS_MEMPOLICY. */\n+\tif (nodes_private_mpol_allowed(&nsc->mask2))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse if (nodes_intersects(nsc->mask2, node_states[N_MEMORY_PRIVATE]))\n+\t\treturn -EINVAL;\n+\n+\treturn mpol_ops[pol->mode].create(pol, &nsc->mask2);\n }\n \n /*\n@@ -500,6 +506,7 @@ static void mpol_rebind_default(struct mempolicy *pol, const nodemask_t *nodes)\n static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n {\n \tnodemask_t tmp;\n+\tint nid;\n \n \tif (pol->flags & MPOL_F_STATIC_NODES)\n \t\tnodes_and(tmp, pol->w.user_nodemask, *nodes);\n@@ -514,6 +521,21 @@ static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n \tif (nodes_empty(tmp))\n \t\ttmp = *nodes;\n \n+\t/*\n+\t * Drop private nodes that don't have mempolicy support.\n+\t * cpusets guarantees at least one N_MEMORY node in effective_mems\n+\t * and mems_allowed, so dropping private nodes here is safe.\n+\t */\n+\tfor_each_node_mask(nid, tmp) {\n+\t\tif (node_state(nid, N_MEMORY_PRIVATE) &&\n+\t\t    !node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_clear(nid, tmp);\n+\t}\n+\tif (nodes_intersects(tmp, node_states[N_MEMORY_PRIVATE]))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse\n+\t\tpol->flags &= ~MPOL_F_PRIVATE;\n+\n \tpol->nodes = tmp;\n }\n \n@@ -661,6 +683,9 @@ static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)\n \t}\n \tif (!queue_folio_required(folio, qp))\n \t\treturn;\n+\tif (folio_is_private_node(folio) &&\n+\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\treturn;\n \tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||\n \t    !vma_migratable(walk->vma) ||\n \t    !migrate_folio_add(folio, qp->pagelist, qp->flags))\n@@ -717,6 +742,9 @@ static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n \t\tif (!folio || folio_is_zone_device(folio))\n \t\t\tcontinue;\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\t\tcontinue;\n \t\tif (folio_test_large(folio) && max_nr != 1)\n \t\t\tnr = folio_pte_batch(folio, pte, ptent, max_nr);\n \t\t/*\n@@ -1451,6 +1479,9 @@ static struct folio *alloc_migration_target_by_mpol(struct folio *src,\n \telse\n \t\tgfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL | __GFP_COMP;\n \n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \treturn folio_alloc_mpol(gfp, order, pol, ilx, nid);\n }\n #else\n@@ -2280,6 +2311,15 @@ static nodemask_t *policy_nodemask(gfp_t gfp, struct mempolicy *pol,\n \t\t\tnodemask = &pol->nodes;\n \t\tif (pol->home_node != NUMA_NO_NODE)\n \t\t\t*nid = pol->home_node;\n+\t\telse if ((pol->flags & MPOL_F_PRIVATE) &&\n+\t\t\t !node_isset(*nid, pol->nodes)) {\n+\t\t\t/*\n+\t\t\t * Private nodes are not in N_MEMORY nodes' zonelists.\n+\t\t\t * When the preferred nid (usually numa_node_id()) can't\n+\t\t\t * reach the policy nodes, start from a policy node.\n+\t\t\t */\n+\t\t\t*nid = first_node(pol->nodes);\n+\t\t}\n \t\t/*\n \t\t * __GFP_THISNODE shouldn't even be used with the bind policy\n \t\t * because we might easily break the expectation to stay on the\n@@ -2533,6 +2573,10 @@ struct folio *vma_alloc_folio_noprof(gfp_t gfp, int order, struct vm_area_struct\n \t\tgfp |= __GFP_NOWARN;\n \n \tpol = get_vma_policy(vma, addr, order, &ilx);\n+\n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \tfolio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n \tmpol_cond_put(pol);\n \treturn folio;\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 5a1b35421d78..ec6c1f8e85d8 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3849,8 +3849,13 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\t * if another process has NUMA bindings and is causing\n \t\t * kswapd wakeups on only some nodes. Avoid accidental\n \t\t * \"node_reclaim_mode\"-like behavior in this case.\n+\t\t *\n+\t\t * Nodes without kswapd (some private nodes) are never\n+\t\t * skipped - this causes some mempolicies to silently\n+\t\t * fall back to DRAM even if the node is eligible.\n \t\t */\n \t\tif (skip_kswapd_nodes &&\n+\t\t    zone->zone_pgdat->kswapd &&\n \t\t    !waitqueue_active(&zone->zone_pgdat->kswapd_wait)) {\n \t\t\tskipped_kswapd_nodes = true;\n \t\t\tcontinue;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-14-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the memory-tier subsystem needing to know which private nodes should appear as demotion targets, and responded by explaining that they added NP_OPS_DEMOTION (BIT(2)) to allow private nodes to be added as demotion targets, implemented backpressure support for private nodes to reject new demotions cleanly, and modified the demotion path to try demotion to private nodes individually before clearing them from the demotion target mask. The author also mentioned that they should re-do the demotion logic in a future version to allow less fallback and kick kswapd instead of inducing LRU inversions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "explained their approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The memory-tier subsystem needs to know which private nodes should\nappear as demotion targets.\n\nAdd NP_OPS_DEMOTION (BIT(2)):\n   Node can be added as a demotion target by memory-tiers.\n\nAdd demotion backpressure support so private nodes can reject\nnew demotions cleanly, allowing vmscan to fall back to swap.\n\nIn the demotion path, try demotion to private nodes invididually,\nthen clear private nodes from the demotion target mask until a\nnon-private node is found, then fall back to the remaining mask.\nThis prevents LRU inversion while still allowing forward progress.\n\nThis is the closest match to the current behavior without making\nprivate nodes inaccessible or preventing forward progress. We\nshould probably completely re-do the demotion logic to allow less\nfallback and kick kswapd instead - right now we induce LRU\ninversions by simply falling back to any node in the demotion list.\n\nAdd memory_tier_refresh_demotion() export for services to trigger\nre-evaluation of demotion targets after changing their flags.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory-tiers.h |  9 +++++++\n include/linux/node_private.h | 22 +++++++++++++++++\n mm/internal.h                |  7 ++++++\n mm/memory-tiers.c            | 46 ++++++++++++++++++++++++++++++++----\n mm/page_alloc.c              | 12 +++++++---\n mm/vmscan.c                  | 30 ++++++++++++++++++++++-\n 6 files changed, 117 insertions(+), 9 deletions(-)\n\ndiff --git a/include/linux/memory-tiers.h b/include/linux/memory-tiers.h\nindex 3e1159f6762c..e1476432e359 100644\n--- a/include/linux/memory-tiers.h\n+++ b/include/linux/memory-tiers.h\n@@ -58,6 +58,7 @@ struct memory_dev_type *mt_get_memory_type(int adist);\n int next_demotion_node(int node);\n void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets);\n bool node_is_toptier(int node);\n+void memory_tier_refresh_demotion(void);\n #else\n static inline int next_demotion_node(int node)\n {\n@@ -73,6 +74,10 @@ static inline bool node_is_toptier(int node)\n {\n \treturn true;\n }\n+\n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n #endif\n \n #else\n@@ -106,6 +111,10 @@ static inline bool node_is_toptier(int node)\n \treturn true;\n }\n \n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n+\n static inline int register_mt_adistance_algorithm(struct notifier_block *nb)\n {\n \treturn 0;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e9b58afa366b..e254e36056cd 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -88,6 +88,8 @@ struct node_private_ops {\n #define NP_OPS_MIGRATION\t\tBIT(0)\n /* Allow mempolicy-directed allocation and mbind migration to this node */\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n+/* Node participates as a demotion target in memory-tiers */\n+#define NP_OPS_DEMOTION\t\t\tBIT(2)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -101,12 +103,14 @@ struct node_private_ops {\n  *\t\tcallbacks that may sleep; 0 = fully released)\n  * @released: Signaled when refcount drops to 0; unregister waits on this\n  * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ * @migration_blocked: Service signals migrations should pause\n  */\n struct node_private {\n \tvoid *owner;\n \trefcount_t refcount;\n \tstruct completion released;\n \tconst struct node_private_ops *ops;\n+\tbool migration_blocked;\n };\n \n #ifdef CONFIG_NUMA\n@@ -306,6 +310,19 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \t}\n \treturn eligible;\n }\n+\n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\tstruct node_private *np;\n+\tbool blocked;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tblocked = np && READ_ONCE(np->migration_blocked);\n+\trcu_read_unlock();\n+\n+\treturn blocked;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -404,6 +421,11 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \treturn false;\n }\n \n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 6ab4679fe943..5950e20d4023 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t  const nodemask_t *candidates);\n extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t      gfp_t gfp_mask);\n #else\n@@ -1220,6 +1222,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t\t const nodemask_t *candidates)\n+{\n+\treturn NUMA_NO_NODE;\n+}\n static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t\t     gfp_t gfp_mask)\n {\ndiff --git a/mm/memory-tiers.c b/mm/memory-tiers.c\nindex 9c742e18e48f..434190fdc078 100644\n--- a/mm/memory-tiers.c\n+++ b/mm/memory-tiers.c\n@@ -3,6 +3,7 @@\n #include <linux/lockdep.h>\n #include <linux/sysfs.h>\n #include <linux/kobject.h>\n+#include <linux/node_private.h>\n #include <linux/memory.h>\n #include <linux/memory-tiers.h>\n #include <linux/notifier.h>\n@@ -380,6 +381,8 @@ static void disable_all_demotion_targets(void)\n \t\tif (memtier)\n \t\t\tmemtier->lower_tier_mask = NODE_MASK_NONE;\n \t}\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE)\n+\t\tnode_demotion[node].preferred = NODE_MASK_NONE;\n \t/*\n \t * Ensure that the \"disable\" is visible across the system.\n \t * Readers will see either a combination of before+disable\n@@ -421,6 +424,7 @@ static void establish_demotion_targets(void)\n \tint target = NUMA_NO_NODE, node;\n \tint distance, best_distance;\n \tnodemask_t tier_nodes, lower_tier;\n+\tnodemask_t all_memory;\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n@@ -429,6 +433,13 @@ static void establish_demotion_targets(void)\n \n \tdisable_all_demotion_targets();\n \n+\t/* Include private nodes that have opted in to demotion. */\n+\tall_memory = node_states[N_MEMORY];\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(node, NP_OPS_DEMOTION))\n+\t\t\tnode_set(node, all_memory);\n+\t}\n+\n \tfor_each_node_state(node, N_MEMORY) {\n \t\tbest_distance = -1;\n \t\tnd = &node_demotion[node];\n@@ -442,12 +453,12 @@ static void establish_demotion_targets(void)\n \t\tmemtier = list_next_entry(memtier, list);\n \t\ttier_nodes = get_memtier_nodemask(memtier);\n \t\t/*\n-\t\t * find_next_best_node, use 'used' nodemask as a skip list.\n+\t\t * find_next_best_node_in, use 'used' nodemask as a skip list.\n \t\t * Add all memory nodes except the selected memory tier\n \t\t * nodelist to skip list so that we find the best node from the\n \t\t * memtier nodelist.\n \t\t */\n-\t\tnodes_andnot(tier_nodes, node_states[N_MEMORY], tier_nodes);\n+\t\tnodes_andnot(tier_nodes, all_memory, tier_nodes);\n \n \t\t/*\n \t\t * Find all the nodes in the memory tier node list of same best distance.\n@@ -455,7 +466,8 @@ static void establish_demotion_targets(void)\n \t\t * in the preferred mask when allocating pages during demotion.\n \t\t */\n \t\tdo {\n-\t\t\ttarget = find_next_best_node(node, &tier_nodes);\n+\t\t\ttarget = find_next_best_node_in(node, &tier_nodes,\n+\t\t\t\t\t\t\t&all_memory);\n \t\t\tif (target == NUMA_NO_NODE)\n \t\t\t\tbreak;\n \n@@ -495,7 +507,7 @@ static void establish_demotion_targets(void)\n \t * allocation to a set of nodes that is closer the above selected\n \t * preferred node.\n \t */\n-\tlower_tier = node_states[N_MEMORY];\n+\tlower_tier = all_memory;\n \tlist_for_each_entry(memtier, &memory_tiers, list) {\n \t\t/*\n \t\t * Keep removing current tier from lower_tier nodes,\n@@ -542,7 +554,7 @@ static struct memory_tier *set_node_memory_tier(int node)\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n-\tif (!node_state(node, N_MEMORY))\n+\tif (!node_state(node, N_MEMORY) && !node_state(node, N_MEMORY_PRIVATE))\n \t\treturn ERR_PTR(-EINVAL);\n \n \tmt_calc_adistance(node, &adist);\n@@ -865,6 +877,30 @@ int mt_calc_adistance(int node, int *adist)\n }\n EXPORT_SYMBOL_GPL(mt_calc_adistance);\n \n+/**\n+ * memory_tier_refresh_demotion() - Re-establish demotion targets\n+ *\n+ * Called by services after registering or unregistering ops->migrate_to on\n+ * a private node, so that establish_demotion_targets() picks up the change.\n+ */\n+void memory_tier_refresh_demotion(void)\n+{\n+\tint nid;\n+\n+\tmutex_lock(&memory_tier_lock);\n+\t/*\n+\t * Ensure private nodes are registered with a tier, otherwise\n+\t * they won't show up in any node's demotion targets nodemask.\n+\t */\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (!__node_get_memory_tier(nid))\n+\t\t\tset_node_memory_tier(nid);\n+\t}\n+\testablish_demotion_targets();\n+\tmutex_unlock(&memory_tier_lock);\n+}\n+EXPORT_SYMBOL_GPL(memory_tier_refresh_demotion);\n+\n static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\t\t\t\t      unsigned long action, void *_arg)\n {\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex ec6c1f8e85d8..e272dfdc6b00 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5589,7 +5589,8 @@ static int node_load[MAX_NUMNODES];\n  *\n  * Return: node id of the found node or %NUMA_NO_NODE if no node is found.\n  */\n-int find_next_best_node(int node, nodemask_t *used_node_mask)\n+int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t   const nodemask_t *candidates)\n {\n \tint n, val;\n \tint min_val = INT_MAX;\n@@ -5599,12 +5600,12 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \t * Use the local node if we haven't already, but for memoryless local\n \t * node, we should skip it and fall back to other nodes.\n \t */\n-\tif (!node_isset(node, *used_node_mask) && node_state(node, N_MEMORY)) {\n+\tif (!node_isset(node, *used_node_mask) && node_isset(node, *candidates)) {\n \t\tnode_set(node, *used_node_mask);\n \t\treturn node;\n \t}\n \n-\tfor_each_node_state(n, N_MEMORY) {\n+\tfor_each_node_mask(n, *candidates) {\n \n \t\t/* Don't want a node to appear more than once */\n \t\tif (node_isset(n, *used_node_mask))\n@@ -5636,6 +5637,11 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \treturn best_node;\n }\n \n+int find_next_best_node(int node, nodemask_t *used_node_mask)\n+{\n+\treturn find_next_best_node_in(node, used_node_mask,\n+\t\t\t\t      &node_states[N_MEMORY]);\n+}\n \n /*\n  * Build zonelists ordered by node and zones within node.\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 6113be4d3519..0f534428ea88 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -355,6 +356,10 @@ static bool can_demote(int nid, struct scan_control *sc,\n \tif (demotion_nid == NUMA_NO_NODE)\n \t\treturn false;\n \n+\t/* Don't demote when the target's service signals backpressure */\n+\tif (node_private_migration_blocked(demotion_nid))\n+\t\treturn false;\n+\n \t/* If demotion node isn't in the cgroup's mems_allowed, fall back */\n \treturn mem_cgroup_node_allowed(memcg, demotion_nid);\n }\n@@ -1022,8 +1027,10 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \t\t\t\t     struct pglist_data *pgdat)\n {\n \tint target_nid = next_demotion_node(pgdat->node_id);\n-\tunsigned int nr_succeeded;\n+\tint first_nid = target_nid;\n+\tunsigned int nr_succeeded = 0;\n \tnodemask_t allowed_mask;\n+\tint ret;\n \n \tstruct migration_target_control mtc = {\n \t\t/*\n@@ -1046,6 +1053,27 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \n \tnode_get_allowed_targets(pgdat, &allowed_mask);\n \n+\t/* Try private node targets until we find non-private node */\n+\twhile (node_state(target_nid, N_MEMORY_PRIVATE)) {\n+\t\tunsigned int nr = 0;\n+\n+\t\tret = node_private_migrate_to(demote_folios, target_nid,\n+\t\t\t\t\t      MIGRATE_ASYNC, MR_DEMOTION,\n+\t\t\t\t\t      &nr);\n+\t\tnr_succeeded += nr;\n+\t\tif (ret == 0 || list_empty(demote_folios))\n+\t\t\treturn nr_succeeded;\n+\n+\t\ttarget_nid = next_node_in(target_nid, allowed_mask);\n+\t\tif (target_nid == first_nid)\n+\t\t\treturn nr_succeeded;\n+\t\tif (!node_state(target_nid, N_MEMORY_PRIVATE))\n+\t\t\tbreak;\n+\t}\n+\n+\t/* target_nid is a non-private node; use standard migration */\n+\tmtc.nid = target_nid;\n+\n \t/* Demotion ignores all cpuset and mempolicy settings */\n \tmigrate_pages(demote_folios, alloc_demote_folio, NULL,\n \t\t      (unsigned long)&mtc, MIGRATE_ASYNC, MR_DEMOTION,\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-15-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about preventing mprotect from silently upgrading PTEs to writable on private nodes by adding NP_OPS_PROTECT_WRITE and suppressing write-upgrade in change_pte_range() and change_huge_pmd(). They also added a handle_fault callback to the node's ops structure, allowing services to handle write faults with promotion or other custom logic. The author acknowledged that NP_OPS_MEMPOLICY is incompatible with NP_OPS_PROTECT_WRITE.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical issue",
                "provided additional code changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Services that intercept write faults (e.g., for promotion tracking)\nneed PTEs to stay read-only. This requires preventing mprotect\nfrom silently upgrade the PTE, bypassing the service's handle_fault\ncallback.\n\nAdd NP_OPS_PROTECT_WRITE and folio_managed_wrprotect().\n\nIn change_pte_range() and change_huge_pmd(), suppress PTE write-upgrade\nwhen MM_CP_TRY_CHANGE_WRITABLE is sees the folio is write-protected.\n\nIn handle_pte_fault() and do_huge_pmd_wp_page(), dispatch to the node's\nops->handle_fault callback when set, allowing the service to handle write\nfaults with promotion or other custom logic.\n\nNP_OPS_MEMPOLICY is incompatible with NP_OPS_PROTECT_WRITE to avoid the\nfootgun of binding a writable VMA to a write-protected node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++\n include/linux/node_private.h | 22 ++++++++\n mm/huge_memory.c             | 17 ++++++-\n mm/internal.h                | 99 ++++++++++++++++++++++++++++++++++++\n mm/memory.c                  | 15 ++++++\n mm/migrate.c                 | 14 +----\n mm/mprotect.c                |  4 +-\n 7 files changed, 159 insertions(+), 16 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex c08b5a948779..a4955b9b5b93 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -957,6 +957,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    (ops->flags & NP_OPS_PROTECT_WRITE))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e254e36056cd..27d6e5d84e61 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -70,6 +70,24 @@ struct vm_fault;\n  *     PFN-based metadata (compression tables, device page tables, DMA\n  *     mappings, etc.) before any access through the page tables.\n  *\n+ * @handle_fault: Handle fault on folio on this private node.\n+ *   [folio-referenced callback, PTL held on entry]\n+ *\n+ *   Called from handle_pte_fault() (PTE level) or do_huge_pmd_wp_page()\n+ *   (PMD level) after lock acquisition and entry verification.\n+ *   @folio is the faulting folio, @level indicates the page table level.\n+ *\n+ *   For PGTABLE_LEVEL_PTE: vmf->pte is mapped and vmf->ptl is the\n+ *   PTE lock.  Release via pte_unmap_unlock(vmf->pte, vmf->ptl).\n+ *\n+ *   For PGTABLE_LEVEL_PMD: vmf->pte is NULL and vmf->ptl is the\n+ *   PMD lock.  Release via spin_unlock(vmf->ptl).\n+ *\n+ *   The callback MUST release PTL on ALL paths.\n+ *   The caller will NOT touch the page table entry after this returns.\n+ *\n+ *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -81,6 +99,8 @@ struct node_private_ops {\n \t\t\t\t  enum migrate_reason reason,\n \t\t\t\t  unsigned int *nr_succeeded);\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n+\tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t   enum pgtable_level level);\n \tunsigned long flags;\n };\n \n@@ -90,6 +110,8 @@ struct node_private_ops {\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n /* Node participates as a demotion target in memory-tiers */\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n+/* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n+#define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 2ecae494291a..d9ba6593244d 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -2063,12 +2063,14 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tstruct page *page;\n \tunsigned long haddr = vmf->address & HPAGE_PMD_MASK;\n \tpmd_t orig_pmd = vmf->orig_pmd;\n+\tvm_fault_t ret;\n+\n \n \tvmf->ptl = pmd_lockptr(vma->vm_mm, vmf->pmd);\n \tVM_BUG_ON_VMA(!vma->anon_vma, vma);\n \n \tif (is_huge_zero_pmd(orig_pmd)) {\n-\t\tvm_fault_t ret = do_huge_zero_wp_pmd(vmf);\n+\t\tret = do_huge_zero_wp_pmd(vmf);\n \n \t\tif (!(ret & VM_FAULT_FALLBACK))\n \t\t\treturn ret;\n@@ -2088,6 +2090,13 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tfolio = page_folio(page);\n \tVM_BUG_ON_PAGE(!PageHead(page), page);\n \n+\t/* Private-managed write-protect: let the service handle the fault */\n+\tif (unlikely(folio_is_private_managed(folio))) {\n+\t\tif (folio_managed_handle_fault(folio, vmf,\n+\t\t\t\t\t      PGTABLE_LEVEL_PMD, &ret))\n+\t\t\treturn ret;\n+\t}\n+\n \t/* Early check when only holding the PT lock. */\n \tif (PageAnonExclusive(page))\n \t\tgoto reuse;\n@@ -2633,7 +2642,8 @@ int change_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,\n \n \t/* See change_pte_range(). */\n \tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) && !pmd_write(entry) &&\n-\t    can_change_pmd_writable(vma, addr, entry))\n+\t    can_change_pmd_writable(vma, addr, entry) &&\n+\t    !folio_managed_wrprotect(pmd_folio(entry)))\n \t\tentry = pmd_mkwrite(entry, vma);\n \n \tret = HPAGE_PMD_NR;\n@@ -4943,6 +4953,9 @@ void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n \tif (folio_test_dirty(folio) && softleaf_is_migration_dirty(entry))\n \t\tpmde = pmd_mkdirty(pmde);\n \n+\tif (folio_managed_wrprotect(folio))\n+\t\tpmde = pmd_wrprotect(pmde);\n+\n \tif (folio_is_device_private(folio)) {\n \t\tswp_entry_t entry;\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 5950e20d4023..ae4ff86e8dc6 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -11,6 +11,7 @@\n #include <linux/khugepaged.h>\n #include <linux/mm.h>\n #include <linux/mm_inline.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/pagewalk.h>\n #include <linux/rmap.h>\n@@ -18,6 +19,7 @@\n #include <linux/leafops.h>\n #include <linux/swap_cgroup.h>\n #include <linux/tracepoint-defs.h>\n+#include <linux/node_private.h>\n \n /* Internal core VMA manipulation functions. */\n #include \"vma.h\"\n@@ -1449,6 +1451,103 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/*\n+ * folio_managed_handle_fault - Dispatch fault on managed-memory folio\n+ * @folio: the faulting folio (must not be NULL)\n+ * @vmf: the vm_fault descriptor (PTL held: vmf->ptl locked)\n+ * @level: page table level (PGTABLE_LEVEL_PTE or PGTABLE_LEVEL_PMD)\n+ * @ret: output fault result if handled\n+ *\n+ * Called with PTL held.  If a handle_fault callback exists, it is invoked\n+ * with PTL still held.  The callback is responsible for releasing PTL on\n+ * all paths.\n+ *\n+ * Returns true if the service handled the fault (PTL released by callback,\n+ * caller returns *ret).  Returns false if no handler exists (PTL still held,\n+ * caller continues with normal fault handling).\n+ */\n+static inline bool folio_managed_handle_fault(struct folio *folio,\n+\t\t\t\t\t      struct vm_fault *vmf,\n+\t\t\t\t\t      enum pgtable_level level,\n+\t\t\t\t\t      vm_fault_t *ret)\n+{\n+\t/* Zone device pages use swap entries; handled in do_swap_page */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->handle_fault) {\n+\t\t\t*ret = ops->handle_fault(folio, vmf, level);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n+/**\n+ * folio_managed_wrprotect - Should this folio's mappings stay write-protected?\n+ * @folio: the folio to check\n+ *\n+ * Returns true if the folio is on a private node with NP_OPS_PROTECT_WRITE,\n+ * meaning page table entries (PTE or PMD) should not be made writable.\n+ * Write faults are intercepted by the service's handle_fault callback\n+ * to promote the folio to DRAM.\n+ *\n+ * Used by:\n+ *   - change_pte_range() / change_huge_pmd(): prevent mprotect write-upgrade\n+ *   - remove_migration_pte() / remove_migration_pmd(): strip write after migration\n+ *   - do_huge_pmd_wp_page(): dispatch to fault handler instead of reuse\n+ */\n+static inline bool folio_managed_wrprotect(struct folio *folio)\n+{\n+\treturn unlikely(folio_is_private_node(folio) &&\n+\t\t\tfolio_private_flags(folio, NP_OPS_PROTECT_WRITE));\n+}\n+\n+/**\n+ * folio_managed_fixup_migration_pte - Fixup PTE after migration for\n+ *                                     managed memory pages.\n+ * @new: the destination page\n+ * @pte: the PTE being installed (normal PTE built by caller)\n+ * @old_pte: the original PTE (before migration, for swap entry flags)\n+ * @vma: the VMA\n+ *\n+ * For MEMORY_DEVICE_PRIVATE pages: replaces the PTE with a device-private\n+ * swap entry, preserving soft_dirty and uffd_wp from old_pte.\n+ *\n+ * For N_MEMORY_PRIVATE pages with NP_OPS_PROTECT_WRITE: strips the write\n+ * bit so the next write triggers the fault handler for promotion.\n+ *\n+ * For normal pages: returns pte unmodified.\n+ */\n+static inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n+\t\t\t\t\t\t      pte_t pte,\n+\t\t\t\t\t\t      pte_t old_pte,\n+\t\t\t\t\t\t      struct vm_area_struct *vma)\n+{\n+\tif (unlikely(is_device_private_page(new))) {\n+\t\tsoftleaf_t entry;\n+\n+\t\tif (pte_write(pte))\n+\t\t\tentry = make_writable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\telse\n+\t\t\tentry = make_readable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\tpte = softleaf_to_pte(entry);\n+\t\tif (pte_swp_soft_dirty(old_pte))\n+\t\t\tpte = pte_swp_mksoft_dirty(pte);\n+\t\tif (pte_swp_uffd_wp(old_pte))\n+\t\t\tpte = pte_swp_mkuffd_wp(pte);\n+\t} else if (folio_managed_wrprotect(page_folio(new))) {\n+\t\tpte = pte_wrprotect(pte);\n+\t}\n+\treturn pte;\n+}\n+\n /**\n  * folio_managed_migrate_notify - Notify service that a folio changed location\n  * @src: the old folio (about to be freed)\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 2a55edc48a65..0f78988befef 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -6079,6 +6079,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n \t */\n+\tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n+\t\twritable = false;\n+\t\tignore_writable = true;\n+\t}\n \tif (folio && folio_test_large(folio))\n \t\tnuma_rebuild_large_mapping(vmf, vma, folio, pte, ignore_writable,\n \t\t\t\t\t   pte_write_upgrade);\n@@ -6228,6 +6232,7 @@ static void fix_spurious_fault(struct vm_fault *vmf,\n  */\n static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n {\n+\tstruct folio *folio;\n \tpte_t entry;\n \n \tif (unlikely(pmd_none(*vmf->pmd))) {\n@@ -6284,6 +6289,16 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n \t\tupdate_mmu_tlb(vmf->vma, vmf->address, vmf->pte);\n \t\tgoto unlock;\n \t}\n+\n+\tfolio = vm_normal_folio(vmf->vma, vmf->address, entry);\n+\tif (unlikely(folio && folio_is_private_managed(folio))) {\n+\t\tvm_fault_t fault_ret;\n+\n+\t\tif (folio_managed_handle_fault(folio, vmf, PGTABLE_LEVEL_PTE,\n+\t\t\t\t\t       &fault_ret))\n+\t\t\treturn fault_ret;\n+\t}\n+\n \tif (vmf->flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {\n \t\tif (!pte_write(entry))\n \t\t\treturn do_wp_page(vmf);\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex a54d4af04df3..f632e8b03504 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -398,19 +398,7 @@ static bool remove_migration_pte(struct folio *folio,\n \t\tif (folio_test_anon(folio) && !softleaf_is_migration_read(entry))\n \t\t\trmap_flags |= RMAP_EXCLUSIVE;\n \n-\t\tif (unlikely(is_device_private_page(new))) {\n-\t\t\tif (pte_write(pte))\n-\t\t\t\tentry = make_writable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\telse\n-\t\t\t\tentry = make_readable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\tpte = softleaf_to_pte(entry);\n-\t\t\tif (pte_swp_soft_dirty(old_pte))\n-\t\t\t\tpte = pte_swp_mksoft_dirty(pte);\n-\t\t\tif (pte_swp_uffd_wp(old_pte))\n-\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n-\t\t}\n+\t\tpte = folio_managed_fixup_migration_pte(new, pte, old_pte, vma);\n \n #ifdef CONFIG_HUGETLB_PAGE\n \t\tif (folio_test_hugetlb(folio)) {\ndiff --git a/mm/mprotect.c b/mm/mprotect.c\nindex 283889e4f1ce..830be609bc24 100644\n--- a/mm/mprotect.c\n+++ b/mm/mprotect.c\n@@ -30,6 +30,7 @@\n #include <linux/mm_inline.h>\n #include <linux/pgtable.h>\n #include <linux/userfaultfd_k.h>\n+#include <linux/node_private.h>\n #include <uapi/linux/mman.h>\n #include <asm/cacheflush.h>\n #include <asm/mmu_context.h>\n@@ -290,7 +291,8 @@ static long change_pte_range(struct mmu_gather *tlb,\n \t\t\t * COW or special handling is required.\n \t\t\t */\n \t\t\tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) &&\n-\t\t\t     !pte_write(ptent))\n+\t\t\t     !pte_write(ptent) &&\n+\t\t\t     !(folio && folio_managed_wrprotect(folio)))\n \t\t\t\tset_write_prot_commit_flush_ptes(vma, folio, page,\n \t\t\t\taddr, pte, oldpte, ptent, nr_ptes, tlb);\n \t\t\telse\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-16-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author addressed a concern about boosted reclaim suppressing may_swap and may_writepage, which prevents kswapd from making progress when demotion is not possible. The author added a reclaim_policy callback to struct node_private_ops and a struct node_reclaim_policy to allow private nodes to override the normal boost policy and control reclaim on their pages.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "added new code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services that drive kswapd via watermark_boost need\ncontrol over the reclaim policy.  There are three problems:\n\n1) Boosted reclaim suppresses may_swap and may_writepage.  When\n   demotion is not possible, swap is the only evict path, so kswapd\n   cannot make progress and pages are stranded.\n\n2) __setup_per_zone_wmarks() unconditionally zeros watermark_boost,\n   killing the service's pressure signal.\n\n3) Not all private nodes want reclaim to touch their pages.\n\nAdd a reclaim_policy callback to struct node_private_ops and a\nstruct node_reclaim_policy with:\n\n  - active:             set by the helper when a callback was invoked\n  - may_swap:           allow swap writeback during boosted reclaim\n  - may_writepage:      allow writepage during boosted reclaim\n  - managed_watermarks: service owns watermark_boost lifecycle\n\nWe do not allow disabling swap/writepage, as core MM may have\nexplicitly enabled them on a non-boosted pass.\n\nWe only allow enablign swap/writepage, so that the supression during\na boost can be overridden.  This allows a device to force evictions\neven when the system otherwise would not percieve pressure.\n\nThis is important for a service like compressed RAM, as device capacity\nmay differ from reported capacity, and device may want to relieve real\npressure (poor compression ratio) as opposed to percieved pressure\n(i.e. how many pages are in use).\n\nAdd zone_reclaim_allowed() to filter private nodes that have not\nopted into reclaim.\n\nRegular nodes fall through to cpuset_zone_allowed() unchanged.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 28 ++++++++++++++++++++++++++++\n mm/internal.h                | 36 ++++++++++++++++++++++++++++++++++++\n mm/page_alloc.c              | 11 ++++++++++-\n mm/vmscan.c                  | 25 +++++++++++++++++++++++--\n 4 files changed, 97 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 27d6e5d84e61..34be52383255 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -14,6 +14,24 @@ struct page;\n struct vm_area_struct;\n struct vm_fault;\n \n+/**\n+ * struct node_reclaim_policy - Reclaim policy overrides for private nodes\n+ * @active: set by node_private_reclaim_policy() when a callback was invoked\n+ * @may_swap: allow swap writeback during boosted reclaim\n+ * @may_writepage: allow writepage during boosted reclaim\n+ * @managed_watermarks: service owns watermark_boost lifecycle; kswapd must\n+ *                      not clear it after boosted reclaim\n+ *\n+ * Passed to the reclaim_policy callback so each private node service can\n+ * inject its own reclaim policy before kswapd runs boosted reclaim.\n+ */\n+struct node_reclaim_policy {\n+\tbool active;\n+\tbool may_swap;\n+\tbool may_writepage;\n+\tbool managed_watermarks;\n+};\n+\n /**\n  * struct node_private_ops - Callbacks for private node services\n  *\n@@ -88,6 +106,13 @@ struct vm_fault;\n  *\n  *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n  *\n+ * @reclaim_policy: Configure reclaim policy for boosted reclaim.\n+ *   [called hodling rcu_read_lock, MUST NOT sleep]\n+ *   Called by kswapd before boosted reclaim to let the service override\n+ *   may_swap / may_writepage.  If provided, the service also owns the\n+ *   watermark_boost lifecycle (kswapd will not clear it).\n+ *   If NULL, normal boost policy applies.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -101,6 +126,7 @@ struct node_private_ops {\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n+\tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n \tunsigned long flags;\n };\n \n@@ -112,6 +138,8 @@ struct node_private_ops {\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n /* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n+/* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n+#define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/internal.h b/mm/internal.h\nindex ae4ff86e8dc6..db32cb2d7a29 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1572,6 +1572,42 @@ static inline void folio_managed_migrate_notify(struct folio *src,\n \t\tops->folio_migrate(src, dst);\n }\n \n+/**\n+ * node_private_reclaim_policy - invoke the service's reclaim policy callback\n+ * @nid: NUMA node id\n+ * @policy: reclaim policy struct to fill in\n+ *\n+ * Called by kswapd before boosted reclaim.  Zeroes @policy, then if the\n+ * private node service provides a reclaim_policy callback, invokes it\n+ * and sets policy->active to true.\n+ */\n+#ifdef CONFIG_NUMA\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tstruct node_private *np;\n+\n+\tmemset(policy, 0, sizeof(*policy));\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (np && np->ops && np->ops->reclaim_policy) {\n+\t\tnp->ops->reclaim_policy(nid, policy);\n+\t\tpolicy->active = true;\n+\t}\n+\trcu_read_unlock();\n+}\n+#else\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tmemset(policy, 0, sizeof(*policy));\n+}\n+#endif\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e272dfdc6b00..9692048ab5fb 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -55,6 +55,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/node_private.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -6437,6 +6438,8 @@ static void __setup_per_zone_wmarks(void)\n \tunsigned long lowmem_pages = 0;\n \tstruct zone *zone;\n \tunsigned long flags;\n+\tstruct node_reclaim_policy rp;\n+\tint prev_nid = NUMA_NO_NODE;\n \n \t/* Calculate total number of !ZONE_HIGHMEM and !ZONE_MOVABLE pages */\n \tfor_each_zone(zone) {\n@@ -6446,6 +6449,7 @@ static void __setup_per_zone_wmarks(void)\n \n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n+\t\tint nid = zone_to_nid(zone);\n \n \t\tspin_lock_irqsave(&zone->lock, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n@@ -6482,7 +6486,12 @@ static void __setup_per_zone_wmarks(void)\n \t\t\t    mult_frac(zone_managed_pages(zone),\n \t\t\t\t      watermark_scale_factor, 10000));\n \n-\t\tzone->watermark_boost = 0;\n+\t\tif (nid != prev_nid) {\n+\t\t\tnode_private_reclaim_policy(nid, &rp);\n+\t\t\tprev_nid = nid;\n+\t\t}\n+\t\tif (!rp.managed_watermarks)\n+\t\t\tzone->watermark_boost = 0;\n \t\tzone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_HIGH] = low_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 0f534428ea88..07de666c1276 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -73,6 +73,13 @@\n #define CREATE_TRACE_POINTS\n #include <trace/events/vmscan.h>\n \n+static inline bool zone_reclaim_allowed(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn zone_private_flags(zone, NP_OPS_RECLAIM);\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n struct scan_control {\n \t/* How many pages shrink_list() should reclaim */\n \tunsigned long nr_to_reclaim;\n@@ -6274,7 +6281,7 @@ static void shrink_zones(struct zonelist *zonelist, struct scan_control *sc)\n \t\t * to global LRU.\n \t\t */\n \t\tif (!cgroup_reclaim(sc)) {\n-\t\t\tif (!cpuset_zone_allowed(zone,\n+\t\t\tif (!zone_reclaim_allowed(zone,\n \t\t\t\t\t\t GFP_KERNEL | __GFP_HARDWALL))\n \t\t\t\tcontinue;\n \n@@ -6992,6 +6999,7 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \tunsigned long zone_boosts[MAX_NR_ZONES] = { 0, };\n \tbool boosted;\n \tstruct zone *zone;\n+\tstruct node_reclaim_policy policy;\n \tstruct scan_control sc = {\n \t\t.gfp_mask = GFP_KERNEL,\n \t\t.order = order,\n@@ -7016,6 +7024,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t}\n \tboosted = nr_boost_reclaim;\n \n+\t/* Query/cache private node reclaim policy once per balance() */\n+\tnode_private_reclaim_policy(pgdat->node_id, &policy);\n+\n restart:\n \tset_reclaim_active(pgdat, highest_zoneidx);\n \tsc.priority = DEF_PRIORITY;\n@@ -7083,6 +7094,12 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\tsc.may_writepage = !laptop_mode && !nr_boost_reclaim;\n \t\tsc.may_swap = !nr_boost_reclaim;\n \n+\t\t/* Private nodes may enable swap/writepage when using boost */\n+\t\tif (policy.active) {\n+\t\t\tsc.may_swap |= policy.may_swap;\n+\t\t\tsc.may_writepage |= policy.may_writepage;\n+\t\t}\n+\n \t\t/*\n \t\t * Do some background aging, to give pages a chance to be\n \t\t * referenced before reclaiming. All pages are rotated\n@@ -7176,6 +7193,10 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\t\tif (!zone_boosts[i])\n \t\t\t\tcontinue;\n \n+\t\t\t/* Some private nodes may own the\\ boost lifecycle */\n+\t\t\tif (policy.managed_watermarks)\n+\t\t\t\tcontinue;\n+\n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n \t\t\tspin_lock_irqsave(&zone->lock, flags);\n@@ -7406,7 +7427,7 @@ void wakeup_kswapd(struct zone *zone, gfp_t gfp_flags, int order,\n \tif (!managed_zone(zone))\n \t\treturn;\n \n-\tif (!cpuset_zone_allowed(zone, gfp_flags))\n+\tif (!zone_reclaim_allowed(zone, gfp_flags))\n \t\treturn;\n \n \tpgdat = zone->zone_pgdat;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-17-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern that the OOM killer may select an undeserving victim if it doesn't know whether killing a task can actually free memory on private nodes. The author introduced NP_OPS_OOM_ELIGIBLE and helpers node_oom_eligible() and zone_oom_eligible(), updated constrained_alloc() to use these functions, and removed cpuset_mems_allowed_intersects().",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "introduced new code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The OOM killer must know whether killing a task can actually free\nmemory such that pressure is reduced.\n\nA private node only contributes to relieving pressure if it participates\nin both reclaim and demotion. Without this check, the check, the OOM\nkiller may select an undeserving victim.\n\nIntroduce NP_OPS_OOM_ELIGIBLE and helpers node_oom_eligible() and\nzone_oom_eligible().\n\nReplace cpuset_mems_allowed_intersects() in oom_cpuset_eligible()\nwith oom_mems_intersect() that iterates N_MEMORY nodes and skips\nineligible private nodes.\n\nUpdate constrained_alloc() to use zone_oom_eligible() for constraint\ndetection and node_oom_eligible() to exclude ineligible nodes from\ntotalpages accounting.\n\nRemove cpuset_mems_allowed_intersects() as it has no remaining callers.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cpuset.h       |  9 -------\n include/linux/node_private.h |  3 +++\n kernel/cgroup/cpuset.c       | 17 ------------\n mm/oom_kill.c                | 52 ++++++++++++++++++++++++++++++++----\n 4 files changed, 50 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/cpuset.h b/include/linux/cpuset.h\nindex 7b2f3f6b68a9..53ccfb00b277 100644\n--- a/include/linux/cpuset.h\n+++ b/include/linux/cpuset.h\n@@ -97,9 +97,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-extern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t  const struct task_struct *tsk2);\n-\n #ifdef CONFIG_CPUSETS_V1\n #define cpuset_memory_pressure_bump() \t\t\t\t\\\n \tdo {\t\t\t\t\t\t\t\\\n@@ -241,12 +238,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-static inline int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t\t const struct task_struct *tsk2)\n-{\n-\treturn 1;\n-}\n-\n static inline void cpuset_memory_pressure_bump(void) {}\n \n static inline void cpuset_task_status_allowed(struct seq_file *m,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34be52383255..34d862f09e24 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -141,6 +141,9 @@ struct node_private_ops {\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n+/* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n+#define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 1a597f0c7c6c..29789d544fd5 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -4530,23 +4530,6 @@ int cpuset_mem_spread_node(void)\n \treturn cpuset_spread_node(&current->cpuset_mem_spread_rotor);\n }\n \n-/**\n- * cpuset_mems_allowed_intersects - Does @tsk1's mems_allowed intersect @tsk2's?\n- * @tsk1: pointer to task_struct of some task.\n- * @tsk2: pointer to task_struct of some other task.\n- *\n- * Description: Return true if @tsk1's mems_allowed intersects the\n- * mems_allowed of @tsk2.  Used by the OOM killer to determine if\n- * one of the task's memory usage might impact the memory available\n- * to the other.\n- **/\n-\n-int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t   const struct task_struct *tsk2)\n-{\n-\treturn nodes_intersects(tsk1->mems_allowed, tsk2->mems_allowed);\n-}\n-\n /**\n  * cpuset_print_current_mems_allowed - prints current's cpuset and mems_allowed\n  *\ndiff --git a/mm/oom_kill.c b/mm/oom_kill.c\nindex 5eb11fbba704..cd0d65ccd1e8 100644\n--- a/mm/oom_kill.c\n+++ b/mm/oom_kill.c\n@@ -74,7 +74,45 @@ static inline bool is_memcg_oom(struct oom_control *oc)\n \treturn oc->memcg != NULL;\n }\n \n+/* Private nodes are only eligible if they support both reclaim and demotion */\n+static inline bool node_oom_eligible(int nid)\n+{\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn (node_private_flags(nid) & NP_OPS_OOM_ELIGIBLE) ==\n+\t\tNP_OPS_OOM_ELIGIBLE;\n+}\n+\n+static inline bool zone_oom_eligible(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (!node_oom_eligible(zone_to_nid(zone)))\n+\t\treturn false;\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n #ifdef CONFIG_NUMA\n+/*\n+ * Killing a task can only relieve system pressure if freed memory can be\n+ * demoted there and reclaim can operate on the node's pages, so we\n+ * omit private nodes that aren't eligible.\n+ */\n+static bool oom_mems_intersect(const struct task_struct *tsk1,\n+\t\t\t       const struct task_struct *tsk2)\n+{\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (!node_isset(nid, tsk1->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_isset(nid, tsk2->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_oom_eligible(nid))\n+\t\t\tcontinue;\n+\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n /**\n  * oom_cpuset_eligible() - check task eligibility for kill\n  * @start: task struct of which task to consider\n@@ -107,9 +145,10 @@ static bool oom_cpuset_eligible(struct task_struct *start,\n \t\t} else {\n \t\t\t/*\n \t\t\t * This is not a mempolicy constrained oom, so only\n-\t\t\t * check the mems of tsk's cpuset.\n+\t\t\t * check the mems of tsk's cpuset, excluding private\n+\t\t\t * nodes that do not participate in kernel reclaim.\n \t\t\t */\n-\t\t\tret = cpuset_mems_allowed_intersects(current, tsk);\n+\t\t\tret = oom_mems_intersect(current, tsk);\n \t\t}\n \t\tif (ret)\n \t\t\tbreak;\n@@ -291,16 +330,19 @@ static enum oom_constraint constrained_alloc(struct oom_control *oc)\n \t\treturn CONSTRAINT_MEMORY_POLICY;\n \t}\n \n-\t/* Check this allocation failure is caused by cpuset's wall function */\n+\t/* Check this allocation failure is caused by cpuset or private node constraints */\n \tfor_each_zone_zonelist_nodemask(zone, z, oc->zonelist,\n \t\t\thighest_zoneidx, oc->nodemask)\n-\t\tif (!cpuset_zone_allowed(zone, oc->gfp_mask))\n+\t\tif (!zone_oom_eligible(zone, oc->gfp_mask))\n \t\t\tcpuset_limited = true;\n \n \tif (cpuset_limited) {\n \t\toc->totalpages = total_swap_pages;\n-\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed)\n+\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed) {\n+\t\t\tif (!node_oom_eligible(nid))\n+\t\t\t\tcontinue;\n \t\t\toc->totalpages += node_present_pages(nid);\n+\t\t}\n \t\treturn CONSTRAINT_CPUSET;\n \t}\n \treturn CONSTRAINT_NONE;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-18-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern that not all private nodes may wish to engage in NUMA balancing faults by introducing an opt-in method (NP_OPS_NUMA_BALANCING) and modifying the do_numa_page() function to enforce write-protection on private-node folios with NP_OPS_PROTECT_WRITE. The author added new helper functions, including folio_managed_allows_numa(), which filters for private nodes that have opted in to NUMA balancing.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Not all private nodes may wish to engage in NUMA balancing faults.\n\nAdd the NP_OPS_NUMA_BALANCING flag (BIT(5)) as an opt-in method.\n\nIntroduce folio_managed_allows_numa() helper:\n   ZONE_DEVICE folios always return false (never NUMA-scanned)\n   NP_OPS_NUMA_BALANCING filters for private nodes\n\nIn do_numa_page(), if a private-node folio with NP_OPS_PROTECT_WRITE\nis still on its node after a failed/skipped migration, enforce\nwrite-protection so the next write triggers handle_fault.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h | 16 ++++++++++++++++\n mm/memory.c                  | 11 +++++++++++\n mm/mempolicy.c               |  5 ++++-\n 4 files changed, 35 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex a4955b9b5b93..88aaac45e814 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -961,6 +961,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (ops->flags & NP_OPS_PROTECT_WRITE))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_NUMA_BALANCING) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34d862f09e24..5ac60db1f044 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -140,6 +140,8 @@ struct node_private_ops {\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n+/* Allow NUMA balancing to scan and migrate folios on this node */\n+#define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n@@ -263,6 +265,15 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n }\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\tif (!folio_is_private_managed(folio))\n+\t\treturn true;\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\treturn folio_private_flags(folio, NP_OPS_NUMA_BALANCING);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \tif (folio_is_zone_device(folio))\n@@ -443,6 +454,11 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\treturn !folio_is_zone_device(folio);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \treturn -ENOENT;\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 0f78988befef..88a581baae40 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -78,6 +78,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/pgalloc.h>\n #include <linux/uaccess.h>\n+#include <linux/node_private.h>\n \n #include <trace/events/kmem.h>\n \n@@ -6041,6 +6042,12 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \tif (!folio || folio_is_zone_device(folio))\n \t\tgoto out_map;\n \n+\t/*\n+\t * We do not need to check private-node folios here because the private\n+\t * memory service either never opted in to NUMA balancing, or it did\n+\t * and we need to restore private PTE controls on the failure path.\n+\t */\n+\n \tnid = folio_nid(folio);\n \tnr_pages = folio_nr_pages(folio);\n \n@@ -6078,6 +6085,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t/*\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n+\t *\n+\t * If the folio is still on a private node with NP_OPS_PROTECT_WRITE,\n+\t * enforce write-protection so the next write triggers handle_fault.\n+\t * This covers migration-failed and migration-skipped paths.\n \t */\n \tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n \t\twritable = false;\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 8ac014950e88..8a3a9916ab59 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -861,7 +861,10 @@ bool folio_can_map_prot_numa(struct folio *folio, struct vm_area_struct *vma,\n {\n \tint nid;\n \n-\tif (!folio || folio_is_zone_device(folio) || folio_test_ksm(folio))\n+\tif (!folio || folio_test_ksm(folio))\n+\t\treturn false;\n+\n+\tif (unlikely(!folio_managed_allows_numa(folio)))\n \t\treturn false;\n \n \t/* Also skip shared copy-on-write folios */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-19-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about compaction on private nodes, explaining that it requires migration and services may have PFN-based metadata that needs updating. They added a folio_migrate callback to fire before faults are unblocked, modified zone_supports_compaction() to check for NP_OPS_COMPACTION in N_MEMORY_PRIVATE zones, and filtered three direct compaction zone loops.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "added new code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node zones should not be compacted unless the service explicitly\nopts in - as compaction requires migration and services may have\nPFN-based metadata that needs updating.\n\nAdd a folio_migrate callback which fires from migrate_folio_move() for\neach relocated folio before faults are unblocked.\n\nAdd zone_supports_compaction() which returns true for normal zones and\nchecks NP_OPS_COMPACTION for N_MEMORY_PRIVATE zones.\n\nFilter three direct compaction zone loops:\n  - compaction_zonelist_suitable() (reclaimer eligibility)\n  - try_to_compact_pages()         (direct compaction)\n  - compact_node()                 (proactive/manual compaction)\n\nkcompactd paths are intentionally unfiltered -- the service is\nresponsible for starting kcompactd on its node.\n\nNP_OPS_COMPACTION requires NP_OPS_MIGRATION.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h |  2 ++\n mm/compaction.c              | 26 ++++++++++++++++++++++++++\n 3 files changed, 32 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 88aaac45e814..da523aca18fa 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -965,6 +965,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_COMPACTION) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 5ac60db1f044..fe0336773ddb 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -142,6 +142,8 @@ struct node_private_ops {\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n /* Allow NUMA balancing to scan and migrate folios on this node */\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n+/* Allow compaction to run on the node.  Service must start kcompactd. */\n+#define NP_OPS_COMPACTION\t\tBIT(6)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 6a65145b03d8..d8532b957ec6 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,9 +24,26 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/node_private.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n+\n+/*\n+ * Private node zones require NP_OPS_COMPACTION to opt in.  Normal zones\n+ * always support compaction.\n+ */\n+static inline bool zone_supports_compaction(struct zone *zone)\n+{\n+#ifdef CONFIG_NUMA\n+\tif (!node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn zone_private_flags(zone, NP_OPS_COMPACTION);\n+#else\n+\treturn true;\n+#endif\n+}\n+\n /*\n  * Fragmentation score check interval for proactive compaction purposes.\n  */\n@@ -2443,6 +2460,9 @@ bool compaction_zonelist_suitable(struct alloc_context *ac, int order,\n \t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tunsigned long available;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\t/*\n \t\t * Do not consider all the reclaimable memory because we do not\n \t\t * want to trash just for a single high order allocation which\n@@ -2832,6 +2852,9 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\n \t\t\trc = max_t(enum compact_result, COMPACT_DEFERRED, rc);\n@@ -2906,6 +2929,9 @@ static int compact_node(pg_data_t *pgdat, bool proactive)\n \t\tif (!populated_zone(zone))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (fatal_signal_pending(current))\n \t\t\treturn -EINTR;\n \n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-20-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private node folios being longterm-pinnable by default, explaining that this would freeze the service's control for the duration of the pin and potentially cause issues with hot-unplugability or shared CPU-device state. The author added a new flag NP_OPS_LONGTERM_PIN to allow services to opt in to longterm pinning, which will be checked by the out-of-line helper node_private_allows_longterm_pin().",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not be longterm-pinnable by default.\nA pinned folio is frozen in place, no migration, compaction, or\nreclaim, so the service loses control for the duration of the pin.\n\nSome services may depend on hot-unplugability and must disallow\nlongterm pinning.  Others (accelerators with shared CPU-device state)\nneed pinning to work.\n\nAdd NP_OPS_LONGTERM_PIN flag for services to opt in with. Hook into\nfolio_is_longterm_pinnable() in mm.h, which all GUP callers\nout-of-line helper, node_private_allows_longterm_pin(),  called\nonly for N_MEMORY_PRIVATE nodes.\n\nWithout the flag: folio_is_longterm_pinnable() returns false, migration\nfails (no __GFP_PRIVATE in GFP mask) and pin_user_pages(FOLL_LONGTERM)\nreturns -ENOMEM.\n\nWith the flag: pin succeeds and the folio stays on the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 15 +++++++++++++++\n include/linux/mm.h           | 22 ++++++++++++++++++++++\n include/linux/node_private.h |  2 ++\n 3 files changed, 39 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex da523aca18fa..5d2487fd54f4 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -866,6 +866,21 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n static DEFINE_MUTEX(node_private_lock);\n static bool node_private_initialized;\n \n+/**\n+ * node_private_allows_longterm_pin - Check if a private node allows longterm pinning\n+ * @nid: Node identifier\n+ *\n+ * Out-of-line helper for folio_is_longterm_pinnable() since mm.h cannot\n+ * include node_private.h (circular dependency).\n+ *\n+ * Returns true if the node has NP_OPS_LONGTERM_PIN set.\n+ */\n+bool node_private_allows_longterm_pin(int nid)\n+{\n+\treturn node_private_has_flag(nid, NP_OPS_LONGTERM_PIN);\n+}\n+EXPORT_SYMBOL_GPL(node_private_allows_longterm_pin);\n+\n /**\n  * node_private_register - Register a private node\n  * @nid: Node identifier\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex fb1819ad42c3..9088fd08aeb9 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -2192,6 +2192,13 @@ static inline bool is_zero_folio(const struct folio *folio)\n \n /* MIGRATE_CMA and ZONE_MOVABLE do not allow pin folios */\n #ifdef CONFIG_MIGRATION\n+\n+#ifdef CONFIG_NUMA\n+bool node_private_allows_longterm_pin(int nid);\n+#else\n+static inline bool node_private_allows_longterm_pin(int nid) { return false; }\n+#endif\n+\n static inline bool folio_is_longterm_pinnable(struct folio *folio)\n {\n #ifdef CONFIG_CMA\n@@ -2215,6 +2222,21 @@ static inline bool folio_is_longterm_pinnable(struct folio *folio)\n \tif (folio_is_fsdax(folio))\n \t\treturn false;\n \n+\t/*\n+\t * Private node folios are not longterm pinnable by default.\n+\t * Services that support pinning opt in via NP_OPS_LONGTERM_PIN.\n+\t * node_private_allows_longterm_pin() is out-of-line because\n+\t * node_private.h includes mm.h (circular dependency).\n+\t *\n+\t * Guarded by CONFIG_NUMA because on !CONFIG_NUMA the single-node\n+\t * node_state() stub returns true for node 0, which would make\n+\t * all folios non-pinnable via the false-returning stub.\n+\t */\n+#ifdef CONFIG_NUMA\n+\tif (node_state(folio_nid(folio), N_MEMORY_PRIVATE))\n+\t\treturn node_private_allows_longterm_pin(folio_nid(folio));\n+#endif\n+\n \t/* Otherwise, non-movable zone folios can be pinned. */\n \treturn !folio_is_zone_movable(folio);\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex fe0336773ddb..7a7438fb9eda 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -144,6 +144,8 @@ struct node_private_ops {\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n /* Allow compaction to run on the node.  Service must start kcompactd. */\n #define NP_OPS_COMPACTION\t\tBIT(6)\n+/* Allow longterm DMA pinning (RDMA, VFIO, etc.) of folios on this node */\n+#define NP_OPS_LONGTERM_PIN\t\tBIT(7)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-21-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about notifying private-node services of hardware errors on their nodes by adding a memory_failure callback to struct node_private_ops, which will be called after TestSetPageHWPoison succeeds and before get_hwpoison_page. The kernel always proceeds with standard hwpoison handling for online pages.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a void memory_failure notification callback to struct\nnode_private_ops so services managing N_MEMORY_PRIVATE nodes notified\nwhen a page on their node experiences a hardware error.\n\nThe callback is notification only -- the kernel always proceeds with\nstandard hwpoison handling for online pages.\n\nThe notification hook fires after TestSetPageHWPoison succeeds and\nbefore get_hwpoison_page giving the service a chance to clean up.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 16 ++++++++++++++++\n mm/memory-failure.c          | 15 +++++++++++++++\n 3 files changed, 37 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7a7438fb9eda..d2669f68ac20 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -113,6 +113,10 @@ struct node_reclaim_policy {\n  *   watermark_boost lifecycle (kswapd will not clear it).\n  *   If NULL, normal boost policy applies.\n  *\n+ * @memory_failure: Notification of hardware error on a page on this node.\n+ *   [folio-referenced callback]\n+ *   Notification only, kernel always handles the failure.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -127,6 +131,8 @@ struct node_private_ops {\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n \tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n+\tvoid (*memory_failure)(struct folio *folio, unsigned long pfn,\n+\t\t\t       int mf_flags);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex db32cb2d7a29..64467ca774f1 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1608,6 +1608,22 @@ static inline void node_private_reclaim_policy(int nid,\n }\n #endif\n \n+static inline void folio_managed_memory_failure(struct folio *folio,\n+\t\t\t\t\t\tunsigned long pfn,\n+\t\t\t\t\t\tint mf_flags)\n+{\n+\t/* Zone device pages handle memory failure via dev_pagemap_ops */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn;\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->memory_failure)\n+\t\t\tops->memory_failure(folio, pfn, mf_flags);\n+\t}\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/memory-failure.c b/mm/memory-failure.c\nindex c80c2907da33..79c91d44ec1e 100644\n--- a/mm/memory-failure.c\n+++ b/mm/memory-failure.c\n@@ -2379,6 +2379,15 @@ int memory_failure(unsigned long pfn, int flags)\n \t\tgoto unlock_mutex;\n \t}\n \n+\t/*\n+\t * Notify private-node services about the hardware error so they\n+\t * can update internal tracking (e.g., CXL poison lists, stop\n+\t * demoting to failing DIMMs).  This is notification only -- the\n+\t * kernel proceeds with standard hwpoison handling regardless.\n+\t */\n+\tif (unlikely(page_is_private_managed(p)))\n+\t\tfolio_managed_memory_failure(page_folio(p), pfn, flags);\n+\n \t/*\n \t * We need/can do nothing about count=0 pages.\n \t * 1) it's a free page, and therefore in safe hand:\n@@ -2825,6 +2834,12 @@ static int soft_offline_in_use_page(struct page *page)\n \t\treturn 0;\n \t}\n \n+\tif (!folio_managed_allows_migrate(folio)) {\n+\t\tpr_info(\"%#lx: cannot migrate private node folio\\n\", pfn);\n+\t\tfolio_put(folio);\n+\t\treturn -EBUSY;\n+\t}\n+\n \tisolated = isolate_folio_to_list(folio, &pagelist);\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-22-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the ordering of operations in hotplugging memory as N_MEMORY_PRIVATE, explaining that their new function combines node_private_region_register() and __add_memory_driver_managed() to ensure proper ordering. They also added checks for private nodes when starting reclaim/compaction daemons.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a new function for drivers to hotplug memory as N_MEMORY_PRIVATE.\n\nThis function combines node_private_region_register() with\n__add_memory_driver_managed() to ensure proper ordering:\n\n1. Register the private region first (sets private node context)\n2. Then hotplug the memory (sets N_MEMORY_PRIVATE)\n3. On failure, unregister the private region to avoid leaving the\n   node in an inconsistent state.\n\nWhen the last of memory is removed, hotplug also removes the private\nnode context. If migration is not supported and the node is still\nonline, fire a warning (likely bug in the driver).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory_hotplug.h |  11 +++\n include/linux/mmzone.h         |  12 ++++\n mm/memory_hotplug.c            | 122 ++++++++++++++++++++++++++++++---\n 3 files changed, 135 insertions(+), 10 deletions(-)\n\ndiff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h\nindex 1f19f08552ea..e5abade9450a 100644\n--- a/include/linux/memory_hotplug.h\n+++ b/include/linux/memory_hotplug.h\n@@ -293,6 +293,7 @@ extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n extern int remove_memory(u64 start, u64 size);\n extern void __remove_memory(u64 start, u64 size);\n extern int offline_and_remove_memory(u64 start, u64 size);\n+extern int offline_and_remove_private_memory(int nid, u64 start, u64 size);\n \n #else\n static inline void try_offline_node(int nid) {}\n@@ -309,6 +310,12 @@ static inline int remove_memory(u64 start, u64 size)\n }\n \n static inline void __remove_memory(u64 start, u64 size) {}\n+\n+static inline int offline_and_remove_private_memory(int nid, u64 start,\n+\t\t\t\t\t\t    u64 size)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n@@ -326,6 +333,10 @@ int __add_memory_driver_managed(int nid, u64 start, u64 size,\n extern int add_memory_driver_managed(int nid, u64 start, u64 size,\n \t\t\t\t     const char *resource_name,\n \t\t\t\t     mhp_t mhp_flags);\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np);\n extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,\n \t\t\t\t   unsigned long nr_pages,\n \t\t\t\t   struct vmem_altmap *altmap, int migratetype,\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 992eb1c5a2c6..cc532b67ad3f 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1524,6 +1524,18 @@ typedef struct pglist_data {\n #endif\n } pg_data_t;\n \n+#ifdef CONFIG_NUMA\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn pgdat->private;\n+}\n+#else\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn false;\n+}\n+#endif\n+\n #define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n #define node_spanned_pages(nid)\t(NODE_DATA(nid)->node_spanned_pages)\n \ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex d2dc527bd5b0..9d72f44a30dc 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1173,8 +1174,7 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tmove_pfn_range_to_zone(zone, pfn, nr_pages, NULL, MIGRATE_MOVABLE,\n \t\t\t       true);\n \n-\tif (!node_state(nid, N_MEMORY)) {\n-\t\t/* Adding memory to the node for the first time */\n+\tif (!node_state(nid, N_MEMORY) && !node_state(nid, N_MEMORY_PRIVATE)) {\n \t\tnode_arg.nid = nid;\n \t\tret = node_notify(NODE_ADDING_FIRST_MEMORY, &node_arg);\n \t\tret = notifier_to_errno(ret);\n@@ -1208,8 +1208,12 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tonline_pages_range(pfn, nr_pages);\n \tadjust_present_page_count(pfn_to_page(pfn), group, nr_pages);\n \n-\tif (node_arg.nid >= 0)\n-\t\tnode_set_state(nid, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (pgdat_is_private(NODE_DATA(nid)))\n+\t\t\tnode_set_state(nid, N_MEMORY_PRIVATE);\n+\t\telse\n+\t\t\tnode_set_state(nid, N_MEMORY);\n+\t}\n \tif (need_zonelists_rebuild)\n \t\tbuild_all_zonelists(NULL);\n \n@@ -1227,8 +1231,14 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t/* reinitialise watermarks and update pcp limits */\n \tinit_per_zone_wmark_min();\n \n-\tkswapd_run(nid);\n-\tkcompactd_run(nid);\n+\t/*\n+\t * Don't start reclaim/compaction daemons for private nodes.\n+\t * Private node services will decide whether to start these services.\n+\t */\n+\tif (!pgdat_is_private(NODE_DATA(nid))) {\n+\t\tkswapd_run(nid);\n+\t\tkcompactd_run(nid);\n+\t}\n \n \tif (node_arg.nid >= 0)\n \t\t/* First memory added successfully. Notify consumers. */\n@@ -1722,6 +1732,54 @@ int add_memory_driver_managed(int nid, u64 start, u64 size,\n }\n EXPORT_SYMBOL_GPL(add_memory_driver_managed);\n \n+/**\n+ * add_private_memory_driver_managed - add driver-managed N_MEMORY_PRIVATE memory\n+ * @nid: NUMA node ID (or memory group ID when MHP_NID_IS_MGID is set)\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ * @resource_name: \"System RAM ($DRIVER)\" format\n+ * @mhp_flags: Memory hotplug flags\n+ * @online_type: MMOP_* online type\n+ * @np: Driver-owned node_private structure (owner, refcount)\n+ *\n+ * Registers node_private first, then hotplugs the memory.\n+ *\n+ * On failure, unregisters the node_private.\n+ */\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np)\n+{\n+\tstruct memory_group *group;\n+\tint real_nid = nid;\n+\tint rc;\n+\n+\tif (!np)\n+\t\treturn -EINVAL;\n+\n+\tif (mhp_flags & MHP_NID_IS_MGID) {\n+\t\tgroup = memory_group_find_by_id(nid);\n+\t\tif (!group)\n+\t\t\treturn -EINVAL;\n+\t\treal_nid = group->nid;\n+\t}\n+\n+\trc = node_private_register(real_nid, np);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\trc = __add_memory_driver_managed(nid, start, size, resource_name,\n+\t\t\t\t\t mhp_flags, online_type);\n+\tif (rc) {\n+\t\tnode_private_unregister(real_nid);\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(add_private_memory_driver_managed);\n+\n /*\n  * Platforms should define arch_get_mappable_range() that provides\n  * maximum possible addressable physical memory range for which the\n@@ -1872,6 +1930,15 @@ static void do_migrate_range(unsigned long start_pfn, unsigned long end_pfn)\n \t\t\tgoto put_folio;\n \t\t}\n \n+\t\t/* Private nodes w/o migration must ensure folios are offline */\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION)) {\n+\t\t\tWARN_ONCE(1, \"hot-unplug on non-migratable node %d pfn %lx\\n\",\n+\t\t\t\t  folio_nid(folio), pfn);\n+\t\t\tpfn = folio_pfn(folio) + folio_nr_pages(folio) - 1;\n+\t\t\tgoto put_folio;\n+\t\t}\n+\n \t\tif (!isolate_folio_to_list(folio, &source)) {\n \t\t\tif (__ratelimit(&migrate_rs)) {\n \t\t\t\tpr_warn(\"failed to isolate pfn %lx\\n\",\n@@ -2014,8 +2081,8 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \n \t/*\n \t * Check whether the node will have no present pages after we offline\n-\t * 'nr_pages' more. If so, we know that the node will become empty, and\n-\t * so we will clear N_MEMORY for it.\n+\t * 'nr_pages' more. If so, send pre-notification for last memory removal.\n+\t * We will clear N_MEMORY(_PRIVATE) if this is the case.\n \t */\n \tif (nr_pages >= pgdat->node_present_pages) {\n \t\tnode_arg.nid = node;\n@@ -2108,8 +2175,12 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * Make sure to mark the node as memory-less before rebuilding the zone\n \t * list. Otherwise this node would still appear in the fallback lists.\n \t */\n-\tif (node_arg.nid >= 0)\n-\t\tnode_clear_state(node, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (node_state(node, N_MEMORY))\n+\t\t\tnode_clear_state(node, N_MEMORY);\n+\t\telse if (node_state(node, N_MEMORY_PRIVATE))\n+\t\t\tnode_clear_state(node, N_MEMORY_PRIVATE);\n+\t}\n \tif (!populated_zone(zone)) {\n \t\tzone_pcp_reset(zone);\n \t\tbuild_all_zonelists(NULL);\n@@ -2461,4 +2532,35 @@ int offline_and_remove_memory(u64 start, u64 size)\n \treturn rc;\n }\n EXPORT_SYMBOL_GPL(offline_and_remove_memory);\n+\n+/**\n+ * offline_and_remove_private_memory - offline, remove, and unregister private memory\n+ * @nid: NUMA node ID of the private memory\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ *\n+ * Counterpart to add_private_memory_driver_managed().  Offlines and removes\n+ * the memory range, then attempts to unregister the node_private.\n+ *\n+ * offline_and_remove_memory() clears N_MEMORY_PRIVATE when the last block\n+ * is offlined, which allows node_private_unregister() to clear the\n+ * pgdat->node_private pointer.  If other private memory ranges remain on\n+ * the node, node_private_unregister() returns -EBUSY (N_MEMORY_PRIVATE\n+ * is still set) and the node_private remains registered.\n+ *\n+ * Return: 0 on full success (memory removed and node_private unregistered),\n+ *         -EBUSY if memory was removed but node still has other private memory,\n+ *         other negative error code if offline/remove failed.\n+ */\n+int offline_and_remove_private_memory(int nid, u64 start, u64 size)\n+{\n+\tint rc;\n+\n+\trc = offline_and_remove_memory(start, size);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\treturn node_private_unregister(nid);\n+}\n+EXPORT_SYMBOL_GPL(offline_and_remove_private_memory);\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-23-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author addressed a concern about the need for device pressure communication via watermark_boost on the private node's zone, explaining that CRAM provides this bridge and registers node_private_ops to handle faults, migration, demotion, write protection, reclaim policy, and more. The author confirmed that the system stabilizes under memory pressure by limiting entry into CRAM through demotion.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "confirmed the approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CRAM (Compressed RAM) subsystem that manages folios demoted\nto N_MEMORY_PRIVATE nodes via the standard kernel LRU.\n\nWe limit entry into CRAM by demotion in to provide devices a way for\ndrivers to close access - which allows the system to stabiliz under\nmemory pressure (the device can run out of real memory when compression\nratios drop too far).\n\nWe utilize write-protect to prevent unbounded writes to compressed\nmemory pages, which may cause run-away compression ratio loss without\na reliable way to prevent the degenerate case (cascading poisons).\n\nCRAM provides the bridge between the mm/ private node infrastructure\nand compressed memory hardware.  Folios are aged by kswapd on the\nprivate node and reclaimed to swap when the device signals pressure.\n\nWrite faults trigger promotion back to regular DRAM via the\nops->handle_fault callback.\n\nDevice pressure is communicated via watermark_boost on the private\nnode's zone.\n\nCRAM registers node_private_ops with:\n  - handle_fault:   promotes folio back to DRAM on write\n  - migrate_to:     custom demotion to the CRAM node\n  - folio_migrate:  (no-op)\n  - free_folio:     zeroes pages on free to scrub stale data\n  - reclaim_policy: provides mayswap/writeback/boost overrides\n  - flags: NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n\t   NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE\n           NP_OPS_RECLAIM\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cram.h |  66 ++++++\n mm/Kconfig           |  10 +\n mm/Makefile          |   1 +\n mm/cram.c            | 508 +++++++++++++++++++++++++++++++++++++++++++\n 4 files changed, 585 insertions(+)\n create mode 100644 include/linux/cram.h\n create mode 100644 mm/cram.c\n\ndiff --git a/include/linux/cram.h b/include/linux/cram.h\nnew file mode 100644\nindex 000000000000..a3c10362fd4f\n--- /dev/null\n+++ b/include/linux/cram.h\n@@ -0,0 +1,66 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_CRAM_H\n+#define _LINUX_CRAM_H\n+\n+#include <linux/mm_types.h>\n+\n+struct folio;\n+struct list_head;\n+struct vm_fault;\n+\n+#define CRAM_PRESSURE_MAX\t1000\n+\n+/**\n+ * cram_flush_cb_t - Driver callback invoked when a folio on a private node\n+ *                   is freed (refcount reaches zero).\n+ * @folio: the folio being freed\n+ * @private: opaque driver data passed at registration\n+ *\n+ * Return:\n+ *   0: Flush resolved -- page should return to buddy allocator (e.g., flush\n+ *      record bit was set, meaning this free is from our own flush resolution)\n+ *   1: Page deferred -- driver took a reference, page will be flushed later.\n+ *      Do NOT return to buddy allocator.\n+ *   2: Buffer full -- caller should zero the page and return to buddy.\n+ */\n+typedef int (*cram_flush_cb_t)(struct folio *folio, void *private);\n+\n+#ifdef CONFIG_CRAM\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data);\n+int cram_unregister_private_node(int nid);\n+int cram_unpurge(int nid);\n+void cram_set_pressure(int nid, unsigned int pressure);\n+void cram_clear_pressure(int nid);\n+\n+#else /* !CONFIG_CRAM */\n+\n+static inline int cram_register_private_node(int nid, void *owner,\n+\t\t\t\t\t     cram_flush_cb_t flush_cb,\n+\t\t\t\t\t     void *flush_data)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unregister_private_node(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unpurge(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+}\n+\n+static inline void cram_clear_pressure(int nid)\n+{\n+}\n+\n+#endif /* CONFIG_CRAM */\n+\n+#endif /* _LINUX_CRAM_H */\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex bd0ea5454af8..054462b954d8 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -662,6 +662,16 @@ config MIGRATION\n config DEVICE_MIGRATION\n \tdef_bool MIGRATION && ZONE_DEVICE\n \n+config CRAM\n+\tbool \"Compressed RAM - private node memory management\"\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\tdepends on MEMORY_HOTPLUG\n+\thelp\n+\t  Enables management of N_MEMORY_PRIVATE nodes for compressed RAM\n+\t  and similar use cases. Provides demotion, promotion, and lifecycle\n+\t  management for private memory nodes.\n+\n config ARCH_ENABLE_HUGEPAGE_MIGRATION\n \tbool\n \ndiff --git a/mm/Makefile b/mm/Makefile\nindex 2d0570a16e5b..0e1421512643 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -98,6 +98,7 @@ obj-$(CONFIG_MEMTEST)\t\t+= memtest.o\n obj-$(CONFIG_MIGRATION) += migrate.o\n obj-$(CONFIG_NUMA) += memory-tiers.o\n obj-$(CONFIG_DEVICE_MIGRATION) += migrate_device.o\n+obj-$(CONFIG_CRAM) += cram.o\n obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o\n obj-$(CONFIG_PAGE_COUNTER) += page_counter.o\n obj-$(CONFIG_LIVEUPDATE) += memfd_luo.o\ndiff --git a/mm/cram.c b/mm/cram.c\nnew file mode 100644\nindex 000000000000..6709e61f5b9d\n--- /dev/null\n+++ b/mm/cram.c\n@@ -0,0 +1,508 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * mm/cram.c - Compressed RAM / private node memory management\n+ *\n+ * Copyright 2026 Meta Technologies Inc.\n+ *   Author: Gregory Price <gourry@gourry.net>\n+ *\n+ * Manages folios demoted to N_MEMORY_PRIVATE nodes via the standard kernel\n+ * LRU.  Folios are aged by kswapd on the private node and reclaimed to swap\n+ * (demotion is suppressed for private nodes).  Write faults trigger promotion\n+ * back to regular DRAM via the ops->handle_fault callback.\n+ *\n+ * All reclaim/demotion uses the standard vmscan infrastructure. Device pressure\n+ * is communicated via watermark_boost on the private node's zone.\n+ */\n+\n+#include <linux/atomic.h>\n+#include <linux/cpuset.h>\n+#include <linux/cram.h>\n+#include <linux/errno.h>\n+#include <linux/gfp.h>\n+#include <linux/jiffies.h>\n+#include <linux/highmem.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/list.h>\n+#include <linux/migrate.h>\n+#include <linux/mm.h>\n+#include <linux/huge_mm.h>\n+#include <linux/mmzone.h>\n+#include <linux/mutex.h>\n+#include <linux/nodemask.h>\n+#include <linux/node_private.h>\n+#include <linux/pagemap.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+#include <linux/swap.h>\n+\n+#include \"internal.h\"\n+\n+struct cram_node {\n+\tvoid\t\t*owner;\n+\tbool\t\tpurged;\t\t/* node is being torn down */\n+\tunsigned int\tpressure;\n+\trefcount_t\trefcount;\n+\tcram_flush_cb_t\tflush_cb;\t/* optional driver flush callback */\n+\tvoid\t\t*flush_data;\t/* opaque data for flush_cb */\n+};\n+\n+static struct cram_node *cram_nodes[MAX_NUMNODES];\n+static DEFINE_MUTEX(cram_mutex);\n+\n+static inline bool cram_valid_nid(int nid)\n+{\n+\treturn nid >= 0 && nid < MAX_NUMNODES;\n+}\n+\n+static inline struct cram_node *get_cram_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn NULL;\n+\n+\trcu_read_lock();\n+\tcn = rcu_dereference(cram_nodes[nid]);\n+\tif (cn && !refcount_inc_not_zero(&cn->refcount))\n+\t\tcn = NULL;\n+\trcu_read_unlock();\n+\n+\treturn cn;\n+}\n+\n+static inline void put_cram_node(struct cram_node *cn)\n+{\n+\tif (cn)\n+\t\trefcount_dec(&cn->refcount);\n+}\n+\n+static void cram_zero_folio(struct folio *folio)\n+{\n+\tunsigned int i, nr = folio_nr_pages(folio);\n+\n+\tif (want_init_on_free())\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr; i++)\n+\t\tclear_highpage(folio_page(folio, i));\n+}\n+\n+static bool cram_free_folio_cb(struct folio *folio)\n+{\n+\tint nid = folio_nid(folio);\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\tgoto zero_and_free;\n+\n+\tif (!cn->flush_cb)\n+\t\tgoto zero_and_free_put;\n+\n+\tret = cn->flush_cb(folio, cn->flush_data);\n+\tput_cram_node(cn);\n+\n+\tswitch (ret) {\n+\tcase 0:\n+\t\t/* Flush resolved: return to buddy (already zeroed by device) */\n+\t\treturn false;\n+\tcase 1:\n+\t\t/* Deferred: driver holds a ref, do not free to buddy */\n+\t\treturn true;\n+\tcase 2:\n+\tdefault:\n+\t\t/* Buffer full or unknown: zero locally, return to buddy */\n+\t\tgoto zero_and_free;\n+\t}\n+\n+zero_and_free_put:\n+\tput_cram_node(cn);\n+zero_and_free:\n+\tcram_zero_folio(folio);\n+\treturn false;\n+}\n+\n+static struct folio *alloc_cram_folio(struct folio *src, unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_PRIVATE | __GFP_KSWAPD_RECLAIM |\n+\t\t     __GFP_HIGHMEM | __GFP_MOVABLE |\n+\t\t     __GFP_NOWARN | __GFP_NORETRY;\n+\n+\t/* Stop allocating if backpressure fired mid-batch */\n+\tif (node_private_migration_blocked(nid))\n+\t\treturn NULL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc_node(gfp, order, nid);\n+}\n+\n+static void cram_put_new_folio(struct folio *folio, unsigned long private)\n+{\n+\tcram_zero_folio(folio);\n+\tfolio_put(folio);\n+}\n+\n+/*\n+ * Allocate a DRAM folio for promotion out of a private node.\n+ *\n+ * Unlike alloc_migration_target(), this does NOT strip __GFP_RECLAIM for\n+ * large folios, the generic helper does that because THP allocations are\n+ * opportunistic, but promotion from a private node is mandatory: the page\n+ * MUST move to DRAM or the process cannot make forward progress.\n+ *\n+ * __GFP_RETRY_MAYFAIL tells the allocator to try hard (multiple reclaim\n+ * rounds, wait for writeback) before giving up.\n+ */\n+static struct folio *alloc_cram_promote_folio(struct folio *src,\n+\t\t\t\t\t      unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc(gfp, order, nid, NULL);\n+}\n+\n+static int cram_migrate_to(struct list_head *demote_folios, int to_nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason,\n+\t\t\t   unsigned int *nr_succeeded)\n+{\n+\tstruct cram_node *cn;\n+\tunsigned int nr_success = 0;\n+\tint ret = 0;\n+\n+\tcn = get_cram_node(to_nid);\n+\tif (!cn)\n+\t\treturn -ENODEV;\n+\n+\tif (cn->purged) {\n+\t\tret = -ENODEV;\n+\t\tgoto out;\n+\t}\n+\n+\t/* Block new demotions at maximum pressure */\n+\tif (READ_ONCE(cn->pressure) >= CRAM_PRESSURE_MAX) {\n+\t\tret = -ENOSPC;\n+\t\tgoto out;\n+\t}\n+\n+\tret = migrate_pages(demote_folios, alloc_cram_folio, cram_put_new_folio,\n+\t\t\t    (unsigned long)to_nid, mode, reason,\n+\t\t\t    &nr_success);\n+\n+\t/*\n+\t * migrate_folio_move() calls folio_add_lru() for each migrated\n+\t * folio, but that only adds the folio to a per-CPU batch, \n+\t * PG_lru is not set until the batch is drained.  Drain now so\n+\t * that cram_fault() can isolate these folios immediately.\n+\t *\n+\t * Use lru_add_drain_all() because migrate_pages() may process\n+\t * folios across CPUs, and the local drain might miss batches\n+\t * filled on other CPUs.\n+\t */\n+\tif (nr_success)\n+\t\tlru_add_drain_all();\n+out:\n+\tput_cram_node(cn);\n+\tif (nr_succeeded)\n+\t\t*nr_succeeded = nr_success;\n+\treturn ret;\n+}\n+\n+static void cram_release_ptl(struct vm_fault *vmf, enum pgtable_level level)\n+{\n+\tif (level == PGTABLE_LEVEL_PTE)\n+\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n+\telse\n+\t\tspin_unlock(vmf->ptl);\n+}\n+\n+static vm_fault_t cram_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t     enum pgtable_level level)\n+{\n+\tstruct folio *f, *f2;\n+\tstruct cram_node *cn;\n+\tunsigned int nr_succeeded = 0;\n+\tint nid;\n+\tLIST_HEAD(folios);\n+\n+\tnid = folio_nid(folio);\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn) {\n+\t\tcram_release_ptl(vmf, level);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Isolate from LRU while holding PTL.  This serializes against\n+\t * other CPUs faulting on the same folio: only one CPU can clear\n+\t * PG_lru under the PTL, and it proceeds to migration.  Other\n+\t * CPUs find the folio already isolated and bail out, preventing\n+\t * the refcount pile-up that causes migrate_pages() to fail with\n+\t * -EAGAIN.\n+\t *\n+\t * No explicit folio_get() is needed: the page table entry holds\n+\t * a reference (we still hold PTL), and folio_isolate_lru() takes\n+\t * its own reference.  This matches do_numa_page()'s pattern.\n+\t *\n+\t * PG_lru should already be set: cram_migrate_to() drains per-CPU\n+\t * LRU batches after migration, and the failure path below\n+\t * drains after putback.\n+\t */\n+\tif (!folio_isolate_lru(folio)) {\n+\t\tput_cram_node(cn);\n+\t\tcram_release_ptl(vmf, level);\n+\t\tcond_resched();\n+\t\treturn 0;\n+\t}\n+\n+\t/* Folio isolated, release PTL, proceed to migration */\n+\tcram_release_ptl(vmf, level);\n+\n+\tnode_stat_mod_folio(folio,\n+\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(folio),\n+\t\t\t    folio_nr_pages(folio));\n+\tlist_add(&folio->lru, &folios);\n+\n+\tmigrate_pages(&folios, alloc_cram_promote_folio, NULL,\n+\t\t      (unsigned long)numa_node_id(),\n+\t\t      MIGRATE_SYNC, MR_NUMA_MISPLACED, &nr_succeeded);\n+\n+\t/* Put failed folios back on LRU; retry on next fault */\n+\tlist_for_each_entry_safe(f, f2, &folios, lru) {\n+\t\tlist_del(&f->lru);\n+\t\tnode_stat_mod_folio(f,\n+\t\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(f),\n+\t\t\t\t    -folio_nr_pages(f));\n+\t\tfolio_putback_lru(f);\n+\t}\n+\n+\t/*\n+\t * If migration failed, folio_putback_lru() batched the folio\n+\t * into this CPU's per-CPU LRU cache (PG_lru not yet set).\n+\t * Drain now so the folio is immediately visible on the LRU,\n+\t * the next fault can then isolate it without an IPI storm\n+\t * via lru_add_drain_all().\n+\t *\n+\t * Return VM_FAULT_RETRY after releasing the fault lock so the\n+\t * arch handler retries from scratch.  Without this, returning 0\n+\t * causes a tight livelock: the process immediately re-faults on\n+\t * the same write-protected entry, alloc fails again, and\n+\t * VM_FAULT_OOM eventually leaks out through a stale path.\n+\t * VM_FAULT_RETRY gives the system breathing room to reclaim.\n+\t */\n+\tif (!nr_succeeded) {\n+\t\tlru_add_drain();\n+\t\tcond_resched();\n+\t\tput_cram_node(cn);\n+\t\trelease_fault_lock(vmf);\n+\t\treturn VM_FAULT_RETRY;\n+\t}\n+\n+\tcond_resched();\n+\tput_cram_node(cn);\n+\treturn 0;\n+}\n+\n+static void cram_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static void cram_reclaim_policy(int nid, struct node_reclaim_policy *policy)\n+{\n+\tpolicy->may_swap = true;\n+\tpolicy->may_writepage = true;\n+\tpolicy->managed_watermarks = true;\n+}\n+\n+static vm_fault_t cram_handle_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t    enum pgtable_level level)\n+{\n+\treturn cram_fault(folio, vmf, level);\n+}\n+\n+static const struct node_private_ops cram_ops = {\n+\t.handle_fault\t\t= cram_handle_fault,\n+\t.migrate_to\t\t= cram_migrate_to,\n+\t.folio_migrate\t\t= cram_folio_migrate,\n+\t.free_folio\t\t= cram_free_folio_cb,\n+\t.reclaim_policy\t\t= cram_reclaim_policy,\n+\t.flags\t\t\t= NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n+\t\t\t\t  NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE |\n+\t\t\t\t  NP_OPS_RECLAIM,\n+};\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data)\n+{\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (cn) {\n+\t\tif (cn->owner != owner) {\n+\t\t\tmutex_unlock(&cram_mutex);\n+\t\t\treturn -EBUSY;\n+\t\t}\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn 0;\n+\t}\n+\n+\tcn = kzalloc(sizeof(*cn), GFP_KERNEL);\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENOMEM;\n+\t}\n+\n+\tcn->owner = owner;\n+\tcn->pressure = 0;\n+\tcn->flush_cb = flush_cb;\n+\tcn->flush_data = flush_data;\n+\trefcount_set(&cn->refcount, 1);\n+\n+\tret = node_private_set_ops(nid, &cram_ops);\n+\tif (ret) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\tkfree(cn);\n+\t\treturn ret;\n+\t}\n+\n+\trcu_assign_pointer(cram_nodes[nid], cn);\n+\n+\t/* Start kswapd on the private node for LRU aging and reclaim */\n+\tkswapd_run(nid);\n+\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* Now that ops->migrate_to is set, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_register_private_node);\n+\n+int cram_unregister_private_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tkswapd_stop(nid);\n+\n+\tWARN_ON(node_private_clear_ops(nid, &cram_ops));\n+\trcu_assign_pointer(cram_nodes[nid], NULL);\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* ops->migrate_to cleared, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\n+\tsynchronize_rcu();\n+\twhile (!refcount_dec_if_one(&cn->refcount))\n+\t\tcond_resched();\n+\tkfree(cn);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unregister_private_node);\n+\n+int cram_unpurge(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tcn->purged = false;\n+\n+\tmutex_unlock(&cram_mutex);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unpurge);\n+\n+void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+\tstruct cram_node *cn;\n+\tstruct node_private *np;\n+\tstruct zone *zone;\n+\tunsigned long managed, boost;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\treturn;\n+\n+\tif (pressure > CRAM_PRESSURE_MAX)\n+\t\tpressure = CRAM_PRESSURE_MAX;\n+\n+\tWRITE_ONCE(cn->pressure, pressure);\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\t/* Block demotions only at maximum pressure */\n+\tif (np)\n+\t\tWRITE_ONCE(np->migration_blocked,\n+\t\t\t   pressure >= CRAM_PRESSURE_MAX);\n+\trcu_read_unlock();\n+\n+\tzone = NULL;\n+\tfor (int i = 0; i < MAX_NR_ZONES; i++) {\n+\t\tstruct zone *z = &NODE_DATA(nid)->node_zones[i];\n+\n+\t\tif (zone_managed_pages(z) > 0) {\n+\t\t\tzone = z;\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+\tif (!zone) {\n+\t\tput_cram_node(cn);\n+\t\treturn;\n+\t}\n+\tmanaged = zone_managed_pages(zone);\n+\n+\t/* Boost proportional to pressure. 0:no boost, 1000:full managed */\n+\tboost = (managed * (unsigned long)pressure) / CRAM_PRESSURE_MAX;\n+\tWRITE_ONCE(zone->watermark_boost, boost);\n+\n+\tif (boost) {\n+\t\tset_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);\n+\t\twakeup_kswapd(zone, GFP_KERNEL, 0, ZONE_MOVABLE);\n+\t}\n+\n+\tput_cram_node(cn);\n+}\n+EXPORT_SYMBOL_GPL(cram_set_pressure);\n+\n+void cram_clear_pressure(int nid)\n+{\n+\tcram_set_pressure(nid, 0);\n+}\n+EXPORT_SYMBOL_GPL(cram_clear_pressure);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-24-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the driver's memory hotplug lifecycle, specifically how it eliminates the intermediate dax_region/dax device layer and directly performs memory hotplug operations. The author explains that this approach supports memory tier integration for proper NUMA placement and will be extended to support private memory nodes in the future.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "The driver registers a sysram_regionN device as a child of the CXL region, managing the memory hotplug lifecycle through device add/remove."
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CXL sysram region for direct memory hotplug of CXL RAM regions.\n\nThis region eliminates the intermediate dax_region/dax device layer by\ndirectly performing memory hotplug operations.\n\nKey features:\n- Supports memory tier integration for proper NUMA placement\n- Uses the CXL_SYSRAM_ONLINE_* Kconfig options for default online type\n- Automatically hotplugs memory on probe if online type is configured\n- Will be extended to support private memory nodes in the future\n\nThe driver registers a sysram_regionN device as a child of the CXL\nregion, managing the memory hotplug lifecycle through device add/remove.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/Makefile        |   1 +\n drivers/cxl/core/core.h          |   4 +\n drivers/cxl/core/port.c          |   2 +\n drivers/cxl/core/region_sysram.c | 351 +++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h                |  48 +++++\n 5 files changed, 406 insertions(+)\n create mode 100644 drivers/cxl/core/region_sysram.c\n\ndiff --git a/drivers/cxl/core/Makefile b/drivers/cxl/core/Makefile\nindex d3ec8aea64c5..d7ce52c50810 100644\n--- a/drivers/cxl/core/Makefile\n+++ b/drivers/cxl/core/Makefile\n@@ -18,6 +18,7 @@ cxl_core-$(CONFIG_TRACING) += trace.o\n cxl_core-$(CONFIG_CXL_REGION) += region.o\n cxl_core-$(CONFIG_CXL_REGION) += region_dax.o\n cxl_core-$(CONFIG_CXL_REGION) += region_pmem.o\n+cxl_core-$(CONFIG_CXL_REGION) += region_sysram.o\n cxl_core-$(CONFIG_CXL_MCE) += mce.o\n cxl_core-$(CONFIG_CXL_FEATURES) += features.o\n cxl_core-$(CONFIG_CXL_EDAC_MEM_FEATURES) += edac.o\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 6e1f695fd155..973bbcae43f7 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -35,6 +35,7 @@ extern struct device_attribute dev_attr_delete_region;\n extern struct device_attribute dev_attr_region;\n extern const struct device_type cxl_pmem_region_type;\n extern const struct device_type cxl_dax_region_type;\n+extern const struct device_type cxl_sysram_type;\n extern const struct device_type cxl_region_type;\n \n int cxl_decoder_detach(struct cxl_region *cxlr,\n@@ -46,6 +47,7 @@ int cxl_decoder_detach(struct cxl_region *cxlr,\n #define SET_CXL_REGION_ATTR(x) (&dev_attr_##x.attr),\n #define CXL_PMEM_REGION_TYPE(x) (&cxl_pmem_region_type)\n #define CXL_DAX_REGION_TYPE(x) (&cxl_dax_region_type)\n+#define CXL_SYSRAM_TYPE(x) (&cxl_sysram_type)\n int cxl_region_init(void);\n void cxl_region_exit(void);\n int cxl_get_poison_by_endpoint(struct cxl_port *port);\n@@ -54,6 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\n@@ -88,6 +91,7 @@ static inline void cxl_region_exit(void)\n #define SET_CXL_REGION_ATTR(x)\n #define CXL_PMEM_REGION_TYPE(x) NULL\n #define CXL_DAX_REGION_TYPE(x) NULL\n+#define CXL_SYSRAM_TYPE(x) NULL\n #endif\n \n struct cxl_send_command;\ndiff --git a/drivers/cxl/core/port.c b/drivers/cxl/core/port.c\nindex 5c82e6f32572..d6e82b3c2b64 100644\n--- a/drivers/cxl/core/port.c\n+++ b/drivers/cxl/core/port.c\n@@ -66,6 +66,8 @@ static int cxl_device_id(const struct device *dev)\n \t\treturn CXL_DEVICE_PMEM_REGION;\n \tif (dev->type == CXL_DAX_REGION_TYPE())\n \t\treturn CXL_DEVICE_DAX_REGION;\n+\tif (dev->type == CXL_SYSRAM_TYPE())\n+\t\treturn CXL_DEVICE_SYSRAM;\n \tif (is_cxl_port(dev)) {\n \t\tif (is_cxl_root(to_cxl_port(dev)))\n \t\t\treturn CXL_DEVICE_ROOT;\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nnew file mode 100644\nindex 000000000000..47a415deb352\n--- /dev/null\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -0,0 +1,351 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Sysram Region - Direct memory hotplug for CXL RAM regions\n+ *\n+ * This interface directly performs memory hotplug for CXL RAM regions,\n+ * eliminating the indirection through DAX.\n+ */\n+\n+#include <linux/memory_hotplug.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/memory.h>\n+#include <linux/device.h>\n+#include <linux/slab.h>\n+#include <linux/mm.h>\n+#include <cxlmem.h>\n+#include <cxl.h>\n+#include \"core.h\"\n+\n+static const char *sysram_res_name = \"System RAM (CXL)\";\n+\n+/**\n+ * cxl_region_find_sysram - Find the sysram device associated with a region\n+ * @cxlr: The CXL region\n+ *\n+ * Finds and returns the sysram child device of a CXL region.\n+ * The caller must release the device reference with put_device()\n+ * when done with the returned pointer.\n+ *\n+ * Return: Pointer to cxl_sysram, or NULL if not found\n+ */\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram;\n+\tstruct device *sdev;\n+\tchar sname[32];\n+\n+\tsnprintf(sname, sizeof(sname), \"sysram_region%d\", cxlr->id);\n+\tsdev = device_find_child_by_name(&cxlr->dev, sname);\n+\tif (!sdev)\n+\t\treturn NULL;\n+\n+\tsysram = to_cxl_sysram(sdev);\n+\treturn sysram;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_region_find_sysram, \"CXL\");\n+\n+static int sysram_get_numa_node(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_region_params *p = &cxlr->params;\n+\tint nid;\n+\n+\tnid = phys_to_target_node(p->res->start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(p->res->start);\n+\n+\treturn nid;\n+}\n+\n+static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n+{\n+\tstruct resource *res;\n+\tmhp_t mhp_flags;\n+\tint rc;\n+\n+\tif (sysram->res)\n+\t\treturn -EBUSY;\n+\n+\tres = request_mem_region(sysram->hpa_range.start,\n+\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t sysram->res_name);\n+\tif (!res)\n+\t\treturn -EBUSY;\n+\n+\tsysram->res = res;\n+\n+\t/*\n+\t * Set flags appropriate for System RAM. Leave ..._BUSY clear\n+\t * so that add_memory() can add a child resource.\n+\t */\n+\tres->flags = IORESOURCE_SYSTEM_RAM;\n+\n+\tmhp_flags = MHP_NID_IS_MGID;\n+\n+\t/*\n+\t * Ensure that future kexec'd kernels will not treat\n+\t * this as RAM automatically.\n+\t */\n+\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t online_type);\n+\tif (rc) {\n+\t\tremove_resource(res);\n+\t\tkfree(res);\n+\t\tsysram->res = NULL;\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n+{\n+\tint rc;\n+\n+\tif (!sysram->res)\n+\t\treturn 0;\n+\n+\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t       range_len(&sysram->hpa_range));\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tif (sysram->res) {\n+\t\tremove_resource(sysram->res);\n+\t\tkfree(sysram->res);\n+\t\tsysram->res = NULL;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn sysram_hotplug_remove(sysram);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_offline_and_remove, \"CXL\");\n+\n+static void cxl_sysram_release(struct device *dev)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\n+\tif (sysram->res)\n+\t\tsysram_hotplug_remove(sysram);\n+\n+\tkfree(sysram->res_name);\n+\n+\tif (sysram->mgid >= 0)\n+\t\tmemory_group_unregister(sysram->mgid);\n+\n+\tif (sysram->mtype)\n+\t\tclear_node_memory_type(sysram->numa_node, sysram->mtype);\n+\n+\tkfree(sysram);\n+}\n+\n+static ssize_t hotplug_store(struct device *dev,\n+\t\t\t     struct device_attribute *attr,\n+\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\tint online_type, rc;\n+\n+\tonline_type = mhp_online_type_from_str(buf);\n+\tif (online_type < 0)\n+\t\treturn online_type;\n+\n+\tif (online_type == MMOP_OFFLINE)\n+\t\trc = sysram_hotplug_remove(sysram);\n+\telse\n+\t\trc = sysram_hotplug_add(sysram, online_type);\n+\n+\tif (rc)\n+\t\tdev_warn(dev, \"hotplug %s failed: %d\\n\",\n+\t\t\t online_type == MMOP_OFFLINE ? \"offline\" : \"online\", rc);\n+\n+\treturn rc ? rc : len;\n+}\n+static DEVICE_ATTR_WO(hotplug);\n+\n+static struct attribute *cxl_sysram_attrs[] = {\n+\t&dev_attr_hotplug.attr,\n+\tNULL\n+};\n+\n+static const struct attribute_group cxl_sysram_attribute_group = {\n+\t.attrs = cxl_sysram_attrs,\n+};\n+\n+static const struct attribute_group *cxl_sysram_attribute_groups[] = {\n+\t&cxl_base_attribute_group,\n+\t&cxl_sysram_attribute_group,\n+\tNULL\n+};\n+\n+const struct device_type cxl_sysram_type = {\n+\t.name = \"cxl_sysram\",\n+\t.release = cxl_sysram_release,\n+\t.groups = cxl_sysram_attribute_groups,\n+};\n+\n+static bool is_cxl_sysram(struct device *dev)\n+{\n+\treturn dev->type == &cxl_sysram_type;\n+}\n+\n+struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\tif (dev_WARN_ONCE(dev, !is_cxl_sysram(dev),\n+\t\t\t  \"not a cxl_sysram device\\n\"))\n+\t\treturn NULL;\n+\treturn container_of(dev, struct cxl_sysram, dev);\n+}\n+EXPORT_SYMBOL_NS_GPL(to_cxl_sysram, \"CXL\");\n+\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram)\n+{\n+\treturn &sysram->dev;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_dev, \"CXL\");\n+\n+static struct lock_class_key cxl_sysram_key;\n+\n+static enum mmop cxl_sysram_get_default_online_type(void)\n+{\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_SYSTEM_DEFAULT))\n+\t\treturn mhp_get_default_online_type();\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_MOVABLE))\n+\t\treturn MMOP_ONLINE_MOVABLE;\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_NORMAL))\n+\t\treturn MMOP_ONLINE;\n+\treturn MMOP_OFFLINE;\n+}\n+\n+static struct cxl_sysram *cxl_sysram_alloc(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram __free(kfree) = NULL;\n+\tstruct device *dev;\n+\n+\tsysram = kzalloc(sizeof(*sysram), GFP_KERNEL);\n+\tif (!sysram)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tsysram->online_type = cxl_sysram_get_default_online_type();\n+\tsysram->last_hotplug_cmd = MMOP_OFFLINE;\n+\tsysram->numa_node = -1;\n+\tsysram->mgid = -1;\n+\n+\tdev = &sysram->dev;\n+\tsysram->cxlr = cxlr;\n+\tdevice_initialize(dev);\n+\tlockdep_set_class(&dev->mutex, &cxl_sysram_key);\n+\tdevice_set_pm_not_required(dev);\n+\tdev->parent = &cxlr->dev;\n+\tdev->bus = &cxl_bus_type;\n+\tdev->type = &cxl_sysram_type;\n+\n+\treturn_ptr(sysram);\n+}\n+\n+static void sysram_unregister(void *_sysram)\n+{\n+\tstruct cxl_sysram *sysram = _sysram;\n+\n+\tdevice_unregister(&sysram->dev);\n+}\n+\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+{\n+\tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n+\tstruct memory_dev_type *mtype;\n+\tstruct range hpa_range;\n+\tstruct device *dev;\n+\tint adist = MEMTIER_DEFAULT_LOWTIER_ADISTANCE;\n+\tint numa_node;\n+\tint rc;\n+\n+\trc = cxl_region_get_hpa_range(cxlr, &hpa_range);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\thpa_range = memory_block_align_range(&hpa_range);\n+\tif (hpa_range.start >= hpa_range.end) {\n+\t\tdev_warn(&cxlr->dev, \"region too small after alignment\\n\");\n+\t\treturn -ENOSPC;\n+\t}\n+\n+\tsysram = cxl_sysram_alloc(cxlr);\n+\tif (IS_ERR(sysram))\n+\t\treturn PTR_ERR(sysram);\n+\n+\tsysram->hpa_range = hpa_range;\n+\n+\tsysram->res_name = kasprintf(GFP_KERNEL, \"cxl_sysram%d\", cxlr->id);\n+\tif (!sysram->res_name)\n+\t\treturn -ENOMEM;\n+\n+\t/* Override default online type if caller specified one */\n+\tif (online_type >= 0)\n+\t\tsysram->online_type = online_type;\n+\n+\tdev = &sysram->dev;\n+\n+\trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Setup memory tier before adding device */\n+\tnuma_node = sysram_get_numa_node(cxlr);\n+\tif (numa_node < 0) {\n+\t\tdev_warn(&cxlr->dev, \"rejecting region with invalid node: %d\\n\",\n+\t\t\t numa_node);\n+\t\treturn -EINVAL;\n+\t}\n+\tsysram->numa_node = numa_node;\n+\n+\tmt_calc_adistance(numa_node, &adist);\n+\tmtype = mt_get_memory_type(adist);\n+\tif (IS_ERR(mtype))\n+\t\treturn PTR_ERR(mtype);\n+\tsysram->mtype = mtype;\n+\n+\tinit_node_memory_type(numa_node, mtype);\n+\n+\t/* Register memory group for this region */\n+\trc = memory_group_register_static(numa_node,\n+\t\t\t\t\t  PFN_UP(range_len(&hpa_range)));\n+\tif (rc < 0)\n+\t\treturn rc;\n+\tsysram->mgid = rc;\n+\n+\trc = device_add(dev);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n+\t\tdev_name(dev));\n+\n+\t/*\n+\t * Dynamic capacity regions (DCD) will have memory added later.\n+\t * For static RAM regions, hotplug the entire range now.\n+\t */\n+\tif (cxlr->mode != CXL_PARTMODE_RAM)\n+\t\tgoto out;\n+\n+\t/* If default online_type is a valid online mode, immediately hotplug */\n+\tif (sysram->online_type > MMOP_OFFLINE) {\n+\t\trc = sysram_hotplug_add(sysram, sysram->online_type);\n+\t\tif (rc)\n+\t\t\tdev_warn(dev, \"hotplug failed: %d\\n\", rc);\n+\t\telse\n+\t\t\tsysram->last_hotplug_cmd = sysram->online_type;\n+\t}\n+\n+out:\n+\treturn devm_add_action_or_reset(&cxlr->dev, sysram_unregister,\n+\t\t\t\t\tno_free_ptr(sysram));\n+}\n+EXPORT_SYMBOL_NS_GPL(devm_cxl_add_sysram, \"CXL\");\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex f899f240f229..8e8342fd4fde 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -607,6 +607,34 @@ struct cxl_dax_region {\n \tenum dax_driver_type dax_driver;\n };\n \n+/**\n+ * struct cxl_sysram - CXL SysRAM region for system memory hotplug\n+ * @dev: device for this sysram\n+ * @cxlr: parent cxl_region\n+ * @online_type: Default memory online type for new hotplug ops (MMOP_* value)\n+ * @last_hotplug_cmd: Last hotplug command submitted (MMOP_* value)\n+ * @hpa_range: Host physical address range for the region\n+ * @res_name: Resource name for the memory region\n+ * @res: Memory resource (set when hotplugged)\n+ * @mgid: Memory group id\n+ * @mtype: Memory tier type\n+ * @numa_node: NUMA node for this memory\n+ *\n+ * Device that directly performs memory hotplug for CXL RAM regions.\n+ */\n+struct cxl_sysram {\n+\tstruct device dev;\n+\tstruct cxl_region *cxlr;\n+\tenum mmop online_type;\n+\tint last_hotplug_cmd;\n+\tstruct range hpa_range;\n+\tconst char *res_name;\n+\tstruct resource *res;\n+\tint mgid;\n+\tstruct memory_dev_type *mtype;\n+\tint numa_node;\n+};\n+\n /**\n  * struct cxl_port - logical collection of upstream port devices and\n  *\t\t     downstream port devices to construct a CXL memory\n@@ -807,6 +835,7 @@ DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device\n DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n DEFINE_FREE(put_cxl_dax_region, struct cxl_dax_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n+DEFINE_FREE(put_cxl_sysram, struct cxl_sysram *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n \n int devm_cxl_enumerate_ports(struct cxl_memdev *cxlmd);\n void cxl_bus_rescan(void);\n@@ -889,6 +918,7 @@ void cxl_destroy_region(struct cxl_region *cxlr);\n struct device *cxl_region_dev(struct cxl_region *cxlr);\n enum cxl_partition_mode cxl_region_mode(struct cxl_region *cxlr);\n int cxl_get_region_range(struct cxl_region *cxlr, struct range *range);\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr);\n int cxl_get_committed_regions(struct cxl_memdev *cxlmd,\n \t\t\t      struct cxl_region **regions, int max_regions);\n struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n@@ -936,6 +966,7 @@ void cxl_driver_unregister(struct cxl_driver *cxl_drv);\n #define CXL_DEVICE_PMEM_REGION\t\t7\n #define CXL_DEVICE_DAX_REGION\t\t8\n #define CXL_DEVICE_PMU\t\t\t9\n+#define CXL_DEVICE_SYSRAM\t\t10\n \n #define MODULE_ALIAS_CXL(type) MODULE_ALIAS(\"cxl:t\" __stringify(type) \"*\")\n #define CXL_MODALIAS_FMT \"cxl:t%d\"\n@@ -954,6 +985,10 @@ bool is_cxl_pmem_region(struct device *dev);\n struct cxl_pmem_region *to_cxl_pmem_region(struct device *dev);\n int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n+struct cxl_sysram *to_cxl_sysram(struct device *dev);\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n static inline bool is_cxl_pmem_region(struct device *dev)\n@@ -972,6 +1007,19 @@ static inline struct cxl_dax_region *to_cxl_dax_region(struct device *dev)\n {\n \treturn NULL;\n }\n+static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\treturn NULL;\n+}\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+\t\t\t\t      enum mmop online_type)\n+{\n+\treturn -ENXIO;\n+}\n+static inline int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn -ENXIO;\n+}\n static inline u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint,\n \t\t\t\t\t       u64 spa)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-25-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the cxl_sysram region not supporting N_MEMORY_PRIVATE hotplug, and responded by modifying the devm_cxl_add_sysram() function to take an additional 'private' parameter that enables private node registration when set. The author also updated the sysram_hotplug_add() function to use add_private_memory_driver_managed() for private regions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Extend the cxl_sysram region to support N_MEMORY_PRIVATE hotplug\nvia add_private_memory_driver_managed(). When a caller passes\nprivate=true to devm_cxl_add_sysram(), the memory is registered\nas a private node, isolating it from normal allocations and reclaim.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/core.h          |  2 +-\n drivers/cxl/core/region_sysram.c | 50 +++++++++++++++++++++++++-------\n drivers/cxl/cxl.h                |  9 ++++--\n 3 files changed, 48 insertions(+), 13 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 973bbcae43f7..8ca3d6d41fe4 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -56,7 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nindex 47a415deb352..77aaa52e7332 100644\n--- a/drivers/cxl/core/region_sysram.c\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -85,12 +85,23 @@ static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n \t/*\n \t * Ensure that future kexec'd kernels will not treat\n \t * this as RAM automatically.\n+\t *\n+\t * For private regions, use add_private_memory_driver_managed()\n+\t * to register as N_MEMORY_PRIVATE which isolates the memory from\n+\t * normal allocations and reclaim.\n \t */\n-\trc = __add_memory_driver_managed(sysram->mgid,\n-\t\t\t\t\t sysram->hpa_range.start,\n-\t\t\t\t\t range_len(&sysram->hpa_range),\n-\t\t\t\t\t sysram_res_name, mhp_flags,\n-\t\t\t\t\t online_type);\n+\tif (sysram->private)\n+\t\trc = add_private_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t       sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t       online_type, &sysram->np);\n+\telse\n+\t\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t online_type);\n \tif (rc) {\n \t\tremove_resource(res);\n \t\tkfree(res);\n@@ -108,10 +119,23 @@ static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n \tif (!sysram->res)\n \t\treturn 0;\n \n-\trc = offline_and_remove_memory(sysram->hpa_range.start,\n-\t\t\t\t       range_len(&sysram->hpa_range));\n-\tif (rc)\n-\t\treturn rc;\n+\tif (sysram->private) {\n+\t\trc = offline_and_remove_private_memory(sysram->numa_node,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\t/*\n+\t\t * -EBUSY means memory was removed but node_private_unregister()\n+\t\t * could not complete because other regions share the node.\n+\t\t * Continue to resource cleanup since the memory is gone.\n+\t\t */\n+\t\tif (rc && rc != -EBUSY)\n+\t\t\treturn rc;\n+\t} else {\n+\t\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\t}\n \n \tif (sysram->res) {\n \t\tremove_resource(sysram->res);\n@@ -257,7 +281,8 @@ static void sysram_unregister(void *_sysram)\n \tdevice_unregister(&sysram->dev);\n }\n \n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n+\t\t\tenum mmop online_type)\n {\n \tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n \tstruct memory_dev_type *mtype;\n@@ -291,6 +316,11 @@ int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n \tif (online_type >= 0)\n \t\tsysram->online_type = online_type;\n \n+\t/* Set up private node registration if requested */\n+\tsysram->private = private;\n+\tif (private)\n+\t\tsysram->np.owner = sysram;\n+\n \tdev = &sysram->dev;\n \n \trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 8e8342fd4fde..54e5f9ac59dc 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -10,6 +10,7 @@\n #include <linux/bitops.h>\n #include <linux/log2.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n #include <linux/io.h>\n #include <linux/range.h>\n #include <linux/dax.h>\n@@ -619,6 +620,8 @@ struct cxl_dax_region {\n  * @mgid: Memory group id\n  * @mtype: Memory tier type\n  * @numa_node: NUMA node for this memory\n+ * @private: true if this region uses N_MEMORY_PRIVATE hotplug\n+ * @np: private node registration state (valid when @private is true)\n  *\n  * Device that directly performs memory hotplug for CXL RAM regions.\n  */\n@@ -633,6 +636,8 @@ struct cxl_sysram {\n \tint mgid;\n \tstruct memory_dev_type *mtype;\n \tint numa_node;\n+\tbool private;\n+\tstruct node_private np;\n };\n \n /**\n@@ -987,7 +992,7 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n struct cxl_sysram *to_cxl_sysram(struct device *dev);\n struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n@@ -1011,7 +1016,7 @@ static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n {\n \treturn NULL;\n }\n-static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n \t\t\t\t      enum mmop online_type)\n {\n \treturn -ENXIO;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-26-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the lack of PTE controls in the CXL mempolicy driver, explaining that the memory behaves like normal DRAM but is isolated from default allocations and can only be reached via explicit mempolicy (set_mempolicy or mbind).",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a sample CXL type-3 driver that registers device memory as\nprivate-node NUMA memory reachable only via explicit mempolicy\n(set_mempolicy / mbind).\n\nProbe flow:\n  1. Call cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Look for pre-committed RAM regions; if none exist, create one\n     using cxl_get_hpa_freespace() + cxl_request_dpa() +\n     cxl_create_region()\n  3. Convert the region to sysram via devm_cxl_add_sysram() with\n     private=true and MMOP_ONLINE_MOVABLE\n  4. Register node_private_ops with NP_OPS_MIGRATION | NP_OPS_MEMPOLICY\n     so the node is excluded from default allocations\n\nThe migrate_to callback uses alloc_migration_target() with\n__GFP_THISNODE | __GFP_PRIVATE to keep pages on the target node.\n\nMove struct migration_target_control from mm/internal.h to\ninclude/linux/migrate.h so the driver can use alloc_migration_target()\nwithout depending on mm-internal headers.\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/Kconfig                           |   2 +\n drivers/cxl/Makefile                          |   2 +\n drivers/cxl/type3_drivers/Kconfig             |   2 +\n drivers/cxl/type3_drivers/Makefile            |   2 +\n .../cxl/type3_drivers/cxl_mempolicy/Kconfig   |  16 +\n .../cxl/type3_drivers/cxl_mempolicy/Makefile  |   4 +\n .../type3_drivers/cxl_mempolicy/mempolicy.c   | 297 ++++++++++++++++++\n include/linux/migrate.h                       |   7 +-\n mm/internal.h                                 |   7 -\n 9 files changed, 331 insertions(+), 8 deletions(-)\n create mode 100644 drivers/cxl/type3_drivers/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n\ndiff --git a/drivers/cxl/Kconfig b/drivers/cxl/Kconfig\nindex f99aa7274d12..1648cdeaa0c9 100644\n--- a/drivers/cxl/Kconfig\n+++ b/drivers/cxl/Kconfig\n@@ -278,4 +278,6 @@ config CXL_ATL\n \tdepends on CXL_REGION\n \tdepends on ACPI_PRMT && AMD_NB\n \n+source \"drivers/cxl/type3_drivers/Kconfig\"\n+\n endif\ndiff --git a/drivers/cxl/Makefile b/drivers/cxl/Makefile\nindex 2caa90fa4bf2..94d2b2233bf8 100644\n--- a/drivers/cxl/Makefile\n+++ b/drivers/cxl/Makefile\n@@ -19,3 +19,5 @@ cxl_acpi-y := acpi.o\n cxl_pmem-y := pmem.o security.o\n cxl_mem-y := mem.o\n cxl_pci-y := pci.o\n+\n+obj-y += type3_drivers/\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nnew file mode 100644\nindex 000000000000..369b21763856\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nnew file mode 100644\nindex 000000000000..2b82265ff118\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\nnew file mode 100644\nindex 000000000000..3c45da237b9f\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n@@ -0,0 +1,16 @@\n+config CXL_MEMPOLICY\n+\ttristate \"CXL Private Memory with Mempolicy Support\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\thelp\n+\t  Minimal driver for CXL memory devices that registers memory as\n+\t  N_MEMORY_PRIVATE with mempolicy support.  The memory is isolated\n+\t  from default allocations and can only be reached via explicit\n+\t  mempolicy (set_mempolicy or mbind).\n+\n+\t  No compression, no PTE controls, the memory behaves like normal\n+\t  DRAM but is excluded from fallback allocations.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\nnew file mode 100644\nindex 000000000000..dfb58fc88ad9\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy.o\n+cxl_mempolicy-y := mempolicy.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\nnew file mode 100644\nindex 000000000000..1c19818eb268\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n@@ -0,0 +1,297 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Mempolicy Driver\n+ *\n+ * Minimal driver for CXL memory devices that registers memory as\n+ * N_MEMORY_PRIVATE with mempolicy support but no PTE controls.  The\n+ * memory behaves like normal DRAM but is isolated from default allocations,\n+ * it can only be reached via explicit mempolicy (set_mempolicy/mbind).\n+ *\n+ * Usage:\n+ *   1. Unbind device from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   2. Bind to cxl_mempolicy:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n+ */\n+\n+#include <linux/module.h>\n+#include <linux/pci.h>\n+#include <linux/xarray.h>\n+#include <linux/node_private.h>\n+#include <linux/migrate.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+struct cxl_mempolicy_ctx {\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint nid;\n+};\n+\n+static DEFINE_XARRAY(ctx_xa);\n+\n+static struct cxl_mempolicy_ctx *memdev_to_ctx(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\n+\treturn xa_load(&ctx_xa, (unsigned long)pdev);\n+}\n+\n+static int cxl_mempolicy_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason,\n+\t\t\t\t    unsigned int *nr_succeeded)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE |\n+\t\t\t    __GFP_PRIVATE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, nr_succeeded);\n+}\n+\n+static void cxl_mempolicy_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static const struct node_private_ops cxl_mempolicy_ops = {\n+\t.migrate_to\t= cxl_mempolicy_migrate_to,\n+\t.folio_migrate\t= cxl_mempolicy_folio_migrate,\n+\t.flags = NP_OPS_MIGRATION | NP_OPS_MEMPOLICY,\n+};\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tctx->cxled = cxled;\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\treturn cxlr;\n+}\n+\n+static int setup_private_node(struct cxl_memdev *cxlmd,\n+\t\t\t      struct cxl_region *cxlr)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct range hpa_range;\n+\tint rc;\n+\n+\tdevice_release_driver(cxl_region_dev(cxlr));\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to add sysram: %d\\n\", rc);\n+\t\tif (device_attach(cxl_region_dev(cxlr)) < 0)\n+\t\t\tdev_warn(cxl_region_dev(cxlr),\n+\t\t\t\t \"failed to re-attach driver\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tctx->nid = phys_to_target_node(hpa_range.start);\n+\tif (ctx->nid == NUMA_NO_NODE)\n+\t\tctx->nid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\trc = node_private_set_ops(ctx->nid, &cxl_mempolicy_ops);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to set ops on node %d: %d\\n\", ctx->nid, rc);\n+\t\tctx->nid = NUMA_NO_NODE;\n+\t\treturn rc;\n+\t}\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"node %d registered as private mempolicy memory\\n\", ctx->nid);\n+\treturn 0;\n+}\n+\n+static int cxl_mempolicy_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i;\n+\tint rc;\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"cxl_mempolicy attach: looking for regions\\n\");\n+\n+\t/* Phase 1: look for pre-committed RAM regions */\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) != CXL_PARTMODE_RAM) {\n+\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tcxlr = regions[i];\n+\t\trc = setup_private_node(cxlmd, cxlr);\n+\t\tput_device(cxl_region_dev(cxlr));\n+\t\tif (rc == 0) {\n+\t\t\t/* Release remaining region references */\n+\t\t\tfor (i++; i < nr; i++)\n+\t\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\treturn 0;\n+\t\t}\n+\t}\n+\n+\t/* Phase 2: no committed regions, create one */\n+\tdev_info(&cxlmd->dev,\n+\t\t \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"no RAM capacity: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = setup_private_node(cxlmd, cxlr);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to setup private node: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\t/* Only take ownership of regions we created (Phase 2) */\n+\tmemdev_to_ctx(cxlmd)->cxlr = cxlr;\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_mempolicy_attach = {\n+\t.probe = cxl_mempolicy_attach_probe,\n+};\n+\n+static int cxl_mempolicy_probe(struct pci_dev *pdev,\n+\t\t\t       const struct pci_device_id *id)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probing device\\n\");\n+\n+\tctx = devm_kzalloc(&pdev->dev, sizeof(*ctx), GFP_KERNEL);\n+\tif (!ctx)\n+\t\treturn -ENOMEM;\n+\tctx->nid = NUMA_NO_NODE;\n+\n+\trc = xa_insert(&ctx_xa, (unsigned long)pdev, ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_mempolicy_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_mempolicy_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = xa_erase(&ctx_xa, (unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: removing device\\n\");\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\tif (ctx->nid != NUMA_NO_NODE)\n+\t\tWARN_ON(node_private_clear_ops(ctx->nid, &cxl_mempolicy_ops));\n+\n+\tif (ctx->cxlr) {\n+\t\tcxl_destroy_region(ctx->cxlr);\n+\t\tctx->cxlr = NULL;\n+\t}\n+\n+\tif (ctx->cxled) {\n+\t\tcxl_dpa_free(ctx->cxled);\n+\t\tctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_mempolicy_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_mempolicy_pci_tbl);\n+\n+static struct pci_driver cxl_mempolicy_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_mempolicy_pci_tbl,\n+\t.probe\t\t= cxl_mempolicy_probe,\n+\t.remove\t\t= cxl_mempolicy_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_mempolicy_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Private Memory with Mempolicy Support\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 7b2da3875ff2..1f9fb61f3932 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -10,7 +10,12 @@\n typedef struct folio *new_folio_t(struct folio *folio, unsigned long private);\n typedef void free_folio_t(struct folio *folio, unsigned long private);\n \n-struct migration_target_control;\n+struct migration_target_control {\n+\tint nid;\t\t/* preferred node id */\n+\tnodemask_t *nmask;\n+\tgfp_t gfp_mask;\n+\tenum migrate_reason reason;\n+};\n \n /**\n  * struct movable_operations - Driver page migration\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 64467ca774f1..85cd11189854 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1352,13 +1352,6 @@ extern const struct trace_print_flags gfpflag_names[];\n \n void setup_zone_pageset(struct zone *zone);\n \n-struct migration_target_control {\n-\tint nid;\t\t/* preferred node id */\n-\tnodemask_t *nmask;\n-\tgfp_t gfp_mask;\n-\tenum migrate_reason reason;\n-};\n-\n /*\n  * mm/filemap.c\n  */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-27-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the cxl_compression driver's interaction with the CRAM subsystem, specifically how it handles page reclamation and zeroing. The author explains that page reclamation uses the standard CXL Media Operations Zero command (opcode 0x4402), but falls back to inline CPU zeroing if the device does not support it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a generic CXL type-3 driver for compressed memory controllers.\n\nThe driver provides an alternative PCI binding that converts CXL\nRAM regions to private-node sysram and registers them with the\nCRAM subsystem for transparent demotion/promotion.\n\nProbe flow:\n  1. cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Discover/convert auto-RAM regions or create a RAM region\n  3. Convert to private-node sysram via devm_cxl_add_sysram()\n  4. Register with CRAM via cram_register_private_node()\n\nPage flush pipeline:\n  When a CRAM folio is freed, the CRAM free_folio   callback buffers\n  it into a per-CPU RCU-protected flush buffer to offload the operation.\n\n  A periodic kthread swaps the per-CPU buffers under RCU, then sends\n  batched Sanitize-Zero commands so the device can zero pages.\n\n  A flush_record bitmap tracks in-flight pages to avoid re-buffering on\n  the second free_folio entry after folio_put().\n\n  Overflow from full buffers is handled by a per-CPU workqueue fallback.\n\nWatermark interrupts:\n  MSI-X vector 12 - delivers \"Low\" watermark interrupts\n  MSI-X vector 13 - delivers \"High\" watermark interrupts\n  This adjusts CRAM pressure:\n\tLow  - increases pressure.\n  \tHigh - reduces pressure.\n\n  A dynamic watermark mode cycles through four phases with\n  progressively tighter thresholds.\n\n  Static watermark mode sets pressure 0 or MAX respectively.\n\nTeardown ordering:\n  pre_teardown  - cram_unregister + retry-loop memory offline\n  post_teardown - kthread stop, drain all flush buffers via CCI\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/type3_drivers/Kconfig             |    1 +\n drivers/cxl/type3_drivers/Makefile            |    1 +\n .../cxl/type3_drivers/cxl_compression/Kconfig |   20 +\n .../type3_drivers/cxl_compression/Makefile    |    4 +\n .../cxl_compression/compression.c             | 1025 +++++++++++++++++\n 5 files changed, 1051 insertions(+)\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/compression.c\n\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nindex 369b21763856..98f73e46730e 100644\n--- a/drivers/cxl/type3_drivers/Kconfig\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\n+source \"drivers/cxl/type3_drivers/cxl_compression/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nindex 2b82265ff118..f5b0766d92af 100644\n--- a/drivers/cxl/type3_drivers/Makefile\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression/\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Kconfig b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\nnew file mode 100644\nindex 000000000000..8c891a48b000\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\n@@ -0,0 +1,20 @@\n+config CXL_COMPRESSION\n+\ttristate \"CXL Compression Memory Driver\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on CRAM\n+\thelp\n+\t  This driver provides an alternative PCI binding for CXL memory\n+\t  devices with compressed memory support. It converts CXL RAM\n+\t  regions to sysram for direct memory hotplug and registers with\n+\t  the CRAM subsystem for transparent compression.\n+\n+\t  Page reclamation uses the standard CXL Media Operations Zero\n+\t  command (opcode 0x4402). If the device does not support it,\n+\t  the driver falls back to inline CPU zeroing.\n+\n+\t  Usage: First unbind the device from cxl_pci, then bind to\n+\t  cxl_compression. The driver will initialize the CXL device and\n+\t  convert any RAM regions to use direct memory hotplug via sysram.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Makefile b/drivers/cxl/type3_drivers/cxl_compression/Makefile\nnew file mode 100644\nindex 000000000000..46f34809bf74\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression.o\n+cxl_compression-y := compression.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/compression.c b/drivers/cxl/type3_drivers/cxl_compression/compression.c\nnew file mode 100644\nindex 000000000000..e4c8b62227e2\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/compression.c\n@@ -0,0 +1,1025 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Compression Driver\n+ *\n+ * This driver provides an alternative binding for CXL memory devices that\n+ * converts all associated RAM regions to sysram_regions for direct memory\n+ * hotplug, bypassing the standard dax region path.\n+ *\n+ * Page reclamation uses the standard CXL Media Operations Zero command\n+ * (opcode 0x4402, class 0x01, subclass 0x01).  Watermark interrupts\n+ * are delivered via separate MSI-X vectors (12 for lthresh, 13 for\n+ * hthresh), injected externally via QMP.\n+ *\n+ * Usage:\n+ *   1. Device initially binds to cxl_pci at boot\n+ *   2. Unbind from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   3. Bind to cxl_compression:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n+ */\n+\n+#include <linux/unaligned.h>\n+#include <linux/io-64-nonatomic-lo-hi.h>\n+#include <linux/module.h>\n+#include <linux/delay.h>\n+#include <linux/sizes.h>\n+#include <linux/mutex.h>\n+#include <linux/list.h>\n+#include <linux/pci.h>\n+#include <linux/io.h>\n+#include <linux/interrupt.h>\n+#include <linux/bitmap.h>\n+#include <linux/highmem.h>\n+#include <linux/workqueue.h>\n+#include <linux/kthread.h>\n+#include <linux/rcupdate.h>\n+#include <linux/percpu.h>\n+#include <linux/sched.h>\n+#include <linux/cram.h>\n+#include <linux/memory_hotplug.h>\n+#include <linux/xarray.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+/*\n+ * Per-device compression context lookup.\n+ *\n+ * pci_set_drvdata() MUST store cxlds because mbox_to_cxlds() uses\n+ * dev_get_drvdata() to recover the cxl_dev_state from the mailbox host\n+ * device.  Storing anything else in pci drvdata breaks every CXL mailbox\n+ * command.  Use an xarray keyed by pci_dev pointer so that multiple\n+ * devices can bind concurrently without colliding.\n+ */\n+static DEFINE_XARRAY(comp_ctx_xa);\n+\n+static struct cxl_compression_ctx *pdev_to_comp_ctx(struct pci_dev *pdev)\n+{\n+\treturn xa_load(&comp_ctx_xa, (unsigned long)pdev);\n+}\n+\n+#define CXL_MEDIA_OP_OPCODE\t\t0x4402\n+#define CXL_MEDIA_OP_CLASS_SANITIZE\t0x01\n+#define CXL_MEDIA_OP_SUBC_ZERO\t\t0x01\n+\n+struct cxl_dpa_range {\n+\t__le64 starting_dpa;\n+\t__le64 length;\n+} __packed;\n+\n+struct cxl_media_op_input {\n+\tu8 media_operation_class;\n+\tu8 media_operation_subclass;\n+\t__le16 reserved;\n+\t__le32 dpa_range_count;\n+\tstruct cxl_dpa_range ranges[];\n+} __packed;\n+\n+#define CXL_CT3_MSIX_LTHRESH\t\t12\n+#define CXL_CT3_MSIX_HTHRESH\t\t13\n+#define CXL_CT3_MSIX_VECTOR_NR\t\t14\n+#define CXL_FLUSH_INTERVAL_DEFAULT_MS\t1000\n+\n+static unsigned int flush_buf_size;\n+module_param(flush_buf_size, uint, 0444);\n+MODULE_PARM_DESC(flush_buf_size,\n+\t\t \"Max DPA ranges per media ops CCI command (0 = use hw max)\");\n+\n+static unsigned int flush_interval_ms = CXL_FLUSH_INTERVAL_DEFAULT_MS;\n+module_param(flush_interval_ms, uint, 0644);\n+MODULE_PARM_DESC(flush_interval_ms,\n+\t\t \"Flush worker interval in ms (default 1000)\");\n+\n+struct cxl_flush_buf {\n+\tunsigned int count;\n+\tunsigned int max;\t\t\t/* max ranges per command */\n+\tstruct cxl_media_op_input *cmd;\t\t/* pre-allocated CCI payload */\n+\tstruct folio **folios;\t\t\t/* parallel folio tracking */\n+};\n+\n+struct cxl_flush_ctx;\n+\n+struct cxl_pcpu_flush {\n+\tstruct cxl_flush_buf __rcu *active;\t/* callback writes here */\n+\tstruct cxl_flush_buf *overflow_spare;\t/* spare for overflow work */\n+\tstruct work_struct overflow_work;\t/* per-CPU overflow flush */\n+\tstruct cxl_flush_ctx *ctx;\t\t/* backpointer */\n+};\n+\n+/**\n+ * struct cxl_flush_ctx - Per-region flush context\n+ * @flush_record: two-level bitmap, 1 bit per 4KB page, tracks in-flight ops\n+ * @flush_record_pages: number of pages in the flush_record array\n+ * @nr_pages: total number of 4KB pages in the region\n+ * @base_pfn: starting PFN of the region (for DPA offset calculation)\n+ * @buf_max: max DPA ranges per CCI command\n+ * @media_ops_supported: true if device supports media operations zero\n+ * @pcpu: per-CPU flush state\n+ * @kthread_spares: array[nr_cpu_ids] of spare buffers for the kthread\n+ * @flush_thread: round-robin kthread\n+ * @mbox: pointer to CXL mailbox for sending CCI commands\n+ * @dev: device for logging\n+ * @nid: NUMA node of the private region\n+ */\n+struct cxl_flush_ctx {\n+\tunsigned long\t**flush_record;\n+\tunsigned int\t flush_record_pages;\n+\tunsigned long\t nr_pages;\n+\tunsigned long\t base_pfn;\n+\tunsigned int\t buf_max;\n+\tbool\t\t media_ops_supported;\n+\tstruct cxl_pcpu_flush __percpu *pcpu;\n+\tstruct cxl_flush_buf **kthread_spares;\n+\tstruct task_struct *flush_thread;\n+\tstruct cxl_mailbox *mbox;\n+\tstruct device\t*dev;\n+\tint\t\t nid;\n+};\n+\n+/* Bits per page-sized bitmap chunk */\n+#define FLUSH_RECORD_BITS_PER_PAGE\t(PAGE_SIZE * BITS_PER_BYTE)\n+#define FLUSH_RECORD_SHIFT\t\t(PAGE_SHIFT + 3)\n+\n+static unsigned long **flush_record_alloc(unsigned long nr_bits,\n+\t\t\t\t\t  unsigned int *nr_pages_out)\n+{\n+\tunsigned int nr_pages = DIV_ROUND_UP(nr_bits, FLUSH_RECORD_BITS_PER_PAGE);\n+\tunsigned long **pages;\n+\tunsigned int i;\n+\n+\tpages = kcalloc(nr_pages, sizeof(*pages), GFP_KERNEL);\n+\tif (!pages)\n+\t\treturn NULL;\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tpages[i] = (unsigned long *)get_zeroed_page(GFP_KERNEL);\n+\t\tif (!pages[i])\n+\t\t\tgoto err;\n+\t}\n+\n+\t*nr_pages_out = nr_pages;\n+\treturn pages;\n+\n+err:\n+\twhile (i--)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+\treturn NULL;\n+}\n+\n+static void flush_record_free(unsigned long **pages, unsigned int nr_pages)\n+{\n+\tunsigned int i;\n+\n+\tif (!pages)\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr_pages; i++)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+}\n+\n+static inline bool flush_record_test_and_clear(unsigned long **pages,\n+\t\t\t\t\t       unsigned long idx)\n+{\n+\treturn test_and_clear_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\t\t\t  pages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static inline void flush_record_set(unsigned long **pages, unsigned long idx)\n+{\n+\tset_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\tpages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static struct cxl_flush_buf *cxl_flush_buf_alloc(unsigned int max, int nid)\n+{\n+\tstruct cxl_flush_buf *buf;\n+\n+\tbuf = kzalloc_node(sizeof(*buf), GFP_KERNEL, nid);\n+\tif (!buf)\n+\t\treturn NULL;\n+\n+\tbuf->max = max;\n+\tbuf->cmd = kvzalloc_node(struct_size(buf->cmd, ranges, max),\n+\t\t\t\t GFP_KERNEL, nid);\n+\tif (!buf->cmd)\n+\t\tgoto err_cmd;\n+\n+\tbuf->folios = kcalloc_node(max, sizeof(struct folio *),\n+\t\t\t\t   GFP_KERNEL, nid);\n+\tif (!buf->folios)\n+\t\tgoto err_folios;\n+\n+\treturn buf;\n+\n+err_folios:\n+\tkvfree(buf->cmd);\n+err_cmd:\n+\tkfree(buf);\n+\treturn NULL;\n+}\n+\n+static void cxl_flush_buf_free(struct cxl_flush_buf *buf)\n+{\n+\tif (!buf)\n+\t\treturn;\n+\tkvfree(buf->cmd);\n+\tkfree(buf->folios);\n+\tkfree(buf);\n+}\n+\n+static inline void cxl_flush_buf_reset(struct cxl_flush_buf *buf)\n+{\n+\tbuf->count = 0;\n+}\n+\n+static void cxl_flush_buf_send(struct cxl_flush_ctx *ctx,\n+\t\t\t       struct cxl_flush_buf *buf)\n+{\n+\tstruct cxl_mbox_cmd mbox_cmd;\n+\tunsigned int count = buf->count;\n+\tunsigned int i;\n+\tint rc;\n+\n+\tif (count == 0)\n+\t\treturn;\n+\n+\tif (!ctx->media_ops_supported) {\n+\t\t/* No device support, zero all folios inline */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t\tgoto release;\n+\t}\n+\n+\tbuf->cmd->media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE;\n+\tbuf->cmd->media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO;\n+\tbuf->cmd->reserved = 0;\n+\tbuf->cmd->dpa_range_count = cpu_to_le32(count);\n+\n+\tmbox_cmd = (struct cxl_mbox_cmd) {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = buf->cmd,\n+\t\t.size_in = struct_size(buf->cmd, ranges, count),\n+\t\t.poll_interval_ms = 1000,\n+\t\t.poll_count = 30,\n+\t};\n+\n+\trc = cxl_internal_send_cmd(ctx->mbox, &mbox_cmd);\n+\tif (rc) {\n+\t\tdev_warn(ctx->dev,\n+\t\t\t \"media ops zero CCI command failed: %d\\n\", rc);\n+\n+\t\t/* Zero all folios inline on failure */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t}\n+\n+release:\n+\tfor (i = 0; i < count; i++)\n+\t\tfolio_put(buf->folios[i]);\n+\n+\tcxl_flush_buf_reset(buf);\n+}\n+\n+static int cxl_compression_flush_cb(struct folio *folio, void *private)\n+{\n+\tstruct cxl_flush_ctx *ctx = private;\n+\tunsigned long pfn = folio_pfn(folio);\n+\tunsigned long idx = pfn - ctx->base_pfn;\n+\tunsigned long nr = folio_nr_pages(folio);\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tunsigned long flags;\n+\tunsigned int pos;\n+\n+\t/* Case (a): flush record bit set, resolution from our media op */\n+\tif (flush_record_test_and_clear(ctx->flush_record, idx))\n+\t\treturn 0;\n+\n+\tdev_dbg_ratelimited(ctx->dev,\n+\t\t\t     \"flush_cb: folio pfn=%lx order=%u idx=%lu cpu=%d\\n\",\n+\t\t\t     pfn, folio_order(folio), idx,\n+\t\t\t     raw_smp_processor_id());\n+\n+\tlocal_irq_save(flags);\n+\trcu_read_lock();\n+\n+\tpcpu = this_cpu_ptr(ctx->pcpu);\n+\tbuf = rcu_dereference(pcpu->active);\n+\n+\tif (unlikely(!buf || buf->count >= buf->max)) {\n+\t\trcu_read_unlock();\n+\t\tlocal_irq_restore(flags);\n+\t\tif (buf)\n+\t\t\tschedule_work_on(raw_smp_processor_id(),\n+\t\t\t\t\t &pcpu->overflow_work);\n+\t\treturn 2;\n+\t}\n+\n+\t/* Case (b): write DPA range directly into pre-formatted CCI buffer */\n+\tfolio_get(folio);\n+\tflush_record_set(ctx->flush_record, idx);\n+\n+\tpos = buf->count;\n+\tbuf->folios[pos] = folio;\n+\tbuf->cmd->ranges[pos].starting_dpa = cpu_to_le64((u64)idx * PAGE_SIZE);\n+\tbuf->cmd->ranges[pos].length = cpu_to_le64((u64)nr * PAGE_SIZE);\n+\tbuf->count = pos + 1;\n+\n+\trcu_read_unlock();\n+\tlocal_irq_restore(flags);\n+\n+\treturn 1;\n+}\n+\n+static int cxl_flush_kthread_fn(void *data)\n+{\n+\tstruct cxl_flush_ctx *ctx = data;\n+\tstruct cxl_flush_buf *dirty;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tint cpu;\n+\tbool any_dirty;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tany_dirty = false;\n+\n+\t\t/* Phase 1: Swap all per-CPU buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tstruct cxl_flush_buf *spare = ctx->kthread_spares[cpu];\n+\n+\t\t\tif (!spare)\n+\t\t\t\tcontinue;\n+\n+\t\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\t\tcxl_flush_buf_reset(spare);\n+\t\t\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\t\t\tctx->kthread_spares[cpu] = dirty;\n+\n+\t\t\tif (dirty && dirty->count > 0) {\n+\t\t\t\tdev_dbg(ctx->dev,\n+\t\t\t\t\t \"flush_kthread: cpu=%d has %u dirty ranges\\n\",\n+\t\t\t\t\t cpu, dirty->count);\n+\t\t\t\tany_dirty = true;\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (!any_dirty)\n+\t\t\tgoto sleep;\n+\n+\t\t/* Phase 2: Single synchronize_rcu for all swaps */\n+\t\tsynchronize_rcu();\n+\n+\t\t/* Phase 3: Send CCI commands for dirty buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tdirty = ctx->kthread_spares[cpu];\n+\t\t\tif (dirty && dirty->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, dirty);\n+\t\t\t/* dirty is now clean, stays as kthread_spares[cpu] */\n+\t\t}\n+\n+sleep:\n+\t\tschedule_timeout_interruptible(\n+\t\t\tmsecs_to_jiffies(flush_interval_ms));\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static void cxl_flush_overflow_work(struct work_struct *work)\n+{\n+\tstruct cxl_pcpu_flush *pcpu =\n+\t\tcontainer_of(work, struct cxl_pcpu_flush, overflow_work);\n+\tstruct cxl_flush_ctx *ctx = pcpu->ctx;\n+\tstruct cxl_flush_buf *dirty, *spare;\n+\tunsigned long flags;\n+\n+\tdev_dbg(ctx->dev, \"flush_overflow: cpu=%d buffer full, flushing\\n\",\n+\t\t raw_smp_processor_id());\n+\n+\tspare = pcpu->overflow_spare;\n+\tif (!spare)\n+\t\treturn;\n+\n+\tcxl_flush_buf_reset(spare);\n+\n+\tlocal_irq_save(flags);\n+\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\tlocal_irq_restore(flags);\n+\n+\tpcpu->overflow_spare = dirty;\n+\n+\tsynchronize_rcu();\n+\tcxl_flush_buf_send(ctx, dirty);\n+}\n+\n+struct cxl_teardown_ctx {\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+};\n+\n+static void cxl_compression_pre_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\n+\tif (!tctx->flush_ctx)\n+\t\treturn;\n+\n+\t/*\n+\t * Unregister the CRAM node before memory goes offline.\n+\t * node_private_clear_ops requires the node_private to still\n+\t * exist, which is destroyed during memory removal.\n+\t */\n+\tcram_unregister_private_node(tctx->nid);\n+\n+\t/*\n+\t * Offline and remove CXL memory with retry.  CXL compressed\n+\t * memory may have pages pinned by in-flight flush operations;\n+\t * keep retrying until they complete.  Once done, sysram->res\n+\t * is NULL so the devm sysram_unregister action that follows\n+\t * will skip the hotplug removal.\n+\t */\n+\tif (tctx->sysram) {\n+\t\tint rc, retries = 0;\n+\n+\t\twhile (true) {\n+\t\t\trc = cxl_sysram_offline_and_remove(tctx->sysram);\n+\t\t\tif (!rc)\n+\t\t\t\tbreak;\n+\t\t\tif (++retries > 60) {\n+\t\t\t\tpr_err(\"cxl_compression: memory offline failed after %d retries, giving up\\n\",\n+\t\t\t\t       retries);\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tpr_info(\"cxl_compression: memory offline failed (%d), retrying...\\n\",\n+\t\t\t\trc);\n+\t\t\tmsleep(1000);\n+\t\t}\n+\t}\n+}\n+\n+static void cxl_compression_post_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\tstruct cxl_flush_ctx *ctx = tctx->flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tint cpu;\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\t/* cram_unregister_private_node already called in pre_teardown */\n+\n+\tif (ctx->flush_thread) {\n+\t\tkthread_stop(ctx->flush_thread);\n+\t\tctx->flush_thread = NULL;\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\tcancel_work_sync(&pcpu->overflow_work);\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tif (buf && buf->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, buf);\n+\n+\t\tif (pcpu->overflow_spare && pcpu->overflow_spare->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares && ctx->kthread_spares[cpu]) {\n+\t\t\tbuf = ctx->kthread_spares[cpu];\n+\t\t\tif (buf->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, buf);\n+\t\t}\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(ctx->kthread_spares[cpu]);\n+\t}\n+\n+\tkfree(ctx->kthread_spares);\n+\tfree_percpu(ctx->pcpu);\n+\tflush_record_free(ctx->flush_record, ctx->flush_record_pages);\n+}\n+\n+/**\n+ * struct cxl_compression_ctx - Per-device context for compression driver\n+ * @mbox: CXL mailbox for issuing CCI commands\n+ * @pdev: PCI device\n+ * @flush_ctx: Flush context for deferred page reclamation\n+ * @tctx: Teardown context for devm actions\n+ * @sysram: Sysram device for offline+remove in remove path\n+ * @nid: NUMA node ID, NUMA_NO_NODE if unset\n+ * @cxlmd: The memdev associated with this context\n+ * @cxlr: Region created by this driver (NULL if pre-existing)\n+ * @cxled: Endpoint decoder with DPA allocated by this driver\n+ * @regions_converted: Number of regions successfully converted\n+ * @media_ops_supported: Device supports media operations zero (0x4402)\n+ */\n+struct cxl_compression_ctx {\n+\tstruct cxl_mailbox *mbox;\n+\tstruct pci_dev *pdev;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+\tstruct cxl_memdev *cxlmd;\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint regions_converted;\n+\tbool media_ops_supported;\n+};\n+\n+/*\n+ * Probe whether the device supports Media Operations Zero (0x4402).\n+ * Send a zero-count command, a conforming device returns SUCCESS,\n+ * a device that doesn't support it returns UNSUPPORTED (-ENXIO).\n+ */\n+static bool cxl_probe_media_ops_zero(struct cxl_mailbox *mbox,\n+\t\t\t\t     struct device *dev)\n+{\n+\tstruct cxl_media_op_input probe = {\n+\t\t.media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE,\n+\t\t.media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO,\n+\t\t.dpa_range_count = 0,\n+\t};\n+\tstruct cxl_mbox_cmd cmd = {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = &probe,\n+\t\t.size_in = sizeof(probe),\n+\t};\n+\tint rc;\n+\n+\trc = cxl_internal_send_cmd(mbox, &cmd);\n+\tif (rc) {\n+\t\tdev_info(dev,\n+\t\t\t \"media operations zero not supported (rc=%d), using inline zeroing\\n\",\n+\t\t\t rc);\n+\t\treturn false;\n+\t}\n+\n+\tdev_info(dev, \"media operations zero (0x4402) supported\\n\");\n+\treturn true;\n+}\n+\n+struct cxl_compression_wm_ctx {\n+\tstruct device *dev;\n+\tint nid;\n+};\n+\n+static irqreturn_t cxl_compression_lthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"lthresh watermark: pressuring node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, CRAM_PRESSURE_MAX);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static irqreturn_t cxl_compression_hthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"hthresh watermark: resuming node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, 0);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static int convert_region_to_sysram(struct cxl_region *cxlr,\n+\t\t\t\t    struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct device *dev = cxl_region_dev(cxlr);\n+\tstruct cxl_compression_wm_ctx *wm_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tresource_size_t region_start, region_size;\n+\tstruct range hpa_range;\n+\tint nid;\n+\tint irq;\n+\tint cpu;\n+\tint rc;\n+\n+\tif (cxl_region_mode(cxlr) != CXL_PARTMODE_RAM) {\n+\t\tdev_dbg(dev, \"skipping non-RAM region (mode=%d)\\n\",\n+\t\t\tcxl_region_mode(cxlr));\n+\t\treturn 0;\n+\t}\n+\n+\tdev_info(dev, \"converting region to sysram\\n\");\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to add sysram region: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\ttctx = devm_kzalloc(dev, sizeof(*tctx), GFP_KERNEL);\n+\tif (!tctx)\n+\t\treturn -ENOMEM;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_post_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Find the sysram child device for pre_teardown */\n+\tcomp_ctx->sysram = cxl_region_find_sysram(cxlr);\n+\tif (comp_ctx->sysram)\n+\t\ttctx->sysram = comp_ctx->sysram;\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tnid = phys_to_target_node(hpa_range.start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\tregion_start = hpa_range.start;\n+\tregion_size = range_len(&hpa_range);\n+\n+\tflush_ctx = devm_kzalloc(dev, sizeof(*flush_ctx), GFP_KERNEL);\n+\tif (!flush_ctx)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->base_pfn = PHYS_PFN(region_start);\n+\tflush_ctx->nr_pages = region_size >> PAGE_SHIFT;\n+\tflush_ctx->flush_record = flush_record_alloc(flush_ctx->nr_pages,\n+\t\t\t\t\t\t     &flush_ctx->flush_record_pages);\n+\tif (!flush_ctx->flush_record)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->mbox = comp_ctx->mbox;\n+\tflush_ctx->dev = dev;\n+\tflush_ctx->nid = nid;\n+\tflush_ctx->media_ops_supported = comp_ctx->media_ops_supported;\n+\n+\t/*\n+\t * Cap buffer at max DPA ranges that fit in one CCI payload.\n+\t * Header is 8 bytes (struct cxl_media_op_input), each range\n+\t * is 16 bytes (struct cxl_dpa_range).  The module parameter\n+\t * flush_buf_size can further limit this (0 = use hw max).\n+\t */\n+\tflush_ctx->buf_max = (flush_ctx->mbox->payload_size -\n+\t\t\t      sizeof(struct cxl_media_op_input)) /\n+\t\t\t     sizeof(struct cxl_dpa_range);\n+\tif (flush_buf_size && flush_buf_size < flush_ctx->buf_max)\n+\t\tflush_ctx->buf_max = flush_buf_size;\n+\tif (flush_ctx->buf_max == 0)\n+\t\tflush_ctx->buf_max = 1;\n+\n+\tdev_info(dev,\n+\t\t \"flush buffer: %u DPA ranges per command (payload %zu bytes, media_ops %s)\\n\",\n+\t\t flush_ctx->buf_max, flush_ctx->mbox->payload_size,\n+\t\t flush_ctx->media_ops_supported ? \"yes\" : \"no\");\n+\n+\tflush_ctx->pcpu = alloc_percpu(struct cxl_pcpu_flush);\n+\tif (!flush_ctx->pcpu)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->kthread_spares = kcalloc(nr_cpu_ids,\n+\t\t\t\t\t    sizeof(struct cxl_flush_buf *),\n+\t\t\t\t\t    GFP_KERNEL);\n+\tif (!flush_ctx->kthread_spares)\n+\t\tgoto err_pcpu_init;\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *active_buf, *overflow_buf, *spare_buf;\n+\n+\t\tactive_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!active_buf)\n+\t\t\tgoto err_pcpu_init;\n+\n+\t\toverflow_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!overflow_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tspare_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!spare_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tcxl_flush_buf_free(overflow_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\t\tpcpu->ctx = flush_ctx;\n+\t\trcu_assign_pointer(pcpu->active, active_buf);\n+\t\tpcpu->overflow_spare = overflow_buf;\n+\t\tINIT_WORK(&pcpu->overflow_work, cxl_flush_overflow_work);\n+\n+\t\tflush_ctx->kthread_spares[cpu] = spare_buf;\n+\t}\n+\n+\tflush_ctx->flush_thread = kthread_create_on_node(\n+\t\tcxl_flush_kthread_fn, flush_ctx, nid, \"cxl-flush/%d\", nid);\n+\tif (IS_ERR(flush_ctx->flush_thread)) {\n+\t\trc = PTR_ERR(flush_ctx->flush_thread);\n+\t\tflush_ctx->flush_thread = NULL;\n+\t\tgoto err_pcpu_init;\n+\t}\n+\twake_up_process(flush_ctx->flush_thread);\n+\n+\trc = cram_register_private_node(nid, cxlr,\n+\t\t\t\t\tcxl_compression_flush_cb, flush_ctx);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to register cram node %d: %d\\n\", nid, rc);\n+\t\tgoto err_pcpu_init;\n+\t}\n+\n+\ttctx->flush_ctx = flush_ctx;\n+\ttctx->nid = nid;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_pre_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcomp_ctx->flush_ctx = flush_ctx;\n+\tcomp_ctx->tctx = tctx;\n+\tcomp_ctx->nid = nid;\n+\n+\t/*\n+\t * Register watermark IRQ handlers on &pdev->dev for\n+\t * MSI-X vector 12 (lthresh) and vector 13 (hthresh).\n+\t */\n+\twm_ctx = devm_kzalloc(&pdev->dev, sizeof(*wm_ctx), GFP_KERNEL);\n+\tif (!wm_ctx)\n+\t\treturn -ENOMEM;\n+\n+\twm_ctx->dev = &pdev->dev;\n+\twm_ctx->nid = nid;\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_LTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_lthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-lthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register lthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_HTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_hthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-hthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register hthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\treturn 0;\n+\n+err_pcpu_init:\n+\tif (flush_ctx->flush_thread)\n+\t\tkthread_stop(flush_ctx->flush_thread);\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *buf;\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (flush_ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(flush_ctx->kthread_spares[cpu]);\n+\t}\n+\tkfree(flush_ctx->kthread_spares);\n+\tfree_percpu(flush_ctx->pcpu);\n+\tflush_record_free(flush_ctx->flush_record, flush_ctx->flush_record_pages);\n+\treturn rc ? rc : -ENOMEM;\n+}\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\tpdev_to_comp_ctx(to_pci_dev(cxlmd->dev.parent))->cxled = cxled;\n+\treturn cxlr;\n+}\n+\n+static int cxl_compression_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i, converted = 0, errors = 0;\n+\tint rc;\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\t/* Probe device for media operations zero support */\n+\tcomp_ctx->media_ops_supported =\n+\t\tcxl_probe_media_ops_zero(comp_ctx->mbox,\n+\t\t\t\t\t &cxlmd->dev);\n+\n+\tdev_info(&cxlmd->dev, \"compression attach: looking for regions\\n\");\n+\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) == CXL_PARTMODE_RAM) {\n+\t\t\trc = convert_region_to_sysram(regions[i], pdev);\n+\t\t\tif (rc)\n+\t\t\t\terrors++;\n+\t\t\telse\n+\t\t\t\tconverted++;\n+\t\t}\n+\t\tput_device(cxl_region_dev(regions[i]));\n+\t}\n+\n+\tif (converted > 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"converted %d regions to sysram (%d errors)\\n\",\n+\t\t\t converted, errors);\n+\t\treturn errors ? -EIO : 0;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"could not create RAM region: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = convert_region_to_sysram(cxlr, pdev);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to convert region to sysram: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tcomp_ctx->cxlr = cxlr;\n+\n+\tdev_info(&cxlmd->dev, \"created and converted region %s to sysram\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_compression_attach = {\n+\t.probe = cxl_compression_attach_probe,\n+};\n+\n+static int cxl_compression_probe(struct pci_dev *pdev,\n+\t\t\t\t const struct pci_device_id *id)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probing device\\n\");\n+\n+\tcomp_ctx = devm_kzalloc(&pdev->dev, sizeof(*comp_ctx), GFP_KERNEL);\n+\tif (!comp_ctx)\n+\t\treturn -ENOMEM;\n+\tcomp_ctx->nid = NUMA_NO_NODE;\n+\tcomp_ctx->pdev = pdev;\n+\n+\trc = xa_insert(&comp_ctx_xa, (unsigned long)pdev, comp_ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_compression_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&comp_ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_compression_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = xa_erase(&comp_ctx_xa,\n+\t\t\t\t\t\t\t(unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: removing device\\n\");\n+\n+\tif (!comp_ctx || comp_ctx->nid == NUMA_NO_NODE)\n+\t\treturn;\n+\n+\t/*\n+\t * Destroy the region, devm actions on the region device handle teardown\n+\t * in registration-reverse order:\n+\t *   1. pre_teardown:  cram_unregister + retry-forever memory offline\n+\t *   2. sysram_unregister: device_unregister (sysram->res is NULL\n+\t *      after pre_teardown, so cxl_sysram_release skips hotplug)\n+\t *   3. post_teardown: kthread stop, flush cleanup\n+\t *\n+\t * PCI MMIO is still live so CCI commands in post_teardown work.\n+\t */\n+\tif (comp_ctx->cxlr) {\n+\t\tcxl_destroy_region(comp_ctx->cxlr);\n+\t\tcomp_ctx->cxlr = NULL;\n+\t}\n+\n+\tif (comp_ctx->cxled) {\n+\t\tcxl_dpa_free(comp_ctx->cxled);\n+\t\tcomp_ctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_compression_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ /* terminate list */ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_compression_pci_tbl);\n+\n+static struct pci_driver cxl_compression_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_compression_pci_tbl,\n+\t.probe\t\t= cxl_compression_probe,\n+\t.remove\t\t= cxl_compression_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_compression_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Compression Memory Driver with SysRAM regions\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-28-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David expressed concern about adding special-casing similar to ZONE_DEVICE, specifically mentioning the folio_managed_() function in mprotect.c",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concern",
                "special-casing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm concerned about adding more special-casing (similar to what we \nalready added for ZONE_DEVICE) all over the place.\n\nLike the whole folio_managed_() stuff in mprotect.c\n\nHaving that said, sounds like a reasonable topic to discuss.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "c10400db-2259-4465-a07e-19d0691101a4@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged a concern about the lack of semantics in zone_device hooks for this specific use case, proposed two alternative solutions: reusing vma_wants_writenotify() or adding a new hook to page table code, and offered to try one of these alternatives in a future version.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged concern",
                "proposed alternative"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It's a valid concern - and is why I tried to re-use as many of the\nzone_device hooks as possible.  It does not seem zone_device has quite\nthe same semantics for a case like this, so I had to make something new.\n\nDEVICE_COHERENT injects a temporary swap entry to allow the device to do\na large atomic operation - then the page table is restored and the CPU\nis free to change entries as it pleases.\n\nAnother option would be to add the hook to vma_wants_writenotify()\ninstead of the page table code - and mask MM_CP_TRY_CHANGE_WRITABLE.\n\nThis would require adding a vma flag - or maybe a count of protected /\ndevice pages.\n\nint mprotect_fixup() {\n    ...\n    if (vma_wants_manual_pte_write_upgrade(vma))\n        mm_cp_flags |= MM_CP_TRY_CHANGE_WRITABLE;\n}\n\nbool vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)\n{\n    if (vma->managed_wrprotect)\n        return true;\n}\n\nThat would localize the change in folio_managed_fixup_migration_pte() :\n\nstatic inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n                                                      pte_t pte,\n                                                      pte_t old_pte,\n                                                      struct vm_area_struct *vma)\n{\n    ...\n    } else if (folio_managed_wrprotect(page_folio(new))) {\n        pte = pte_wrprotect(pte);\n+       atomic_inc(&vma->managed_wrprotect);\n    }\n    return pte;\n}\n\nThis would cover both the huge_memory.c and mprotect, and maybe that's\njust generally cleaner? I can try that to see if it actually works.\n\n~Gregory",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "message_id": "aZxqP7J1kOClQUPQ@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that existing hooks can be used for write protection and agreed to remove redundant code from page table walks, but still considers the approach valid.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged existing solution",
                "agreed to simplify"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "scratch all this - existing hooks exist for exactly this purpose:\n\n\tcan_change_[pte|pmd]_writable()\n\nSurprised I missed this.\n\nI can clean this up to remove it from the page table walks.\n\nStill valid to question whether we want this, but at least the hook\nlives with other write-protect hooks now.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "aZx7hsVNU0XOCCiG@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Alistair Popple",
              "summary": "Reviewer Alistair Popple questioned the necessity of N_MEMORY_PRIVATE, suggesting that existing ZONE_DEVICE implementations could be adapted or reused instead, and expressed skepticism about reusing the mm buddy allocator as a primary motivator.\n\nReviewer Alistair Popple noted that the patch provides a standard interface to userspace for managing device memory, and suggested using existing NUMA APIs as a reasonable approach.\n\nReviewer Alistair Popple noted that the proposed cxl_compression driver is similar to ZONE_DEVICE's dev_pagemap_ops(), and questioned why it couldn't be extended instead of duplicating code. He suggested exploring alternative solutions, such as using page_ext or a future folio-based struct page.\n\nThe reviewer, Alistair Popple, expressed concerns that the proposed cxl_compression PCI driver is similar to existing ZONE_DEVICE methods and suggested building upon the existing feature set instead of adding another mechanism.\n\nReviewer Alistair Popple noted that the implementation duplicates a lot of hooks, similar to those provided by ZONE_DEVICE, and expressed interest in discussing the implementation further.\n\nReviewer Alistair Popple questioned whether the mm allocator is necessary for private memory allocation, suggesting that a device allocator library could be written or reused from drm_buddy.c\n\nReviewer questioned the characterization of ZONE_DEVICE pages as not being real struct pages, suggesting that perspective depends on one's role within the mm subsystem and asked for clarification on the specific limitations being addressed.\n\nReviewer suggested that ZONE_DEVICE_COHERENT could be extended to support the use case, proposing a couple of extra dev_pagemap_ops and being able to go on the LRU would achieve similar functionality.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "skepticism",
                "requested changes",
                "no clear disagreement or request for change",
                "suggested alternatives",
                "duplicates hooks",
                "expressed interest in discussion",
                "questioning",
                "request_for_clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Having had to re-implement entire portions of mm/ in a driver I agree this isn't\nsomething anyone sane should do :-) However aspects of ZONE_DEVICE were added\nprecisely to help with that so I'm not sure N_MEMORY_PRIVATE is the only or best\nway to do that.\n\nBased on our discussion at LPC I believe one of the primary motivators here was\nto re-use the existing mm buddy allocator rather than writing your own. I remain\nto be convinced that alone is justification enough for doing all this - DRM for\nexample already has quite a nice standalone buddy allocator (drm_buddy.c) that\ncould presumably be used, or adapted for use, by any device driver.\n\nThe interesting part of this series (which I have skimmed but not read in\ndetail) is how device memory gets exposed to userspace - this is something that\nexisting ZONE_DEVICE implementations don't address, instead leaving it up to\ndrivers and associated userspace stacks to deal with allocation, migration, etc.\n\n---\n\nThis is I think is one of the key things that should be enabled - providing a\nstandard interface to userspace for managing device memory. The existing NUMA\nAPIs do seem like a reasonable way to do this.\n\n---\n\nOne does not have to squint too hard to see that the above is not so different\nfrom what ZONE_DEVICE provides today via dev_pagemap_ops(). So I think I think\nit would be worth outlining why the existing ZONE_DEVICE mechanism can't be\nextended to provide these kind of services.\n\nThis seems to add a bunch of code just to use NODE_DATA instead of page->pgmap,\nwithout really explaining why just extending dev_pagemap_ops wouldn't work. The\nobvious reason is that if you want to support things like reclaim, compaction,\netc. these pages need to be on the LRU, which is a little bit hard when that\nfield is also used by the pgmap pointer for ZONE_DEVICE pages.\n\nBut it might be good to explore other options for storing the pgmap - for\nexample page_ext could be used.  Or I hear struct page may go away in place of\nfolios any day now, so maybe that gives us space for both :-)\n\n---\n\nThe above also looks pretty similar to the existing ZONE_DEVICE methods for\ndoing this which is another reason to argue for just building up the feature set\nof the existing boondoggle rather than adding another thingymebob.\n\nIt seems the key thing we are looking for is:\n\n1) A userspace API to allocate/manage device memory (ie. move_pages(), mbind(),\netc.)\n\n2) Allowing reclaim/LRU list processing of device memory.\n\n---\n\ndiscussion (hopefully I can make it to LSFMM). Mostly I'm interested in the\nimplementation as this does on the surface seem to sprinkle around and duplicate\na lot of hooks similar to what ZONE_DEVICE already provides.\n\n---\n\nFor basic allocation I agree this is the case. But there's no reason some device\nallocator library couldn't be written. Or in fact as pointed out above reuse the\nalready existing one in drm_buddy.c.  So would be interested to hear arguments\nfor why allocation has to be done by the mm allocator and/or why an allocation\nlibrary wouldn't work here given DRM already has them.\n\n---\n\nZONE_DEVICE pages are in fact real struct pages, but I will concede that\nperspective probably depends on which bits of the mm you play in. The real\nlimitations you seem to be addressing is more around how we get these pages in\nan LRU, or are there other limitations?\n\n---\n\nWhat I'd like to explore is why ZONE_DEVICE_COHERENT couldn't just be extended\nto support your usecase? It seems a couple of extra dev_pagemap_ops and being\nable to go on the LRU would get you there.\n\n - Alistair",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "fzy6f6dpv3oq3ksr2mkst7pz3daeb3buhuvdvcw4633pcl7h6u@mxjgiwpg5acv",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledges that using ZONE_DEVICE as an abstraction is flawed, explains how it leads to unnecessary complexity, and suggests reusing the buddy allocator instead.\n\nThe author explains that the callback similarity between ZONE_DEVICE and private nodes is intentional, as they require the same set of hooks but with different defaults. The author argues that extending ZONE_DEVICE into new areas would be cumbersome and inefficient, and instead chose to reuse existing code.\n\nAuthor agrees that using NODE_DATA is a better approach than per-page pgmap, and points out that existing code supports managing multiple devices with the same numa node.\n\nThe author is addressing concerns about implementing mempolicy support for N_MEMORY_PRIVATE, specifically how to handle ZONE_DEVICE NUMA UAPI and LRU management. The author acknowledges that getting mempolicy to work requires adding code to vma_alloc_folio_noprof and managing pages with the buddy. They also point out the limitations of Zone Device, such as no free lists or watermarks, making it difficult to implement LRU. The author presents two options: putting pages in the buddy or adding pgmap->device_alloc() callbacks at every allocation site.\n\nAuthor acknowledged reviewer's concern about reusing mm/ services and explained that using the buddy underpins the rest of these services, making it a more efficient approach.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a fix is needed",
                "acknowledges feedback",
                "provides explanation",
                "agreement",
                "existing_code_supports",
                "presents alternatives",
                "acknowledged",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree that buddy-access alone is insufficient justification, it\nstarted off that way - but if you want mempolicy/NUMA UAPI access,\nit turns into \"Re-use all of MM\" - and that means using the buddy.\n\nI also expected ZONE_DEVICE vs NODE_DATA to be the primary discussion,\n\nI raise replacing it as a thought experiment, but not the proposal.\n\nThe idea that drm/ is going to switch to private nodes is outside the\nrealm of reality, but part of that is because of years of infrastructure\nbuilt on the assumption that re-using mm/ is infeasible.\n\nBut, lets talk about DEVICE_COHERENT\n\n---\n\nDEVICE_COHERENT is the odd-man out among ZONE_DEVICE modes. The others\nuse softleaf entries and don't allow direct mappings.\n\n(DEVICE_PRIVATE sort of does if you squint, but you can also view that\n a bit like PROT_NONE or read-only controls to force migrations).\n\nIf you take DEVICE_COHERENT and:\n\n- Move pgmap out of the struct page (page_ext, NODE_DATA, etc) to free\n  the LRU list_head\n- Put pages in the buddy (free lists, watermarks, managed_pages) or add\n  pgmap->device_alloc() at every allocation callsite / buddy hook\n- Add LRU support (aging, reclaim, compaction)\n- Add isolated gating (new GFP flag and adjusted zonelist filtering)\n- Add new dev_pagemap_ops callbacks for the various mm/ features\n- Audit evey folio_is_zone_device() to distinguish zone device modes\n\n... you've built N_MEMORY_PRIVATE inside ZONE_DEVICE. Except now\npage_zone(page) returns ZONE_DEVICE - so you inherit the wrong\ndefaults at every existing ZONE_DEVICE check. \n\nSkip-sites become things to opt-out of instead of opting into.\n\nYou just end up with\n\nif (folio_is_zone_device(folio))\n    if (folio_is_my_special_zone_device())\n    else ....\n\nand this just generalizes to\n\nif (folio_is_private_managed(folio))\n    folio_managed_my_hooked_operation()\n\nSo you get the same code, but have added more complexity to ZONE_DEVICE.\n\nI don't think that's needed if we just recognize ZONE is the wrong\nabstraction to be operating on.\n\nHonestly, even ZONE_MOVABLE becomes pointless with N_MEMORY_PRIVATE\nif you disallow longterm pinning - because the managing service handles\nallocations (it has to inject GFP_PRIVATE to get access) or selectively\nenables the mm/ services it knows are safe (mempolicy).\n\nEven if you allow longterm pinning, if your service controls what does\nthe pinning it can still be reclaimable - just manually (killing\nprocesses) instead of letting hotplug do it via migration.\n\nIf your service only allocates movable pages - your ZONE_NORMAL is\neffectively ZONE_MOVABLE.  \n\nIn some cases we use ZONE_MOVABLE to prevent the kernel from allocating\nmemory onto devices (like CXL).  This means struct page is forced to\ntake up DRAM or use memmap_on_memory - meaning you lose high-value\ncapacity or sacrifice contiguity (less huge page support).\n\nThis entire problem can evaporate if you can just use ZONE_NORMAL.\n\nThere are a lot of benefits to just re-using the buddy like this.\n\nZones are the wrong abstraction and cause more problems.\n\n---\n\nYou don't have to squint because it was deliberate :]\n\nThe callback similarity is the feature - they're the same logical\noperations.  The difference is the direction of the defaults.\n\nExtending ZONE_DEVICE into these areas requires the same set of hooks,\nplus distinguishing \"old ZONE_DEVICE\" from \"new ZONE_DEVICE\".\n\nWhere there are new injection sites, it's because ZONE_DEVICE opts\nout of ever touching that code in some other silently implied way.\n\nFor example, reclaim/compaction doesn't run because ZONE_DEVICE doesn't\nadd to managed_pages (among other reasons).\n\nYou'd have to go figure out how to hack those things into ZONE_DEVICE \n*and then* opt every *other* ZONE_DEVICE mode *back out*.\n\nSo you still end up with something like this anyway:\n\nstatic inline bool folio_managed_handle_fault(struct folio *folio,\n                                              struct vm_fault *vmf,\n                                              enum pgtable_level level,\n                                              vm_fault_t *ret)\n{\n        /* Zone device pages use swap entries; handled in do_swap_page */\n        if (folio_is_zone_device(folio))\n                return false;\n\n        if (folio_is_private_node(folio))\n\t\t...\n        return false;\n}\n\n---\n\nIf NUMA is the interface we want, then NODE_DATA is the right direction\nregardless of struct page's future or what zone it lives in.\n\nThere's no reason to keep per-page pgmap w/ device-to-node mappings.\n\nYou can have one driver manage multiple devices with the same numa node\nif it uses the same owner context (PFN already differentiates devices).\n\nThe existing code allows for this.\n\n---\n\nOn (1): ZONE_DEVICE NUMA UAPI is harder than it looks from the surface\n\nMuch of the kernel mm/ infrastructure is written on top of the buddy and\nexpects N_MEMORY to be the sole arbiter of \"Where to Acquire Pages\".\n\nMempolicy depends on:\n   - Buddy support or a new alloc hook around the buddy\n\n   - Migration support (mbind() after allocation migrates)\n     - Migration also deeply assumes buddy and LRU support\n\n   - Changing validations on node states\n     - mempolicy checks N_MEMORY membership, so you have to hack\n       N_MEMORY onto ZONE_DEVICE\n       (or teach it about a new node state... N_MEMORY_PRIVATE)\n\n\nGetting mempolicy to work with N_MEMORY_PRIVATE amounts to adding 2\nlines of code in vma_alloc_folio_noprof:\n\nstruct folio *vma_alloc_folio_noprof(gfp_t gfp, int order,\n                                     struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr)\n{\n        if (pol->flags & MPOL_F_PRIVATE)\n                gfp |= __GFP_PRIVATE;\n\n        folio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n\t/* Woo! I faulted a DEVICE PAGE! */\n}\n\nBut this requires the pages to be managed by the buddy.\n\nThe rest of the mempolicy support is around keeping sane nodemasks when\nthings like cpuset.mems rebinds occur and validating you don't end up\nwith private nodes that don't support mempolicy in your nodemask.\n\nYou have to do all of this anyway, but with the added bonus of fighting\nwith the overloaded nature of ZONE_DEVICE at every step.\n\n==========\n\nOn (2): Assume you solve LRU. \n\nZone Device has no free lists, managed_pages, or watermarks.\n\nkswapd can't run, compaction has no targets, vmscan's pressure model\ndoesn't function.  These all come for free when the pages are\nbuddy-managed on a real zone.  Why re-invent the wheel?\n\n==========\n\nSo you really have two options here:\n\na) Put pages in the buddy, or\n\nb) Add pgmap->device_alloc() callbacks at every allocation site that\n   could target a node:\n     - vma_alloc_folio\n     - alloc_migration_target\n     - alloc_demote_folio\n     - alloc_pages_node\n     - alloc_contig_pages\n     - list goes on\n\nOr more likely - hooking get_page_from_freelist.  Which at that\npoint... just use the buddy?  You're already deep in the hot path.\n\n---\n\nUsing the buddy underpins the rest of mm/ services we want to re-use.\n\nThat's basically it.  Otherwise you have to inject hooks into every\nsurface that touches the buddy...\n\n... or in the buddy (get_page_from_freelist), at which point why not\njust use the buddy?\n\n~Gregory",
              "reply_to": "Alistair Popple",
              "message_date": "2026-02-24",
              "message_id": "aZ3BEn_73Rk8Fn7L@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/2] cxl/region: fix region leak when attach_target fails in cxl_add_to_region",
          "message_id": "20260221043013.1420169-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260221043013.1420169-1-gourry@gourry.net/",
          "date": "2026-02-21T04:30:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-21",
          "patch_summary": "This patch fixes a region leak in the cxl driver when attaching a target to a region fails. When attach_target() returns an error, the auto-discovered region remains registered and consumes its HPA resource without reaching a COMMIT state. The patch tracks whether the region was created by the current call to cxl_add_to_region(), and if so, it calls drop_region() on attach_target() failure to unregister the region and release the HPA resource. This prevents subsequent region creation attempts from failing due to an already reserved HPA range.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "Author addressed a concern about device_attach() being called on auto-discovered regions when a custom attach callback is set. They explained that this causes issues with dax_kmem refusing to offline memory, and proposed skipping device_attach() in such cases.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a CXL memdev has a custom attach callback, cxl_add_to_region()\nshould not call device_attach() on the auto-discovered region.\n\nThe default device_attach() binds the dax driver, which may online\nmemory via dax_kmem.  The custom attach callback then has to tear down\nthe dax stack to convert the region to sysram, but dax_kmem refuses to\noffline memory during its remove path, leaving regions stuck online.\n\nSkip device_attach() when cxlmd->attach is set.  The attach callback\nis responsible for setting up the region after auto-discovery completes\n(e.g. adding it as sysram directly).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/region.c | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 276046d49f88..e5edeabd9262 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -3971,6 +3971,12 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n \t}\n \n \tif (attach) {\n+\t\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n+\n+\t\t/* Skip device_attach if memdev has is own attach callback */\n+\t\tif (cxlmd->attach)\n+\t\t\treturn 0;\n+\n \t\t/*\n \t\t * If device_attach() fails the range may still be active via\n \t\t * the platform-firmware memory map, otherwise the driver for\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "20260221043013.1420169-2-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author is pushing back against the review, stating that the patch should be disregarded because it uses a function introduced by another developer.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "pushing back",
                "disregard this patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "BAH - disregard this patch, it uses drop_region which is introduced by\nAlejandro here:\n\nhttps://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/",
              "reply_to": "",
              "message_date": "2026-02-21",
              "message_id": "aZk_9iYMh2QPYNDz@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Alison Schofield",
              "summary": "Reviewer noted that the patch is similar to a previous one (cxl/region: Unregister auto-created region when assembly fails) and suggested dropping the newly created region on attach_target failure, but disagreed with leaving broken things in place as previously reviewed",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "similar patch",
                "requested change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I see you dropping this, perhaps just for the moment, because\nthe drop_region() you wanted to use is not available yet.\n\nThis looks a lot like \n\thttps://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/\n\tcxl/region: Unregister auto-created region when assembly fails\n\tWhen auto-created region assembly fails the region remains registered\n\tbut disabled. The region continues to reserve its memory resource,\n\tpreventing DAX from registering the memory.\n\tUnregister the region on assembly failure to release the resource.\n\nAnd the review comments on that one, or at least on that thread in\ngeneral, was to leave all the broken things in place.\nI didn't agree with that, and hope to see this version move ahead\nwhen you have the drop_region you need.\n\n-- Alison",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "aZyvGnKfWI1Mku-c@aschofie-mobl2.lan",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that the patch is a minor cleanup and doesn't have significant impact yet, but didn't address the underlying issue or provide a plan for future revisions.\n\nThe author addressed Alison Schofield's concern that the patch does not handle the case where an auto-region is created but then fails to be attached, causing subsequent region creation attempts to fail due to resource conflicts. The author explained that this issue only occurs in a specific failure scenario involving two devices unbind/bind cycling at the same time and that it's a 'pretty narrow failure scenario'.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged minor issue",
                "no clear resolution",
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah it's not a particularly useful cleanup in the current\ninfrastructure because nothing actually uses this pattern (yet).\n\n---\n\nThe important note here is the difference between auto-regions and\nmanually created regions.  For auto-regions, you might have another\nendpoint show up looking for the partially created region - and then\njust go off and create it anyway because it thinks it was first.\n\nBut in my driver, i'm explicitly converting these auto-regions into\nother things, and if that fails it causes *all other* region creation to\nfail - even if it wasn't actually dependent on that original region.\n\nThis is only an issue if you have two devices unbind/bind cycling at\nthe same time - i.e.\n\n   echo 0000:d0:00.00 > cxl_pci/unbind\n   echo 0000:e0:00.00 > cxl_pci/unbind\n   echo 0000:d0:00.00 > mydriver/bind\n   echo 0000:e0:00.00 > mydriver/bind\n\nIf the platform has pre-programmed and locked the decoders, and one of\nthe two devices fails to probe and leaves a hanging partially\ncreated region, the other device will fail too.\n\nIt's a pretty narrow failure scenario.\n\n~Gregory",
              "reply_to": "Alison Schofield",
              "message_date": "2026-02-23",
              "message_id": "aZy1VGindEm-NbFn@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Alison Schofield",
              "summary": "Reviewer noted that the patch's fix will eventually lead to another failure due to a similar issue, and requested further consideration of this problem.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "further consideration"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That's by design, and that'll eventually fail too.\n\nBut - is see how your case is different. Thanks for the explanation.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "aZzuak0CpP6kTtke@aschofie-mobl2.lan",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] cxl/core: fix test_bit misuse with CXL_DECODER_F_ bitmask flags",
          "message_id": "20260221021810.1390342-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260221021810.1390342-1-gourry@gourry.net/",
          "date": "2026-02-21T02:18:16Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-21",
          "patch_summary": "The patch fixes a bug in the CXL (Compute Express Link) driver where bitmasks are being passed to test_bit() incorrectly, leading to potential issues. The fix involves replacing test_bit() with direct bitmask tests for flags CXL_DECODER_F_LOCK and CXL_DECODER_F_NORMALIZED_ADDRESSING.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Benjamin Cheatham",
              "summary": "Pointed out that the bug had already been reported by Alison Schofield two weeks prior and suggested adding a Reported-by tag to acknowledge her work.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgement of prior work"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "Gregory Price",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Acknowledged the patch and asked Gregory to add a review tag, but also mentioned that he is waiting on 7.0-rc1 for cxl/fixes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgement of prior work"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\n\nOn 2/23/26 10:45 AM, Gregory Price wrote:\n> On Mon, Feb 23, 2026 at 11:33:13AM -0600, Cheatham, Benjamin wrote:\n>> On 2/20/2026 8:18 PM, Gregory Price wrote:\n>>> CXL_DECODER_F_LOCK (BIT(4) = 16) and CXL_DECODER_F_NORMALIZED_ADDRESSING\n>>> (BIT(6) = 64) are bit masks, but three call sites pass them to test_bit()\n>>> which expects a bit number.\n>>>\n>>> Replace test_bit() with direct bitmask tests, consistent with every other\n>>> use of these flags.\n>>>\n>>> Fixes: 2230c4bdc412 (\"cxl: Add handling of locked CXL decoder\")\n>>> Signed-off-by: Gregory Price <gourry@gourry.net>\n>>\n>> Alison sent out a patch [1] two weeks ago for this. I suspect you found this bug\n>> independently, so I figured I should point it out. Otherwise, I would add a Reported-by (or some\n>> other tag) with her name.\n>>\n>> Thanks,\n>> Ben\n>>\n>> [1]: https://lore.kernel.org/linux-cxl/20260206181404.1025991-1-alison.schofield@intel.com/\n> \n> Ah, yeah, missed this, and did find independently when testing unbinds.\n> \n> Wasn't on cxl/next so I thought it hadn't been found yet.\n\nYeah waiting on 7.0-rc1 for cxl/fixes. I also asked her to split the patches into 2 fixes. But if you don't mind go add your review tag that'd be great! :) \n\n> \n> Cool, thanks!\n> ~Gregory\n\n\n",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3] cxl/memdev: fix deadlock in cxl_memdev_autoremove() on attach failure",
          "message_id": "20260211192228.2148713-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260211192228.2148713-1-gourry@gourry.net/",
          "date": "2026-02-11T19:22:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch fixes a deadlock in the cxl_memdev_autoremove() function, which occurs when the attach callback is provided but the CXL topology fails to enumerate. The fix involves adding a new function cxl_memdev_attach_failed() to correctly set the scope of the check.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Davidlohr Bueso",
              "summary": "Preferred the original 'if (!cxl_memdev_did_attach())' check, but still found the patch readable and acceptable.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEUTRAL"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Wed, 11 Feb 2026, Gregory Price wrote:\n\n>cxl_memdev_autoremove() takes device_lock(&cxlmd->dev) via guard(device)\n>and then calls cxl_memdev_unregister() when the attach callback was\n>provided but cxl_mem_probe() failed to bind.\n>\n>cxl_memdev_unregister() calls\n>  cdev_device_del()\n>    device_del()\n>      bus_remove_device()\n>        device_release_driver()\n>\n>This path is reached when a driver uses the @attach parameter to\n>devm_cxl_add_memdev() and the CXL topology fails to enumerate (e.g.\n>DVSEC range registers decode outside platform-defined CXL ranges,\n>causing the endpoint port probe to fail).\n>\n>Add cxl_memdev_attach_failed() to set the scope of the check correctly.\n\nI preferred the \"if (!cxl_memdev_did_attach())\" but the below still\nreads nicely.\n\nReviewed-by: Davidlohr Bueso <dave@stgolabs.net>\n",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dan Williams",
              "summary": "Found the patch acceptable and suggested adding a 'Reported-by' tag to acknowledge the kreview tool.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEUTRAL"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Applied the patch to the cxl/fixes branch.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\n\nOn 2/11/26 12:22 PM, Gregory Price wrote:\n> cxl_memdev_autoremove() takes device_lock(&cxlmd->dev) via guard(device)\n> and then calls cxl_memdev_unregister() when the attach callback was\n> provided but cxl_mem_probe() failed to bind.\n> \n> cxl_memdev_unregister() calls\n>   cdev_device_del()\n>     device_del()\n>       bus_remove_device()\n>         device_release_driver()\n> \n> This path is reached when a driver uses the @attach parameter to\n> devm_cxl_add_memdev() and the CXL topology fails to enumerate (e.g.\n> DVSEC range registers decode outside platform-defined CXL ranges,\n> causing the endpoint port probe to fail).\n> \n> Add cxl_memdev_attach_failed() to set the scope of the check correctly.\n> \n> Fixes: 29317f8dc6ed (\"cxl/mem: Introduce cxl_memdev_attach for CXL-dependent operation\")\n> Signed-off-by: Gregory Price <gourry@gourry.net>\n\nApplied to cxl/fixes\n318c58852e68\n\n> ---\n>  drivers/cxl/core/memdev.c | 13 +++++++++----\n>  1 file changed, 9 insertions(+), 4 deletions(-)\n> \n> diff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\n> index af3d0cc65138..25ca4443e4f7 100644\n> --- a/drivers/cxl/core/memdev.c\n> +++ b/drivers/cxl/core/memdev.c\n> @@ -1089,10 +1089,8 @@ static int cxlmd_add(struct cxl_memdev *cxlmd, struct cxl_dev_state *cxlds)\n>  DEFINE_FREE(put_cxlmd, struct cxl_memdev *,\n>  \t    if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> -static struct cxl_memdev *cxl_memdev_autoremove(struct cxl_memdev *cxlmd)\n> +static bool cxl_memdev_attach_failed(struct cxl_memdev *cxlmd)\n>  {\n> -\tint rc;\n> -\n>  \t/*\n>  \t * If @attach is provided fail if the driver is not attached upon\n>  \t * return. Note that failure here could be the result of a race to\n> @@ -1100,7 +1098,14 @@ static struct cxl_memdev *cxl_memdev_autoremove(struct cxl_memdev *cxlmd)\n>  \t * succeeded and then cxl_mem unbound before the lock is acquired.\n>  \t */\n>  \tguard(device)(&cxlmd->dev);\n> -\tif (cxlmd->attach && !cxlmd->dev.driver) {\n> +\treturn (cxlmd->attach && !cxlmd->dev.driver);\n> +}\n> +\n> +static struct cxl_memdev *cxl_memdev_autoremove(struct cxl_memdev *cxlmd)\n> +{\n> +\tint rc;\n> +\n> +\tif (cxl_memdev_attach_failed(cxlmd)) {\n>  \t\tcxl_memdev_unregister(cxlmd);\n>  \t\treturn ERR_PTR(-ENXIO);\n>  \t}\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v3 2/2] cxl: Fix race of nvdimm_bus object when creating nvdimm objects",
          "message_id": "aZzSMwc2evqS8uBc@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZzSMwc2evqS8uBc@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T22:18:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dave Jiang (author)",
              "summary": "The author is addressing a concern about the race condition in nvdimm_bus object creation by moving the symbol devm_cxl_add_nvdimm_bridge() to cxl_pmem.c, making it exportable and dependent on cxl_pmem kernel module. This change does not introduce any functional changes besides code movement.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "acknowledges the race condition"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Moving the symbol devm_cxl_add_nvdimm_bridge() to\ndrivers/cxl/cxl_pmem.c, so that cxl_pmem can export a symbol that gives\ncxl_acpi a depedency on cxl_pmem kernel module. This is a prepatory patch\nto resolve the issue of a race for nvdimm_bus object that is created\nduring cxl_acpi_probe().\n\nNo functional changes besides moving code.\n\nSuggested-by: Dan Williams <dan.j.williams@intel.com>\nSigned-off-by: Dave Jiang <dave.jiang@intel.com>\n---\nv3:\n- Just move the symbol with a wrapper function. (Dan)\n---\n drivers/cxl/core/pmem.c | 13 +++----------\n drivers/cxl/cxl.h       |  2 ++\n drivers/cxl/pmem.c      | 14 ++++++++++++++\n 3 files changed, 19 insertions(+), 10 deletions(-)\n\ndiff --git a/drivers/cxl/core/pmem.c b/drivers/cxl/core/pmem.c\nindex e7b1e6fa0ea0..7c78344acc6d 100644\n--- a/drivers/cxl/core/pmem.c\n+++ b/drivers/cxl/core/pmem.c\n@@ -115,15 +115,8 @@ static void unregister_nvb(void *_cxl_nvb)\n \tdevice_unregister(&cxl_nvb->dev);\n }\n \n-/**\n- * devm_cxl_add_nvdimm_bridge() - add the root of a LIBNVDIMM topology\n- * @host: platform firmware root device\n- * @port: CXL port at the root of a CXL topology\n- *\n- * Return: bridge device that can host cxl_nvdimm objects\n- */\n-struct cxl_nvdimm_bridge *devm_cxl_add_nvdimm_bridge(struct device *host,\n-\t\t\t\t\t\t     struct cxl_port *port)\n+struct cxl_nvdimm_bridge *__devm_cxl_add_nvdimm_bridge(struct device *host,\n+\t\t\t\t\t\t       struct cxl_port *port)\n {\n \tstruct cxl_nvdimm_bridge *cxl_nvb;\n \tstruct device *dev;\n@@ -155,7 +148,7 @@ struct cxl_nvdimm_bridge *devm_cxl_add_nvdimm_bridge(struct device *host,\n \tput_device(dev);\n \treturn ERR_PTR(rc);\n }\n-EXPORT_SYMBOL_NS_GPL(devm_cxl_add_nvdimm_bridge, \"CXL\");\n+EXPORT_SYMBOL_FOR_MODULES(__devm_cxl_add_nvdimm_bridge, \"cxl_pmem\");\n \n static void cxl_nvdimm_release(struct device *dev)\n {\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 04c673e7cdb0..f5850800f400 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -920,6 +920,8 @@ void cxl_driver_unregister(struct cxl_driver *cxl_drv);\n struct cxl_nvdimm_bridge *to_cxl_nvdimm_bridge(struct device *dev);\n struct cxl_nvdimm_bridge *devm_cxl_add_nvdimm_bridge(struct device *host,\n \t\t\t\t\t\t     struct cxl_port *port);\n+struct cxl_nvdimm_bridge *__devm_cxl_add_nvdimm_bridge(struct device *host,\n+\t\t\t\t\t\t       struct cxl_port *port);\n struct cxl_nvdimm *to_cxl_nvdimm(struct device *dev);\n bool is_cxl_nvdimm(struct device *dev);\n int devm_cxl_add_nvdimm(struct device *host, struct cxl_port *port,\ndiff --git a/drivers/cxl/pmem.c b/drivers/cxl/pmem.c\nindex e197883690ef..714beaf1704b 100644\n--- a/drivers/cxl/pmem.c\n+++ b/drivers/cxl/pmem.c\n@@ -13,6 +13,20 @@\n \n static __read_mostly DECLARE_BITMAP(exclusive_cmds, CXL_MEM_COMMAND_ID_MAX);\n \n+/**\n+ * __devm_cxl_add_nvdimm_bridge() - add the root of a LIBNVDIMM topology\n+ * @host: platform firmware root device\n+ * @port: CXL port at the root of a CXL topology\n+ *\n+ * Return: bridge device that can host cxl_nvdimm objects\n+ */\n+struct cxl_nvdimm_bridge *devm_cxl_add_nvdimm_bridge(struct device *host,\n+\t\t\t\t\t\t     struct cxl_port *port)\n+{\n+\treturn __devm_cxl_add_nvdimm_bridge(host, port);\n+}\n+EXPORT_SYMBOL_NS_GPL(devm_cxl_add_nvdimm_bridge, \"CXL\");\n+\n static void clear_exclusive(void *mds)\n {\n \tclear_exclusive_cxl_commands(mds, exclusive_cmds);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-13",
              "message_id": "20260213224038.549798-2-dave.jiang@intel.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang (author)",
              "summary": "The author is addressing a concern about the cxl_nvdimm_bridge_failed_attach() function not being called when the nvdimm_bus object is missing, causing a NULL pointer dereference in __nd_device_register(). The author agrees that this is a problem and explains how they plan to fix it by adding a check in devm_cxl_add_nvdimm() to ensure that the cxl_nvb driver is attached before checking if the nvdimm_bus object is valid. They also mention that they will set an invalidated flag for cxl_nvdimm when the nvdimm_bus is released.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a fix is needed",
                "explains how to fix it"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Found issue during running of cxl-translate.sh unit test. Adding a 3s\nsleep right before the test seems to make the issue reproduce fairly\nconsistently. The cxl_translate module has dependency on cxl_acpi and\ncauses orphaned nvdimm objects to reprobe after cxl_acpi is removed.\nThe nvdimm_bus object is registered by the cxl_nvb object when\ncxl_acpi_probe() is called. With the nvdimm_bus object missing,\n__nd_device_register() will trigger NULL pointer dereference when\naccessing the dev->parent that points to &nvdimm_bus->dev.\n\n[  192.884510] BUG: kernel NULL pointer dereference, address: 000000000000006c\n[  192.895383] Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS edk2-20250812-19.fc42 08/12/2025\n[  192.897721] Workqueue: cxl_port cxl_bus_rescan_queue [cxl_core]\n[  192.899459] RIP: 0010:kobject_get+0xc/0x90\n[  192.924871] Call Trace:\n[  192.925959]  <TASK>\n[  192.926976]  ? pm_runtime_init+0xb9/0xe0\n[  192.929712]  __nd_device_register.part.0+0x4d/0xc0 [libnvdimm]\n[  192.933314]  __nvdimm_create+0x206/0x290 [libnvdimm]\n[  192.936662]  cxl_nvdimm_probe+0x119/0x1d0 [cxl_pmem]\n[  192.940245]  cxl_bus_probe+0x1a/0x60 [cxl_core]\n[  192.943349]  really_probe+0xde/0x380\n\nThis patch also relies on the previous change where\ndevm_cxl_add_nvdimm_bridge() is called from drivers/cxl/pmem.c instead\nof drivers/cxl/core.c to ensure the dependency of cxl_acpi on cxl_pmem.\n\n1. Set probe_type of cxl_nvb to PROBE_FORCE_SYNCHRONOUS to ensure the\n   driver is probed synchronously when add_device() is called.\n2. Add a check in __devm_cxl_add_nvdimm_bridge() to ensure that the\n   cxl_nvb driver is attached during cxl_acpi_probe().\n3. Take the cxl_root uport_dev lock and the cxl_nvb->dev lock in\n   devm_cxl_add_nvdimm() before checking nvdimm_bus is valid.\n4. Set cxl_nvdimm flag to CXL_NVD_F_INVALIDATED so cxl_nvdimm_probe()\n   will exit with -EBUSY.\n\nThe removal of cxl_nvdimm devices should prevent any orphaned devices\nfrom probing once the nvdimm_bus is gone.\n\nFixes: 21083f51521f (\"cxl/pmem: Register 'pmem' / cxl_nvdimm devices\")\nSigned-off-by: Dave Jiang <dave.jiang@intel.com>\n\n---\nv3:\n- squash patch 2 and 3 (Dan)\n- Remove nvdimm_bus check from cxl_nvdimm driver (Dan)\n- Set an invalidated flag for cxl_nvdimm when nvdimm_bus is released. (Dan)\n- Create a helper to see if driver is attached. (Dan)\n---\n drivers/cxl/core/pmem.c | 27 +++++++++++++++++++++++++++\n drivers/cxl/cxl.h       |  5 +++++\n drivers/cxl/pmem.c      |  8 +++++++-\n 3 files changed, 39 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/cxl/core/pmem.c b/drivers/cxl/core/pmem.c\nindex 7c78344acc6d..375948ec5fed 100644\n--- a/drivers/cxl/core/pmem.c\n+++ b/drivers/cxl/core/pmem.c\n@@ -115,6 +115,15 @@ static void unregister_nvb(void *_cxl_nvb)\n \tdevice_unregister(&cxl_nvb->dev);\n }\n \n+static bool cxl_nvdimm_bridge_failed_attach(struct cxl_nvdimm_bridge *cxl_nvb)\n+{\n+\tstruct device *dev = &cxl_nvb->dev;\n+\n+\tguard(device)(dev);\n+\t/* If the device has no driver, then it failed to attach. */\n+\treturn dev->driver == NULL;\n+}\n+\n struct cxl_nvdimm_bridge *__devm_cxl_add_nvdimm_bridge(struct device *host,\n \t\t\t\t\t\t       struct cxl_port *port)\n {\n@@ -138,6 +147,11 @@ struct cxl_nvdimm_bridge *__devm_cxl_add_nvdimm_bridge(struct device *host,\n \tif (rc)\n \t\tgoto err;\n \n+\tif (cxl_nvdimm_bridge_failed_attach(cxl_nvb)) {\n+\t\tunregister_nvb(cxl_nvb);\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n \trc = devm_add_action_or_reset(host, unregister_nvb, cxl_nvb);\n \tif (rc)\n \t\treturn ERR_PTR(rc);\n@@ -248,6 +262,19 @@ int devm_cxl_add_nvdimm(struct device *host, struct cxl_port *port,\n \tif (!cxl_nvb)\n \t\treturn -ENODEV;\n \n+\t/*\n+\t * Take the uport_dev lock to guard against race of nvdimm_bus object.\n+\t * cxl_acpi_probe() registers the nvdimm_bus and is done under the\n+\t * root port uport_dev lock.\n+\t *\n+\t * Take the cxl_nvb device lock to ensure that cxl_nvb driver is in a\n+\t * consistent state. And the driver registers nvdimm_bus.\n+\t */\n+\tguard(device)(cxl_nvb->port->uport_dev);\n+\tguard(device)(&cxl_nvb->dev);\n+\tif (!cxl_nvb->nvdimm_bus)\n+\t\treturn -ENODEV;\n+\n \tcxl_nvd = cxl_nvdimm_alloc(cxl_nvb, cxlmd);\n \tif (IS_ERR(cxl_nvd)) {\n \t\trc = PTR_ERR(cxl_nvd);\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex f5850800f400..9b947286eb9b 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -574,11 +574,16 @@ struct cxl_nvdimm_bridge {\n \n #define CXL_DEV_ID_LEN 19\n \n+enum {\n+\tCXL_NVD_F_INVALIDATED = 0,\n+};\n+\n struct cxl_nvdimm {\n \tstruct device dev;\n \tstruct cxl_memdev *cxlmd;\n \tu8 dev_id[CXL_DEV_ID_LEN]; /* for nvdimm, string of 'serial' */\n \tu64 dirty_shutdowns;\n+\tunsigned long flags;\n };\n \n struct cxl_pmem_region_mapping {\ndiff --git a/drivers/cxl/pmem.c b/drivers/cxl/pmem.c\nindex 714beaf1704b..61f7a0b352aa 100644\n--- a/drivers/cxl/pmem.c\n+++ b/drivers/cxl/pmem.c\n@@ -143,6 +143,9 @@ static int cxl_nvdimm_probe(struct device *dev)\n \tstruct nvdimm *nvdimm;\n \tint rc;\n \n+\tif (test_bit(CXL_NVD_F_INVALIDATED, &cxl_nvd->flags))\n+\t\treturn -EBUSY;\n+\n \tset_exclusive_cxl_commands(mds, exclusive_cmds);\n \trc = devm_add_action_or_reset(dev, clear_exclusive, mds);\n \tif (rc)\n@@ -323,8 +326,10 @@ static int detach_nvdimm(struct device *dev, void *data)\n \tscoped_guard(device, dev) {\n \t\tif (dev->driver) {\n \t\t\tcxl_nvd = to_cxl_nvdimm(dev);\n-\t\t\tif (cxl_nvd->cxlmd && cxl_nvd->cxlmd->cxl_nvb == data)\n+\t\t\tif (cxl_nvd->cxlmd && cxl_nvd->cxlmd->cxl_nvb == data) {\n \t\t\t\trelease = true;\n+\t\t\t\tset_bit(CXL_NVD_F_INVALIDATED, &cxl_nvd->flags);\n+\t\t\t}\n \t\t}\n \t}\n \tif (release)\n@@ -367,6 +372,7 @@ static struct cxl_driver cxl_nvdimm_bridge_driver = {\n \t.probe = cxl_nvdimm_bridge_probe,\n \t.id = CXL_DEVICE_NVDIMM_BRIDGE,\n \t.drv = {\n+\t\t.probe_type = PROBE_FORCE_SYNCHRONOUS,\n \t\t.suppress_bind_attrs = true,\n \t},\n };\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-13",
              "message_id": "20260213224038.549798-3-dave.jiang@intel.com",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer noted that the cxl_nvb reference is leaked due to a missing check for nvdimm_bus existence, and suggested adding a conditional statement to handle this case",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "leaks reference",
                "missing check"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think this leaks the reference on cxl_nvb taken by \n\n\tdevm_cxl_add_nvdimm()\n\t\tdevice_find_child()\n\nMaybe:\n     if (!cxl_nvb->nvdimm_bus) {\n             rc = -ENODEV;\n             goto err_alloc;\n     }\n\n\nReported-by: kreview-0811365\n\n~Gregory",
              "reply_to": "Dave Jiang",
              "message_date": "2026-02-23",
              "message_id": "aZzSMwc2evqS8uBc@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang (author)",
              "summary": "Author acknowledged a concern about the race condition in nvdimm_bus object creation and explained that the patch already addresses this issue by introducing a flag to invalidate cxl_nvdimm when releasing nvdimm_bus.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "patch already addresses"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Applied to cxl/fixes\n16fb82cadd63 cxl: Fix race of nvdimm_bus object when creating nvdimm objects\ne7e222ad73d9 cxl: Move devm_cxl_add_nvdimm_bridge() to cxl_pmem.ko",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "23e7eadb-bdff-4fd9-9c1f-ac55e02f5664@intel.com",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/2] cxl/region: fix region leak when attach_target fails in cxl_add_to_region",
          "message_id": "aZy1VGindEm-NbFn@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZy1VGindEm-NbFn@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T20:15:20Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "Author addressed a concern about regions being stuck online due to dax_kmem refusing to offline memory during its remove path. They explained that skipping device_attach() when cxlmd->attach is set allows the attach callback to set up the region after auto-discovery completes, and agreed to make this change.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a CXL memdev has a custom attach callback, cxl_add_to_region()\nshould not call device_attach() on the auto-discovered region.\n\nThe default device_attach() binds the dax driver, which may online\nmemory via dax_kmem.  The custom attach callback then has to tear down\nthe dax stack to convert the region to sysram, but dax_kmem refuses to\noffline memory during its remove path, leaving regions stuck online.\n\nSkip device_attach() when cxlmd->attach is set.  The attach callback\nis responsible for setting up the region after auto-discovery completes\n(e.g. adding it as sysram directly).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/region.c | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 276046d49f88..e5edeabd9262 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -3971,6 +3971,12 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n \t}\n \n \tif (attach) {\n+\t\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n+\n+\t\t/* Skip device_attach if memdev has is own attach callback */\n+\t\tif (cxlmd->attach)\n+\t\t\treturn 0;\n+\n \t\t/*\n \t\t * If device_attach() fails the range may still be active via\n \t\t * the platform-firmware memory map, otherwise the driver for\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "20260221043013.1420169-2-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author is dismissing the patch due to a dependency on a different patch, which introduces a function used in this patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disregard",
                "dependency"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "BAH - disregard this patch, it uses drop_region which is introduced by\nAlejandro here:\n\nhttps://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/",
              "reply_to": "",
              "message_date": "2026-02-21",
              "message_id": "aZk_9iYMh2QPYNDz@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Alison Schofield",
              "summary": "Reviewer noted that the patch's current implementation does not unregister the auto-created region when attach_target() fails, and suggested dropping the region using a future available function (drop_region()) to release the HPA resource.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I see you dropping this, perhaps just for the moment, because\nthe drop_region() you wanted to use is not available yet.\n\nThis looks a lot like \n\thttps://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/\n\tcxl/region: Unregister auto-created region when assembly fails\n\tWhen auto-created region assembly fails the region remains registered\n\tbut disabled. The region continues to reserve its memory resource,\n\tpreventing DAX from registering the memory.\n\tUnregister the region on assembly failure to release the resource.\n\nAnd the review comments on that one, or at least on that thread in\ngeneral, was to leave all the broken things in place.\nI didn't agree with that, and hope to see this version move ahead\nwhen you have the drop_region you need.\n\n-- Alison",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "aZyvGnKfWI1Mku-c@aschofie-mobl2.lan",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that the patch is not currently useful due to lack of usage, but did not commit to revising or removing it.\n\nThe author acknowledged that the patch addresses a specific issue related to auto-regions and manually created regions in a narrow failure scenario, where two devices unbind/bind cycle at the same time, causing one device to fail due to a partially created region. The author explained that this is not a general problem but rather a specific edge case.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a limitation",
                "did not promise a fix",
                "acknowledged",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah it's not a particularly useful cleanup in the current\ninfrastructure because nothing actually uses this pattern (yet).\n\n---\n\nThe important note here is the difference between auto-regions and\nmanually created regions.  For auto-regions, you might have another\nendpoint show up looking for the partially created region - and then\njust go off and create it anyway because it thinks it was first.\n\nBut in my driver, i'm explicitly converting these auto-regions into\nother things, and if that fails it causes *all other* region creation to\nfail - even if it wasn't actually dependent on that original region.\n\nThis is only an issue if you have two devices unbind/bind cycling at\nthe same time - i.e.\n\n   echo 0000:d0:00.00 > cxl_pci/unbind\n   echo 0000:e0:00.00 > cxl_pci/unbind\n   echo 0000:d0:00.00 > mydriver/bind\n   echo 0000:e0:00.00 > mydriver/bind\n\nIf the platform has pre-programmed and locked the decoders, and one of\nthe two devices fails to probe and leaves a hanging partially\ncreated region, the other device will fail too.\n\nIt's a pretty narrow failure scenario.\n\n~Gregory",
              "reply_to": "Alison Schofield",
              "message_date": "2026-02-23",
              "message_id": "aZy1VGindEm-NbFn@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Alison Schofield",
              "summary": "Reviewer noted that the patch's handling of attach_target() failure is consistent with existing behavior, but requested clarification on why this specific case is handled differently",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification_request"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That's by design, and that'll eventually fail too.\n\nBut - is see how your case is different. Thanks for the explanation.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "aZzuak0CpP6kTtke@aschofie-mobl2.lan",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer noted that while the patch looks good, they are unsure if the added error state is sufficient for debugging and tracing purposes, requesting further work on this aspect.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Feel free to add it to this series. I have started to send individual \nseries as you know but the part changing the region creation will \nrequire more work than the already sent.\n\nAbout this fix, it looks good to me, although I have to admit I'm a bit \nlost after following the discussion Allison points to. If we want to \nkeep the state of failure for forensics, not sure if the \ndebugging/tracing or default error info in this case will be enough.\n\nIn any case:\n\nReviewed-by: Alejandro Lucero <alucerop@amd.com>",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "87347db6-6838-4a60-ad1c-037f526f16b6@amd.com",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] cxl/core: fix test_bit misuse with CXL_DECODER_F_ bitmask flags",
          "message_id": "aZySU-tcjVvYcb23@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZySU-tcjVvYcb23@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T17:45:58Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch fixes a misuse of the test_bit() function in two call sites, replacing it with direct bitmask tests.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Benjamin Cheatham",
              "summary": "Noted that a similar patch had been previously submitted by Alison Schofield, suggesting that the author add a Reported-by tag to acknowledge this.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "Gregory Price",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Acknowledged the issue and suggested that the author add their review tag, but also mentioned waiting on 7.0-rc1 for cxl/fixes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\n\nOn 2/23/26 10:45 AM, Gregory Price wrote:\n> On Mon, Feb 23, 2026 at 11:33:13AM -0600, Cheatham, Benjamin wrote:\n>> On 2/20/2026 8:18 PM, Gregory Price wrote:\n>>> CXL_DECODER_F_LOCK (BIT(4) = 16) and CXL_DECODER_F_NORMALIZED_ADDRESSING\n>>> (BIT(6) = 64) are bit masks, but three call sites pass them to test_bit()\n>>> which expects a bit number.\n>>>\n>>> Replace test_bit() with direct bitmask tests, consistent with every other\n>>> use of these flags.\n>>>\n>>> Fixes: 2230c4bdc412 (\"cxl: Add handling of locked CXL decoder\")\n>>> Signed-off-by: Gregory Price <gourry@gourry.net>\n>>\n>> Alison sent out a patch [1] two weeks ago for this. I suspect you found this bug\n>> independently, so I figured I should point it out. Otherwise, I would add a Reported-by (or some\n>> other tag) with her name.\n>>\n>> Thanks,\n>> Ben\n>>\n>> [1]: https://lore.kernel.org/linux-cxl/20260206181404.1025991-1-alison.schofield@intel.com/\n> \n> Ah, yeah, missed this, and did find independently when testing unbinds.\n> \n> Wasn't on cxl/next so I thought it hadn't been found yet.\n\nYeah waiting on 7.0-rc1 for cxl/fixes. I also asked her to split the patches into 2 fixes. But if you don't mind go add your review tag that'd be great! :) \n\n> \n> Cool, thanks!\n> ~Gregory\n\n\n",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC][RFC PATCH v4 00/27] Private Memory Nodes (w/ Compressed RAM)",
          "message_id": "aZx7hsVNU0XOCCiG@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZx7hsVNU0XOCCiG@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T16:08:42Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about locking, acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY nodes are intended to contain general System RAM. Today, some\ndevice drivers hotplug their memory (marked Specific Purpose or Reserved)\nto get access to mm/ services, but don't intend it for general consumption.\n\nCreate N_MEMORY_PRIVATE for memory nodes whose memory is not intended for\ngeneral consumption. This state is mutually exclusive with N_MEMORY.\n\nAdd the node_private infrastructure for N_MEMORY_PRIVATE nodes:\n\n  - struct node_private: Per-node container stored in NODE_DATA(nid),\n    holding driver callbacks (ops), owner, and refcount.\n\n  - struct node_private_ops: Initial structure with void *reserved\n    placeholder and flags field.  Callbacks will be added by subsequent\n    commits as each consumer is wired up.\n\n  - folio_is_private_node() / page_is_private_node(): check if a\n    folio/page resides on a private node.\n\n  - folio_node_private_ops() / node_private_flags(): retrieve the ops\n    vtable or flags for a folio's node.\n\n  - Registration API: node_private_register()/unregister() for drivers\n    to register callbacks for private nodes. Only one driver callback\n    can be registered per node - attempting to register different ops\n    returns -EBUSY.\n\n  - sysfs attribute exposing N_MEMORY_PRIVATE node state.\n\nZonelist construction changes for private nodes are deferred to a\nsubsequent commit.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 197 ++++++++++++++++++++++++++++++++\n include/linux/mmzone.h       |   4 +\n include/linux/node_private.h | 210 +++++++++++++++++++++++++++++++++++\n include/linux/nodemask.h     |   1 +\n 4 files changed, 412 insertions(+)\n create mode 100644 include/linux/node_private.h\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 00cf4532f121..646dc48a23b5 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -22,6 +22,7 @@\n #include <linux/swap.h>\n #include <linux/slab.h>\n #include <linux/memblock.h>\n+#include <linux/node_private.h>\n \n static const struct bus_type node_subsys = {\n \t.name = \"node\",\n@@ -861,6 +862,198 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n \t\t\t   (void *)&nid, register_mem_block_under_node_hotplug);\n \treturn;\n }\n+\n+static DEFINE_MUTEX(node_private_lock);\n+static bool node_private_initialized;\n+\n+/**\n+ * node_private_register - Register a private node\n+ * @nid: Node identifier\n+ * @np: The node_private structure (driver-allocated, driver-owned)\n+ *\n+ * Register a driver for a private node. Only one driver can register\n+ * per node. If another driver has already registered (with different np),\n+ * -EBUSY is returned. Re-registration with the same np is allowed.\n+ *\n+ * The driver owns the node_private memory and must ensure it remains valid\n+ * until refcount reaches 0 after node_private_unregister().\n+ *\n+ * Returns 0 on success, negative errno on failure.\n+ */\n+int node_private_register(int nid, struct node_private *np)\n+{\n+\tstruct node_private *existing;\n+\tpg_data_t *pgdat;\n+\tint ret = 0;\n+\n+\tif (!np || !node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tif (!node_private_initialized)\n+\t\treturn -ENODEV;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\t/* N_MEMORY_PRIVATE and N_MEMORY are mutually exclusive */\n+\tif (node_state(nid, N_MEMORY)) {\n+\t\tret = -EBUSY;\n+\t\tgoto out;\n+\t}\n+\n+\tpgdat = NODE_DATA(nid);\n+\texisting = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t\t     lockdep_is_held(&node_private_lock));\n+\n+\t/* Only one source my register this node */\n+\tif (existing) {\n+\t\tif (existing != np) {\n+\t\t\tret = -EBUSY;\n+\t\t\tgoto out;\n+\t\t}\n+\t\tgoto out;\n+\t}\n+\n+\trefcount_set(&np->refcount, 1);\n+\tinit_completion(&np->released);\n+\n+\trcu_assign_pointer(pgdat->node_private, np);\n+\tpgdat->private = true;\n+\n+out:\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_register);\n+\n+/**\n+ * node_private_set_ops - Set service callbacks on a registered private node\n+ * @nid: Node identifier\n+ * @ops: Service callbacks and flags (driver-owned, must outlive registration)\n+ *\n+ * Validates flag dependencies and sets the ops on the node's node_private.\n+ * The node must already be registered via node_private_register().\n+ *\n+ * Returns 0 on success, -EINVAL for invalid flag combinations,\n+ * -ENODEV if no node_private is registered on @nid.\n+ */\n+int node_private_set_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!ops)\n+\t\treturn -EINVAL;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse\n+\t\tnp->ops = ops;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_set_ops);\n+\n+/**\n+ * node_private_clear_ops - Clear service callbacks from a private node\n+ * @nid: Node identifier\n+ * @ops: Expected ops pointer (must match current ops)\n+ *\n+ * Clears the ops only if @ops matches the currently registered ops,\n+ * preventing one service from accidentally clearing another's callbacks.\n+ *\n+ * Returns 0 on success, -ENODEV if no node_private is registered,\n+ * -EINVAL if @ops does not match.\n+ */\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse if (np->ops != ops)\n+\t\tret = -EINVAL;\n+\telse\n+\t\tnp->ops = NULL;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_clear_ops);\n+\n+/**\n+ * node_private_unregister - Unregister a private node\n+ * @nid: Node identifier\n+ *\n+ * Unregister the driver from a private node. Only succeeds if all memory\n+ * has been offlined and the node is no longer N_MEMORY_PRIVATE.\n+ * When successful, drops the refcount to 0 indicating the driver can\n+ * free its context.\n+ *\n+ * N_MEMORY_PRIVATE state is cleared by offline_pages() when the last\n+ * memory is offlined, not by this function.\n+ *\n+ * Return: 0 if unregistered, -EBUSY if N_MEMORY_PRIVATE is still set\n+ * (other memory blocks remain on this node).\n+ */\n+int node_private_unregister(int nid)\n+{\n+\tstruct node_private *np;\n+\tpg_data_t *pgdat;\n+\n+\tif (!node_possible(nid))\n+\t\treturn 0;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\tpgdat = NODE_DATA(nid);\n+\tnp = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Only unregister if all memory is offline and N_MEMORY_PRIVATE is\n+\t * cleared. N_MEMORY_PRIVATE is cleared by offline_pages() when the\n+\t * last memory block is offlined.\n+\t */\n+\tif (node_state(nid, N_MEMORY_PRIVATE)) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn -EBUSY;\n+\t}\n+\n+\trcu_assign_pointer(pgdat->node_private, NULL);\n+\tpgdat->private = false;\n+\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\n+\tsynchronize_rcu();\n+\n+\tif (!refcount_dec_and_test(&np->refcount))\n+\t\twait_for_completion(&np->released);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(node_private_unregister);\n+\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n /**\n@@ -959,6 +1152,7 @@ static struct node_attr node_state_attr[] = {\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n \t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\n \t\t\t\t\t   N_GENERIC_INITIATOR),\n@@ -972,6 +1166,7 @@ static struct attribute *node_state_attrs[] = {\n \t&node_state_attr[N_HIGH_MEMORY].attr.attr,\n #endif\n \t&node_state_attr[N_MEMORY].attr.attr,\n+\t&node_state_attr[N_MEMORY_PRIVATE].attr.attr,\n \t&node_state_attr[N_CPU].attr.attr,\n \t&node_state_attr[N_GENERIC_INITIATOR].attr.attr,\n \tNULL\n@@ -1007,5 +1202,7 @@ void __init node_dev_init(void)\n \t\t\tpanic(\"%s() failed to add node: %d\\n\", __func__, ret);\n \t}\n \n+\tnode_private_initialized = true;\n+\n \tregister_memory_blocks_under_nodes();\n }\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex b01cb1e49896..992eb1c5a2c6 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -25,6 +25,8 @@\n #include <linux/zswap.h>\n #include <asm/page.h>\n \n+struct node_private;\n+\n /* Free memory management - zoned buddy allocator.  */\n #ifndef CONFIG_ARCH_FORCE_MAX_ORDER\n #define MAX_PAGE_ORDER 10\n@@ -1514,6 +1516,8 @@ typedef struct pglist_data {\n \tatomic_long_t\t\tvm_stat[NR_VM_NODE_STAT_ITEMS];\n #ifdef CONFIG_NUMA\n \tstruct memory_tier __rcu *memtier;\n+\tstruct node_private __rcu *node_private;\n+\tbool private;\n #endif\n #ifdef CONFIG_MEMORY_FAILURE\n \tstruct memory_failure_stats mf_stats;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nnew file mode 100644\nindex 000000000000..6a70ec39d569\n--- /dev/null\n+++ b/include/linux/node_private.h\n@@ -0,0 +1,210 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_NODE_PRIVATE_H\n+#define _LINUX_NODE_PRIVATE_H\n+\n+#include <linux/completion.h>\n+#include <linux/mm.h>\n+#include <linux/nodemask.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+\n+struct page;\n+struct vm_area_struct;\n+struct vm_fault;\n+\n+/**\n+ * struct node_private_ops - Callbacks for private node services\n+ *\n+ * Services register these callbacks to intercept MM operations that affect\n+ * their private nodes.\n+ *\n+ * Flag bits control which MM subsystems may operate on folios on this node.\n+ *\n+ * The pgdat->node_private pointer is RCU-protected.  Callbacks fall into\n+ * three categories based on their calling context:\n+ *\n+ * Folio-referenced callbacks (RCU released before callback):\n+ *   The caller holds a reference to a folio on the private node, which\n+ *   pins the node's memory online and prevents node_private teardown.\n+ *\n+ * Refcounted callbacks (RCU released before callback):\n+ *   The caller has no folio on the private node (e.g., folios are on a\n+ *   source node being migrated TO this node).  A temporary refcount is\n+ *   taken on node_private under rcu_read_lock to keep the structure (and\n+ *   the service module) alive across the callback.  node_private_unregister\n+ *   waits for all temporary references to drain before returning.\n+ *\n+ * Non-folio callbacks (rcu_read_lock held during callback):\n+ *   No folio reference exists, so rcu_read_lock is held across the\n+ *   callback to prevent node_private from being freed.\n+ *   These callbacks MUST NOT sleep.\n+ *\n+ * @flags: Operation exclusion flags (NP_OPS_* constants).\n+ *\n+ */\n+struct node_private_ops {\n+\tunsigned long flags;\n+};\n+\n+/**\n+ * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n+ *\n+ * This structure is allocated by the driver and passed to node_private_register().\n+ * The driver owns the memory and must ensure it remains valid until after\n+ * node_private_unregister() returns with the reference count dropped to 0.\n+ *\n+ * @owner: Opaque driver identifier\n+ * @refcount: Reference count (1 = registered; temporary refs for non-folio\n+ *\t\tcallbacks that may sleep; 0 = fully released)\n+ * @released: Signaled when refcount drops to 0; unregister waits on this\n+ * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ */\n+struct node_private {\n+\tvoid *owner;\n+\trefcount_t refcount;\n+\tstruct completion released;\n+\tconst struct node_private_ops *ops;\n+};\n+\n+#ifdef CONFIG_NUMA\n+\n+#include <linux/mmzone.h>\n+\n+/**\n+ * folio_is_private_node - Check if folio is on an N_MEMORY_PRIVATE node\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio resides on a private node.\n+ */\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn node_state(folio_nid(folio), N_MEMORY_PRIVATE);\n+}\n+\n+/**\n+ * page_is_private_node - Check if page is on an N_MEMORY_PRIVATE node\n+ * @page: The page to check\n+ *\n+ * Returns true if the page resides on a private node.\n+ */\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\tconst struct node_private_ops *ops;\n+\tstruct node_private *np;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(folio_nid(folio))->node_private);\n+\tops = np ? np->ops : NULL;\n+\trcu_read_unlock();\n+\n+\treturn ops;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\tstruct node_private *np;\n+\tunsigned long flags;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tflags = (np && np->ops) ? np->ops->flags : 0;\n+\trcu_read_unlock();\n+\n+\treturn flags;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn node_private_flags(folio_nid(f)) & flag;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn node_private_flags(nid) & flag;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn node_private_flags(zone_to_nid(z)) & flag;\n+}\n+\n+#else /* !CONFIG_NUMA */\n+\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn false;\n+}\n+\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn false;\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\treturn NULL;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+#endif /* CONFIG_NUMA */\n+\n+#if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\n+\n+int node_private_register(int nid, struct node_private *np);\n+int node_private_unregister(int nid);\n+int node_private_set_ops(int nid, const struct node_private_ops *ops);\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n+\n+#else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n+\n+static inline int node_private_register(int nid, struct node_private *np)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_unregister(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline int node_private_set_ops(int nid,\n+\t\t\t\t       const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_clear_ops(int nid,\n+\t\t\t\t\t const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+#endif /* CONFIG_NUMA && CONFIG_MEMORY_HOTPLUG */\n+\n+#endif /* _LINUX_NODE_PRIVATE_H */\ndiff --git a/include/linux/nodemask.h b/include/linux/nodemask.h\nindex bd38648c998d..c9bcfd5a9a06 100644\n--- a/include/linux/nodemask.h\n+++ b/include/linux/nodemask.h\n@@ -391,6 +391,7 @@ enum node_states {\n \tN_HIGH_MEMORY = N_NORMAL_MEMORY,\n #endif\n \tN_MEMORY,\t\t/* The node has memory(regular, high, movable) */\n+\tN_MEMORY_PRIVATE,\t/* The node's memory is private */\n \tN_CPU,\t\t/* The node has one or more cpus */\n \tN_GENERIC_INITIATOR,\t/* The node has one or more Generic Initiators */\n \tNR_NODE_STATES\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-2-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about allocations landing on private nodes without explicit permission by introducing __GFP_PRIVATE and updating cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE nodes unless this flag is set.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY_PRIVATE nodes hold device-managed memory that should not be\nused for general allocations. Without a gating mechanism, any allocation\ncould land on a private node if it appears in the task's mems_allowed.\n\nIntroduce __GFP_PRIVATE that explicitly opts in to allocation from\nN_MEMORY_PRIVATE nodes.\n\nAdd the GFP_PRIVATE compound mask (__GFP_PRIVATE | __GFP_THISNODE)\nfor callers that explicitly target private nodes to help prevent\nfallback allocations from DRAM.\n\nUpdate cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE\nnodes unless __GFP_PRIVATE is set.\n\nIn interrupt context, only N_MEMORY nodes are valid.\n\nUpdate cpuset_handle_hotplug() to include N_MEMORY_PRIVATE nodes in\nthe effective mems set, allowing cgroup-level control over private\nnode access.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/gfp_types.h      | 15 +++++++++++++--\n include/trace/events/mmflags.h |  4 ++--\n kernel/cgroup/cpuset.c         | 32 ++++++++++++++++++++++++++++----\n 3 files changed, 43 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/gfp_types.h b/include/linux/gfp_types.h\nindex 3de43b12209e..ac375f9a0fc2 100644\n--- a/include/linux/gfp_types.h\n+++ b/include/linux/gfp_types.h\n@@ -33,7 +33,7 @@ enum {\n \t___GFP_IO_BIT,\n \t___GFP_FS_BIT,\n \t___GFP_ZERO_BIT,\n-\t___GFP_UNUSED_BIT,\t/* 0x200u unused */\n+\t___GFP_PRIVATE_BIT,\n \t___GFP_DIRECT_RECLAIM_BIT,\n \t___GFP_KSWAPD_RECLAIM_BIT,\n \t___GFP_WRITE_BIT,\n@@ -69,7 +69,7 @@ enum {\n #define ___GFP_IO\t\tBIT(___GFP_IO_BIT)\n #define ___GFP_FS\t\tBIT(___GFP_FS_BIT)\n #define ___GFP_ZERO\t\tBIT(___GFP_ZERO_BIT)\n-/* 0x200u unused */\n+#define ___GFP_PRIVATE\t\tBIT(___GFP_PRIVATE_BIT)\n #define ___GFP_DIRECT_RECLAIM\tBIT(___GFP_DIRECT_RECLAIM_BIT)\n #define ___GFP_KSWAPD_RECLAIM\tBIT(___GFP_KSWAPD_RECLAIM_BIT)\n #define ___GFP_WRITE\t\tBIT(___GFP_WRITE_BIT)\n@@ -139,6 +139,11 @@ enum {\n  * %__GFP_ACCOUNT causes the allocation to be accounted to kmemcg.\n  *\n  * %__GFP_NO_OBJ_EXT causes slab allocation to have no object extension.\n+ *\n+ * %__GFP_PRIVATE allows allocation from N_MEMORY_PRIVATE nodes (e.g., compressed\n+ * memory, accelerator memory). Without this flag, allocations are restricted\n+ * to N_MEMORY nodes only. Used by migration/demotion paths when explicitly\n+ * targeting private nodes.\n  */\n #define __GFP_RECLAIMABLE ((__force gfp_t)___GFP_RECLAIMABLE)\n #define __GFP_WRITE\t((__force gfp_t)___GFP_WRITE)\n@@ -146,6 +151,7 @@ enum {\n #define __GFP_THISNODE\t((__force gfp_t)___GFP_THISNODE)\n #define __GFP_ACCOUNT\t((__force gfp_t)___GFP_ACCOUNT)\n #define __GFP_NO_OBJ_EXT   ((__force gfp_t)___GFP_NO_OBJ_EXT)\n+#define __GFP_PRIVATE\t((__force gfp_t)___GFP_PRIVATE)\n \n /**\n  * DOC: Watermark modifiers\n@@ -367,6 +373,10 @@ enum {\n  * available and will not wake kswapd/kcompactd on failure. The _LIGHT\n  * version does not attempt reclaim/compaction at all and is by default used\n  * in page fault path, while the non-light is used by khugepaged.\n+ *\n+ * %GFP_PRIVATE adds %__GFP_THISNODE by default to prevent any fallback\n+ * allocations to other nodes, given that the caller was already attempting\n+ * to access driver-managed memory explicitly.\n  */\n #define GFP_ATOMIC\t(__GFP_HIGH|__GFP_KSWAPD_RECLAIM)\n #define GFP_KERNEL\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n@@ -382,5 +392,6 @@ enum {\n #define GFP_TRANSHUGE_LIGHT\t((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \\\n \t\t\t __GFP_NOMEMALLOC | __GFP_NOWARN) & ~__GFP_RECLAIM)\n #define GFP_TRANSHUGE\t(GFP_TRANSHUGE_LIGHT | __GFP_DIRECT_RECLAIM)\n+#define GFP_PRIVATE\t(__GFP_PRIVATE | __GFP_THISNODE)\n \n #endif /* __LINUX_GFP_TYPES_H */\ndiff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h\nindex a6e5a44c9b42..f042cd848451 100644\n--- a/include/trace/events/mmflags.h\n+++ b/include/trace/events/mmflags.h\n@@ -37,7 +37,8 @@\n \tTRACE_GFP_EM(HARDWALL)\t\t\t\\\n \tTRACE_GFP_EM(THISNODE)\t\t\t\\\n \tTRACE_GFP_EM(ACCOUNT)\t\t\t\\\n-\tTRACE_GFP_EM(ZEROTAGS)\n+\tTRACE_GFP_EM(ZEROTAGS)\t\t\t\\\n+\tTRACE_GFP_EM(PRIVATE)\n \n #ifdef CONFIG_KASAN_HW_TAGS\n # define TRACE_GFP_FLAGS_KASAN\t\t\t\\\n@@ -73,7 +74,6 @@\n TRACE_GFP_FLAGS\n \n /* Just in case these are ever used */\n-TRACE_DEFINE_ENUM(___GFP_UNUSED_BIT);\n TRACE_DEFINE_ENUM(___GFP_LAST_BIT);\n \n #define gfpflag_string(flag) {(__force unsigned long)flag, #flag}\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 473aa9261e16..1a597f0c7c6c 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -444,21 +444,32 @@ static void guarantee_active_cpus(struct task_struct *tsk,\n }\n \n /*\n- * Return in *pmask the portion of a cpusets's mems_allowed that\n+ * Return in *pmask the portion of a cpuset's mems_allowed that\n  * are online, with memory.  If none are online with memory, walk\n  * up the cpuset hierarchy until we find one that does have some\n  * online mems.  The top cpuset always has some mems online.\n  *\n  * One way or another, we guarantee to return some non-empty subset\n- * of node_states[N_MEMORY].\n+ * of node_states[N_MEMORY].  N_MEMORY_PRIVATE nodes from the\n+ * original cpuset are preserved, but only N_MEMORY nodes are\n+ * pulled from ancestors.\n  *\n  * Call with callback_lock or cpuset_mutex held.\n  */\n static void guarantee_online_mems(struct cpuset *cs, nodemask_t *pmask)\n {\n+\tstruct cpuset *orig_cs = cs;\n+\tint nid;\n+\n \twhile (!nodes_intersects(cs->effective_mems, node_states[N_MEMORY]))\n \t\tcs = parent_cs(cs);\n+\n \tnodes_and(*pmask, cs->effective_mems, node_states[N_MEMORY]);\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_isset(nid, orig_cs->effective_mems))\n+\t\t\tnode_set(nid, *pmask);\n+\t}\n }\n \n /**\n@@ -4075,7 +4086,9 @@ static void cpuset_handle_hotplug(void)\n \n \t/* fetch the available cpus/mems and find out which changed how */\n \tcpumask_copy(&new_cpus, cpu_active_mask);\n-\tnew_mems = node_states[N_MEMORY];\n+\n+\t/* Include N_MEMORY_PRIVATE so cpuset controls access the same way */\n+\tnodes_or(new_mems, node_states[N_MEMORY], node_states[N_MEMORY_PRIVATE]);\n \n \t/*\n \t * If subpartitions_cpus is populated, it is likely that the check\n@@ -4488,10 +4501,21 @@ bool cpuset_node_allowed(struct cgroup *cgroup, int nid)\n  * __alloc_pages() will include all nodes.  If the slab allocator\n  * is passed an offline node, it will fall back to the local node.\n  * See kmem_cache_alloc_node().\n+ *\n+ *\n+ * Private nodes aren't eligible for these allocations, so skip them.\n+ * guarantee_online_mems guaranttes at least one N_MEMORY node is set.\n  */\n static int cpuset_spread_node(int *rotor)\n {\n-\treturn *rotor = next_node_in(*rotor, current->mems_allowed);\n+\tint node;\n+\n+\tdo {\n+\t\tnode = next_node_in(*rotor, current->mems_allowed);\n+\t\t*rotor = node;\n+\t} while (node_state(node, N_MEMORY_PRIVATE));\n+\n+\treturn node;\n }\n \n /**\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-3-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern that the patch's cpuset filtering pattern does not account for N_MEMORY_PRIVATE nodes on systems without cpusets, leading to private-node zones leaking into allocation paths. The author agreed to consolidate zone filtering by introducing a new helper function numa_zone_allowed(), which checks cpuset membership when cpusets are enabled and gates N_MEMORY_PRIVATE zones behind __GFP_PRIVATE globally.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Various locations in mm/ open-code cpuset filtering with:\n\n  cpusets_enabled() && ALLOC_CPUSET && !__cpuset_zone_allowed()\n\nThis pattern does not account for N_MEMORY_PRIVATE nodes on systems\nwithout cpusets, so private-node zones can leak into allocation\npaths that should only see general-purpose memory.\n\nAdd numa_zone_allowed() which consolidates zone filtering. It checks\ncpuset membership when cpusets are enabled, and otherwise gates\nN_MEMORY_PRIVATE zones behind __GFP_PRIVATE globally.\n\nReplace the open-coded patterns in mm/ with the new helper.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/compaction.c |  6 ++----\n mm/hugetlb.c    |  2 +-\n mm/internal.h   |  7 +++++++\n mm/page_alloc.c | 31 ++++++++++++++++++++-----------\n mm/slub.c       |  3 ++-\n 5 files changed, 32 insertions(+), 17 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..6a65145b03d8 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -2829,10 +2829,8 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tenum compact_result status;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 51273baec9e5..f2b914ab5910 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -1353,7 +1353,7 @@ static struct folio *dequeue_hugetlb_folio_nodemask(struct hstate *h, gfp_t gfp_\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {\n \t\tstruct folio *folio;\n \n-\t\tif (!cpuset_zone_allowed(zone, gfp_mask))\n+\t\tif (!numa_zone_alloc_allowed(ALLOC_CPUSET, zone, gfp_mask))\n \t\t\tcontinue;\n \t\t/*\n \t\t * no need to ask again on the same node. Pool is node rather than\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 23ee14790227..97023748e6a9 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t      gfp_t gfp_mask);\n #else\n #define node_reclaim_mode 0\n \n@@ -1218,6 +1220,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t\t     gfp_t gfp_mask)\n+{\n+\treturn true;\n+}\n #endif\n \n static inline bool node_reclaim_enabled(void)\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 2facee0805da..47f2619d3840 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3690,6 +3690,21 @@ static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n \treturn node_distance(zone_to_nid(local_zone), zone_to_nid(zone)) <=\n \t\t\t\tnode_reclaim_distance;\n }\n+\n+/* Returns true if allocation from this zone is permitted */\n+bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone, gfp_t gfp_mask)\n+{\n+\t/* Gate N_MEMORY_PRIVATE zones behind __GFP_PRIVATE */\n+\tif (!(gfp_mask & __GFP_PRIVATE) &&\n+\t    node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn false;\n+\n+\t/* If cpusets is being used, check mems_allowed */\n+\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET))\n+\t\treturn cpuset_zone_allowed(zone, gfp_mask);\n+\n+\treturn true;\n+}\n #else\t/* CONFIG_NUMA */\n static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n {\n@@ -3781,10 +3796,8 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\tstruct page *page;\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \t\t/*\n \t\t * When allocating a page cache page for writing, we\n \t\t * want to get it from a node that is within its dirty\n@@ -4585,10 +4598,8 @@ should_reclaim_retry(gfp_t gfp_mask, unsigned order,\n \t\tunsigned long min_wmark = min_wmark_pages(zone);\n \t\tbool wmark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tavailable = reclaimable = zone_reclaimable_pages(zone);\n \t\tavailable += zone_page_state_snapshot(zone, NR_FREE_PAGES);\n@@ -5084,10 +5095,8 @@ unsigned long alloc_pages_bulk_noprof(gfp_t gfp, int preferred_nid,\n \tfor_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&\n-\t\t    !__cpuset_zone_allowed(zone, gfp)) {\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp))\n \t\t\tcontinue;\n-\t\t}\n \n \t\tif (nr_online_nodes > 1 && zone != zonelist_zone(ac.preferred_zoneref) &&\n \t\t    zone_to_nid(zone) != zonelist_node_idx(ac.preferred_zoneref)) {\ndiff --git a/mm/slub.c b/mm/slub.c\nindex 861592ac5425..e4bd6ede81d1 100644\n--- a/mm/slub.c\n+++ b/mm/slub.c\n@@ -3595,7 +3595,8 @@ static struct slab *get_any_partial(struct kmem_cache *s,\n \n \t\t\tn = get_node(s, zone_to_nid(zone));\n \n-\t\t\tif (n && cpuset_zone_allowed(zone, pc->flags) &&\n+\t\t\tif (n && numa_zone_alloc_allowed(ALLOC_CPUSET, zone,\n+\t\t\t\t\t\t   pc->flags) &&\n \t\t\t\t\tn->nr_partial > s->min_partial) {\n \t\t\t\tslab = get_partial_node(s, n, pc);\n \t\t\t\tif (slab) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-4-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about N_MEMORY fallback lists including N_MEMORY_PRIVATE nodes, explaining that this would allow allocation from them in some scenarios and cause iterations over ineligible nodes. The author agreed to add code to handle private node primary fallback lists correctly.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY fallback lists should not include N_MEMORY_PRIVATE nodes, at\nworst this would allow allocation from them in some scenarios, and at\nbest it causes iterations over nodes that aren't eligible.\n\nPrivate node primary fallback lists do include N_MEMORY nodes so\nkernel/slab allocations made on behalf of the private node can\nfall back to DRAM when __GFP_PRIVATE is not set.\n\nThe nofallback list contains only the node's own zones, restricting\n__GFP_THISNODE allocations to the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/page_alloc.c | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 47f2619d3840..5a1b35421d78 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5683,6 +5683,26 @@ static void build_zonelists(pg_data_t *pgdat)\n \tlocal_node = pgdat->node_id;\n \tprev_node = local_node;\n \n+\t/*\n+\t * Private nodes need N_MEMORY nodes as fallback for kernel allocations\n+\t * (e.g., slab objects allocated on behalf of this node).\n+\t */\n+\tif (node_state(local_node, N_MEMORY_PRIVATE)) {\n+\t\tnode_order[nr_nodes++] = local_node;\n+\t\tnode_set(local_node, used_mask);\n+\n+\t\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0)\n+\t\t\tnode_order[nr_nodes++] = node;\n+\n+\t\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n+\t\tbuild_thisnode_zonelists(pgdat);\n+\t\tpr_info(\"Fallback order for Node %d (private):\", local_node);\n+\t\tfor (node = 0; node < nr_nodes; node++)\n+\t\t\tpr_cont(\" %d\", node_order[node]);\n+\t\tpr_cont(\"\\n\");\n+\t\treturn;\n+\t}\n+\n \tmemset(node_order, 0, sizeof(node_order));\n \twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-5-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the need for a unified predicate to exclude both ZONE_DEVICE and N_MEMORY_PRIVATE folios from MM operations, proposing to add a new function folio_is_private_managed() that returns true for either type of folio. The author agreed to replace existing checks with this new predicate.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Multiple mm/ subsystems already skip operations for ZONE_DEVICE folios,\nand N_MEMORY_PRIVATE folios share the checkpoints for ZONE_DEVICE pages.\n\nAdd folio_is_private_managed() as a unified predicate that returns true\nfor folios on N_MEMORY_PRIVATE nodes or in ZONE_DEVICE.\n\nThis predicate replaces folio_is_zone_device at skip sites where both\nfolio types should be excluded from an MM operation.\n\nAt some locations, explicit zone_device vs private_node checks are more\nappropriate when the operations between the two fundamentally differ.\n\nThe !CONFIG_NUMA stubs fall through to folio_is_zone_device() only,\npreserving existing behavior when NUMA is disabled.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 6a70ec39d569..7687a4cf990c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -92,6 +92,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio) || folio_is_private_node(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n@@ -146,6 +156,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn false;\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-6-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about memory locking in private nodes, agreeing that they should not be mlocked and explaining that the existing folio_is_zone_device check already handles this for zone devices. The author extended this check to include private nodes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed with the approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nmlocked.  The existing folio_is_zone_device check is already correctly\nplaced to handle this - simply extend it for private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/mlock.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/mlock.c b/mm/mlock.c\nindex 2f699c3497a5..c56159253e45 100644\n--- a/mm/mlock.c\n+++ b/mm/mlock.c\n@@ -25,6 +25,7 @@\n #include <linux/memcontrol.h>\n #include <linux/mm_inline.h>\n #include <linux/secretmem.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -366,7 +367,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (is_huge_zero_pmd(*pmd))\n \t\t\tgoto out;\n \t\tfolio = pmd_folio(*pmd);\n-\t\tif (folio_is_zone_device(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)))\n \t\t\tgoto out;\n \t\tif (vma->vm_flags & VM_LOCKED)\n \t\t\tmlock_folio(folio);\n@@ -386,7 +387,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (!pte_present(ptent))\n \t\t\tcontinue;\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\tstep = folio_mlock_step(folio, pte, addr, end);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-7-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about madvise operations interfering with device driver memory management in private nodes, agreeing to extend the existing zone_device check to cover private nodes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nsubjectto madvise cold/pageout/free operations that would interfere\nwith the driver's memory management.\n\nExtend the existing zone_device check to cover private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/madvise.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/madvise.c b/mm/madvise.c\nindex b617b1be0f53..3aac105e840b 100644\n--- a/mm/madvise.c\n+++ b/mm/madvise.c\n@@ -32,6 +32,7 @@\n #include <linux/leafops.h>\n #include <linux/shmem_fs.h>\n #include <linux/mmu_notifier.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlb.h>\n \n@@ -475,7 +476,7 @@ static int madvise_cold_or_pageout_pte_range(pmd_t *pmd,\n \t\t\tcontinue;\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n@@ -704,7 +705,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,\n \t\t}\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-8-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that private node folios should not participate in KSM merging by default, and agreed to extend existing zone_device checks to cover private node folios as well.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not participate in KSM merging by default.\nThe driver manages the memory lifecycle and KSM's page sharing can\ninterfere with driver operations.\n\nExtend the existing zone_device checks in get_mergeable_page and\nksm_next_page_pmd_entry to cover private node folios as well.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/ksm.c | 9 ++++++---\n 1 file changed, 6 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/ksm.c b/mm/ksm.c\nindex 2d89a7c8b4eb..c48e95a6fff9 100644\n--- a/mm/ksm.c\n+++ b/mm/ksm.c\n@@ -40,6 +40,7 @@\n #include <linux/oom.h>\n #include <linux/numa.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include \"internal.h\"\n@@ -808,7 +809,7 @@ static struct page *get_mergeable_page(struct ksm_rmap_item *rmap_item)\n \n \tfolio = folio_walk_start(&fw, vma, addr, 0);\n \tif (folio) {\n-\t\tif (!folio_is_zone_device(folio) &&\n+\t\tif (!folio_is_private_managed(folio) &&\n \t\t    folio_test_anon(folio)) {\n \t\t\tfolio_get(folio);\n \t\t\tpage = fw.page;\n@@ -2521,7 +2522,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\t\tgoto not_found_unlock;\n \t\t\tfolio = page_folio(page);\n \n-\t\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t\t    !folio_test_anon(folio))\n \t\t\t\tgoto not_found_unlock;\n \n \t\t\tpage += ((addr & (PMD_SIZE - 1)) >> PAGE_SHIFT);\n@@ -2545,7 +2547,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\tcontinue;\n \t\tfolio = page_folio(page);\n \n-\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t    !folio_test_anon(folio))\n \t\t\tcontinue;\n \t\tgoto found_unlock;\n \t}\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-9-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about collapse operations potentially promoting pages from private nodes to local nodes, which could lead to LRU inversion and defeat memory tiering. The author agreed to handle this issue similarly to zone_device for now, but noted that it may be possible to support collapse later for some private node services that report explicit support for collapse (and migration).",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "A collapse operation allocates a new large folio and migrates the\nsmaller folios into it.  This is an issue for private nodes:\n\n  1. The private node service may not support migration\n  2. Collapse may promotes pages from the private node to a local node,\n     which may result in an LRU inversion that defeats memory tiering.\n\nHandle this just like zone_device for now.\n\nIt may be possible to support this later for some private node services\nthat report explicit support for collapse (and migration).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/khugepaged.c | 7 ++++---\n 1 file changed, 4 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/khugepaged.c b/mm/khugepaged.c\nindex 97d1b2824386..36f6bc5da53c 100644\n--- a/mm/khugepaged.c\n+++ b/mm/khugepaged.c\n@@ -21,6 +21,7 @@\n #include <linux/shmem_fs.h>\n #include <linux/dax.h>\n #include <linux/ksm.h>\n+#include <linux/node_private.h>\n #include <linux/pgalloc.h>\n \n #include <asm/tlb.h>\n@@ -571,7 +572,7 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,\n \t\t\tgoto out;\n \t\t}\n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out;\n \t\t}\n@@ -1323,7 +1324,7 @@ static int hpage_collapse_scan_pmd(struct mm_struct *mm,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out_unmap;\n \t\t}\n@@ -1575,7 +1576,7 @@ int collapse_pte_mapped_thp(struct mm_struct *mm, unsigned long addr,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, ptent);\n-\t\tif (WARN_ON_ONCE(page && is_zone_device_page(page)))\n+\t\tif (WARN_ON_ONCE(page && page_is_private_managed(page)))\n \t\t\tpage = NULL;\n \t\t/*\n \t\t * Note that uprobe, debugger, or MAP_PRIVATE may change the\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-10-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the cleanup of folios when their refcount drops to zero, explaining that private nodes and zone devices have different semantics for this operation. They added a new function `folio_managed_on_free()` to handle both cases, which returns true if the folio is fully handled (zone_device) or false if it should continue through the normal free path (private_node). The author confirmed that they will include this change in the next version of the patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a folio's refcount drops to zero, the service may need to perform\ncleanup before the page returns to the buddy allocator (e.g. zeroing\npages to scrub stale compressed data / release compression ratio).\n\nAdd folio_managed_on_free() to wrap both zone_device and private node\nsemantics for this operation since they are the same.\n\nOne difference between zone_device and private node folios:\n  - private nodes may choose to either take a reference and return true\n    (\"handled\"), or return false to return it back to the buddy.\n\n  - zone_device returns the page to the buddy (always returns true)\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 30 ++++++++++++++++++++++++++++++\n mm/swap.c                    | 21 ++++++++++-----------\n 3 files changed, 46 insertions(+), 11 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7687a4cf990c..09ea7c4cb13c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -39,10 +39,16 @@ struct vm_fault;\n  *   callback to prevent node_private from being freed.\n  *   These callbacks MUST NOT sleep.\n  *\n+ * @free_folio: Called when a folio refcount drops to 0\n+ *   [folio-referenced callback]\n+ *   Returns: true if handled (skip return to buddy)\n+ *            false if no op (return to buddy)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n+\tbool (*free_folio)(struct folio *folio);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 97023748e6a9..658da41cdb8e 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1412,6 +1412,36 @@ int numa_migrate_check(struct folio *folio, struct vm_fault *vmf,\n void free_zone_device_folio(struct folio *folio);\n int migrate_device_coherent_folio(struct folio *folio);\n \n+/**\n+ * folio_managed_on_free - Notify managed-memory service that folio\n+ *                         refcount reached zero.\n+ * @folio: the folio being freed\n+ *\n+ * Returns true if the folio is fully handled (zone_device -- caller\n+ * must return immediately).  Returns false if the callback ran but\n+ * the folio should continue through the normal free path\n+ * (private_node -- pages go back to buddy).\n+ *\n+ * Returns false for normal folios (no-op).\n+ */\n+static inline bool folio_managed_on_free(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio)) {\n+\t\tfree_zone_device_folio(folio);\n+\t\treturn true;\n+\t}\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->free_folio) {\n+\t\t\tif (ops->free_folio(folio))\n+\t\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/swap.c b/mm/swap.c\nindex 2260dcd2775e..dca306e1ae6d 100644\n--- a/mm/swap.c\n+++ b/mm/swap.c\n@@ -37,6 +37,7 @@\n #include <linux/page_idle.h>\n #include <linux/local_lock.h>\n #include <linux/buffer_head.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -96,10 +97,9 @@ static void page_cache_release(struct folio *folio)\n \n void __folio_put(struct folio *folio)\n {\n-\tif (unlikely(folio_is_zone_device(folio))) {\n-\t\tfree_zone_device_folio(folio);\n-\t\treturn;\n-\t}\n+\tif (unlikely(folio_is_private_managed(folio)))\n+\t\tif (folio_managed_on_free(folio))\n+\t\t\treturn;\n \n \tif (folio_test_hugetlb(folio)) {\n \t\tfree_huge_folio(folio);\n@@ -961,19 +961,18 @@ void folios_put_refs(struct folio_batch *folios, unsigned int *refs)\n \t\tif (is_huge_zero_folio(folio))\n \t\t\tcontinue;\n \n-\t\tif (folio_is_zone_device(folio)) {\n+\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n+\t\t\tcontinue;\n+\n+\t\tif (unlikely(folio_is_private_managed(folio))) {\n \t\t\tif (lruvec) {\n \t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n \t\t\t\tlruvec = NULL;\n \t\t\t}\n-\t\t\tif (folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\t\tfree_zone_device_folio(folio);\n-\t\t\tcontinue;\n+\t\t\tif (folio_managed_on_free(folio))\n+\t\t\t\tcontinue;\n \t\t}\n \n-\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\tcontinue;\n-\n \t\t/* hugetlb has its own memcg */\n \t\tif (folio_test_hugetlb(folio)) {\n \t\t\tif (lruvec) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-11-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the lack of notification for private node services when a THP folio is split. They added an optional callback to the ops struct and updated __folio_split() to call this new callback, similar to what zone_device does.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "provided a solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private node services may need to update internal metadata when\na THP folio is split.  ZONE_DEVICE already has a split callback via\npgmap->ops; private nodes can provide the same capability.\n\nJust like zone_device, some private node services may want to know\nabout a folio being split.  Add this optional callback to the ops\nstruct and add a wrapper for zone_device and private node callback\ndispatch to be consolidated.\n\nWire this into __folio_split() where the zone_device check was made.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 33 +++++++++++++++++++++++++++++++++\n mm/huge_memory.c             |  6 ++++--\n 2 files changed, 37 insertions(+), 2 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 09ea7c4cb13c..f9dd2d25c8a5 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -3,6 +3,7 @@\n #define _LINUX_NODE_PRIVATE_H\n \n #include <linux/completion.h>\n+#include <linux/memremap.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -44,11 +45,19 @@ struct vm_fault;\n  *   Returns: true if handled (skip return to buddy)\n  *            false if no op (return to buddy)\n  *\n+ * @folio_split: Notification that a folio on this private node is being split.\n+ *    [folio-referenced callback]\n+ *     Called from the folio split path via folio_managed_split_cb().\n+ *     @folio is the original folio; @new_folio is the newly created folio,\n+ *     or NULL when called for the final (original) folio after all sub-folios\n+ *     have been split off.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n+\tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n \tunsigned long flags;\n };\n \n@@ -150,6 +159,24 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn node_private_flags(zone_to_nid(z)) & flag;\n }\n \n+static inline void node_private_split_cb(struct folio *folio,\n+\t\t\t\t\t struct folio *new_folio)\n+{\n+\tconst struct node_private_ops *ops = folio_node_private_ops(folio);\n+\n+\tif (ops && ops->folio_split)\n+\t\tops->folio_split(folio, new_folio);\n+}\n+\n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+\telse if (folio_is_private_node(original_folio))\n+\t\tnode_private_split_cb(original_folio, new_folio);\n+}\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -198,6 +225,12 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn false;\n }\n \n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+}\n #endif /* CONFIG_NUMA */\n \n #if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 40cf59301c21..2ecae494291a 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -24,6 +24,7 @@\n #include <linux/freezer.h>\n #include <linux/mman.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/debugfs.h>\n #include <linux/migrate.h>\n@@ -3850,7 +3851,7 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \n \t\t\tnext = folio_next(new_folio);\n \n-\t\t\tzone_device_private_split_cb(folio, new_folio);\n+\t\t\tfolio_managed_split_cb(folio, new_folio);\n \n \t\t\tfolio_ref_unfreeze(new_folio,\n \t\t\t\t\t   folio_cache_ref_count(new_folio) + 1);\n@@ -3889,7 +3890,8 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \t\t\tfolio_put_refs(new_folio, nr_pages);\n \t\t}\n \n-\t\tzone_device_private_split_cb(folio, NULL);\n+\t\tfolio_managed_split_cb(folio, NULL);\n+\n \t\t/*\n \t\t * Unfreeze @folio only after all page cache entries, which\n \t\t * used to point to it, have been updated with new folios.\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-12-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about user-driven migration to private nodes, agreeing that ZONE_DEVICE always rejects user migration but private nodes should be able to opt in. They added the NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper to support this feature, allowing migrate_pages syscall to target private nodes when the destination node supports NP_OPS_MIGRATION.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "added new functionality"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services may want to support user-driven migration\n(migrate_pages syscall, mbind) to allow data movement between regular\nand private nodes.\n\nZONE_DEVICE always rejects user migration, but private nodes should\nbe able to opt in.\n\nAdd NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper that\ndispatches migration requests.  Private nodes can either set the flag\nand provide a custom migrate_to callback for driver-managed migration.\n\nIn migrate_to_node(), allows GFP_PRIVATE when the destination node\nsupports NP_OPS_MIGRATION, enabling migrate_pages syscall to target\nprivate nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |   4 ++\n include/linux/migrate.h      |  10 +++\n include/linux/node_private.h | 122 +++++++++++++++++++++++++++++++++++\n mm/damon/paddr.c             |   3 +\n mm/internal.h                |  24 +++++++\n mm/mempolicy.c               |  10 +--\n mm/migrate.c                 |  49 ++++++++++----\n mm/rmap.c                    |   4 +-\n 8 files changed, 206 insertions(+), 20 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 646dc48a23b5..e587f5781135 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -949,6 +949,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \tif (!node_possible(nid))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MIGRATION) &&\n+\t    (!ops->migrate_to || !ops->folio_migrate))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 26ca00c325d9..7b2da3875ff2 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -71,6 +71,9 @@ void folio_migrate_flags(struct folio *newfolio, struct folio *folio);\n int folio_migrate_mapping(struct address_space *mapping,\n \t\tstruct folio *newfolio, struct folio *folio, int extra_count);\n int set_movable_ops(const struct movable_operations *ops, enum pagetype type);\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason);\n \n #else\n \n@@ -96,6 +99,13 @@ static inline int set_movable_ops(const struct movable_operations *ops, enum pag\n {\n \treturn -ENOSYS;\n }\n+static inline int migrate_folios_to_node(struct list_head *folios,\n+\t\t\t\t\t\t  int nid,\n+\t\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t\t  enum migrate_reason reason)\n+{\n+\treturn -ENOSYS;\n+}\n \n #endif /* CONFIG_MIGRATION */\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex f9dd2d25c8a5..0c5be1ee6e60 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -4,6 +4,7 @@\n \n #include <linux/completion.h>\n #include <linux/memremap.h>\n+#include <linux/migrate_mode.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -52,15 +53,40 @@ struct vm_fault;\n  *     or NULL when called for the final (original) folio after all sub-folios\n  *     have been split off.\n  *\n+ * @migrate_to: Migrate folios TO this node.\n+ *\t[refcounted callback]\n+ *\tReturns: 0 on full success, >0 = number of folios that failed to\n+ *\t\t migrate, <0 = error.  Matches migrate_pages() semantics.\n+ *\t\t @nr_succeeded is set to the number of successfully migrated\n+ *\t\t folios (may be NULL if caller doesn't need it).\n+ *\n+ * @folio_migrate: Post-migration notification that a folio on this private node\n+ *    changed physical location (on the same node or a different node).\n+ *    [folio-referenced callback]\n+ *     Called from migrate_folio_move() after data has been copied but before\n+ *     migration entries are replaced with real PTEs.  Both @src and @dst are\n+ *     locked.  Faults block in migration_entry_wait() until\n+ *     remove_migration_ptes() runs, so the service can safely update\n+ *     PFN-based metadata (compression tables, device page tables, DMA\n+ *     mappings, etc.) before any access through the page tables.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n \tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n+\tint (*migrate_to)(struct list_head *folios, int nid,\n+\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t  unsigned int *nr_succeeded);\n+\tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tunsigned long flags;\n };\n \n+/* Allow user/kernel migration; requires migrate_to and folio_migrate */\n+#define NP_OPS_MIGRATION\t\tBIT(0)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\n@@ -177,6 +203,81 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n \t\tnode_private_split_cb(original_folio, new_folio);\n }\n \n+#ifdef CONFIG_MEMORY_HOTPLUG\n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn -ENOENT;\n+\treturn node_private_has_flag(folio_nid(folio), NP_OPS_MIGRATION) ?\n+\t       folio_nid(folio) : -ENOENT;\n+}\n+\n+/**\n+ * folio_managed_allows_migrate - Check if a managed folio supports migration\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio can be migrated.  For zone_device folios, only\n+ * device_private and device_coherent support migration.  For private node\n+ * folios, migration requires NP_OPS_MIGRATION.  Normal folios always\n+ * return true.\n+ */\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\tif (folio_is_private_node(folio))\n+\t\treturn folio_private_flags(folio, NP_OPS_MIGRATION);\n+\treturn true;\n+}\n+\n+/**\n+ * node_private_migrate_to - Attempt service-specific migration to a private node\n+ * @folios: list of folios to migrate (may sleep)\n+ * @nid: target node\n+ * @mode: migration mode (MIGRATE_ASYNC, MIGRATE_SYNC, etc.)\n+ * @reason: migration reason (MR_DEMOTION, MR_SYSCALL, etc.)\n+ * @nr_succeeded: optional output for number of successfully migrated folios\n+ *\n+ * If @nid is an N_MEMORY_PRIVATE node with a migrate_to callback,\n+ * invokes the callback and returns the result with migrate_pages()\n+ * semantics (0 = full success, >0 = failure count, <0 = error).\n+ * Returns -ENODEV if the node is not private or the service is being\n+ * torn down.\n+ *\n+ * The source folios are on other nodes, so they do not pin the target\n+ * node's node_private.  A temporary refcount is taken under rcu_read_lock\n+ * to keep node_private (and the service module) alive across the callback.\n+ */\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\tint (*fn)(struct list_head *, int, enum migrate_mode,\n+\t\t  enum migrate_reason, unsigned int *);\n+\tstruct node_private *np;\n+\tint ret;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (!np || !np->ops || !np->ops->migrate_to ||\n+\t    !refcount_inc_not_zero(&np->refcount)) {\n+\t\trcu_read_unlock();\n+\t\treturn -ENODEV;\n+\t}\n+\tfn = np->ops->migrate_to;\n+\trcu_read_unlock();\n+\n+\tret = fn(folios, nid, mode, reason, nr_succeeded);\n+\n+\tif (refcount_dec_and_test(&np->refcount))\n+\t\tcomplete(&np->released);\n+\n+\treturn ret;\n+}\n+#endif /* CONFIG_MEMORY_HOTPLUG */\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -242,6 +343,27 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\treturn -ENOENT;\n+}\n+\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\treturn true;\n+}\n+\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\treturn -ENODEV;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/damon/paddr.c b/mm/damon/paddr.c\nindex 07a8aead439e..532b8e2c62b0 100644\n--- a/mm/damon/paddr.c\n+++ b/mm/damon/paddr.c\n@@ -277,6 +277,9 @@ static unsigned long damon_pa_migrate(struct damon_region *r,\n \t\telse\n \t\t\t*sz_filter_passed += folio_size(folio) / addr_unit;\n \n+\t\tif (!folio_managed_allows_migrate(folio))\n+\t\t\tgoto put_folio;\n+\n \t\tif (!folio_isolate_lru(folio))\n \t\t\tgoto put_folio;\n \t\tlist_add(&folio->lru, &folio_list);\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 658da41cdb8e..6ab4679fe943 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1442,6 +1442,30 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/**\n+ * folio_managed_migrate_notify - Notify service that a folio changed location\n+ * @src: the old folio (about to be freed)\n+ * @dst: the new folio (data already copied, migration entries still in place)\n+ *\n+ * Called from migrate_folio_move() after data has been copied but before\n+ * remove_migration_ptes() installs real PTEs pointing to @dst.  While\n+ * migration entries are in place, faults block in migration_entry_wait(),\n+ * so the service can safely update PFN-based metadata before any access\n+ * through the page tables.  Both @src and @dst are locked.\n+ */\n+static inline void folio_managed_migrate_notify(struct folio *src,\n+\t\t\t\t\t\tstruct folio *dst)\n+{\n+\tconst struct node_private_ops *ops;\n+\n+\tif (!folio_is_private_node(src))\n+\t\treturn;\n+\n+\tops = folio_node_private_ops(src);\n+\tif (ops && ops->folio_migrate)\n+\t\tops->folio_migrate(src, dst);\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 68a98ba57882..2b0f9762d171 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -111,6 +111,7 @@\n #include <linux/mmu_notifier.h>\n #include <linux/printk.h>\n #include <linux/leafops.h>\n+#include <linux/node_private.h>\n #include <linux/gcd.h>\n \n #include <asm/tlbflush.h>\n@@ -1282,11 +1283,6 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tLIST_HEAD(pagelist);\n \tlong nr_failed;\n \tlong err = 0;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = dest,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n \tnodes_clear(nmask);\n \tnode_set(source, nmask);\n@@ -1311,8 +1307,8 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tmmap_read_unlock(mm);\n \n \tif (!list_empty(&pagelist)) {\n-\t\terr = migrate_pages(&pagelist, alloc_migration_target, NULL,\n-\t\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\t\terr = migrate_folios_to_node(&pagelist, dest, MIGRATE_SYNC,\n+\t\t\t\t\t     MR_SYSCALL);\n \t\tif (err)\n \t\t\tputback_movable_pages(&pagelist);\n \t}\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 5169f9717f60..a54d4af04df3 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -43,6 +43,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/memory-tiers.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1387,6 +1388,8 @@ static int migrate_folio_move(free_folio_t put_new_folio, unsigned long private,\n \tif (old_page_state & PAGE_WAS_MLOCKED)\n \t\tlru_add_drain();\n \n+\tfolio_managed_migrate_notify(src, dst);\n+\n \tif (old_page_state & PAGE_WAS_MAPPED)\n \t\tremove_migration_ptes(src, dst, 0);\n \n@@ -2165,6 +2168,7 @@ int migrate_pages(struct list_head *from, new_folio_t get_new_folio,\n \n \treturn rc_gather;\n }\n+EXPORT_SYMBOL_GPL(migrate_pages);\n \n struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n {\n@@ -2204,6 +2208,31 @@ struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n \n \treturn __folio_alloc(gfp_mask, order, nid, mtc->nmask);\n }\n+EXPORT_SYMBOL_GPL(alloc_migration_target);\n+\n+static int __migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, NULL);\n+}\n+\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason)\n+{\n+\tif (node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_private_migrate_to(folios, nid, mode,\n+\t\t\t\t\t       reason, NULL);\n+\treturn __migrate_folios_to_node(folios, nid, mode, reason);\n+}\n \n #ifdef CONFIG_NUMA\n \n@@ -2221,14 +2250,8 @@ static int store_status(int __user *status, int start, int value, int nr)\n static int do_move_pages_to_node(struct list_head *pagelist, int node)\n {\n \tint err;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = node,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n-\terr = migrate_pages(pagelist, alloc_migration_target, NULL,\n-\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\terr = migrate_folios_to_node(pagelist, node, MIGRATE_SYNC, MR_SYSCALL);\n \tif (err)\n \t\tputback_movable_pages(pagelist);\n \treturn err;\n@@ -2240,7 +2263,7 @@ static int __add_folio_for_migration(struct folio *folio, int node,\n \tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\treturn -EFAULT;\n \n-\tif (folio_is_zone_device(folio))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn -ENOENT;\n \n \tif (folio_nid(folio) == node)\n@@ -2364,7 +2387,8 @@ static int do_pages_move(struct mm_struct *mm, nodemask_t task_nodes,\n \t\terr = -ENODEV;\n \t\tif (node < 0 || node >= MAX_NUMNODES)\n \t\t\tgoto out_flush;\n-\t\tif (!node_state(node, N_MEMORY))\n+\t\tif (!node_state(node, N_MEMORY) &&\n+\t\t    !node_state(node, N_MEMORY_PRIVATE))\n \t\t\tgoto out_flush;\n \n \t\terr = -EACCES;\n@@ -2449,8 +2473,8 @@ static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n \t\tif (folio) {\n \t\t\tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\t\t\terr = -EFAULT;\n-\t\t\telse if (folio_is_zone_device(folio))\n-\t\t\t\terr = -ENOENT;\n+\t\t\telse if (unlikely(folio_is_private_managed(folio)))\n+\t\t\t\terr = folio_managed_allows_user_migrate(folio);\n \t\t\telse\n \t\t\t\terr = folio_nid(folio);\n \t\t\tfolio_walk_end(&fw, vma);\n@@ -2660,6 +2684,9 @@ int migrate_misplaced_folio_prepare(struct folio *folio,\n \tint nr_pages = folio_nr_pages(folio);\n \tpg_data_t *pgdat = NODE_DATA(node);\n \n+\tif (!folio_managed_allows_migrate(folio))\n+\t\treturn -ENOENT;\n+\n \tif (folio_is_file_lru(folio)) {\n \t\t/*\n \t\t * Do not migrate file folios that are mapped in multiple\ndiff --git a/mm/rmap.c b/mm/rmap.c\nindex f955f02d570e..805f9ceb82f3 100644\n--- a/mm/rmap.c\n+++ b/mm/rmap.c\n@@ -72,6 +72,7 @@\n #include <linux/backing-dev.h>\n #include <linux/page_idle.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/userfaultfd_k.h>\n #include <linux/mm_inline.h>\n #include <linux/oom.h>\n@@ -2616,8 +2617,7 @@ void try_to_migrate(struct folio *folio, enum ttu_flags flags)\n \t\t\t\t\tTTU_SYNC | TTU_BATCH_FLUSH)))\n \t\treturn;\n \n-\tif (folio_is_zone_device(folio) &&\n-\t    (!folio_is_device_private(folio) && !folio_is_device_coherent(folio)))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn;\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-13-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about allowing userland to directly allocate from a private node via set_mempolicy() and mbind(), but not wanting that node as normal allocable system memory in the fallback lists. The author added a flag NP_OPS_MEMPOLICY requiring NP_OPS_MIGRATION, updated sysfs \"has_memory\" attribute, and modified mempolicy migration sites to include __GFP_PRIVATE.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private nodes want userland to directly allocate from the node\nvia set_mempolicy() and mbind() - but don't want that node as normal\nallocable system memory in the fallback lists.\n\nAdd NP_OPS_MEMPOLICY flag requiring NP_OPS_MIGRATION (since mbind can\ndrive migrations).  Only allow private nodes in policy nodemasks if\nall private nodes in the mask support NP_OPS_MEMPOLICY. This prevents\n__GFP_PRIVATE from unlocking nodes without NP_OPS_MEMPOLICY support.\n\nAdd __GFP_PRIVATE to mempolicy migration sites so moves to opted-in\nprivate nodes succeed.\n\nUpdate the sysfs \"has_memory\" attribute to include N_MEMORY_PRIVATE\nnodes with NP_OPS_MEMPOLICY set, allowing existing numactl userland\ntools to work without modification.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c            | 22 +++++++++++++-\n include/linux/node_private.h   | 40 +++++++++++++++++++++++++\n include/uapi/linux/mempolicy.h |  1 +\n mm/mempolicy.c                 | 54 ++++++++++++++++++++++++++++++----\n mm/page_alloc.c                |  5 ++++\n 5 files changed, 116 insertions(+), 6 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex e587f5781135..c08b5a948779 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -953,6 +953,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (!ops->migrate_to || !ops->folio_migrate))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\n@@ -1145,6 +1149,21 @@ static ssize_t show_node_state(struct device *dev,\n \t\t\t  nodemask_pr_args(&node_states[na->state]));\n }\n \n+/* has_memory includes N_MEMORY + N_MEMORY_PRIVATE that support mempolicy. */\n+static ssize_t show_has_memory(struct device *dev,\n+\t\t\t       struct device_attribute *attr, char *buf)\n+{\n+\tnodemask_t mask = node_states[N_MEMORY];\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_set(nid, mask);\n+\t}\n+\n+\treturn sysfs_emit(buf, \"%*pbl\\n\", nodemask_pr_args(&mask));\n+}\n+\n #define _NODE_ATTR(name, state) \\\n \t{ __ATTR(name, 0444, show_node_state, NULL), state }\n \n@@ -1155,7 +1174,8 @@ static struct node_attr node_state_attr[] = {\n #ifdef CONFIG_HIGHMEM\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n-\t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY] = { __ATTR(has_memory, 0444, show_has_memory, NULL),\n+\t\t       N_MEMORY },\n \t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 0c5be1ee6e60..e9b58afa366b 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -86,6 +86,8 @@ struct node_private_ops {\n \n /* Allow user/kernel migration; requires migrate_to and folio_migrate */\n #define NP_OPS_MIGRATION\t\tBIT(0)\n+/* Allow mempolicy-directed allocation and mbind migration to this node */\n+#define NP_OPS_MEMPOLICY\t\tBIT(1)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -276,6 +278,34 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \n \treturn ret;\n }\n+\n+static inline bool node_mpol_eligible(int nid)\n+{\n+\tbool ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_state(nid, N_MEMORY);\n+\n+\trcu_read_lock();\n+\tret = node_private_has_flag(nid, NP_OPS_MEMPOLICY);\n+\trcu_read_unlock();\n+\treturn ret;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\tint nid;\n+\tbool eligible = false;\n+\n+\tfor_each_node_mask(nid, *nodes) {\n+\t\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\t\tcontinue;\n+\t\tif (!node_mpol_eligible(nid))\n+\t\t\treturn false;\n+\t\teligible = true;\n+\t}\n+\treturn eligible;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -364,6 +394,16 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \treturn -ENODEV;\n }\n \n+static inline bool node_mpol_eligible(int nid)\n+{\n+\treturn false;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/include/uapi/linux/mempolicy.h b/include/uapi/linux/mempolicy.h\nindex 8fbbe613611a..b606eae983c8 100644\n--- a/include/uapi/linux/mempolicy.h\n+++ b/include/uapi/linux/mempolicy.h\n@@ -64,6 +64,7 @@ enum {\n #define MPOL_F_SHARED  (1 << 0)\t/* identify shared policies */\n #define MPOL_F_MOF\t(1 << 3) /* this policy wants migrate on fault */\n #define MPOL_F_MORON\t(1 << 4) /* Migrate On protnone Reference On Node */\n+#define MPOL_F_PRIVATE\t(1 << 5) /* policy targets private node; use __GFP_PRIVATE */\n \n /*\n  * Enabling zone reclaim means the page allocator will attempt to fulfill\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 2b0f9762d171..8ac014950e88 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -406,8 +406,6 @@ static int mpol_new_preferred(struct mempolicy *pol, const nodemask_t *nodes)\n static int mpol_set_nodemask(struct mempolicy *pol,\n \t\t     const nodemask_t *nodes, struct nodemask_scratch *nsc)\n {\n-\tint ret;\n-\n \t/*\n \t * Default (pol==NULL) resp. local memory policies are not a\n \t * subject of any remapping. They also do not need any special\n@@ -416,9 +414,12 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \tif (!pol || pol->mode == MPOL_LOCAL)\n \t\treturn 0;\n \n-\t/* Check N_MEMORY */\n+\t/* Check N_MEMORY and N_MEMORY_PRIVATE*/\n \tnodes_and(nsc->mask1,\n \t\t  cpuset_current_mems_allowed, node_states[N_MEMORY]);\n+\tnodes_and(nsc->mask2, cpuset_current_mems_allowed,\n+\t\t  node_states[N_MEMORY_PRIVATE]);\n+\tnodes_or(nsc->mask1, nsc->mask1, nsc->mask2);\n \n \tVM_BUG_ON(!nodes);\n \n@@ -432,8 +433,13 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \telse\n \t\tpol->w.cpuset_mems_allowed = cpuset_current_mems_allowed;\n \n-\tret = mpol_ops[pol->mode].create(pol, &nsc->mask2);\n-\treturn ret;\n+\t/* All private nodes in the mask must have NP_OPS_MEMPOLICY. */\n+\tif (nodes_private_mpol_allowed(&nsc->mask2))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse if (nodes_intersects(nsc->mask2, node_states[N_MEMORY_PRIVATE]))\n+\t\treturn -EINVAL;\n+\n+\treturn mpol_ops[pol->mode].create(pol, &nsc->mask2);\n }\n \n /*\n@@ -500,6 +506,7 @@ static void mpol_rebind_default(struct mempolicy *pol, const nodemask_t *nodes)\n static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n {\n \tnodemask_t tmp;\n+\tint nid;\n \n \tif (pol->flags & MPOL_F_STATIC_NODES)\n \t\tnodes_and(tmp, pol->w.user_nodemask, *nodes);\n@@ -514,6 +521,21 @@ static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n \tif (nodes_empty(tmp))\n \t\ttmp = *nodes;\n \n+\t/*\n+\t * Drop private nodes that don't have mempolicy support.\n+\t * cpusets guarantees at least one N_MEMORY node in effective_mems\n+\t * and mems_allowed, so dropping private nodes here is safe.\n+\t */\n+\tfor_each_node_mask(nid, tmp) {\n+\t\tif (node_state(nid, N_MEMORY_PRIVATE) &&\n+\t\t    !node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_clear(nid, tmp);\n+\t}\n+\tif (nodes_intersects(tmp, node_states[N_MEMORY_PRIVATE]))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse\n+\t\tpol->flags &= ~MPOL_F_PRIVATE;\n+\n \tpol->nodes = tmp;\n }\n \n@@ -661,6 +683,9 @@ static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)\n \t}\n \tif (!queue_folio_required(folio, qp))\n \t\treturn;\n+\tif (folio_is_private_node(folio) &&\n+\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\treturn;\n \tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||\n \t    !vma_migratable(walk->vma) ||\n \t    !migrate_folio_add(folio, qp->pagelist, qp->flags))\n@@ -717,6 +742,9 @@ static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n \t\tif (!folio || folio_is_zone_device(folio))\n \t\t\tcontinue;\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\t\tcontinue;\n \t\tif (folio_test_large(folio) && max_nr != 1)\n \t\t\tnr = folio_pte_batch(folio, pte, ptent, max_nr);\n \t\t/*\n@@ -1451,6 +1479,9 @@ static struct folio *alloc_migration_target_by_mpol(struct folio *src,\n \telse\n \t\tgfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL | __GFP_COMP;\n \n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \treturn folio_alloc_mpol(gfp, order, pol, ilx, nid);\n }\n #else\n@@ -2280,6 +2311,15 @@ static nodemask_t *policy_nodemask(gfp_t gfp, struct mempolicy *pol,\n \t\t\tnodemask = &pol->nodes;\n \t\tif (pol->home_node != NUMA_NO_NODE)\n \t\t\t*nid = pol->home_node;\n+\t\telse if ((pol->flags & MPOL_F_PRIVATE) &&\n+\t\t\t !node_isset(*nid, pol->nodes)) {\n+\t\t\t/*\n+\t\t\t * Private nodes are not in N_MEMORY nodes' zonelists.\n+\t\t\t * When the preferred nid (usually numa_node_id()) can't\n+\t\t\t * reach the policy nodes, start from a policy node.\n+\t\t\t */\n+\t\t\t*nid = first_node(pol->nodes);\n+\t\t}\n \t\t/*\n \t\t * __GFP_THISNODE shouldn't even be used with the bind policy\n \t\t * because we might easily break the expectation to stay on the\n@@ -2533,6 +2573,10 @@ struct folio *vma_alloc_folio_noprof(gfp_t gfp, int order, struct vm_area_struct\n \t\tgfp |= __GFP_NOWARN;\n \n \tpol = get_vma_policy(vma, addr, order, &ilx);\n+\n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \tfolio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n \tmpol_cond_put(pol);\n \treturn folio;\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 5a1b35421d78..ec6c1f8e85d8 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3849,8 +3849,13 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\t * if another process has NUMA bindings and is causing\n \t\t * kswapd wakeups on only some nodes. Avoid accidental\n \t\t * \"node_reclaim_mode\"-like behavior in this case.\n+\t\t *\n+\t\t * Nodes without kswapd (some private nodes) are never\n+\t\t * skipped - this causes some mempolicies to silently\n+\t\t * fall back to DRAM even if the node is eligible.\n \t\t */\n \t\tif (skip_kswapd_nodes &&\n+\t\t    zone->zone_pgdat->kswapd &&\n \t\t    !waitqueue_active(&zone->zone_pgdat->kswapd_wait)) {\n \t\t\tskipped_kswapd_nodes = true;\n \t\t\tcontinue;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-14-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the memory-tier subsystem needing to know which private nodes should appear as demotion targets. They acknowledged that adding NP_OPS_DEMOTION and implementing backpressure support would allow private nodes to reject new demotions cleanly, preventing LRU inversion while still allowing forward progress. The author also mentioned that completely re-doing the demotion logic might be necessary in the future.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The memory-tier subsystem needs to know which private nodes should\nappear as demotion targets.\n\nAdd NP_OPS_DEMOTION (BIT(2)):\n   Node can be added as a demotion target by memory-tiers.\n\nAdd demotion backpressure support so private nodes can reject\nnew demotions cleanly, allowing vmscan to fall back to swap.\n\nIn the demotion path, try demotion to private nodes invididually,\nthen clear private nodes from the demotion target mask until a\nnon-private node is found, then fall back to the remaining mask.\nThis prevents LRU inversion while still allowing forward progress.\n\nThis is the closest match to the current behavior without making\nprivate nodes inaccessible or preventing forward progress. We\nshould probably completely re-do the demotion logic to allow less\nfallback and kick kswapd instead - right now we induce LRU\ninversions by simply falling back to any node in the demotion list.\n\nAdd memory_tier_refresh_demotion() export for services to trigger\nre-evaluation of demotion targets after changing their flags.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory-tiers.h |  9 +++++++\n include/linux/node_private.h | 22 +++++++++++++++++\n mm/internal.h                |  7 ++++++\n mm/memory-tiers.c            | 46 ++++++++++++++++++++++++++++++++----\n mm/page_alloc.c              | 12 +++++++---\n mm/vmscan.c                  | 30 ++++++++++++++++++++++-\n 6 files changed, 117 insertions(+), 9 deletions(-)\n\ndiff --git a/include/linux/memory-tiers.h b/include/linux/memory-tiers.h\nindex 3e1159f6762c..e1476432e359 100644\n--- a/include/linux/memory-tiers.h\n+++ b/include/linux/memory-tiers.h\n@@ -58,6 +58,7 @@ struct memory_dev_type *mt_get_memory_type(int adist);\n int next_demotion_node(int node);\n void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets);\n bool node_is_toptier(int node);\n+void memory_tier_refresh_demotion(void);\n #else\n static inline int next_demotion_node(int node)\n {\n@@ -73,6 +74,10 @@ static inline bool node_is_toptier(int node)\n {\n \treturn true;\n }\n+\n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n #endif\n \n #else\n@@ -106,6 +111,10 @@ static inline bool node_is_toptier(int node)\n \treturn true;\n }\n \n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n+\n static inline int register_mt_adistance_algorithm(struct notifier_block *nb)\n {\n \treturn 0;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e9b58afa366b..e254e36056cd 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -88,6 +88,8 @@ struct node_private_ops {\n #define NP_OPS_MIGRATION\t\tBIT(0)\n /* Allow mempolicy-directed allocation and mbind migration to this node */\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n+/* Node participates as a demotion target in memory-tiers */\n+#define NP_OPS_DEMOTION\t\t\tBIT(2)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -101,12 +103,14 @@ struct node_private_ops {\n  *\t\tcallbacks that may sleep; 0 = fully released)\n  * @released: Signaled when refcount drops to 0; unregister waits on this\n  * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ * @migration_blocked: Service signals migrations should pause\n  */\n struct node_private {\n \tvoid *owner;\n \trefcount_t refcount;\n \tstruct completion released;\n \tconst struct node_private_ops *ops;\n+\tbool migration_blocked;\n };\n \n #ifdef CONFIG_NUMA\n@@ -306,6 +310,19 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \t}\n \treturn eligible;\n }\n+\n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\tstruct node_private *np;\n+\tbool blocked;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tblocked = np && READ_ONCE(np->migration_blocked);\n+\trcu_read_unlock();\n+\n+\treturn blocked;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -404,6 +421,11 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \treturn false;\n }\n \n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 6ab4679fe943..5950e20d4023 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t  const nodemask_t *candidates);\n extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t      gfp_t gfp_mask);\n #else\n@@ -1220,6 +1222,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t\t const nodemask_t *candidates)\n+{\n+\treturn NUMA_NO_NODE;\n+}\n static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t\t     gfp_t gfp_mask)\n {\ndiff --git a/mm/memory-tiers.c b/mm/memory-tiers.c\nindex 9c742e18e48f..434190fdc078 100644\n--- a/mm/memory-tiers.c\n+++ b/mm/memory-tiers.c\n@@ -3,6 +3,7 @@\n #include <linux/lockdep.h>\n #include <linux/sysfs.h>\n #include <linux/kobject.h>\n+#include <linux/node_private.h>\n #include <linux/memory.h>\n #include <linux/memory-tiers.h>\n #include <linux/notifier.h>\n@@ -380,6 +381,8 @@ static void disable_all_demotion_targets(void)\n \t\tif (memtier)\n \t\t\tmemtier->lower_tier_mask = NODE_MASK_NONE;\n \t}\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE)\n+\t\tnode_demotion[node].preferred = NODE_MASK_NONE;\n \t/*\n \t * Ensure that the \"disable\" is visible across the system.\n \t * Readers will see either a combination of before+disable\n@@ -421,6 +424,7 @@ static void establish_demotion_targets(void)\n \tint target = NUMA_NO_NODE, node;\n \tint distance, best_distance;\n \tnodemask_t tier_nodes, lower_tier;\n+\tnodemask_t all_memory;\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n@@ -429,6 +433,13 @@ static void establish_demotion_targets(void)\n \n \tdisable_all_demotion_targets();\n \n+\t/* Include private nodes that have opted in to demotion. */\n+\tall_memory = node_states[N_MEMORY];\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(node, NP_OPS_DEMOTION))\n+\t\t\tnode_set(node, all_memory);\n+\t}\n+\n \tfor_each_node_state(node, N_MEMORY) {\n \t\tbest_distance = -1;\n \t\tnd = &node_demotion[node];\n@@ -442,12 +453,12 @@ static void establish_demotion_targets(void)\n \t\tmemtier = list_next_entry(memtier, list);\n \t\ttier_nodes = get_memtier_nodemask(memtier);\n \t\t/*\n-\t\t * find_next_best_node, use 'used' nodemask as a skip list.\n+\t\t * find_next_best_node_in, use 'used' nodemask as a skip list.\n \t\t * Add all memory nodes except the selected memory tier\n \t\t * nodelist to skip list so that we find the best node from the\n \t\t * memtier nodelist.\n \t\t */\n-\t\tnodes_andnot(tier_nodes, node_states[N_MEMORY], tier_nodes);\n+\t\tnodes_andnot(tier_nodes, all_memory, tier_nodes);\n \n \t\t/*\n \t\t * Find all the nodes in the memory tier node list of same best distance.\n@@ -455,7 +466,8 @@ static void establish_demotion_targets(void)\n \t\t * in the preferred mask when allocating pages during demotion.\n \t\t */\n \t\tdo {\n-\t\t\ttarget = find_next_best_node(node, &tier_nodes);\n+\t\t\ttarget = find_next_best_node_in(node, &tier_nodes,\n+\t\t\t\t\t\t\t&all_memory);\n \t\t\tif (target == NUMA_NO_NODE)\n \t\t\t\tbreak;\n \n@@ -495,7 +507,7 @@ static void establish_demotion_targets(void)\n \t * allocation to a set of nodes that is closer the above selected\n \t * preferred node.\n \t */\n-\tlower_tier = node_states[N_MEMORY];\n+\tlower_tier = all_memory;\n \tlist_for_each_entry(memtier, &memory_tiers, list) {\n \t\t/*\n \t\t * Keep removing current tier from lower_tier nodes,\n@@ -542,7 +554,7 @@ static struct memory_tier *set_node_memory_tier(int node)\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n-\tif (!node_state(node, N_MEMORY))\n+\tif (!node_state(node, N_MEMORY) && !node_state(node, N_MEMORY_PRIVATE))\n \t\treturn ERR_PTR(-EINVAL);\n \n \tmt_calc_adistance(node, &adist);\n@@ -865,6 +877,30 @@ int mt_calc_adistance(int node, int *adist)\n }\n EXPORT_SYMBOL_GPL(mt_calc_adistance);\n \n+/**\n+ * memory_tier_refresh_demotion() - Re-establish demotion targets\n+ *\n+ * Called by services after registering or unregistering ops->migrate_to on\n+ * a private node, so that establish_demotion_targets() picks up the change.\n+ */\n+void memory_tier_refresh_demotion(void)\n+{\n+\tint nid;\n+\n+\tmutex_lock(&memory_tier_lock);\n+\t/*\n+\t * Ensure private nodes are registered with a tier, otherwise\n+\t * they won't show up in any node's demotion targets nodemask.\n+\t */\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (!__node_get_memory_tier(nid))\n+\t\t\tset_node_memory_tier(nid);\n+\t}\n+\testablish_demotion_targets();\n+\tmutex_unlock(&memory_tier_lock);\n+}\n+EXPORT_SYMBOL_GPL(memory_tier_refresh_demotion);\n+\n static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\t\t\t\t      unsigned long action, void *_arg)\n {\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex ec6c1f8e85d8..e272dfdc6b00 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5589,7 +5589,8 @@ static int node_load[MAX_NUMNODES];\n  *\n  * Return: node id of the found node or %NUMA_NO_NODE if no node is found.\n  */\n-int find_next_best_node(int node, nodemask_t *used_node_mask)\n+int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t   const nodemask_t *candidates)\n {\n \tint n, val;\n \tint min_val = INT_MAX;\n@@ -5599,12 +5600,12 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \t * Use the local node if we haven't already, but for memoryless local\n \t * node, we should skip it and fall back to other nodes.\n \t */\n-\tif (!node_isset(node, *used_node_mask) && node_state(node, N_MEMORY)) {\n+\tif (!node_isset(node, *used_node_mask) && node_isset(node, *candidates)) {\n \t\tnode_set(node, *used_node_mask);\n \t\treturn node;\n \t}\n \n-\tfor_each_node_state(n, N_MEMORY) {\n+\tfor_each_node_mask(n, *candidates) {\n \n \t\t/* Don't want a node to appear more than once */\n \t\tif (node_isset(n, *used_node_mask))\n@@ -5636,6 +5637,11 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \treturn best_node;\n }\n \n+int find_next_best_node(int node, nodemask_t *used_node_mask)\n+{\n+\treturn find_next_best_node_in(node, used_node_mask,\n+\t\t\t\t      &node_states[N_MEMORY]);\n+}\n \n /*\n  * Build zonelists ordered by node and zones within node.\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 6113be4d3519..0f534428ea88 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -355,6 +356,10 @@ static bool can_demote(int nid, struct scan_control *sc,\n \tif (demotion_nid == NUMA_NO_NODE)\n \t\treturn false;\n \n+\t/* Don't demote when the target's service signals backpressure */\n+\tif (node_private_migration_blocked(demotion_nid))\n+\t\treturn false;\n+\n \t/* If demotion node isn't in the cgroup's mems_allowed, fall back */\n \treturn mem_cgroup_node_allowed(memcg, demotion_nid);\n }\n@@ -1022,8 +1027,10 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \t\t\t\t     struct pglist_data *pgdat)\n {\n \tint target_nid = next_demotion_node(pgdat->node_id);\n-\tunsigned int nr_succeeded;\n+\tint first_nid = target_nid;\n+\tunsigned int nr_succeeded = 0;\n \tnodemask_t allowed_mask;\n+\tint ret;\n \n \tstruct migration_target_control mtc = {\n \t\t/*\n@@ -1046,6 +1053,27 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \n \tnode_get_allowed_targets(pgdat, &allowed_mask);\n \n+\t/* Try private node targets until we find non-private node */\n+\twhile (node_state(target_nid, N_MEMORY_PRIVATE)) {\n+\t\tunsigned int nr = 0;\n+\n+\t\tret = node_private_migrate_to(demote_folios, target_nid,\n+\t\t\t\t\t      MIGRATE_ASYNC, MR_DEMOTION,\n+\t\t\t\t\t      &nr);\n+\t\tnr_succeeded += nr;\n+\t\tif (ret == 0 || list_empty(demote_folios))\n+\t\t\treturn nr_succeeded;\n+\n+\t\ttarget_nid = next_node_in(target_nid, allowed_mask);\n+\t\tif (target_nid == first_nid)\n+\t\t\treturn nr_succeeded;\n+\t\tif (!node_state(target_nid, N_MEMORY_PRIVATE))\n+\t\t\tbreak;\n+\t}\n+\n+\t/* target_nid is a non-private node; use standard migration */\n+\tmtc.nid = target_nid;\n+\n \t/* Demotion ignores all cpuset and mempolicy settings */\n \tmigrate_pages(demote_folios, alloc_demote_folio, NULL,\n \t\t      (unsigned long)&mtc, MIGRATE_ASYNC, MR_DEMOTION,\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-15-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about services that intercept write faults needing PTEs to stay read-only, and responded by adding NP_OPS_PROTECT_WRITE and folio_managed_wrprotect() to prevent mprotect from silently upgrading the PTE. The author also suppressed PTE write-upgrade in change_pte_range() and change_huge_pmd(), and dispatched to the node's ops->handle_fault callback when set, allowing the service to handle write faults with promotion or other custom logic.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Services that intercept write faults (e.g., for promotion tracking)\nneed PTEs to stay read-only. This requires preventing mprotect\nfrom silently upgrade the PTE, bypassing the service's handle_fault\ncallback.\n\nAdd NP_OPS_PROTECT_WRITE and folio_managed_wrprotect().\n\nIn change_pte_range() and change_huge_pmd(), suppress PTE write-upgrade\nwhen MM_CP_TRY_CHANGE_WRITABLE is sees the folio is write-protected.\n\nIn handle_pte_fault() and do_huge_pmd_wp_page(), dispatch to the node's\nops->handle_fault callback when set, allowing the service to handle write\nfaults with promotion or other custom logic.\n\nNP_OPS_MEMPOLICY is incompatible with NP_OPS_PROTECT_WRITE to avoid the\nfootgun of binding a writable VMA to a write-protected node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++\n include/linux/node_private.h | 22 ++++++++\n mm/huge_memory.c             | 17 ++++++-\n mm/internal.h                | 99 ++++++++++++++++++++++++++++++++++++\n mm/memory.c                  | 15 ++++++\n mm/migrate.c                 | 14 +----\n mm/mprotect.c                |  4 +-\n 7 files changed, 159 insertions(+), 16 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex c08b5a948779..a4955b9b5b93 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -957,6 +957,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    (ops->flags & NP_OPS_PROTECT_WRITE))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e254e36056cd..27d6e5d84e61 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -70,6 +70,24 @@ struct vm_fault;\n  *     PFN-based metadata (compression tables, device page tables, DMA\n  *     mappings, etc.) before any access through the page tables.\n  *\n+ * @handle_fault: Handle fault on folio on this private node.\n+ *   [folio-referenced callback, PTL held on entry]\n+ *\n+ *   Called from handle_pte_fault() (PTE level) or do_huge_pmd_wp_page()\n+ *   (PMD level) after lock acquisition and entry verification.\n+ *   @folio is the faulting folio, @level indicates the page table level.\n+ *\n+ *   For PGTABLE_LEVEL_PTE: vmf->pte is mapped and vmf->ptl is the\n+ *   PTE lock.  Release via pte_unmap_unlock(vmf->pte, vmf->ptl).\n+ *\n+ *   For PGTABLE_LEVEL_PMD: vmf->pte is NULL and vmf->ptl is the\n+ *   PMD lock.  Release via spin_unlock(vmf->ptl).\n+ *\n+ *   The callback MUST release PTL on ALL paths.\n+ *   The caller will NOT touch the page table entry after this returns.\n+ *\n+ *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -81,6 +99,8 @@ struct node_private_ops {\n \t\t\t\t  enum migrate_reason reason,\n \t\t\t\t  unsigned int *nr_succeeded);\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n+\tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t   enum pgtable_level level);\n \tunsigned long flags;\n };\n \n@@ -90,6 +110,8 @@ struct node_private_ops {\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n /* Node participates as a demotion target in memory-tiers */\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n+/* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n+#define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 2ecae494291a..d9ba6593244d 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -2063,12 +2063,14 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tstruct page *page;\n \tunsigned long haddr = vmf->address & HPAGE_PMD_MASK;\n \tpmd_t orig_pmd = vmf->orig_pmd;\n+\tvm_fault_t ret;\n+\n \n \tvmf->ptl = pmd_lockptr(vma->vm_mm, vmf->pmd);\n \tVM_BUG_ON_VMA(!vma->anon_vma, vma);\n \n \tif (is_huge_zero_pmd(orig_pmd)) {\n-\t\tvm_fault_t ret = do_huge_zero_wp_pmd(vmf);\n+\t\tret = do_huge_zero_wp_pmd(vmf);\n \n \t\tif (!(ret & VM_FAULT_FALLBACK))\n \t\t\treturn ret;\n@@ -2088,6 +2090,13 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tfolio = page_folio(page);\n \tVM_BUG_ON_PAGE(!PageHead(page), page);\n \n+\t/* Private-managed write-protect: let the service handle the fault */\n+\tif (unlikely(folio_is_private_managed(folio))) {\n+\t\tif (folio_managed_handle_fault(folio, vmf,\n+\t\t\t\t\t      PGTABLE_LEVEL_PMD, &ret))\n+\t\t\treturn ret;\n+\t}\n+\n \t/* Early check when only holding the PT lock. */\n \tif (PageAnonExclusive(page))\n \t\tgoto reuse;\n@@ -2633,7 +2642,8 @@ int change_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,\n \n \t/* See change_pte_range(). */\n \tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) && !pmd_write(entry) &&\n-\t    can_change_pmd_writable(vma, addr, entry))\n+\t    can_change_pmd_writable(vma, addr, entry) &&\n+\t    !folio_managed_wrprotect(pmd_folio(entry)))\n \t\tentry = pmd_mkwrite(entry, vma);\n \n \tret = HPAGE_PMD_NR;\n@@ -4943,6 +4953,9 @@ void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n \tif (folio_test_dirty(folio) && softleaf_is_migration_dirty(entry))\n \t\tpmde = pmd_mkdirty(pmde);\n \n+\tif (folio_managed_wrprotect(folio))\n+\t\tpmde = pmd_wrprotect(pmde);\n+\n \tif (folio_is_device_private(folio)) {\n \t\tswp_entry_t entry;\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 5950e20d4023..ae4ff86e8dc6 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -11,6 +11,7 @@\n #include <linux/khugepaged.h>\n #include <linux/mm.h>\n #include <linux/mm_inline.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/pagewalk.h>\n #include <linux/rmap.h>\n@@ -18,6 +19,7 @@\n #include <linux/leafops.h>\n #include <linux/swap_cgroup.h>\n #include <linux/tracepoint-defs.h>\n+#include <linux/node_private.h>\n \n /* Internal core VMA manipulation functions. */\n #include \"vma.h\"\n@@ -1449,6 +1451,103 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/*\n+ * folio_managed_handle_fault - Dispatch fault on managed-memory folio\n+ * @folio: the faulting folio (must not be NULL)\n+ * @vmf: the vm_fault descriptor (PTL held: vmf->ptl locked)\n+ * @level: page table level (PGTABLE_LEVEL_PTE or PGTABLE_LEVEL_PMD)\n+ * @ret: output fault result if handled\n+ *\n+ * Called with PTL held.  If a handle_fault callback exists, it is invoked\n+ * with PTL still held.  The callback is responsible for releasing PTL on\n+ * all paths.\n+ *\n+ * Returns true if the service handled the fault (PTL released by callback,\n+ * caller returns *ret).  Returns false if no handler exists (PTL still held,\n+ * caller continues with normal fault handling).\n+ */\n+static inline bool folio_managed_handle_fault(struct folio *folio,\n+\t\t\t\t\t      struct vm_fault *vmf,\n+\t\t\t\t\t      enum pgtable_level level,\n+\t\t\t\t\t      vm_fault_t *ret)\n+{\n+\t/* Zone device pages use swap entries; handled in do_swap_page */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->handle_fault) {\n+\t\t\t*ret = ops->handle_fault(folio, vmf, level);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n+/**\n+ * folio_managed_wrprotect - Should this folio's mappings stay write-protected?\n+ * @folio: the folio to check\n+ *\n+ * Returns true if the folio is on a private node with NP_OPS_PROTECT_WRITE,\n+ * meaning page table entries (PTE or PMD) should not be made writable.\n+ * Write faults are intercepted by the service's handle_fault callback\n+ * to promote the folio to DRAM.\n+ *\n+ * Used by:\n+ *   - change_pte_range() / change_huge_pmd(): prevent mprotect write-upgrade\n+ *   - remove_migration_pte() / remove_migration_pmd(): strip write after migration\n+ *   - do_huge_pmd_wp_page(): dispatch to fault handler instead of reuse\n+ */\n+static inline bool folio_managed_wrprotect(struct folio *folio)\n+{\n+\treturn unlikely(folio_is_private_node(folio) &&\n+\t\t\tfolio_private_flags(folio, NP_OPS_PROTECT_WRITE));\n+}\n+\n+/**\n+ * folio_managed_fixup_migration_pte - Fixup PTE after migration for\n+ *                                     managed memory pages.\n+ * @new: the destination page\n+ * @pte: the PTE being installed (normal PTE built by caller)\n+ * @old_pte: the original PTE (before migration, for swap entry flags)\n+ * @vma: the VMA\n+ *\n+ * For MEMORY_DEVICE_PRIVATE pages: replaces the PTE with a device-private\n+ * swap entry, preserving soft_dirty and uffd_wp from old_pte.\n+ *\n+ * For N_MEMORY_PRIVATE pages with NP_OPS_PROTECT_WRITE: strips the write\n+ * bit so the next write triggers the fault handler for promotion.\n+ *\n+ * For normal pages: returns pte unmodified.\n+ */\n+static inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n+\t\t\t\t\t\t      pte_t pte,\n+\t\t\t\t\t\t      pte_t old_pte,\n+\t\t\t\t\t\t      struct vm_area_struct *vma)\n+{\n+\tif (unlikely(is_device_private_page(new))) {\n+\t\tsoftleaf_t entry;\n+\n+\t\tif (pte_write(pte))\n+\t\t\tentry = make_writable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\telse\n+\t\t\tentry = make_readable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\tpte = softleaf_to_pte(entry);\n+\t\tif (pte_swp_soft_dirty(old_pte))\n+\t\t\tpte = pte_swp_mksoft_dirty(pte);\n+\t\tif (pte_swp_uffd_wp(old_pte))\n+\t\t\tpte = pte_swp_mkuffd_wp(pte);\n+\t} else if (folio_managed_wrprotect(page_folio(new))) {\n+\t\tpte = pte_wrprotect(pte);\n+\t}\n+\treturn pte;\n+}\n+\n /**\n  * folio_managed_migrate_notify - Notify service that a folio changed location\n  * @src: the old folio (about to be freed)\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 2a55edc48a65..0f78988befef 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -6079,6 +6079,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n \t */\n+\tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n+\t\twritable = false;\n+\t\tignore_writable = true;\n+\t}\n \tif (folio && folio_test_large(folio))\n \t\tnuma_rebuild_large_mapping(vmf, vma, folio, pte, ignore_writable,\n \t\t\t\t\t   pte_write_upgrade);\n@@ -6228,6 +6232,7 @@ static void fix_spurious_fault(struct vm_fault *vmf,\n  */\n static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n {\n+\tstruct folio *folio;\n \tpte_t entry;\n \n \tif (unlikely(pmd_none(*vmf->pmd))) {\n@@ -6284,6 +6289,16 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n \t\tupdate_mmu_tlb(vmf->vma, vmf->address, vmf->pte);\n \t\tgoto unlock;\n \t}\n+\n+\tfolio = vm_normal_folio(vmf->vma, vmf->address, entry);\n+\tif (unlikely(folio && folio_is_private_managed(folio))) {\n+\t\tvm_fault_t fault_ret;\n+\n+\t\tif (folio_managed_handle_fault(folio, vmf, PGTABLE_LEVEL_PTE,\n+\t\t\t\t\t       &fault_ret))\n+\t\t\treturn fault_ret;\n+\t}\n+\n \tif (vmf->flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {\n \t\tif (!pte_write(entry))\n \t\t\treturn do_wp_page(vmf);\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex a54d4af04df3..f632e8b03504 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -398,19 +398,7 @@ static bool remove_migration_pte(struct folio *folio,\n \t\tif (folio_test_anon(folio) && !softleaf_is_migration_read(entry))\n \t\t\trmap_flags |= RMAP_EXCLUSIVE;\n \n-\t\tif (unlikely(is_device_private_page(new))) {\n-\t\t\tif (pte_write(pte))\n-\t\t\t\tentry = make_writable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\telse\n-\t\t\t\tentry = make_readable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\tpte = softleaf_to_pte(entry);\n-\t\t\tif (pte_swp_soft_dirty(old_pte))\n-\t\t\t\tpte = pte_swp_mksoft_dirty(pte);\n-\t\t\tif (pte_swp_uffd_wp(old_pte))\n-\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n-\t\t}\n+\t\tpte = folio_managed_fixup_migration_pte(new, pte, old_pte, vma);\n \n #ifdef CONFIG_HUGETLB_PAGE\n \t\tif (folio_test_hugetlb(folio)) {\ndiff --git a/mm/mprotect.c b/mm/mprotect.c\nindex 283889e4f1ce..830be609bc24 100644\n--- a/mm/mprotect.c\n+++ b/mm/mprotect.c\n@@ -30,6 +30,7 @@\n #include <linux/mm_inline.h>\n #include <linux/pgtable.h>\n #include <linux/userfaultfd_k.h>\n+#include <linux/node_private.h>\n #include <uapi/linux/mman.h>\n #include <asm/cacheflush.h>\n #include <asm/mmu_context.h>\n@@ -290,7 +291,8 @@ static long change_pte_range(struct mmu_gather *tlb,\n \t\t\t * COW or special handling is required.\n \t\t\t */\n \t\t\tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) &&\n-\t\t\t     !pte_write(ptent))\n+\t\t\t     !pte_write(ptent) &&\n+\t\t\t     !(folio && folio_managed_wrprotect(folio)))\n \t\t\t\tset_write_prot_commit_flush_ptes(vma, folio, page,\n \t\t\t\taddr, pte, oldpte, ptent, nr_ptes, tlb);\n \t\t\telse\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-16-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about kswapd's reclaim policy on private nodes, explaining that boosted reclaim suppresses may_swap and may_writepage, leading to stranded pages. They proposed adding a reclaim_policy callback to struct node_private_ops and a struct node_reclaim_policy with flags for swap/writepage and managed_watermarks. The author also added zone_reclaim_allowed() to filter private nodes that have opted into reclaim.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "proposed changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services that drive kswapd via watermark_boost need\ncontrol over the reclaim policy.  There are three problems:\n\n1) Boosted reclaim suppresses may_swap and may_writepage.  When\n   demotion is not possible, swap is the only evict path, so kswapd\n   cannot make progress and pages are stranded.\n\n2) __setup_per_zone_wmarks() unconditionally zeros watermark_boost,\n   killing the service's pressure signal.\n\n3) Not all private nodes want reclaim to touch their pages.\n\nAdd a reclaim_policy callback to struct node_private_ops and a\nstruct node_reclaim_policy with:\n\n  - active:             set by the helper when a callback was invoked\n  - may_swap:           allow swap writeback during boosted reclaim\n  - may_writepage:      allow writepage during boosted reclaim\n  - managed_watermarks: service owns watermark_boost lifecycle\n\nWe do not allow disabling swap/writepage, as core MM may have\nexplicitly enabled them on a non-boosted pass.\n\nWe only allow enablign swap/writepage, so that the supression during\na boost can be overridden.  This allows a device to force evictions\neven when the system otherwise would not percieve pressure.\n\nThis is important for a service like compressed RAM, as device capacity\nmay differ from reported capacity, and device may want to relieve real\npressure (poor compression ratio) as opposed to percieved pressure\n(i.e. how many pages are in use).\n\nAdd zone_reclaim_allowed() to filter private nodes that have not\nopted into reclaim.\n\nRegular nodes fall through to cpuset_zone_allowed() unchanged.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 28 ++++++++++++++++++++++++++++\n mm/internal.h                | 36 ++++++++++++++++++++++++++++++++++++\n mm/page_alloc.c              | 11 ++++++++++-\n mm/vmscan.c                  | 25 +++++++++++++++++++++++--\n 4 files changed, 97 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 27d6e5d84e61..34be52383255 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -14,6 +14,24 @@ struct page;\n struct vm_area_struct;\n struct vm_fault;\n \n+/**\n+ * struct node_reclaim_policy - Reclaim policy overrides for private nodes\n+ * @active: set by node_private_reclaim_policy() when a callback was invoked\n+ * @may_swap: allow swap writeback during boosted reclaim\n+ * @may_writepage: allow writepage during boosted reclaim\n+ * @managed_watermarks: service owns watermark_boost lifecycle; kswapd must\n+ *                      not clear it after boosted reclaim\n+ *\n+ * Passed to the reclaim_policy callback so each private node service can\n+ * inject its own reclaim policy before kswapd runs boosted reclaim.\n+ */\n+struct node_reclaim_policy {\n+\tbool active;\n+\tbool may_swap;\n+\tbool may_writepage;\n+\tbool managed_watermarks;\n+};\n+\n /**\n  * struct node_private_ops - Callbacks for private node services\n  *\n@@ -88,6 +106,13 @@ struct vm_fault;\n  *\n  *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n  *\n+ * @reclaim_policy: Configure reclaim policy for boosted reclaim.\n+ *   [called hodling rcu_read_lock, MUST NOT sleep]\n+ *   Called by kswapd before boosted reclaim to let the service override\n+ *   may_swap / may_writepage.  If provided, the service also owns the\n+ *   watermark_boost lifecycle (kswapd will not clear it).\n+ *   If NULL, normal boost policy applies.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -101,6 +126,7 @@ struct node_private_ops {\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n+\tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n \tunsigned long flags;\n };\n \n@@ -112,6 +138,8 @@ struct node_private_ops {\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n /* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n+/* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n+#define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/internal.h b/mm/internal.h\nindex ae4ff86e8dc6..db32cb2d7a29 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1572,6 +1572,42 @@ static inline void folio_managed_migrate_notify(struct folio *src,\n \t\tops->folio_migrate(src, dst);\n }\n \n+/**\n+ * node_private_reclaim_policy - invoke the service's reclaim policy callback\n+ * @nid: NUMA node id\n+ * @policy: reclaim policy struct to fill in\n+ *\n+ * Called by kswapd before boosted reclaim.  Zeroes @policy, then if the\n+ * private node service provides a reclaim_policy callback, invokes it\n+ * and sets policy->active to true.\n+ */\n+#ifdef CONFIG_NUMA\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tstruct node_private *np;\n+\n+\tmemset(policy, 0, sizeof(*policy));\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (np && np->ops && np->ops->reclaim_policy) {\n+\t\tnp->ops->reclaim_policy(nid, policy);\n+\t\tpolicy->active = true;\n+\t}\n+\trcu_read_unlock();\n+}\n+#else\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tmemset(policy, 0, sizeof(*policy));\n+}\n+#endif\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e272dfdc6b00..9692048ab5fb 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -55,6 +55,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/node_private.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -6437,6 +6438,8 @@ static void __setup_per_zone_wmarks(void)\n \tunsigned long lowmem_pages = 0;\n \tstruct zone *zone;\n \tunsigned long flags;\n+\tstruct node_reclaim_policy rp;\n+\tint prev_nid = NUMA_NO_NODE;\n \n \t/* Calculate total number of !ZONE_HIGHMEM and !ZONE_MOVABLE pages */\n \tfor_each_zone(zone) {\n@@ -6446,6 +6449,7 @@ static void __setup_per_zone_wmarks(void)\n \n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n+\t\tint nid = zone_to_nid(zone);\n \n \t\tspin_lock_irqsave(&zone->lock, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n@@ -6482,7 +6486,12 @@ static void __setup_per_zone_wmarks(void)\n \t\t\t    mult_frac(zone_managed_pages(zone),\n \t\t\t\t      watermark_scale_factor, 10000));\n \n-\t\tzone->watermark_boost = 0;\n+\t\tif (nid != prev_nid) {\n+\t\t\tnode_private_reclaim_policy(nid, &rp);\n+\t\t\tprev_nid = nid;\n+\t\t}\n+\t\tif (!rp.managed_watermarks)\n+\t\t\tzone->watermark_boost = 0;\n \t\tzone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_HIGH] = low_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 0f534428ea88..07de666c1276 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -73,6 +73,13 @@\n #define CREATE_TRACE_POINTS\n #include <trace/events/vmscan.h>\n \n+static inline bool zone_reclaim_allowed(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn zone_private_flags(zone, NP_OPS_RECLAIM);\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n struct scan_control {\n \t/* How many pages shrink_list() should reclaim */\n \tunsigned long nr_to_reclaim;\n@@ -6274,7 +6281,7 @@ static void shrink_zones(struct zonelist *zonelist, struct scan_control *sc)\n \t\t * to global LRU.\n \t\t */\n \t\tif (!cgroup_reclaim(sc)) {\n-\t\t\tif (!cpuset_zone_allowed(zone,\n+\t\t\tif (!zone_reclaim_allowed(zone,\n \t\t\t\t\t\t GFP_KERNEL | __GFP_HARDWALL))\n \t\t\t\tcontinue;\n \n@@ -6992,6 +6999,7 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \tunsigned long zone_boosts[MAX_NR_ZONES] = { 0, };\n \tbool boosted;\n \tstruct zone *zone;\n+\tstruct node_reclaim_policy policy;\n \tstruct scan_control sc = {\n \t\t.gfp_mask = GFP_KERNEL,\n \t\t.order = order,\n@@ -7016,6 +7024,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t}\n \tboosted = nr_boost_reclaim;\n \n+\t/* Query/cache private node reclaim policy once per balance() */\n+\tnode_private_reclaim_policy(pgdat->node_id, &policy);\n+\n restart:\n \tset_reclaim_active(pgdat, highest_zoneidx);\n \tsc.priority = DEF_PRIORITY;\n@@ -7083,6 +7094,12 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\tsc.may_writepage = !laptop_mode && !nr_boost_reclaim;\n \t\tsc.may_swap = !nr_boost_reclaim;\n \n+\t\t/* Private nodes may enable swap/writepage when using boost */\n+\t\tif (policy.active) {\n+\t\t\tsc.may_swap |= policy.may_swap;\n+\t\t\tsc.may_writepage |= policy.may_writepage;\n+\t\t}\n+\n \t\t/*\n \t\t * Do some background aging, to give pages a chance to be\n \t\t * referenced before reclaiming. All pages are rotated\n@@ -7176,6 +7193,10 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\t\tif (!zone_boosts[i])\n \t\t\t\tcontinue;\n \n+\t\t\t/* Some private nodes may own the\\ boost lifecycle */\n+\t\t\tif (policy.managed_watermarks)\n+\t\t\t\tcontinue;\n+\n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n \t\t\tspin_lock_irqsave(&zone->lock, flags);\n@@ -7406,7 +7427,7 @@ void wakeup_kswapd(struct zone *zone, gfp_t gfp_flags, int order,\n \tif (!managed_zone(zone))\n \t\treturn;\n \n-\tif (!cpuset_zone_allowed(zone, gfp_flags))\n+\tif (!zone_reclaim_allowed(zone, gfp_flags))\n \t\treturn;\n \n \tpgdat = zone->zone_pgdat;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-17-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed the concern that the OOM killer may select an undeserving victim if it doesn't know whether killing a task can actually free memory on a private node. The author introduced NP_OPS_OOM_ELIGIBLE and helpers node_oom_eligible() and zone_oom_eligible() to check if a private node is reclaim-eligible, and updated constrained_alloc() to use these checks.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The OOM killer must know whether killing a task can actually free\nmemory such that pressure is reduced.\n\nA private node only contributes to relieving pressure if it participates\nin both reclaim and demotion. Without this check, the check, the OOM\nkiller may select an undeserving victim.\n\nIntroduce NP_OPS_OOM_ELIGIBLE and helpers node_oom_eligible() and\nzone_oom_eligible().\n\nReplace cpuset_mems_allowed_intersects() in oom_cpuset_eligible()\nwith oom_mems_intersect() that iterates N_MEMORY nodes and skips\nineligible private nodes.\n\nUpdate constrained_alloc() to use zone_oom_eligible() for constraint\ndetection and node_oom_eligible() to exclude ineligible nodes from\ntotalpages accounting.\n\nRemove cpuset_mems_allowed_intersects() as it has no remaining callers.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cpuset.h       |  9 -------\n include/linux/node_private.h |  3 +++\n kernel/cgroup/cpuset.c       | 17 ------------\n mm/oom_kill.c                | 52 ++++++++++++++++++++++++++++++++----\n 4 files changed, 50 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/cpuset.h b/include/linux/cpuset.h\nindex 7b2f3f6b68a9..53ccfb00b277 100644\n--- a/include/linux/cpuset.h\n+++ b/include/linux/cpuset.h\n@@ -97,9 +97,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-extern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t  const struct task_struct *tsk2);\n-\n #ifdef CONFIG_CPUSETS_V1\n #define cpuset_memory_pressure_bump() \t\t\t\t\\\n \tdo {\t\t\t\t\t\t\t\\\n@@ -241,12 +238,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-static inline int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t\t const struct task_struct *tsk2)\n-{\n-\treturn 1;\n-}\n-\n static inline void cpuset_memory_pressure_bump(void) {}\n \n static inline void cpuset_task_status_allowed(struct seq_file *m,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34be52383255..34d862f09e24 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -141,6 +141,9 @@ struct node_private_ops {\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n+/* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n+#define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 1a597f0c7c6c..29789d544fd5 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -4530,23 +4530,6 @@ int cpuset_mem_spread_node(void)\n \treturn cpuset_spread_node(&current->cpuset_mem_spread_rotor);\n }\n \n-/**\n- * cpuset_mems_allowed_intersects - Does @tsk1's mems_allowed intersect @tsk2's?\n- * @tsk1: pointer to task_struct of some task.\n- * @tsk2: pointer to task_struct of some other task.\n- *\n- * Description: Return true if @tsk1's mems_allowed intersects the\n- * mems_allowed of @tsk2.  Used by the OOM killer to determine if\n- * one of the task's memory usage might impact the memory available\n- * to the other.\n- **/\n-\n-int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t   const struct task_struct *tsk2)\n-{\n-\treturn nodes_intersects(tsk1->mems_allowed, tsk2->mems_allowed);\n-}\n-\n /**\n  * cpuset_print_current_mems_allowed - prints current's cpuset and mems_allowed\n  *\ndiff --git a/mm/oom_kill.c b/mm/oom_kill.c\nindex 5eb11fbba704..cd0d65ccd1e8 100644\n--- a/mm/oom_kill.c\n+++ b/mm/oom_kill.c\n@@ -74,7 +74,45 @@ static inline bool is_memcg_oom(struct oom_control *oc)\n \treturn oc->memcg != NULL;\n }\n \n+/* Private nodes are only eligible if they support both reclaim and demotion */\n+static inline bool node_oom_eligible(int nid)\n+{\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn (node_private_flags(nid) & NP_OPS_OOM_ELIGIBLE) ==\n+\t\tNP_OPS_OOM_ELIGIBLE;\n+}\n+\n+static inline bool zone_oom_eligible(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (!node_oom_eligible(zone_to_nid(zone)))\n+\t\treturn false;\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n #ifdef CONFIG_NUMA\n+/*\n+ * Killing a task can only relieve system pressure if freed memory can be\n+ * demoted there and reclaim can operate on the node's pages, so we\n+ * omit private nodes that aren't eligible.\n+ */\n+static bool oom_mems_intersect(const struct task_struct *tsk1,\n+\t\t\t       const struct task_struct *tsk2)\n+{\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (!node_isset(nid, tsk1->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_isset(nid, tsk2->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_oom_eligible(nid))\n+\t\t\tcontinue;\n+\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n /**\n  * oom_cpuset_eligible() - check task eligibility for kill\n  * @start: task struct of which task to consider\n@@ -107,9 +145,10 @@ static bool oom_cpuset_eligible(struct task_struct *start,\n \t\t} else {\n \t\t\t/*\n \t\t\t * This is not a mempolicy constrained oom, so only\n-\t\t\t * check the mems of tsk's cpuset.\n+\t\t\t * check the mems of tsk's cpuset, excluding private\n+\t\t\t * nodes that do not participate in kernel reclaim.\n \t\t\t */\n-\t\t\tret = cpuset_mems_allowed_intersects(current, tsk);\n+\t\t\tret = oom_mems_intersect(current, tsk);\n \t\t}\n \t\tif (ret)\n \t\t\tbreak;\n@@ -291,16 +330,19 @@ static enum oom_constraint constrained_alloc(struct oom_control *oc)\n \t\treturn CONSTRAINT_MEMORY_POLICY;\n \t}\n \n-\t/* Check this allocation failure is caused by cpuset's wall function */\n+\t/* Check this allocation failure is caused by cpuset or private node constraints */\n \tfor_each_zone_zonelist_nodemask(zone, z, oc->zonelist,\n \t\t\thighest_zoneidx, oc->nodemask)\n-\t\tif (!cpuset_zone_allowed(zone, oc->gfp_mask))\n+\t\tif (!zone_oom_eligible(zone, oc->gfp_mask))\n \t\t\tcpuset_limited = true;\n \n \tif (cpuset_limited) {\n \t\toc->totalpages = total_swap_pages;\n-\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed)\n+\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed) {\n+\t\t\tif (!node_oom_eligible(nid))\n+\t\t\t\tcontinue;\n \t\t\toc->totalpages += node_present_pages(nid);\n+\t\t}\n \t\treturn CONSTRAINT_CPUSET;\n \t}\n \treturn CONSTRAINT_NONE;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-18-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about NUMA balancing faults on private nodes by introducing an opt-in method (NP_OPS_NUMA_BALANCING) and adding a helper function to filter for private nodes that have opted in. The author also added code to enforce write-protection if a folio is still on its node after a failed or skipped migration.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Not all private nodes may wish to engage in NUMA balancing faults.\n\nAdd the NP_OPS_NUMA_BALANCING flag (BIT(5)) as an opt-in method.\n\nIntroduce folio_managed_allows_numa() helper:\n   ZONE_DEVICE folios always return false (never NUMA-scanned)\n   NP_OPS_NUMA_BALANCING filters for private nodes\n\nIn do_numa_page(), if a private-node folio with NP_OPS_PROTECT_WRITE\nis still on its node after a failed/skipped migration, enforce\nwrite-protection so the next write triggers handle_fault.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h | 16 ++++++++++++++++\n mm/memory.c                  | 11 +++++++++++\n mm/mempolicy.c               |  5 ++++-\n 4 files changed, 35 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex a4955b9b5b93..88aaac45e814 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -961,6 +961,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (ops->flags & NP_OPS_PROTECT_WRITE))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_NUMA_BALANCING) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34d862f09e24..5ac60db1f044 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -140,6 +140,8 @@ struct node_private_ops {\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n+/* Allow NUMA balancing to scan and migrate folios on this node */\n+#define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n@@ -263,6 +265,15 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n }\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\tif (!folio_is_private_managed(folio))\n+\t\treturn true;\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\treturn folio_private_flags(folio, NP_OPS_NUMA_BALANCING);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \tif (folio_is_zone_device(folio))\n@@ -443,6 +454,11 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\treturn !folio_is_zone_device(folio);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \treturn -ENOENT;\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 0f78988befef..88a581baae40 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -78,6 +78,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/pgalloc.h>\n #include <linux/uaccess.h>\n+#include <linux/node_private.h>\n \n #include <trace/events/kmem.h>\n \n@@ -6041,6 +6042,12 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \tif (!folio || folio_is_zone_device(folio))\n \t\tgoto out_map;\n \n+\t/*\n+\t * We do not need to check private-node folios here because the private\n+\t * memory service either never opted in to NUMA balancing, or it did\n+\t * and we need to restore private PTE controls on the failure path.\n+\t */\n+\n \tnid = folio_nid(folio);\n \tnr_pages = folio_nr_pages(folio);\n \n@@ -6078,6 +6085,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t/*\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n+\t *\n+\t * If the folio is still on a private node with NP_OPS_PROTECT_WRITE,\n+\t * enforce write-protection so the next write triggers handle_fault.\n+\t * This covers migration-failed and migration-skipped paths.\n \t */\n \tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n \t\twritable = false;\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 8ac014950e88..8a3a9916ab59 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -861,7 +861,10 @@ bool folio_can_map_prot_numa(struct folio *folio, struct vm_area_struct *vma,\n {\n \tint nid;\n \n-\tif (!folio || folio_is_zone_device(folio) || folio_test_ksm(folio))\n+\tif (!folio || folio_test_ksm(folio))\n+\t\treturn false;\n+\n+\tif (unlikely(!folio_managed_allows_numa(folio)))\n \t\treturn false;\n \n \t/* Also skip shared copy-on-write folios */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-19-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about compaction on private nodes, agreeing that it should not be allowed unless the service explicitly opts in and adding checks to prevent direct compaction on these zones. The author also added a folio_migrate callback to update PFN-based metadata before faults are unblocked.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "added new code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node zones should not be compacted unless the service explicitly\nopts in - as compaction requires migration and services may have\nPFN-based metadata that needs updating.\n\nAdd a folio_migrate callback which fires from migrate_folio_move() for\neach relocated folio before faults are unblocked.\n\nAdd zone_supports_compaction() which returns true for normal zones and\nchecks NP_OPS_COMPACTION for N_MEMORY_PRIVATE zones.\n\nFilter three direct compaction zone loops:\n  - compaction_zonelist_suitable() (reclaimer eligibility)\n  - try_to_compact_pages()         (direct compaction)\n  - compact_node()                 (proactive/manual compaction)\n\nkcompactd paths are intentionally unfiltered -- the service is\nresponsible for starting kcompactd on its node.\n\nNP_OPS_COMPACTION requires NP_OPS_MIGRATION.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h |  2 ++\n mm/compaction.c              | 26 ++++++++++++++++++++++++++\n 3 files changed, 32 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 88aaac45e814..da523aca18fa 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -965,6 +965,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_COMPACTION) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 5ac60db1f044..fe0336773ddb 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -142,6 +142,8 @@ struct node_private_ops {\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n /* Allow NUMA balancing to scan and migrate folios on this node */\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n+/* Allow compaction to run on the node.  Service must start kcompactd. */\n+#define NP_OPS_COMPACTION\t\tBIT(6)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 6a65145b03d8..d8532b957ec6 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,9 +24,26 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/node_private.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n+\n+/*\n+ * Private node zones require NP_OPS_COMPACTION to opt in.  Normal zones\n+ * always support compaction.\n+ */\n+static inline bool zone_supports_compaction(struct zone *zone)\n+{\n+#ifdef CONFIG_NUMA\n+\tif (!node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn zone_private_flags(zone, NP_OPS_COMPACTION);\n+#else\n+\treturn true;\n+#endif\n+}\n+\n /*\n  * Fragmentation score check interval for proactive compaction purposes.\n  */\n@@ -2443,6 +2460,9 @@ bool compaction_zonelist_suitable(struct alloc_context *ac, int order,\n \t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tunsigned long available;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\t/*\n \t\t * Do not consider all the reclaimable memory because we do not\n \t\t * want to trash just for a single high order allocation which\n@@ -2832,6 +2852,9 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\n \t\t\trc = max_t(enum compact_result, COMPACT_DEFERRED, rc);\n@@ -2906,6 +2929,9 @@ static int compact_node(pg_data_t *pgdat, bool proactive)\n \t\tif (!populated_zone(zone))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (fatal_signal_pending(current))\n \t\t\treturn -EINTR;\n \n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-20-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about longterm pinning of private node folios, explaining that it should not be enabled by default due to potential loss of control for the service. They added a new flag NP_OPS_LONGTERM_PIN and modified the folio_is_longterm_pinnable() function in mm.h to check for this flag before allowing longterm pinning.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not be longterm-pinnable by default.\nA pinned folio is frozen in place, no migration, compaction, or\nreclaim, so the service loses control for the duration of the pin.\n\nSome services may depend on hot-unplugability and must disallow\nlongterm pinning.  Others (accelerators with shared CPU-device state)\nneed pinning to work.\n\nAdd NP_OPS_LONGTERM_PIN flag for services to opt in with. Hook into\nfolio_is_longterm_pinnable() in mm.h, which all GUP callers\nout-of-line helper, node_private_allows_longterm_pin(),  called\nonly for N_MEMORY_PRIVATE nodes.\n\nWithout the flag: folio_is_longterm_pinnable() returns false, migration\nfails (no __GFP_PRIVATE in GFP mask) and pin_user_pages(FOLL_LONGTERM)\nreturns -ENOMEM.\n\nWith the flag: pin succeeds and the folio stays on the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 15 +++++++++++++++\n include/linux/mm.h           | 22 ++++++++++++++++++++++\n include/linux/node_private.h |  2 ++\n 3 files changed, 39 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex da523aca18fa..5d2487fd54f4 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -866,6 +866,21 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n static DEFINE_MUTEX(node_private_lock);\n static bool node_private_initialized;\n \n+/**\n+ * node_private_allows_longterm_pin - Check if a private node allows longterm pinning\n+ * @nid: Node identifier\n+ *\n+ * Out-of-line helper for folio_is_longterm_pinnable() since mm.h cannot\n+ * include node_private.h (circular dependency).\n+ *\n+ * Returns true if the node has NP_OPS_LONGTERM_PIN set.\n+ */\n+bool node_private_allows_longterm_pin(int nid)\n+{\n+\treturn node_private_has_flag(nid, NP_OPS_LONGTERM_PIN);\n+}\n+EXPORT_SYMBOL_GPL(node_private_allows_longterm_pin);\n+\n /**\n  * node_private_register - Register a private node\n  * @nid: Node identifier\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex fb1819ad42c3..9088fd08aeb9 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -2192,6 +2192,13 @@ static inline bool is_zero_folio(const struct folio *folio)\n \n /* MIGRATE_CMA and ZONE_MOVABLE do not allow pin folios */\n #ifdef CONFIG_MIGRATION\n+\n+#ifdef CONFIG_NUMA\n+bool node_private_allows_longterm_pin(int nid);\n+#else\n+static inline bool node_private_allows_longterm_pin(int nid) { return false; }\n+#endif\n+\n static inline bool folio_is_longterm_pinnable(struct folio *folio)\n {\n #ifdef CONFIG_CMA\n@@ -2215,6 +2222,21 @@ static inline bool folio_is_longterm_pinnable(struct folio *folio)\n \tif (folio_is_fsdax(folio))\n \t\treturn false;\n \n+\t/*\n+\t * Private node folios are not longterm pinnable by default.\n+\t * Services that support pinning opt in via NP_OPS_LONGTERM_PIN.\n+\t * node_private_allows_longterm_pin() is out-of-line because\n+\t * node_private.h includes mm.h (circular dependency).\n+\t *\n+\t * Guarded by CONFIG_NUMA because on !CONFIG_NUMA the single-node\n+\t * node_state() stub returns true for node 0, which would make\n+\t * all folios non-pinnable via the false-returning stub.\n+\t */\n+#ifdef CONFIG_NUMA\n+\tif (node_state(folio_nid(folio), N_MEMORY_PRIVATE))\n+\t\treturn node_private_allows_longterm_pin(folio_nid(folio));\n+#endif\n+\n \t/* Otherwise, non-movable zone folios can be pinned. */\n \treturn !folio_is_zone_movable(folio);\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex fe0336773ddb..7a7438fb9eda 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -144,6 +144,8 @@ struct node_private_ops {\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n /* Allow compaction to run on the node.  Service must start kcompactd. */\n #define NP_OPS_COMPACTION\t\tBIT(6)\n+/* Allow longterm DMA pinning (RDMA, VFIO, etc.) of folios on this node */\n+#define NP_OPS_LONGTERM_PIN\t\tBIT(7)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-21-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about handling hardware errors on private nodes by adding a notification callback to struct node_private_ops. The callback is used to notify services managing N_MEMORY_PRIVATE nodes when a page on their node experiences a hardware error, allowing them to clean up before the kernel proceeds with standard hwpoison handling.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a void memory_failure notification callback to struct\nnode_private_ops so services managing N_MEMORY_PRIVATE nodes notified\nwhen a page on their node experiences a hardware error.\n\nThe callback is notification only -- the kernel always proceeds with\nstandard hwpoison handling for online pages.\n\nThe notification hook fires after TestSetPageHWPoison succeeds and\nbefore get_hwpoison_page giving the service a chance to clean up.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 16 ++++++++++++++++\n mm/memory-failure.c          | 15 +++++++++++++++\n 3 files changed, 37 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7a7438fb9eda..d2669f68ac20 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -113,6 +113,10 @@ struct node_reclaim_policy {\n  *   watermark_boost lifecycle (kswapd will not clear it).\n  *   If NULL, normal boost policy applies.\n  *\n+ * @memory_failure: Notification of hardware error on a page on this node.\n+ *   [folio-referenced callback]\n+ *   Notification only, kernel always handles the failure.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -127,6 +131,8 @@ struct node_private_ops {\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n \tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n+\tvoid (*memory_failure)(struct folio *folio, unsigned long pfn,\n+\t\t\t       int mf_flags);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex db32cb2d7a29..64467ca774f1 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1608,6 +1608,22 @@ static inline void node_private_reclaim_policy(int nid,\n }\n #endif\n \n+static inline void folio_managed_memory_failure(struct folio *folio,\n+\t\t\t\t\t\tunsigned long pfn,\n+\t\t\t\t\t\tint mf_flags)\n+{\n+\t/* Zone device pages handle memory failure via dev_pagemap_ops */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn;\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->memory_failure)\n+\t\t\tops->memory_failure(folio, pfn, mf_flags);\n+\t}\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/memory-failure.c b/mm/memory-failure.c\nindex c80c2907da33..79c91d44ec1e 100644\n--- a/mm/memory-failure.c\n+++ b/mm/memory-failure.c\n@@ -2379,6 +2379,15 @@ int memory_failure(unsigned long pfn, int flags)\n \t\tgoto unlock_mutex;\n \t}\n \n+\t/*\n+\t * Notify private-node services about the hardware error so they\n+\t * can update internal tracking (e.g., CXL poison lists, stop\n+\t * demoting to failing DIMMs).  This is notification only -- the\n+\t * kernel proceeds with standard hwpoison handling regardless.\n+\t */\n+\tif (unlikely(page_is_private_managed(p)))\n+\t\tfolio_managed_memory_failure(page_folio(p), pfn, flags);\n+\n \t/*\n \t * We need/can do nothing about count=0 pages.\n \t * 1) it's a free page, and therefore in safe hand:\n@@ -2825,6 +2834,12 @@ static int soft_offline_in_use_page(struct page *page)\n \t\treturn 0;\n \t}\n \n+\tif (!folio_managed_allows_migrate(folio)) {\n+\t\tpr_info(\"%#lx: cannot migrate private node folio\\n\", pfn);\n+\t\tfolio_put(folio);\n+\t\treturn -EBUSY;\n+\t}\n+\n \tisolated = isolate_folio_to_list(folio, &pagelist);\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-22-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the ordering of operations in hotplugging memory as N_MEMORY_PRIVATE, explaining that their new function combines node_private_region_register() and __add_memory_driver_managed() to ensure proper ordering: registering the private region first, then hotplugging the memory. The author also added checks for migration support and online status when removing private memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "addressed_concern",
                "provided_explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a new function for drivers to hotplug memory as N_MEMORY_PRIVATE.\n\nThis function combines node_private_region_register() with\n__add_memory_driver_managed() to ensure proper ordering:\n\n1. Register the private region first (sets private node context)\n2. Then hotplug the memory (sets N_MEMORY_PRIVATE)\n3. On failure, unregister the private region to avoid leaving the\n   node in an inconsistent state.\n\nWhen the last of memory is removed, hotplug also removes the private\nnode context. If migration is not supported and the node is still\nonline, fire a warning (likely bug in the driver).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory_hotplug.h |  11 +++\n include/linux/mmzone.h         |  12 ++++\n mm/memory_hotplug.c            | 122 ++++++++++++++++++++++++++++++---\n 3 files changed, 135 insertions(+), 10 deletions(-)\n\ndiff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h\nindex 1f19f08552ea..e5abade9450a 100644\n--- a/include/linux/memory_hotplug.h\n+++ b/include/linux/memory_hotplug.h\n@@ -293,6 +293,7 @@ extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n extern int remove_memory(u64 start, u64 size);\n extern void __remove_memory(u64 start, u64 size);\n extern int offline_and_remove_memory(u64 start, u64 size);\n+extern int offline_and_remove_private_memory(int nid, u64 start, u64 size);\n \n #else\n static inline void try_offline_node(int nid) {}\n@@ -309,6 +310,12 @@ static inline int remove_memory(u64 start, u64 size)\n }\n \n static inline void __remove_memory(u64 start, u64 size) {}\n+\n+static inline int offline_and_remove_private_memory(int nid, u64 start,\n+\t\t\t\t\t\t    u64 size)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n@@ -326,6 +333,10 @@ int __add_memory_driver_managed(int nid, u64 start, u64 size,\n extern int add_memory_driver_managed(int nid, u64 start, u64 size,\n \t\t\t\t     const char *resource_name,\n \t\t\t\t     mhp_t mhp_flags);\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np);\n extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,\n \t\t\t\t   unsigned long nr_pages,\n \t\t\t\t   struct vmem_altmap *altmap, int migratetype,\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 992eb1c5a2c6..cc532b67ad3f 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1524,6 +1524,18 @@ typedef struct pglist_data {\n #endif\n } pg_data_t;\n \n+#ifdef CONFIG_NUMA\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn pgdat->private;\n+}\n+#else\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn false;\n+}\n+#endif\n+\n #define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n #define node_spanned_pages(nid)\t(NODE_DATA(nid)->node_spanned_pages)\n \ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex d2dc527bd5b0..9d72f44a30dc 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1173,8 +1174,7 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tmove_pfn_range_to_zone(zone, pfn, nr_pages, NULL, MIGRATE_MOVABLE,\n \t\t\t       true);\n \n-\tif (!node_state(nid, N_MEMORY)) {\n-\t\t/* Adding memory to the node for the first time */\n+\tif (!node_state(nid, N_MEMORY) && !node_state(nid, N_MEMORY_PRIVATE)) {\n \t\tnode_arg.nid = nid;\n \t\tret = node_notify(NODE_ADDING_FIRST_MEMORY, &node_arg);\n \t\tret = notifier_to_errno(ret);\n@@ -1208,8 +1208,12 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tonline_pages_range(pfn, nr_pages);\n \tadjust_present_page_count(pfn_to_page(pfn), group, nr_pages);\n \n-\tif (node_arg.nid >= 0)\n-\t\tnode_set_state(nid, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (pgdat_is_private(NODE_DATA(nid)))\n+\t\t\tnode_set_state(nid, N_MEMORY_PRIVATE);\n+\t\telse\n+\t\t\tnode_set_state(nid, N_MEMORY);\n+\t}\n \tif (need_zonelists_rebuild)\n \t\tbuild_all_zonelists(NULL);\n \n@@ -1227,8 +1231,14 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t/* reinitialise watermarks and update pcp limits */\n \tinit_per_zone_wmark_min();\n \n-\tkswapd_run(nid);\n-\tkcompactd_run(nid);\n+\t/*\n+\t * Don't start reclaim/compaction daemons for private nodes.\n+\t * Private node services will decide whether to start these services.\n+\t */\n+\tif (!pgdat_is_private(NODE_DATA(nid))) {\n+\t\tkswapd_run(nid);\n+\t\tkcompactd_run(nid);\n+\t}\n \n \tif (node_arg.nid >= 0)\n \t\t/* First memory added successfully. Notify consumers. */\n@@ -1722,6 +1732,54 @@ int add_memory_driver_managed(int nid, u64 start, u64 size,\n }\n EXPORT_SYMBOL_GPL(add_memory_driver_managed);\n \n+/**\n+ * add_private_memory_driver_managed - add driver-managed N_MEMORY_PRIVATE memory\n+ * @nid: NUMA node ID (or memory group ID when MHP_NID_IS_MGID is set)\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ * @resource_name: \"System RAM ($DRIVER)\" format\n+ * @mhp_flags: Memory hotplug flags\n+ * @online_type: MMOP_* online type\n+ * @np: Driver-owned node_private structure (owner, refcount)\n+ *\n+ * Registers node_private first, then hotplugs the memory.\n+ *\n+ * On failure, unregisters the node_private.\n+ */\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np)\n+{\n+\tstruct memory_group *group;\n+\tint real_nid = nid;\n+\tint rc;\n+\n+\tif (!np)\n+\t\treturn -EINVAL;\n+\n+\tif (mhp_flags & MHP_NID_IS_MGID) {\n+\t\tgroup = memory_group_find_by_id(nid);\n+\t\tif (!group)\n+\t\t\treturn -EINVAL;\n+\t\treal_nid = group->nid;\n+\t}\n+\n+\trc = node_private_register(real_nid, np);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\trc = __add_memory_driver_managed(nid, start, size, resource_name,\n+\t\t\t\t\t mhp_flags, online_type);\n+\tif (rc) {\n+\t\tnode_private_unregister(real_nid);\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(add_private_memory_driver_managed);\n+\n /*\n  * Platforms should define arch_get_mappable_range() that provides\n  * maximum possible addressable physical memory range for which the\n@@ -1872,6 +1930,15 @@ static void do_migrate_range(unsigned long start_pfn, unsigned long end_pfn)\n \t\t\tgoto put_folio;\n \t\t}\n \n+\t\t/* Private nodes w/o migration must ensure folios are offline */\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION)) {\n+\t\t\tWARN_ONCE(1, \"hot-unplug on non-migratable node %d pfn %lx\\n\",\n+\t\t\t\t  folio_nid(folio), pfn);\n+\t\t\tpfn = folio_pfn(folio) + folio_nr_pages(folio) - 1;\n+\t\t\tgoto put_folio;\n+\t\t}\n+\n \t\tif (!isolate_folio_to_list(folio, &source)) {\n \t\t\tif (__ratelimit(&migrate_rs)) {\n \t\t\t\tpr_warn(\"failed to isolate pfn %lx\\n\",\n@@ -2014,8 +2081,8 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \n \t/*\n \t * Check whether the node will have no present pages after we offline\n-\t * 'nr_pages' more. If so, we know that the node will become empty, and\n-\t * so we will clear N_MEMORY for it.\n+\t * 'nr_pages' more. If so, send pre-notification for last memory removal.\n+\t * We will clear N_MEMORY(_PRIVATE) if this is the case.\n \t */\n \tif (nr_pages >= pgdat->node_present_pages) {\n \t\tnode_arg.nid = node;\n@@ -2108,8 +2175,12 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * Make sure to mark the node as memory-less before rebuilding the zone\n \t * list. Otherwise this node would still appear in the fallback lists.\n \t */\n-\tif (node_arg.nid >= 0)\n-\t\tnode_clear_state(node, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (node_state(node, N_MEMORY))\n+\t\t\tnode_clear_state(node, N_MEMORY);\n+\t\telse if (node_state(node, N_MEMORY_PRIVATE))\n+\t\t\tnode_clear_state(node, N_MEMORY_PRIVATE);\n+\t}\n \tif (!populated_zone(zone)) {\n \t\tzone_pcp_reset(zone);\n \t\tbuild_all_zonelists(NULL);\n@@ -2461,4 +2532,35 @@ int offline_and_remove_memory(u64 start, u64 size)\n \treturn rc;\n }\n EXPORT_SYMBOL_GPL(offline_and_remove_memory);\n+\n+/**\n+ * offline_and_remove_private_memory - offline, remove, and unregister private memory\n+ * @nid: NUMA node ID of the private memory\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ *\n+ * Counterpart to add_private_memory_driver_managed().  Offlines and removes\n+ * the memory range, then attempts to unregister the node_private.\n+ *\n+ * offline_and_remove_memory() clears N_MEMORY_PRIVATE when the last block\n+ * is offlined, which allows node_private_unregister() to clear the\n+ * pgdat->node_private pointer.  If other private memory ranges remain on\n+ * the node, node_private_unregister() returns -EBUSY (N_MEMORY_PRIVATE\n+ * is still set) and the node_private remains registered.\n+ *\n+ * Return: 0 on full success (memory removed and node_private unregistered),\n+ *         -EBUSY if memory was removed but node still has other private memory,\n+ *         other negative error code if offline/remove failed.\n+ */\n+int offline_and_remove_private_memory(int nid, u64 start, u64 size)\n+{\n+\tint rc;\n+\n+\trc = offline_and_remove_memory(start, size);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\treturn node_private_unregister(nid);\n+}\n+EXPORT_SYMBOL_GPL(offline_and_remove_private_memory);\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-23-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the interaction between CRAM and the standard kernel LRU, specifically how CRAM manages folios demoted to N_MEMORY_PRIVATE nodes. The author explains that CRAM limits entry into CRAM by demotion to provide devices a way for drivers to close access, allowing the system to stabilize under memory pressure. They also describe how write faults trigger promotion back to regular DRAM via the ops->handle_fault callback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CRAM (Compressed RAM) subsystem that manages folios demoted\nto N_MEMORY_PRIVATE nodes via the standard kernel LRU.\n\nWe limit entry into CRAM by demotion in to provide devices a way for\ndrivers to close access - which allows the system to stabiliz under\nmemory pressure (the device can run out of real memory when compression\nratios drop too far).\n\nWe utilize write-protect to prevent unbounded writes to compressed\nmemory pages, which may cause run-away compression ratio loss without\na reliable way to prevent the degenerate case (cascading poisons).\n\nCRAM provides the bridge between the mm/ private node infrastructure\nand compressed memory hardware.  Folios are aged by kswapd on the\nprivate node and reclaimed to swap when the device signals pressure.\n\nWrite faults trigger promotion back to regular DRAM via the\nops->handle_fault callback.\n\nDevice pressure is communicated via watermark_boost on the private\nnode's zone.\n\nCRAM registers node_private_ops with:\n  - handle_fault:   promotes folio back to DRAM on write\n  - migrate_to:     custom demotion to the CRAM node\n  - folio_migrate:  (no-op)\n  - free_folio:     zeroes pages on free to scrub stale data\n  - reclaim_policy: provides mayswap/writeback/boost overrides\n  - flags: NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n\t   NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE\n           NP_OPS_RECLAIM\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cram.h |  66 ++++++\n mm/Kconfig           |  10 +\n mm/Makefile          |   1 +\n mm/cram.c            | 508 +++++++++++++++++++++++++++++++++++++++++++\n 4 files changed, 585 insertions(+)\n create mode 100644 include/linux/cram.h\n create mode 100644 mm/cram.c\n\ndiff --git a/include/linux/cram.h b/include/linux/cram.h\nnew file mode 100644\nindex 000000000000..a3c10362fd4f\n--- /dev/null\n+++ b/include/linux/cram.h\n@@ -0,0 +1,66 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_CRAM_H\n+#define _LINUX_CRAM_H\n+\n+#include <linux/mm_types.h>\n+\n+struct folio;\n+struct list_head;\n+struct vm_fault;\n+\n+#define CRAM_PRESSURE_MAX\t1000\n+\n+/**\n+ * cram_flush_cb_t - Driver callback invoked when a folio on a private node\n+ *                   is freed (refcount reaches zero).\n+ * @folio: the folio being freed\n+ * @private: opaque driver data passed at registration\n+ *\n+ * Return:\n+ *   0: Flush resolved -- page should return to buddy allocator (e.g., flush\n+ *      record bit was set, meaning this free is from our own flush resolution)\n+ *   1: Page deferred -- driver took a reference, page will be flushed later.\n+ *      Do NOT return to buddy allocator.\n+ *   2: Buffer full -- caller should zero the page and return to buddy.\n+ */\n+typedef int (*cram_flush_cb_t)(struct folio *folio, void *private);\n+\n+#ifdef CONFIG_CRAM\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data);\n+int cram_unregister_private_node(int nid);\n+int cram_unpurge(int nid);\n+void cram_set_pressure(int nid, unsigned int pressure);\n+void cram_clear_pressure(int nid);\n+\n+#else /* !CONFIG_CRAM */\n+\n+static inline int cram_register_private_node(int nid, void *owner,\n+\t\t\t\t\t     cram_flush_cb_t flush_cb,\n+\t\t\t\t\t     void *flush_data)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unregister_private_node(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unpurge(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+}\n+\n+static inline void cram_clear_pressure(int nid)\n+{\n+}\n+\n+#endif /* CONFIG_CRAM */\n+\n+#endif /* _LINUX_CRAM_H */\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex bd0ea5454af8..054462b954d8 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -662,6 +662,16 @@ config MIGRATION\n config DEVICE_MIGRATION\n \tdef_bool MIGRATION && ZONE_DEVICE\n \n+config CRAM\n+\tbool \"Compressed RAM - private node memory management\"\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\tdepends on MEMORY_HOTPLUG\n+\thelp\n+\t  Enables management of N_MEMORY_PRIVATE nodes for compressed RAM\n+\t  and similar use cases. Provides demotion, promotion, and lifecycle\n+\t  management for private memory nodes.\n+\n config ARCH_ENABLE_HUGEPAGE_MIGRATION\n \tbool\n \ndiff --git a/mm/Makefile b/mm/Makefile\nindex 2d0570a16e5b..0e1421512643 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -98,6 +98,7 @@ obj-$(CONFIG_MEMTEST)\t\t+= memtest.o\n obj-$(CONFIG_MIGRATION) += migrate.o\n obj-$(CONFIG_NUMA) += memory-tiers.o\n obj-$(CONFIG_DEVICE_MIGRATION) += migrate_device.o\n+obj-$(CONFIG_CRAM) += cram.o\n obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o\n obj-$(CONFIG_PAGE_COUNTER) += page_counter.o\n obj-$(CONFIG_LIVEUPDATE) += memfd_luo.o\ndiff --git a/mm/cram.c b/mm/cram.c\nnew file mode 100644\nindex 000000000000..6709e61f5b9d\n--- /dev/null\n+++ b/mm/cram.c\n@@ -0,0 +1,508 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * mm/cram.c - Compressed RAM / private node memory management\n+ *\n+ * Copyright 2026 Meta Technologies Inc.\n+ *   Author: Gregory Price <gourry@gourry.net>\n+ *\n+ * Manages folios demoted to N_MEMORY_PRIVATE nodes via the standard kernel\n+ * LRU.  Folios are aged by kswapd on the private node and reclaimed to swap\n+ * (demotion is suppressed for private nodes).  Write faults trigger promotion\n+ * back to regular DRAM via the ops->handle_fault callback.\n+ *\n+ * All reclaim/demotion uses the standard vmscan infrastructure. Device pressure\n+ * is communicated via watermark_boost on the private node's zone.\n+ */\n+\n+#include <linux/atomic.h>\n+#include <linux/cpuset.h>\n+#include <linux/cram.h>\n+#include <linux/errno.h>\n+#include <linux/gfp.h>\n+#include <linux/jiffies.h>\n+#include <linux/highmem.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/list.h>\n+#include <linux/migrate.h>\n+#include <linux/mm.h>\n+#include <linux/huge_mm.h>\n+#include <linux/mmzone.h>\n+#include <linux/mutex.h>\n+#include <linux/nodemask.h>\n+#include <linux/node_private.h>\n+#include <linux/pagemap.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+#include <linux/swap.h>\n+\n+#include \"internal.h\"\n+\n+struct cram_node {\n+\tvoid\t\t*owner;\n+\tbool\t\tpurged;\t\t/* node is being torn down */\n+\tunsigned int\tpressure;\n+\trefcount_t\trefcount;\n+\tcram_flush_cb_t\tflush_cb;\t/* optional driver flush callback */\n+\tvoid\t\t*flush_data;\t/* opaque data for flush_cb */\n+};\n+\n+static struct cram_node *cram_nodes[MAX_NUMNODES];\n+static DEFINE_MUTEX(cram_mutex);\n+\n+static inline bool cram_valid_nid(int nid)\n+{\n+\treturn nid >= 0 && nid < MAX_NUMNODES;\n+}\n+\n+static inline struct cram_node *get_cram_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn NULL;\n+\n+\trcu_read_lock();\n+\tcn = rcu_dereference(cram_nodes[nid]);\n+\tif (cn && !refcount_inc_not_zero(&cn->refcount))\n+\t\tcn = NULL;\n+\trcu_read_unlock();\n+\n+\treturn cn;\n+}\n+\n+static inline void put_cram_node(struct cram_node *cn)\n+{\n+\tif (cn)\n+\t\trefcount_dec(&cn->refcount);\n+}\n+\n+static void cram_zero_folio(struct folio *folio)\n+{\n+\tunsigned int i, nr = folio_nr_pages(folio);\n+\n+\tif (want_init_on_free())\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr; i++)\n+\t\tclear_highpage(folio_page(folio, i));\n+}\n+\n+static bool cram_free_folio_cb(struct folio *folio)\n+{\n+\tint nid = folio_nid(folio);\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\tgoto zero_and_free;\n+\n+\tif (!cn->flush_cb)\n+\t\tgoto zero_and_free_put;\n+\n+\tret = cn->flush_cb(folio, cn->flush_data);\n+\tput_cram_node(cn);\n+\n+\tswitch (ret) {\n+\tcase 0:\n+\t\t/* Flush resolved: return to buddy (already zeroed by device) */\n+\t\treturn false;\n+\tcase 1:\n+\t\t/* Deferred: driver holds a ref, do not free to buddy */\n+\t\treturn true;\n+\tcase 2:\n+\tdefault:\n+\t\t/* Buffer full or unknown: zero locally, return to buddy */\n+\t\tgoto zero_and_free;\n+\t}\n+\n+zero_and_free_put:\n+\tput_cram_node(cn);\n+zero_and_free:\n+\tcram_zero_folio(folio);\n+\treturn false;\n+}\n+\n+static struct folio *alloc_cram_folio(struct folio *src, unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_PRIVATE | __GFP_KSWAPD_RECLAIM |\n+\t\t     __GFP_HIGHMEM | __GFP_MOVABLE |\n+\t\t     __GFP_NOWARN | __GFP_NORETRY;\n+\n+\t/* Stop allocating if backpressure fired mid-batch */\n+\tif (node_private_migration_blocked(nid))\n+\t\treturn NULL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc_node(gfp, order, nid);\n+}\n+\n+static void cram_put_new_folio(struct folio *folio, unsigned long private)\n+{\n+\tcram_zero_folio(folio);\n+\tfolio_put(folio);\n+}\n+\n+/*\n+ * Allocate a DRAM folio for promotion out of a private node.\n+ *\n+ * Unlike alloc_migration_target(), this does NOT strip __GFP_RECLAIM for\n+ * large folios, the generic helper does that because THP allocations are\n+ * opportunistic, but promotion from a private node is mandatory: the page\n+ * MUST move to DRAM or the process cannot make forward progress.\n+ *\n+ * __GFP_RETRY_MAYFAIL tells the allocator to try hard (multiple reclaim\n+ * rounds, wait for writeback) before giving up.\n+ */\n+static struct folio *alloc_cram_promote_folio(struct folio *src,\n+\t\t\t\t\t      unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc(gfp, order, nid, NULL);\n+}\n+\n+static int cram_migrate_to(struct list_head *demote_folios, int to_nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason,\n+\t\t\t   unsigned int *nr_succeeded)\n+{\n+\tstruct cram_node *cn;\n+\tunsigned int nr_success = 0;\n+\tint ret = 0;\n+\n+\tcn = get_cram_node(to_nid);\n+\tif (!cn)\n+\t\treturn -ENODEV;\n+\n+\tif (cn->purged) {\n+\t\tret = -ENODEV;\n+\t\tgoto out;\n+\t}\n+\n+\t/* Block new demotions at maximum pressure */\n+\tif (READ_ONCE(cn->pressure) >= CRAM_PRESSURE_MAX) {\n+\t\tret = -ENOSPC;\n+\t\tgoto out;\n+\t}\n+\n+\tret = migrate_pages(demote_folios, alloc_cram_folio, cram_put_new_folio,\n+\t\t\t    (unsigned long)to_nid, mode, reason,\n+\t\t\t    &nr_success);\n+\n+\t/*\n+\t * migrate_folio_move() calls folio_add_lru() for each migrated\n+\t * folio, but that only adds the folio to a per-CPU batch, \n+\t * PG_lru is not set until the batch is drained.  Drain now so\n+\t * that cram_fault() can isolate these folios immediately.\n+\t *\n+\t * Use lru_add_drain_all() because migrate_pages() may process\n+\t * folios across CPUs, and the local drain might miss batches\n+\t * filled on other CPUs.\n+\t */\n+\tif (nr_success)\n+\t\tlru_add_drain_all();\n+out:\n+\tput_cram_node(cn);\n+\tif (nr_succeeded)\n+\t\t*nr_succeeded = nr_success;\n+\treturn ret;\n+}\n+\n+static void cram_release_ptl(struct vm_fault *vmf, enum pgtable_level level)\n+{\n+\tif (level == PGTABLE_LEVEL_PTE)\n+\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n+\telse\n+\t\tspin_unlock(vmf->ptl);\n+}\n+\n+static vm_fault_t cram_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t     enum pgtable_level level)\n+{\n+\tstruct folio *f, *f2;\n+\tstruct cram_node *cn;\n+\tunsigned int nr_succeeded = 0;\n+\tint nid;\n+\tLIST_HEAD(folios);\n+\n+\tnid = folio_nid(folio);\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn) {\n+\t\tcram_release_ptl(vmf, level);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Isolate from LRU while holding PTL.  This serializes against\n+\t * other CPUs faulting on the same folio: only one CPU can clear\n+\t * PG_lru under the PTL, and it proceeds to migration.  Other\n+\t * CPUs find the folio already isolated and bail out, preventing\n+\t * the refcount pile-up that causes migrate_pages() to fail with\n+\t * -EAGAIN.\n+\t *\n+\t * No explicit folio_get() is needed: the page table entry holds\n+\t * a reference (we still hold PTL), and folio_isolate_lru() takes\n+\t * its own reference.  This matches do_numa_page()'s pattern.\n+\t *\n+\t * PG_lru should already be set: cram_migrate_to() drains per-CPU\n+\t * LRU batches after migration, and the failure path below\n+\t * drains after putback.\n+\t */\n+\tif (!folio_isolate_lru(folio)) {\n+\t\tput_cram_node(cn);\n+\t\tcram_release_ptl(vmf, level);\n+\t\tcond_resched();\n+\t\treturn 0;\n+\t}\n+\n+\t/* Folio isolated, release PTL, proceed to migration */\n+\tcram_release_ptl(vmf, level);\n+\n+\tnode_stat_mod_folio(folio,\n+\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(folio),\n+\t\t\t    folio_nr_pages(folio));\n+\tlist_add(&folio->lru, &folios);\n+\n+\tmigrate_pages(&folios, alloc_cram_promote_folio, NULL,\n+\t\t      (unsigned long)numa_node_id(),\n+\t\t      MIGRATE_SYNC, MR_NUMA_MISPLACED, &nr_succeeded);\n+\n+\t/* Put failed folios back on LRU; retry on next fault */\n+\tlist_for_each_entry_safe(f, f2, &folios, lru) {\n+\t\tlist_del(&f->lru);\n+\t\tnode_stat_mod_folio(f,\n+\t\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(f),\n+\t\t\t\t    -folio_nr_pages(f));\n+\t\tfolio_putback_lru(f);\n+\t}\n+\n+\t/*\n+\t * If migration failed, folio_putback_lru() batched the folio\n+\t * into this CPU's per-CPU LRU cache (PG_lru not yet set).\n+\t * Drain now so the folio is immediately visible on the LRU,\n+\t * the next fault can then isolate it without an IPI storm\n+\t * via lru_add_drain_all().\n+\t *\n+\t * Return VM_FAULT_RETRY after releasing the fault lock so the\n+\t * arch handler retries from scratch.  Without this, returning 0\n+\t * causes a tight livelock: the process immediately re-faults on\n+\t * the same write-protected entry, alloc fails again, and\n+\t * VM_FAULT_OOM eventually leaks out through a stale path.\n+\t * VM_FAULT_RETRY gives the system breathing room to reclaim.\n+\t */\n+\tif (!nr_succeeded) {\n+\t\tlru_add_drain();\n+\t\tcond_resched();\n+\t\tput_cram_node(cn);\n+\t\trelease_fault_lock(vmf);\n+\t\treturn VM_FAULT_RETRY;\n+\t}\n+\n+\tcond_resched();\n+\tput_cram_node(cn);\n+\treturn 0;\n+}\n+\n+static void cram_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static void cram_reclaim_policy(int nid, struct node_reclaim_policy *policy)\n+{\n+\tpolicy->may_swap = true;\n+\tpolicy->may_writepage = true;\n+\tpolicy->managed_watermarks = true;\n+}\n+\n+static vm_fault_t cram_handle_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t    enum pgtable_level level)\n+{\n+\treturn cram_fault(folio, vmf, level);\n+}\n+\n+static const struct node_private_ops cram_ops = {\n+\t.handle_fault\t\t= cram_handle_fault,\n+\t.migrate_to\t\t= cram_migrate_to,\n+\t.folio_migrate\t\t= cram_folio_migrate,\n+\t.free_folio\t\t= cram_free_folio_cb,\n+\t.reclaim_policy\t\t= cram_reclaim_policy,\n+\t.flags\t\t\t= NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n+\t\t\t\t  NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE |\n+\t\t\t\t  NP_OPS_RECLAIM,\n+};\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data)\n+{\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (cn) {\n+\t\tif (cn->owner != owner) {\n+\t\t\tmutex_unlock(&cram_mutex);\n+\t\t\treturn -EBUSY;\n+\t\t}\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn 0;\n+\t}\n+\n+\tcn = kzalloc(sizeof(*cn), GFP_KERNEL);\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENOMEM;\n+\t}\n+\n+\tcn->owner = owner;\n+\tcn->pressure = 0;\n+\tcn->flush_cb = flush_cb;\n+\tcn->flush_data = flush_data;\n+\trefcount_set(&cn->refcount, 1);\n+\n+\tret = node_private_set_ops(nid, &cram_ops);\n+\tif (ret) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\tkfree(cn);\n+\t\treturn ret;\n+\t}\n+\n+\trcu_assign_pointer(cram_nodes[nid], cn);\n+\n+\t/* Start kswapd on the private node for LRU aging and reclaim */\n+\tkswapd_run(nid);\n+\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* Now that ops->migrate_to is set, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_register_private_node);\n+\n+int cram_unregister_private_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tkswapd_stop(nid);\n+\n+\tWARN_ON(node_private_clear_ops(nid, &cram_ops));\n+\trcu_assign_pointer(cram_nodes[nid], NULL);\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* ops->migrate_to cleared, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\n+\tsynchronize_rcu();\n+\twhile (!refcount_dec_if_one(&cn->refcount))\n+\t\tcond_resched();\n+\tkfree(cn);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unregister_private_node);\n+\n+int cram_unpurge(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tcn->purged = false;\n+\n+\tmutex_unlock(&cram_mutex);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unpurge);\n+\n+void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+\tstruct cram_node *cn;\n+\tstruct node_private *np;\n+\tstruct zone *zone;\n+\tunsigned long managed, boost;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\treturn;\n+\n+\tif (pressure > CRAM_PRESSURE_MAX)\n+\t\tpressure = CRAM_PRESSURE_MAX;\n+\n+\tWRITE_ONCE(cn->pressure, pressure);\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\t/* Block demotions only at maximum pressure */\n+\tif (np)\n+\t\tWRITE_ONCE(np->migration_blocked,\n+\t\t\t   pressure >= CRAM_PRESSURE_MAX);\n+\trcu_read_unlock();\n+\n+\tzone = NULL;\n+\tfor (int i = 0; i < MAX_NR_ZONES; i++) {\n+\t\tstruct zone *z = &NODE_DATA(nid)->node_zones[i];\n+\n+\t\tif (zone_managed_pages(z) > 0) {\n+\t\t\tzone = z;\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+\tif (!zone) {\n+\t\tput_cram_node(cn);\n+\t\treturn;\n+\t}\n+\tmanaged = zone_managed_pages(zone);\n+\n+\t/* Boost proportional to pressure. 0:no boost, 1000:full managed */\n+\tboost = (managed * (unsigned long)pressure) / CRAM_PRESSURE_MAX;\n+\tWRITE_ONCE(zone->watermark_boost, boost);\n+\n+\tif (boost) {\n+\t\tset_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);\n+\t\twakeup_kswapd(zone, GFP_KERNEL, 0, ZONE_MOVABLE);\n+\t}\n+\n+\tput_cram_node(cn);\n+}\n+EXPORT_SYMBOL_GPL(cram_set_pressure);\n+\n+void cram_clear_pressure(int nid)\n+{\n+\tcram_set_pressure(nid, 0);\n+}\n+EXPORT_SYMBOL_GPL(cram_clear_pressure);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-24-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the need for direct memory hotplug operations in CXL RAM regions, explained that their patch adds a sysram region to eliminate the intermediate dax_region/dax device layer and directly perform memory hotplug operations. The author confirmed that this feature will be extended to support private memory nodes in the future.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "author confirmed the issue is resolved or agrees with the approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CXL sysram region for direct memory hotplug of CXL RAM regions.\n\nThis region eliminates the intermediate dax_region/dax device layer by\ndirectly performing memory hotplug operations.\n\nKey features:\n- Supports memory tier integration for proper NUMA placement\n- Uses the CXL_SYSRAM_ONLINE_* Kconfig options for default online type\n- Automatically hotplugs memory on probe if online type is configured\n- Will be extended to support private memory nodes in the future\n\nThe driver registers a sysram_regionN device as a child of the CXL\nregion, managing the memory hotplug lifecycle through device add/remove.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/Makefile        |   1 +\n drivers/cxl/core/core.h          |   4 +\n drivers/cxl/core/port.c          |   2 +\n drivers/cxl/core/region_sysram.c | 351 +++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h                |  48 +++++\n 5 files changed, 406 insertions(+)\n create mode 100644 drivers/cxl/core/region_sysram.c\n\ndiff --git a/drivers/cxl/core/Makefile b/drivers/cxl/core/Makefile\nindex d3ec8aea64c5..d7ce52c50810 100644\n--- a/drivers/cxl/core/Makefile\n+++ b/drivers/cxl/core/Makefile\n@@ -18,6 +18,7 @@ cxl_core-$(CONFIG_TRACING) += trace.o\n cxl_core-$(CONFIG_CXL_REGION) += region.o\n cxl_core-$(CONFIG_CXL_REGION) += region_dax.o\n cxl_core-$(CONFIG_CXL_REGION) += region_pmem.o\n+cxl_core-$(CONFIG_CXL_REGION) += region_sysram.o\n cxl_core-$(CONFIG_CXL_MCE) += mce.o\n cxl_core-$(CONFIG_CXL_FEATURES) += features.o\n cxl_core-$(CONFIG_CXL_EDAC_MEM_FEATURES) += edac.o\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 6e1f695fd155..973bbcae43f7 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -35,6 +35,7 @@ extern struct device_attribute dev_attr_delete_region;\n extern struct device_attribute dev_attr_region;\n extern const struct device_type cxl_pmem_region_type;\n extern const struct device_type cxl_dax_region_type;\n+extern const struct device_type cxl_sysram_type;\n extern const struct device_type cxl_region_type;\n \n int cxl_decoder_detach(struct cxl_region *cxlr,\n@@ -46,6 +47,7 @@ int cxl_decoder_detach(struct cxl_region *cxlr,\n #define SET_CXL_REGION_ATTR(x) (&dev_attr_##x.attr),\n #define CXL_PMEM_REGION_TYPE(x) (&cxl_pmem_region_type)\n #define CXL_DAX_REGION_TYPE(x) (&cxl_dax_region_type)\n+#define CXL_SYSRAM_TYPE(x) (&cxl_sysram_type)\n int cxl_region_init(void);\n void cxl_region_exit(void);\n int cxl_get_poison_by_endpoint(struct cxl_port *port);\n@@ -54,6 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\n@@ -88,6 +91,7 @@ static inline void cxl_region_exit(void)\n #define SET_CXL_REGION_ATTR(x)\n #define CXL_PMEM_REGION_TYPE(x) NULL\n #define CXL_DAX_REGION_TYPE(x) NULL\n+#define CXL_SYSRAM_TYPE(x) NULL\n #endif\n \n struct cxl_send_command;\ndiff --git a/drivers/cxl/core/port.c b/drivers/cxl/core/port.c\nindex 5c82e6f32572..d6e82b3c2b64 100644\n--- a/drivers/cxl/core/port.c\n+++ b/drivers/cxl/core/port.c\n@@ -66,6 +66,8 @@ static int cxl_device_id(const struct device *dev)\n \t\treturn CXL_DEVICE_PMEM_REGION;\n \tif (dev->type == CXL_DAX_REGION_TYPE())\n \t\treturn CXL_DEVICE_DAX_REGION;\n+\tif (dev->type == CXL_SYSRAM_TYPE())\n+\t\treturn CXL_DEVICE_SYSRAM;\n \tif (is_cxl_port(dev)) {\n \t\tif (is_cxl_root(to_cxl_port(dev)))\n \t\t\treturn CXL_DEVICE_ROOT;\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nnew file mode 100644\nindex 000000000000..47a415deb352\n--- /dev/null\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -0,0 +1,351 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Sysram Region - Direct memory hotplug for CXL RAM regions\n+ *\n+ * This interface directly performs memory hotplug for CXL RAM regions,\n+ * eliminating the indirection through DAX.\n+ */\n+\n+#include <linux/memory_hotplug.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/memory.h>\n+#include <linux/device.h>\n+#include <linux/slab.h>\n+#include <linux/mm.h>\n+#include <cxlmem.h>\n+#include <cxl.h>\n+#include \"core.h\"\n+\n+static const char *sysram_res_name = \"System RAM (CXL)\";\n+\n+/**\n+ * cxl_region_find_sysram - Find the sysram device associated with a region\n+ * @cxlr: The CXL region\n+ *\n+ * Finds and returns the sysram child device of a CXL region.\n+ * The caller must release the device reference with put_device()\n+ * when done with the returned pointer.\n+ *\n+ * Return: Pointer to cxl_sysram, or NULL if not found\n+ */\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram;\n+\tstruct device *sdev;\n+\tchar sname[32];\n+\n+\tsnprintf(sname, sizeof(sname), \"sysram_region%d\", cxlr->id);\n+\tsdev = device_find_child_by_name(&cxlr->dev, sname);\n+\tif (!sdev)\n+\t\treturn NULL;\n+\n+\tsysram = to_cxl_sysram(sdev);\n+\treturn sysram;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_region_find_sysram, \"CXL\");\n+\n+static int sysram_get_numa_node(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_region_params *p = &cxlr->params;\n+\tint nid;\n+\n+\tnid = phys_to_target_node(p->res->start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(p->res->start);\n+\n+\treturn nid;\n+}\n+\n+static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n+{\n+\tstruct resource *res;\n+\tmhp_t mhp_flags;\n+\tint rc;\n+\n+\tif (sysram->res)\n+\t\treturn -EBUSY;\n+\n+\tres = request_mem_region(sysram->hpa_range.start,\n+\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t sysram->res_name);\n+\tif (!res)\n+\t\treturn -EBUSY;\n+\n+\tsysram->res = res;\n+\n+\t/*\n+\t * Set flags appropriate for System RAM. Leave ..._BUSY clear\n+\t * so that add_memory() can add a child resource.\n+\t */\n+\tres->flags = IORESOURCE_SYSTEM_RAM;\n+\n+\tmhp_flags = MHP_NID_IS_MGID;\n+\n+\t/*\n+\t * Ensure that future kexec'd kernels will not treat\n+\t * this as RAM automatically.\n+\t */\n+\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t online_type);\n+\tif (rc) {\n+\t\tremove_resource(res);\n+\t\tkfree(res);\n+\t\tsysram->res = NULL;\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n+{\n+\tint rc;\n+\n+\tif (!sysram->res)\n+\t\treturn 0;\n+\n+\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t       range_len(&sysram->hpa_range));\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tif (sysram->res) {\n+\t\tremove_resource(sysram->res);\n+\t\tkfree(sysram->res);\n+\t\tsysram->res = NULL;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn sysram_hotplug_remove(sysram);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_offline_and_remove, \"CXL\");\n+\n+static void cxl_sysram_release(struct device *dev)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\n+\tif (sysram->res)\n+\t\tsysram_hotplug_remove(sysram);\n+\n+\tkfree(sysram->res_name);\n+\n+\tif (sysram->mgid >= 0)\n+\t\tmemory_group_unregister(sysram->mgid);\n+\n+\tif (sysram->mtype)\n+\t\tclear_node_memory_type(sysram->numa_node, sysram->mtype);\n+\n+\tkfree(sysram);\n+}\n+\n+static ssize_t hotplug_store(struct device *dev,\n+\t\t\t     struct device_attribute *attr,\n+\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\tint online_type, rc;\n+\n+\tonline_type = mhp_online_type_from_str(buf);\n+\tif (online_type < 0)\n+\t\treturn online_type;\n+\n+\tif (online_type == MMOP_OFFLINE)\n+\t\trc = sysram_hotplug_remove(sysram);\n+\telse\n+\t\trc = sysram_hotplug_add(sysram, online_type);\n+\n+\tif (rc)\n+\t\tdev_warn(dev, \"hotplug %s failed: %d\\n\",\n+\t\t\t online_type == MMOP_OFFLINE ? \"offline\" : \"online\", rc);\n+\n+\treturn rc ? rc : len;\n+}\n+static DEVICE_ATTR_WO(hotplug);\n+\n+static struct attribute *cxl_sysram_attrs[] = {\n+\t&dev_attr_hotplug.attr,\n+\tNULL\n+};\n+\n+static const struct attribute_group cxl_sysram_attribute_group = {\n+\t.attrs = cxl_sysram_attrs,\n+};\n+\n+static const struct attribute_group *cxl_sysram_attribute_groups[] = {\n+\t&cxl_base_attribute_group,\n+\t&cxl_sysram_attribute_group,\n+\tNULL\n+};\n+\n+const struct device_type cxl_sysram_type = {\n+\t.name = \"cxl_sysram\",\n+\t.release = cxl_sysram_release,\n+\t.groups = cxl_sysram_attribute_groups,\n+};\n+\n+static bool is_cxl_sysram(struct device *dev)\n+{\n+\treturn dev->type == &cxl_sysram_type;\n+}\n+\n+struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\tif (dev_WARN_ONCE(dev, !is_cxl_sysram(dev),\n+\t\t\t  \"not a cxl_sysram device\\n\"))\n+\t\treturn NULL;\n+\treturn container_of(dev, struct cxl_sysram, dev);\n+}\n+EXPORT_SYMBOL_NS_GPL(to_cxl_sysram, \"CXL\");\n+\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram)\n+{\n+\treturn &sysram->dev;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_dev, \"CXL\");\n+\n+static struct lock_class_key cxl_sysram_key;\n+\n+static enum mmop cxl_sysram_get_default_online_type(void)\n+{\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_SYSTEM_DEFAULT))\n+\t\treturn mhp_get_default_online_type();\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_MOVABLE))\n+\t\treturn MMOP_ONLINE_MOVABLE;\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_NORMAL))\n+\t\treturn MMOP_ONLINE;\n+\treturn MMOP_OFFLINE;\n+}\n+\n+static struct cxl_sysram *cxl_sysram_alloc(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram __free(kfree) = NULL;\n+\tstruct device *dev;\n+\n+\tsysram = kzalloc(sizeof(*sysram), GFP_KERNEL);\n+\tif (!sysram)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tsysram->online_type = cxl_sysram_get_default_online_type();\n+\tsysram->last_hotplug_cmd = MMOP_OFFLINE;\n+\tsysram->numa_node = -1;\n+\tsysram->mgid = -1;\n+\n+\tdev = &sysram->dev;\n+\tsysram->cxlr = cxlr;\n+\tdevice_initialize(dev);\n+\tlockdep_set_class(&dev->mutex, &cxl_sysram_key);\n+\tdevice_set_pm_not_required(dev);\n+\tdev->parent = &cxlr->dev;\n+\tdev->bus = &cxl_bus_type;\n+\tdev->type = &cxl_sysram_type;\n+\n+\treturn_ptr(sysram);\n+}\n+\n+static void sysram_unregister(void *_sysram)\n+{\n+\tstruct cxl_sysram *sysram = _sysram;\n+\n+\tdevice_unregister(&sysram->dev);\n+}\n+\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+{\n+\tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n+\tstruct memory_dev_type *mtype;\n+\tstruct range hpa_range;\n+\tstruct device *dev;\n+\tint adist = MEMTIER_DEFAULT_LOWTIER_ADISTANCE;\n+\tint numa_node;\n+\tint rc;\n+\n+\trc = cxl_region_get_hpa_range(cxlr, &hpa_range);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\thpa_range = memory_block_align_range(&hpa_range);\n+\tif (hpa_range.start >= hpa_range.end) {\n+\t\tdev_warn(&cxlr->dev, \"region too small after alignment\\n\");\n+\t\treturn -ENOSPC;\n+\t}\n+\n+\tsysram = cxl_sysram_alloc(cxlr);\n+\tif (IS_ERR(sysram))\n+\t\treturn PTR_ERR(sysram);\n+\n+\tsysram->hpa_range = hpa_range;\n+\n+\tsysram->res_name = kasprintf(GFP_KERNEL, \"cxl_sysram%d\", cxlr->id);\n+\tif (!sysram->res_name)\n+\t\treturn -ENOMEM;\n+\n+\t/* Override default online type if caller specified one */\n+\tif (online_type >= 0)\n+\t\tsysram->online_type = online_type;\n+\n+\tdev = &sysram->dev;\n+\n+\trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Setup memory tier before adding device */\n+\tnuma_node = sysram_get_numa_node(cxlr);\n+\tif (numa_node < 0) {\n+\t\tdev_warn(&cxlr->dev, \"rejecting region with invalid node: %d\\n\",\n+\t\t\t numa_node);\n+\t\treturn -EINVAL;\n+\t}\n+\tsysram->numa_node = numa_node;\n+\n+\tmt_calc_adistance(numa_node, &adist);\n+\tmtype = mt_get_memory_type(adist);\n+\tif (IS_ERR(mtype))\n+\t\treturn PTR_ERR(mtype);\n+\tsysram->mtype = mtype;\n+\n+\tinit_node_memory_type(numa_node, mtype);\n+\n+\t/* Register memory group for this region */\n+\trc = memory_group_register_static(numa_node,\n+\t\t\t\t\t  PFN_UP(range_len(&hpa_range)));\n+\tif (rc < 0)\n+\t\treturn rc;\n+\tsysram->mgid = rc;\n+\n+\trc = device_add(dev);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n+\t\tdev_name(dev));\n+\n+\t/*\n+\t * Dynamic capacity regions (DCD) will have memory added later.\n+\t * For static RAM regions, hotplug the entire range now.\n+\t */\n+\tif (cxlr->mode != CXL_PARTMODE_RAM)\n+\t\tgoto out;\n+\n+\t/* If default online_type is a valid online mode, immediately hotplug */\n+\tif (sysram->online_type > MMOP_OFFLINE) {\n+\t\trc = sysram_hotplug_add(sysram, sysram->online_type);\n+\t\tif (rc)\n+\t\t\tdev_warn(dev, \"hotplug failed: %d\\n\", rc);\n+\t\telse\n+\t\t\tsysram->last_hotplug_cmd = sysram->online_type;\n+\t}\n+\n+out:\n+\treturn devm_add_action_or_reset(&cxlr->dev, sysram_unregister,\n+\t\t\t\t\tno_free_ptr(sysram));\n+}\n+EXPORT_SYMBOL_NS_GPL(devm_cxl_add_sysram, \"CXL\");\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex f899f240f229..8e8342fd4fde 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -607,6 +607,34 @@ struct cxl_dax_region {\n \tenum dax_driver_type dax_driver;\n };\n \n+/**\n+ * struct cxl_sysram - CXL SysRAM region for system memory hotplug\n+ * @dev: device for this sysram\n+ * @cxlr: parent cxl_region\n+ * @online_type: Default memory online type for new hotplug ops (MMOP_* value)\n+ * @last_hotplug_cmd: Last hotplug command submitted (MMOP_* value)\n+ * @hpa_range: Host physical address range for the region\n+ * @res_name: Resource name for the memory region\n+ * @res: Memory resource (set when hotplugged)\n+ * @mgid: Memory group id\n+ * @mtype: Memory tier type\n+ * @numa_node: NUMA node for this memory\n+ *\n+ * Device that directly performs memory hotplug for CXL RAM regions.\n+ */\n+struct cxl_sysram {\n+\tstruct device dev;\n+\tstruct cxl_region *cxlr;\n+\tenum mmop online_type;\n+\tint last_hotplug_cmd;\n+\tstruct range hpa_range;\n+\tconst char *res_name;\n+\tstruct resource *res;\n+\tint mgid;\n+\tstruct memory_dev_type *mtype;\n+\tint numa_node;\n+};\n+\n /**\n  * struct cxl_port - logical collection of upstream port devices and\n  *\t\t     downstream port devices to construct a CXL memory\n@@ -807,6 +835,7 @@ DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device\n DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n DEFINE_FREE(put_cxl_dax_region, struct cxl_dax_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n+DEFINE_FREE(put_cxl_sysram, struct cxl_sysram *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n \n int devm_cxl_enumerate_ports(struct cxl_memdev *cxlmd);\n void cxl_bus_rescan(void);\n@@ -889,6 +918,7 @@ void cxl_destroy_region(struct cxl_region *cxlr);\n struct device *cxl_region_dev(struct cxl_region *cxlr);\n enum cxl_partition_mode cxl_region_mode(struct cxl_region *cxlr);\n int cxl_get_region_range(struct cxl_region *cxlr, struct range *range);\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr);\n int cxl_get_committed_regions(struct cxl_memdev *cxlmd,\n \t\t\t      struct cxl_region **regions, int max_regions);\n struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n@@ -936,6 +966,7 @@ void cxl_driver_unregister(struct cxl_driver *cxl_drv);\n #define CXL_DEVICE_PMEM_REGION\t\t7\n #define CXL_DEVICE_DAX_REGION\t\t8\n #define CXL_DEVICE_PMU\t\t\t9\n+#define CXL_DEVICE_SYSRAM\t\t10\n \n #define MODULE_ALIAS_CXL(type) MODULE_ALIAS(\"cxl:t\" __stringify(type) \"*\")\n #define CXL_MODALIAS_FMT \"cxl:t%d\"\n@@ -954,6 +985,10 @@ bool is_cxl_pmem_region(struct device *dev);\n struct cxl_pmem_region *to_cxl_pmem_region(struct device *dev);\n int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n+struct cxl_sysram *to_cxl_sysram(struct device *dev);\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n static inline bool is_cxl_pmem_region(struct device *dev)\n@@ -972,6 +1007,19 @@ static inline struct cxl_dax_region *to_cxl_dax_region(struct device *dev)\n {\n \treturn NULL;\n }\n+static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\treturn NULL;\n+}\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+\t\t\t\t      enum mmop online_type)\n+{\n+\treturn -ENXIO;\n+}\n+static inline int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn -ENXIO;\n+}\n static inline u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint,\n \t\t\t\t\t       u64 spa)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-25-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the cxl_sysram region not supporting N_MEMORY_PRIVATE hotplug, and responded by extending the devm_cxl_add_sysram() function to take an additional 'private' parameter, which when set will register the memory as a private node using add_private_memory_driver_managed(). The patch includes updated code in drivers/cxl/core/region_sysram.c to handle this new functionality.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Extend the cxl_sysram region to support N_MEMORY_PRIVATE hotplug\nvia add_private_memory_driver_managed(). When a caller passes\nprivate=true to devm_cxl_add_sysram(), the memory is registered\nas a private node, isolating it from normal allocations and reclaim.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/core.h          |  2 +-\n drivers/cxl/core/region_sysram.c | 50 +++++++++++++++++++++++++-------\n drivers/cxl/cxl.h                |  9 ++++--\n 3 files changed, 48 insertions(+), 13 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 973bbcae43f7..8ca3d6d41fe4 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -56,7 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nindex 47a415deb352..77aaa52e7332 100644\n--- a/drivers/cxl/core/region_sysram.c\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -85,12 +85,23 @@ static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n \t/*\n \t * Ensure that future kexec'd kernels will not treat\n \t * this as RAM automatically.\n+\t *\n+\t * For private regions, use add_private_memory_driver_managed()\n+\t * to register as N_MEMORY_PRIVATE which isolates the memory from\n+\t * normal allocations and reclaim.\n \t */\n-\trc = __add_memory_driver_managed(sysram->mgid,\n-\t\t\t\t\t sysram->hpa_range.start,\n-\t\t\t\t\t range_len(&sysram->hpa_range),\n-\t\t\t\t\t sysram_res_name, mhp_flags,\n-\t\t\t\t\t online_type);\n+\tif (sysram->private)\n+\t\trc = add_private_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t       sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t       online_type, &sysram->np);\n+\telse\n+\t\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t online_type);\n \tif (rc) {\n \t\tremove_resource(res);\n \t\tkfree(res);\n@@ -108,10 +119,23 @@ static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n \tif (!sysram->res)\n \t\treturn 0;\n \n-\trc = offline_and_remove_memory(sysram->hpa_range.start,\n-\t\t\t\t       range_len(&sysram->hpa_range));\n-\tif (rc)\n-\t\treturn rc;\n+\tif (sysram->private) {\n+\t\trc = offline_and_remove_private_memory(sysram->numa_node,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\t/*\n+\t\t * -EBUSY means memory was removed but node_private_unregister()\n+\t\t * could not complete because other regions share the node.\n+\t\t * Continue to resource cleanup since the memory is gone.\n+\t\t */\n+\t\tif (rc && rc != -EBUSY)\n+\t\t\treturn rc;\n+\t} else {\n+\t\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\t}\n \n \tif (sysram->res) {\n \t\tremove_resource(sysram->res);\n@@ -257,7 +281,8 @@ static void sysram_unregister(void *_sysram)\n \tdevice_unregister(&sysram->dev);\n }\n \n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n+\t\t\tenum mmop online_type)\n {\n \tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n \tstruct memory_dev_type *mtype;\n@@ -291,6 +316,11 @@ int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n \tif (online_type >= 0)\n \t\tsysram->online_type = online_type;\n \n+\t/* Set up private node registration if requested */\n+\tsysram->private = private;\n+\tif (private)\n+\t\tsysram->np.owner = sysram;\n+\n \tdev = &sysram->dev;\n \n \trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 8e8342fd4fde..54e5f9ac59dc 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -10,6 +10,7 @@\n #include <linux/bitops.h>\n #include <linux/log2.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n #include <linux/io.h>\n #include <linux/range.h>\n #include <linux/dax.h>\n@@ -619,6 +620,8 @@ struct cxl_dax_region {\n  * @mgid: Memory group id\n  * @mtype: Memory tier type\n  * @numa_node: NUMA node for this memory\n+ * @private: true if this region uses N_MEMORY_PRIVATE hotplug\n+ * @np: private node registration state (valid when @private is true)\n  *\n  * Device that directly performs memory hotplug for CXL RAM regions.\n  */\n@@ -633,6 +636,8 @@ struct cxl_sysram {\n \tint mgid;\n \tstruct memory_dev_type *mtype;\n \tint numa_node;\n+\tbool private;\n+\tstruct node_private np;\n };\n \n /**\n@@ -987,7 +992,7 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n struct cxl_sysram *to_cxl_sysram(struct device *dev);\n struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n@@ -1011,7 +1016,7 @@ static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n {\n \treturn NULL;\n }\n-static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n \t\t\t\t      enum mmop online_type)\n {\n \treturn -ENXIO;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-26-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the CXL type-3 driver's registration of device memory as private-node NUMA memory, explaining that they added a sample driver to register node-private ops and migrate pages using alloc_migration_target() with __GFP_THISNODE | __GFP_PRIVATE. The author did not indicate any plans for further revision.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a sample CXL type-3 driver that registers device memory as\nprivate-node NUMA memory reachable only via explicit mempolicy\n(set_mempolicy / mbind).\n\nProbe flow:\n  1. Call cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Look for pre-committed RAM regions; if none exist, create one\n     using cxl_get_hpa_freespace() + cxl_request_dpa() +\n     cxl_create_region()\n  3. Convert the region to sysram via devm_cxl_add_sysram() with\n     private=true and MMOP_ONLINE_MOVABLE\n  4. Register node_private_ops with NP_OPS_MIGRATION | NP_OPS_MEMPOLICY\n     so the node is excluded from default allocations\n\nThe migrate_to callback uses alloc_migration_target() with\n__GFP_THISNODE | __GFP_PRIVATE to keep pages on the target node.\n\nMove struct migration_target_control from mm/internal.h to\ninclude/linux/migrate.h so the driver can use alloc_migration_target()\nwithout depending on mm-internal headers.\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/Kconfig                           |   2 +\n drivers/cxl/Makefile                          |   2 +\n drivers/cxl/type3_drivers/Kconfig             |   2 +\n drivers/cxl/type3_drivers/Makefile            |   2 +\n .../cxl/type3_drivers/cxl_mempolicy/Kconfig   |  16 +\n .../cxl/type3_drivers/cxl_mempolicy/Makefile  |   4 +\n .../type3_drivers/cxl_mempolicy/mempolicy.c   | 297 ++++++++++++++++++\n include/linux/migrate.h                       |   7 +-\n mm/internal.h                                 |   7 -\n 9 files changed, 331 insertions(+), 8 deletions(-)\n create mode 100644 drivers/cxl/type3_drivers/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n\ndiff --git a/drivers/cxl/Kconfig b/drivers/cxl/Kconfig\nindex f99aa7274d12..1648cdeaa0c9 100644\n--- a/drivers/cxl/Kconfig\n+++ b/drivers/cxl/Kconfig\n@@ -278,4 +278,6 @@ config CXL_ATL\n \tdepends on CXL_REGION\n \tdepends on ACPI_PRMT && AMD_NB\n \n+source \"drivers/cxl/type3_drivers/Kconfig\"\n+\n endif\ndiff --git a/drivers/cxl/Makefile b/drivers/cxl/Makefile\nindex 2caa90fa4bf2..94d2b2233bf8 100644\n--- a/drivers/cxl/Makefile\n+++ b/drivers/cxl/Makefile\n@@ -19,3 +19,5 @@ cxl_acpi-y := acpi.o\n cxl_pmem-y := pmem.o security.o\n cxl_mem-y := mem.o\n cxl_pci-y := pci.o\n+\n+obj-y += type3_drivers/\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nnew file mode 100644\nindex 000000000000..369b21763856\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nnew file mode 100644\nindex 000000000000..2b82265ff118\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\nnew file mode 100644\nindex 000000000000..3c45da237b9f\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n@@ -0,0 +1,16 @@\n+config CXL_MEMPOLICY\n+\ttristate \"CXL Private Memory with Mempolicy Support\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\thelp\n+\t  Minimal driver for CXL memory devices that registers memory as\n+\t  N_MEMORY_PRIVATE with mempolicy support.  The memory is isolated\n+\t  from default allocations and can only be reached via explicit\n+\t  mempolicy (set_mempolicy or mbind).\n+\n+\t  No compression, no PTE controls, the memory behaves like normal\n+\t  DRAM but is excluded from fallback allocations.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\nnew file mode 100644\nindex 000000000000..dfb58fc88ad9\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy.o\n+cxl_mempolicy-y := mempolicy.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\nnew file mode 100644\nindex 000000000000..1c19818eb268\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n@@ -0,0 +1,297 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Mempolicy Driver\n+ *\n+ * Minimal driver for CXL memory devices that registers memory as\n+ * N_MEMORY_PRIVATE with mempolicy support but no PTE controls.  The\n+ * memory behaves like normal DRAM but is isolated from default allocations,\n+ * it can only be reached via explicit mempolicy (set_mempolicy/mbind).\n+ *\n+ * Usage:\n+ *   1. Unbind device from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   2. Bind to cxl_mempolicy:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n+ */\n+\n+#include <linux/module.h>\n+#include <linux/pci.h>\n+#include <linux/xarray.h>\n+#include <linux/node_private.h>\n+#include <linux/migrate.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+struct cxl_mempolicy_ctx {\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint nid;\n+};\n+\n+static DEFINE_XARRAY(ctx_xa);\n+\n+static struct cxl_mempolicy_ctx *memdev_to_ctx(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\n+\treturn xa_load(&ctx_xa, (unsigned long)pdev);\n+}\n+\n+static int cxl_mempolicy_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason,\n+\t\t\t\t    unsigned int *nr_succeeded)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE |\n+\t\t\t    __GFP_PRIVATE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, nr_succeeded);\n+}\n+\n+static void cxl_mempolicy_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static const struct node_private_ops cxl_mempolicy_ops = {\n+\t.migrate_to\t= cxl_mempolicy_migrate_to,\n+\t.folio_migrate\t= cxl_mempolicy_folio_migrate,\n+\t.flags = NP_OPS_MIGRATION | NP_OPS_MEMPOLICY,\n+};\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tctx->cxled = cxled;\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\treturn cxlr;\n+}\n+\n+static int setup_private_node(struct cxl_memdev *cxlmd,\n+\t\t\t      struct cxl_region *cxlr)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct range hpa_range;\n+\tint rc;\n+\n+\tdevice_release_driver(cxl_region_dev(cxlr));\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to add sysram: %d\\n\", rc);\n+\t\tif (device_attach(cxl_region_dev(cxlr)) < 0)\n+\t\t\tdev_warn(cxl_region_dev(cxlr),\n+\t\t\t\t \"failed to re-attach driver\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tctx->nid = phys_to_target_node(hpa_range.start);\n+\tif (ctx->nid == NUMA_NO_NODE)\n+\t\tctx->nid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\trc = node_private_set_ops(ctx->nid, &cxl_mempolicy_ops);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to set ops on node %d: %d\\n\", ctx->nid, rc);\n+\t\tctx->nid = NUMA_NO_NODE;\n+\t\treturn rc;\n+\t}\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"node %d registered as private mempolicy memory\\n\", ctx->nid);\n+\treturn 0;\n+}\n+\n+static int cxl_mempolicy_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i;\n+\tint rc;\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"cxl_mempolicy attach: looking for regions\\n\");\n+\n+\t/* Phase 1: look for pre-committed RAM regions */\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) != CXL_PARTMODE_RAM) {\n+\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tcxlr = regions[i];\n+\t\trc = setup_private_node(cxlmd, cxlr);\n+\t\tput_device(cxl_region_dev(cxlr));\n+\t\tif (rc == 0) {\n+\t\t\t/* Release remaining region references */\n+\t\t\tfor (i++; i < nr; i++)\n+\t\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\treturn 0;\n+\t\t}\n+\t}\n+\n+\t/* Phase 2: no committed regions, create one */\n+\tdev_info(&cxlmd->dev,\n+\t\t \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"no RAM capacity: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = setup_private_node(cxlmd, cxlr);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to setup private node: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\t/* Only take ownership of regions we created (Phase 2) */\n+\tmemdev_to_ctx(cxlmd)->cxlr = cxlr;\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_mempolicy_attach = {\n+\t.probe = cxl_mempolicy_attach_probe,\n+};\n+\n+static int cxl_mempolicy_probe(struct pci_dev *pdev,\n+\t\t\t       const struct pci_device_id *id)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probing device\\n\");\n+\n+\tctx = devm_kzalloc(&pdev->dev, sizeof(*ctx), GFP_KERNEL);\n+\tif (!ctx)\n+\t\treturn -ENOMEM;\n+\tctx->nid = NUMA_NO_NODE;\n+\n+\trc = xa_insert(&ctx_xa, (unsigned long)pdev, ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_mempolicy_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_mempolicy_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = xa_erase(&ctx_xa, (unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: removing device\\n\");\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\tif (ctx->nid != NUMA_NO_NODE)\n+\t\tWARN_ON(node_private_clear_ops(ctx->nid, &cxl_mempolicy_ops));\n+\n+\tif (ctx->cxlr) {\n+\t\tcxl_destroy_region(ctx->cxlr);\n+\t\tctx->cxlr = NULL;\n+\t}\n+\n+\tif (ctx->cxled) {\n+\t\tcxl_dpa_free(ctx->cxled);\n+\t\tctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_mempolicy_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_mempolicy_pci_tbl);\n+\n+static struct pci_driver cxl_mempolicy_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_mempolicy_pci_tbl,\n+\t.probe\t\t= cxl_mempolicy_probe,\n+\t.remove\t\t= cxl_mempolicy_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_mempolicy_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Private Memory with Mempolicy Support\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 7b2da3875ff2..1f9fb61f3932 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -10,7 +10,12 @@\n typedef struct folio *new_folio_t(struct folio *folio, unsigned long private);\n typedef void free_folio_t(struct folio *folio, unsigned long private);\n \n-struct migration_target_control;\n+struct migration_target_control {\n+\tint nid;\t\t/* preferred node id */\n+\tnodemask_t *nmask;\n+\tgfp_t gfp_mask;\n+\tenum migrate_reason reason;\n+};\n \n /**\n  * struct movable_operations - Driver page migration\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 64467ca774f1..85cd11189854 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1352,13 +1352,6 @@ extern const struct trace_print_flags gfpflag_names[];\n \n void setup_zone_pageset(struct zone *zone);\n \n-struct migration_target_control {\n-\tint nid;\t\t/* preferred node id */\n-\tnodemask_t *nmask;\n-\tgfp_t gfp_mask;\n-\tenum migrate_reason reason;\n-};\n-\n /*\n  * mm/filemap.c\n  */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-27-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the lack of a generic CXL type-3 driver for compressed memory controllers, which was previously missing from the patch series. The author has added this driver and explained its functionality, including page flush pipeline, watermark interrupts, and teardown ordering.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "added new code",
                "explained existing functionality"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a generic CXL type-3 driver for compressed memory controllers.\n\nThe driver provides an alternative PCI binding that converts CXL\nRAM regions to private-node sysram and registers them with the\nCRAM subsystem for transparent demotion/promotion.\n\nProbe flow:\n  1. cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Discover/convert auto-RAM regions or create a RAM region\n  3. Convert to private-node sysram via devm_cxl_add_sysram()\n  4. Register with CRAM via cram_register_private_node()\n\nPage flush pipeline:\n  When a CRAM folio is freed, the CRAM free_folio   callback buffers\n  it into a per-CPU RCU-protected flush buffer to offload the operation.\n\n  A periodic kthread swaps the per-CPU buffers under RCU, then sends\n  batched Sanitize-Zero commands so the device can zero pages.\n\n  A flush_record bitmap tracks in-flight pages to avoid re-buffering on\n  the second free_folio entry after folio_put().\n\n  Overflow from full buffers is handled by a per-CPU workqueue fallback.\n\nWatermark interrupts:\n  MSI-X vector 12 - delivers \"Low\" watermark interrupts\n  MSI-X vector 13 - delivers \"High\" watermark interrupts\n  This adjusts CRAM pressure:\n\tLow  - increases pressure.\n  \tHigh - reduces pressure.\n\n  A dynamic watermark mode cycles through four phases with\n  progressively tighter thresholds.\n\n  Static watermark mode sets pressure 0 or MAX respectively.\n\nTeardown ordering:\n  pre_teardown  - cram_unregister + retry-loop memory offline\n  post_teardown - kthread stop, drain all flush buffers via CCI\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/type3_drivers/Kconfig             |    1 +\n drivers/cxl/type3_drivers/Makefile            |    1 +\n .../cxl/type3_drivers/cxl_compression/Kconfig |   20 +\n .../type3_drivers/cxl_compression/Makefile    |    4 +\n .../cxl_compression/compression.c             | 1025 +++++++++++++++++\n 5 files changed, 1051 insertions(+)\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/compression.c\n\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nindex 369b21763856..98f73e46730e 100644\n--- a/drivers/cxl/type3_drivers/Kconfig\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\n+source \"drivers/cxl/type3_drivers/cxl_compression/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nindex 2b82265ff118..f5b0766d92af 100644\n--- a/drivers/cxl/type3_drivers/Makefile\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression/\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Kconfig b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\nnew file mode 100644\nindex 000000000000..8c891a48b000\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\n@@ -0,0 +1,20 @@\n+config CXL_COMPRESSION\n+\ttristate \"CXL Compression Memory Driver\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on CRAM\n+\thelp\n+\t  This driver provides an alternative PCI binding for CXL memory\n+\t  devices with compressed memory support. It converts CXL RAM\n+\t  regions to sysram for direct memory hotplug and registers with\n+\t  the CRAM subsystem for transparent compression.\n+\n+\t  Page reclamation uses the standard CXL Media Operations Zero\n+\t  command (opcode 0x4402). If the device does not support it,\n+\t  the driver falls back to inline CPU zeroing.\n+\n+\t  Usage: First unbind the device from cxl_pci, then bind to\n+\t  cxl_compression. The driver will initialize the CXL device and\n+\t  convert any RAM regions to use direct memory hotplug via sysram.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Makefile b/drivers/cxl/type3_drivers/cxl_compression/Makefile\nnew file mode 100644\nindex 000000000000..46f34809bf74\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression.o\n+cxl_compression-y := compression.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/compression.c b/drivers/cxl/type3_drivers/cxl_compression/compression.c\nnew file mode 100644\nindex 000000000000..e4c8b62227e2\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/compression.c\n@@ -0,0 +1,1025 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Compression Driver\n+ *\n+ * This driver provides an alternative binding for CXL memory devices that\n+ * converts all associated RAM regions to sysram_regions for direct memory\n+ * hotplug, bypassing the standard dax region path.\n+ *\n+ * Page reclamation uses the standard CXL Media Operations Zero command\n+ * (opcode 0x4402, class 0x01, subclass 0x01).  Watermark interrupts\n+ * are delivered via separate MSI-X vectors (12 for lthresh, 13 for\n+ * hthresh), injected externally via QMP.\n+ *\n+ * Usage:\n+ *   1. Device initially binds to cxl_pci at boot\n+ *   2. Unbind from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   3. Bind to cxl_compression:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n+ */\n+\n+#include <linux/unaligned.h>\n+#include <linux/io-64-nonatomic-lo-hi.h>\n+#include <linux/module.h>\n+#include <linux/delay.h>\n+#include <linux/sizes.h>\n+#include <linux/mutex.h>\n+#include <linux/list.h>\n+#include <linux/pci.h>\n+#include <linux/io.h>\n+#include <linux/interrupt.h>\n+#include <linux/bitmap.h>\n+#include <linux/highmem.h>\n+#include <linux/workqueue.h>\n+#include <linux/kthread.h>\n+#include <linux/rcupdate.h>\n+#include <linux/percpu.h>\n+#include <linux/sched.h>\n+#include <linux/cram.h>\n+#include <linux/memory_hotplug.h>\n+#include <linux/xarray.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+/*\n+ * Per-device compression context lookup.\n+ *\n+ * pci_set_drvdata() MUST store cxlds because mbox_to_cxlds() uses\n+ * dev_get_drvdata() to recover the cxl_dev_state from the mailbox host\n+ * device.  Storing anything else in pci drvdata breaks every CXL mailbox\n+ * command.  Use an xarray keyed by pci_dev pointer so that multiple\n+ * devices can bind concurrently without colliding.\n+ */\n+static DEFINE_XARRAY(comp_ctx_xa);\n+\n+static struct cxl_compression_ctx *pdev_to_comp_ctx(struct pci_dev *pdev)\n+{\n+\treturn xa_load(&comp_ctx_xa, (unsigned long)pdev);\n+}\n+\n+#define CXL_MEDIA_OP_OPCODE\t\t0x4402\n+#define CXL_MEDIA_OP_CLASS_SANITIZE\t0x01\n+#define CXL_MEDIA_OP_SUBC_ZERO\t\t0x01\n+\n+struct cxl_dpa_range {\n+\t__le64 starting_dpa;\n+\t__le64 length;\n+} __packed;\n+\n+struct cxl_media_op_input {\n+\tu8 media_operation_class;\n+\tu8 media_operation_subclass;\n+\t__le16 reserved;\n+\t__le32 dpa_range_count;\n+\tstruct cxl_dpa_range ranges[];\n+} __packed;\n+\n+#define CXL_CT3_MSIX_LTHRESH\t\t12\n+#define CXL_CT3_MSIX_HTHRESH\t\t13\n+#define CXL_CT3_MSIX_VECTOR_NR\t\t14\n+#define CXL_FLUSH_INTERVAL_DEFAULT_MS\t1000\n+\n+static unsigned int flush_buf_size;\n+module_param(flush_buf_size, uint, 0444);\n+MODULE_PARM_DESC(flush_buf_size,\n+\t\t \"Max DPA ranges per media ops CCI command (0 = use hw max)\");\n+\n+static unsigned int flush_interval_ms = CXL_FLUSH_INTERVAL_DEFAULT_MS;\n+module_param(flush_interval_ms, uint, 0644);\n+MODULE_PARM_DESC(flush_interval_ms,\n+\t\t \"Flush worker interval in ms (default 1000)\");\n+\n+struct cxl_flush_buf {\n+\tunsigned int count;\n+\tunsigned int max;\t\t\t/* max ranges per command */\n+\tstruct cxl_media_op_input *cmd;\t\t/* pre-allocated CCI payload */\n+\tstruct folio **folios;\t\t\t/* parallel folio tracking */\n+};\n+\n+struct cxl_flush_ctx;\n+\n+struct cxl_pcpu_flush {\n+\tstruct cxl_flush_buf __rcu *active;\t/* callback writes here */\n+\tstruct cxl_flush_buf *overflow_spare;\t/* spare for overflow work */\n+\tstruct work_struct overflow_work;\t/* per-CPU overflow flush */\n+\tstruct cxl_flush_ctx *ctx;\t\t/* backpointer */\n+};\n+\n+/**\n+ * struct cxl_flush_ctx - Per-region flush context\n+ * @flush_record: two-level bitmap, 1 bit per 4KB page, tracks in-flight ops\n+ * @flush_record_pages: number of pages in the flush_record array\n+ * @nr_pages: total number of 4KB pages in the region\n+ * @base_pfn: starting PFN of the region (for DPA offset calculation)\n+ * @buf_max: max DPA ranges per CCI command\n+ * @media_ops_supported: true if device supports media operations zero\n+ * @pcpu: per-CPU flush state\n+ * @kthread_spares: array[nr_cpu_ids] of spare buffers for the kthread\n+ * @flush_thread: round-robin kthread\n+ * @mbox: pointer to CXL mailbox for sending CCI commands\n+ * @dev: device for logging\n+ * @nid: NUMA node of the private region\n+ */\n+struct cxl_flush_ctx {\n+\tunsigned long\t**flush_record;\n+\tunsigned int\t flush_record_pages;\n+\tunsigned long\t nr_pages;\n+\tunsigned long\t base_pfn;\n+\tunsigned int\t buf_max;\n+\tbool\t\t media_ops_supported;\n+\tstruct cxl_pcpu_flush __percpu *pcpu;\n+\tstruct cxl_flush_buf **kthread_spares;\n+\tstruct task_struct *flush_thread;\n+\tstruct cxl_mailbox *mbox;\n+\tstruct device\t*dev;\n+\tint\t\t nid;\n+};\n+\n+/* Bits per page-sized bitmap chunk */\n+#define FLUSH_RECORD_BITS_PER_PAGE\t(PAGE_SIZE * BITS_PER_BYTE)\n+#define FLUSH_RECORD_SHIFT\t\t(PAGE_SHIFT + 3)\n+\n+static unsigned long **flush_record_alloc(unsigned long nr_bits,\n+\t\t\t\t\t  unsigned int *nr_pages_out)\n+{\n+\tunsigned int nr_pages = DIV_ROUND_UP(nr_bits, FLUSH_RECORD_BITS_PER_PAGE);\n+\tunsigned long **pages;\n+\tunsigned int i;\n+\n+\tpages = kcalloc(nr_pages, sizeof(*pages), GFP_KERNEL);\n+\tif (!pages)\n+\t\treturn NULL;\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tpages[i] = (unsigned long *)get_zeroed_page(GFP_KERNEL);\n+\t\tif (!pages[i])\n+\t\t\tgoto err;\n+\t}\n+\n+\t*nr_pages_out = nr_pages;\n+\treturn pages;\n+\n+err:\n+\twhile (i--)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+\treturn NULL;\n+}\n+\n+static void flush_record_free(unsigned long **pages, unsigned int nr_pages)\n+{\n+\tunsigned int i;\n+\n+\tif (!pages)\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr_pages; i++)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+}\n+\n+static inline bool flush_record_test_and_clear(unsigned long **pages,\n+\t\t\t\t\t       unsigned long idx)\n+{\n+\treturn test_and_clear_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\t\t\t  pages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static inline void flush_record_set(unsigned long **pages, unsigned long idx)\n+{\n+\tset_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\tpages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static struct cxl_flush_buf *cxl_flush_buf_alloc(unsigned int max, int nid)\n+{\n+\tstruct cxl_flush_buf *buf;\n+\n+\tbuf = kzalloc_node(sizeof(*buf), GFP_KERNEL, nid);\n+\tif (!buf)\n+\t\treturn NULL;\n+\n+\tbuf->max = max;\n+\tbuf->cmd = kvzalloc_node(struct_size(buf->cmd, ranges, max),\n+\t\t\t\t GFP_KERNEL, nid);\n+\tif (!buf->cmd)\n+\t\tgoto err_cmd;\n+\n+\tbuf->folios = kcalloc_node(max, sizeof(struct folio *),\n+\t\t\t\t   GFP_KERNEL, nid);\n+\tif (!buf->folios)\n+\t\tgoto err_folios;\n+\n+\treturn buf;\n+\n+err_folios:\n+\tkvfree(buf->cmd);\n+err_cmd:\n+\tkfree(buf);\n+\treturn NULL;\n+}\n+\n+static void cxl_flush_buf_free(struct cxl_flush_buf *buf)\n+{\n+\tif (!buf)\n+\t\treturn;\n+\tkvfree(buf->cmd);\n+\tkfree(buf->folios);\n+\tkfree(buf);\n+}\n+\n+static inline void cxl_flush_buf_reset(struct cxl_flush_buf *buf)\n+{\n+\tbuf->count = 0;\n+}\n+\n+static void cxl_flush_buf_send(struct cxl_flush_ctx *ctx,\n+\t\t\t       struct cxl_flush_buf *buf)\n+{\n+\tstruct cxl_mbox_cmd mbox_cmd;\n+\tunsigned int count = buf->count;\n+\tunsigned int i;\n+\tint rc;\n+\n+\tif (count == 0)\n+\t\treturn;\n+\n+\tif (!ctx->media_ops_supported) {\n+\t\t/* No device support, zero all folios inline */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t\tgoto release;\n+\t}\n+\n+\tbuf->cmd->media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE;\n+\tbuf->cmd->media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO;\n+\tbuf->cmd->reserved = 0;\n+\tbuf->cmd->dpa_range_count = cpu_to_le32(count);\n+\n+\tmbox_cmd = (struct cxl_mbox_cmd) {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = buf->cmd,\n+\t\t.size_in = struct_size(buf->cmd, ranges, count),\n+\t\t.poll_interval_ms = 1000,\n+\t\t.poll_count = 30,\n+\t};\n+\n+\trc = cxl_internal_send_cmd(ctx->mbox, &mbox_cmd);\n+\tif (rc) {\n+\t\tdev_warn(ctx->dev,\n+\t\t\t \"media ops zero CCI command failed: %d\\n\", rc);\n+\n+\t\t/* Zero all folios inline on failure */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t}\n+\n+release:\n+\tfor (i = 0; i < count; i++)\n+\t\tfolio_put(buf->folios[i]);\n+\n+\tcxl_flush_buf_reset(buf);\n+}\n+\n+static int cxl_compression_flush_cb(struct folio *folio, void *private)\n+{\n+\tstruct cxl_flush_ctx *ctx = private;\n+\tunsigned long pfn = folio_pfn(folio);\n+\tunsigned long idx = pfn - ctx->base_pfn;\n+\tunsigned long nr = folio_nr_pages(folio);\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tunsigned long flags;\n+\tunsigned int pos;\n+\n+\t/* Case (a): flush record bit set, resolution from our media op */\n+\tif (flush_record_test_and_clear(ctx->flush_record, idx))\n+\t\treturn 0;\n+\n+\tdev_dbg_ratelimited(ctx->dev,\n+\t\t\t     \"flush_cb: folio pfn=%lx order=%u idx=%lu cpu=%d\\n\",\n+\t\t\t     pfn, folio_order(folio), idx,\n+\t\t\t     raw_smp_processor_id());\n+\n+\tlocal_irq_save(flags);\n+\trcu_read_lock();\n+\n+\tpcpu = this_cpu_ptr(ctx->pcpu);\n+\tbuf = rcu_dereference(pcpu->active);\n+\n+\tif (unlikely(!buf || buf->count >= buf->max)) {\n+\t\trcu_read_unlock();\n+\t\tlocal_irq_restore(flags);\n+\t\tif (buf)\n+\t\t\tschedule_work_on(raw_smp_processor_id(),\n+\t\t\t\t\t &pcpu->overflow_work);\n+\t\treturn 2;\n+\t}\n+\n+\t/* Case (b): write DPA range directly into pre-formatted CCI buffer */\n+\tfolio_get(folio);\n+\tflush_record_set(ctx->flush_record, idx);\n+\n+\tpos = buf->count;\n+\tbuf->folios[pos] = folio;\n+\tbuf->cmd->ranges[pos].starting_dpa = cpu_to_le64((u64)idx * PAGE_SIZE);\n+\tbuf->cmd->ranges[pos].length = cpu_to_le64((u64)nr * PAGE_SIZE);\n+\tbuf->count = pos + 1;\n+\n+\trcu_read_unlock();\n+\tlocal_irq_restore(flags);\n+\n+\treturn 1;\n+}\n+\n+static int cxl_flush_kthread_fn(void *data)\n+{\n+\tstruct cxl_flush_ctx *ctx = data;\n+\tstruct cxl_flush_buf *dirty;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tint cpu;\n+\tbool any_dirty;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tany_dirty = false;\n+\n+\t\t/* Phase 1: Swap all per-CPU buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tstruct cxl_flush_buf *spare = ctx->kthread_spares[cpu];\n+\n+\t\t\tif (!spare)\n+\t\t\t\tcontinue;\n+\n+\t\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\t\tcxl_flush_buf_reset(spare);\n+\t\t\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\t\t\tctx->kthread_spares[cpu] = dirty;\n+\n+\t\t\tif (dirty && dirty->count > 0) {\n+\t\t\t\tdev_dbg(ctx->dev,\n+\t\t\t\t\t \"flush_kthread: cpu=%d has %u dirty ranges\\n\",\n+\t\t\t\t\t cpu, dirty->count);\n+\t\t\t\tany_dirty = true;\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (!any_dirty)\n+\t\t\tgoto sleep;\n+\n+\t\t/* Phase 2: Single synchronize_rcu for all swaps */\n+\t\tsynchronize_rcu();\n+\n+\t\t/* Phase 3: Send CCI commands for dirty buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tdirty = ctx->kthread_spares[cpu];\n+\t\t\tif (dirty && dirty->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, dirty);\n+\t\t\t/* dirty is now clean, stays as kthread_spares[cpu] */\n+\t\t}\n+\n+sleep:\n+\t\tschedule_timeout_interruptible(\n+\t\t\tmsecs_to_jiffies(flush_interval_ms));\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static void cxl_flush_overflow_work(struct work_struct *work)\n+{\n+\tstruct cxl_pcpu_flush *pcpu =\n+\t\tcontainer_of(work, struct cxl_pcpu_flush, overflow_work);\n+\tstruct cxl_flush_ctx *ctx = pcpu->ctx;\n+\tstruct cxl_flush_buf *dirty, *spare;\n+\tunsigned long flags;\n+\n+\tdev_dbg(ctx->dev, \"flush_overflow: cpu=%d buffer full, flushing\\n\",\n+\t\t raw_smp_processor_id());\n+\n+\tspare = pcpu->overflow_spare;\n+\tif (!spare)\n+\t\treturn;\n+\n+\tcxl_flush_buf_reset(spare);\n+\n+\tlocal_irq_save(flags);\n+\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\tlocal_irq_restore(flags);\n+\n+\tpcpu->overflow_spare = dirty;\n+\n+\tsynchronize_rcu();\n+\tcxl_flush_buf_send(ctx, dirty);\n+}\n+\n+struct cxl_teardown_ctx {\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+};\n+\n+static void cxl_compression_pre_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\n+\tif (!tctx->flush_ctx)\n+\t\treturn;\n+\n+\t/*\n+\t * Unregister the CRAM node before memory goes offline.\n+\t * node_private_clear_ops requires the node_private to still\n+\t * exist, which is destroyed during memory removal.\n+\t */\n+\tcram_unregister_private_node(tctx->nid);\n+\n+\t/*\n+\t * Offline and remove CXL memory with retry.  CXL compressed\n+\t * memory may have pages pinned by in-flight flush operations;\n+\t * keep retrying until they complete.  Once done, sysram->res\n+\t * is NULL so the devm sysram_unregister action that follows\n+\t * will skip the hotplug removal.\n+\t */\n+\tif (tctx->sysram) {\n+\t\tint rc, retries = 0;\n+\n+\t\twhile (true) {\n+\t\t\trc = cxl_sysram_offline_and_remove(tctx->sysram);\n+\t\t\tif (!rc)\n+\t\t\t\tbreak;\n+\t\t\tif (++retries > 60) {\n+\t\t\t\tpr_err(\"cxl_compression: memory offline failed after %d retries, giving up\\n\",\n+\t\t\t\t       retries);\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tpr_info(\"cxl_compression: memory offline failed (%d), retrying...\\n\",\n+\t\t\t\trc);\n+\t\t\tmsleep(1000);\n+\t\t}\n+\t}\n+}\n+\n+static void cxl_compression_post_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\tstruct cxl_flush_ctx *ctx = tctx->flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tint cpu;\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\t/* cram_unregister_private_node already called in pre_teardown */\n+\n+\tif (ctx->flush_thread) {\n+\t\tkthread_stop(ctx->flush_thread);\n+\t\tctx->flush_thread = NULL;\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\tcancel_work_sync(&pcpu->overflow_work);\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tif (buf && buf->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, buf);\n+\n+\t\tif (pcpu->overflow_spare && pcpu->overflow_spare->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares && ctx->kthread_spares[cpu]) {\n+\t\t\tbuf = ctx->kthread_spares[cpu];\n+\t\t\tif (buf->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, buf);\n+\t\t}\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(ctx->kthread_spares[cpu]);\n+\t}\n+\n+\tkfree(ctx->kthread_spares);\n+\tfree_percpu(ctx->pcpu);\n+\tflush_record_free(ctx->flush_record, ctx->flush_record_pages);\n+}\n+\n+/**\n+ * struct cxl_compression_ctx - Per-device context for compression driver\n+ * @mbox: CXL mailbox for issuing CCI commands\n+ * @pdev: PCI device\n+ * @flush_ctx: Flush context for deferred page reclamation\n+ * @tctx: Teardown context for devm actions\n+ * @sysram: Sysram device for offline+remove in remove path\n+ * @nid: NUMA node ID, NUMA_NO_NODE if unset\n+ * @cxlmd: The memdev associated with this context\n+ * @cxlr: Region created by this driver (NULL if pre-existing)\n+ * @cxled: Endpoint decoder with DPA allocated by this driver\n+ * @regions_converted: Number of regions successfully converted\n+ * @media_ops_supported: Device supports media operations zero (0x4402)\n+ */\n+struct cxl_compression_ctx {\n+\tstruct cxl_mailbox *mbox;\n+\tstruct pci_dev *pdev;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+\tstruct cxl_memdev *cxlmd;\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint regions_converted;\n+\tbool media_ops_supported;\n+};\n+\n+/*\n+ * Probe whether the device supports Media Operations Zero (0x4402).\n+ * Send a zero-count command, a conforming device returns SUCCESS,\n+ * a device that doesn't support it returns UNSUPPORTED (-ENXIO).\n+ */\n+static bool cxl_probe_media_ops_zero(struct cxl_mailbox *mbox,\n+\t\t\t\t     struct device *dev)\n+{\n+\tstruct cxl_media_op_input probe = {\n+\t\t.media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE,\n+\t\t.media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO,\n+\t\t.dpa_range_count = 0,\n+\t};\n+\tstruct cxl_mbox_cmd cmd = {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = &probe,\n+\t\t.size_in = sizeof(probe),\n+\t};\n+\tint rc;\n+\n+\trc = cxl_internal_send_cmd(mbox, &cmd);\n+\tif (rc) {\n+\t\tdev_info(dev,\n+\t\t\t \"media operations zero not supported (rc=%d), using inline zeroing\\n\",\n+\t\t\t rc);\n+\t\treturn false;\n+\t}\n+\n+\tdev_info(dev, \"media operations zero (0x4402) supported\\n\");\n+\treturn true;\n+}\n+\n+struct cxl_compression_wm_ctx {\n+\tstruct device *dev;\n+\tint nid;\n+};\n+\n+static irqreturn_t cxl_compression_lthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"lthresh watermark: pressuring node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, CRAM_PRESSURE_MAX);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static irqreturn_t cxl_compression_hthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"hthresh watermark: resuming node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, 0);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static int convert_region_to_sysram(struct cxl_region *cxlr,\n+\t\t\t\t    struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct device *dev = cxl_region_dev(cxlr);\n+\tstruct cxl_compression_wm_ctx *wm_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tresource_size_t region_start, region_size;\n+\tstruct range hpa_range;\n+\tint nid;\n+\tint irq;\n+\tint cpu;\n+\tint rc;\n+\n+\tif (cxl_region_mode(cxlr) != CXL_PARTMODE_RAM) {\n+\t\tdev_dbg(dev, \"skipping non-RAM region (mode=%d)\\n\",\n+\t\t\tcxl_region_mode(cxlr));\n+\t\treturn 0;\n+\t}\n+\n+\tdev_info(dev, \"converting region to sysram\\n\");\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to add sysram region: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\ttctx = devm_kzalloc(dev, sizeof(*tctx), GFP_KERNEL);\n+\tif (!tctx)\n+\t\treturn -ENOMEM;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_post_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Find the sysram child device for pre_teardown */\n+\tcomp_ctx->sysram = cxl_region_find_sysram(cxlr);\n+\tif (comp_ctx->sysram)\n+\t\ttctx->sysram = comp_ctx->sysram;\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tnid = phys_to_target_node(hpa_range.start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\tregion_start = hpa_range.start;\n+\tregion_size = range_len(&hpa_range);\n+\n+\tflush_ctx = devm_kzalloc(dev, sizeof(*flush_ctx), GFP_KERNEL);\n+\tif (!flush_ctx)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->base_pfn = PHYS_PFN(region_start);\n+\tflush_ctx->nr_pages = region_size >> PAGE_SHIFT;\n+\tflush_ctx->flush_record = flush_record_alloc(flush_ctx->nr_pages,\n+\t\t\t\t\t\t     &flush_ctx->flush_record_pages);\n+\tif (!flush_ctx->flush_record)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->mbox = comp_ctx->mbox;\n+\tflush_ctx->dev = dev;\n+\tflush_ctx->nid = nid;\n+\tflush_ctx->media_ops_supported = comp_ctx->media_ops_supported;\n+\n+\t/*\n+\t * Cap buffer at max DPA ranges that fit in one CCI payload.\n+\t * Header is 8 bytes (struct cxl_media_op_input), each range\n+\t * is 16 bytes (struct cxl_dpa_range).  The module parameter\n+\t * flush_buf_size can further limit this (0 = use hw max).\n+\t */\n+\tflush_ctx->buf_max = (flush_ctx->mbox->payload_size -\n+\t\t\t      sizeof(struct cxl_media_op_input)) /\n+\t\t\t     sizeof(struct cxl_dpa_range);\n+\tif (flush_buf_size && flush_buf_size < flush_ctx->buf_max)\n+\t\tflush_ctx->buf_max = flush_buf_size;\n+\tif (flush_ctx->buf_max == 0)\n+\t\tflush_ctx->buf_max = 1;\n+\n+\tdev_info(dev,\n+\t\t \"flush buffer: %u DPA ranges per command (payload %zu bytes, media_ops %s)\\n\",\n+\t\t flush_ctx->buf_max, flush_ctx->mbox->payload_size,\n+\t\t flush_ctx->media_ops_supported ? \"yes\" : \"no\");\n+\n+\tflush_ctx->pcpu = alloc_percpu(struct cxl_pcpu_flush);\n+\tif (!flush_ctx->pcpu)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->kthread_spares = kcalloc(nr_cpu_ids,\n+\t\t\t\t\t    sizeof(struct cxl_flush_buf *),\n+\t\t\t\t\t    GFP_KERNEL);\n+\tif (!flush_ctx->kthread_spares)\n+\t\tgoto err_pcpu_init;\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *active_buf, *overflow_buf, *spare_buf;\n+\n+\t\tactive_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!active_buf)\n+\t\t\tgoto err_pcpu_init;\n+\n+\t\toverflow_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!overflow_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tspare_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!spare_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tcxl_flush_buf_free(overflow_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\t\tpcpu->ctx = flush_ctx;\n+\t\trcu_assign_pointer(pcpu->active, active_buf);\n+\t\tpcpu->overflow_spare = overflow_buf;\n+\t\tINIT_WORK(&pcpu->overflow_work, cxl_flush_overflow_work);\n+\n+\t\tflush_ctx->kthread_spares[cpu] = spare_buf;\n+\t}\n+\n+\tflush_ctx->flush_thread = kthread_create_on_node(\n+\t\tcxl_flush_kthread_fn, flush_ctx, nid, \"cxl-flush/%d\", nid);\n+\tif (IS_ERR(flush_ctx->flush_thread)) {\n+\t\trc = PTR_ERR(flush_ctx->flush_thread);\n+\t\tflush_ctx->flush_thread = NULL;\n+\t\tgoto err_pcpu_init;\n+\t}\n+\twake_up_process(flush_ctx->flush_thread);\n+\n+\trc = cram_register_private_node(nid, cxlr,\n+\t\t\t\t\tcxl_compression_flush_cb, flush_ctx);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to register cram node %d: %d\\n\", nid, rc);\n+\t\tgoto err_pcpu_init;\n+\t}\n+\n+\ttctx->flush_ctx = flush_ctx;\n+\ttctx->nid = nid;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_pre_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcomp_ctx->flush_ctx = flush_ctx;\n+\tcomp_ctx->tctx = tctx;\n+\tcomp_ctx->nid = nid;\n+\n+\t/*\n+\t * Register watermark IRQ handlers on &pdev->dev for\n+\t * MSI-X vector 12 (lthresh) and vector 13 (hthresh).\n+\t */\n+\twm_ctx = devm_kzalloc(&pdev->dev, sizeof(*wm_ctx), GFP_KERNEL);\n+\tif (!wm_ctx)\n+\t\treturn -ENOMEM;\n+\n+\twm_ctx->dev = &pdev->dev;\n+\twm_ctx->nid = nid;\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_LTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_lthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-lthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register lthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_HTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_hthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-hthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register hthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\treturn 0;\n+\n+err_pcpu_init:\n+\tif (flush_ctx->flush_thread)\n+\t\tkthread_stop(flush_ctx->flush_thread);\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *buf;\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (flush_ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(flush_ctx->kthread_spares[cpu]);\n+\t}\n+\tkfree(flush_ctx->kthread_spares);\n+\tfree_percpu(flush_ctx->pcpu);\n+\tflush_record_free(flush_ctx->flush_record, flush_ctx->flush_record_pages);\n+\treturn rc ? rc : -ENOMEM;\n+}\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\tpdev_to_comp_ctx(to_pci_dev(cxlmd->dev.parent))->cxled = cxled;\n+\treturn cxlr;\n+}\n+\n+static int cxl_compression_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i, converted = 0, errors = 0;\n+\tint rc;\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\t/* Probe device for media operations zero support */\n+\tcomp_ctx->media_ops_supported =\n+\t\tcxl_probe_media_ops_zero(comp_ctx->mbox,\n+\t\t\t\t\t &cxlmd->dev);\n+\n+\tdev_info(&cxlmd->dev, \"compression attach: looking for regions\\n\");\n+\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) == CXL_PARTMODE_RAM) {\n+\t\t\trc = convert_region_to_sysram(regions[i], pdev);\n+\t\t\tif (rc)\n+\t\t\t\terrors++;\n+\t\t\telse\n+\t\t\t\tconverted++;\n+\t\t}\n+\t\tput_device(cxl_region_dev(regions[i]));\n+\t}\n+\n+\tif (converted > 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"converted %d regions to sysram (%d errors)\\n\",\n+\t\t\t converted, errors);\n+\t\treturn errors ? -EIO : 0;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"could not create RAM region: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = convert_region_to_sysram(cxlr, pdev);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to convert region to sysram: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tcomp_ctx->cxlr = cxlr;\n+\n+\tdev_info(&cxlmd->dev, \"created and converted region %s to sysram\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_compression_attach = {\n+\t.probe = cxl_compression_attach_probe,\n+};\n+\n+static int cxl_compression_probe(struct pci_dev *pdev,\n+\t\t\t\t const struct pci_device_id *id)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probing device\\n\");\n+\n+\tcomp_ctx = devm_kzalloc(&pdev->dev, sizeof(*comp_ctx), GFP_KERNEL);\n+\tif (!comp_ctx)\n+\t\treturn -ENOMEM;\n+\tcomp_ctx->nid = NUMA_NO_NODE;\n+\tcomp_ctx->pdev = pdev;\n+\n+\trc = xa_insert(&comp_ctx_xa, (unsigned long)pdev, comp_ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_compression_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&comp_ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_compression_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = xa_erase(&comp_ctx_xa,\n+\t\t\t\t\t\t\t(unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: removing device\\n\");\n+\n+\tif (!comp_ctx || comp_ctx->nid == NUMA_NO_NODE)\n+\t\treturn;\n+\n+\t/*\n+\t * Destroy the region, devm actions on the region device handle teardown\n+\t * in registration-reverse order:\n+\t *   1. pre_teardown:  cram_unregister + retry-forever memory offline\n+\t *   2. sysram_unregister: device_unregister (sysram->res is NULL\n+\t *      after pre_teardown, so cxl_sysram_release skips hotplug)\n+\t *   3. post_teardown: kthread stop, flush cleanup\n+\t *\n+\t * PCI MMIO is still live so CCI commands in post_teardown work.\n+\t */\n+\tif (comp_ctx->cxlr) {\n+\t\tcxl_destroy_region(comp_ctx->cxlr);\n+\t\tcomp_ctx->cxlr = NULL;\n+\t}\n+\n+\tif (comp_ctx->cxled) {\n+\t\tcxl_dpa_free(comp_ctx->cxled);\n+\t\tcomp_ctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_compression_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ /* terminate list */ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_compression_pci_tbl);\n+\n+static struct pci_driver cxl_compression_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_compression_pci_tbl,\n+\t.probe\t\t= cxl_compression_probe,\n+\t.remove\t\t= cxl_compression_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_compression_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Compression Memory Driver with SysRAM regions\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-28-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) expressed concern about adding more special-casing similar to ZONE_DEVICE, specifically mentioning the folio_managed_() stuff in mprotect.c",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concern",
                "special-casing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm concerned about adding more special-casing (similar to what we \nalready added for ZONE_DEVICE) all over the place.\n\nLike the whole folio_managed_() stuff in mprotect.c\n\nHaving that said, sounds like a reasonable topic to discuss.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "c10400db-2259-4465-a07e-19d0691101a4@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged a concern about the semantics of zone_device hooks not being suitable for this case, proposed two alternative solutions: reusing vma_wants_writenotify() or adding a new hook to page table code, and offered to try one of these alternatives in a future version.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "offered to try an alternative"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It's a valid concern - and is why I tried to re-use as many of the\nzone_device hooks as possible.  It does not seem zone_device has quite\nthe same semantics for a case like this, so I had to make something new.\n\nDEVICE_COHERENT injects a temporary swap entry to allow the device to do\na large atomic operation - then the page table is restored and the CPU\nis free to change entries as it pleases.\n\nAnother option would be to add the hook to vma_wants_writenotify()\ninstead of the page table code - and mask MM_CP_TRY_CHANGE_WRITABLE.\n\nThis would require adding a vma flag - or maybe a count of protected /\ndevice pages.\n\nint mprotect_fixup() {\n    ...\n    if (vma_wants_manual_pte_write_upgrade(vma))\n        mm_cp_flags |= MM_CP_TRY_CHANGE_WRITABLE;\n}\n\nbool vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)\n{\n    if (vma->managed_wrprotect)\n        return true;\n}\n\nThat would localize the change in folio_managed_fixup_migration_pte() :\n\nstatic inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n                                                      pte_t pte,\n                                                      pte_t old_pte,\n                                                      struct vm_area_struct *vma)\n{\n    ...\n    } else if (folio_managed_wrprotect(page_folio(new))) {\n        pte = pte_wrprotect(pte);\n+       atomic_inc(&vma->managed_wrprotect);\n    }\n    return pte;\n}\n\nThis would cover both the huge_memory.c and mprotect, and maybe that's\njust generally cleaner? I can try that to see if it actually works.\n\n~Gregory",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "message_id": "aZxqP7J1kOClQUPQ@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author acknowledged that their patch was unnecessary because existing hooks can be used for write protection, and they plan to clean up the code in a future version.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for fix",
                "planned cleanup"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "scratch all this - existing hooks exist for exactly this purpose:\n\n\tcan_change_[pte|pmd]_writable()\n\nSurprised I missed this.\n\nI can clean this up to remove it from the page table walks.\n\nStill valid to question whether we want this, but at least the hook\nlives with other write-protect hooks now.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "aZx7hsVNU0XOCCiG@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Alistair Popple",
              "summary": "Reviewer Alistair Popple questioned the necessity of N_MEMORY_PRIVATE, suggesting that existing ZONE_DEVICE implementations could be adapted or reused instead, and expressed skepticism about reusing the mm buddy allocator as a primary motivator.\n\nReviewer Alistair Popple noted that the patch provides a standard interface to userspace for managing device memory, suggesting it as one of the key features and implying that existing NUMA APIs are suitable for this purpose.\n\nReviewer Alistair Popple noted that the proposed private memory nodes mechanism is similar to ZONE_DEVICE and questioned why it cannot be extended instead of duplicating code, pointing out a potential lock ordering issue with reclaim paths.\n\nThe reviewer, Alistair Popple, expressed concerns that the proposed solution for private memory nodes is redundant and unnecessary, suggesting that it's better to build upon existing ZONE_DEVICE methods rather than introducing a new feature set.\n\nReviewer Alistair Popple noted that the patch introduces a large number of hooks, similar to those in ZONE_DEVICE, and expressed concern about code duplication.\n\nReviewer Alistair Popple questioned whether the mm allocator is necessary for private memory node allocation, suggesting that a device allocator library could be written or reused from drm_buddy.c\n\nReviewer Alistair Popple questioned the patch's focus on ZONE_DEVICE pages, suggesting that the actual limitations being addressed may be related to getting these pages into an LRU or other issues.\n\nReviewer suggested extending ZONE_DEVICE_COHERENT to support the use case, proposing that adding a few dev_pagemap_ops and allowing it to be on the LRU would achieve the desired functionality.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "skepticism",
                "requested changes",
                "no clear signal",
                "suggested alternative solution",
                "concerns about code duplication",
                "expressed interest in implementation details",
                "questioning",
                "clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Having had to re-implement entire portions of mm/ in a driver I agree this isn't\nsomething anyone sane should do :-) However aspects of ZONE_DEVICE were added\nprecisely to help with that so I'm not sure N_MEMORY_PRIVATE is the only or best\nway to do that.\n\nBased on our discussion at LPC I believe one of the primary motivators here was\nto re-use the existing mm buddy allocator rather than writing your own. I remain\nto be convinced that alone is justification enough for doing all this - DRM for\nexample already has quite a nice standalone buddy allocator (drm_buddy.c) that\ncould presumably be used, or adapted for use, by any device driver.\n\nThe interesting part of this series (which I have skimmed but not read in\ndetail) is how device memory gets exposed to userspace - this is something that\nexisting ZONE_DEVICE implementations don't address, instead leaving it up to\ndrivers and associated userspace stacks to deal with allocation, migration, etc.\n\n---\n\nThis is I think is one of the key things that should be enabled - providing a\nstandard interface to userspace for managing device memory. The existing NUMA\nAPIs do seem like a reasonable way to do this.\n\n---\n\nOne does not have to squint too hard to see that the above is not so different\nfrom what ZONE_DEVICE provides today via dev_pagemap_ops(). So I think I think\nit would be worth outlining why the existing ZONE_DEVICE mechanism can't be\nextended to provide these kind of services.\n\nThis seems to add a bunch of code just to use NODE_DATA instead of page->pgmap,\nwithout really explaining why just extending dev_pagemap_ops wouldn't work. The\nobvious reason is that if you want to support things like reclaim, compaction,\netc. these pages need to be on the LRU, which is a little bit hard when that\nfield is also used by the pgmap pointer for ZONE_DEVICE pages.\n\nBut it might be good to explore other options for storing the pgmap - for\nexample page_ext could be used.  Or I hear struct page may go away in place of\nfolios any day now, so maybe that gives us space for both :-)\n\n---\n\nThe above also looks pretty similar to the existing ZONE_DEVICE methods for\ndoing this which is another reason to argue for just building up the feature set\nof the existing boondoggle rather than adding another thingymebob.\n\nIt seems the key thing we are looking for is:\n\n1) A userspace API to allocate/manage device memory (ie. move_pages(), mbind(),\netc.)\n\n2) Allowing reclaim/LRU list processing of device memory.\n\n---\n\ndiscussion (hopefully I can make it to LSFMM). Mostly I'm interested in the\nimplementation as this does on the surface seem to sprinkle around and duplicate\na lot of hooks similar to what ZONE_DEVICE already provides.\n\n---\n\nFor basic allocation I agree this is the case. But there's no reason some device\nallocator library couldn't be written. Or in fact as pointed out above reuse the\nalready existing one in drm_buddy.c.  So would be interested to hear arguments\nfor why allocation has to be done by the mm allocator and/or why an allocation\nlibrary wouldn't work here given DRM already has them.\n\n---\n\nZONE_DEVICE pages are in fact real struct pages, but I will concede that\nperspective probably depends on which bits of the mm you play in. The real\nlimitations you seem to be addressing is more around how we get these pages in\nan LRU, or are there other limitations?\n\n---\n\nWhat I'd like to explore is why ZONE_DEVICE_COHERENT couldn't just be extended\nto support your usecase? It seems a couple of extra dev_pagemap_ops and being\nable to go on the LRU would get you there.\n\n - Alistair",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "fzy6f6dpv3oq3ksr2mkst7pz3daeb3buhuvdvcw4633pcl7h6u@mxjgiwpg5acv",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledges that using ZONE_DEVICE is not necessary, as N_MEMORY_PRIVATE can be achieved by reusing the buddy allocator, which would simplify the code and eliminate unnecessary complexity.\n\nAuthor explained that the callback similarity between ZONE_DEVICE and private nodes is intentional, due to the need for similar hooks in both cases, but with different default directions.\n\nAuthor responded to feedback about per-page pgmap and device-to-node mappings, agreeing that NODE_DATA is a better direction and suggesting that one driver can manage multiple devices with the same numa node using the same owner context.\n\nThe author is addressing concerns about implementing mempolicy support for N_MEMORY_PRIVATE, specifically how to handle ZONE_DEVICE NUMA UAPI and the lack of LRU support. The author acknowledges that getting mempolicy to work requires adding code to vma_alloc_folio_noprof and implies that using the buddy is a simpler solution than re-inventing wheel-like functionality.\n\nThe author acknowledged Alistair's feedback that using the buddy allocator underpins the rest of mm/ services, and explained that this is a deliberate design choice to avoid injecting hooks into every surface that touches the buddy.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a fix is needed",
                "clarification",
                "explanation",
                "agreed",
                "no reason",
                "acknowledges fix needed",
                "implies complexity",
                "acknowledged",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree that buddy-access alone is insufficient justification, it\nstarted off that way - but if you want mempolicy/NUMA UAPI access,\nit turns into \"Re-use all of MM\" - and that means using the buddy.\n\nI also expected ZONE_DEVICE vs NODE_DATA to be the primary discussion,\n\nI raise replacing it as a thought experiment, but not the proposal.\n\nThe idea that drm/ is going to switch to private nodes is outside the\nrealm of reality, but part of that is because of years of infrastructure\nbuilt on the assumption that re-using mm/ is infeasible.\n\nBut, lets talk about DEVICE_COHERENT\n\n---\n\nDEVICE_COHERENT is the odd-man out among ZONE_DEVICE modes. The others\nuse softleaf entries and don't allow direct mappings.\n\n(DEVICE_PRIVATE sort of does if you squint, but you can also view that\n a bit like PROT_NONE or read-only controls to force migrations).\n\nIf you take DEVICE_COHERENT and:\n\n- Move pgmap out of the struct page (page_ext, NODE_DATA, etc) to free\n  the LRU list_head\n- Put pages in the buddy (free lists, watermarks, managed_pages) or add\n  pgmap->device_alloc() at every allocation callsite / buddy hook\n- Add LRU support (aging, reclaim, compaction)\n- Add isolated gating (new GFP flag and adjusted zonelist filtering)\n- Add new dev_pagemap_ops callbacks for the various mm/ features\n- Audit evey folio_is_zone_device() to distinguish zone device modes\n\n... you've built N_MEMORY_PRIVATE inside ZONE_DEVICE. Except now\npage_zone(page) returns ZONE_DEVICE - so you inherit the wrong\ndefaults at every existing ZONE_DEVICE check. \n\nSkip-sites become things to opt-out of instead of opting into.\n\nYou just end up with\n\nif (folio_is_zone_device(folio))\n    if (folio_is_my_special_zone_device())\n    else ....\n\nand this just generalizes to\n\nif (folio_is_private_managed(folio))\n    folio_managed_my_hooked_operation()\n\nSo you get the same code, but have added more complexity to ZONE_DEVICE.\n\nI don't think that's needed if we just recognize ZONE is the wrong\nabstraction to be operating on.\n\nHonestly, even ZONE_MOVABLE becomes pointless with N_MEMORY_PRIVATE\nif you disallow longterm pinning - because the managing service handles\nallocations (it has to inject GFP_PRIVATE to get access) or selectively\nenables the mm/ services it knows are safe (mempolicy).\n\nEven if you allow longterm pinning, if your service controls what does\nthe pinning it can still be reclaimable - just manually (killing\nprocesses) instead of letting hotplug do it via migration.\n\nIf your service only allocates movable pages - your ZONE_NORMAL is\neffectively ZONE_MOVABLE.  \n\nIn some cases we use ZONE_MOVABLE to prevent the kernel from allocating\nmemory onto devices (like CXL).  This means struct page is forced to\ntake up DRAM or use memmap_on_memory - meaning you lose high-value\ncapacity or sacrifice contiguity (less huge page support).\n\nThis entire problem can evaporate if you can just use ZONE_NORMAL.\n\nThere are a lot of benefits to just re-using the buddy like this.\n\nZones are the wrong abstraction and cause more problems.\n\n---\n\nYou don't have to squint because it was deliberate :]\n\nThe callback similarity is the feature - they're the same logical\noperations.  The difference is the direction of the defaults.\n\nExtending ZONE_DEVICE into these areas requires the same set of hooks,\nplus distinguishing \"old ZONE_DEVICE\" from \"new ZONE_DEVICE\".\n\nWhere there are new injection sites, it's because ZONE_DEVICE opts\nout of ever touching that code in some other silently implied way.\n\nFor example, reclaim/compaction doesn't run because ZONE_DEVICE doesn't\nadd to managed_pages (among other reasons).\n\nYou'd have to go figure out how to hack those things into ZONE_DEVICE \n*and then* opt every *other* ZONE_DEVICE mode *back out*.\n\nSo you still end up with something like this anyway:\n\nstatic inline bool folio_managed_handle_fault(struct folio *folio,\n                                              struct vm_fault *vmf,\n                                              enum pgtable_level level,\n                                              vm_fault_t *ret)\n{\n        /* Zone device pages use swap entries; handled in do_swap_page */\n        if (folio_is_zone_device(folio))\n                return false;\n\n        if (folio_is_private_node(folio))\n\t\t...\n        return false;\n}\n\n---\n\nIf NUMA is the interface we want, then NODE_DATA is the right direction\nregardless of struct page's future or what zone it lives in.\n\nThere's no reason to keep per-page pgmap w/ device-to-node mappings.\n\nYou can have one driver manage multiple devices with the same numa node\nif it uses the same owner context (PFN already differentiates devices).\n\nThe existing code allows for this.\n\n---\n\nOn (1): ZONE_DEVICE NUMA UAPI is harder than it looks from the surface\n\nMuch of the kernel mm/ infrastructure is written on top of the buddy and\nexpects N_MEMORY to be the sole arbiter of \"Where to Acquire Pages\".\n\nMempolicy depends on:\n   - Buddy support or a new alloc hook around the buddy\n\n   - Migration support (mbind() after allocation migrates)\n     - Migration also deeply assumes buddy and LRU support\n\n   - Changing validations on node states\n     - mempolicy checks N_MEMORY membership, so you have to hack\n       N_MEMORY onto ZONE_DEVICE\n       (or teach it about a new node state... N_MEMORY_PRIVATE)\n\n\nGetting mempolicy to work with N_MEMORY_PRIVATE amounts to adding 2\nlines of code in vma_alloc_folio_noprof:\n\nstruct folio *vma_alloc_folio_noprof(gfp_t gfp, int order,\n                                     struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr)\n{\n        if (pol->flags & MPOL_F_PRIVATE)\n                gfp |= __GFP_PRIVATE;\n\n        folio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n\t/* Woo! I faulted a DEVICE PAGE! */\n}\n\nBut this requires the pages to be managed by the buddy.\n\nThe rest of the mempolicy support is around keeping sane nodemasks when\nthings like cpuset.mems rebinds occur and validating you don't end up\nwith private nodes that don't support mempolicy in your nodemask.\n\nYou have to do all of this anyway, but with the added bonus of fighting\nwith the overloaded nature of ZONE_DEVICE at every step.\n\n==========\n\nOn (2): Assume you solve LRU. \n\nZone Device has no free lists, managed_pages, or watermarks.\n\nkswapd can't run, compaction has no targets, vmscan's pressure model\ndoesn't function.  These all come for free when the pages are\nbuddy-managed on a real zone.  Why re-invent the wheel?\n\n==========\n\nSo you really have two options here:\n\na) Put pages in the buddy, or\n\nb) Add pgmap->device_alloc() callbacks at every allocation site that\n   could target a node:\n     - vma_alloc_folio\n     - alloc_migration_target\n     - alloc_demote_folio\n     - alloc_pages_node\n     - alloc_contig_pages\n     - list goes on\n\nOr more likely - hooking get_page_from_freelist.  Which at that\npoint... just use the buddy?  You're already deep in the hot path.\n\n---\n\nUsing the buddy underpins the rest of mm/ services we want to re-use.\n\nThat's basically it.  Otherwise you have to inject hooks into every\nsurface that touches the buddy...\n\n... or in the buddy (get_page_from_freelist), at which point why not\njust use the buddy?\n\n~Gregory",
              "reply_to": "Alistair Popple",
              "message_date": "2026-02-24",
              "message_id": "aZ3BEn_73Rk8Fn7L@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author is considering an alternative approach to implement private memory isolation, which would eliminate the need for introducing N_MEMORY_PRIVATE and instead rely on checking NODE_DATA(target_nid)->private.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "considering alternative",
                "looking at it a bit more"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This gave me something to chew on\n\nI think this can be done without introducing N_MEMORY_PRIVATE and just\nchecking:   NODE_DATA(target_nid)->private\n\nmeaning these nodes can just be N_MEMORY with the same isolations.\n\nI'll look at this a bit more.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "aZ3X3Jni0HZXZMVl@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH v5 00/10] mm: Hot page tracking and promotion infrastructure",
          "message_id": "aZxsBifRchLn2m42@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZxsBifRchLn2m42@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T15:02:36Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed a concern about the PTL lock being held when calling migrate_misplaced_folio_prepare() from a kernel thread context, and explained that allowing NULL VMA will not require holding the PTL lock in this case.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "We want isolation of misplaced folios to work in contexts\nwhere VMA isn't available, typically when performing migrations\nfrom a kernel thread context. In order to prepare for that,\nallow migrate_misplaced_folio_prepare() to be called with\na NULL VMA.\n\nWhen migrate_misplaced_folio_prepare() is called with non-NULL\nVMA, it will check if the folio is mapped shared and that requires\nholding PTL lock. This path isn't taken when the function is\ninvoked with NULL VMA (migration outside of process context).\nTherefore, when VMA == NULL, migrate_misplaced_folio_prepare()\ndoes not require the caller to hold the PTL.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n mm/migrate.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 5169f9717f60..70f8f3ad4fd8 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -2652,7 +2652,8 @@ static struct folio *alloc_misplaced_dst_folio(struct folio *src,\n \n /*\n  * Prepare for calling migrate_misplaced_folio() by isolating the folio if\n- * permitted. Must be called with the PTL still held.\n+ * permitted. Must be called with the PTL still held if called with a non-NULL\n+ * vma.\n  */\n int migrate_misplaced_folio_prepare(struct folio *folio,\n \t\tstruct vm_area_struct *vma, int node)\n@@ -2669,7 +2670,7 @@ int migrate_misplaced_folio_prepare(struct folio *folio,\n \t\t * See folio_maybe_mapped_shared() on possible imprecision\n \t\t * when we cannot easily detect if a folio is shared.\n \t\t */\n-\t\tif ((vma->vm_flags & VM_EXEC) && folio_maybe_mapped_shared(folio))\n+\t\tif (vma && (vma->vm_flags & VM_EXEC) && folio_maybe_mapped_shared(folio))\n \t\t\treturn -EACCES;\n \n \t\t/*\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "20260129144043.231636-2-bharata@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed a concern about the inefficiency of migrate_misplaced_folio() handling only one folio per call, and introduced a new function migrate_misplaced_folios_batch() that leverages migrate_pages() internally for improved performance. The caller must isolate folios beforehand using migrate_misplaced_folio_prepare().",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "improved performance"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Gregory Price <gourry@gourry.net>\n\nTiered memory systems often require migrating multiple folios at once.\nCurrently, migrate_misplaced_folio() handles only one folio per call,\nwhich is inefficient for batch operations. This patch introduces\nmigrate_misplaced_folios_batch(), a batch variant that leverages\nmigrate_pages() internally for improved performance.\n\nThe caller must isolate folios beforehand using\nmigrate_misplaced_folio_prepare(). On return, the folio list will be\nempty regardless of success or failure.\n\nThis function will be used by pghot kmigrated thread.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n[Rewrote commit description]\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n include/linux/migrate.h |  6 ++++++\n mm/migrate.c            | 36 ++++++++++++++++++++++++++++++++++++\n 2 files changed, 42 insertions(+)\n\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 26ca00c325d9..f28326b88592 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -103,6 +103,7 @@ static inline int set_movable_ops(const struct movable_operations *ops, enum pag\n int migrate_misplaced_folio_prepare(struct folio *folio,\n \t\tstruct vm_area_struct *vma, int node);\n int migrate_misplaced_folio(struct folio *folio, int node);\n+int migrate_misplaced_folios_batch(struct list_head *folio_list, int node);\n #else\n static inline int migrate_misplaced_folio_prepare(struct folio *folio,\n \t\tstruct vm_area_struct *vma, int node)\n@@ -113,6 +114,11 @@ static inline int migrate_misplaced_folio(struct folio *folio, int node)\n {\n \treturn -EAGAIN; /* can't migrate now */\n }\n+static inline int migrate_misplaced_folios_batch(struct list_head *folio_list,\n+\t\t\t\t\t\t int node)\n+{\n+\treturn -EAGAIN; /* can't migrate now */\n+}\n #endif /* CONFIG_NUMA_BALANCING */\n \n #ifdef CONFIG_MIGRATION\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 70f8f3ad4fd8..4a3a9a4ff435 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -2747,5 +2747,41 @@ int migrate_misplaced_folio(struct folio *folio, int node)\n \tBUG_ON(!list_empty(&migratepages));\n \treturn nr_remaining ? -EAGAIN : 0;\n }\n+\n+/**\n+ * migrate_misplaced_folios_batch() - Batch variant of migrate_misplaced_folio.\n+ * Attempts to migrate a folio list to the specified destination.\n+ * @folio_list: Isolated list of folios to be batch-migrated.\n+ * @node: The NUMA node ID to where the folios should be migrated.\n+ *\n+ * Caller is expected to have isolated the folios by calling\n+ * migrate_misplaced_folio_prepare(), which will result in an\n+ * elevated reference count on the folio.\n+ *\n+ * This function will un-isolate the folios, drop the elevated reference\n+ * and remove them from the list before returning.\n+ *\n+ * Return: 0 on success and -EAGAIN on failure or partial migration.\n+ *         On return, @folio_list will be empty regardless of success/failure.\n+ */\n+int migrate_misplaced_folios_batch(struct list_head *folio_list, int node)\n+{\n+\tpg_data_t *pgdat = NODE_DATA(node);\n+\tunsigned int nr_succeeded = 0;\n+\tint nr_remaining;\n+\n+\tnr_remaining = migrate_pages(folio_list, alloc_misplaced_dst_folio,\n+\t\t\t\t     NULL, node, MIGRATE_ASYNC,\n+\t\t\t\t     MR_NUMA_MISPLACED, &nr_succeeded);\n+\tif (nr_remaining)\n+\t\tputback_movable_pages(folio_list);\n+\n+\tif (nr_succeeded) {\n+\t\tcount_vm_numa_events(NUMA_PAGE_MIGRATE, nr_succeeded);\n+\t\tmod_node_page_state(pgdat, PGPROMOTE_SUCCESS, nr_succeeded);\n+\t}\n+\tWARN_ON(!list_empty(folio_list));\n+\treturn nr_remaining ? -EAGAIN : 0;\n+}\n #endif /* CONFIG_NUMA_BALANCING */\n #endif /* CONFIG_NUMA */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "20260129144043.231636-3-bharata@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "Author addressed a concern about the hotness record size and layout, explaining that it uses a u8 variable (1 byte) for default mode and 4 bytes in precision mode, and provided details on how the hotness information is stored and accessed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This introduces a subsystem for collecting memory access\ninformation from different sources. It maintains the hotness\ninformation based on the access history and time of access.\n\nAdditionally, it provides per-lower-tier-node kernel threads\n(named kmigrated) that periodically promote the pages that\nare eligible for promotion.\n\nSub-systems that generate hot page access info can report that\nusing this API:\n\nint pghot_record_access(unsigned long pfn, int nid, int src,\n                        unsigned long time)\n\n@pfn: The PFN of the memory accessed\n@nid: The accessing NUMA node ID\n@src: The temperature source (subsystem) that generated the\n      access info\n@time: The access time in jiffies\n\nSome temperature sources may not provide the nid from which\nthe page was accessed. This is true for sources that use\npage table scanning for PTE Accessed bit. For such sources,\na configurable/default toptier node is used as promotion\ntarget.\n\nThe hotness information is stored for every page of lower\ntier memory in a u8 variable (1 byte) that is part of\nmem_section data structure.\n\nkmigrated is a per-lower-tier-node kernel thread that migrates\nthe folios marked for migration in batches. Each kmigrated\nthread walks the PFN range spanning its node and checks\nfor potential migration candidates.\n\nA bunch of tunables for enabling different hotness sources,\nsetting target_nid, frequency threshold are provided in debugfs.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n Documentation/admin-guide/mm/pghot.txt |  84 ++++++\n include/linux/mmzone.h                 |  21 ++\n include/linux/pghot.h                  |  94 +++++++\n include/linux/vm_event_item.h          |   6 +\n mm/Kconfig                             |  14 +\n mm/Makefile                            |   1 +\n mm/mm_init.c                           |  10 +\n mm/pghot-default.c                     |  73 +++++\n mm/pghot-tunables.c                    | 189 +++++++++++++\n mm/pghot.c                             | 370 +++++++++++++++++++++++++\n mm/vmstat.c                            |   6 +\n 11 files changed, 868 insertions(+)\n create mode 100644 Documentation/admin-guide/mm/pghot.txt\n create mode 100644 include/linux/pghot.h\n create mode 100644 mm/pghot-default.c\n create mode 100644 mm/pghot-tunables.c\n create mode 100644 mm/pghot.c\n\ndiff --git a/Documentation/admin-guide/mm/pghot.txt b/Documentation/admin-guide/mm/pghot.txt\nnew file mode 100644\nindex 000000000000..01291b72e7ab\n--- /dev/null\n+++ b/Documentation/admin-guide/mm/pghot.txt\n@@ -0,0 +1,84 @@\n+.. SPDX-License-Identifier: GPL-2.0\n+\n+=================================\n+PGHOT: Hot Page Tracking Tunables\n+=================================\n+\n+Overview\n+========\n+The PGHOT subsystem tracks frequently accessed pages in lower-tier memory and\n+promotes them to faster tiers. It uses per-PFN hotness metadata and asynchronous\n+migration via per-node kernel threads (kmigrated).\n+\n+This document describes tunables available via **debugfs** and **sysctl** for\n+PGHOT.\n+\n+Debugfs Interface\n+=================\n+Path: /sys/kernel/debug/pghot/\n+\n+1. **enabled_sources**\n+   - Bitmask to enable/disable hotness sources.\n+   - Bits:\n+     - 0: Hardware hints (value 0x1)\n+     - 1: Page table scan (value 0x2)\n+     - 2: Hint faults (value 0x4)\n+   - Default: 0 (disabled)\n+   - Example:\n+     # echo 0x7 > /sys/kernel/debug/pghot/enabled_sources\n+     Enables all sources.\n+\n+2. **target_nid**\n+   - Toptier NUMA node ID to which hot pages should be promoted when source\n+     does not provide nid. Used when hotness source can't provide accessing\n+     NID or when the tracking mode is default.\n+   - Default: 0\n+   - Example:\n+     # echo 1 > /sys/kernel/debug/pghot/target_nid\n+\n+3. **freq_threshold**\n+   - Minimum access frequency before a page is marked ready for promotion.\n+   - Range: 1 to 3\n+   - Default: 2\n+   - Example:\n+     # echo 3 > /sys/kernel/debug/pghot/freq_threshold\n+\n+4. **kmigrated_sleep_ms**\n+   - Sleep interval (ms) for kmigrated thread between scans.\n+   - Default: 100\n+\n+5. **kmigrated_batch_nr**\n+   - Maximum number of folios migrated in one batch.\n+   - Default: 512\n+\n+Sysctl Interface\n+================\n+1. pghot_promote_freq_window_ms\n+\n+Path: /proc/sys/vm/pghot_promote_freq_window_ms\n+\n+- Controls the time window (in ms) for counting access frequency. A page is\n+  considered hot only when **freq_threshold** number of accesses occur with\n+  this time period.\n+- Default: 4000 (4 seconds)\n+- Example:\n+  # sysctl vm.pghot_promote_freq_window_ms=3000\n+\n+Vmstat Counters\n+===============\n+Following vmstat counters provide some stats about pghot subsystem.\n+\n+Path: /proc/vmstat\n+\n+1. **pghot_recorded_accesses**\n+   - Number of total hot page accesses recorded by pghot.\n+\n+2. **pghot_recorded_hwhints**\n+   - Number of recorded accesses reported by hwhints source.\n+\n+3. **pghot_recorded_pgtscans**\n+   - Number of recorded accesses reported by PTE A-bit based source.\n+\n+4. **pghot_recorded_hintfaults**\n+   - Number of recorded accesses reported by NUMA Balancing based\n+     hotness source.\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 75ef7c9f9307..22e08befb096 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1064,6 +1064,7 @@ enum pgdat_flags {\n \t\t\t\t\t * many pages under writeback\n \t\t\t\t\t */\n \tPGDAT_RECLAIM_LOCKED,\t\t/* prevents concurrent reclaim */\n+\tPGDAT_KMIGRATED_ACTIVATE,\t/* activates kmigrated */\n };\n \n enum zone_flags {\n@@ -1518,6 +1519,10 @@ typedef struct pglist_data {\n #ifdef CONFIG_MEMORY_FAILURE\n \tstruct memory_failure_stats mf_stats;\n #endif\n+#ifdef CONFIG_PGHOT\n+\tstruct task_struct *kmigrated;\n+\twait_queue_head_t kmigrated_wait;\n+#endif\n } pg_data_t;\n \n #define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n@@ -1916,12 +1921,28 @@ struct mem_section {\n \tunsigned long section_mem_map;\n \n \tstruct mem_section_usage *usage;\n+#ifdef CONFIG_PGHOT\n+\t/*\n+\t * Per-PFN hotness data for this section.\n+\t * Array of phi_t (u8 in default mode).\n+\t * LSB is used as PGHOT_SECTION_HOT_BIT flag.\n+\t */\n+\tvoid *hot_map;\n+#endif\n #ifdef CONFIG_PAGE_EXTENSION\n \t/*\n \t * If SPARSEMEM, pgdat doesn't have page_ext pointer. We use\n \t * section. (see page_ext.h about this.)\n \t */\n \tstruct page_ext *page_ext;\n+#endif\n+\t/*\n+\t * Padding to maintain consistent mem_section size when exactly\n+\t * one of PGHOT or PAGE_EXTENSION is enabled. This ensures\n+\t * optimal alignment regardless of configuration.\n+\t */\n+#if (defined(CONFIG_PGHOT) && !defined(CONFIG_PAGE_EXTENSION)) || \\\n+\t\t(!defined(CONFIG_PGHOT) && defined(CONFIG_PAGE_EXTENSION))\n \tunsigned long pad;\n #endif\n \t/*\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nnew file mode 100644\nindex 000000000000..88e57aab697b\n--- /dev/null\n+++ b/include/linux/pghot.h\n@@ -0,0 +1,94 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_PGHOT_H\n+#define _LINUX_PGHOT_H\n+\n+/* Page hotness temperature sources */\n+enum pghot_src {\n+\tPGHOT_HW_HINTS,\n+\tPGHOT_PGTABLE_SCAN,\n+\tPGHOT_HINT_FAULT,\n+};\n+\n+#ifdef CONFIG_PGHOT\n+#include <linux/static_key.h>\n+\n+extern unsigned int pghot_target_nid;\n+extern unsigned int pghot_src_enabled;\n+extern unsigned int pghot_freq_threshold;\n+extern unsigned int kmigrated_sleep_ms;\n+extern unsigned int kmigrated_batch_nr;\n+extern unsigned int sysctl_pghot_freq_window;\n+\n+void pghot_debug_init(void);\n+\n+DECLARE_STATIC_KEY_FALSE(pghot_src_hwhints);\n+DECLARE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n+DECLARE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n+\n+/*\n+ * Bit positions to enable individual sources in pghot/records_enabled\n+ * of debugfs.\n+ */\n+enum pghot_src_enabled {\n+\tPGHOT_HWHINTS_BIT = 0,\n+\tPGHOT_PGTSCAN_BIT,\n+\tPGHOT_HINTFAULT_BIT,\n+\tPGHOT_MAX_BIT\n+};\n+\n+#define PGHOT_HWHINTS_ENABLED\t\tBIT(PGHOT_HWHINTS_BIT)\n+#define PGHOT_PGTSCAN_ENABLED\t\tBIT(PGHOT_PGTSCAN_BIT)\n+#define PGHOT_HINTFAULT_ENABLED\t\tBIT(PGHOT_HINTFAULT_BIT)\n+#define PGHOT_SRC_ENABLED_MASK\t\tGENMASK(PGHOT_MAX_BIT - 1, 0)\n+\n+#define PGHOT_DEFAULT_FREQ_THRESHOLD\t2\n+\n+#define KMIGRATED_DEFAULT_SLEEP_MS\t100\n+#define KMIGRATED_DEFAULT_BATCH_NR\t512\n+\n+#define PGHOT_DEFAULT_NODE\t\t0\n+\n+#define PGHOT_DEFAULT_FREQ_WINDOW\t(4 * MSEC_PER_SEC)\n+\n+/*\n+ * Bits 0-6 are used to store frequency and time.\n+ * Bit 7 is used to indicate the page is ready for migration.\n+ */\n+#define PGHOT_MIGRATE_READY\t\t7\n+\n+#define PGHOT_FREQ_WIDTH\t\t2\n+/* Bucketed time is stored in 5 bits which can represent up to 4s with HZ=1000 */\n+#define PGHOT_TIME_BUCKETS_WIDTH\t7\n+#define PGHOT_TIME_WIDTH\t\t5\n+#define PGHOT_NID_WIDTH\t\t\t10\n+\n+#define PGHOT_FREQ_SHIFT\t\t0\n+#define PGHOT_TIME_SHIFT\t\t(PGHOT_FREQ_SHIFT + PGHOT_FREQ_WIDTH)\n+\n+#define PGHOT_FREQ_MASK\t\t\tGENMASK(PGHOT_FREQ_WIDTH - 1, 0)\n+#define PGHOT_TIME_MASK\t\t\tGENMASK(PGHOT_TIME_WIDTH - 1, 0)\n+#define PGHOT_TIME_BUCKETS_MASK\t\t(PGHOT_TIME_MASK << PGHOT_TIME_BUCKETS_WIDTH)\n+\n+#define PGHOT_NID_MAX\t\t\t((1 << PGHOT_NID_WIDTH) - 1)\n+#define PGHOT_FREQ_MAX\t\t\t((1 << PGHOT_FREQ_WIDTH) - 1)\n+#define PGHOT_TIME_MAX\t\t\t((1 << PGHOT_TIME_WIDTH) - 1)\n+\n+typedef u8 phi_t;\n+\n+#define PGHOT_RECORD_SIZE\t\tsizeof(phi_t)\n+\n+#define PGHOT_SECTION_HOT_BIT\t\t0\n+#define PGHOT_SECTION_HOT_MASK\t\tBIT(PGHOT_SECTION_HOT_BIT)\n+\n+unsigned long pghot_access_latency(unsigned long old_time, unsigned long time);\n+bool pghot_update_record(phi_t *phi, int nid, unsigned long now);\n+int pghot_get_record(phi_t *phi, int *nid, int *freq, unsigned long *time);\n+\n+int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now);\n+#else\n+static inline int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now)\n+{\n+\treturn 0;\n+}\n+#endif /* CONFIG_PGHOT */\n+#endif /* _LINUX_PGHOT_H */\ndiff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\nindex 92f80b4d69a6..5b8fd93b55fd 100644\n--- a/include/linux/vm_event_item.h\n+++ b/include/linux/vm_event_item.h\n@@ -188,6 +188,12 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n \t\tKSTACK_REST,\n #endif\n #endif /* CONFIG_DEBUG_STACK_USAGE */\n+#ifdef CONFIG_PGHOT\n+\t\tPGHOT_RECORDED_ACCESSES,\n+\t\tPGHOT_RECORD_HWHINTS,\n+\t\tPGHOT_RECORD_PGTSCANS,\n+\t\tPGHOT_RECORD_HINTFAULTS,\n+#endif /* CONFIG_PGHOT */\n \t\tNR_VM_EVENT_ITEMS\n };\n \ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex bd0ea5454af8..f4f0147faac5 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -1464,6 +1464,20 @@ config PT_RECLAIM\n config FIND_NORMAL_PAGE\n \tdef_bool n\n \n+config PGHOT\n+\tbool \"Hot page tracking and promotion\"\n+\tdef_bool n\n+\tdepends on NUMA && MIGRATION && SPARSEMEM && MMU\n+\thelp\n+\t  A sub-system to track page accesses in lower tier memory and\n+\t  maintain hot page information. Promotes hot pages from lower\n+\t  tiers to top tier by using the memory access information provided\n+\t  by various sources. Asynchronous promotion is done by per-node\n+\t  kernel threads.\n+\n+\t  This adds 1 byte of metadata overhead per page in lower-tier\n+\t  memory nodes.\n+\n source \"mm/damon/Kconfig\"\n \n endmenu\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 2d0570a16e5b..655a27f3a215 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -147,3 +147,4 @@ obj-$(CONFIG_SHRINKER_DEBUG) += shrinker_debug.o\n obj-$(CONFIG_EXECMEM) += execmem.o\n obj-$(CONFIG_TMPFS_QUOTA) += shmem_quota.o\n obj-$(CONFIG_PT_RECLAIM) += pt_reclaim.o\n+obj-$(CONFIG_PGHOT) += pghot.o pghot-tunables.o pghot-default.o\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex fc2a6f1e518f..64109feaa1c3 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -1401,6 +1401,15 @@ static void pgdat_init_kcompactd(struct pglist_data *pgdat)\n static void pgdat_init_kcompactd(struct pglist_data *pgdat) {}\n #endif\n \n+#ifdef CONFIG_PGHOT\n+static void pgdat_init_kmigrated(struct pglist_data *pgdat)\n+{\n+\tinit_waitqueue_head(&pgdat->kmigrated_wait);\n+}\n+#else\n+static inline void pgdat_init_kmigrated(struct pglist_data *pgdat) {}\n+#endif\n+\n static void __meminit pgdat_init_internals(struct pglist_data *pgdat)\n {\n \tint i;\n@@ -1410,6 +1419,7 @@ static void __meminit pgdat_init_internals(struct pglist_data *pgdat)\n \n \tpgdat_init_split_queue(pgdat);\n \tpgdat_init_kcompactd(pgdat);\n+\tpgdat_init_kmigrated(pgdat);\n \n \tinit_waitqueue_head(&pgdat->kswapd_wait);\n \tinit_waitqueue_head(&pgdat->pfmemalloc_wait);\ndiff --git a/mm/pghot-default.c b/mm/pghot-default.c\nnew file mode 100644\nindex 000000000000..e0a3b2ed2592\n--- /dev/null\n+++ b/mm/pghot-default.c\n@@ -0,0 +1,73 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * pghot: Default mode\n+ *\n+ * 1 byte hotness record per PFN.\n+ * Bucketed time and frequency tracked as part of the record.\n+ * Promotion to @pghot_target_nid by default.\n+ */\n+\n+#include <linux/pghot.h>\n+#include <linux/jiffies.h>\n+\n+/*\n+ * @time is regular time, @old_time is bucketed time.\n+ */\n+unsigned long pghot_access_latency(unsigned long old_time, unsigned long time)\n+{\n+\ttime &= PGHOT_TIME_BUCKETS_MASK;\n+\told_time <<= PGHOT_TIME_BUCKETS_WIDTH;\n+\n+\treturn jiffies_to_msecs((time - old_time) & PGHOT_TIME_BUCKETS_MASK);\n+}\n+\n+bool pghot_update_record(phi_t *phi, int nid, unsigned long now)\n+{\n+\tphi_t freq, old_freq, hotness, old_hotness, old_time;\n+\tphi_t time = now >> PGHOT_TIME_BUCKETS_WIDTH;\n+\n+\told_hotness = READ_ONCE(*phi);\n+\tdo {\n+\t\tbool new_window = false;\n+\n+\t\thotness = old_hotness;\n+\t\told_freq = (hotness >> PGHOT_FREQ_SHIFT) & PGHOT_FREQ_MASK;\n+\t\told_time = (hotness >> PGHOT_TIME_SHIFT) & PGHOT_TIME_MASK;\n+\n+\t\tif (pghot_access_latency(old_time, now) > sysctl_pghot_freq_window)\n+\t\t\tnew_window = true;\n+\n+\t\tif (new_window)\n+\t\t\tfreq = 1;\n+\t\telse if (old_freq < PGHOT_FREQ_MAX)\n+\t\t\tfreq = old_freq + 1;\n+\t\telse\n+\t\t\tfreq = old_freq;\n+\n+\t\thotness &= ~(PGHOT_FREQ_MASK << PGHOT_FREQ_SHIFT);\n+\t\thotness &= ~(PGHOT_TIME_MASK << PGHOT_TIME_SHIFT);\n+\n+\t\thotness |= (freq & PGHOT_FREQ_MASK) << PGHOT_FREQ_SHIFT;\n+\t\thotness |= (time & PGHOT_TIME_MASK) << PGHOT_TIME_SHIFT;\n+\n+\t\tif (freq >= pghot_freq_threshold)\n+\t\t\thotness |= BIT(PGHOT_MIGRATE_READY);\n+\t} while (unlikely(!try_cmpxchg(phi, &old_hotness, hotness)));\n+\treturn !!(hotness & BIT(PGHOT_MIGRATE_READY));\n+}\n+\n+int pghot_get_record(phi_t *phi, int *nid, int *freq, unsigned long *time)\n+{\n+\tphi_t old_hotness, hotness = 0;\n+\n+\told_hotness = READ_ONCE(*phi);\n+\tdo {\n+\t\tif (!(old_hotness & BIT(PGHOT_MIGRATE_READY)))\n+\t\t\treturn -EINVAL;\n+\t} while (unlikely(!try_cmpxchg(phi, &old_hotness, hotness)));\n+\n+\t*nid = pghot_target_nid;\n+\t*freq = (old_hotness >> PGHOT_FREQ_SHIFT) & PGHOT_FREQ_MASK;\n+\t*time = (old_hotness >> PGHOT_TIME_SHIFT) & PGHOT_TIME_MASK;\n+\treturn 0;\n+}\ndiff --git a/mm/pghot-tunables.c b/mm/pghot-tunables.c\nnew file mode 100644\nindex 000000000000..79afbcb1e4f0\n--- /dev/null\n+++ b/mm/pghot-tunables.c\n@@ -0,0 +1,189 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * pghot tunables in debugfs\n+ */\n+#include <linux/pghot.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/debugfs.h>\n+\n+static struct dentry *debugfs_pghot;\n+static DEFINE_MUTEX(pghot_tunables_lock);\n+\n+static ssize_t pghot_freq_th_write(struct file *filp, const char __user *ubuf,\n+\t\t\t\t   size_t cnt, loff_t *ppos)\n+{\n+\tchar buf[16];\n+\tunsigned int freq;\n+\n+\tif (cnt > 15)\n+\t\tcnt = 15;\n+\n+\tif (copy_from_user(&buf, ubuf, cnt))\n+\t\treturn -EFAULT;\n+\tbuf[cnt] = '\\0';\n+\n+\tif (kstrtouint(buf, 10, &freq))\n+\t\treturn -EINVAL;\n+\n+\tif (!freq || freq > PGHOT_FREQ_MAX)\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&pghot_tunables_lock);\n+\tpghot_freq_threshold = freq;\n+\tmutex_unlock(&pghot_tunables_lock);\n+\n+\t*ppos += cnt;\n+\treturn cnt;\n+}\n+\n+static int pghot_freq_th_show(struct seq_file *m, void *v)\n+{\n+\tseq_printf(m, \"%d\\n\", pghot_freq_threshold);\n+\treturn 0;\n+}\n+\n+static int pghot_freq_th_open(struct inode *inode, struct file *filp)\n+{\n+\treturn single_open(filp, pghot_freq_th_show, NULL);\n+}\n+\n+static const struct file_operations pghot_freq_th_fops = {\n+\t.open\t\t= pghot_freq_th_open,\n+\t.write\t\t= pghot_freq_th_write,\n+\t.read\t\t= seq_read,\n+\t.llseek\t\t= seq_lseek,\n+\t.release\t= seq_release,\n+};\n+\n+static ssize_t pghot_target_nid_write(struct file *filp, const char __user *ubuf,\n+\t\t\t\t      size_t cnt, loff_t *ppos)\n+{\n+\tchar buf[16];\n+\tunsigned int nid;\n+\n+\tif (cnt > 15)\n+\t\tcnt = 15;\n+\n+\tif (copy_from_user(&buf, ubuf, cnt))\n+\t\treturn -EFAULT;\n+\tbuf[cnt] = '\\0';\n+\n+\tif (kstrtouint(buf, 10, &nid))\n+\t\treturn -EINVAL;\n+\n+\tif (nid > PGHOT_NID_MAX || !node_online(nid) || !node_is_toptier(nid))\n+\t\treturn -EINVAL;\n+\tmutex_lock(&pghot_tunables_lock);\n+\tpghot_target_nid = nid;\n+\tmutex_unlock(&pghot_tunables_lock);\n+\n+\t*ppos += cnt;\n+\treturn cnt;\n+}\n+\n+static int pghot_target_nid_show(struct seq_file *m, void *v)\n+{\n+\tseq_printf(m, \"%d\\n\", pghot_target_nid);\n+\treturn 0;\n+}\n+\n+static int pghot_target_nid_open(struct inode *inode, struct file *filp)\n+{\n+\treturn single_open(filp, pghot_target_nid_show, NULL);\n+}\n+\n+static const struct file_operations pghot_target_nid_fops = {\n+\t.open\t\t= pghot_target_nid_open,\n+\t.write\t\t= pghot_target_nid_write,\n+\t.read\t\t= seq_read,\n+\t.llseek\t\t= seq_lseek,\n+\t.release\t= seq_release,\n+};\n+\n+static void pghot_src_enabled_update(unsigned int enabled)\n+{\n+\tunsigned int changed = pghot_src_enabled ^ enabled;\n+\n+\tif (changed & PGHOT_HWHINTS_ENABLED) {\n+\t\tif (enabled & PGHOT_HWHINTS_ENABLED)\n+\t\t\tstatic_branch_enable(&pghot_src_hwhints);\n+\t\telse\n+\t\t\tstatic_branch_disable(&pghot_src_hwhints);\n+\t}\n+\n+\tif (changed & PGHOT_PGTSCAN_ENABLED) {\n+\t\tif (enabled & PGHOT_PGTSCAN_ENABLED)\n+\t\t\tstatic_branch_enable(&pghot_src_pgtscans);\n+\t\telse\n+\t\t\tstatic_branch_disable(&pghot_src_pgtscans);\n+\t}\n+\n+\tif (changed & PGHOT_HINTFAULT_ENABLED) {\n+\t\tif (enabled & PGHOT_HINTFAULT_ENABLED)\n+\t\t\tstatic_branch_enable(&pghot_src_hintfaults);\n+\t\telse\n+\t\t\tstatic_branch_disable(&pghot_src_hintfaults);\n+\t}\n+}\n+\n+static ssize_t pghot_src_enabled_write(struct file *filp, const char __user *ubuf,\n+\t\t\t\t\t   size_t cnt, loff_t *ppos)\n+{\n+\tchar buf[16];\n+\tunsigned int enabled;\n+\n+\tif (cnt > 15)\n+\t\tcnt = 15;\n+\n+\tif (copy_from_user(&buf, ubuf, cnt))\n+\t\treturn -EFAULT;\n+\tbuf[cnt] = '\\0';\n+\n+\tif (kstrtouint(buf, 0, &enabled))\n+\t\treturn -EINVAL;\n+\n+\tif (enabled & ~PGHOT_SRC_ENABLED_MASK)\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&pghot_tunables_lock);\n+\tpghot_src_enabled_update(enabled);\n+\tpghot_src_enabled = enabled;\n+\tmutex_unlock(&pghot_tunables_lock);\n+\n+\t*ppos += cnt;\n+\treturn cnt;\n+}\n+\n+static int pghot_src_enabled_show(struct seq_file *m, void *v)\n+{\n+\tseq_printf(m, \"%d\\n\", pghot_src_enabled);\n+\treturn 0;\n+}\n+\n+static int pghot_src_enabled_open(struct inode *inode, struct file *filp)\n+{\n+\treturn single_open(filp, pghot_src_enabled_show, NULL);\n+}\n+\n+static const struct file_operations pghot_src_enabled_fops = {\n+\t.open\t\t= pghot_src_enabled_open,\n+\t.write\t\t= pghot_src_enabled_write,\n+\t.read\t\t= seq_read,\n+\t.llseek\t\t= seq_lseek,\n+\t.release\t= seq_release,\n+};\n+\n+void pghot_debug_init(void)\n+{\n+\tdebugfs_pghot = debugfs_create_dir(\"pghot\", NULL);\n+\tdebugfs_create_file(\"enabled_sources\", 0644, debugfs_pghot, NULL,\n+\t\t\t    &pghot_src_enabled_fops);\n+\tdebugfs_create_file(\"target_nid\", 0644, debugfs_pghot, NULL,\n+\t\t\t    &pghot_target_nid_fops);\n+\tdebugfs_create_file(\"freq_threshold\", 0644, debugfs_pghot, NULL,\n+\t\t\t    &pghot_freq_th_fops);\n+\tdebugfs_create_u32(\"kmigrated_sleep_ms\", 0644, debugfs_pghot,\n+\t\t\t    &kmigrated_sleep_ms);\n+\tdebugfs_create_u32(\"kmigrated_batch_nr\", 0644, debugfs_pghot,\n+\t\t\t    &kmigrated_batch_nr);\n+}\ndiff --git a/mm/pghot.c b/mm/pghot.c\nnew file mode 100644\nindex 000000000000..95b5012d5b99\n--- /dev/null\n+++ b/mm/pghot.c\n@@ -0,0 +1,370 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * Maintains information about hot pages from slower tier nodes and\n+ * promotes them.\n+ *\n+ * Per-PFN hotness information is stored for lower tier nodes in\n+ * mem_section.\n+ *\n+ * In the default mode, a single byte (u8) is used to store\n+ * the frequency of access and last access time. Promotions are done\n+ * to a default toptier NID.\n+ *\n+ * A kernel thread named kmigrated is provided to migrate or promote\n+ * the hot pages. kmigrated runs for each lower tier node. It iterates\n+ * over the node's PFNs and  migrates pages marked for migration into\n+ * their targeted nodes.\n+ */\n+#include <linux/mm.h>\n+#include <linux/migrate.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/pghot.h>\n+\n+unsigned int pghot_target_nid = PGHOT_DEFAULT_NODE;\n+unsigned int pghot_src_enabled;\n+unsigned int pghot_freq_threshold = PGHOT_DEFAULT_FREQ_THRESHOLD;\n+unsigned int kmigrated_sleep_ms = KMIGRATED_DEFAULT_SLEEP_MS;\n+unsigned int kmigrated_batch_nr = KMIGRATED_DEFAULT_BATCH_NR;\n+\n+unsigned int sysctl_pghot_freq_window = PGHOT_DEFAULT_FREQ_WINDOW;\n+\n+DEFINE_STATIC_KEY_FALSE(pghot_src_hwhints);\n+DEFINE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n+DEFINE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n+\n+#ifdef CONFIG_SYSCTL\n+static const struct ctl_table pghot_sysctls[] = {\n+\t{\n+\t\t.procname       = \"pghot_promote_freq_window_ms\",\n+\t\t.data           = &sysctl_pghot_freq_window,\n+\t\t.maxlen         = sizeof(unsigned int),\n+\t\t.mode           = 0644,\n+\t\t.proc_handler   = proc_dointvec_minmax,\n+\t\t.extra1         = SYSCTL_ZERO,\n+\t},\n+};\n+#endif\n+\n+static bool kmigrated_started __ro_after_init;\n+\n+/**\n+ * pghot_record_access() - Record page accesses from lower tier memory\n+ * for the purpose of tracking page hotness and subsequent promotion.\n+ *\n+ * @pfn: PFN of the page\n+ * @nid: Unused\n+ * @src: The identifier of the sub-system that reports the access\n+ * @now: Access time in jiffies\n+ *\n+ * Updates the frequency and time of access and marks the page as\n+ * ready for migration if the frequency crosses a threshold. The pages\n+ * marked for migration are migrated by kmigrated kernel thread.\n+ *\n+ * Return: 0 on success and -EINVAL on failure to record the access.\n+ */\n+int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now)\n+{\n+\tstruct mem_section *ms;\n+\tstruct folio *folio;\n+\tphi_t *phi, *hot_map;\n+\tstruct page *page;\n+\n+\tif (!kmigrated_started)\n+\t\treturn -EINVAL;\n+\n+\tif (nid >= PGHOT_NID_MAX)\n+\t\treturn -EINVAL;\n+\n+\tswitch (src) {\n+\tcase PGHOT_HW_HINTS:\n+\t\tif (!static_branch_likely(&pghot_src_hwhints))\n+\t\t\treturn -EINVAL;\n+\t\tcount_vm_event(PGHOT_RECORD_HWHINTS);\n+\t\tbreak;\n+\tcase PGHOT_PGTABLE_SCAN:\n+\t\tif (!static_branch_likely(&pghot_src_pgtscans))\n+\t\t\treturn -EINVAL;\n+\t\tcount_vm_event(PGHOT_RECORD_PGTSCANS);\n+\t\tbreak;\n+\tcase PGHOT_HINT_FAULT:\n+\t\tif (!static_branch_likely(&pghot_src_hintfaults))\n+\t\t\treturn -EINVAL;\n+\t\tcount_vm_event(PGHOT_RECORD_HINTFAULTS);\n+\t\tbreak;\n+\tdefault:\n+\t\treturn -EINVAL;\n+\t}\n+\n+\t/*\n+\t * Record only accesses from lower tiers.\n+\t */\n+\tif (node_is_toptier(pfn_to_nid(pfn)))\n+\t\treturn 0;\n+\n+\t/*\n+\t * Reject the non-migratable pages right away.\n+\t */\n+\tpage = pfn_to_online_page(pfn);\n+\tif (!page || is_zone_device_page(page))\n+\t\treturn 0;\n+\n+\tfolio = page_folio(page);\n+\tif (!folio_test_lru(folio))\n+\t\treturn 0;\n+\n+\t/* Get the hotness slot corresponding to the 1st PFN of the folio */\n+\tpfn = folio_pfn(folio);\n+\tms = __pfn_to_section(pfn);\n+\tif (!ms || !ms->hot_map)\n+\t\treturn -EINVAL;\n+\n+\thot_map = (phi_t *)(((unsigned long)(ms->hot_map)) & ~PGHOT_SECTION_HOT_MASK);\n+\tphi = &hot_map[pfn % PAGES_PER_SECTION];\n+\n+\tcount_vm_event(PGHOT_RECORDED_ACCESSES);\n+\n+\t/*\n+\t * Update the hotness parameters.\n+\t */\n+\tif (pghot_update_record(phi, nid, now)) {\n+\t\tset_bit(PGHOT_SECTION_HOT_BIT, (unsigned long *)&ms->hot_map);\n+\t\tset_bit(PGDAT_KMIGRATED_ACTIVATE, &page_pgdat(page)->flags);\n+\t}\n+\treturn 0;\n+}\n+\n+static int pghot_get_hotness(unsigned long pfn, int *nid, int *freq,\n+\t\t\t     unsigned long *time)\n+{\n+\tphi_t *phi, *hot_map;\n+\tstruct mem_section *ms;\n+\n+\tms = __pfn_to_section(pfn);\n+\tif (!ms || !ms->hot_map)\n+\t\treturn -EINVAL;\n+\n+\thot_map = (phi_t *)(((unsigned long)(ms->hot_map)) & ~PGHOT_SECTION_HOT_MASK);\n+\tphi = &hot_map[pfn % PAGES_PER_SECTION];\n+\n+\treturn pghot_get_record(phi, nid, freq, time);\n+}\n+\n+/*\n+ * Walks the PFNs of the zone, isolates and migrates them in batches.\n+ */\n+static void kmigrated_walk_zone(unsigned long start_pfn, unsigned long end_pfn,\n+\t\t\t\tint src_nid)\n+{\n+\tint cur_nid = NUMA_NO_NODE;\n+\tLIST_HEAD(migrate_list);\n+\tint batch_count = 0;\n+\tstruct folio *folio;\n+\tstruct page *page;\n+\tunsigned long pfn;\n+\n+\tpfn = start_pfn;\n+\tdo {\n+\t\tint nid = NUMA_NO_NODE, nr = 1;\n+\t\tint freq = 0;\n+\t\tunsigned long time = 0;\n+\n+\t\tif (!pfn_valid(pfn))\n+\t\t\tgoto out_next;\n+\n+\t\tpage = pfn_to_online_page(pfn);\n+\t\tif (!page)\n+\t\t\tgoto out_next;\n+\n+\t\tfolio = page_folio(page);\n+\t\tnr = folio_nr_pages(folio);\n+\t\tif (folio_nid(folio) != src_nid)\n+\t\t\tgoto out_next;\n+\n+\t\tif (!folio_test_lru(folio))\n+\t\t\tgoto out_next;\n+\n+\t\tif (pghot_get_hotness(pfn, &nid, &freq, &time))\n+\t\t\tgoto out_next;\n+\n+\t\tif (nid == NUMA_NO_NODE)\n+\t\t\tnid = pghot_target_nid;\n+\n+\t\tif (folio_nid(folio) == nid)\n+\t\t\tgoto out_next;\n+\n+\t\tif (migrate_misplaced_folio_prepare(folio, NULL, nid))\n+\t\t\tgoto out_next;\n+\n+\t\tif (cur_nid == NUMA_NO_NODE)\n+\t\t\tcur_nid = nid;\n+\n+\t\t/* If NID changed, flush the previous batch first */\n+\t\tif (cur_nid != nid) {\n+\t\t\tif (!list_empty(&migrate_list))\n+\t\t\t\tmigrate_misplaced_folios_batch(&migrate_list, cur_nid);\n+\t\t\tcur_nid = nid;\n+\t\t\tbatch_count = 0;\n+\t\t\tcond_resched();\n+\t\t}\n+\n+\t\tlist_add(&folio->lru, &migrate_list);\n+\n+\t\tif (++batch_count > kmigrated_batch_nr) {\n+\t\t\tmigrate_misplaced_folios_batch(&migrate_list, cur_nid);\n+\t\t\tbatch_count = 0;\n+\t\t\tcond_resched();\n+\t\t}\n+out_next:\n+\t\tpfn += nr;\n+\t} while (pfn < end_pfn);\n+\tif (!list_empty(&migrate_list))\n+\t\tmigrate_misplaced_folios_batch(&migrate_list, cur_nid);\n+}\n+\n+static void kmigrated_do_work(pg_data_t *pgdat)\n+{\n+\tunsigned long section_nr, s_begin, start_pfn;\n+\tstruct mem_section *ms;\n+\tint nid;\n+\n+\tclear_bit(PGDAT_KMIGRATED_ACTIVATE, &pgdat->flags);\n+\t/* s_begin = first_present_section_nr(); */\n+\ts_begin = next_present_section_nr(-1);\n+\tfor_each_present_section_nr(s_begin, section_nr) {\n+\t\tstart_pfn = section_nr_to_pfn(section_nr);\n+\t\tms = __nr_to_section(section_nr);\n+\n+\t\tif (!pfn_valid(start_pfn))\n+\t\t\tcontinue;\n+\n+\t\tnid = pfn_to_nid(start_pfn);\n+\t\tif (node_is_toptier(nid) || nid != pgdat->node_id)\n+\t\t\tcontinue;\n+\n+\t\tif (!test_and_clear_bit(PGHOT_SECTION_HOT_BIT, (unsigned long *)&ms->hot_map))\n+\t\t\tcontinue;\n+\n+\t\tkmigrated_walk_zone(start_pfn, start_pfn + PAGES_PER_SECTION,\n+\t\t\t\t    pgdat->node_id);\n+\t}\n+}\n+\n+static inline bool kmigrated_work_requested(pg_data_t *pgdat)\n+{\n+\treturn test_bit(PGDAT_KMIGRATED_ACTIVATE, &pgdat->flags);\n+}\n+\n+/*\n+ * Per-node kthread that iterates over its PFNs and migrates the\n+ * pages that have been marked for migration.\n+ */\n+static int kmigrated(void *p)\n+{\n+\tlong timeout = msecs_to_jiffies(kmigrated_sleep_ms);\n+\tpg_data_t *pgdat = p;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tif (wait_event_timeout(pgdat->kmigrated_wait, kmigrated_work_requested(pgdat),\n+\t\t\t\t       timeout))\n+\t\t\tkmigrated_do_work(pgdat);\n+\t}\n+\treturn 0;\n+}\n+\n+static int kmigrated_run(int nid)\n+{\n+\tpg_data_t *pgdat = NODE_DATA(nid);\n+\tint ret;\n+\n+\tif (node_is_toptier(nid))\n+\t\treturn 0;\n+\n+\tif (!pgdat->kmigrated) {\n+\t\tpgdat->kmigrated = kthread_create_on_node(kmigrated, pgdat, nid,\n+\t\t\t\t\t\t\t  \"kmigrated%d\", nid);\n+\t\tif (IS_ERR(pgdat->kmigrated)) {\n+\t\t\tret = PTR_ERR(pgdat->kmigrated);\n+\t\t\tpgdat->kmigrated = NULL;\n+\t\t\tpr_err(\"Failed to start kmigrated%d, ret %d\\n\", nid, ret);\n+\t\t\treturn ret;\n+\t\t}\n+\t\tpr_info(\"pghot: Started kmigrated thread for node %d\\n\", nid);\n+\t}\n+\twake_up_process(pgdat->kmigrated);\n+\treturn 0;\n+}\n+\n+static void pghot_free_hot_map(void)\n+{\n+\tunsigned long section_nr, s_begin;\n+\tstruct mem_section *ms;\n+\n+\t/* s_begin = first_present_section_nr(); */\n+\ts_begin = next_present_section_nr(-1);\n+\tfor_each_present_section_nr(s_begin, section_nr) {\n+\t\tms = __nr_to_section(section_nr);\n+\t\tkfree(ms->hot_map);\n+\t}\n+}\n+\n+static int pghot_alloc_hot_map(void)\n+{\n+\tunsigned long section_nr, s_begin, start_pfn;\n+\tstruct mem_section *ms;\n+\tint nid;\n+\n+\t/* s_begin = first_present_section_nr(); */\n+\ts_begin = next_present_section_nr(-1);\n+\tfor_each_present_section_nr(s_begin, section_nr) {\n+\t\tms = __nr_to_section(section_nr);\n+\t\tstart_pfn = section_nr_to_pfn(section_nr);\n+\t\tnid = pfn_to_nid(start_pfn);\n+\n+\t\tif (node_is_toptier(nid) || !pfn_valid(start_pfn))\n+\t\t\tcontinue;\n+\n+\t\tms->hot_map = kcalloc_node(PAGES_PER_SECTION, PGHOT_RECORD_SIZE, GFP_KERNEL,\n+\t\t\t\t\t   nid);\n+\t\tif (!ms->hot_map)\n+\t\t\tgoto out_free_hot_map;\n+\t}\n+\treturn 0;\n+\n+out_free_hot_map:\n+\tpghot_free_hot_map();\n+\treturn -ENOMEM;\n+}\n+\n+static int __init pghot_init(void)\n+{\n+\tpg_data_t *pgdat;\n+\tint nid, ret;\n+\n+\tret = pghot_alloc_hot_map();\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tret = kmigrated_run(nid);\n+\t\tif (ret)\n+\t\t\tgoto out_stop_kthread;\n+\t}\n+\tregister_sysctl_init(\"vm\", pghot_sysctls);\n+\tpghot_debug_init();\n+\n+\tkmigrated_started = true;\n+\treturn 0;\n+\n+out_stop_kthread:\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tpgdat = NODE_DATA(nid);\n+\t\tif (pgdat->kmigrated) {\n+\t\t\tkthread_stop(pgdat->kmigrated);\n+\t\t\tpgdat->kmigrated = NULL;\n+\t\t}\n+\t}\n+\tpghot_free_hot_map();\n+\treturn ret;\n+}\n+\n+late_initcall_sync(pghot_init)\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 65de88cdf40e..f6f91b9dd887 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -1501,6 +1501,12 @@ const char * const vmstat_text[] = {\n \t[I(KSTACK_REST)]\t\t\t= \"kstack_rest\",\n #endif\n #endif\n+#ifdef CONFIG_PGHOT\n+\t[I(PGHOT_RECORDED_ACCESSES)]\t\t= \"pghot_recorded_accesses\",\n+\t[I(PGHOT_RECORD_HWHINTS)]\t\t= \"pghot_recorded_hwhints\",\n+\t[I(PGHOT_RECORD_PGTSCANS)]\t\t= \"pghot_recorded_pgtscans\",\n+\t[I(PGHOT_RECORD_HINTFAULTS)]\t\t= \"pghot_recorded_hintfaults\",\n+#endif /* CONFIG_PGHOT */\n #undef I\n #endif /* CONFIG_VM_EVENT_COUNTERS */\n };\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "20260129144043.231636-4-bharata@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author acknowledged that the default mode has limited time tracking and no explicit toptier NID tracking, but explained that precision mode addresses these issues by storing hotness information in 4 bytes per PFN, allowing for more fine-grained access time tracking and toptier NID tracking. The author noted that this precise mode is typically useful when the toptier consists of multiple nodes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a limitation",
                "explained a technical detail"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By default, one byte per PFN is used to store hotness information.\nLimited number of bits are used to store the access time leading\nto coarse-grained time tracking. Also there aren't enough bits\nto track the toptier NID explicitly and hence the default target_nid\nis used for promotion.\n\nThis precise mode relaxes the above situation by storing the\nhotness information in 4 bytes per PFN. More fine-grained\naccess time tracking and toptier NID tracking becomes possible\nin this mode.\n\nTypically useful when toptier consists of more than one node.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n Documentation/admin-guide/mm/pghot.txt |  4 +-\n include/linux/mmzone.h                 |  2 +-\n include/linux/pghot.h                  | 31 ++++++++++++\n mm/Kconfig                             | 11 ++++\n mm/Makefile                            |  7 ++-\n mm/pghot-precise.c                     | 70 ++++++++++++++++++++++++++\n mm/pghot.c                             | 13 +++--\n 7 files changed, 130 insertions(+), 8 deletions(-)\n create mode 100644 mm/pghot-precise.c\n\ndiff --git a/Documentation/admin-guide/mm/pghot.txt b/Documentation/admin-guide/mm/pghot.txt\nindex 01291b72e7ab..b329e692ef89 100644\n--- a/Documentation/admin-guide/mm/pghot.txt\n+++ b/Documentation/admin-guide/mm/pghot.txt\n@@ -38,7 +38,7 @@ Path: /sys/kernel/debug/pghot/\n \n 3. **freq_threshold**\n    - Minimum access frequency before a page is marked ready for promotion.\n-   - Range: 1 to 3\n+   - Range: 1 to 3 in default mode, 1 to 7 in precision mode.\n    - Default: 2\n    - Example:\n      # echo 3 > /sys/kernel/debug/pghot/freq_threshold\n@@ -60,7 +60,7 @@ Path: /proc/sys/vm/pghot_promote_freq_window_ms\n - Controls the time window (in ms) for counting access frequency. A page is\n   considered hot only when **freq_threshold** number of accesses occur with\n   this time period.\n-- Default: 4000 (4 seconds)\n+- Default: 4000 (4 seconds) in default mode and 5000 (5s) in precision mode.\n - Example:\n   # sysctl vm.pghot_promote_freq_window_ms=3000\n \ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 22e08befb096..49c374064fc2 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1924,7 +1924,7 @@ struct mem_section {\n #ifdef CONFIG_PGHOT\n \t/*\n \t * Per-PFN hotness data for this section.\n-\t * Array of phi_t (u8 in default mode).\n+\t * Array of phi_t (u8 in default mode, u32 in precision mode).\n \t * LSB is used as PGHOT_SECTION_HOT_BIT flag.\n \t */\n \tvoid *hot_map;\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nindex 88e57aab697b..d3d59b0c0cf6 100644\n--- a/include/linux/pghot.h\n+++ b/include/linux/pghot.h\n@@ -48,6 +48,36 @@ enum pghot_src_enabled {\n \n #define PGHOT_DEFAULT_NODE\t\t0\n \n+#if defined(CONFIG_PGHOT_PRECISE)\n+#define PGHOT_DEFAULT_FREQ_WINDOW\t(5 * MSEC_PER_SEC)\n+\n+/*\n+ * Bits 0-26 are used to store nid, frequency and time.\n+ * Bits 27-30 are unused now.\n+ * Bit 31 is used to indicate the page is ready for migration.\n+ */\n+#define PGHOT_MIGRATE_READY\t\t31\n+\n+#define PGHOT_NID_WIDTH\t\t\t10\n+#define PGHOT_FREQ_WIDTH\t\t3\n+/* time is stored in 14 bits which can represent up to 16s with HZ=1000 */\n+#define PGHOT_TIME_WIDTH\t\t14\n+\n+#define PGHOT_NID_SHIFT\t\t\t0\n+#define PGHOT_FREQ_SHIFT\t\t(PGHOT_NID_SHIFT + PGHOT_NID_WIDTH)\n+#define PGHOT_TIME_SHIFT\t\t(PGHOT_FREQ_SHIFT + PGHOT_FREQ_WIDTH)\n+\n+#define PGHOT_NID_MASK\t\t\tGENMASK(PGHOT_NID_WIDTH - 1, 0)\n+#define PGHOT_FREQ_MASK\t\t\tGENMASK(PGHOT_FREQ_WIDTH - 1, 0)\n+#define PGHOT_TIME_MASK\t\t\tGENMASK(PGHOT_TIME_WIDTH - 1, 0)\n+\n+#define PGHOT_NID_MAX\t\t\t((1 << PGHOT_NID_WIDTH) - 1)\n+#define PGHOT_FREQ_MAX\t\t\t((1 << PGHOT_FREQ_WIDTH) - 1)\n+#define PGHOT_TIME_MAX\t\t\t((1 << PGHOT_TIME_WIDTH) - 1)\n+\n+typedef u32 phi_t;\n+\n+#else\t/* !CONFIG_PGHOT_PRECISE */\n #define PGHOT_DEFAULT_FREQ_WINDOW\t(4 * MSEC_PER_SEC)\n \n /*\n@@ -74,6 +104,7 @@ enum pghot_src_enabled {\n #define PGHOT_TIME_MAX\t\t\t((1 << PGHOT_TIME_WIDTH) - 1)\n \n typedef u8 phi_t;\n+#endif /* CONFIG_PGHOT_PRECISE */\n \n #define PGHOT_RECORD_SIZE\t\tsizeof(phi_t)\n \ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex f4f0147faac5..fde5aee3e16f 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -1478,6 +1478,17 @@ config PGHOT\n \t  This adds 1 byte of metadata overhead per page in lower-tier\n \t  memory nodes.\n \n+config PGHOT_PRECISE\n+\tbool \"Hot page tracking precision mode\"\n+\tdef_bool n\n+\tdepends on PGHOT\n+\thelp\n+\t  Enables precision mode for tracking hot pages with pghot sub-system.\n+\t  Adds fine-grained access time tracking and explicit toptier target\n+\t  NID tracking. Precise hot page tracking comes at the cost of using\n+\t  4 bytes per page against the default one byte per page. Preferable\n+\t  to enable this on systems with multiple nodes in toptier.\n+\n source \"mm/damon/Kconfig\"\n \n endmenu\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 655a27f3a215..89f999647752 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -147,4 +147,9 @@ obj-$(CONFIG_SHRINKER_DEBUG) += shrinker_debug.o\n obj-$(CONFIG_EXECMEM) += execmem.o\n obj-$(CONFIG_TMPFS_QUOTA) += shmem_quota.o\n obj-$(CONFIG_PT_RECLAIM) += pt_reclaim.o\n-obj-$(CONFIG_PGHOT) += pghot.o pghot-tunables.o pghot-default.o\n+obj-$(CONFIG_PGHOT) += pghot.o pghot-tunables.o\n+ifdef CONFIG_PGHOT_PRECISE\n+obj-$(CONFIG_PGHOT) += pghot-precise.o\n+else\n+obj-$(CONFIG_PGHOT) += pghot-default.o\n+endif\ndiff --git a/mm/pghot-precise.c b/mm/pghot-precise.c\nnew file mode 100644\nindex 000000000000..d8d4f15b3f9f\n--- /dev/null\n+++ b/mm/pghot-precise.c\n@@ -0,0 +1,70 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * pghot: Precision mode\n+ *\n+ * 4 byte hotness record per PFN (u32)\n+ * NID, time and frequency tracked as part of the record.\n+ */\n+\n+#include <linux/pghot.h>\n+#include <linux/jiffies.h>\n+\n+unsigned long pghot_access_latency(unsigned long old_time, unsigned long time)\n+{\n+\treturn jiffies_to_msecs((time - old_time) & PGHOT_TIME_MASK);\n+}\n+\n+bool pghot_update_record(phi_t *phi, int nid, unsigned long now)\n+{\n+\tphi_t freq, old_freq, hotness, old_hotness, old_time, old_nid;\n+\tphi_t time = now & PGHOT_TIME_MASK;\n+\n+\told_hotness = READ_ONCE(*phi);\n+\tdo {\n+\t\tbool new_window = false;\n+\n+\t\thotness = old_hotness;\n+\t\told_nid = (hotness >> PGHOT_NID_SHIFT) & PGHOT_NID_MASK;\n+\t\told_freq = (hotness >> PGHOT_FREQ_SHIFT) & PGHOT_FREQ_MASK;\n+\t\told_time = (hotness >> PGHOT_TIME_SHIFT) & PGHOT_TIME_MASK;\n+\n+\t\tif (pghot_access_latency(old_time, time) > sysctl_pghot_freq_window)\n+\t\t\tnew_window = true;\n+\n+\t\tif (new_window)\n+\t\t\tfreq = 1;\n+\t\telse if (old_freq < PGHOT_FREQ_MAX)\n+\t\t\tfreq = old_freq + 1;\n+\t\telse\n+\t\t\tfreq = old_freq;\n+\t\tnid = (nid == NUMA_NO_NODE) ? pghot_target_nid : nid;\n+\n+\t\thotness &= ~(PGHOT_NID_MASK << PGHOT_NID_SHIFT);\n+\t\thotness &= ~(PGHOT_FREQ_MASK << PGHOT_FREQ_SHIFT);\n+\t\thotness &= ~(PGHOT_TIME_MASK << PGHOT_TIME_SHIFT);\n+\n+\t\thotness |= (nid & PGHOT_NID_MASK) << PGHOT_NID_SHIFT;\n+\t\thotness |= (freq & PGHOT_FREQ_MASK) << PGHOT_FREQ_SHIFT;\n+\t\thotness |= (time & PGHOT_TIME_MASK) << PGHOT_TIME_SHIFT;\n+\n+\t\tif (freq >= pghot_freq_threshold)\n+\t\t\thotness |= BIT(PGHOT_MIGRATE_READY);\n+\t} while (unlikely(!try_cmpxchg(phi, &old_hotness, hotness)));\n+\treturn !!(hotness & BIT(PGHOT_MIGRATE_READY));\n+}\n+\n+int pghot_get_record(phi_t *phi, int *nid, int *freq, unsigned long *time)\n+{\n+\tphi_t old_hotness, hotness = 0;\n+\n+\told_hotness = READ_ONCE(*phi);\n+\tdo {\n+\t\tif (!(old_hotness & BIT(PGHOT_MIGRATE_READY)))\n+\t\t\treturn -EINVAL;\n+\t} while (unlikely(!try_cmpxchg(phi, &old_hotness, hotness)));\n+\n+\t*nid = (old_hotness >> PGHOT_NID_SHIFT) & PGHOT_NID_MASK;\n+\t*freq = (old_hotness >> PGHOT_FREQ_SHIFT) & PGHOT_FREQ_MASK;\n+\t*time = (old_hotness >> PGHOT_TIME_SHIFT) & PGHOT_TIME_MASK;\n+\treturn 0;\n+}\ndiff --git a/mm/pghot.c b/mm/pghot.c\nindex 95b5012d5b99..bf1d9029cbaa 100644\n--- a/mm/pghot.c\n+++ b/mm/pghot.c\n@@ -10,6 +10,9 @@\n  * the frequency of access and last access time. Promotions are done\n  * to a default toptier NID.\n  *\n+ * In the precision mode, 4 bytes are used to store the frequency\n+ * of access, last access time and the accessing NID.\n+ *\n  * A kernel thread named kmigrated is provided to migrate or promote\n  * the hot pages. kmigrated runs for each lower tier node. It iterates\n  * over the node's PFNs and  migrates pages marked for migration into\n@@ -52,13 +55,15 @@ static bool kmigrated_started __ro_after_init;\n  * for the purpose of tracking page hotness and subsequent promotion.\n  *\n  * @pfn: PFN of the page\n- * @nid: Unused\n+ * @nid: Target NID to where the page needs to be migrated in precision\n+ *       mode but unused in default mode\n  * @src: The identifier of the sub-system that reports the access\n  * @now: Access time in jiffies\n  *\n- * Updates the frequency and time of access and marks the page as\n- * ready for migration if the frequency crosses a threshold. The pages\n- * marked for migration are migrated by kmigrated kernel thread.\n+ * Updates the NID (in precision mode only), frequency and time of access\n+ * and marks the page as ready for migration if the frequency crosses a\n+ * threshold. The pages marked for migration are migrated by kmigrated\n+ * kernel thread.\n  *\n  * Return: 0 on success and -EINVAL on failure to record the access.\n  */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "20260129144043.231636-5-bharata@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed concerns about the complexity and overlap of hot page promotion mechanisms in NUMA Balancing, explaining that pghot will offload detection, classification, and promotion to a common system, while NUMA Balancing will focus on hint fault detection. The author confirmed this change is intentional and aligns with the goals of pghot.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged concerns",
                "aligned with design goals"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Currently hot page promotion (NUMA_BALANCING_MEMORY_TIERING\nmode of NUMA Balancing) does hot page detection (via hint faults),\nhot page classification and eventual promotion, all by itself and\nsits within the scheduler.\n\nWith pghot, the new hot page tracking and promotion mechanism\nbeing available, NUMA Balancing can limit itself to detection\nof hot pages (via hint faults) and off-load rest of the\nfunctionality to the common hot page tracking system.\n\npghot_record_access(PGHOT_HINT_FAULT) API is used to feed the\nhot page info to pghot. In addition, the migration rate limiting\nand dynamic threshold logic are moved to kmigrated so that the\nsame can be used for hot pages reported by other sources too.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n kernel/sched/debug.c |   1 -\n kernel/sched/fair.c  | 152 ++-----------------------------------------\n mm/huge_memory.c     |  26 ++------\n mm/memory.c          |  31 ++-------\n mm/pghot.c           | 124 +++++++++++++++++++++++++++++++++++\n 5 files changed, 141 insertions(+), 193 deletions(-)\n\ndiff --git a/kernel/sched/debug.c b/kernel/sched/debug.c\nindex 41caa22e0680..02931902a9c6 100644\n--- a/kernel/sched/debug.c\n+++ b/kernel/sched/debug.c\n@@ -520,7 +520,6 @@ static __init int sched_init_debug(void)\n \tdebugfs_create_u32(\"scan_period_min_ms\", 0644, numa, &sysctl_numa_balancing_scan_period_min);\n \tdebugfs_create_u32(\"scan_period_max_ms\", 0644, numa, &sysctl_numa_balancing_scan_period_max);\n \tdebugfs_create_u32(\"scan_size_mb\", 0644, numa, &sysctl_numa_balancing_scan_size);\n-\tdebugfs_create_u32(\"hot_threshold_ms\", 0644, numa, &sysctl_numa_balancing_hot_threshold);\n #endif /* CONFIG_NUMA_BALANCING */\n \n \tdebugfs_create_file(\"debug\", 0444, debugfs_sched, NULL, &sched_debug_fops);\ndiff --git a/kernel/sched/fair.c b/kernel/sched/fair.c\nindex da46c3164537..4e70f58fbbfa 100644\n--- a/kernel/sched/fair.c\n+++ b/kernel/sched/fair.c\n@@ -125,11 +125,6 @@ int __weak arch_asym_cpu_priority(int cpu)\n static unsigned int sysctl_sched_cfs_bandwidth_slice\t\t= 5000UL;\n #endif\n \n-#ifdef CONFIG_NUMA_BALANCING\n-/* Restrict the NUMA promotion throughput (MB/s) for each target node. */\n-static unsigned int sysctl_numa_balancing_promote_rate_limit = 65536;\n-#endif\n-\n #ifdef CONFIG_SYSCTL\n static const struct ctl_table sched_fair_sysctls[] = {\n #ifdef CONFIG_CFS_BANDWIDTH\n@@ -142,16 +137,6 @@ static const struct ctl_table sched_fair_sysctls[] = {\n \t\t.extra1         = SYSCTL_ONE,\n \t},\n #endif\n-#ifdef CONFIG_NUMA_BALANCING\n-\t{\n-\t\t.procname\t= \"numa_balancing_promote_rate_limit_MBps\",\n-\t\t.data\t\t= &sysctl_numa_balancing_promote_rate_limit,\n-\t\t.maxlen\t\t= sizeof(unsigned int),\n-\t\t.mode\t\t= 0644,\n-\t\t.proc_handler\t= proc_dointvec_minmax,\n-\t\t.extra1\t\t= SYSCTL_ZERO,\n-\t},\n-#endif /* CONFIG_NUMA_BALANCING */\n };\n \n static int __init sched_fair_sysctl_init(void)\n@@ -1427,9 +1412,6 @@ unsigned int sysctl_numa_balancing_scan_size = 256;\n /* Scan @scan_size MB every @scan_period after an initial @scan_delay in ms */\n unsigned int sysctl_numa_balancing_scan_delay = 1000;\n \n-/* The page with hint page fault latency < threshold in ms is considered hot */\n-unsigned int sysctl_numa_balancing_hot_threshold = MSEC_PER_SEC;\n-\n struct numa_group {\n \trefcount_t refcount;\n \n@@ -1784,108 +1766,6 @@ static inline bool cpupid_valid(int cpupid)\n \treturn cpupid_to_cpu(cpupid) < nr_cpu_ids;\n }\n \n-/*\n- * For memory tiering mode, if there are enough free pages (more than\n- * enough watermark defined here) in fast memory node, to take full\n- * advantage of fast memory capacity, all recently accessed slow\n- * memory pages will be migrated to fast memory node without\n- * considering hot threshold.\n- */\n-static bool pgdat_free_space_enough(struct pglist_data *pgdat)\n-{\n-\tint z;\n-\tunsigned long enough_wmark;\n-\n-\tenough_wmark = max(1UL * 1024 * 1024 * 1024 >> PAGE_SHIFT,\n-\t\t\t   pgdat->node_present_pages >> 4);\n-\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n-\t\tstruct zone *zone = pgdat->node_zones + z;\n-\n-\t\tif (!populated_zone(zone))\n-\t\t\tcontinue;\n-\n-\t\tif (zone_watermark_ok(zone, 0,\n-\t\t\t\t      promo_wmark_pages(zone) + enough_wmark,\n-\t\t\t\t      ZONE_MOVABLE, 0))\n-\t\t\treturn true;\n-\t}\n-\treturn false;\n-}\n-\n-/*\n- * For memory tiering mode, when page tables are scanned, the scan\n- * time will be recorded in struct page in addition to make page\n- * PROT_NONE for slow memory page.  So when the page is accessed, in\n- * hint page fault handler, the hint page fault latency is calculated\n- * via,\n- *\n- *\thint page fault latency = hint page fault time - scan time\n- *\n- * The smaller the hint page fault latency, the higher the possibility\n- * for the page to be hot.\n- */\n-static int numa_hint_fault_latency(struct folio *folio)\n-{\n-\tint last_time, time;\n-\n-\ttime = jiffies_to_msecs(jiffies);\n-\tlast_time = folio_xchg_access_time(folio, time);\n-\n-\treturn (time - last_time) & PAGE_ACCESS_TIME_MASK;\n-}\n-\n-/*\n- * For memory tiering mode, too high promotion/demotion throughput may\n- * hurt application latency.  So we provide a mechanism to rate limit\n- * the number of pages that are tried to be promoted.\n- */\n-static bool numa_promotion_rate_limit(struct pglist_data *pgdat,\n-\t\t\t\t      unsigned long rate_limit, int nr)\n-{\n-\tunsigned long nr_cand;\n-\tunsigned int now, start;\n-\n-\tnow = jiffies_to_msecs(jiffies);\n-\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE, nr);\n-\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n-\tstart = pgdat->nbp_rl_start;\n-\tif (now - start > MSEC_PER_SEC &&\n-\t    cmpxchg(&pgdat->nbp_rl_start, start, now) == start)\n-\t\tpgdat->nbp_rl_nr_cand = nr_cand;\n-\tif (nr_cand - pgdat->nbp_rl_nr_cand >= rate_limit)\n-\t\treturn true;\n-\treturn false;\n-}\n-\n-#define NUMA_MIGRATION_ADJUST_STEPS\t16\n-\n-static void numa_promotion_adjust_threshold(struct pglist_data *pgdat,\n-\t\t\t\t\t    unsigned long rate_limit,\n-\t\t\t\t\t    unsigned int ref_th)\n-{\n-\tunsigned int now, start, th_period, unit_th, th;\n-\tunsigned long nr_cand, ref_cand, diff_cand;\n-\n-\tnow = jiffies_to_msecs(jiffies);\n-\tth_period = sysctl_numa_balancing_scan_period_max;\n-\tstart = pgdat->nbp_th_start;\n-\tif (now - start > th_period &&\n-\t    cmpxchg(&pgdat->nbp_th_start, start, now) == start) {\n-\t\tref_cand = rate_limit *\n-\t\t\tsysctl_numa_balancing_scan_period_max / MSEC_PER_SEC;\n-\t\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n-\t\tdiff_cand = nr_cand - pgdat->nbp_th_nr_cand;\n-\t\tunit_th = ref_th * 2 / NUMA_MIGRATION_ADJUST_STEPS;\n-\t\tth = pgdat->nbp_threshold ? : ref_th;\n-\t\tif (diff_cand > ref_cand * 11 / 10)\n-\t\t\tth = max(th - unit_th, unit_th);\n-\t\telse if (diff_cand < ref_cand * 9 / 10)\n-\t\t\tth = min(th + unit_th, ref_th * 2);\n-\t\tpgdat->nbp_th_nr_cand = nr_cand;\n-\t\tpgdat->nbp_threshold = th;\n-\t}\n-}\n-\n bool should_numa_migrate_memory(struct task_struct *p, struct folio *folio,\n \t\t\t\tint src_nid, int dst_cpu)\n {\n@@ -1901,33 +1781,11 @@ bool should_numa_migrate_memory(struct task_struct *p, struct folio *folio,\n \n \t/*\n \t * The pages in slow memory node should be migrated according\n-\t * to hot/cold instead of private/shared.\n-\t */\n-\tif (folio_use_access_time(folio)) {\n-\t\tstruct pglist_data *pgdat;\n-\t\tunsigned long rate_limit;\n-\t\tunsigned int latency, th, def_th;\n-\t\tlong nr = folio_nr_pages(folio);\n-\n-\t\tpgdat = NODE_DATA(dst_nid);\n-\t\tif (pgdat_free_space_enough(pgdat)) {\n-\t\t\t/* workload changed, reset hot threshold */\n-\t\t\tpgdat->nbp_threshold = 0;\n-\t\t\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE_NRL, nr);\n-\t\t\treturn true;\n-\t\t}\n-\n-\t\tdef_th = sysctl_numa_balancing_hot_threshold;\n-\t\trate_limit = MB_TO_PAGES(sysctl_numa_balancing_promote_rate_limit);\n-\t\tnuma_promotion_adjust_threshold(pgdat, rate_limit, def_th);\n-\n-\t\tth = pgdat->nbp_threshold ? : def_th;\n-\t\tlatency = numa_hint_fault_latency(folio);\n-\t\tif (latency >= th)\n-\t\t\treturn false;\n-\n-\t\treturn !numa_promotion_rate_limit(pgdat, rate_limit, nr);\n-\t}\n+\t * to hot/cold instead of private/shared. Also the migration\n+\t * of such pages are handled by kmigrated.\n+\t */\n+\tif (folio_use_access_time(folio))\n+\t\treturn true;\n \n \tthis_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);\n \tlast_cpupid = folio_xchg_last_cpupid(folio, this_cpupid);\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 40cf59301c21..f52587e70b3c 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -40,6 +40,7 @@\n #include <linux/pgalloc.h>\n #include <linux/pgalloc_tag.h>\n #include <linux/pagewalk.h>\n+#include <linux/pghot.h>\n \n #include <asm/tlb.h>\n #include \"internal.h\"\n@@ -2217,29 +2218,12 @@ vm_fault_t do_huge_pmd_numa_page(struct vm_fault *vmf)\n \n \ttarget_nid = numa_migrate_check(folio, vmf, haddr, &flags, writable,\n \t\t\t\t\t&last_cpupid);\n+\tnid = target_nid;\n \tif (target_nid == NUMA_NO_NODE)\n \t\tgoto out_map;\n-\tif (migrate_misplaced_folio_prepare(folio, vma, target_nid)) {\n-\t\tflags |= TNF_MIGRATE_FAIL;\n-\t\tgoto out_map;\n-\t}\n-\t/* The folio is isolated and isolation code holds a folio reference. */\n-\tspin_unlock(vmf->ptl);\n-\twritable = false;\n \n-\tif (!migrate_misplaced_folio(folio, target_nid)) {\n-\t\tflags |= TNF_MIGRATED;\n-\t\tnid = target_nid;\n-\t\ttask_numa_fault(last_cpupid, nid, HPAGE_PMD_NR, flags);\n-\t\treturn 0;\n-\t}\n+\twritable = false;\n \n-\tflags |= TNF_MIGRATE_FAIL;\n-\tvmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);\n-\tif (unlikely(!pmd_same(pmdp_get(vmf->pmd), vmf->orig_pmd))) {\n-\t\tspin_unlock(vmf->ptl);\n-\t\treturn 0;\n-\t}\n out_map:\n \t/* Restore the PMD */\n \tpmd = pmd_modify(pmdp_get(vmf->pmd), vma->vm_page_prot);\n@@ -2250,8 +2234,10 @@ vm_fault_t do_huge_pmd_numa_page(struct vm_fault *vmf)\n \tupdate_mmu_cache_pmd(vma, vmf->address, vmf->pmd);\n \tspin_unlock(vmf->ptl);\n \n-\tif (nid != NUMA_NO_NODE)\n+\tif (nid != NUMA_NO_NODE) {\n+\t\tpghot_record_access(folio_pfn(folio), nid, PGHOT_HINT_FAULT, jiffies);\n \t\ttask_numa_fault(last_cpupid, nid, HPAGE_PMD_NR, flags);\n+\t}\n \treturn 0;\n }\n \ndiff --git a/mm/memory.c b/mm/memory.c\nindex 2a55edc48a65..98a9a3b675a0 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -75,6 +75,7 @@\n #include <linux/perf_event.h>\n #include <linux/ptrace.h>\n #include <linux/vmalloc.h>\n+#include <linux/pghot.h>\n #include <linux/sched/sysctl.h>\n #include <linux/pgalloc.h>\n #include <linux/uaccess.h>\n@@ -6046,34 +6047,12 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \n \ttarget_nid = numa_migrate_check(folio, vmf, vmf->address, &flags,\n \t\t\t\t\twritable, &last_cpupid);\n+\tnid = target_nid;\n \tif (target_nid == NUMA_NO_NODE)\n \t\tgoto out_map;\n-\tif (migrate_misplaced_folio_prepare(folio, vma, target_nid)) {\n-\t\tflags |= TNF_MIGRATE_FAIL;\n-\t\tgoto out_map;\n-\t}\n-\t/* The folio is isolated and isolation code holds a folio reference. */\n-\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n+\n \twritable = false;\n \tignore_writable = true;\n-\n-\t/* Migrate to the requested node */\n-\tif (!migrate_misplaced_folio(folio, target_nid)) {\n-\t\tnid = target_nid;\n-\t\tflags |= TNF_MIGRATED;\n-\t\ttask_numa_fault(last_cpupid, nid, nr_pages, flags);\n-\t\treturn 0;\n-\t}\n-\n-\tflags |= TNF_MIGRATE_FAIL;\n-\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n-\t\t\t\t       vmf->address, &vmf->ptl);\n-\tif (unlikely(!vmf->pte))\n-\t\treturn 0;\n-\tif (unlikely(!pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {\n-\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n-\t\treturn 0;\n-\t}\n out_map:\n \t/*\n \t * Make it present again, depending on how arch implements\n@@ -6087,8 +6066,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t\t\t\t\t    writable);\n \tpte_unmap_unlock(vmf->pte, vmf->ptl);\n \n-\tif (nid != NUMA_NO_NODE)\n+\tif (nid != NUMA_NO_NODE) {\n+\t\tpghot_record_access(folio_pfn(folio), nid, PGHOT_HINT_FAULT, jiffies);\n \t\ttask_numa_fault(last_cpupid, nid, nr_pages, flags);\n+\t}\n \treturn 0;\n }\n \ndiff --git a/mm/pghot.c b/mm/pghot.c\nindex bf1d9029cbaa..6fc76c1eaff8 100644\n--- a/mm/pghot.c\n+++ b/mm/pghot.c\n@@ -17,6 +17,9 @@\n  * the hot pages. kmigrated runs for each lower tier node. It iterates\n  * over the node's PFNs and  migrates pages marked for migration into\n  * their targeted nodes.\n+ *\n+ * Migration rate-limiting and dynamic threshold logic implementations\n+ * were moved from NUMA Balancing mode 2.\n  */\n #include <linux/mm.h>\n #include <linux/migrate.h>\n@@ -31,6 +34,12 @@ unsigned int kmigrated_batch_nr = KMIGRATED_DEFAULT_BATCH_NR;\n \n unsigned int sysctl_pghot_freq_window = PGHOT_DEFAULT_FREQ_WINDOW;\n \n+/* Restrict the NUMA promotion throughput (MB/s) for each target node. */\n+static unsigned int sysctl_pghot_promote_rate_limit = 65536;\n+\n+#define KMIGRATED_MIGRATION_ADJUST_STEPS\t16\n+#define KMIGRATED_PROMOTION_THRESHOLD_WINDOW\t60000\n+\n DEFINE_STATIC_KEY_FALSE(pghot_src_hwhints);\n DEFINE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n DEFINE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n@@ -45,6 +54,14 @@ static const struct ctl_table pghot_sysctls[] = {\n \t\t.proc_handler   = proc_dointvec_minmax,\n \t\t.extra1         = SYSCTL_ZERO,\n \t},\n+\t{\n+\t\t.procname\t= \"pghot_promote_rate_limit_MBps\",\n+\t\t.data\t\t= &sysctl_pghot_promote_rate_limit,\n+\t\t.maxlen\t\t= sizeof(unsigned int),\n+\t\t.mode\t\t= 0644,\n+\t\t.proc_handler\t= proc_dointvec_minmax,\n+\t\t.extra1\t\t= SYSCTL_ZERO,\n+\t},\n };\n #endif\n \n@@ -138,6 +155,110 @@ int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now)\n \treturn 0;\n }\n \n+/*\n+ * For memory tiering mode, if there are enough free pages (more than\n+ * enough watermark defined here) in fast memory node, to take full\n+ * advantage of fast memory capacity, all recently accessed slow\n+ * memory pages will be migrated to fast memory node without\n+ * considering hot threshold.\n+ */\n+static bool pgdat_free_space_enough(struct pglist_data *pgdat)\n+{\n+\tint z;\n+\tunsigned long enough_wmark;\n+\n+\tenough_wmark = max(1UL * 1024 * 1024 * 1024 >> PAGE_SHIFT,\n+\t\t\t   pgdat->node_present_pages >> 4);\n+\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n+\t\tstruct zone *zone = pgdat->node_zones + z;\n+\n+\t\tif (!populated_zone(zone))\n+\t\t\tcontinue;\n+\n+\t\tif (zone_watermark_ok(zone, 0,\n+\t\t\t\t      promo_wmark_pages(zone) + enough_wmark,\n+\t\t\t\t      ZONE_MOVABLE, 0))\n+\t\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n+/*\n+ * For memory tiering mode, too high promotion/demotion throughput may\n+ * hurt application latency.  So we provide a mechanism to rate limit\n+ * the number of pages that are tried to be promoted.\n+ */\n+static bool kmigrated_promotion_rate_limit(struct pglist_data *pgdat, unsigned long rate_limit,\n+\t\t\t\t\t   int nr, unsigned long now_ms)\n+{\n+\tunsigned long nr_cand;\n+\tunsigned int start;\n+\n+\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE, nr);\n+\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n+\tstart = pgdat->nbp_rl_start;\n+\tif (now_ms - start > MSEC_PER_SEC &&\n+\t    cmpxchg(&pgdat->nbp_rl_start, start, now_ms) == start)\n+\t\tpgdat->nbp_rl_nr_cand = nr_cand;\n+\tif (nr_cand - pgdat->nbp_rl_nr_cand >= rate_limit)\n+\t\treturn true;\n+\treturn false;\n+}\n+\n+static void kmigrated_promotion_adjust_threshold(struct pglist_data *pgdat,\n+\t\t\t\t\t\t unsigned long rate_limit, unsigned int ref_th,\n+\t\t\t\t\t\t unsigned long now_ms)\n+{\n+\tunsigned int start, th_period, unit_th, th;\n+\tunsigned long nr_cand, ref_cand, diff_cand;\n+\n+\tth_period = KMIGRATED_PROMOTION_THRESHOLD_WINDOW;\n+\tstart = pgdat->nbp_th_start;\n+\tif (now_ms - start > th_period &&\n+\t    cmpxchg(&pgdat->nbp_th_start, start, now_ms) == start) {\n+\t\tref_cand = rate_limit *\n+\t\t\tKMIGRATED_PROMOTION_THRESHOLD_WINDOW / MSEC_PER_SEC;\n+\t\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n+\t\tdiff_cand = nr_cand - pgdat->nbp_th_nr_cand;\n+\t\tunit_th = ref_th * 2 / KMIGRATED_MIGRATION_ADJUST_STEPS;\n+\t\tth = pgdat->nbp_threshold ? : ref_th;\n+\t\tif (diff_cand > ref_cand * 11 / 10)\n+\t\t\tth = max(th - unit_th, unit_th);\n+\t\telse if (diff_cand < ref_cand * 9 / 10)\n+\t\t\tth = min(th + unit_th, ref_th * 2);\n+\t\tpgdat->nbp_th_nr_cand = nr_cand;\n+\t\tpgdat->nbp_threshold = th;\n+\t}\n+}\n+\n+static bool kmigrated_should_migrate_memory(unsigned long nr_pages, int nid,\n+\t\t\t\t\t    unsigned long time)\n+{\n+\tstruct pglist_data *pgdat;\n+\tunsigned long rate_limit;\n+\tunsigned int th, def_th;\n+\tunsigned long now_ms = jiffies_to_msecs(jiffies); /* Based on full-width jiffies */\n+\tunsigned long now = jiffies;\n+\n+\tpgdat = NODE_DATA(nid);\n+\tif (pgdat_free_space_enough(pgdat)) {\n+\t\t/* workload changed, reset hot threshold */\n+\t\tpgdat->nbp_threshold = 0;\n+\t\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE_NRL, nr_pages);\n+\t\treturn true;\n+\t}\n+\n+\tdef_th = sysctl_pghot_freq_window;\n+\trate_limit = MB_TO_PAGES(sysctl_pghot_promote_rate_limit);\n+\tkmigrated_promotion_adjust_threshold(pgdat, rate_limit, def_th, now_ms);\n+\n+\tth = pgdat->nbp_threshold ? : def_th;\n+\tif (pghot_access_latency(time, now) >= th)\n+\t\treturn false;\n+\n+\treturn !kmigrated_promotion_rate_limit(pgdat, rate_limit, nr_pages, now_ms);\n+}\n+\n static int pghot_get_hotness(unsigned long pfn, int *nid, int *freq,\n \t\t\t     unsigned long *time)\n {\n@@ -197,6 +318,9 @@ static void kmigrated_walk_zone(unsigned long start_pfn, unsigned long end_pfn,\n \t\tif (folio_nid(folio) == nid)\n \t\t\tgoto out_next;\n \n+\t\tif (!kmigrated_should_migrate_memory(nr, nid, time))\n+\t\t\tgoto out_next;\n+\n \t\tif (migrate_misplaced_folio_prepare(folio, NULL, nid))\n \t\t\tgoto out_next;\n \n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "20260129144043.231636-6-bharata@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed a concern about memory access tracking, explaining that the IBS (Instruction Based Sampling) feature in AMD processors can be used to track memory accesses and provide information such as physical address, virtual address, and whether the access came from a slower tier. The author plans to use this data for further action by the pghot subsystem.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "addressed_concern",
                "plans_to_implement"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Use IBS (Instruction Based Sampling) feature present\nin AMD processors for memory access tracking. The access\ninformation obtained from IBS via NMI is fed to pghot\nsub-system for futher action.\n\nIn addition to many other information related to the memory\naccess, IBS provides physical (and virtual) address of the access\nand indicates if the access came from slower tier. Only memory\naccesses originating from slower tiers are further acted upon\nby this driver.\n\nThe samples are initially accumulated in percpu buffers which\nare flushed to pghot hot page tracking mechanism using irq_work.\n\nTODO: Many counters are added to vmstat just as debugging aid\nfor now.\n\nAbout IBS\n---------\nIBS can be programmed to provide data about instruction\nexecution periodically. This is done by programming a desired\nsample count (number of ops) in a control register. When the\nprogrammed number of ops are dispatched, a micro-op gets tagged,\nvarious information about the tagged micro-op's execution is\npopulated in IBS execution MSRs and an interrupt is raised.\nWhile IBS provides a lot of data for each sample, for the\npurpose of  memory access profiling, we are interested in\nlinear and physical address of the memory access that reached\nDRAM. Recent AMD processors provide further filtering where\nit is possible to limit the sampling to those ops that had\nan L3 miss which greately reduces the non-useful samples.\n\nWhile IBS provides capability to sample instruction fetch\nand execution, only IBS execution sampling is used here\nto collect data about memory accesses that occur during\nthe instruction execution.\n\nMore information about IBS is available in Sec 13.3 of\nAMD64 Architecture Programmer's Manual, Volume 2:System\nProgramming which is present at:\nhttps://bugzilla.kernel.org/attachment.cgi?id=288923\n\nInformation about MSRs used for programming IBS can be\nfound in Sec 2.1.14.4 of PPR Vol 1 for AMD Family 19h\nModel 11h B1 which is currently present at:\nhttps://www.amd.com/system/files/TechDocs/55901_0.25.zip\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n arch/x86/events/amd/ibs.c        |  10 +\n arch/x86/include/asm/msr-index.h |  16 ++\n arch/x86/mm/Makefile             |   1 +\n arch/x86/mm/ibs.c                | 317 +++++++++++++++++++++++++++++++\n include/linux/pghot.h            |   8 +\n include/linux/vm_event_item.h    |  19 ++\n mm/Kconfig                       |  13 ++\n mm/vmstat.c                      |  19 ++\n 8 files changed, 403 insertions(+)\n create mode 100644 arch/x86/mm/ibs.c\n\ndiff --git a/arch/x86/events/amd/ibs.c b/arch/x86/events/amd/ibs.c\nindex aca89f23d2e0..dc544d084c17 100644\n--- a/arch/x86/events/amd/ibs.c\n+++ b/arch/x86/events/amd/ibs.c\n@@ -13,6 +13,7 @@\n #include <linux/ptrace.h>\n #include <linux/syscore_ops.h>\n #include <linux/sched/clock.h>\n+#include <linux/pghot.h>\n \n #include <asm/apic.h>\n #include <asm/msr.h>\n@@ -1760,6 +1761,15 @@ static __init int amd_ibs_init(void)\n {\n \tu32 caps;\n \n+\t/*\n+\t * TODO: Find a clean way to disable perf IBS so that IBS\n+\t * can be used for memory access profiling.\n+\t */\n+\tif (hwmem_access_profiler_inuse()) {\n+\t\tpr_info(\"IBS isn't available for perf use\\n\");\n+\t\treturn 0;\n+\t}\n+\n \tcaps = __get_ibs_caps();\n \tif (!caps)\n \t\treturn -ENODEV;\t/* ibs not supported by the cpu */\ndiff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h\nindex 3d0a0950d20a..3c5d69ec83a2 100644\n--- a/arch/x86/include/asm/msr-index.h\n+++ b/arch/x86/include/asm/msr-index.h\n@@ -784,6 +784,22 @@\n /* AMD Last Branch Record MSRs */\n #define MSR_AMD64_LBR_SELECT\t\t\t0xc000010e\n \n+/* AMD IBS MSR bits */\n+#define MSR_AMD64_IBSOPDATA2_DATASRC\t\t\t0x7\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_LCL_CACHE\t\t0x1\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_PEER_CACHE_NEAR\t0x2\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_DRAM\t\t0x3\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_FAR_CCX_CACHE\t0x5\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_EXT_MEM\t\t0x8\n+#define\tMSR_AMD64_IBSOPDATA2_RMTNODE\t\t\t0x10\n+\n+#define MSR_AMD64_IBSOPDATA3_LDOP\t\tBIT_ULL(0)\n+#define MSR_AMD64_IBSOPDATA3_STOP\t\tBIT_ULL(1)\n+#define MSR_AMD64_IBSOPDATA3_DCMISS\t\tBIT_ULL(7)\n+#define MSR_AMD64_IBSOPDATA3_LADDR_VALID\tBIT_ULL(17)\n+#define MSR_AMD64_IBSOPDATA3_PADDR_VALID\tBIT_ULL(18)\n+#define MSR_AMD64_IBSOPDATA3_L2MISS\t\tBIT_ULL(20)\n+\n /* Zen4 */\n #define MSR_ZEN4_BP_CFG                 0xc001102e\n #define MSR_ZEN4_BP_CFG_BP_SPEC_REDUCE_BIT 4\ndiff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile\nindex 5b9908f13dcf..361a456582e9 100644\n--- a/arch/x86/mm/Makefile\n+++ b/arch/x86/mm/Makefile\n@@ -57,3 +57,4 @@ obj-$(CONFIG_X86_MEM_ENCRYPT)\t+= mem_encrypt.o\n obj-$(CONFIG_AMD_MEM_ENCRYPT)\t+= mem_encrypt_amd.o\n \n obj-$(CONFIG_AMD_MEM_ENCRYPT)\t+= mem_encrypt_boot.o\n+obj-$(CONFIG_HWMEM_PROFILER)\t+= ibs.o\ndiff --git a/arch/x86/mm/ibs.c b/arch/x86/mm/ibs.c\nnew file mode 100644\nindex 000000000000..752f688375f9\n--- /dev/null\n+++ b/arch/x86/mm/ibs.c\n@@ -0,0 +1,317 @@\n+// SPDX-License-Identifier: GPL-2.0\n+\n+#include <linux/init.h>\n+#include <linux/pghot.h>\n+#include <linux/percpu.h>\n+#include <linux/workqueue.h>\n+#include <linux/irq_work.h>\n+\n+#include <asm/nmi.h>\n+#include <asm/perf_event.h> /* TODO: Move defns like IBS_OP_ENABLE into non-perf header */\n+#include <asm/apic.h>\n+\n+bool hwmem_access_profiling;\n+\n+static u64 ibs_config __read_mostly;\n+static u32 ibs_caps;\n+\n+#define IBS_NR_SAMPLES\t150\n+\n+/*\n+ * Basic access info captured for each memory access.\n+ */\n+struct ibs_sample {\n+\tunsigned long pfn;\n+\tunsigned long time;\t/* jiffies when accessed */\n+\tint nid;\t\t/* Accessing node ID, if known */\n+};\n+\n+/*\n+ * Percpu buffer of access samples. Samples are accumulated here\n+ * before pushing them to pghot sub-system for further action.\n+ */\n+struct ibs_sample_pcpu {\n+\tstruct ibs_sample samples[IBS_NR_SAMPLES];\n+\tint head, tail;\n+};\n+\n+struct ibs_sample_pcpu __percpu *ibs_s;\n+\n+/*\n+ * The workqueue for pushing the percpu access samples to pghot sub-system.\n+ */\n+static struct work_struct ibs_work;\n+static struct irq_work ibs_irq_work;\n+\n+bool hwmem_access_profiler_inuse(void)\n+{\n+\treturn hwmem_access_profiling;\n+}\n+\n+/*\n+ * Record the IBS-reported access sample in percpu buffer.\n+ * Called from IBS NMI handler.\n+ */\n+static int ibs_push_sample(unsigned long pfn, int nid, unsigned long time)\n+{\n+\tstruct ibs_sample_pcpu *ibs_pcpu = raw_cpu_ptr(ibs_s);\n+\tint next = ibs_pcpu->head + 1;\n+\n+\tif (next >= IBS_NR_SAMPLES)\n+\t\tnext = 0;\n+\n+\tif (next == ibs_pcpu->tail)\n+\t\treturn 0;\n+\n+\tibs_pcpu->samples[ibs_pcpu->head].pfn = pfn;\n+\tibs_pcpu->samples[ibs_pcpu->head].time = time;\n+\tibs_pcpu->samples[ibs_pcpu->head].nid = nid;\n+\tibs_pcpu->head = next;\n+\treturn 1;\n+}\n+\n+static int ibs_pop_sample(struct ibs_sample *s)\n+{\n+\tstruct ibs_sample_pcpu *ibs_pcpu = raw_cpu_ptr(ibs_s);\n+\n+\tint next = ibs_pcpu->tail + 1;\n+\n+\tif (ibs_pcpu->head == ibs_pcpu->tail)\n+\t\treturn 0;\n+\n+\tif (next >= IBS_NR_SAMPLES)\n+\t\tnext = 0;\n+\n+\t*s = ibs_pcpu->samples[ibs_pcpu->tail];\n+\tibs_pcpu->tail = next;\n+\treturn 1;\n+}\n+\n+/*\n+ * Remove access samples from percpu buffer and send them\n+ * to pghot sub-system for further action.\n+ */\n+static void ibs_work_handler(struct work_struct *work)\n+{\n+\tstruct ibs_sample s;\n+\n+\twhile (ibs_pop_sample(&s))\n+\t\tpghot_record_access(s.pfn, s.nid, PGHOT_HW_HINTS, s.time);\n+}\n+\n+static void ibs_irq_handler(struct irq_work *i)\n+{\n+\tschedule_work_on(smp_processor_id(), &ibs_work);\n+}\n+\n+/*\n+ * IBS NMI handler: Process the memory access info reported by IBS.\n+ *\n+ * Reads the MSRs to collect all the information about the reported\n+ * memory access, validates the access, stores the valid sample and\n+ * schedules the work on this CPU to further process the sample.\n+ */\n+static int ibs_overflow_handler(unsigned int cmd, struct pt_regs *regs)\n+{\n+\tstruct mm_struct *mm = current->mm;\n+\tu64 ops_ctl, ops_data3, ops_data2;\n+\tu64 laddr = -1, paddr = -1;\n+\tu64 data_src, rmt_node;\n+\tstruct page *page;\n+\tunsigned long pfn;\n+\n+\trdmsrl(MSR_AMD64_IBSOPCTL, ops_ctl);\n+\n+\t/*\n+\t * When IBS sampling period is reprogrammed via read-modify-update\n+\t * of MSR_AMD64_IBSOPCTL, overflow NMIs could be generated with\n+\t * IBS_OP_ENABLE not set. For such cases, return as HANDLED.\n+\t *\n+\t * With this, the handler will say \"handled\" for all NMIs that\n+\t * aren't related to this NMI.  This stems from the limitation of\n+\t * having both status and control bits in one MSR.\n+\t */\n+\tif (!(ops_ctl & IBS_OP_VAL))\n+\t\tgoto handled;\n+\n+\twrmsrl(MSR_AMD64_IBSOPCTL, ops_ctl & ~IBS_OP_VAL);\n+\n+\tcount_vm_event(HWHINT_NR_EVENTS);\n+\n+\tif (!user_mode(regs)) {\n+\t\tcount_vm_event(HWHINT_KERNEL);\n+\t\tgoto handled;\n+\t}\n+\n+\tif (!mm) {\n+\t\tcount_vm_event(HWHINT_KTHREAD);\n+\t\tgoto handled;\n+\t}\n+\n+\trdmsrl(MSR_AMD64_IBSOPDATA3, ops_data3);\n+\n+\t/* Load/Store ops only */\n+\t/* TODO: DataSrc isn't valid for stores, so filter out stores? */\n+\tif (!(ops_data3 & (MSR_AMD64_IBSOPDATA3_LDOP |\n+\t\t\t   MSR_AMD64_IBSOPDATA3_STOP))) {\n+\t\tcount_vm_event(HWHINT_NON_LOAD_STORES);\n+\t\tgoto handled;\n+\t}\n+\n+\t/* Discard the sample if it was L1 or L2 hit */\n+\tif (!(ops_data3 & (MSR_AMD64_IBSOPDATA3_DCMISS |\n+\t\t\t   MSR_AMD64_IBSOPDATA3_L2MISS))) {\n+\t\tcount_vm_event(HWHINT_DC_L2_HITS);\n+\t\tgoto handled;\n+\t}\n+\n+\trdmsrl(MSR_AMD64_IBSOPDATA2, ops_data2);\n+\tdata_src = ops_data2 & MSR_AMD64_IBSOPDATA2_DATASRC;\n+\tif (ibs_caps & IBS_CAPS_ZEN4)\n+\t\tdata_src |= ((ops_data2 & 0xC0) >> 3);\n+\n+\tswitch (data_src) {\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_LCL_CACHE:\n+\t\tcount_vm_event(HWHINT_LOCAL_L3L1L2);\n+\t\tbreak;\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_PEER_CACHE_NEAR:\n+\t\tcount_vm_event(HWHINT_LOCAL_PEER_CACHE_NEAR);\n+\t\tbreak;\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_DRAM:\n+\t\tcount_vm_event(HWHINT_DRAM_ACCESSES);\n+\t\tbreak;\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_EXT_MEM:\n+\t\tcount_vm_event(HWHINT_CXL_ACCESSES);\n+\t\tbreak;\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_FAR_CCX_CACHE:\n+\t\tcount_vm_event(HWHINT_FAR_CACHE_HITS);\n+\t\tbreak;\n+\t}\n+\n+\trmt_node = ops_data2 & MSR_AMD64_IBSOPDATA2_RMTNODE;\n+\tif (rmt_node)\n+\t\tcount_vm_event(HWHINT_REMOTE_NODE);\n+\n+\t/* Is linear addr valid? */\n+\tif (ops_data3 & MSR_AMD64_IBSOPDATA3_LADDR_VALID)\n+\t\trdmsrl(MSR_AMD64_IBSDCLINAD, laddr);\n+\telse {\n+\t\tcount_vm_event(HWHINT_LADDR_INVALID);\n+\t\tgoto handled;\n+\t}\n+\n+\t/* Discard kernel address accesses */\n+\tif (laddr & (1UL << 63)) {\n+\t\tcount_vm_event(HWHINT_KERNEL_ADDR);\n+\t\tgoto handled;\n+\t}\n+\n+\t/* Is phys addr valid? */\n+\tif (ops_data3 & MSR_AMD64_IBSOPDATA3_PADDR_VALID)\n+\t\trdmsrl(MSR_AMD64_IBSDCPHYSAD, paddr);\n+\telse {\n+\t\tcount_vm_event(HWHINT_PADDR_INVALID);\n+\t\tgoto handled;\n+\t}\n+\n+\tpfn = PHYS_PFN(paddr);\n+\tpage = pfn_to_online_page(pfn);\n+\tif (!page)\n+\t\tgoto handled;\n+\n+\tif (!PageLRU(page)) {\n+\t\tcount_vm_event(HWHINT_NON_LRU);\n+\t\tgoto handled;\n+\t}\n+\n+\tif (!ibs_push_sample(pfn, numa_node_id(), jiffies)) {\n+\t\tcount_vm_event(HWHINT_BUFFER_FULL);\n+\t\tgoto handled;\n+\t}\n+\n+\tirq_work_queue(&ibs_irq_work);\n+\tcount_vm_event(HWHINT_USEFUL_SAMPLES);\n+\n+handled:\n+\treturn NMI_HANDLED;\n+}\n+\n+static inline int get_ibs_lvt_offset(void)\n+{\n+\tu64 val;\n+\n+\trdmsrl(MSR_AMD64_IBSCTL, val);\n+\tif (!(val & IBSCTL_LVT_OFFSET_VALID))\n+\t\treturn -EINVAL;\n+\n+\treturn val & IBSCTL_LVT_OFFSET_MASK;\n+}\n+\n+static void setup_APIC_ibs(void)\n+{\n+\tint offset;\n+\n+\toffset = get_ibs_lvt_offset();\n+\tif (offset < 0)\n+\t\tgoto failed;\n+\n+\tif (!setup_APIC_eilvt(offset, 0, APIC_EILVT_MSG_NMI, 0))\n+\t\treturn;\n+failed:\n+\tpr_warn(\"IBS APIC setup failed on cpu #%d\\n\",\n+\t\tsmp_processor_id());\n+}\n+\n+static void clear_APIC_ibs(void)\n+{\n+\tint offset;\n+\n+\toffset = get_ibs_lvt_offset();\n+\tif (offset >= 0)\n+\t\tsetup_APIC_eilvt(offset, 0, APIC_EILVT_MSG_FIX, 1);\n+}\n+\n+static int x86_amd_ibs_access_profile_startup(unsigned int cpu)\n+{\n+\tsetup_APIC_ibs();\n+\treturn 0;\n+}\n+\n+static int x86_amd_ibs_access_profile_teardown(unsigned int cpu)\n+{\n+\tclear_APIC_ibs();\n+\treturn 0;\n+}\n+\n+static int __init ibs_access_profiling_init(void)\n+{\n+\tif (!boot_cpu_has(X86_FEATURE_IBS)) {\n+\t\tpr_info(\"IBS capability is unavailable for access profiling\\n\");\n+\t\treturn 0;\n+\t}\n+\n+\tibs_s = alloc_percpu_gfp(struct ibs_sample_pcpu, GFP_KERNEL | __GFP_ZERO);\n+\tif (!ibs_s)\n+\t\treturn 0;\n+\n+\tINIT_WORK(&ibs_work, ibs_work_handler);\n+\tinit_irq_work(&ibs_irq_work, ibs_irq_handler);\n+\n+\t/* Uses IBS Op sampling */\n+\tibs_config = IBS_OP_CNT_CTL | IBS_OP_ENABLE;\n+\tibs_caps = cpuid_eax(IBS_CPUID_FEATURES);\n+\tif (ibs_caps & IBS_CAPS_ZEN4)\n+\t\tibs_config |= IBS_OP_L3MISSONLY;\n+\n+\tregister_nmi_handler(NMI_LOCAL, ibs_overflow_handler, 0, \"ibs\");\n+\n+\tcpuhp_setup_state(CPUHP_AP_PERF_X86_AMD_IBS_STARTING,\n+\t\t\t  \"x86/amd/ibs_access_profile:starting\",\n+\t\t\t  x86_amd_ibs_access_profile_startup,\n+\t\t\t  x86_amd_ibs_access_profile_teardown);\n+\n+\tpr_info(\"IBS setup for memory access profiling\\n\");\n+\treturn 0;\n+}\n+\n+arch_initcall(ibs_access_profiling_init);\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nindex d3d59b0c0cf6..20ea9767dbdd 100644\n--- a/include/linux/pghot.h\n+++ b/include/linux/pghot.h\n@@ -2,6 +2,14 @@\n #ifndef _LINUX_PGHOT_H\n #define _LINUX_PGHOT_H\n \n+#include <linux/types.h>\n+\n+#ifdef CONFIG_HWMEM_PROFILER\n+bool hwmem_access_profiler_inuse(void);\n+#else\n+static inline bool hwmem_access_profiler_inuse(void) { return false; }\n+#endif\n+\n /* Page hotness temperature sources */\n enum pghot_src {\n \tPGHOT_HW_HINTS,\ndiff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\nindex 5b8fd93b55fd..67efbca9051c 100644\n--- a/include/linux/vm_event_item.h\n+++ b/include/linux/vm_event_item.h\n@@ -193,6 +193,25 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n \t\tPGHOT_RECORD_HWHINTS,\n \t\tPGHOT_RECORD_PGTSCANS,\n \t\tPGHOT_RECORD_HINTFAULTS,\n+#ifdef CONFIG_HWMEM_PROFILER\n+\t\tHWHINT_NR_EVENTS,\n+\t\tHWHINT_KERNEL,\n+\t\tHWHINT_KTHREAD,\n+\t\tHWHINT_NON_LOAD_STORES,\n+\t\tHWHINT_DC_L2_HITS,\n+\t\tHWHINT_LOCAL_L3L1L2,\n+\t\tHWHINT_LOCAL_PEER_CACHE_NEAR,\n+\t\tHWHINT_FAR_CACHE_HITS,\n+\t\tHWHINT_DRAM_ACCESSES,\n+\t\tHWHINT_CXL_ACCESSES,\n+\t\tHWHINT_REMOTE_NODE,\n+\t\tHWHINT_LADDR_INVALID,\n+\t\tHWHINT_KERNEL_ADDR,\n+\t\tHWHINT_PADDR_INVALID,\n+\t\tHWHINT_NON_LRU,\n+\t\tHWHINT_BUFFER_FULL,\n+\t\tHWHINT_USEFUL_SAMPLES,\n+#endif /* CONFIG_HWMEM_PROFILER */\n #endif /* CONFIG_PGHOT */\n \t\tNR_VM_EVENT_ITEMS\n };\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex fde5aee3e16f..07b16aece877 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -1489,6 +1489,19 @@ config PGHOT_PRECISE\n \t  4 bytes per page against the default one byte per page. Preferable\n \t  to enable this on systems with multiple nodes in toptier.\n \n+config HWMEM_PROFILER\n+\tbool \"HW based memory access profiling\"\n+\tdef_bool n\n+\tdepends on PGHOT\n+\tdepends on X86_64\n+\thelp\n+\t  Some hardware platforms are capable of providing memory access\n+\t  information in direct and actionable manner. Instruction Based\n+\t  Sampling (IBS) present on AMD Zen CPUs in one such example.\n+\t  Memory accesses obtained via such HW based mechanisms are\n+\t  rolled up to PGHOT sub-system for further action like hot page\n+\t  promotion or NUMA Balancing\n+\n source \"mm/damon/Kconfig\"\n \n endmenu\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex f6f91b9dd887..62c47f44edf0 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -1506,6 +1506,25 @@ const char * const vmstat_text[] = {\n \t[I(PGHOT_RECORD_HWHINTS)]\t\t= \"pghot_recorded_hwhints\",\n \t[I(PGHOT_RECORD_PGTSCANS)]\t\t= \"pghot_recorded_pgtscans\",\n \t[I(PGHOT_RECORD_HINTFAULTS)]\t\t= \"pghot_recorded_hintfaults\",\n+#ifdef CONFIG_HWMEM_PROFILER\n+\t[I(HWHINT_NR_EVENTS)]\t\t\t= \"hwhint_nr_events\",\n+\t[I(HWHINT_KERNEL)]\t\t\t= \"hwhint_kernel\",\n+\t[I(HWHINT_KTHREAD)]\t\t\t= \"hwhint_kthread\",\n+\t[I(HWHINT_NON_LOAD_STORES)]\t\t= \"hwhint_non_load_stores\",\n+\t[I(HWHINT_DC_L2_HITS)]\t\t\t= \"hwhint_dc_l2_hits\",\n+\t[I(HWHINT_LOCAL_L3L1L2)]\t\t= \"hwhint_local_l3l1l2\",\n+\t[I(HWHINT_LOCAL_PEER_CACHE_NEAR)]\t= \"hwhint_local_peer_cache_near\",\n+\t[I(HWHINT_FAR_CACHE_HITS)]\t\t= \"hwhint_far_cache_hits\",\n+\t[I(HWHINT_DRAM_ACCESSES)]\t\t= \"hwhint_dram_accesses\",\n+\t[I(HWHINT_CXL_ACCESSES)]\t\t= \"hwhint_cxl_accesses\",\n+\t[I(HWHINT_REMOTE_NODE)]\t\t\t= \"hwhint_remote_node\",\n+\t[I(HWHINT_LADDR_INVALID)]\t\t= \"hwhint_invalid_laddr\",\n+\t[I(HWHINT_KERNEL_ADDR)]\t\t\t= \"hwhint_kernel_addr\",\n+\t[I(HWHINT_PADDR_INVALID)]\t\t= \"hwhint_invalid_paddr\",\n+\t[I(HWHINT_NON_LRU)]\t\t\t= \"hwhint_non_lru\",\n+\t[I(HWHINT_BUFFER_FULL)]\t\t\t= \"hwhint_buffer_full\",\n+\t[I(HWHINT_USEFUL_SAMPLES)]\t\t= \"hwhint_useful_samples\",\n+#endif /* CONFIG_HWMEM_PROFILER */\n #endif /* CONFIG_PGHOT */\n #undef I\n #endif /* CONFIG_VM_EVENT_COUNTERS */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "20260129144043.231636-7-bharata@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed a concern about IBS memory access data collection for user memory accesses, explaining that profiling is turned ON only for user mode execution and turned OFF for kernel mode execution. The author also acknowledged TODOs related to IBS sampling rate and arch/vendor separation/isolation of the code.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "TODOs",
                "acknowledged"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Enable IBS memory access data collection for user memory\naccesses by programming the required MSRs. The profiling\nis turned ON only for user mode execution and turned OFF\nfor kernel mode execution. Profiling is explicitly disabled\nfor NMI handler too.\n\nTODOs:\n\n- IBS sampling rate is kept fixed for now.\n- Arch/vendor separation/isolation of the code needs relook.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n arch/x86/include/asm/entry-common.h |  3 +++\n arch/x86/include/asm/hardirq.h      |  2 ++\n arch/x86/mm/ibs.c                   | 32 +++++++++++++++++++++++++++++\n include/linux/pghot.h               |  4 ++++\n 4 files changed, 41 insertions(+)\n\ndiff --git a/arch/x86/include/asm/entry-common.h b/arch/x86/include/asm/entry-common.h\nindex ce3eb6d5fdf9..0f381a63669e 100644\n--- a/arch/x86/include/asm/entry-common.h\n+++ b/arch/x86/include/asm/entry-common.h\n@@ -4,6 +4,7 @@\n \n #include <linux/randomize_kstack.h>\n #include <linux/user-return-notifier.h>\n+#include <linux/pghot.h>\n \n #include <asm/nospec-branch.h>\n #include <asm/io_bitmap.h>\n@@ -13,6 +14,7 @@\n /* Check that the stack and regs on entry from user mode are sane. */\n static __always_inline void arch_enter_from_user_mode(struct pt_regs *regs)\n {\n+\thwmem_access_profiling_stop();\n \tif (IS_ENABLED(CONFIG_DEBUG_ENTRY)) {\n \t\t/*\n \t\t * Make sure that the entry code gave us a sensible EFLAGS\n@@ -106,6 +108,7 @@ static inline void arch_exit_to_user_mode_prepare(struct pt_regs *regs,\n static __always_inline void arch_exit_to_user_mode(void)\n {\n \tamd_clear_divider();\n+\thwmem_access_profiling_start();\n }\n #define arch_exit_to_user_mode arch_exit_to_user_mode\n \ndiff --git a/arch/x86/include/asm/hardirq.h b/arch/x86/include/asm/hardirq.h\nindex 6b6d472baa0b..e80c305c17d1 100644\n--- a/arch/x86/include/asm/hardirq.h\n+++ b/arch/x86/include/asm/hardirq.h\n@@ -91,4 +91,6 @@ static __always_inline bool kvm_get_cpu_l1tf_flush_l1d(void)\n static __always_inline void kvm_set_cpu_l1tf_flush_l1d(void) { }\n #endif /* IS_ENABLED(CONFIG_KVM_INTEL) */\n \n+#define arch_nmi_enter()\thwmem_access_profiling_stop()\n+#define arch_nmi_exit()\t\thwmem_access_profiling_start()\n #endif /* _ASM_X86_HARDIRQ_H */\ndiff --git a/arch/x86/mm/ibs.c b/arch/x86/mm/ibs.c\nindex 752f688375f9..d0d93f09432d 100644\n--- a/arch/x86/mm/ibs.c\n+++ b/arch/x86/mm/ibs.c\n@@ -16,6 +16,7 @@ static u64 ibs_config __read_mostly;\n static u32 ibs_caps;\n \n #define IBS_NR_SAMPLES\t150\n+#define IBS_SAMPLE_PERIOD      10000\n \n /*\n  * Basic access info captured for each memory access.\n@@ -43,6 +44,36 @@ struct ibs_sample_pcpu __percpu *ibs_s;\n static struct work_struct ibs_work;\n static struct irq_work ibs_irq_work;\n \n+void hwmem_access_profiling_stop(void)\n+{\n+\tu64 ops_ctl;\n+\n+\tif (!hwmem_access_profiling)\n+\t\treturn;\n+\n+\trdmsrl(MSR_AMD64_IBSOPCTL, ops_ctl);\n+\twrmsrl(MSR_AMD64_IBSOPCTL, ops_ctl & ~IBS_OP_ENABLE);\n+}\n+\n+void hwmem_access_profiling_start(void)\n+{\n+\tu64 config = 0;\n+\tunsigned int period = IBS_SAMPLE_PERIOD;\n+\n+\tif (!hwmem_access_profiling)\n+\t\treturn;\n+\n+\t/* Disable IBS for kernel thread */\n+\tif (!current->mm)\n+\t\tgoto out;\n+\n+\tconfig = (period >> 4) & IBS_OP_MAX_CNT;\n+\tconfig |= (period & IBS_OP_MAX_CNT_EXT_MASK);\n+\tconfig |= ibs_config;\n+out:\n+\twrmsrl(MSR_AMD64_IBSOPCTL, config);\n+}\n+\n bool hwmem_access_profiler_inuse(void)\n {\n \treturn hwmem_access_profiling;\n@@ -310,6 +341,7 @@ static int __init ibs_access_profiling_init(void)\n \t\t\t  x86_amd_ibs_access_profile_startup,\n \t\t\t  x86_amd_ibs_access_profile_teardown);\n \n+\thwmem_access_profiling = true;\n \tpr_info(\"IBS setup for memory access profiling\\n\");\n \treturn 0;\n }\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nindex 20ea9767dbdd..603791183102 100644\n--- a/include/linux/pghot.h\n+++ b/include/linux/pghot.h\n@@ -6,8 +6,12 @@\n \n #ifdef CONFIG_HWMEM_PROFILER\n bool hwmem_access_profiler_inuse(void);\n+void hwmem_access_profiling_start(void);\n+void hwmem_access_profiling_stop(void);\n #else\n static inline bool hwmem_access_profiler_inuse(void) { return false; }\n+static inline void hwmem_access_profiling_start(void) {}\n+static inline void hwmem_access_profiling_stop(void) {}\n #endif\n \n /* Page hotness temperature sources */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "20260129144043.231636-8-bharata@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author is addressing a concern about the existing MGLRU page table walking logic not being resumable, and has proposed refactoring it to make it resumable by introducing two new hooks: accessed callback and flush callback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "refactor",
                "new hooks"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Kinsey Ho <kinseyho@google.com>\n\nRefactor the existing MGLRU page table walking logic to make it\nresumable.\n\nAdditionally, introduce two hooks into the MGLRU page table walk:\naccessed callback and flush callback. The accessed callback is called\nfor each accessed page detected via the scanned accessed bit. The flush\ncallback is called when the accessed callback reports that a flush is\nrequired. This allows for processing pages in batches for efficiency.\n\nWith a generalised page table walk, introduce a new scan function which\nrepeatedly scans on the same young generation and does not add a new\nyoung generation.\n\nSigned-off-by: Kinsey Ho <kinseyho@google.com>\nSigned-off-by: Yuanchu Xie <yuanchu@google.com>\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n include/linux/mmzone.h |   5 ++\n mm/internal.h          |   4 +\n mm/vmscan.c            | 181 +++++++++++++++++++++++++++++++----------\n 3 files changed, 145 insertions(+), 45 deletions(-)\n\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 49c374064fc2..26350a4951ff 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -548,6 +548,8 @@ struct lru_gen_mm_walk {\n \tunsigned long seq;\n \t/* the next address within an mm to scan */\n \tunsigned long next_addr;\n+\t/* called for each accessed pte/pmd */\n+\tbool (*accessed_cb)(unsigned long pfn);\n \t/* to batch promoted pages */\n \tint nr_pages[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES];\n \t/* to batch the mm stats */\n@@ -555,6 +557,9 @@ struct lru_gen_mm_walk {\n \t/* total batched items */\n \tint batched;\n \tint swappiness;\n+\t/* for the pmd under scanning */\n+\tint nr_young_pte;\n+\tint nr_total_pte;\n \tbool force_scan;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex e430da900430..426db1ae286f 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -538,6 +538,10 @@ extern unsigned long highest_memmap_pfn;\n bool folio_isolate_lru(struct folio *folio);\n void folio_putback_lru(struct folio *folio);\n extern void reclaim_throttle(pg_data_t *pgdat, enum vmscan_throttle_state reason);\n+void set_task_reclaim_state(struct task_struct *task,\n+\t\t\t\t   struct reclaim_state *rs);\n+void lru_gen_scan_lruvec(struct lruvec *lruvec, unsigned long seq,\n+\t\t\t bool (*accessed_cb)(unsigned long), void (*flush_cb)(void));\n #ifdef CONFIG_NUMA\n int user_proactive_reclaim(char *buf,\n \t\t\t   struct mem_cgroup *memcg, pg_data_t *pgdat);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 670fe9fae5ba..02f3dd128638 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -289,7 +289,7 @@ static int sc_swappiness(struct scan_control *sc, struct mem_cgroup *memcg)\n \t\t\tcontinue;\t\t\t\t\\\n \t\telse\n \n-static void set_task_reclaim_state(struct task_struct *task,\n+void set_task_reclaim_state(struct task_struct *task,\n \t\t\t\t   struct reclaim_state *rs)\n {\n \t/* Check for an overwrite */\n@@ -3058,7 +3058,7 @@ static bool iterate_mm_list(struct lru_gen_mm_walk *walk, struct mm_struct **ite\n \n \tVM_WARN_ON_ONCE(mm_state->seq + 1 < walk->seq);\n \n-\tif (walk->seq <= mm_state->seq)\n+\tif (!walk->accessed_cb && walk->seq <= mm_state->seq)\n \t\tgoto done;\n \n \tif (!mm_state->head)\n@@ -3484,16 +3484,14 @@ static void walk_update_folio(struct lru_gen_mm_walk *walk, struct folio *folio,\n \t}\n }\n \n-static bool walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n-\t\t\t   struct mm_walk *args)\n+static int walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n+\t\t\t   struct mm_walk *args, bool *suitable)\n {\n \tint i;\n \tbool dirty;\n \tpte_t *pte;\n \tspinlock_t *ptl;\n \tunsigned long addr;\n-\tint total = 0;\n-\tint young = 0;\n \tstruct folio *last = NULL;\n \tstruct lru_gen_mm_walk *walk = args->private;\n \tstruct mem_cgroup *memcg = lruvec_memcg(walk->lruvec);\n@@ -3501,19 +3499,24 @@ static bool walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n \tDEFINE_MAX_SEQ(walk->lruvec);\n \tint gen = lru_gen_from_seq(max_seq);\n \tpmd_t pmdval;\n+\tint err = 0;\n \n \tpte = pte_offset_map_rw_nolock(args->mm, pmd, start & PMD_MASK, &pmdval, &ptl);\n-\tif (!pte)\n-\t\treturn false;\n+\tif (!pte) {\n+\t\t*suitable = false;\n+\t\treturn err;\n+\t}\n \n \tif (!spin_trylock(ptl)) {\n \t\tpte_unmap(pte);\n-\t\treturn true;\n+\t\t*suitable = true;\n+\t\treturn err;\n \t}\n \n \tif (unlikely(!pmd_same(pmdval, pmdp_get_lockless(pmd)))) {\n \t\tpte_unmap_unlock(pte, ptl);\n-\t\treturn false;\n+\t\t*suitable = false;\n+\t\treturn err;\n \t}\n \n \tarch_enter_lazy_mmu_mode();\n@@ -3522,8 +3525,9 @@ static bool walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n \t\tunsigned long pfn;\n \t\tstruct folio *folio;\n \t\tpte_t ptent = ptep_get(pte + i);\n+\t\tbool do_flush;\n \n-\t\ttotal++;\n+\t\twalk->nr_total_pte++;\n \t\twalk->mm_stats[MM_LEAF_TOTAL]++;\n \n \t\tpfn = get_pte_pfn(ptent, args->vma, addr, pgdat);\n@@ -3547,23 +3551,36 @@ static bool walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n \t\tif (pte_dirty(ptent))\n \t\t\tdirty = true;\n \n-\t\tyoung++;\n+\t\twalk->nr_young_pte++;\n \t\twalk->mm_stats[MM_LEAF_YOUNG]++;\n+\n+\t\tif (!walk->accessed_cb)\n+\t\t\tcontinue;\n+\n+\t\tdo_flush = walk->accessed_cb(pfn);\n+\t\tif (do_flush) {\n+\t\t\twalk->next_addr = addr + PAGE_SIZE;\n+\n+\t\t\terr = -EAGAIN;\n+\t\t\tbreak;\n+\t\t}\n \t}\n \n \twalk_update_folio(walk, last, gen, dirty);\n \tlast = NULL;\n \n-\tif (i < PTRS_PER_PTE && get_next_vma(PMD_MASK, PAGE_SIZE, args, &start, &end))\n+\tif (!err && i < PTRS_PER_PTE &&\n+\t    get_next_vma(PMD_MASK, PAGE_SIZE, args, &start, &end))\n \t\tgoto restart;\n \n \tarch_leave_lazy_mmu_mode();\n \tpte_unmap_unlock(pte, ptl);\n \n-\treturn suitable_to_scan(total, young);\n+\t*suitable = suitable_to_scan(walk->nr_total_pte, walk->nr_young_pte);\n+\treturn err;\n }\n \n-static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area_struct *vma,\n+static int walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area_struct *vma,\n \t\t\t\t  struct mm_walk *args, unsigned long *bitmap, unsigned long *first)\n {\n \tint i;\n@@ -3576,6 +3593,7 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \tstruct pglist_data *pgdat = lruvec_pgdat(walk->lruvec);\n \tDEFINE_MAX_SEQ(walk->lruvec);\n \tint gen = lru_gen_from_seq(max_seq);\n+\tint err = 0;\n \n \tVM_WARN_ON_ONCE(pud_leaf(*pud));\n \n@@ -3583,13 +3601,13 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \tif (*first == -1) {\n \t\t*first = addr;\n \t\tbitmap_zero(bitmap, MIN_LRU_BATCH);\n-\t\treturn;\n+\t\treturn err;\n \t}\n \n \ti = addr == -1 ? 0 : pmd_index(addr) - pmd_index(*first);\n \tif (i && i <= MIN_LRU_BATCH) {\n \t\t__set_bit(i - 1, bitmap);\n-\t\treturn;\n+\t\treturn err;\n \t}\n \n \tpmd = pmd_offset(pud, *first);\n@@ -3603,6 +3621,7 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \tdo {\n \t\tunsigned long pfn;\n \t\tstruct folio *folio;\n+\t\tbool do_flush;\n \n \t\t/* don't round down the first address */\n \t\taddr = i ? (*first & PMD_MASK) + i * PMD_SIZE : *first;\n@@ -3639,6 +3658,17 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \t\t\tdirty = true;\n \n \t\twalk->mm_stats[MM_LEAF_YOUNG]++;\n+\t\tif (!walk->accessed_cb)\n+\t\t\tgoto next;\n+\n+\t\tdo_flush = walk->accessed_cb(pfn);\n+\t\tif (do_flush) {\n+\t\t\ti = find_next_bit(bitmap, MIN_LRU_BATCH, i) + 1;\n+\n+\t\t\twalk->next_addr = (*first & PMD_MASK) + i * PMD_SIZE;\n+\t\t\terr = -EAGAIN;\n+\t\t\tbreak;\n+\t\t}\n next:\n \t\ti = i > MIN_LRU_BATCH ? 0 : find_next_bit(bitmap, MIN_LRU_BATCH, i) + 1;\n \t} while (i <= MIN_LRU_BATCH);\n@@ -3649,9 +3679,10 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \tspin_unlock(ptl);\n done:\n \t*first = -1;\n+\treturn err;\n }\n \n-static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n+static int walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \t\t\t   struct mm_walk *args)\n {\n \tint i;\n@@ -3663,6 +3694,7 @@ static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \tunsigned long first = -1;\n \tstruct lru_gen_mm_walk *walk = args->private;\n \tstruct lru_gen_mm_state *mm_state = get_mm_state(walk->lruvec);\n+\tint err = 0;\n \n \tVM_WARN_ON_ONCE(pud_leaf(*pud));\n \n@@ -3676,6 +3708,7 @@ static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \t/* walk_pte_range() may call get_next_vma() */\n \tvma = args->vma;\n \tfor (i = pmd_index(start), addr = start; addr != end; i++, addr = next) {\n+\t\tbool suitable;\n \t\tpmd_t val = pmdp_get_lockless(pmd + i);\n \n \t\tnext = pmd_addr_end(addr, end);\n@@ -3692,7 +3725,10 @@ static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \t\t\twalk->mm_stats[MM_LEAF_TOTAL]++;\n \n \t\t\tif (pfn != -1)\n-\t\t\t\twalk_pmd_range_locked(pud, addr, vma, args, bitmap, &first);\n+\t\t\t\terr = walk_pmd_range_locked(pud, addr, vma, args,\n+\t\t\t\t\t\tbitmap, &first);\n+\t\t\tif (err)\n+\t\t\t\treturn err;\n \t\t\tcontinue;\n \t\t}\n \n@@ -3701,33 +3737,51 @@ static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \t\t\tif (!pmd_young(val))\n \t\t\t\tcontinue;\n \n-\t\t\twalk_pmd_range_locked(pud, addr, vma, args, bitmap, &first);\n+\t\t\terr = walk_pmd_range_locked(pud, addr, vma, args,\n+\t\t\t\t\t\tbitmap, &first);\n+\t\t\tif (err)\n+\t\t\t\treturn err;\n \t\t}\n \n \t\tif (!walk->force_scan && !test_bloom_filter(mm_state, walk->seq, pmd + i))\n \t\t\tcontinue;\n \n+\t\terr = walk_pte_range(&val, addr, next, args, &suitable);\n+\t\tif (err && walk->next_addr < next && first == -1)\n+\t\t\treturn err;\n+\n+\t\twalk->nr_total_pte = 0;\n+\t\twalk->nr_young_pte = 0;\n+\n \t\twalk->mm_stats[MM_NONLEAF_FOUND]++;\n \n-\t\tif (!walk_pte_range(&val, addr, next, args))\n-\t\t\tcontinue;\n+\t\tif (!suitable)\n+\t\t\tgoto next;\n \n \t\twalk->mm_stats[MM_NONLEAF_ADDED]++;\n \n \t\t/* carry over to the next generation */\n \t\tupdate_bloom_filter(mm_state, walk->seq + 1, pmd + i);\n+next:\n+\t\tif (err) {\n+\t\t\twalk->next_addr = first;\n+\t\t\treturn err;\n+\t\t}\n \t}\n \n-\twalk_pmd_range_locked(pud, -1, vma, args, bitmap, &first);\n+\terr = walk_pmd_range_locked(pud, -1, vma, args, bitmap, &first);\n \n-\tif (i < PTRS_PER_PMD && get_next_vma(PUD_MASK, PMD_SIZE, args, &start, &end))\n+\tif (!err && i < PTRS_PER_PMD &&\n+\t    get_next_vma(PUD_MASK, PMD_SIZE, args, &start, &end))\n \t\tgoto restart;\n+\n+\treturn err;\n }\n \n static int walk_pud_range(p4d_t *p4d, unsigned long start, unsigned long end,\n \t\t\t  struct mm_walk *args)\n {\n-\tint i;\n+\tint i, err;\n \tpud_t *pud;\n \tunsigned long addr;\n \tunsigned long next;\n@@ -3745,7 +3799,9 @@ static int walk_pud_range(p4d_t *p4d, unsigned long start, unsigned long end,\n \t\tif (!pud_present(val) || WARN_ON_ONCE(pud_leaf(val)))\n \t\t\tcontinue;\n \n-\t\twalk_pmd_range(&val, addr, next, args);\n+\t\terr = walk_pmd_range(&val, addr, next, args);\n+\t\tif (err)\n+\t\t\treturn err;\n \n \t\tif (need_resched() || walk->batched >= MAX_LRU_BATCH) {\n \t\t\tend = (addr | ~PUD_MASK) + 1;\n@@ -3766,40 +3822,48 @@ static int walk_pud_range(p4d_t *p4d, unsigned long start, unsigned long end,\n \treturn -EAGAIN;\n }\n \n-static void walk_mm(struct mm_struct *mm, struct lru_gen_mm_walk *walk)\n+static int try_walk_mm(struct mm_struct *mm, struct lru_gen_mm_walk *walk)\n {\n+\tint err;\n \tstatic const struct mm_walk_ops mm_walk_ops = {\n \t\t.test_walk = should_skip_vma,\n \t\t.p4d_entry = walk_pud_range,\n \t\t.walk_lock = PGWALK_RDLOCK,\n \t};\n-\tint err;\n \tstruct lruvec *lruvec = walk->lruvec;\n \n-\twalk->next_addr = FIRST_USER_ADDRESS;\n+\tDEFINE_MAX_SEQ(lruvec);\n \n-\tdo {\n-\t\tDEFINE_MAX_SEQ(lruvec);\n+\terr = -EBUSY;\n \n-\t\terr = -EBUSY;\n+\t/* another thread might have called inc_max_seq() */\n+\tif (walk->seq != max_seq)\n+\t\treturn err;\n \n-\t\t/* another thread might have called inc_max_seq() */\n-\t\tif (walk->seq != max_seq)\n-\t\t\tbreak;\n+\t/* the caller might be holding the lock for write */\n+\tif (mmap_read_trylock(mm)) {\n+\t\terr = walk_page_range(mm, walk->next_addr, ULONG_MAX,\n+\t\t\t\t      &mm_walk_ops, walk);\n \n-\t\t/* the caller might be holding the lock for write */\n-\t\tif (mmap_read_trylock(mm)) {\n-\t\t\terr = walk_page_range(mm, walk->next_addr, ULONG_MAX, &mm_walk_ops, walk);\n+\t\tmmap_read_unlock(mm);\n+\t}\n \n-\t\t\tmmap_read_unlock(mm);\n-\t\t}\n+\tif (walk->batched) {\n+\t\tspin_lock_irq(&lruvec->lru_lock);\n+\t\treset_batch_size(walk);\n+\t\tspin_unlock_irq(&lruvec->lru_lock);\n+\t}\n \n-\t\tif (walk->batched) {\n-\t\t\tspin_lock_irq(&lruvec->lru_lock);\n-\t\t\treset_batch_size(walk);\n-\t\t\tspin_unlock_irq(&lruvec->lru_lock);\n-\t\t}\n+\treturn err;\n+}\n+\n+static void walk_mm(struct mm_struct *mm, struct lru_gen_mm_walk *walk)\n+{\n+\tint err;\n \n+\twalk->next_addr = FIRST_USER_ADDRESS;\n+\tdo {\n+\t\terr = try_walk_mm(mm, walk);\n \t\tcond_resched();\n \t} while (err == -EAGAIN);\n }\n@@ -4011,6 +4075,33 @@ static bool inc_max_seq(struct lruvec *lruvec, unsigned long seq, int swappiness\n \treturn success;\n }\n \n+void lru_gen_scan_lruvec(struct lruvec *lruvec, unsigned long seq,\n+\t\t\t bool (*accessed_cb)(unsigned long), void (*flush_cb)(void))\n+{\n+\tstruct lru_gen_mm_walk *walk = current->reclaim_state->mm_walk;\n+\tstruct mm_struct *mm = NULL;\n+\n+\twalk->lruvec = lruvec;\n+\twalk->seq = seq;\n+\twalk->accessed_cb = accessed_cb;\n+\twalk->swappiness = MAX_SWAPPINESS;\n+\n+\tdo {\n+\t\tint err = -EBUSY;\n+\n+\t\titerate_mm_list(walk, &mm);\n+\t\tif (!mm)\n+\t\t\tbreak;\n+\n+\t\twalk->next_addr = FIRST_USER_ADDRESS;\n+\t\tdo {\n+\t\t\terr = try_walk_mm(mm, walk);\n+\t\t\tcond_resched();\n+\t\t\tflush_cb();\n+\t\t} while (err == -EAGAIN);\n+\t} while (mm);\n+}\n+\n static bool try_to_inc_max_seq(struct lruvec *lruvec, unsigned long seq,\n \t\t\t       int swappiness, bool force_scan)\n {\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "20260129144043.231636-9-bharata@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author is addressing a concern about the lack of integration with existing memory management infrastructure, specifically the MGLRU page table walk. They introduced a new kernel daemon, klruscand, that periodically invokes the MGLRU page table walk to gather access information and forward it to the pghot subsystem for promotion decisions. The author acknowledges this addresses the concern but does not explicitly state if further revision is needed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged_concern",
                "provided_alternative_solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Kinsey Ho <kinseyho@google.com>\n\nIntroduce a new kernel daemon, klruscand, that periodically invokes the\nMGLRU page table walk. It leverages the new callbacks to gather access\ninformation and forwards it to pghot sub-system for promotion decisions.\n\nThis benefits from reusing the existing MGLRU page table walk\ninfrastructure, which is optimized with features such as hierarchical\nscanning and bloom filters to reduce CPU overhead.\n\nAs an additional optimization to be added in the future, we can tune\nthe scan intervals for each memcg.\n\nSigned-off-by: Kinsey Ho <kinseyho@google.com>\nSigned-off-by: Yuanchu Xie <yuanchu@google.com>\n[Reduced the scan interval to 500ms, KLRUSCAND to default n in config]\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n mm/Kconfig     |   8 ++++\n mm/Makefile    |   1 +\n mm/klruscand.c | 110 +++++++++++++++++++++++++++++++++++++++++++++++++\n 3 files changed, 119 insertions(+)\n create mode 100644 mm/klruscand.c\n\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex 07b16aece877..9e9eca8db8bf 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -1502,6 +1502,14 @@ config HWMEM_PROFILER\n \t  rolled up to PGHOT sub-system for further action like hot page\n \t  promotion or NUMA Balancing\n \n+config KLRUSCAND\n+\tbool \"Kernel lower tier access scan daemon\"\n+\tdefault n\n+\tdepends on PGHOT && LRU_GEN_WALKS_MMU\n+\thelp\n+\t  Scan for accesses from lower tiers by invoking MGLRU to perform\n+\t  page table walks.\n+\n source \"mm/damon/Kconfig\"\n \n endmenu\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 89f999647752..c68df497a063 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -153,3 +153,4 @@ obj-$(CONFIG_PGHOT) += pghot-precise.o\n else\n obj-$(CONFIG_PGHOT) += pghot-default.o\n endif\n+obj-$(CONFIG_KLRUSCAND) += klruscand.o\ndiff --git a/mm/klruscand.c b/mm/klruscand.c\nnew file mode 100644\nindex 000000000000..13a41b38d67d\n--- /dev/null\n+++ b/mm/klruscand.c\n@@ -0,0 +1,110 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+#include <linux/memcontrol.h>\n+#include <linux/kthread.h>\n+#include <linux/module.h>\n+#include <linux/vmalloc.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/pghot.h>\n+\n+#include \"internal.h\"\n+\n+#define KLRUSCAND_INTERVAL 500\n+#define BATCH_SIZE (2 << 16)\n+\n+static struct task_struct *scan_thread;\n+static unsigned long pfn_batch[BATCH_SIZE];\n+static int batch_index;\n+\n+static void flush_cb(void)\n+{\n+\tint i;\n+\n+\tfor (i = 0; i < batch_index; i++) {\n+\t\tunsigned long pfn = pfn_batch[i];\n+\n+\t\tpghot_record_access(pfn, NUMA_NO_NODE, PGHOT_PGTABLE_SCAN, jiffies);\n+\n+\t\tif (i % 16 == 0)\n+\t\t\tcond_resched();\n+\t}\n+\tbatch_index = 0;\n+}\n+\n+static bool accessed_cb(unsigned long pfn)\n+{\n+\tWARN_ON_ONCE(batch_index == BATCH_SIZE);\n+\n+\tif (batch_index < BATCH_SIZE)\n+\t\tpfn_batch[batch_index++] = pfn;\n+\n+\treturn batch_index == BATCH_SIZE;\n+}\n+\n+static int klruscand_run(void *unused)\n+{\n+\tstruct lru_gen_mm_walk *walk;\n+\n+\twalk = kzalloc(sizeof(*walk),\n+\t\t       __GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN);\n+\tif (!walk)\n+\t\treturn -ENOMEM;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tunsigned long next_wake_time;\n+\t\tlong sleep_time;\n+\t\tstruct mem_cgroup *memcg;\n+\t\tint flags;\n+\t\tint nid;\n+\n+\t\tnext_wake_time = jiffies + msecs_to_jiffies(KLRUSCAND_INTERVAL);\n+\n+\t\tfor_each_node_state(nid, N_MEMORY) {\n+\t\t\tpg_data_t *pgdat = NODE_DATA(nid);\n+\t\t\tstruct reclaim_state rs = { 0 };\n+\n+\t\t\tif (node_is_toptier(nid))\n+\t\t\t\tcontinue;\n+\n+\t\t\trs.mm_walk = walk;\n+\t\t\tset_task_reclaim_state(current, &rs);\n+\t\t\tflags = memalloc_noreclaim_save();\n+\n+\t\t\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);\n+\t\t\tdo {\n+\t\t\t\tstruct lruvec *lruvec =\n+\t\t\t\t\tmem_cgroup_lruvec(memcg, pgdat);\n+\t\t\t\tunsigned long max_seq =\n+\t\t\t\t\tREAD_ONCE((lruvec)->lrugen.max_seq);\n+\n+\t\t\t\tlru_gen_scan_lruvec(lruvec, max_seq, accessed_cb, flush_cb);\n+\t\t\t\tcond_resched();\n+\t\t\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)));\n+\n+\t\t\tmemalloc_noreclaim_restore(flags);\n+\t\t\tset_task_reclaim_state(current, NULL);\n+\t\t\tmemset(walk, 0, sizeof(*walk));\n+\t\t}\n+\n+\t\tsleep_time = next_wake_time - jiffies;\n+\t\tif (sleep_time > 0 && sleep_time != MAX_SCHEDULE_TIMEOUT)\n+\t\t\tschedule_timeout_idle(sleep_time);\n+\t}\n+\tkfree(walk);\n+\treturn 0;\n+}\n+\n+static int __init klruscand_init(void)\n+{\n+\tstruct task_struct *task;\n+\n+\ttask = kthread_run(klruscand_run, NULL, \"klruscand\");\n+\n+\tif (IS_ERR(task)) {\n+\t\tpr_err(\"Failed to create klruscand kthread\\n\");\n+\t\treturn PTR_ERR(task);\n+\t}\n+\n+\tscan_thread = task;\n+\treturn 0;\n+}\n+module_init(klruscand_init);\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "20260129144043.231636-10-bharata@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author acknowledged that unmapped page cache pages in lower tiers don't get promoted easily and explained how the patch addresses this issue by using folio_mark_accessed() as a source of hotness, but noted that there is still room for improvement (TODO: better naming and overhead evaluation).",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "room for improvement"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Unmapped page cache pages that end up in lower tiers don't get\npromoted easily. There were attempts to identify such pages and\nget them promoted as part of NUMA Balancing earlier [1]. The\nsame idea is taken forward here by using folio_mark_accessed()\nas a source of hotness.\n\nLower tier accesses from folio_mark_accessed() are reported to\npghot sub-system for hotness tracking and subsequent promotion.\n\nTODO: Need a better naming for this hotness source. Need to\nbetter understand/evaluate the overhead of hotness info\ncollection from this path.\n\n[1] https://lore.kernel.org/linux-mm/20250411221111.493193-1-gourry@gourry.net/\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n Documentation/admin-guide/mm/pghot.txt | 7 ++++++-\n include/linux/pghot.h                  | 5 +++++\n include/linux/vm_event_item.h          | 1 +\n mm/pghot-tunables.c                    | 7 +++++++\n mm/pghot.c                             | 6 ++++++\n mm/swap.c                              | 8 ++++++++\n mm/vmstat.c                            | 1 +\n 7 files changed, 34 insertions(+), 1 deletion(-)\n\ndiff --git a/Documentation/admin-guide/mm/pghot.txt b/Documentation/admin-guide/mm/pghot.txt\nindex b329e692ef89..c8eb61064247 100644\n--- a/Documentation/admin-guide/mm/pghot.txt\n+++ b/Documentation/admin-guide/mm/pghot.txt\n@@ -23,9 +23,10 @@ Path: /sys/kernel/debug/pghot/\n      - 0: Hardware hints (value 0x1)\n      - 1: Page table scan (value 0x2)\n      - 2: Hint faults (value 0x4)\n+     - 3: folio_mark_accessed (value 0x8)\n    - Default: 0 (disabled)\n    - Example:\n-     # echo 0x7 > /sys/kernel/debug/pghot/enabled_sources\n+     # echo 0xf > /sys/kernel/debug/pghot/enabled_sources\n      Enables all sources.\n \n 2. **target_nid**\n@@ -82,3 +83,7 @@ Path: /proc/vmstat\n 4. **pghot_recorded_hintfaults**\n    - Number of recorded accesses reported by NUMA Balancing based\n      hotness source.\n+\n+5. **pghot_recorded_fma**\n+   - Number of recorded accesses reported by folio_mark_accessed()\n+     hotness source.\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nindex 603791183102..8cf9dfb5365a 100644\n--- a/include/linux/pghot.h\n+++ b/include/linux/pghot.h\n@@ -19,6 +19,7 @@ enum pghot_src {\n \tPGHOT_HW_HINTS,\n \tPGHOT_PGTABLE_SCAN,\n \tPGHOT_HINT_FAULT,\n+\tPGHOT_FMA,\n };\n \n #ifdef CONFIG_PGHOT\n@@ -36,6 +37,7 @@ void pghot_debug_init(void);\n DECLARE_STATIC_KEY_FALSE(pghot_src_hwhints);\n DECLARE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n DECLARE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n+DECLARE_STATIC_KEY_FALSE(pghot_src_fma);\n \n /*\n  * Bit positions to enable individual sources in pghot/records_enabled\n@@ -45,6 +47,7 @@ enum pghot_src_enabled {\n \tPGHOT_HWHINTS_BIT = 0,\n \tPGHOT_PGTSCAN_BIT,\n \tPGHOT_HINTFAULT_BIT,\n+\tPGHOT_FMA_BIT,\n \tPGHOT_MAX_BIT\n };\n \n@@ -52,6 +55,8 @@ enum pghot_src_enabled {\n #define PGHOT_PGTSCAN_ENABLED\t\tBIT(PGHOT_PGTSCAN_BIT)\n #define PGHOT_HINTFAULT_ENABLED\t\tBIT(PGHOT_HINTFAULT_BIT)\n #define PGHOT_SRC_ENABLED_MASK\t\tGENMASK(PGHOT_MAX_BIT - 1, 0)\n+#define PGHOT_FMA_ENABLED\t\tBIT(PGHOT_FMA_BIT)\n+#define PGHOT_SRC_ENABLED_MASK\t\tGENMASK(PGHOT_MAX_BIT - 1, 0)\n \n #define PGHOT_DEFAULT_FREQ_THRESHOLD\t2\n \ndiff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\nindex 67efbca9051c..ac1f28646b9c 100644\n--- a/include/linux/vm_event_item.h\n+++ b/include/linux/vm_event_item.h\n@@ -193,6 +193,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n \t\tPGHOT_RECORD_HWHINTS,\n \t\tPGHOT_RECORD_PGTSCANS,\n \t\tPGHOT_RECORD_HINTFAULTS,\n+\t\tPGHOT_RECORD_FMA,\n #ifdef CONFIG_HWMEM_PROFILER\n \t\tHWHINT_NR_EVENTS,\n \t\tHWHINT_KERNEL,\ndiff --git a/mm/pghot-tunables.c b/mm/pghot-tunables.c\nindex 79afbcb1e4f0..11c7f742a1be 100644\n--- a/mm/pghot-tunables.c\n+++ b/mm/pghot-tunables.c\n@@ -124,6 +124,13 @@ static void pghot_src_enabled_update(unsigned int enabled)\n \t\telse\n \t\t\tstatic_branch_disable(&pghot_src_hintfaults);\n \t}\n+\n+\tif (changed & PGHOT_FMA_ENABLED) {\n+\t\tif (enabled & PGHOT_FMA_ENABLED)\n+\t\t\tstatic_branch_enable(&pghot_src_fma);\n+\t\telse\n+\t\t\tstatic_branch_disable(&pghot_src_fma);\n+\t}\n }\n \n static ssize_t pghot_src_enabled_write(struct file *filp, const char __user *ubuf,\ndiff --git a/mm/pghot.c b/mm/pghot.c\nindex 6fc76c1eaff8..537f4af816ff 100644\n--- a/mm/pghot.c\n+++ b/mm/pghot.c\n@@ -43,6 +43,7 @@ static unsigned int sysctl_pghot_promote_rate_limit = 65536;\n DEFINE_STATIC_KEY_FALSE(pghot_src_hwhints);\n DEFINE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n DEFINE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n+DEFINE_STATIC_KEY_FALSE(pghot_src_fma);\n \n #ifdef CONFIG_SYSCTL\n static const struct ctl_table pghot_sysctls[] = {\n@@ -113,6 +114,11 @@ int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now)\n \t\t\treturn -EINVAL;\n \t\tcount_vm_event(PGHOT_RECORD_HINTFAULTS);\n \t\tbreak;\n+\tcase PGHOT_FMA:\n+\t\tif (!static_branch_likely(&pghot_src_fma))\n+\t\t\treturn -EINVAL;\n+\t\tcount_vm_event(PGHOT_RECORD_FMA);\n+\t\tbreak;\n \tdefault:\n \t\treturn -EINVAL;\n \t}\ndiff --git a/mm/swap.c b/mm/swap.c\nindex 2260dcd2775e..31a654b19844 100644\n--- a/mm/swap.c\n+++ b/mm/swap.c\n@@ -37,6 +37,8 @@\n #include <linux/page_idle.h>\n #include <linux/local_lock.h>\n #include <linux/buffer_head.h>\n+#include <linux/pghot.h>\n+#include <linux/memory-tiers.h>\n \n #include \"internal.h\"\n \n@@ -454,8 +456,14 @@ static bool lru_gen_clear_refs(struct folio *folio)\n  */\n void folio_mark_accessed(struct folio *folio)\n {\n+\tunsigned long pfn = folio_pfn(folio);\n+\n \tif (folio_test_dropbehind(folio))\n \t\treturn;\n+\n+\tif (!node_is_toptier(pfn_to_nid(pfn)))\n+\t\tpghot_record_access(pfn, NUMA_NO_NODE, PGHOT_FMA, jiffies);\n+\n \tif (lru_gen_enabled()) {\n \t\tlru_gen_inc_refs(folio);\n \t\treturn;\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 62c47f44edf0..c4d90baf440b 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -1506,6 +1506,7 @@ const char * const vmstat_text[] = {\n \t[I(PGHOT_RECORD_HWHINTS)]\t\t= \"pghot_recorded_hwhints\",\n \t[I(PGHOT_RECORD_PGTSCANS)]\t\t= \"pghot_recorded_pgtscans\",\n \t[I(PGHOT_RECORD_HINTFAULTS)]\t\t= \"pghot_recorded_hintfaults\",\n+\t[I(PGHOT_RECORD_FMA)]\t\t\t= \"pghot_recorded_fma\",\n #ifdef CONFIG_HWMEM_PROFILER\n \t[I(HWHINT_NR_EVENTS)]\t\t\t= \"hwhint_nr_events\",\n \t[I(HWHINT_KERNEL)]\t\t\t= \"hwhint_kernel\",\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "20260129144043.231636-11-bharata@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed a concern about the performance of the pghot subsystem in certain scenarios, specifically the NUMAB2 benchmark and the hwhints source. The author provided microbenchmark results showing that the patched case performs similarly to the base case for these scenarios, but noted that the pgtscan source needs tuning.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "acknowledgment of need for further work"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Here is the first set of results from a microbenchmark:\n\nTest system details\n-------------------\n3 node AMD Zen5 system with 2 regular NUMA nodes (0, 1) and a CXL node (2)\n\n$ numactl -H\navailable: 3 nodes (0-2)\nnode 0 cpus: 0-95,192-287\nnode 0 size: 128460 MB\nnode 1 cpus: 96-191,288-383\nnode 1 size: 128893 MB\nnode 2 cpus:\nnode 2 size: 257993 MB\nnode distances:\nnode   0   1   2\n  0:  10  32  50\n  1:  32  10  60\n  2:  255  255  10\n\nHotness sources\n---------------\nNUMAB0 - Without NUMA Balancing in base case and with no source enabled\n         in the patched case. No migrations occur.\nNUMAB2 - Existing hot page promotion for the base case and\n         use of hint faults as source in the patched case.\npgtscan - Klruscand (MGLRU based PTE A bit scanning) source\nhwhints - IBS as source\n\nPghot by default promotes after two accesses but for NUMAB2 source,\npromotion is done after one access to match the base behaviour.\n(/sys/kernel/debug/pghot/freq_threshold=1)\n\n==============================================================\nScenario 1 - Enough memory in toptier and hence only promotion\n==============================================================\nMulti-threaded application with 64 threads that access memory at 4K granularity\nrepetitively and randomly. The number of accesses per thread and the randomness\npattern for each thread are fixed beforehand. The accesses are divided into\nstores and loads in the ratio of 50:50.\n\nBenchmark threads run on Node 0, while memory is initially provisioned on\nCXL node 2 before the accesses start.\n\nRepetitive accesses results in lowertier pages becoming hot and kmigrated\ndetecting and migrating them. The benchmark score is the time taken to finish\nthe accesses in microseconds. The sooner it finishes the better it is. All the\nnumbers shown below are average of 3 runs.\n\nDefault mode - Time taken (microseconds, lower is better)\n---------------------------------------------------------\nSource          Base            Pghot\n---------------------------------------------------------\nNUMAB0          117,069,417     115,802,776\nNUMAB2          102,918,471     103,378,828\npgtscan         NA              110,203,286\nhwhints         NA              92,880,388\n---------------------------------------------------------\n\nDefault mode - Pages migrated (pgpromote_success)\n---------------------------------------------------------\nSource          Base            Pghot\n---------------------------------------------------------\nNUMAB0          0               0\nNUMAB2          2097147         2097131\npgtscan         NA              2097130\nhwhints         NA              1706556\n---------------------------------------------------------\n\nPrecision mode - Time taken (microseconds, lower is better)\n-----------------------------------------------------------\nSource          Base            Pghot\n-----------------------------------------------------------\nNUMAB0          117,069,417     115,078,527\nNUMAB2          102,918,471     101,742,985\npgtscan         NA              110,024,513     NA\nhwhints         NA              101,163,603     NA\n-----------------------------------------------------------\n\nPrecision mode - Pages migrated (pgpromote_success)\n---------------------------------------------------\nSource          Base            Pghot\n---------------------------------------------------\nNUMAB0          0               0\nNUMAB2          2097147         2097144\npgtscan         NA              2097129\nhwhints         NA              1144304\n---------------------------------------------------\n\n- The NUMAB2 benchmark numbers and pgpromote_success numbers more\n  or less match in base and patched case.\n- Though the pgtscan case promotes all possible pages, the\n  benchmark number suffers. This source needs tuning.\n- Hwhints case is able to provide benchmark numbers similar to\n  base NUMAB2 even with less number of migrations.\n- With both default and precision modes of pghot the benchmark\n  behaves more or less similarly.\n\n==============================================================\nScenario 2 - Toptier memory overcommited, promotion + demotion\n==============================================================\nSingle threaded application that allocates memory on both DRAM and CXL nodes\nusing mmap(MAP_POPULATE). Every 1G region of allocated memory on CXL node is\naccessed at 4K granularity randomly and repetitively to build up the notion\nof hotness in the 1GB region that is under access. This should drive promotion.\nFor promotion to work successfully, the DRAM memory that has been provisioned\n(and not being accessed) should be demoted first. There is enough free memory\nin the CXL node to for demotions.\n\nIn summary, this benchmark creates a memory pressure on DRAM node and does\nCXL memory accesses to drive both demotion and promotion.\n\nThe number of accesses are fixed and hence, the quicker the accessed pages\nget promoted to DRAM, the sooner the benchmark is expected to finish.\nAll the numbers shown below are average of 3 runs.\n\nDRAM-node                       = 1\nCXL-node                        = 2\nInitial DRAM alloc ratio        = 75%\nAllocation-size                 = 171798691840\nInitial DRAM Alloc-size         = 128849018880\nInitial CXL Alloc-size          = 42949672960\nHot-region-size                 = 1073741824\nNr-regions                      = 160\nNr-regions DRAM                 = 120 (provisioned but not accessed)\nNr-hot-regions CXL              = 40\nAccess pattern                  = random\nAccess granularity              = 4096\nDelay b/n accesses              = 0\nLoad/store ratio                = 50l50s\nTHP used                        = no\nNr accesses                     = 42949672960\nNr repetitions                  = 1024\n\nDefault mode - Time taken (microseconds, lower is better)\n------------------------------------------------------\nSource          Base            Pghot\n------------------------------------------------------\nNUMAB0          63,809,267      60,794,786\nNUMAB2          67,541,601      62,376,991\npgtscan         NA              67,902,126\nhwhints         NA              59,872,525\n------------------------------------------------------\n\nDefault mode - Pages migrated (pgpromote_success)\n-------------------------------------------------\nSource          Base            Pghot\n-------------------------------------------------\nNUMAB0          0               0\nNUMAB2          179635          932693  (High R2R variation in base)\npgtscan         NA              27487\nhwhints         NA              274\n---------------------------------------\n\nPrecision mode - Time taken (microseconds, lower is better)\n------------------------------------------------------\nSource          Base            Pghot\n------------------------------------------------------\nNUMAB0          63,809,267      64,553,914\nNUMAB2          67,541,601      62,148,082\npgtscan         NA              65,073,396\nhwhints         NA              59,958,655\n------------------------------------------------------\n\nPrecision mode - Pages migrated (pgpromote_success)\n---------------------------------------------------\nSource          Base            Pghot\n---------------------------------------------------\nNUMAB0          0               0\nNUMAB2          179635          988360  (High R2R variaion in base)\npgtscan         NA              21418   (High R2R variation in patched)\nhwhints         NA              174     (High R2R variation in patched)\n---------------------------------------------------\n\n- The base case itself doesn't show any improvement in benchmark numbers due\n  to hot page promotion. The same pattern is seen in pghot case with all\n  the sources except hwhints. The benchmark itself may need tuning so that\n  promotion helps.\n- There is a high run to run variation in the number of pages promoted in\n  base case.\n- Most promotion attempts in base case fail because the NUMA hint fault\n  latency is found to exceed the threshold value (default threshold\n  is 1000ms) in majority of the promotion attempts.\n- Unlike base NUMAB2 where the hint fault latency is the difference between the\n  PTE update time (during scanning) and the access time (hint fault), pghot uses\n  a single latency threshold (4000ms in pghot-default and 5000ms in\n  pghot-precise) for two purposes.\n        1. If the time difference between successive accesses are within the\n           threshold, the page is marked as hot.\n        2. Later when kmigrated picks up the page for migration, it will migrate\n           only if the difference between the current time and the time when the\n          page was marked hot is with the threshold.\n  Because of the above difference in behaviour, more number of pages get\n  qualified for promotion compared to base NUMAB2.",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "c5f22c8a-ad7d-4a9f-bcd5-15cbee2e8f19@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed a concern about the performance benefits of their patch, specifically in scenarios where toptier memory is overcommitted. They presented benchmark results from redis-memtier and explained that hot page promotion still shows benefits even when there's no clear benefit seen due to overcommitment.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Numbers from redis-memtier benchmark:\n\nTest system details\n-------------------\n3 node AMD Zen5 system with 2 regular NUMA nodes (0, 1) and a CXL node (2)\n\n$ numactl -H\navailable: 3 nodes (0-2)\nnode 0 cpus: 0-95,192-287\nnode 0 size: 128460 MB\nnode 1 cpus: 96-191,288-383\nnode 1 size: 128893 MB\nnode 2 cpus:\nnode 2 size: 257993 MB\nnode distances:\nnode   0   1   2\n  0:  10  32  50\n  1:  32  10  60\n  2:  255  255  10\n\nHotness sources\n---------------\nNUMAB0 - Without NUMA Balancing in base case and with no source enabled\n         in the patched case. No migrations occur.\nNUMAB2 - Existing hot page promotion for the base case and\n         use of hint faults as source in the patched case.\n\nPghot by default promotes after two accesses but for NUMAB2 source,\npromotion is done after one access to match the base behaviour.\n(/sys/kernel/debug/pghot/freq_threshold=1)\n\n==============================================================\nScenario 1 - Enough memory in toptier and hence only promotion\n==============================================================\nIn the setup phase, 64GB database is provisioned and explicitly moved\nto Node 2 by migrating redis-server's memory to Node 2.\nMemtier is run on Node 1.\n\nParallel distribution, 50% of the keys accessed, each 4 times.\n16        Threads\n100       Connections per thread\n77808     Requests per client\n\n==================================================================================================\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9\nLatency       KB/sec\n--------------------------------------------------------------------------------------------------\nBase, NUMAB0\nTotals     225827.75       226.49746       225.27900       425.98300\n454.65500    513106.09\n--------------------------------------------------------------------------------------------------\nBase, NUMAB2\nTotals     254869.29       205.61759       216.06300       399.35900\n454.65500    579091.74\n--------------------------------------------------------------------------------------------------\npghot-default, NUMAB2\nTotals     264229.35       202.81411       215.03900       393.21500\n446.46300    600358.86\n--------------------------------------------------------------------------------------------------\npghot-precise, NUMAB2\nTotals     261136.17       203.32692       215.03900       391.16700\n446.46300    593330.81\n==================================================================================================\n\npgpromote_success\n==================================\nBase, NUMAB0            0\nBase, NUMAB2            10,435,178\npghot-default, NUMAB2   10,435,031\npghot-precise, NUMAB2   10,435,245\n==================================\n\n- There is a clear benefit of hot page promotion seen. Both\n  base and pghot show similar benefits.\n- The number of pages promoted in both cases are more or less\n  same.\n\n==============================================================\nScenario 2 - Toptier memory overcommited, promotion + demotion\n==============================================================\nIn the setup phase, 192GB database is provisioned. The database occupies\nNode 1 entirely(~128GB) and spills over to Node 2 (~64GB).\nMemtier is run on Node 1.\n\nParallel distribution, 50% of the keys accessed, each 4 times.\n16        Threads\n100       Connections per thread\n233424    Requests per client\n\n==================================================================================================\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9\nLatency       KB/sec\n--------------------------------------------------------------------------------------------------\nBase, NUMAB0\nTotals     246474.55       211.90623       192.51100       370.68700\n448.51100    560235.63\n--------------------------------------------------------------------------------------------------\nBase, NUMAB2\nTotals     232790.88       221.18604       214.01500       419.83900\n509.95100    529132.72\n--------------------------------------------------------------------------------------------------\npghot-default, NUMAB2\nTotals     241615.60       216.12761       210.94300       391.16700\n475.13500    549191.27\n--------------------------------------------------------------------------------------------------\npghot-precise, NUMAB2\nTotals     238557.37       217.57630       207.87100       395.26300\n471.03900    542239.92\n==================================================================================================\n                        pgpromote_success       pgdemote_kswapd\n===============================================================\nBase, NUMAB0            0                       832,494\nBase, NUMAB2            352,075                 720,409\npghot-default, NUMAB2   25,865,321              26,154,984\npghot-precise, NUMAB2   25,525,429              25,838,095\n===============================================================\n\n- No clear benefit is seen with hot page promotion both in base and pghot case.\n- Most promotion attempts in base case fail because the NUMA hint fault latency\n  is found to exceed the threshold value (default threshold of 1000ms) in\n  majority of the promotion attempts.\n- Unlike base NUMAB2 where the hint fault latency is the difference between the\n  PTE update time (during scanning) and the access time (hint fault), pghot uses\n  a single latency threshold (4000ms in pghot-default and 5000ms in\n  pghot-precise) for two purposes.\n        1. If the time difference between successive accesses are within the\n           threshold, the page is marked as hot.\n        2. Later when kmigrated picks up the page for migration, it will migrate\n           only if the difference between the current time and the time when the\n          page was marked hot is with the threshold.\n  Because of the above difference in behaviour, more number of pages get\n  qualified for promotion compared to base NUMAB2.",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "4df58408-58d7-41ad-afa7-c42a64689ec8@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author is addressing a concern about the performance of pghot-default mode, specifically why it doesn't show benefits despite achieving similar page promotion numbers as NUMAB2. The author explains that this is because pghot-default promotes to NID=0 by default, which may not be beneficial since processes are running on both Node 0 and Node 1.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Here are Graph500 numbers for the hint fault source:\n\nTest system details\n-------------------\n3 node AMD Zen5 system with 2 regular NUMA nodes (0, 1) and a CXL node (2)\n\n$ numactl -H\navailable: 3 nodes (0-2)\nnode 0 cpus: 0-95,192-287\nnode 0 size: 128460 MB\nnode 1 cpus: 96-191,288-383\nnode 1 size: 128893 MB\nnode 2 cpus:\nnode 2 size: 257993 MB\nnode distances:\nnode   0   1   2\n  0:  10  32  50\n  1:  32  10  60\n  2:  255  255  10\n\nHotness sources\n---------------\nNUMAB0 - Without NUMA Balancing in base case and with no source enabled\n         in the pghot case. No migrations occur.\nNUMAB2 - Existing hot page promotion for the base case and\n         use of hint faults as source in the pghot case.\n\nPghot by default promotes after two accesses but for NUMAB2 source,\npromotion is done after one access to match the base behaviour.\n(/sys/kernel/debug/pghot/freq_threshold=1)\n\nGraph500 details\n----------------\nCommand: mpirun -n 128 --bind-to core --map-by core\ngraph500/src/graph500_reference_bfs 28 16\n\nAfter the graph creation, the processes are stopped and data is migrated\nto CXL node 2 before continuing so that BFS phase starts accessing lower\ntier memory.\n\nTotal memory usage is slightly over 100GB and will fit within Node 0 and 1.\nHence there is no memory pressure to induce demotions.\n\n=====================================================================================\n                        Base            Base            pghot-default\npghot-precise\n                        NUMAB0          NUMAB2          NUMAB2          NUMAB2\n=====================================================================================\nharmonic_mean_TEPS      5.10676e+08     7.56804e+08     5.92473e+08     7.47091e+08\nmean_time               8.41027         5.67508         7.24915         5.74886\nmedian_TEPS             5.11535e+08     7.24252e+08     5.63155e+08     7.71638e+08\nmax_TEPS                5.1785e+08      1.06051e+09     7.88018e+08     1.0504e+09\n\npgpromote_success       0               13557718        13737730        13734469\nnuma_pte_updates        0               26491591        26848847        26726856\nnuma_hint_faults        0               13558077        13882743        13798024\n=====================================================================================\n\n\n- The base case shows a good improvement with NUMAB2(48%) in harmonic_mean_TEPS.\n- The same improvement gets maintained with pghot-precise too (46%).\n- pghot-default mode doesn't show benefit even when achieving similar page promotion\n  numbers. This mode doesn't track accessing NID and by default promotes to NID=0\n  which probably isn't all that beneficial as processes are running on both Node 0\n  and Node 1.",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "911f316b-87cf-45eb-8d9e-412473d7176a@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "Author acknowledged a bug in the folio_isolate_lru() function, which occurs when running the Graph500 benchmark due to not holding a folio reference before calling it. They fixed this issue on their GitHub repository and plan to include the fix in future versions of the patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a bug",
                "planned fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "We should hold a folio reference before the above call which will isolate the\nfolio from LRU. Otherwise we may hit\n\nVM_BUG_ON_FOLIO(!folio_ref_count(folio), folio)\n\nin folio_isolate_lru().\n\nI hit this only when running Graph500 benchmark and have fixed it in\nthe github at: https://github.com/AMDESE/linux-mm/tree/bharata/pghot-rfcv6-pre\n\nThe numbers that I have posted for micro-benchmarks and redis-memtier are\nwithout this fix while Graph500 numbers are with this fix.\n\nRegards,\nBharata.",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "7c6d427a-9fe4-4af0-93c8-18ecb2296e36@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price requested clarification on the meaning of TEPS, a benchmark used in the patchset, and asked whether higher values are better or worse.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "lack of technical expertise",
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Can you contextualize TEPS?  Higher better? Higher worse? etc.\nUnfamiliar with this benchmark.\n\n~Gregory",
              "reply_to": "Bharata Rao",
              "message_date": "2026-02-11",
              "message_id": "aYyomjsBpZ2KFxKG@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer suggested using a random or round-robin selection of an upper-tier node to improve the accuracy of page promotion, as the current implementation lacks access-nid data.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Lacking access-nid data, maybe it's better to select a random (or\nround-robin) node in the upper tier?  That would at least approach 1/N\naccuracy in promotion for most access patterns.\n\n~Gregory",
              "reply_to": "Bharata Rao",
              "message_date": "2026-02-11",
              "message_id": "aYypIYOktgaVLqDM@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer noted that tracking hotness records for zone-device folios may not be necessary and suggested a fast-out to avoid unnecessary tracking, which could also have implications for private-node memory",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Also relevant note from other work I'm doing, we may want a fast-out for\nzone-device folios here.  We should not bother tracking those at all.\n\n(this may also become relevant for private-node memory as well, but I\nmay try to generalize zone_device & private-node checks as the\nconditions are very similar).\n\n~Gregory",
              "reply_to": "Bharata Rao",
              "message_date": "2026-02-11",
              "message_id": "aYypm59N7SlS3Gme@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author acknowledged that zone device folios are not tracked by the pghot subsystem, and they get discarded by the pghot_record_access() function.\n\nAuthor acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, but did not provide further details.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledgment",
                "clarification",
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, zone device folios aren't not tracked by pghot. They get discarded\nby pghot_record_access() itself.\n\n---\n\nGood.\n\nRegards,\nBharata.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-12",
              "message_id": "69ff289a-1574-4d2e-a987-f47f1859aeb1@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author is addressing a concern about the performance metric used in the Graph500 benchmark, confirming that higher TEPS (Traversed Edges Per Second) values are better.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged feedback",
                "confirmed approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "In the Graph500 benchmark, higher TEPS (Traversed Edges Per Second) values are\nbetter.\n\nRegards,\nBharata.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-12",
              "message_id": "0b03e16d-ca4a-4a70-b530-14bbe42bb7ad@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author is addressing concerns about performance issues in the hot page tracking and promotion subsystem, specifically in scenarios where demotion is present. They are providing benchmark results that show similar performance between base and pghot cases, but with increased pte updates and hint faults in some runs. The author states they have yet to understand the exact reason for this behavior.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "yet to understand",
                "same behaviour"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "These numbers are from scenario where demotion is present:\n\n=============================================\nOver-committed scenario, promotion + demotion\n=============================================\nCommand: mpirun -n 128 --bind-to core --map-by core\n/home/bharata/benchmarks/graph500/src/graph500_reference_bfs 30 16\n\nThe scale factor of 30 results in around 400GB of memory being\nprovisioned resulting in the data spilling over to CXL node.\nNo explicit migration of data is done in this case unlike the\nprevious case.\n\n=====================================================================================\n                        Base            Base            pghot-default\npghot-precise\n                        NUMAB0          NUMAB2          NUMAB2          NUMAB2\n=====================================================================================\nharmonic_mean_TEPS      9.28713e+08     7.90431e+08     7.32193e+08     7.81051e+08\nmean_time               18.4984         21.7346         23.4634         21.9956\nmedian_TEPS             9.25707e+08     7.86684e+08     7.27053e+08     7.82823e+08\nmax_TEPS                9.57632e+08     8.4758e+08      8.22172e+08     7.9889e+08\n\npgpromote_success       0               22846743        22807167        25994988\npgpromote_candidate     0               24628924        29436044        27029173\npgpromote_candidate_nrl 0               140921          220             38387\npgdemote_kswapd         0               41523110        45121134        50042594\nnuma_pte_updates        0               121904763       71503891        68779424\nnuma_hint_faults        0               81708126        29583391        27176332\n=====================================================================================\n\n- In the base case, the benchmark suffers when promotion and demotion are\n  enabled (NUMAB2 case).\n- Same behaviour is seen with both modes of pghot.\n- Though the overall benchmark numbers remain more or less same with base and\n  pghot NUMAB2 cases, the number of pte updates and hint faults are seen\n  to spike up during some runs. Yet to understand the exact reason for this.",
              "reply_to": "",
              "message_date": "2026-02-12",
              "message_id": "aa3736ed-1a07-4d55-b9ef-734fae02daa7@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer requested addition of a base-commit field to facilitate automated testing and backporting, making the patch series more manageable",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "In the future can you add a \n\nbase-commit:\n\nfor the series?  Make's it easier to automate pulling it in for testing\nand backports etc.\n\n~Gregory",
              "reply_to": "Bharata Rao",
              "message_date": "2026-02-13",
              "message_id": "aY87i3dG5xmDpWkE@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author acknowledged a suggestion to apply the patch series on a specific commit and will do so, also providing a link to their latest GitHub branch.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged",
                "will do"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Good suggestion, will do thanks.\n\nBTW this series applies on f0b9d8eb98df.\nLatest github branch:\nhttps://github.com/AMDESE/linux-mm/tree/bharata/pghot-rfcv6-pre\n\nRegards,\nBharata.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-16",
              "message_id": "aeb717f7-60a0-4fe1-b34c-d4f8cea02f96@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author is addressing a concern about the performance of pghot-default in NAS Parallel Benchmark (NPB) tests. They provided benchmark numbers showing that pghot-precise matches the base case numbers, but pghot-default suffers due to promotion being limited to the default NID (0). The author notes that this leads to excessive PTE updates, hint faults, demotion, and promotion churn.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a performance issue",
                "promises no fix in this response"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Here are some numbers from NAS Parallel Benchmark (NPB) with BT application:\n\nTest system details\n-------------------\n3 node AMD Zen5 system with 2 regular NUMA nodes (0, 1) and a CXL node (2)\n\n$ numactl -H\navailable: 3 nodes (0-2)\nnode 0 cpus: 0-95,192-287\nnode 0 size: 128460 MB\nnode 1 cpus: 96-191,288-383\nnode 1 size: 128893 MB\nnode 2 cpus:\nnode 2 size: 257993 MB\nnode distances:\nnode   0   1   2\n  0:  10  32  50\n  1:  32  10  60\n  2:  255  255  10\n\nHotness sources\n---------------\nNUMAB0 - Without NUMA Balancing in base case and with no source enabled\n         in the pghot case. No migrations occur.\nNUMAB2 - Existing hot page promotion for the base case and\n         use of hint faults as source in the pghot case.\n         Both promotion and demotion are enabled in this case.\n\nPghot by default promotes after two accesses but for NUMAB2 source,\npromotion is done after one access to match the base behaviour.\n(/sys/kernel/debug/pghot/freq_threshold=1)\n\n\nNAS-BT details\n--------------\nCommand: mpirun -np 16 /usr/bin/numactl --cpunodebind=0,1\nNPB3.4.4/NPB3.4-MPI/bin/bt.F.x\n\nWhile class D uses around 24G of memory (which is too less to show the benefit\nof promition), class E results in around 368G of memory which overflows my\ntoptier. Hence I wanted something in between these classes. So I have  modified\nclass F to the problem size of 768 which results in around 160GB of memory.\n\nAfter the memory consumption stabilizes, all the rank PIDs are paused and\ntheir memory is moved to CXL node using migratepages command. This simulates\nthe situation of memory residing on lower tier node and access by BT processes\nleading to promotion.\n\nTime in seconds - Lower is better\nMop/s total - Higher is better\n=====================================================================================\n                        Base            Base            pghot-default\npghot-precise\n                        NUMAB0          NUMAB2          NUMAB2          NUMAB2\n=====================================================================================\nTime in seconds         7349.86         4422.50         6219.71         4113.56\nMop/s total             53247.66        88493.630       62923.030       95139.810\n\npgpromote_success       0               42181834        248503390       41955718\npgpromote_candidate     0               0               577086192       0\npgpromote_candidate_nrl 0               42181834        29410329        41956171\npgdemote_kswapd         0               0               216489010       0\nnuma_pte_updates        0               42252749        607470975       42037882\nnuma_hint_faults        0               42183772        606540729       41968150\n=====================================================================================\n\n- In the base case, the benchmark numbers improve significantly due to hot page\n  promotion.\n- Though the benchmark runs for hundreds of minutes, the pages get promoted\n  within the first few mins.\n- pghot-precise is able to match the base case numbers.\n- The benchmark suffers in pghot-default case due to promotion being limited\n  to the default NID (0) only. This leads to excessive PTE updates, hint faults,\n  demotion and promotion churn.",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "a8d1efd6-2ca4-4f1d-9c0a-c8aa17732ee9@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price suggested modifying pghot-default to randomly or round-robin select a node on the top tier instead of always using NID(0), which would improve its accuracy and allow for comparison with NUMAB2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Wow, this really seems to justify the extra memory usage.\n\nIs it possible for you to change pghot-default to move the page to a\nrandom (or round-robin) node on the top tier instead of NID(0) by default?\n\nAt least then pghot-default would be correct 1/N % of the time (in theory).\nI'd be curious to see how close it gets to NUMAB2 with that.\n\n~Gregory",
              "reply_to": "Bharata Rao",
              "message_date": "2026-02-23",
              "message_id": "aZxsBifRchLn2m42@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author is addressing a concern about the performance of pghot-default compared to pghot-precise and base NUMAB2 case, providing data that shows the numbers catch up after some time.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "For pghot-default, with target_nid alternating between the available\ntoptier nodes 0 and 1, the numbers catch up with pghot-precise and base\nNUMAB2 case as seen below:\n================================\nTime in seconds         4337.98\nMop/s total             90217.86\n\npgpromote_success       42170085\npgpromote_candidate     0\npgpromote_candidate_nrl 42171963\npgdemote_kswapd         0\nnuma_pte_updates        42338538\nnuma_hint_faults        42185662\n================================\n\nRegards,\nBharata.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "7a574665-0a65-46fb-b87d-d2ae28308759@amd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer expressed skepticism about the patch's performance, wondering if its success was due to chance rather than actual optimization.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "skepticism",
                "questioning"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Fascinating! Thank you for the quick follow up.\n\nI wonder if this was a lucky run, it almost seems *too* perfect.\n\n~Gregory",
              "reply_to": "Bharata Rao",
              "message_date": "2026-02-24",
              "message_id": "aZ3D_8GJit3FYhQc@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC][RFC PATCH v4 00/27] Private Memory Nodes (w/ Compressed RAM)",
          "message_id": "aZxqP7J1kOClQUPQ@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZxqP7J1kOClQUPQ@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T14:55:07Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY nodes are intended to contain general System RAM. Today, some\ndevice drivers hotplug their memory (marked Specific Purpose or Reserved)\nto get access to mm/ services, but don't intend it for general consumption.\n\nCreate N_MEMORY_PRIVATE for memory nodes whose memory is not intended for\ngeneral consumption. This state is mutually exclusive with N_MEMORY.\n\nAdd the node_private infrastructure for N_MEMORY_PRIVATE nodes:\n\n  - struct node_private: Per-node container stored in NODE_DATA(nid),\n    holding driver callbacks (ops), owner, and refcount.\n\n  - struct node_private_ops: Initial structure with void *reserved\n    placeholder and flags field.  Callbacks will be added by subsequent\n    commits as each consumer is wired up.\n\n  - folio_is_private_node() / page_is_private_node(): check if a\n    folio/page resides on a private node.\n\n  - folio_node_private_ops() / node_private_flags(): retrieve the ops\n    vtable or flags for a folio's node.\n\n  - Registration API: node_private_register()/unregister() for drivers\n    to register callbacks for private nodes. Only one driver callback\n    can be registered per node - attempting to register different ops\n    returns -EBUSY.\n\n  - sysfs attribute exposing N_MEMORY_PRIVATE node state.\n\nZonelist construction changes for private nodes are deferred to a\nsubsequent commit.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 197 ++++++++++++++++++++++++++++++++\n include/linux/mmzone.h       |   4 +\n include/linux/node_private.h | 210 +++++++++++++++++++++++++++++++++++\n include/linux/nodemask.h     |   1 +\n 4 files changed, 412 insertions(+)\n create mode 100644 include/linux/node_private.h\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 00cf4532f121..646dc48a23b5 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -22,6 +22,7 @@\n #include <linux/swap.h>\n #include <linux/slab.h>\n #include <linux/memblock.h>\n+#include <linux/node_private.h>\n \n static const struct bus_type node_subsys = {\n \t.name = \"node\",\n@@ -861,6 +862,198 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n \t\t\t   (void *)&nid, register_mem_block_under_node_hotplug);\n \treturn;\n }\n+\n+static DEFINE_MUTEX(node_private_lock);\n+static bool node_private_initialized;\n+\n+/**\n+ * node_private_register - Register a private node\n+ * @nid: Node identifier\n+ * @np: The node_private structure (driver-allocated, driver-owned)\n+ *\n+ * Register a driver for a private node. Only one driver can register\n+ * per node. If another driver has already registered (with different np),\n+ * -EBUSY is returned. Re-registration with the same np is allowed.\n+ *\n+ * The driver owns the node_private memory and must ensure it remains valid\n+ * until refcount reaches 0 after node_private_unregister().\n+ *\n+ * Returns 0 on success, negative errno on failure.\n+ */\n+int node_private_register(int nid, struct node_private *np)\n+{\n+\tstruct node_private *existing;\n+\tpg_data_t *pgdat;\n+\tint ret = 0;\n+\n+\tif (!np || !node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tif (!node_private_initialized)\n+\t\treturn -ENODEV;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\t/* N_MEMORY_PRIVATE and N_MEMORY are mutually exclusive */\n+\tif (node_state(nid, N_MEMORY)) {\n+\t\tret = -EBUSY;\n+\t\tgoto out;\n+\t}\n+\n+\tpgdat = NODE_DATA(nid);\n+\texisting = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t\t     lockdep_is_held(&node_private_lock));\n+\n+\t/* Only one source my register this node */\n+\tif (existing) {\n+\t\tif (existing != np) {\n+\t\t\tret = -EBUSY;\n+\t\t\tgoto out;\n+\t\t}\n+\t\tgoto out;\n+\t}\n+\n+\trefcount_set(&np->refcount, 1);\n+\tinit_completion(&np->released);\n+\n+\trcu_assign_pointer(pgdat->node_private, np);\n+\tpgdat->private = true;\n+\n+out:\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_register);\n+\n+/**\n+ * node_private_set_ops - Set service callbacks on a registered private node\n+ * @nid: Node identifier\n+ * @ops: Service callbacks and flags (driver-owned, must outlive registration)\n+ *\n+ * Validates flag dependencies and sets the ops on the node's node_private.\n+ * The node must already be registered via node_private_register().\n+ *\n+ * Returns 0 on success, -EINVAL for invalid flag combinations,\n+ * -ENODEV if no node_private is registered on @nid.\n+ */\n+int node_private_set_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!ops)\n+\t\treturn -EINVAL;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse\n+\t\tnp->ops = ops;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_set_ops);\n+\n+/**\n+ * node_private_clear_ops - Clear service callbacks from a private node\n+ * @nid: Node identifier\n+ * @ops: Expected ops pointer (must match current ops)\n+ *\n+ * Clears the ops only if @ops matches the currently registered ops,\n+ * preventing one service from accidentally clearing another's callbacks.\n+ *\n+ * Returns 0 on success, -ENODEV if no node_private is registered,\n+ * -EINVAL if @ops does not match.\n+ */\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse if (np->ops != ops)\n+\t\tret = -EINVAL;\n+\telse\n+\t\tnp->ops = NULL;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_clear_ops);\n+\n+/**\n+ * node_private_unregister - Unregister a private node\n+ * @nid: Node identifier\n+ *\n+ * Unregister the driver from a private node. Only succeeds if all memory\n+ * has been offlined and the node is no longer N_MEMORY_PRIVATE.\n+ * When successful, drops the refcount to 0 indicating the driver can\n+ * free its context.\n+ *\n+ * N_MEMORY_PRIVATE state is cleared by offline_pages() when the last\n+ * memory is offlined, not by this function.\n+ *\n+ * Return: 0 if unregistered, -EBUSY if N_MEMORY_PRIVATE is still set\n+ * (other memory blocks remain on this node).\n+ */\n+int node_private_unregister(int nid)\n+{\n+\tstruct node_private *np;\n+\tpg_data_t *pgdat;\n+\n+\tif (!node_possible(nid))\n+\t\treturn 0;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\tpgdat = NODE_DATA(nid);\n+\tnp = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Only unregister if all memory is offline and N_MEMORY_PRIVATE is\n+\t * cleared. N_MEMORY_PRIVATE is cleared by offline_pages() when the\n+\t * last memory block is offlined.\n+\t */\n+\tif (node_state(nid, N_MEMORY_PRIVATE)) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn -EBUSY;\n+\t}\n+\n+\trcu_assign_pointer(pgdat->node_private, NULL);\n+\tpgdat->private = false;\n+\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\n+\tsynchronize_rcu();\n+\n+\tif (!refcount_dec_and_test(&np->refcount))\n+\t\twait_for_completion(&np->released);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(node_private_unregister);\n+\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n /**\n@@ -959,6 +1152,7 @@ static struct node_attr node_state_attr[] = {\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n \t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\n \t\t\t\t\t   N_GENERIC_INITIATOR),\n@@ -972,6 +1166,7 @@ static struct attribute *node_state_attrs[] = {\n \t&node_state_attr[N_HIGH_MEMORY].attr.attr,\n #endif\n \t&node_state_attr[N_MEMORY].attr.attr,\n+\t&node_state_attr[N_MEMORY_PRIVATE].attr.attr,\n \t&node_state_attr[N_CPU].attr.attr,\n \t&node_state_attr[N_GENERIC_INITIATOR].attr.attr,\n \tNULL\n@@ -1007,5 +1202,7 @@ void __init node_dev_init(void)\n \t\t\tpanic(\"%s() failed to add node: %d\\n\", __func__, ret);\n \t}\n \n+\tnode_private_initialized = true;\n+\n \tregister_memory_blocks_under_nodes();\n }\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex b01cb1e49896..992eb1c5a2c6 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -25,6 +25,8 @@\n #include <linux/zswap.h>\n #include <asm/page.h>\n \n+struct node_private;\n+\n /* Free memory management - zoned buddy allocator.  */\n #ifndef CONFIG_ARCH_FORCE_MAX_ORDER\n #define MAX_PAGE_ORDER 10\n@@ -1514,6 +1516,8 @@ typedef struct pglist_data {\n \tatomic_long_t\t\tvm_stat[NR_VM_NODE_STAT_ITEMS];\n #ifdef CONFIG_NUMA\n \tstruct memory_tier __rcu *memtier;\n+\tstruct node_private __rcu *node_private;\n+\tbool private;\n #endif\n #ifdef CONFIG_MEMORY_FAILURE\n \tstruct memory_failure_stats mf_stats;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nnew file mode 100644\nindex 000000000000..6a70ec39d569\n--- /dev/null\n+++ b/include/linux/node_private.h\n@@ -0,0 +1,210 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_NODE_PRIVATE_H\n+#define _LINUX_NODE_PRIVATE_H\n+\n+#include <linux/completion.h>\n+#include <linux/mm.h>\n+#include <linux/nodemask.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+\n+struct page;\n+struct vm_area_struct;\n+struct vm_fault;\n+\n+/**\n+ * struct node_private_ops - Callbacks for private node services\n+ *\n+ * Services register these callbacks to intercept MM operations that affect\n+ * their private nodes.\n+ *\n+ * Flag bits control which MM subsystems may operate on folios on this node.\n+ *\n+ * The pgdat->node_private pointer is RCU-protected.  Callbacks fall into\n+ * three categories based on their calling context:\n+ *\n+ * Folio-referenced callbacks (RCU released before callback):\n+ *   The caller holds a reference to a folio on the private node, which\n+ *   pins the node's memory online and prevents node_private teardown.\n+ *\n+ * Refcounted callbacks (RCU released before callback):\n+ *   The caller has no folio on the private node (e.g., folios are on a\n+ *   source node being migrated TO this node).  A temporary refcount is\n+ *   taken on node_private under rcu_read_lock to keep the structure (and\n+ *   the service module) alive across the callback.  node_private_unregister\n+ *   waits for all temporary references to drain before returning.\n+ *\n+ * Non-folio callbacks (rcu_read_lock held during callback):\n+ *   No folio reference exists, so rcu_read_lock is held across the\n+ *   callback to prevent node_private from being freed.\n+ *   These callbacks MUST NOT sleep.\n+ *\n+ * @flags: Operation exclusion flags (NP_OPS_* constants).\n+ *\n+ */\n+struct node_private_ops {\n+\tunsigned long flags;\n+};\n+\n+/**\n+ * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n+ *\n+ * This structure is allocated by the driver and passed to node_private_register().\n+ * The driver owns the memory and must ensure it remains valid until after\n+ * node_private_unregister() returns with the reference count dropped to 0.\n+ *\n+ * @owner: Opaque driver identifier\n+ * @refcount: Reference count (1 = registered; temporary refs for non-folio\n+ *\t\tcallbacks that may sleep; 0 = fully released)\n+ * @released: Signaled when refcount drops to 0; unregister waits on this\n+ * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ */\n+struct node_private {\n+\tvoid *owner;\n+\trefcount_t refcount;\n+\tstruct completion released;\n+\tconst struct node_private_ops *ops;\n+};\n+\n+#ifdef CONFIG_NUMA\n+\n+#include <linux/mmzone.h>\n+\n+/**\n+ * folio_is_private_node - Check if folio is on an N_MEMORY_PRIVATE node\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio resides on a private node.\n+ */\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn node_state(folio_nid(folio), N_MEMORY_PRIVATE);\n+}\n+\n+/**\n+ * page_is_private_node - Check if page is on an N_MEMORY_PRIVATE node\n+ * @page: The page to check\n+ *\n+ * Returns true if the page resides on a private node.\n+ */\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\tconst struct node_private_ops *ops;\n+\tstruct node_private *np;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(folio_nid(folio))->node_private);\n+\tops = np ? np->ops : NULL;\n+\trcu_read_unlock();\n+\n+\treturn ops;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\tstruct node_private *np;\n+\tunsigned long flags;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tflags = (np && np->ops) ? np->ops->flags : 0;\n+\trcu_read_unlock();\n+\n+\treturn flags;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn node_private_flags(folio_nid(f)) & flag;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn node_private_flags(nid) & flag;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn node_private_flags(zone_to_nid(z)) & flag;\n+}\n+\n+#else /* !CONFIG_NUMA */\n+\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn false;\n+}\n+\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn false;\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\treturn NULL;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+#endif /* CONFIG_NUMA */\n+\n+#if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\n+\n+int node_private_register(int nid, struct node_private *np);\n+int node_private_unregister(int nid);\n+int node_private_set_ops(int nid, const struct node_private_ops *ops);\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n+\n+#else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n+\n+static inline int node_private_register(int nid, struct node_private *np)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_unregister(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline int node_private_set_ops(int nid,\n+\t\t\t\t       const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_clear_ops(int nid,\n+\t\t\t\t\t const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+#endif /* CONFIG_NUMA && CONFIG_MEMORY_HOTPLUG */\n+\n+#endif /* _LINUX_NODE_PRIVATE_H */\ndiff --git a/include/linux/nodemask.h b/include/linux/nodemask.h\nindex bd38648c998d..c9bcfd5a9a06 100644\n--- a/include/linux/nodemask.h\n+++ b/include/linux/nodemask.h\n@@ -391,6 +391,7 @@ enum node_states {\n \tN_HIGH_MEMORY = N_NORMAL_MEMORY,\n #endif\n \tN_MEMORY,\t\t/* The node has memory(regular, high, movable) */\n+\tN_MEMORY_PRIVATE,\t/* The node's memory is private */\n \tN_CPU,\t\t/* The node has one or more cpus */\n \tN_GENERIC_INITIATOR,\t/* The node has one or more Generic Initiators */\n \tNR_NODE_STATES\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-2-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about general allocations landing on private nodes without explicit opt-in, introduced __GFP_PRIVATE to explicitly allow allocation from N_MEMORY_PRIVATE nodes, and updated cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE nodes unless __GFP_PRIVATE is set.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY_PRIVATE nodes hold device-managed memory that should not be\nused for general allocations. Without a gating mechanism, any allocation\ncould land on a private node if it appears in the task's mems_allowed.\n\nIntroduce __GFP_PRIVATE that explicitly opts in to allocation from\nN_MEMORY_PRIVATE nodes.\n\nAdd the GFP_PRIVATE compound mask (__GFP_PRIVATE | __GFP_THISNODE)\nfor callers that explicitly target private nodes to help prevent\nfallback allocations from DRAM.\n\nUpdate cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE\nnodes unless __GFP_PRIVATE is set.\n\nIn interrupt context, only N_MEMORY nodes are valid.\n\nUpdate cpuset_handle_hotplug() to include N_MEMORY_PRIVATE nodes in\nthe effective mems set, allowing cgroup-level control over private\nnode access.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/gfp_types.h      | 15 +++++++++++++--\n include/trace/events/mmflags.h |  4 ++--\n kernel/cgroup/cpuset.c         | 32 ++++++++++++++++++++++++++++----\n 3 files changed, 43 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/gfp_types.h b/include/linux/gfp_types.h\nindex 3de43b12209e..ac375f9a0fc2 100644\n--- a/include/linux/gfp_types.h\n+++ b/include/linux/gfp_types.h\n@@ -33,7 +33,7 @@ enum {\n \t___GFP_IO_BIT,\n \t___GFP_FS_BIT,\n \t___GFP_ZERO_BIT,\n-\t___GFP_UNUSED_BIT,\t/* 0x200u unused */\n+\t___GFP_PRIVATE_BIT,\n \t___GFP_DIRECT_RECLAIM_BIT,\n \t___GFP_KSWAPD_RECLAIM_BIT,\n \t___GFP_WRITE_BIT,\n@@ -69,7 +69,7 @@ enum {\n #define ___GFP_IO\t\tBIT(___GFP_IO_BIT)\n #define ___GFP_FS\t\tBIT(___GFP_FS_BIT)\n #define ___GFP_ZERO\t\tBIT(___GFP_ZERO_BIT)\n-/* 0x200u unused */\n+#define ___GFP_PRIVATE\t\tBIT(___GFP_PRIVATE_BIT)\n #define ___GFP_DIRECT_RECLAIM\tBIT(___GFP_DIRECT_RECLAIM_BIT)\n #define ___GFP_KSWAPD_RECLAIM\tBIT(___GFP_KSWAPD_RECLAIM_BIT)\n #define ___GFP_WRITE\t\tBIT(___GFP_WRITE_BIT)\n@@ -139,6 +139,11 @@ enum {\n  * %__GFP_ACCOUNT causes the allocation to be accounted to kmemcg.\n  *\n  * %__GFP_NO_OBJ_EXT causes slab allocation to have no object extension.\n+ *\n+ * %__GFP_PRIVATE allows allocation from N_MEMORY_PRIVATE nodes (e.g., compressed\n+ * memory, accelerator memory). Without this flag, allocations are restricted\n+ * to N_MEMORY nodes only. Used by migration/demotion paths when explicitly\n+ * targeting private nodes.\n  */\n #define __GFP_RECLAIMABLE ((__force gfp_t)___GFP_RECLAIMABLE)\n #define __GFP_WRITE\t((__force gfp_t)___GFP_WRITE)\n@@ -146,6 +151,7 @@ enum {\n #define __GFP_THISNODE\t((__force gfp_t)___GFP_THISNODE)\n #define __GFP_ACCOUNT\t((__force gfp_t)___GFP_ACCOUNT)\n #define __GFP_NO_OBJ_EXT   ((__force gfp_t)___GFP_NO_OBJ_EXT)\n+#define __GFP_PRIVATE\t((__force gfp_t)___GFP_PRIVATE)\n \n /**\n  * DOC: Watermark modifiers\n@@ -367,6 +373,10 @@ enum {\n  * available and will not wake kswapd/kcompactd on failure. The _LIGHT\n  * version does not attempt reclaim/compaction at all and is by default used\n  * in page fault path, while the non-light is used by khugepaged.\n+ *\n+ * %GFP_PRIVATE adds %__GFP_THISNODE by default to prevent any fallback\n+ * allocations to other nodes, given that the caller was already attempting\n+ * to access driver-managed memory explicitly.\n  */\n #define GFP_ATOMIC\t(__GFP_HIGH|__GFP_KSWAPD_RECLAIM)\n #define GFP_KERNEL\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n@@ -382,5 +392,6 @@ enum {\n #define GFP_TRANSHUGE_LIGHT\t((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \\\n \t\t\t __GFP_NOMEMALLOC | __GFP_NOWARN) & ~__GFP_RECLAIM)\n #define GFP_TRANSHUGE\t(GFP_TRANSHUGE_LIGHT | __GFP_DIRECT_RECLAIM)\n+#define GFP_PRIVATE\t(__GFP_PRIVATE | __GFP_THISNODE)\n \n #endif /* __LINUX_GFP_TYPES_H */\ndiff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h\nindex a6e5a44c9b42..f042cd848451 100644\n--- a/include/trace/events/mmflags.h\n+++ b/include/trace/events/mmflags.h\n@@ -37,7 +37,8 @@\n \tTRACE_GFP_EM(HARDWALL)\t\t\t\\\n \tTRACE_GFP_EM(THISNODE)\t\t\t\\\n \tTRACE_GFP_EM(ACCOUNT)\t\t\t\\\n-\tTRACE_GFP_EM(ZEROTAGS)\n+\tTRACE_GFP_EM(ZEROTAGS)\t\t\t\\\n+\tTRACE_GFP_EM(PRIVATE)\n \n #ifdef CONFIG_KASAN_HW_TAGS\n # define TRACE_GFP_FLAGS_KASAN\t\t\t\\\n@@ -73,7 +74,6 @@\n TRACE_GFP_FLAGS\n \n /* Just in case these are ever used */\n-TRACE_DEFINE_ENUM(___GFP_UNUSED_BIT);\n TRACE_DEFINE_ENUM(___GFP_LAST_BIT);\n \n #define gfpflag_string(flag) {(__force unsigned long)flag, #flag}\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 473aa9261e16..1a597f0c7c6c 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -444,21 +444,32 @@ static void guarantee_active_cpus(struct task_struct *tsk,\n }\n \n /*\n- * Return in *pmask the portion of a cpusets's mems_allowed that\n+ * Return in *pmask the portion of a cpuset's mems_allowed that\n  * are online, with memory.  If none are online with memory, walk\n  * up the cpuset hierarchy until we find one that does have some\n  * online mems.  The top cpuset always has some mems online.\n  *\n  * One way or another, we guarantee to return some non-empty subset\n- * of node_states[N_MEMORY].\n+ * of node_states[N_MEMORY].  N_MEMORY_PRIVATE nodes from the\n+ * original cpuset are preserved, but only N_MEMORY nodes are\n+ * pulled from ancestors.\n  *\n  * Call with callback_lock or cpuset_mutex held.\n  */\n static void guarantee_online_mems(struct cpuset *cs, nodemask_t *pmask)\n {\n+\tstruct cpuset *orig_cs = cs;\n+\tint nid;\n+\n \twhile (!nodes_intersects(cs->effective_mems, node_states[N_MEMORY]))\n \t\tcs = parent_cs(cs);\n+\n \tnodes_and(*pmask, cs->effective_mems, node_states[N_MEMORY]);\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_isset(nid, orig_cs->effective_mems))\n+\t\t\tnode_set(nid, *pmask);\n+\t}\n }\n \n /**\n@@ -4075,7 +4086,9 @@ static void cpuset_handle_hotplug(void)\n \n \t/* fetch the available cpus/mems and find out which changed how */\n \tcpumask_copy(&new_cpus, cpu_active_mask);\n-\tnew_mems = node_states[N_MEMORY];\n+\n+\t/* Include N_MEMORY_PRIVATE so cpuset controls access the same way */\n+\tnodes_or(new_mems, node_states[N_MEMORY], node_states[N_MEMORY_PRIVATE]);\n \n \t/*\n \t * If subpartitions_cpus is populated, it is likely that the check\n@@ -4488,10 +4501,21 @@ bool cpuset_node_allowed(struct cgroup *cgroup, int nid)\n  * __alloc_pages() will include all nodes.  If the slab allocator\n  * is passed an offline node, it will fall back to the local node.\n  * See kmem_cache_alloc_node().\n+ *\n+ *\n+ * Private nodes aren't eligible for these allocations, so skip them.\n+ * guarantee_online_mems guaranttes at least one N_MEMORY node is set.\n  */\n static int cpuset_spread_node(int *rotor)\n {\n-\treturn *rotor = next_node_in(*rotor, current->mems_allowed);\n+\tint node;\n+\n+\tdo {\n+\t\tnode = next_node_in(*rotor, current->mems_allowed);\n+\t\t*rotor = node;\n+\t} while (node_state(node, N_MEMORY_PRIVATE));\n+\n+\treturn node;\n }\n \n /**\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-3-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that the open-coded cpuset filtering in mm/ does not account for N_MEMORY_PRIVATE nodes on systems without cpusets, and added a new helper numa_zone_allowed() to consolidate zone filtering. The author replaced the open-coded patterns in mm/ with the new helper.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "added new code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Various locations in mm/ open-code cpuset filtering with:\n\n  cpusets_enabled() && ALLOC_CPUSET && !__cpuset_zone_allowed()\n\nThis pattern does not account for N_MEMORY_PRIVATE nodes on systems\nwithout cpusets, so private-node zones can leak into allocation\npaths that should only see general-purpose memory.\n\nAdd numa_zone_allowed() which consolidates zone filtering. It checks\ncpuset membership when cpusets are enabled, and otherwise gates\nN_MEMORY_PRIVATE zones behind __GFP_PRIVATE globally.\n\nReplace the open-coded patterns in mm/ with the new helper.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/compaction.c |  6 ++----\n mm/hugetlb.c    |  2 +-\n mm/internal.h   |  7 +++++++\n mm/page_alloc.c | 31 ++++++++++++++++++++-----------\n mm/slub.c       |  3 ++-\n 5 files changed, 32 insertions(+), 17 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..6a65145b03d8 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -2829,10 +2829,8 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tenum compact_result status;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 51273baec9e5..f2b914ab5910 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -1353,7 +1353,7 @@ static struct folio *dequeue_hugetlb_folio_nodemask(struct hstate *h, gfp_t gfp_\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {\n \t\tstruct folio *folio;\n \n-\t\tif (!cpuset_zone_allowed(zone, gfp_mask))\n+\t\tif (!numa_zone_alloc_allowed(ALLOC_CPUSET, zone, gfp_mask))\n \t\t\tcontinue;\n \t\t/*\n \t\t * no need to ask again on the same node. Pool is node rather than\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 23ee14790227..97023748e6a9 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t      gfp_t gfp_mask);\n #else\n #define node_reclaim_mode 0\n \n@@ -1218,6 +1220,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t\t     gfp_t gfp_mask)\n+{\n+\treturn true;\n+}\n #endif\n \n static inline bool node_reclaim_enabled(void)\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 2facee0805da..47f2619d3840 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3690,6 +3690,21 @@ static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n \treturn node_distance(zone_to_nid(local_zone), zone_to_nid(zone)) <=\n \t\t\t\tnode_reclaim_distance;\n }\n+\n+/* Returns true if allocation from this zone is permitted */\n+bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone, gfp_t gfp_mask)\n+{\n+\t/* Gate N_MEMORY_PRIVATE zones behind __GFP_PRIVATE */\n+\tif (!(gfp_mask & __GFP_PRIVATE) &&\n+\t    node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn false;\n+\n+\t/* If cpusets is being used, check mems_allowed */\n+\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET))\n+\t\treturn cpuset_zone_allowed(zone, gfp_mask);\n+\n+\treturn true;\n+}\n #else\t/* CONFIG_NUMA */\n static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n {\n@@ -3781,10 +3796,8 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\tstruct page *page;\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \t\t/*\n \t\t * When allocating a page cache page for writing, we\n \t\t * want to get it from a node that is within its dirty\n@@ -4585,10 +4598,8 @@ should_reclaim_retry(gfp_t gfp_mask, unsigned order,\n \t\tunsigned long min_wmark = min_wmark_pages(zone);\n \t\tbool wmark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tavailable = reclaimable = zone_reclaimable_pages(zone);\n \t\tavailable += zone_page_state_snapshot(zone, NR_FREE_PAGES);\n@@ -5084,10 +5095,8 @@ unsigned long alloc_pages_bulk_noprof(gfp_t gfp, int preferred_nid,\n \tfor_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&\n-\t\t    !__cpuset_zone_allowed(zone, gfp)) {\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp))\n \t\t\tcontinue;\n-\t\t}\n \n \t\tif (nr_online_nodes > 1 && zone != zonelist_zone(ac.preferred_zoneref) &&\n \t\t    zone_to_nid(zone) != zonelist_node_idx(ac.preferred_zoneref)) {\ndiff --git a/mm/slub.c b/mm/slub.c\nindex 861592ac5425..e4bd6ede81d1 100644\n--- a/mm/slub.c\n+++ b/mm/slub.c\n@@ -3595,7 +3595,8 @@ static struct slab *get_any_partial(struct kmem_cache *s,\n \n \t\t\tn = get_node(s, zone_to_nid(zone));\n \n-\t\t\tif (n && cpuset_zone_allowed(zone, pc->flags) &&\n+\t\t\tif (n && numa_zone_alloc_allowed(ALLOC_CPUSET, zone,\n+\t\t\t\t\t\t   pc->flags) &&\n \t\t\t\t\tn->nr_partial > s->min_partial) {\n \t\t\t\tslab = get_partial_node(s, n, pc);\n \t\t\t\tif (slab) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-4-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern that N_MEMORY fallback lists should not include N_MEMORY_PRIVATE nodes to prevent potential allocation issues, and provided a patch to fix this by adding private nodes as fallbacks for kernel allocations in page_alloc.c.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY fallback lists should not include N_MEMORY_PRIVATE nodes, at\nworst this would allow allocation from them in some scenarios, and at\nbest it causes iterations over nodes that aren't eligible.\n\nPrivate node primary fallback lists do include N_MEMORY nodes so\nkernel/slab allocations made on behalf of the private node can\nfall back to DRAM when __GFP_PRIVATE is not set.\n\nThe nofallback list contains only the node's own zones, restricting\n__GFP_THISNODE allocations to the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/page_alloc.c | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 47f2619d3840..5a1b35421d78 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5683,6 +5683,26 @@ static void build_zonelists(pg_data_t *pgdat)\n \tlocal_node = pgdat->node_id;\n \tprev_node = local_node;\n \n+\t/*\n+\t * Private nodes need N_MEMORY nodes as fallback for kernel allocations\n+\t * (e.g., slab objects allocated on behalf of this node).\n+\t */\n+\tif (node_state(local_node, N_MEMORY_PRIVATE)) {\n+\t\tnode_order[nr_nodes++] = local_node;\n+\t\tnode_set(local_node, used_mask);\n+\n+\t\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0)\n+\t\t\tnode_order[nr_nodes++] = node;\n+\n+\t\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n+\t\tbuild_thisnode_zonelists(pgdat);\n+\t\tpr_info(\"Fallback order for Node %d (private):\", local_node);\n+\t\tfor (node = 0; node < nr_nodes; node++)\n+\t\t\tpr_cont(\" %d\", node_order[node]);\n+\t\tpr_cont(\"\\n\");\n+\t\treturn;\n+\t}\n+\n \tmemset(node_order, 0, sizeof(node_order));\n \twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-5-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the need for a unified predicate to exclude both N_MEMORY_PRIVATE and ZONE_DEVICE folios from MM operations, introducing folio_is_private_managed() as a replacement for folio_is_zone_device(). The existing behavior when NUMA is disabled will be preserved. A fix is planned in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a need for improvement",
                "planned to address it"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Multiple mm/ subsystems already skip operations for ZONE_DEVICE folios,\nand N_MEMORY_PRIVATE folios share the checkpoints for ZONE_DEVICE pages.\n\nAdd folio_is_private_managed() as a unified predicate that returns true\nfor folios on N_MEMORY_PRIVATE nodes or in ZONE_DEVICE.\n\nThis predicate replaces folio_is_zone_device at skip sites where both\nfolio types should be excluded from an MM operation.\n\nAt some locations, explicit zone_device vs private_node checks are more\nappropriate when the operations between the two fundamentally differ.\n\nThe !CONFIG_NUMA stubs fall through to folio_is_zone_device() only,\npreserving existing behavior when NUMA is disabled.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 6a70ec39d569..7687a4cf990c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -92,6 +92,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio) || folio_is_private_node(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n@@ -146,6 +156,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn false;\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-6-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about mlocking private node folios, agreeing that they should not be locked and explaining that the existing folio_is_zone_device check already handles this for device drivers. The author extended this check to include private nodes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nmlocked.  The existing folio_is_zone_device check is already correctly\nplaced to handle this - simply extend it for private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/mlock.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/mlock.c b/mm/mlock.c\nindex 2f699c3497a5..c56159253e45 100644\n--- a/mm/mlock.c\n+++ b/mm/mlock.c\n@@ -25,6 +25,7 @@\n #include <linux/memcontrol.h>\n #include <linux/mm_inline.h>\n #include <linux/secretmem.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -366,7 +367,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (is_huge_zero_pmd(*pmd))\n \t\t\tgoto out;\n \t\tfolio = pmd_folio(*pmd);\n-\t\tif (folio_is_zone_device(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)))\n \t\t\tgoto out;\n \t\tif (vma->vm_flags & VM_LOCKED)\n \t\t\tmlock_folio(folio);\n@@ -386,7 +387,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (!pte_present(ptent))\n \t\t\tcontinue;\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\tstep = folio_mlock_step(folio, pte, addr, end);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-7-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about madvise and pageout operations interfering with device driver memory management in private nodes. They agreed to extend the existing zone_device check to cover private nodes, indicating that this fix will be included in a future version of the patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nsubjectto madvise cold/pageout/free operations that would interfere\nwith the driver's memory management.\n\nExtend the existing zone_device check to cover private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/madvise.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/madvise.c b/mm/madvise.c\nindex b617b1be0f53..3aac105e840b 100644\n--- a/mm/madvise.c\n+++ b/mm/madvise.c\n@@ -32,6 +32,7 @@\n #include <linux/leafops.h>\n #include <linux/shmem_fs.h>\n #include <linux/mmu_notifier.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlb.h>\n \n@@ -475,7 +476,7 @@ static int madvise_cold_or_pageout_pte_range(pmd_t *pmd,\n \t\t\tcontinue;\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n@@ -704,7 +705,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,\n \t\t}\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-8-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern that private node folios should not participate in KSM merging by default, as it can interfere with driver operations. The author extended the existing zone_device checks to cover private node folios and made corresponding changes to the ksm.c file.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "made changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not participate in KSM merging by default.\nThe driver manages the memory lifecycle and KSM's page sharing can\ninterfere with driver operations.\n\nExtend the existing zone_device checks in get_mergeable_page and\nksm_next_page_pmd_entry to cover private node folios as well.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/ksm.c | 9 ++++++---\n 1 file changed, 6 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/ksm.c b/mm/ksm.c\nindex 2d89a7c8b4eb..c48e95a6fff9 100644\n--- a/mm/ksm.c\n+++ b/mm/ksm.c\n@@ -40,6 +40,7 @@\n #include <linux/oom.h>\n #include <linux/numa.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include \"internal.h\"\n@@ -808,7 +809,7 @@ static struct page *get_mergeable_page(struct ksm_rmap_item *rmap_item)\n \n \tfolio = folio_walk_start(&fw, vma, addr, 0);\n \tif (folio) {\n-\t\tif (!folio_is_zone_device(folio) &&\n+\t\tif (!folio_is_private_managed(folio) &&\n \t\t    folio_test_anon(folio)) {\n \t\t\tfolio_get(folio);\n \t\t\tpage = fw.page;\n@@ -2521,7 +2522,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\t\tgoto not_found_unlock;\n \t\t\tfolio = page_folio(page);\n \n-\t\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t\t    !folio_test_anon(folio))\n \t\t\t\tgoto not_found_unlock;\n \n \t\t\tpage += ((addr & (PMD_SIZE - 1)) >> PAGE_SHIFT);\n@@ -2545,7 +2547,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\tcontinue;\n \t\tfolio = page_folio(page);\n \n-\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t    !folio_test_anon(folio))\n \t\t\tcontinue;\n \t\tgoto found_unlock;\n \t}\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-9-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about collapse operations potentially promoting pages from private nodes to local nodes, which can defeat memory tiering. They agreed to handle this issue similarly to zone_device and added checks in khugepaged.c to prevent promotion of private node pages.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "added checks"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "A collapse operation allocates a new large folio and migrates the\nsmaller folios into it.  This is an issue for private nodes:\n\n  1. The private node service may not support migration\n  2. Collapse may promotes pages from the private node to a local node,\n     which may result in an LRU inversion that defeats memory tiering.\n\nHandle this just like zone_device for now.\n\nIt may be possible to support this later for some private node services\nthat report explicit support for collapse (and migration).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/khugepaged.c | 7 ++++---\n 1 file changed, 4 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/khugepaged.c b/mm/khugepaged.c\nindex 97d1b2824386..36f6bc5da53c 100644\n--- a/mm/khugepaged.c\n+++ b/mm/khugepaged.c\n@@ -21,6 +21,7 @@\n #include <linux/shmem_fs.h>\n #include <linux/dax.h>\n #include <linux/ksm.h>\n+#include <linux/node_private.h>\n #include <linux/pgalloc.h>\n \n #include <asm/tlb.h>\n@@ -571,7 +572,7 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,\n \t\t\tgoto out;\n \t\t}\n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out;\n \t\t}\n@@ -1323,7 +1324,7 @@ static int hpage_collapse_scan_pmd(struct mm_struct *mm,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out_unmap;\n \t\t}\n@@ -1575,7 +1576,7 @@ int collapse_pte_mapped_thp(struct mm_struct *mm, unsigned long addr,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, ptent);\n-\t\tif (WARN_ON_ONCE(page && is_zone_device_page(page)))\n+\t\tif (WARN_ON_ONCE(page && page_is_private_managed(page)))\n \t\t\tpage = NULL;\n \t\t/*\n \t\t * Note that uprobe, debugger, or MAP_PRIVATE may change the\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-10-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the cleanup of folios when their refcount drops to zero, explaining that private nodes and zone devices have different semantics for this operation. The author added a new function, folio_managed_on_free(), which wraps both zone_device and private node semantics for this operation. This function will be used in place of the existing free_folio() callback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a difference between zone devices and private nodes",
                "added new function to handle cleanup"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a folio's refcount drops to zero, the service may need to perform\ncleanup before the page returns to the buddy allocator (e.g. zeroing\npages to scrub stale compressed data / release compression ratio).\n\nAdd folio_managed_on_free() to wrap both zone_device and private node\nsemantics for this operation since they are the same.\n\nOne difference between zone_device and private node folios:\n  - private nodes may choose to either take a reference and return true\n    (\"handled\"), or return false to return it back to the buddy.\n\n  - zone_device returns the page to the buddy (always returns true)\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 30 ++++++++++++++++++++++++++++++\n mm/swap.c                    | 21 ++++++++++-----------\n 3 files changed, 46 insertions(+), 11 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7687a4cf990c..09ea7c4cb13c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -39,10 +39,16 @@ struct vm_fault;\n  *   callback to prevent node_private from being freed.\n  *   These callbacks MUST NOT sleep.\n  *\n+ * @free_folio: Called when a folio refcount drops to 0\n+ *   [folio-referenced callback]\n+ *   Returns: true if handled (skip return to buddy)\n+ *            false if no op (return to buddy)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n+\tbool (*free_folio)(struct folio *folio);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 97023748e6a9..658da41cdb8e 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1412,6 +1412,36 @@ int numa_migrate_check(struct folio *folio, struct vm_fault *vmf,\n void free_zone_device_folio(struct folio *folio);\n int migrate_device_coherent_folio(struct folio *folio);\n \n+/**\n+ * folio_managed_on_free - Notify managed-memory service that folio\n+ *                         refcount reached zero.\n+ * @folio: the folio being freed\n+ *\n+ * Returns true if the folio is fully handled (zone_device -- caller\n+ * must return immediately).  Returns false if the callback ran but\n+ * the folio should continue through the normal free path\n+ * (private_node -- pages go back to buddy).\n+ *\n+ * Returns false for normal folios (no-op).\n+ */\n+static inline bool folio_managed_on_free(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio)) {\n+\t\tfree_zone_device_folio(folio);\n+\t\treturn true;\n+\t}\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->free_folio) {\n+\t\t\tif (ops->free_folio(folio))\n+\t\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/swap.c b/mm/swap.c\nindex 2260dcd2775e..dca306e1ae6d 100644\n--- a/mm/swap.c\n+++ b/mm/swap.c\n@@ -37,6 +37,7 @@\n #include <linux/page_idle.h>\n #include <linux/local_lock.h>\n #include <linux/buffer_head.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -96,10 +97,9 @@ static void page_cache_release(struct folio *folio)\n \n void __folio_put(struct folio *folio)\n {\n-\tif (unlikely(folio_is_zone_device(folio))) {\n-\t\tfree_zone_device_folio(folio);\n-\t\treturn;\n-\t}\n+\tif (unlikely(folio_is_private_managed(folio)))\n+\t\tif (folio_managed_on_free(folio))\n+\t\t\treturn;\n \n \tif (folio_test_hugetlb(folio)) {\n \t\tfree_huge_folio(folio);\n@@ -961,19 +961,18 @@ void folios_put_refs(struct folio_batch *folios, unsigned int *refs)\n \t\tif (is_huge_zero_folio(folio))\n \t\t\tcontinue;\n \n-\t\tif (folio_is_zone_device(folio)) {\n+\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n+\t\t\tcontinue;\n+\n+\t\tif (unlikely(folio_is_private_managed(folio))) {\n \t\t\tif (lruvec) {\n \t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n \t\t\t\tlruvec = NULL;\n \t\t\t}\n-\t\t\tif (folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\t\tfree_zone_device_folio(folio);\n-\t\t\tcontinue;\n+\t\t\tif (folio_managed_on_free(folio))\n+\t\t\t\tcontinue;\n \t\t}\n \n-\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\tcontinue;\n-\n \t\t/* hugetlb has its own memcg */\n \t\tif (folio_test_hugetlb(folio)) {\n \t\t\tif (lruvec) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-11-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about handling THP folio splits in private nodes by adding an optional callback to the ops struct and updating __folio_split() to dispatch to this new callback, similar to zone_device's existing split callback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical issue",
                "provided a solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private node services may need to update internal metadata when\na THP folio is split.  ZONE_DEVICE already has a split callback via\npgmap->ops; private nodes can provide the same capability.\n\nJust like zone_device, some private node services may want to know\nabout a folio being split.  Add this optional callback to the ops\nstruct and add a wrapper for zone_device and private node callback\ndispatch to be consolidated.\n\nWire this into __folio_split() where the zone_device check was made.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 33 +++++++++++++++++++++++++++++++++\n mm/huge_memory.c             |  6 ++++--\n 2 files changed, 37 insertions(+), 2 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 09ea7c4cb13c..f9dd2d25c8a5 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -3,6 +3,7 @@\n #define _LINUX_NODE_PRIVATE_H\n \n #include <linux/completion.h>\n+#include <linux/memremap.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -44,11 +45,19 @@ struct vm_fault;\n  *   Returns: true if handled (skip return to buddy)\n  *            false if no op (return to buddy)\n  *\n+ * @folio_split: Notification that a folio on this private node is being split.\n+ *    [folio-referenced callback]\n+ *     Called from the folio split path via folio_managed_split_cb().\n+ *     @folio is the original folio; @new_folio is the newly created folio,\n+ *     or NULL when called for the final (original) folio after all sub-folios\n+ *     have been split off.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n+\tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n \tunsigned long flags;\n };\n \n@@ -150,6 +159,24 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn node_private_flags(zone_to_nid(z)) & flag;\n }\n \n+static inline void node_private_split_cb(struct folio *folio,\n+\t\t\t\t\t struct folio *new_folio)\n+{\n+\tconst struct node_private_ops *ops = folio_node_private_ops(folio);\n+\n+\tif (ops && ops->folio_split)\n+\t\tops->folio_split(folio, new_folio);\n+}\n+\n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+\telse if (folio_is_private_node(original_folio))\n+\t\tnode_private_split_cb(original_folio, new_folio);\n+}\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -198,6 +225,12 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn false;\n }\n \n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+}\n #endif /* CONFIG_NUMA */\n \n #if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 40cf59301c21..2ecae494291a 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -24,6 +24,7 @@\n #include <linux/freezer.h>\n #include <linux/mman.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/debugfs.h>\n #include <linux/migrate.h>\n@@ -3850,7 +3851,7 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \n \t\t\tnext = folio_next(new_folio);\n \n-\t\t\tzone_device_private_split_cb(folio, new_folio);\n+\t\t\tfolio_managed_split_cb(folio, new_folio);\n \n \t\t\tfolio_ref_unfreeze(new_folio,\n \t\t\t\t\t   folio_cache_ref_count(new_folio) + 1);\n@@ -3889,7 +3890,8 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \t\t\tfolio_put_refs(new_folio, nr_pages);\n \t\t}\n \n-\t\tzone_device_private_split_cb(folio, NULL);\n+\t\tfolio_managed_split_cb(folio, NULL);\n+\n \t\t/*\n \t\t * Unfreeze @folio only after all page cache entries, which\n \t\t * used to point to it, have been updated with new folios.\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-12-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about user-driven migration to private nodes, acknowledged that ZONE_DEVICE always rejects user migration, and agreed to add the NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper to support user-driven migration. The author also updated the migrate_to_node() function to allow GFP_PRIVATE when the destination node supports NP_OPS_MIGRATION.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to add new functionality"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services may want to support user-driven migration\n(migrate_pages syscall, mbind) to allow data movement between regular\nand private nodes.\n\nZONE_DEVICE always rejects user migration, but private nodes should\nbe able to opt in.\n\nAdd NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper that\ndispatches migration requests.  Private nodes can either set the flag\nand provide a custom migrate_to callback for driver-managed migration.\n\nIn migrate_to_node(), allows GFP_PRIVATE when the destination node\nsupports NP_OPS_MIGRATION, enabling migrate_pages syscall to target\nprivate nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |   4 ++\n include/linux/migrate.h      |  10 +++\n include/linux/node_private.h | 122 +++++++++++++++++++++++++++++++++++\n mm/damon/paddr.c             |   3 +\n mm/internal.h                |  24 +++++++\n mm/mempolicy.c               |  10 +--\n mm/migrate.c                 |  49 ++++++++++----\n mm/rmap.c                    |   4 +-\n 8 files changed, 206 insertions(+), 20 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 646dc48a23b5..e587f5781135 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -949,6 +949,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \tif (!node_possible(nid))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MIGRATION) &&\n+\t    (!ops->migrate_to || !ops->folio_migrate))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 26ca00c325d9..7b2da3875ff2 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -71,6 +71,9 @@ void folio_migrate_flags(struct folio *newfolio, struct folio *folio);\n int folio_migrate_mapping(struct address_space *mapping,\n \t\tstruct folio *newfolio, struct folio *folio, int extra_count);\n int set_movable_ops(const struct movable_operations *ops, enum pagetype type);\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason);\n \n #else\n \n@@ -96,6 +99,13 @@ static inline int set_movable_ops(const struct movable_operations *ops, enum pag\n {\n \treturn -ENOSYS;\n }\n+static inline int migrate_folios_to_node(struct list_head *folios,\n+\t\t\t\t\t\t  int nid,\n+\t\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t\t  enum migrate_reason reason)\n+{\n+\treturn -ENOSYS;\n+}\n \n #endif /* CONFIG_MIGRATION */\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex f9dd2d25c8a5..0c5be1ee6e60 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -4,6 +4,7 @@\n \n #include <linux/completion.h>\n #include <linux/memremap.h>\n+#include <linux/migrate_mode.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -52,15 +53,40 @@ struct vm_fault;\n  *     or NULL when called for the final (original) folio after all sub-folios\n  *     have been split off.\n  *\n+ * @migrate_to: Migrate folios TO this node.\n+ *\t[refcounted callback]\n+ *\tReturns: 0 on full success, >0 = number of folios that failed to\n+ *\t\t migrate, <0 = error.  Matches migrate_pages() semantics.\n+ *\t\t @nr_succeeded is set to the number of successfully migrated\n+ *\t\t folios (may be NULL if caller doesn't need it).\n+ *\n+ * @folio_migrate: Post-migration notification that a folio on this private node\n+ *    changed physical location (on the same node or a different node).\n+ *    [folio-referenced callback]\n+ *     Called from migrate_folio_move() after data has been copied but before\n+ *     migration entries are replaced with real PTEs.  Both @src and @dst are\n+ *     locked.  Faults block in migration_entry_wait() until\n+ *     remove_migration_ptes() runs, so the service can safely update\n+ *     PFN-based metadata (compression tables, device page tables, DMA\n+ *     mappings, etc.) before any access through the page tables.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n \tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n+\tint (*migrate_to)(struct list_head *folios, int nid,\n+\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t  unsigned int *nr_succeeded);\n+\tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tunsigned long flags;\n };\n \n+/* Allow user/kernel migration; requires migrate_to and folio_migrate */\n+#define NP_OPS_MIGRATION\t\tBIT(0)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\n@@ -177,6 +203,81 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n \t\tnode_private_split_cb(original_folio, new_folio);\n }\n \n+#ifdef CONFIG_MEMORY_HOTPLUG\n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn -ENOENT;\n+\treturn node_private_has_flag(folio_nid(folio), NP_OPS_MIGRATION) ?\n+\t       folio_nid(folio) : -ENOENT;\n+}\n+\n+/**\n+ * folio_managed_allows_migrate - Check if a managed folio supports migration\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio can be migrated.  For zone_device folios, only\n+ * device_private and device_coherent support migration.  For private node\n+ * folios, migration requires NP_OPS_MIGRATION.  Normal folios always\n+ * return true.\n+ */\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\tif (folio_is_private_node(folio))\n+\t\treturn folio_private_flags(folio, NP_OPS_MIGRATION);\n+\treturn true;\n+}\n+\n+/**\n+ * node_private_migrate_to - Attempt service-specific migration to a private node\n+ * @folios: list of folios to migrate (may sleep)\n+ * @nid: target node\n+ * @mode: migration mode (MIGRATE_ASYNC, MIGRATE_SYNC, etc.)\n+ * @reason: migration reason (MR_DEMOTION, MR_SYSCALL, etc.)\n+ * @nr_succeeded: optional output for number of successfully migrated folios\n+ *\n+ * If @nid is an N_MEMORY_PRIVATE node with a migrate_to callback,\n+ * invokes the callback and returns the result with migrate_pages()\n+ * semantics (0 = full success, >0 = failure count, <0 = error).\n+ * Returns -ENODEV if the node is not private or the service is being\n+ * torn down.\n+ *\n+ * The source folios are on other nodes, so they do not pin the target\n+ * node's node_private.  A temporary refcount is taken under rcu_read_lock\n+ * to keep node_private (and the service module) alive across the callback.\n+ */\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\tint (*fn)(struct list_head *, int, enum migrate_mode,\n+\t\t  enum migrate_reason, unsigned int *);\n+\tstruct node_private *np;\n+\tint ret;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (!np || !np->ops || !np->ops->migrate_to ||\n+\t    !refcount_inc_not_zero(&np->refcount)) {\n+\t\trcu_read_unlock();\n+\t\treturn -ENODEV;\n+\t}\n+\tfn = np->ops->migrate_to;\n+\trcu_read_unlock();\n+\n+\tret = fn(folios, nid, mode, reason, nr_succeeded);\n+\n+\tif (refcount_dec_and_test(&np->refcount))\n+\t\tcomplete(&np->released);\n+\n+\treturn ret;\n+}\n+#endif /* CONFIG_MEMORY_HOTPLUG */\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -242,6 +343,27 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\treturn -ENOENT;\n+}\n+\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\treturn true;\n+}\n+\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\treturn -ENODEV;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/damon/paddr.c b/mm/damon/paddr.c\nindex 07a8aead439e..532b8e2c62b0 100644\n--- a/mm/damon/paddr.c\n+++ b/mm/damon/paddr.c\n@@ -277,6 +277,9 @@ static unsigned long damon_pa_migrate(struct damon_region *r,\n \t\telse\n \t\t\t*sz_filter_passed += folio_size(folio) / addr_unit;\n \n+\t\tif (!folio_managed_allows_migrate(folio))\n+\t\t\tgoto put_folio;\n+\n \t\tif (!folio_isolate_lru(folio))\n \t\t\tgoto put_folio;\n \t\tlist_add(&folio->lru, &folio_list);\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 658da41cdb8e..6ab4679fe943 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1442,6 +1442,30 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/**\n+ * folio_managed_migrate_notify - Notify service that a folio changed location\n+ * @src: the old folio (about to be freed)\n+ * @dst: the new folio (data already copied, migration entries still in place)\n+ *\n+ * Called from migrate_folio_move() after data has been copied but before\n+ * remove_migration_ptes() installs real PTEs pointing to @dst.  While\n+ * migration entries are in place, faults block in migration_entry_wait(),\n+ * so the service can safely update PFN-based metadata before any access\n+ * through the page tables.  Both @src and @dst are locked.\n+ */\n+static inline void folio_managed_migrate_notify(struct folio *src,\n+\t\t\t\t\t\tstruct folio *dst)\n+{\n+\tconst struct node_private_ops *ops;\n+\n+\tif (!folio_is_private_node(src))\n+\t\treturn;\n+\n+\tops = folio_node_private_ops(src);\n+\tif (ops && ops->folio_migrate)\n+\t\tops->folio_migrate(src, dst);\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 68a98ba57882..2b0f9762d171 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -111,6 +111,7 @@\n #include <linux/mmu_notifier.h>\n #include <linux/printk.h>\n #include <linux/leafops.h>\n+#include <linux/node_private.h>\n #include <linux/gcd.h>\n \n #include <asm/tlbflush.h>\n@@ -1282,11 +1283,6 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tLIST_HEAD(pagelist);\n \tlong nr_failed;\n \tlong err = 0;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = dest,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n \tnodes_clear(nmask);\n \tnode_set(source, nmask);\n@@ -1311,8 +1307,8 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tmmap_read_unlock(mm);\n \n \tif (!list_empty(&pagelist)) {\n-\t\terr = migrate_pages(&pagelist, alloc_migration_target, NULL,\n-\t\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\t\terr = migrate_folios_to_node(&pagelist, dest, MIGRATE_SYNC,\n+\t\t\t\t\t     MR_SYSCALL);\n \t\tif (err)\n \t\t\tputback_movable_pages(&pagelist);\n \t}\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 5169f9717f60..a54d4af04df3 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -43,6 +43,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/memory-tiers.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1387,6 +1388,8 @@ static int migrate_folio_move(free_folio_t put_new_folio, unsigned long private,\n \tif (old_page_state & PAGE_WAS_MLOCKED)\n \t\tlru_add_drain();\n \n+\tfolio_managed_migrate_notify(src, dst);\n+\n \tif (old_page_state & PAGE_WAS_MAPPED)\n \t\tremove_migration_ptes(src, dst, 0);\n \n@@ -2165,6 +2168,7 @@ int migrate_pages(struct list_head *from, new_folio_t get_new_folio,\n \n \treturn rc_gather;\n }\n+EXPORT_SYMBOL_GPL(migrate_pages);\n \n struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n {\n@@ -2204,6 +2208,31 @@ struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n \n \treturn __folio_alloc(gfp_mask, order, nid, mtc->nmask);\n }\n+EXPORT_SYMBOL_GPL(alloc_migration_target);\n+\n+static int __migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, NULL);\n+}\n+\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason)\n+{\n+\tif (node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_private_migrate_to(folios, nid, mode,\n+\t\t\t\t\t       reason, NULL);\n+\treturn __migrate_folios_to_node(folios, nid, mode, reason);\n+}\n \n #ifdef CONFIG_NUMA\n \n@@ -2221,14 +2250,8 @@ static int store_status(int __user *status, int start, int value, int nr)\n static int do_move_pages_to_node(struct list_head *pagelist, int node)\n {\n \tint err;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = node,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n-\terr = migrate_pages(pagelist, alloc_migration_target, NULL,\n-\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\terr = migrate_folios_to_node(pagelist, node, MIGRATE_SYNC, MR_SYSCALL);\n \tif (err)\n \t\tputback_movable_pages(pagelist);\n \treturn err;\n@@ -2240,7 +2263,7 @@ static int __add_folio_for_migration(struct folio *folio, int node,\n \tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\treturn -EFAULT;\n \n-\tif (folio_is_zone_device(folio))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn -ENOENT;\n \n \tif (folio_nid(folio) == node)\n@@ -2364,7 +2387,8 @@ static int do_pages_move(struct mm_struct *mm, nodemask_t task_nodes,\n \t\terr = -ENODEV;\n \t\tif (node < 0 || node >= MAX_NUMNODES)\n \t\t\tgoto out_flush;\n-\t\tif (!node_state(node, N_MEMORY))\n+\t\tif (!node_state(node, N_MEMORY) &&\n+\t\t    !node_state(node, N_MEMORY_PRIVATE))\n \t\t\tgoto out_flush;\n \n \t\terr = -EACCES;\n@@ -2449,8 +2473,8 @@ static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n \t\tif (folio) {\n \t\t\tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\t\t\terr = -EFAULT;\n-\t\t\telse if (folio_is_zone_device(folio))\n-\t\t\t\terr = -ENOENT;\n+\t\t\telse if (unlikely(folio_is_private_managed(folio)))\n+\t\t\t\terr = folio_managed_allows_user_migrate(folio);\n \t\t\telse\n \t\t\t\terr = folio_nid(folio);\n \t\t\tfolio_walk_end(&fw, vma);\n@@ -2660,6 +2684,9 @@ int migrate_misplaced_folio_prepare(struct folio *folio,\n \tint nr_pages = folio_nr_pages(folio);\n \tpg_data_t *pgdat = NODE_DATA(node);\n \n+\tif (!folio_managed_allows_migrate(folio))\n+\t\treturn -ENOENT;\n+\n \tif (folio_is_file_lru(folio)) {\n \t\t/*\n \t\t * Do not migrate file folios that are mapped in multiple\ndiff --git a/mm/rmap.c b/mm/rmap.c\nindex f955f02d570e..805f9ceb82f3 100644\n--- a/mm/rmap.c\n+++ b/mm/rmap.c\n@@ -72,6 +72,7 @@\n #include <linux/backing-dev.h>\n #include <linux/page_idle.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/userfaultfd_k.h>\n #include <linux/mm_inline.h>\n #include <linux/oom.h>\n@@ -2616,8 +2617,7 @@ void try_to_migrate(struct folio *folio, enum ttu_flags flags)\n \t\t\t\t\tTTU_SYNC | TTU_BATCH_FLUSH)))\n \t\treturn;\n \n-\tif (folio_is_zone_device(folio) &&\n-\t    (!folio_is_device_private(folio) && !folio_is_device_coherent(folio)))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn;\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-13-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about allowing userland to directly allocate from private nodes via set_mempolicy() and mbind(), but not wanting those nodes as normal allocable system memory in the fallback lists. The author added a flag NP_OPS_MEMPOLICY requiring NP_OPS_MIGRATION, updated sysfs 'has_memory' attribute, and modified mempolicy migration sites to use __GFP_PRIVATE.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private nodes want userland to directly allocate from the node\nvia set_mempolicy() and mbind() - but don't want that node as normal\nallocable system memory in the fallback lists.\n\nAdd NP_OPS_MEMPOLICY flag requiring NP_OPS_MIGRATION (since mbind can\ndrive migrations).  Only allow private nodes in policy nodemasks if\nall private nodes in the mask support NP_OPS_MEMPOLICY. This prevents\n__GFP_PRIVATE from unlocking nodes without NP_OPS_MEMPOLICY support.\n\nAdd __GFP_PRIVATE to mempolicy migration sites so moves to opted-in\nprivate nodes succeed.\n\nUpdate the sysfs \"has_memory\" attribute to include N_MEMORY_PRIVATE\nnodes with NP_OPS_MEMPOLICY set, allowing existing numactl userland\ntools to work without modification.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c            | 22 +++++++++++++-\n include/linux/node_private.h   | 40 +++++++++++++++++++++++++\n include/uapi/linux/mempolicy.h |  1 +\n mm/mempolicy.c                 | 54 ++++++++++++++++++++++++++++++----\n mm/page_alloc.c                |  5 ++++\n 5 files changed, 116 insertions(+), 6 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex e587f5781135..c08b5a948779 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -953,6 +953,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (!ops->migrate_to || !ops->folio_migrate))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\n@@ -1145,6 +1149,21 @@ static ssize_t show_node_state(struct device *dev,\n \t\t\t  nodemask_pr_args(&node_states[na->state]));\n }\n \n+/* has_memory includes N_MEMORY + N_MEMORY_PRIVATE that support mempolicy. */\n+static ssize_t show_has_memory(struct device *dev,\n+\t\t\t       struct device_attribute *attr, char *buf)\n+{\n+\tnodemask_t mask = node_states[N_MEMORY];\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_set(nid, mask);\n+\t}\n+\n+\treturn sysfs_emit(buf, \"%*pbl\\n\", nodemask_pr_args(&mask));\n+}\n+\n #define _NODE_ATTR(name, state) \\\n \t{ __ATTR(name, 0444, show_node_state, NULL), state }\n \n@@ -1155,7 +1174,8 @@ static struct node_attr node_state_attr[] = {\n #ifdef CONFIG_HIGHMEM\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n-\t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY] = { __ATTR(has_memory, 0444, show_has_memory, NULL),\n+\t\t       N_MEMORY },\n \t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 0c5be1ee6e60..e9b58afa366b 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -86,6 +86,8 @@ struct node_private_ops {\n \n /* Allow user/kernel migration; requires migrate_to and folio_migrate */\n #define NP_OPS_MIGRATION\t\tBIT(0)\n+/* Allow mempolicy-directed allocation and mbind migration to this node */\n+#define NP_OPS_MEMPOLICY\t\tBIT(1)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -276,6 +278,34 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \n \treturn ret;\n }\n+\n+static inline bool node_mpol_eligible(int nid)\n+{\n+\tbool ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_state(nid, N_MEMORY);\n+\n+\trcu_read_lock();\n+\tret = node_private_has_flag(nid, NP_OPS_MEMPOLICY);\n+\trcu_read_unlock();\n+\treturn ret;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\tint nid;\n+\tbool eligible = false;\n+\n+\tfor_each_node_mask(nid, *nodes) {\n+\t\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\t\tcontinue;\n+\t\tif (!node_mpol_eligible(nid))\n+\t\t\treturn false;\n+\t\teligible = true;\n+\t}\n+\treturn eligible;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -364,6 +394,16 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \treturn -ENODEV;\n }\n \n+static inline bool node_mpol_eligible(int nid)\n+{\n+\treturn false;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/include/uapi/linux/mempolicy.h b/include/uapi/linux/mempolicy.h\nindex 8fbbe613611a..b606eae983c8 100644\n--- a/include/uapi/linux/mempolicy.h\n+++ b/include/uapi/linux/mempolicy.h\n@@ -64,6 +64,7 @@ enum {\n #define MPOL_F_SHARED  (1 << 0)\t/* identify shared policies */\n #define MPOL_F_MOF\t(1 << 3) /* this policy wants migrate on fault */\n #define MPOL_F_MORON\t(1 << 4) /* Migrate On protnone Reference On Node */\n+#define MPOL_F_PRIVATE\t(1 << 5) /* policy targets private node; use __GFP_PRIVATE */\n \n /*\n  * Enabling zone reclaim means the page allocator will attempt to fulfill\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 2b0f9762d171..8ac014950e88 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -406,8 +406,6 @@ static int mpol_new_preferred(struct mempolicy *pol, const nodemask_t *nodes)\n static int mpol_set_nodemask(struct mempolicy *pol,\n \t\t     const nodemask_t *nodes, struct nodemask_scratch *nsc)\n {\n-\tint ret;\n-\n \t/*\n \t * Default (pol==NULL) resp. local memory policies are not a\n \t * subject of any remapping. They also do not need any special\n@@ -416,9 +414,12 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \tif (!pol || pol->mode == MPOL_LOCAL)\n \t\treturn 0;\n \n-\t/* Check N_MEMORY */\n+\t/* Check N_MEMORY and N_MEMORY_PRIVATE*/\n \tnodes_and(nsc->mask1,\n \t\t  cpuset_current_mems_allowed, node_states[N_MEMORY]);\n+\tnodes_and(nsc->mask2, cpuset_current_mems_allowed,\n+\t\t  node_states[N_MEMORY_PRIVATE]);\n+\tnodes_or(nsc->mask1, nsc->mask1, nsc->mask2);\n \n \tVM_BUG_ON(!nodes);\n \n@@ -432,8 +433,13 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \telse\n \t\tpol->w.cpuset_mems_allowed = cpuset_current_mems_allowed;\n \n-\tret = mpol_ops[pol->mode].create(pol, &nsc->mask2);\n-\treturn ret;\n+\t/* All private nodes in the mask must have NP_OPS_MEMPOLICY. */\n+\tif (nodes_private_mpol_allowed(&nsc->mask2))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse if (nodes_intersects(nsc->mask2, node_states[N_MEMORY_PRIVATE]))\n+\t\treturn -EINVAL;\n+\n+\treturn mpol_ops[pol->mode].create(pol, &nsc->mask2);\n }\n \n /*\n@@ -500,6 +506,7 @@ static void mpol_rebind_default(struct mempolicy *pol, const nodemask_t *nodes)\n static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n {\n \tnodemask_t tmp;\n+\tint nid;\n \n \tif (pol->flags & MPOL_F_STATIC_NODES)\n \t\tnodes_and(tmp, pol->w.user_nodemask, *nodes);\n@@ -514,6 +521,21 @@ static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n \tif (nodes_empty(tmp))\n \t\ttmp = *nodes;\n \n+\t/*\n+\t * Drop private nodes that don't have mempolicy support.\n+\t * cpusets guarantees at least one N_MEMORY node in effective_mems\n+\t * and mems_allowed, so dropping private nodes here is safe.\n+\t */\n+\tfor_each_node_mask(nid, tmp) {\n+\t\tif (node_state(nid, N_MEMORY_PRIVATE) &&\n+\t\t    !node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_clear(nid, tmp);\n+\t}\n+\tif (nodes_intersects(tmp, node_states[N_MEMORY_PRIVATE]))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse\n+\t\tpol->flags &= ~MPOL_F_PRIVATE;\n+\n \tpol->nodes = tmp;\n }\n \n@@ -661,6 +683,9 @@ static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)\n \t}\n \tif (!queue_folio_required(folio, qp))\n \t\treturn;\n+\tif (folio_is_private_node(folio) &&\n+\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\treturn;\n \tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||\n \t    !vma_migratable(walk->vma) ||\n \t    !migrate_folio_add(folio, qp->pagelist, qp->flags))\n@@ -717,6 +742,9 @@ static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n \t\tif (!folio || folio_is_zone_device(folio))\n \t\t\tcontinue;\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\t\tcontinue;\n \t\tif (folio_test_large(folio) && max_nr != 1)\n \t\t\tnr = folio_pte_batch(folio, pte, ptent, max_nr);\n \t\t/*\n@@ -1451,6 +1479,9 @@ static struct folio *alloc_migration_target_by_mpol(struct folio *src,\n \telse\n \t\tgfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL | __GFP_COMP;\n \n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \treturn folio_alloc_mpol(gfp, order, pol, ilx, nid);\n }\n #else\n@@ -2280,6 +2311,15 @@ static nodemask_t *policy_nodemask(gfp_t gfp, struct mempolicy *pol,\n \t\t\tnodemask = &pol->nodes;\n \t\tif (pol->home_node != NUMA_NO_NODE)\n \t\t\t*nid = pol->home_node;\n+\t\telse if ((pol->flags & MPOL_F_PRIVATE) &&\n+\t\t\t !node_isset(*nid, pol->nodes)) {\n+\t\t\t/*\n+\t\t\t * Private nodes are not in N_MEMORY nodes' zonelists.\n+\t\t\t * When the preferred nid (usually numa_node_id()) can't\n+\t\t\t * reach the policy nodes, start from a policy node.\n+\t\t\t */\n+\t\t\t*nid = first_node(pol->nodes);\n+\t\t}\n \t\t/*\n \t\t * __GFP_THISNODE shouldn't even be used with the bind policy\n \t\t * because we might easily break the expectation to stay on the\n@@ -2533,6 +2573,10 @@ struct folio *vma_alloc_folio_noprof(gfp_t gfp, int order, struct vm_area_struct\n \t\tgfp |= __GFP_NOWARN;\n \n \tpol = get_vma_policy(vma, addr, order, &ilx);\n+\n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \tfolio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n \tmpol_cond_put(pol);\n \treturn folio;\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 5a1b35421d78..ec6c1f8e85d8 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3849,8 +3849,13 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\t * if another process has NUMA bindings and is causing\n \t\t * kswapd wakeups on only some nodes. Avoid accidental\n \t\t * \"node_reclaim_mode\"-like behavior in this case.\n+\t\t *\n+\t\t * Nodes without kswapd (some private nodes) are never\n+\t\t * skipped - this causes some mempolicies to silently\n+\t\t * fall back to DRAM even if the node is eligible.\n \t\t */\n \t\tif (skip_kswapd_nodes &&\n+\t\t    zone->zone_pgdat->kswapd &&\n \t\t    !waitqueue_active(&zone->zone_pgdat->kswapd_wait)) {\n \t\t\tskipped_kswapd_nodes = true;\n \t\t\tcontinue;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-14-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private nodes being used as demotion targets in the memory-tier subsystem, agreeing that they should be added to the demotion target mask and implementing backpressure support to prevent LRU inversion. The author also acknowledged that the current demotion logic is flawed and suggested re-doing it to allow less fallback and kick kswapd instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The memory-tier subsystem needs to know which private nodes should\nappear as demotion targets.\n\nAdd NP_OPS_DEMOTION (BIT(2)):\n   Node can be added as a demotion target by memory-tiers.\n\nAdd demotion backpressure support so private nodes can reject\nnew demotions cleanly, allowing vmscan to fall back to swap.\n\nIn the demotion path, try demotion to private nodes invididually,\nthen clear private nodes from the demotion target mask until a\nnon-private node is found, then fall back to the remaining mask.\nThis prevents LRU inversion while still allowing forward progress.\n\nThis is the closest match to the current behavior without making\nprivate nodes inaccessible or preventing forward progress. We\nshould probably completely re-do the demotion logic to allow less\nfallback and kick kswapd instead - right now we induce LRU\ninversions by simply falling back to any node in the demotion list.\n\nAdd memory_tier_refresh_demotion() export for services to trigger\nre-evaluation of demotion targets after changing their flags.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory-tiers.h |  9 +++++++\n include/linux/node_private.h | 22 +++++++++++++++++\n mm/internal.h                |  7 ++++++\n mm/memory-tiers.c            | 46 ++++++++++++++++++++++++++++++++----\n mm/page_alloc.c              | 12 +++++++---\n mm/vmscan.c                  | 30 ++++++++++++++++++++++-\n 6 files changed, 117 insertions(+), 9 deletions(-)\n\ndiff --git a/include/linux/memory-tiers.h b/include/linux/memory-tiers.h\nindex 3e1159f6762c..e1476432e359 100644\n--- a/include/linux/memory-tiers.h\n+++ b/include/linux/memory-tiers.h\n@@ -58,6 +58,7 @@ struct memory_dev_type *mt_get_memory_type(int adist);\n int next_demotion_node(int node);\n void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets);\n bool node_is_toptier(int node);\n+void memory_tier_refresh_demotion(void);\n #else\n static inline int next_demotion_node(int node)\n {\n@@ -73,6 +74,10 @@ static inline bool node_is_toptier(int node)\n {\n \treturn true;\n }\n+\n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n #endif\n \n #else\n@@ -106,6 +111,10 @@ static inline bool node_is_toptier(int node)\n \treturn true;\n }\n \n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n+\n static inline int register_mt_adistance_algorithm(struct notifier_block *nb)\n {\n \treturn 0;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e9b58afa366b..e254e36056cd 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -88,6 +88,8 @@ struct node_private_ops {\n #define NP_OPS_MIGRATION\t\tBIT(0)\n /* Allow mempolicy-directed allocation and mbind migration to this node */\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n+/* Node participates as a demotion target in memory-tiers */\n+#define NP_OPS_DEMOTION\t\t\tBIT(2)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -101,12 +103,14 @@ struct node_private_ops {\n  *\t\tcallbacks that may sleep; 0 = fully released)\n  * @released: Signaled when refcount drops to 0; unregister waits on this\n  * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ * @migration_blocked: Service signals migrations should pause\n  */\n struct node_private {\n \tvoid *owner;\n \trefcount_t refcount;\n \tstruct completion released;\n \tconst struct node_private_ops *ops;\n+\tbool migration_blocked;\n };\n \n #ifdef CONFIG_NUMA\n@@ -306,6 +310,19 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \t}\n \treturn eligible;\n }\n+\n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\tstruct node_private *np;\n+\tbool blocked;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tblocked = np && READ_ONCE(np->migration_blocked);\n+\trcu_read_unlock();\n+\n+\treturn blocked;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -404,6 +421,11 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \treturn false;\n }\n \n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 6ab4679fe943..5950e20d4023 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t  const nodemask_t *candidates);\n extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t      gfp_t gfp_mask);\n #else\n@@ -1220,6 +1222,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t\t const nodemask_t *candidates)\n+{\n+\treturn NUMA_NO_NODE;\n+}\n static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t\t     gfp_t gfp_mask)\n {\ndiff --git a/mm/memory-tiers.c b/mm/memory-tiers.c\nindex 9c742e18e48f..434190fdc078 100644\n--- a/mm/memory-tiers.c\n+++ b/mm/memory-tiers.c\n@@ -3,6 +3,7 @@\n #include <linux/lockdep.h>\n #include <linux/sysfs.h>\n #include <linux/kobject.h>\n+#include <linux/node_private.h>\n #include <linux/memory.h>\n #include <linux/memory-tiers.h>\n #include <linux/notifier.h>\n@@ -380,6 +381,8 @@ static void disable_all_demotion_targets(void)\n \t\tif (memtier)\n \t\t\tmemtier->lower_tier_mask = NODE_MASK_NONE;\n \t}\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE)\n+\t\tnode_demotion[node].preferred = NODE_MASK_NONE;\n \t/*\n \t * Ensure that the \"disable\" is visible across the system.\n \t * Readers will see either a combination of before+disable\n@@ -421,6 +424,7 @@ static void establish_demotion_targets(void)\n \tint target = NUMA_NO_NODE, node;\n \tint distance, best_distance;\n \tnodemask_t tier_nodes, lower_tier;\n+\tnodemask_t all_memory;\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n@@ -429,6 +433,13 @@ static void establish_demotion_targets(void)\n \n \tdisable_all_demotion_targets();\n \n+\t/* Include private nodes that have opted in to demotion. */\n+\tall_memory = node_states[N_MEMORY];\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(node, NP_OPS_DEMOTION))\n+\t\t\tnode_set(node, all_memory);\n+\t}\n+\n \tfor_each_node_state(node, N_MEMORY) {\n \t\tbest_distance = -1;\n \t\tnd = &node_demotion[node];\n@@ -442,12 +453,12 @@ static void establish_demotion_targets(void)\n \t\tmemtier = list_next_entry(memtier, list);\n \t\ttier_nodes = get_memtier_nodemask(memtier);\n \t\t/*\n-\t\t * find_next_best_node, use 'used' nodemask as a skip list.\n+\t\t * find_next_best_node_in, use 'used' nodemask as a skip list.\n \t\t * Add all memory nodes except the selected memory tier\n \t\t * nodelist to skip list so that we find the best node from the\n \t\t * memtier nodelist.\n \t\t */\n-\t\tnodes_andnot(tier_nodes, node_states[N_MEMORY], tier_nodes);\n+\t\tnodes_andnot(tier_nodes, all_memory, tier_nodes);\n \n \t\t/*\n \t\t * Find all the nodes in the memory tier node list of same best distance.\n@@ -455,7 +466,8 @@ static void establish_demotion_targets(void)\n \t\t * in the preferred mask when allocating pages during demotion.\n \t\t */\n \t\tdo {\n-\t\t\ttarget = find_next_best_node(node, &tier_nodes);\n+\t\t\ttarget = find_next_best_node_in(node, &tier_nodes,\n+\t\t\t\t\t\t\t&all_memory);\n \t\t\tif (target == NUMA_NO_NODE)\n \t\t\t\tbreak;\n \n@@ -495,7 +507,7 @@ static void establish_demotion_targets(void)\n \t * allocation to a set of nodes that is closer the above selected\n \t * preferred node.\n \t */\n-\tlower_tier = node_states[N_MEMORY];\n+\tlower_tier = all_memory;\n \tlist_for_each_entry(memtier, &memory_tiers, list) {\n \t\t/*\n \t\t * Keep removing current tier from lower_tier nodes,\n@@ -542,7 +554,7 @@ static struct memory_tier *set_node_memory_tier(int node)\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n-\tif (!node_state(node, N_MEMORY))\n+\tif (!node_state(node, N_MEMORY) && !node_state(node, N_MEMORY_PRIVATE))\n \t\treturn ERR_PTR(-EINVAL);\n \n \tmt_calc_adistance(node, &adist);\n@@ -865,6 +877,30 @@ int mt_calc_adistance(int node, int *adist)\n }\n EXPORT_SYMBOL_GPL(mt_calc_adistance);\n \n+/**\n+ * memory_tier_refresh_demotion() - Re-establish demotion targets\n+ *\n+ * Called by services after registering or unregistering ops->migrate_to on\n+ * a private node, so that establish_demotion_targets() picks up the change.\n+ */\n+void memory_tier_refresh_demotion(void)\n+{\n+\tint nid;\n+\n+\tmutex_lock(&memory_tier_lock);\n+\t/*\n+\t * Ensure private nodes are registered with a tier, otherwise\n+\t * they won't show up in any node's demotion targets nodemask.\n+\t */\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (!__node_get_memory_tier(nid))\n+\t\t\tset_node_memory_tier(nid);\n+\t}\n+\testablish_demotion_targets();\n+\tmutex_unlock(&memory_tier_lock);\n+}\n+EXPORT_SYMBOL_GPL(memory_tier_refresh_demotion);\n+\n static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\t\t\t\t      unsigned long action, void *_arg)\n {\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex ec6c1f8e85d8..e272dfdc6b00 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5589,7 +5589,8 @@ static int node_load[MAX_NUMNODES];\n  *\n  * Return: node id of the found node or %NUMA_NO_NODE if no node is found.\n  */\n-int find_next_best_node(int node, nodemask_t *used_node_mask)\n+int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t   const nodemask_t *candidates)\n {\n \tint n, val;\n \tint min_val = INT_MAX;\n@@ -5599,12 +5600,12 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \t * Use the local node if we haven't already, but for memoryless local\n \t * node, we should skip it and fall back to other nodes.\n \t */\n-\tif (!node_isset(node, *used_node_mask) && node_state(node, N_MEMORY)) {\n+\tif (!node_isset(node, *used_node_mask) && node_isset(node, *candidates)) {\n \t\tnode_set(node, *used_node_mask);\n \t\treturn node;\n \t}\n \n-\tfor_each_node_state(n, N_MEMORY) {\n+\tfor_each_node_mask(n, *candidates) {\n \n \t\t/* Don't want a node to appear more than once */\n \t\tif (node_isset(n, *used_node_mask))\n@@ -5636,6 +5637,11 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \treturn best_node;\n }\n \n+int find_next_best_node(int node, nodemask_t *used_node_mask)\n+{\n+\treturn find_next_best_node_in(node, used_node_mask,\n+\t\t\t\t      &node_states[N_MEMORY]);\n+}\n \n /*\n  * Build zonelists ordered by node and zones within node.\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 6113be4d3519..0f534428ea88 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -355,6 +356,10 @@ static bool can_demote(int nid, struct scan_control *sc,\n \tif (demotion_nid == NUMA_NO_NODE)\n \t\treturn false;\n \n+\t/* Don't demote when the target's service signals backpressure */\n+\tif (node_private_migration_blocked(demotion_nid))\n+\t\treturn false;\n+\n \t/* If demotion node isn't in the cgroup's mems_allowed, fall back */\n \treturn mem_cgroup_node_allowed(memcg, demotion_nid);\n }\n@@ -1022,8 +1027,10 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \t\t\t\t     struct pglist_data *pgdat)\n {\n \tint target_nid = next_demotion_node(pgdat->node_id);\n-\tunsigned int nr_succeeded;\n+\tint first_nid = target_nid;\n+\tunsigned int nr_succeeded = 0;\n \tnodemask_t allowed_mask;\n+\tint ret;\n \n \tstruct migration_target_control mtc = {\n \t\t/*\n@@ -1046,6 +1053,27 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \n \tnode_get_allowed_targets(pgdat, &allowed_mask);\n \n+\t/* Try private node targets until we find non-private node */\n+\twhile (node_state(target_nid, N_MEMORY_PRIVATE)) {\n+\t\tunsigned int nr = 0;\n+\n+\t\tret = node_private_migrate_to(demote_folios, target_nid,\n+\t\t\t\t\t      MIGRATE_ASYNC, MR_DEMOTION,\n+\t\t\t\t\t      &nr);\n+\t\tnr_succeeded += nr;\n+\t\tif (ret == 0 || list_empty(demote_folios))\n+\t\t\treturn nr_succeeded;\n+\n+\t\ttarget_nid = next_node_in(target_nid, allowed_mask);\n+\t\tif (target_nid == first_nid)\n+\t\t\treturn nr_succeeded;\n+\t\tif (!node_state(target_nid, N_MEMORY_PRIVATE))\n+\t\t\tbreak;\n+\t}\n+\n+\t/* target_nid is a non-private node; use standard migration */\n+\tmtc.nid = target_nid;\n+\n \t/* Demotion ignores all cpuset and mempolicy settings */\n \tmigrate_pages(demote_folios, alloc_demote_folio, NULL,\n \t\t      (unsigned long)&mtc, MIGRATE_ASYNC, MR_DEMOTION,\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-15-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about write faults on private nodes by introducing NP_OPS_PROTECT_WRITE and modifying the PTE handling code to prevent silent upgrade of PTEs, allowing services to handle write faults with promotion or custom logic.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Services that intercept write faults (e.g., for promotion tracking)\nneed PTEs to stay read-only. This requires preventing mprotect\nfrom silently upgrade the PTE, bypassing the service's handle_fault\ncallback.\n\nAdd NP_OPS_PROTECT_WRITE and folio_managed_wrprotect().\n\nIn change_pte_range() and change_huge_pmd(), suppress PTE write-upgrade\nwhen MM_CP_TRY_CHANGE_WRITABLE is sees the folio is write-protected.\n\nIn handle_pte_fault() and do_huge_pmd_wp_page(), dispatch to the node's\nops->handle_fault callback when set, allowing the service to handle write\nfaults with promotion or other custom logic.\n\nNP_OPS_MEMPOLICY is incompatible with NP_OPS_PROTECT_WRITE to avoid the\nfootgun of binding a writable VMA to a write-protected node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++\n include/linux/node_private.h | 22 ++++++++\n mm/huge_memory.c             | 17 ++++++-\n mm/internal.h                | 99 ++++++++++++++++++++++++++++++++++++\n mm/memory.c                  | 15 ++++++\n mm/migrate.c                 | 14 +----\n mm/mprotect.c                |  4 +-\n 7 files changed, 159 insertions(+), 16 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex c08b5a948779..a4955b9b5b93 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -957,6 +957,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    (ops->flags & NP_OPS_PROTECT_WRITE))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e254e36056cd..27d6e5d84e61 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -70,6 +70,24 @@ struct vm_fault;\n  *     PFN-based metadata (compression tables, device page tables, DMA\n  *     mappings, etc.) before any access through the page tables.\n  *\n+ * @handle_fault: Handle fault on folio on this private node.\n+ *   [folio-referenced callback, PTL held on entry]\n+ *\n+ *   Called from handle_pte_fault() (PTE level) or do_huge_pmd_wp_page()\n+ *   (PMD level) after lock acquisition and entry verification.\n+ *   @folio is the faulting folio, @level indicates the page table level.\n+ *\n+ *   For PGTABLE_LEVEL_PTE: vmf->pte is mapped and vmf->ptl is the\n+ *   PTE lock.  Release via pte_unmap_unlock(vmf->pte, vmf->ptl).\n+ *\n+ *   For PGTABLE_LEVEL_PMD: vmf->pte is NULL and vmf->ptl is the\n+ *   PMD lock.  Release via spin_unlock(vmf->ptl).\n+ *\n+ *   The callback MUST release PTL on ALL paths.\n+ *   The caller will NOT touch the page table entry after this returns.\n+ *\n+ *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -81,6 +99,8 @@ struct node_private_ops {\n \t\t\t\t  enum migrate_reason reason,\n \t\t\t\t  unsigned int *nr_succeeded);\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n+\tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t   enum pgtable_level level);\n \tunsigned long flags;\n };\n \n@@ -90,6 +110,8 @@ struct node_private_ops {\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n /* Node participates as a demotion target in memory-tiers */\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n+/* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n+#define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 2ecae494291a..d9ba6593244d 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -2063,12 +2063,14 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tstruct page *page;\n \tunsigned long haddr = vmf->address & HPAGE_PMD_MASK;\n \tpmd_t orig_pmd = vmf->orig_pmd;\n+\tvm_fault_t ret;\n+\n \n \tvmf->ptl = pmd_lockptr(vma->vm_mm, vmf->pmd);\n \tVM_BUG_ON_VMA(!vma->anon_vma, vma);\n \n \tif (is_huge_zero_pmd(orig_pmd)) {\n-\t\tvm_fault_t ret = do_huge_zero_wp_pmd(vmf);\n+\t\tret = do_huge_zero_wp_pmd(vmf);\n \n \t\tif (!(ret & VM_FAULT_FALLBACK))\n \t\t\treturn ret;\n@@ -2088,6 +2090,13 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tfolio = page_folio(page);\n \tVM_BUG_ON_PAGE(!PageHead(page), page);\n \n+\t/* Private-managed write-protect: let the service handle the fault */\n+\tif (unlikely(folio_is_private_managed(folio))) {\n+\t\tif (folio_managed_handle_fault(folio, vmf,\n+\t\t\t\t\t      PGTABLE_LEVEL_PMD, &ret))\n+\t\t\treturn ret;\n+\t}\n+\n \t/* Early check when only holding the PT lock. */\n \tif (PageAnonExclusive(page))\n \t\tgoto reuse;\n@@ -2633,7 +2642,8 @@ int change_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,\n \n \t/* See change_pte_range(). */\n \tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) && !pmd_write(entry) &&\n-\t    can_change_pmd_writable(vma, addr, entry))\n+\t    can_change_pmd_writable(vma, addr, entry) &&\n+\t    !folio_managed_wrprotect(pmd_folio(entry)))\n \t\tentry = pmd_mkwrite(entry, vma);\n \n \tret = HPAGE_PMD_NR;\n@@ -4943,6 +4953,9 @@ void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n \tif (folio_test_dirty(folio) && softleaf_is_migration_dirty(entry))\n \t\tpmde = pmd_mkdirty(pmde);\n \n+\tif (folio_managed_wrprotect(folio))\n+\t\tpmde = pmd_wrprotect(pmde);\n+\n \tif (folio_is_device_private(folio)) {\n \t\tswp_entry_t entry;\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 5950e20d4023..ae4ff86e8dc6 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -11,6 +11,7 @@\n #include <linux/khugepaged.h>\n #include <linux/mm.h>\n #include <linux/mm_inline.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/pagewalk.h>\n #include <linux/rmap.h>\n@@ -18,6 +19,7 @@\n #include <linux/leafops.h>\n #include <linux/swap_cgroup.h>\n #include <linux/tracepoint-defs.h>\n+#include <linux/node_private.h>\n \n /* Internal core VMA manipulation functions. */\n #include \"vma.h\"\n@@ -1449,6 +1451,103 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/*\n+ * folio_managed_handle_fault - Dispatch fault on managed-memory folio\n+ * @folio: the faulting folio (must not be NULL)\n+ * @vmf: the vm_fault descriptor (PTL held: vmf->ptl locked)\n+ * @level: page table level (PGTABLE_LEVEL_PTE or PGTABLE_LEVEL_PMD)\n+ * @ret: output fault result if handled\n+ *\n+ * Called with PTL held.  If a handle_fault callback exists, it is invoked\n+ * with PTL still held.  The callback is responsible for releasing PTL on\n+ * all paths.\n+ *\n+ * Returns true if the service handled the fault (PTL released by callback,\n+ * caller returns *ret).  Returns false if no handler exists (PTL still held,\n+ * caller continues with normal fault handling).\n+ */\n+static inline bool folio_managed_handle_fault(struct folio *folio,\n+\t\t\t\t\t      struct vm_fault *vmf,\n+\t\t\t\t\t      enum pgtable_level level,\n+\t\t\t\t\t      vm_fault_t *ret)\n+{\n+\t/* Zone device pages use swap entries; handled in do_swap_page */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->handle_fault) {\n+\t\t\t*ret = ops->handle_fault(folio, vmf, level);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n+/**\n+ * folio_managed_wrprotect - Should this folio's mappings stay write-protected?\n+ * @folio: the folio to check\n+ *\n+ * Returns true if the folio is on a private node with NP_OPS_PROTECT_WRITE,\n+ * meaning page table entries (PTE or PMD) should not be made writable.\n+ * Write faults are intercepted by the service's handle_fault callback\n+ * to promote the folio to DRAM.\n+ *\n+ * Used by:\n+ *   - change_pte_range() / change_huge_pmd(): prevent mprotect write-upgrade\n+ *   - remove_migration_pte() / remove_migration_pmd(): strip write after migration\n+ *   - do_huge_pmd_wp_page(): dispatch to fault handler instead of reuse\n+ */\n+static inline bool folio_managed_wrprotect(struct folio *folio)\n+{\n+\treturn unlikely(folio_is_private_node(folio) &&\n+\t\t\tfolio_private_flags(folio, NP_OPS_PROTECT_WRITE));\n+}\n+\n+/**\n+ * folio_managed_fixup_migration_pte - Fixup PTE after migration for\n+ *                                     managed memory pages.\n+ * @new: the destination page\n+ * @pte: the PTE being installed (normal PTE built by caller)\n+ * @old_pte: the original PTE (before migration, for swap entry flags)\n+ * @vma: the VMA\n+ *\n+ * For MEMORY_DEVICE_PRIVATE pages: replaces the PTE with a device-private\n+ * swap entry, preserving soft_dirty and uffd_wp from old_pte.\n+ *\n+ * For N_MEMORY_PRIVATE pages with NP_OPS_PROTECT_WRITE: strips the write\n+ * bit so the next write triggers the fault handler for promotion.\n+ *\n+ * For normal pages: returns pte unmodified.\n+ */\n+static inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n+\t\t\t\t\t\t      pte_t pte,\n+\t\t\t\t\t\t      pte_t old_pte,\n+\t\t\t\t\t\t      struct vm_area_struct *vma)\n+{\n+\tif (unlikely(is_device_private_page(new))) {\n+\t\tsoftleaf_t entry;\n+\n+\t\tif (pte_write(pte))\n+\t\t\tentry = make_writable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\telse\n+\t\t\tentry = make_readable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\tpte = softleaf_to_pte(entry);\n+\t\tif (pte_swp_soft_dirty(old_pte))\n+\t\t\tpte = pte_swp_mksoft_dirty(pte);\n+\t\tif (pte_swp_uffd_wp(old_pte))\n+\t\t\tpte = pte_swp_mkuffd_wp(pte);\n+\t} else if (folio_managed_wrprotect(page_folio(new))) {\n+\t\tpte = pte_wrprotect(pte);\n+\t}\n+\treturn pte;\n+}\n+\n /**\n  * folio_managed_migrate_notify - Notify service that a folio changed location\n  * @src: the old folio (about to be freed)\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 2a55edc48a65..0f78988befef 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -6079,6 +6079,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n \t */\n+\tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n+\t\twritable = false;\n+\t\tignore_writable = true;\n+\t}\n \tif (folio && folio_test_large(folio))\n \t\tnuma_rebuild_large_mapping(vmf, vma, folio, pte, ignore_writable,\n \t\t\t\t\t   pte_write_upgrade);\n@@ -6228,6 +6232,7 @@ static void fix_spurious_fault(struct vm_fault *vmf,\n  */\n static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n {\n+\tstruct folio *folio;\n \tpte_t entry;\n \n \tif (unlikely(pmd_none(*vmf->pmd))) {\n@@ -6284,6 +6289,16 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n \t\tupdate_mmu_tlb(vmf->vma, vmf->address, vmf->pte);\n \t\tgoto unlock;\n \t}\n+\n+\tfolio = vm_normal_folio(vmf->vma, vmf->address, entry);\n+\tif (unlikely(folio && folio_is_private_managed(folio))) {\n+\t\tvm_fault_t fault_ret;\n+\n+\t\tif (folio_managed_handle_fault(folio, vmf, PGTABLE_LEVEL_PTE,\n+\t\t\t\t\t       &fault_ret))\n+\t\t\treturn fault_ret;\n+\t}\n+\n \tif (vmf->flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {\n \t\tif (!pte_write(entry))\n \t\t\treturn do_wp_page(vmf);\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex a54d4af04df3..f632e8b03504 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -398,19 +398,7 @@ static bool remove_migration_pte(struct folio *folio,\n \t\tif (folio_test_anon(folio) && !softleaf_is_migration_read(entry))\n \t\t\trmap_flags |= RMAP_EXCLUSIVE;\n \n-\t\tif (unlikely(is_device_private_page(new))) {\n-\t\t\tif (pte_write(pte))\n-\t\t\t\tentry = make_writable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\telse\n-\t\t\t\tentry = make_readable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\tpte = softleaf_to_pte(entry);\n-\t\t\tif (pte_swp_soft_dirty(old_pte))\n-\t\t\t\tpte = pte_swp_mksoft_dirty(pte);\n-\t\t\tif (pte_swp_uffd_wp(old_pte))\n-\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n-\t\t}\n+\t\tpte = folio_managed_fixup_migration_pte(new, pte, old_pte, vma);\n \n #ifdef CONFIG_HUGETLB_PAGE\n \t\tif (folio_test_hugetlb(folio)) {\ndiff --git a/mm/mprotect.c b/mm/mprotect.c\nindex 283889e4f1ce..830be609bc24 100644\n--- a/mm/mprotect.c\n+++ b/mm/mprotect.c\n@@ -30,6 +30,7 @@\n #include <linux/mm_inline.h>\n #include <linux/pgtable.h>\n #include <linux/userfaultfd_k.h>\n+#include <linux/node_private.h>\n #include <uapi/linux/mman.h>\n #include <asm/cacheflush.h>\n #include <asm/mmu_context.h>\n@@ -290,7 +291,8 @@ static long change_pte_range(struct mmu_gather *tlb,\n \t\t\t * COW or special handling is required.\n \t\t\t */\n \t\t\tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) &&\n-\t\t\t     !pte_write(ptent))\n+\t\t\t     !pte_write(ptent) &&\n+\t\t\t     !(folio && folio_managed_wrprotect(folio)))\n \t\t\t\tset_write_prot_commit_flush_ptes(vma, folio, page,\n \t\t\t\taddr, pte, oldpte, ptent, nr_ptes, tlb);\n \t\t\telse\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-16-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about kswapd's ability to make progress during boosted reclaim on private nodes. They acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, and added a reclaim_policy callback to struct node_private_ops to allow services to override may_swap / may_writepage.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services that drive kswapd via watermark_boost need\ncontrol over the reclaim policy.  There are three problems:\n\n1) Boosted reclaim suppresses may_swap and may_writepage.  When\n   demotion is not possible, swap is the only evict path, so kswapd\n   cannot make progress and pages are stranded.\n\n2) __setup_per_zone_wmarks() unconditionally zeros watermark_boost,\n   killing the service's pressure signal.\n\n3) Not all private nodes want reclaim to touch their pages.\n\nAdd a reclaim_policy callback to struct node_private_ops and a\nstruct node_reclaim_policy with:\n\n  - active:             set by the helper when a callback was invoked\n  - may_swap:           allow swap writeback during boosted reclaim\n  - may_writepage:      allow writepage during boosted reclaim\n  - managed_watermarks: service owns watermark_boost lifecycle\n\nWe do not allow disabling swap/writepage, as core MM may have\nexplicitly enabled them on a non-boosted pass.\n\nWe only allow enablign swap/writepage, so that the supression during\na boost can be overridden.  This allows a device to force evictions\neven when the system otherwise would not percieve pressure.\n\nThis is important for a service like compressed RAM, as device capacity\nmay differ from reported capacity, and device may want to relieve real\npressure (poor compression ratio) as opposed to percieved pressure\n(i.e. how many pages are in use).\n\nAdd zone_reclaim_allowed() to filter private nodes that have not\nopted into reclaim.\n\nRegular nodes fall through to cpuset_zone_allowed() unchanged.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 28 ++++++++++++++++++++++++++++\n mm/internal.h                | 36 ++++++++++++++++++++++++++++++++++++\n mm/page_alloc.c              | 11 ++++++++++-\n mm/vmscan.c                  | 25 +++++++++++++++++++++++--\n 4 files changed, 97 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 27d6e5d84e61..34be52383255 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -14,6 +14,24 @@ struct page;\n struct vm_area_struct;\n struct vm_fault;\n \n+/**\n+ * struct node_reclaim_policy - Reclaim policy overrides for private nodes\n+ * @active: set by node_private_reclaim_policy() when a callback was invoked\n+ * @may_swap: allow swap writeback during boosted reclaim\n+ * @may_writepage: allow writepage during boosted reclaim\n+ * @managed_watermarks: service owns watermark_boost lifecycle; kswapd must\n+ *                      not clear it after boosted reclaim\n+ *\n+ * Passed to the reclaim_policy callback so each private node service can\n+ * inject its own reclaim policy before kswapd runs boosted reclaim.\n+ */\n+struct node_reclaim_policy {\n+\tbool active;\n+\tbool may_swap;\n+\tbool may_writepage;\n+\tbool managed_watermarks;\n+};\n+\n /**\n  * struct node_private_ops - Callbacks for private node services\n  *\n@@ -88,6 +106,13 @@ struct vm_fault;\n  *\n  *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n  *\n+ * @reclaim_policy: Configure reclaim policy for boosted reclaim.\n+ *   [called hodling rcu_read_lock, MUST NOT sleep]\n+ *   Called by kswapd before boosted reclaim to let the service override\n+ *   may_swap / may_writepage.  If provided, the service also owns the\n+ *   watermark_boost lifecycle (kswapd will not clear it).\n+ *   If NULL, normal boost policy applies.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -101,6 +126,7 @@ struct node_private_ops {\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n+\tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n \tunsigned long flags;\n };\n \n@@ -112,6 +138,8 @@ struct node_private_ops {\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n /* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n+/* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n+#define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/internal.h b/mm/internal.h\nindex ae4ff86e8dc6..db32cb2d7a29 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1572,6 +1572,42 @@ static inline void folio_managed_migrate_notify(struct folio *src,\n \t\tops->folio_migrate(src, dst);\n }\n \n+/**\n+ * node_private_reclaim_policy - invoke the service's reclaim policy callback\n+ * @nid: NUMA node id\n+ * @policy: reclaim policy struct to fill in\n+ *\n+ * Called by kswapd before boosted reclaim.  Zeroes @policy, then if the\n+ * private node service provides a reclaim_policy callback, invokes it\n+ * and sets policy->active to true.\n+ */\n+#ifdef CONFIG_NUMA\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tstruct node_private *np;\n+\n+\tmemset(policy, 0, sizeof(*policy));\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (np && np->ops && np->ops->reclaim_policy) {\n+\t\tnp->ops->reclaim_policy(nid, policy);\n+\t\tpolicy->active = true;\n+\t}\n+\trcu_read_unlock();\n+}\n+#else\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tmemset(policy, 0, sizeof(*policy));\n+}\n+#endif\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e272dfdc6b00..9692048ab5fb 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -55,6 +55,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/node_private.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -6437,6 +6438,8 @@ static void __setup_per_zone_wmarks(void)\n \tunsigned long lowmem_pages = 0;\n \tstruct zone *zone;\n \tunsigned long flags;\n+\tstruct node_reclaim_policy rp;\n+\tint prev_nid = NUMA_NO_NODE;\n \n \t/* Calculate total number of !ZONE_HIGHMEM and !ZONE_MOVABLE pages */\n \tfor_each_zone(zone) {\n@@ -6446,6 +6449,7 @@ static void __setup_per_zone_wmarks(void)\n \n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n+\t\tint nid = zone_to_nid(zone);\n \n \t\tspin_lock_irqsave(&zone->lock, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n@@ -6482,7 +6486,12 @@ static void __setup_per_zone_wmarks(void)\n \t\t\t    mult_frac(zone_managed_pages(zone),\n \t\t\t\t      watermark_scale_factor, 10000));\n \n-\t\tzone->watermark_boost = 0;\n+\t\tif (nid != prev_nid) {\n+\t\t\tnode_private_reclaim_policy(nid, &rp);\n+\t\t\tprev_nid = nid;\n+\t\t}\n+\t\tif (!rp.managed_watermarks)\n+\t\t\tzone->watermark_boost = 0;\n \t\tzone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_HIGH] = low_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 0f534428ea88..07de666c1276 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -73,6 +73,13 @@\n #define CREATE_TRACE_POINTS\n #include <trace/events/vmscan.h>\n \n+static inline bool zone_reclaim_allowed(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn zone_private_flags(zone, NP_OPS_RECLAIM);\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n struct scan_control {\n \t/* How many pages shrink_list() should reclaim */\n \tunsigned long nr_to_reclaim;\n@@ -6274,7 +6281,7 @@ static void shrink_zones(struct zonelist *zonelist, struct scan_control *sc)\n \t\t * to global LRU.\n \t\t */\n \t\tif (!cgroup_reclaim(sc)) {\n-\t\t\tif (!cpuset_zone_allowed(zone,\n+\t\t\tif (!zone_reclaim_allowed(zone,\n \t\t\t\t\t\t GFP_KERNEL | __GFP_HARDWALL))\n \t\t\t\tcontinue;\n \n@@ -6992,6 +6999,7 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \tunsigned long zone_boosts[MAX_NR_ZONES] = { 0, };\n \tbool boosted;\n \tstruct zone *zone;\n+\tstruct node_reclaim_policy policy;\n \tstruct scan_control sc = {\n \t\t.gfp_mask = GFP_KERNEL,\n \t\t.order = order,\n@@ -7016,6 +7024,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t}\n \tboosted = nr_boost_reclaim;\n \n+\t/* Query/cache private node reclaim policy once per balance() */\n+\tnode_private_reclaim_policy(pgdat->node_id, &policy);\n+\n restart:\n \tset_reclaim_active(pgdat, highest_zoneidx);\n \tsc.priority = DEF_PRIORITY;\n@@ -7083,6 +7094,12 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\tsc.may_writepage = !laptop_mode && !nr_boost_reclaim;\n \t\tsc.may_swap = !nr_boost_reclaim;\n \n+\t\t/* Private nodes may enable swap/writepage when using boost */\n+\t\tif (policy.active) {\n+\t\t\tsc.may_swap |= policy.may_swap;\n+\t\t\tsc.may_writepage |= policy.may_writepage;\n+\t\t}\n+\n \t\t/*\n \t\t * Do some background aging, to give pages a chance to be\n \t\t * referenced before reclaiming. All pages are rotated\n@@ -7176,6 +7193,10 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\t\tif (!zone_boosts[i])\n \t\t\t\tcontinue;\n \n+\t\t\t/* Some private nodes may own the\\ boost lifecycle */\n+\t\t\tif (policy.managed_watermarks)\n+\t\t\t\tcontinue;\n+\n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n \t\t\tspin_lock_irqsave(&zone->lock, flags);\n@@ -7406,7 +7427,7 @@ void wakeup_kswapd(struct zone *zone, gfp_t gfp_flags, int order,\n \tif (!managed_zone(zone))\n \t\treturn;\n \n-\tif (!cpuset_zone_allowed(zone, gfp_flags))\n+\tif (!zone_reclaim_allowed(zone, gfp_flags))\n \t\treturn;\n \n \tpgdat = zone->zone_pgdat;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-17-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern that the OOM killer may select an undeserving victim if it doesn't know whether killing a task can actually free memory on private nodes. The author introduced NP_OPS_OOM_ELIGIBLE and helpers node_oom_eligible() and zone_oom_eligible() to check if a private node is reclaim-eligible, and updated oom_cpuset_eligible() to use this new check.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The OOM killer must know whether killing a task can actually free\nmemory such that pressure is reduced.\n\nA private node only contributes to relieving pressure if it participates\nin both reclaim and demotion. Without this check, the check, the OOM\nkiller may select an undeserving victim.\n\nIntroduce NP_OPS_OOM_ELIGIBLE and helpers node_oom_eligible() and\nzone_oom_eligible().\n\nReplace cpuset_mems_allowed_intersects() in oom_cpuset_eligible()\nwith oom_mems_intersect() that iterates N_MEMORY nodes and skips\nineligible private nodes.\n\nUpdate constrained_alloc() to use zone_oom_eligible() for constraint\ndetection and node_oom_eligible() to exclude ineligible nodes from\ntotalpages accounting.\n\nRemove cpuset_mems_allowed_intersects() as it has no remaining callers.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cpuset.h       |  9 -------\n include/linux/node_private.h |  3 +++\n kernel/cgroup/cpuset.c       | 17 ------------\n mm/oom_kill.c                | 52 ++++++++++++++++++++++++++++++++----\n 4 files changed, 50 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/cpuset.h b/include/linux/cpuset.h\nindex 7b2f3f6b68a9..53ccfb00b277 100644\n--- a/include/linux/cpuset.h\n+++ b/include/linux/cpuset.h\n@@ -97,9 +97,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-extern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t  const struct task_struct *tsk2);\n-\n #ifdef CONFIG_CPUSETS_V1\n #define cpuset_memory_pressure_bump() \t\t\t\t\\\n \tdo {\t\t\t\t\t\t\t\\\n@@ -241,12 +238,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-static inline int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t\t const struct task_struct *tsk2)\n-{\n-\treturn 1;\n-}\n-\n static inline void cpuset_memory_pressure_bump(void) {}\n \n static inline void cpuset_task_status_allowed(struct seq_file *m,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34be52383255..34d862f09e24 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -141,6 +141,9 @@ struct node_private_ops {\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n+/* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n+#define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 1a597f0c7c6c..29789d544fd5 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -4530,23 +4530,6 @@ int cpuset_mem_spread_node(void)\n \treturn cpuset_spread_node(&current->cpuset_mem_spread_rotor);\n }\n \n-/**\n- * cpuset_mems_allowed_intersects - Does @tsk1's mems_allowed intersect @tsk2's?\n- * @tsk1: pointer to task_struct of some task.\n- * @tsk2: pointer to task_struct of some other task.\n- *\n- * Description: Return true if @tsk1's mems_allowed intersects the\n- * mems_allowed of @tsk2.  Used by the OOM killer to determine if\n- * one of the task's memory usage might impact the memory available\n- * to the other.\n- **/\n-\n-int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t   const struct task_struct *tsk2)\n-{\n-\treturn nodes_intersects(tsk1->mems_allowed, tsk2->mems_allowed);\n-}\n-\n /**\n  * cpuset_print_current_mems_allowed - prints current's cpuset and mems_allowed\n  *\ndiff --git a/mm/oom_kill.c b/mm/oom_kill.c\nindex 5eb11fbba704..cd0d65ccd1e8 100644\n--- a/mm/oom_kill.c\n+++ b/mm/oom_kill.c\n@@ -74,7 +74,45 @@ static inline bool is_memcg_oom(struct oom_control *oc)\n \treturn oc->memcg != NULL;\n }\n \n+/* Private nodes are only eligible if they support both reclaim and demotion */\n+static inline bool node_oom_eligible(int nid)\n+{\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn (node_private_flags(nid) & NP_OPS_OOM_ELIGIBLE) ==\n+\t\tNP_OPS_OOM_ELIGIBLE;\n+}\n+\n+static inline bool zone_oom_eligible(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (!node_oom_eligible(zone_to_nid(zone)))\n+\t\treturn false;\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n #ifdef CONFIG_NUMA\n+/*\n+ * Killing a task can only relieve system pressure if freed memory can be\n+ * demoted there and reclaim can operate on the node's pages, so we\n+ * omit private nodes that aren't eligible.\n+ */\n+static bool oom_mems_intersect(const struct task_struct *tsk1,\n+\t\t\t       const struct task_struct *tsk2)\n+{\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (!node_isset(nid, tsk1->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_isset(nid, tsk2->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_oom_eligible(nid))\n+\t\t\tcontinue;\n+\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n /**\n  * oom_cpuset_eligible() - check task eligibility for kill\n  * @start: task struct of which task to consider\n@@ -107,9 +145,10 @@ static bool oom_cpuset_eligible(struct task_struct *start,\n \t\t} else {\n \t\t\t/*\n \t\t\t * This is not a mempolicy constrained oom, so only\n-\t\t\t * check the mems of tsk's cpuset.\n+\t\t\t * check the mems of tsk's cpuset, excluding private\n+\t\t\t * nodes that do not participate in kernel reclaim.\n \t\t\t */\n-\t\t\tret = cpuset_mems_allowed_intersects(current, tsk);\n+\t\t\tret = oom_mems_intersect(current, tsk);\n \t\t}\n \t\tif (ret)\n \t\t\tbreak;\n@@ -291,16 +330,19 @@ static enum oom_constraint constrained_alloc(struct oom_control *oc)\n \t\treturn CONSTRAINT_MEMORY_POLICY;\n \t}\n \n-\t/* Check this allocation failure is caused by cpuset's wall function */\n+\t/* Check this allocation failure is caused by cpuset or private node constraints */\n \tfor_each_zone_zonelist_nodemask(zone, z, oc->zonelist,\n \t\t\thighest_zoneidx, oc->nodemask)\n-\t\tif (!cpuset_zone_allowed(zone, oc->gfp_mask))\n+\t\tif (!zone_oom_eligible(zone, oc->gfp_mask))\n \t\t\tcpuset_limited = true;\n \n \tif (cpuset_limited) {\n \t\toc->totalpages = total_swap_pages;\n-\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed)\n+\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed) {\n+\t\t\tif (!node_oom_eligible(nid))\n+\t\t\t\tcontinue;\n \t\t\toc->totalpages += node_present_pages(nid);\n+\t\t}\n \t\treturn CONSTRAINT_CPUSET;\n \t}\n \treturn CONSTRAINT_NONE;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-18-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about NUMA balancing faults on private nodes, agreeing that not all private nodes may wish to engage in such faults and introducing an opt-in method (NP_OPS_NUMA_BALANCING) as well as a helper function (folio_managed_allows_numa()) to filter for private nodes. The author also added code to enforce write-protection on private-node folios with NP_OPS_PROTECT_WRITE that are still on their node after a failed or skipped migration.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Not all private nodes may wish to engage in NUMA balancing faults.\n\nAdd the NP_OPS_NUMA_BALANCING flag (BIT(5)) as an opt-in method.\n\nIntroduce folio_managed_allows_numa() helper:\n   ZONE_DEVICE folios always return false (never NUMA-scanned)\n   NP_OPS_NUMA_BALANCING filters for private nodes\n\nIn do_numa_page(), if a private-node folio with NP_OPS_PROTECT_WRITE\nis still on its node after a failed/skipped migration, enforce\nwrite-protection so the next write triggers handle_fault.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h | 16 ++++++++++++++++\n mm/memory.c                  | 11 +++++++++++\n mm/mempolicy.c               |  5 ++++-\n 4 files changed, 35 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex a4955b9b5b93..88aaac45e814 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -961,6 +961,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (ops->flags & NP_OPS_PROTECT_WRITE))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_NUMA_BALANCING) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34d862f09e24..5ac60db1f044 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -140,6 +140,8 @@ struct node_private_ops {\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n+/* Allow NUMA balancing to scan and migrate folios on this node */\n+#define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n@@ -263,6 +265,15 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n }\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\tif (!folio_is_private_managed(folio))\n+\t\treturn true;\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\treturn folio_private_flags(folio, NP_OPS_NUMA_BALANCING);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \tif (folio_is_zone_device(folio))\n@@ -443,6 +454,11 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\treturn !folio_is_zone_device(folio);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \treturn -ENOENT;\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 0f78988befef..88a581baae40 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -78,6 +78,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/pgalloc.h>\n #include <linux/uaccess.h>\n+#include <linux/node_private.h>\n \n #include <trace/events/kmem.h>\n \n@@ -6041,6 +6042,12 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \tif (!folio || folio_is_zone_device(folio))\n \t\tgoto out_map;\n \n+\t/*\n+\t * We do not need to check private-node folios here because the private\n+\t * memory service either never opted in to NUMA balancing, or it did\n+\t * and we need to restore private PTE controls on the failure path.\n+\t */\n+\n \tnid = folio_nid(folio);\n \tnr_pages = folio_nr_pages(folio);\n \n@@ -6078,6 +6085,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t/*\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n+\t *\n+\t * If the folio is still on a private node with NP_OPS_PROTECT_WRITE,\n+\t * enforce write-protection so the next write triggers handle_fault.\n+\t * This covers migration-failed and migration-skipped paths.\n \t */\n \tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n \t\twritable = false;\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 8ac014950e88..8a3a9916ab59 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -861,7 +861,10 @@ bool folio_can_map_prot_numa(struct folio *folio, struct vm_area_struct *vma,\n {\n \tint nid;\n \n-\tif (!folio || folio_is_zone_device(folio) || folio_test_ksm(folio))\n+\tif (!folio || folio_test_ksm(folio))\n+\t\treturn false;\n+\n+\tif (unlikely(!folio_managed_allows_numa(folio)))\n \t\treturn false;\n \n \t/* Also skip shared copy-on-write folios */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-19-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about compaction on private nodes, agreeing that it should not be allowed unless the service explicitly opts in and adding checks to prevent direct compaction on these zones. The author also added a folio_migrate callback to update PFN-based metadata during migration.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "added new code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node zones should not be compacted unless the service explicitly\nopts in - as compaction requires migration and services may have\nPFN-based metadata that needs updating.\n\nAdd a folio_migrate callback which fires from migrate_folio_move() for\neach relocated folio before faults are unblocked.\n\nAdd zone_supports_compaction() which returns true for normal zones and\nchecks NP_OPS_COMPACTION for N_MEMORY_PRIVATE zones.\n\nFilter three direct compaction zone loops:\n  - compaction_zonelist_suitable() (reclaimer eligibility)\n  - try_to_compact_pages()         (direct compaction)\n  - compact_node()                 (proactive/manual compaction)\n\nkcompactd paths are intentionally unfiltered -- the service is\nresponsible for starting kcompactd on its node.\n\nNP_OPS_COMPACTION requires NP_OPS_MIGRATION.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h |  2 ++\n mm/compaction.c              | 26 ++++++++++++++++++++++++++\n 3 files changed, 32 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 88aaac45e814..da523aca18fa 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -965,6 +965,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_COMPACTION) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 5ac60db1f044..fe0336773ddb 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -142,6 +142,8 @@ struct node_private_ops {\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n /* Allow NUMA balancing to scan and migrate folios on this node */\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n+/* Allow compaction to run on the node.  Service must start kcompactd. */\n+#define NP_OPS_COMPACTION\t\tBIT(6)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 6a65145b03d8..d8532b957ec6 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,9 +24,26 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/node_private.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n+\n+/*\n+ * Private node zones require NP_OPS_COMPACTION to opt in.  Normal zones\n+ * always support compaction.\n+ */\n+static inline bool zone_supports_compaction(struct zone *zone)\n+{\n+#ifdef CONFIG_NUMA\n+\tif (!node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn zone_private_flags(zone, NP_OPS_COMPACTION);\n+#else\n+\treturn true;\n+#endif\n+}\n+\n /*\n  * Fragmentation score check interval for proactive compaction purposes.\n  */\n@@ -2443,6 +2460,9 @@ bool compaction_zonelist_suitable(struct alloc_context *ac, int order,\n \t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tunsigned long available;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\t/*\n \t\t * Do not consider all the reclaimable memory because we do not\n \t\t * want to trash just for a single high order allocation which\n@@ -2832,6 +2852,9 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\n \t\t\trc = max_t(enum compact_result, COMPACT_DEFERRED, rc);\n@@ -2906,6 +2929,9 @@ static int compact_node(pg_data_t *pgdat, bool proactive)\n \t\tif (!populated_zone(zone))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (fatal_signal_pending(current))\n \t\t\treturn -EINTR;\n \n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-20-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private node folios being longterm-pinnable by default, explaining that this would freeze the service's control over the memory for the duration of the pin. They added a flag NP_OPS_LONGTERM_PIN to allow services to opt-in and modified the folio_is_longterm_pinnable() function in mm.h to check for this flag.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "modification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not be longterm-pinnable by default.\nA pinned folio is frozen in place, no migration, compaction, or\nreclaim, so the service loses control for the duration of the pin.\n\nSome services may depend on hot-unplugability and must disallow\nlongterm pinning.  Others (accelerators with shared CPU-device state)\nneed pinning to work.\n\nAdd NP_OPS_LONGTERM_PIN flag for services to opt in with. Hook into\nfolio_is_longterm_pinnable() in mm.h, which all GUP callers\nout-of-line helper, node_private_allows_longterm_pin(),  called\nonly for N_MEMORY_PRIVATE nodes.\n\nWithout the flag: folio_is_longterm_pinnable() returns false, migration\nfails (no __GFP_PRIVATE in GFP mask) and pin_user_pages(FOLL_LONGTERM)\nreturns -ENOMEM.\n\nWith the flag: pin succeeds and the folio stays on the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 15 +++++++++++++++\n include/linux/mm.h           | 22 ++++++++++++++++++++++\n include/linux/node_private.h |  2 ++\n 3 files changed, 39 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex da523aca18fa..5d2487fd54f4 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -866,6 +866,21 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n static DEFINE_MUTEX(node_private_lock);\n static bool node_private_initialized;\n \n+/**\n+ * node_private_allows_longterm_pin - Check if a private node allows longterm pinning\n+ * @nid: Node identifier\n+ *\n+ * Out-of-line helper for folio_is_longterm_pinnable() since mm.h cannot\n+ * include node_private.h (circular dependency).\n+ *\n+ * Returns true if the node has NP_OPS_LONGTERM_PIN set.\n+ */\n+bool node_private_allows_longterm_pin(int nid)\n+{\n+\treturn node_private_has_flag(nid, NP_OPS_LONGTERM_PIN);\n+}\n+EXPORT_SYMBOL_GPL(node_private_allows_longterm_pin);\n+\n /**\n  * node_private_register - Register a private node\n  * @nid: Node identifier\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex fb1819ad42c3..9088fd08aeb9 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -2192,6 +2192,13 @@ static inline bool is_zero_folio(const struct folio *folio)\n \n /* MIGRATE_CMA and ZONE_MOVABLE do not allow pin folios */\n #ifdef CONFIG_MIGRATION\n+\n+#ifdef CONFIG_NUMA\n+bool node_private_allows_longterm_pin(int nid);\n+#else\n+static inline bool node_private_allows_longterm_pin(int nid) { return false; }\n+#endif\n+\n static inline bool folio_is_longterm_pinnable(struct folio *folio)\n {\n #ifdef CONFIG_CMA\n@@ -2215,6 +2222,21 @@ static inline bool folio_is_longterm_pinnable(struct folio *folio)\n \tif (folio_is_fsdax(folio))\n \t\treturn false;\n \n+\t/*\n+\t * Private node folios are not longterm pinnable by default.\n+\t * Services that support pinning opt in via NP_OPS_LONGTERM_PIN.\n+\t * node_private_allows_longterm_pin() is out-of-line because\n+\t * node_private.h includes mm.h (circular dependency).\n+\t *\n+\t * Guarded by CONFIG_NUMA because on !CONFIG_NUMA the single-node\n+\t * node_state() stub returns true for node 0, which would make\n+\t * all folios non-pinnable via the false-returning stub.\n+\t */\n+#ifdef CONFIG_NUMA\n+\tif (node_state(folio_nid(folio), N_MEMORY_PRIVATE))\n+\t\treturn node_private_allows_longterm_pin(folio_nid(folio));\n+#endif\n+\n \t/* Otherwise, non-movable zone folios can be pinned. */\n \treturn !folio_is_zone_movable(folio);\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex fe0336773ddb..7a7438fb9eda 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -144,6 +144,8 @@ struct node_private_ops {\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n /* Allow compaction to run on the node.  Service must start kcompactd. */\n #define NP_OPS_COMPACTION\t\tBIT(6)\n+/* Allow longterm DMA pinning (RDMA, VFIO, etc.) of folios on this node */\n+#define NP_OPS_LONGTERM_PIN\t\tBIT(7)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-21-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about hardware errors on private nodes by adding a memory_failure notification callback to struct node_private_ops, allowing services managing N_MEMORY_PRIVATE nodes to clean up after such events.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a void memory_failure notification callback to struct\nnode_private_ops so services managing N_MEMORY_PRIVATE nodes notified\nwhen a page on their node experiences a hardware error.\n\nThe callback is notification only -- the kernel always proceeds with\nstandard hwpoison handling for online pages.\n\nThe notification hook fires after TestSetPageHWPoison succeeds and\nbefore get_hwpoison_page giving the service a chance to clean up.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 16 ++++++++++++++++\n mm/memory-failure.c          | 15 +++++++++++++++\n 3 files changed, 37 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7a7438fb9eda..d2669f68ac20 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -113,6 +113,10 @@ struct node_reclaim_policy {\n  *   watermark_boost lifecycle (kswapd will not clear it).\n  *   If NULL, normal boost policy applies.\n  *\n+ * @memory_failure: Notification of hardware error on a page on this node.\n+ *   [folio-referenced callback]\n+ *   Notification only, kernel always handles the failure.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -127,6 +131,8 @@ struct node_private_ops {\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n \tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n+\tvoid (*memory_failure)(struct folio *folio, unsigned long pfn,\n+\t\t\t       int mf_flags);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex db32cb2d7a29..64467ca774f1 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1608,6 +1608,22 @@ static inline void node_private_reclaim_policy(int nid,\n }\n #endif\n \n+static inline void folio_managed_memory_failure(struct folio *folio,\n+\t\t\t\t\t\tunsigned long pfn,\n+\t\t\t\t\t\tint mf_flags)\n+{\n+\t/* Zone device pages handle memory failure via dev_pagemap_ops */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn;\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->memory_failure)\n+\t\t\tops->memory_failure(folio, pfn, mf_flags);\n+\t}\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/memory-failure.c b/mm/memory-failure.c\nindex c80c2907da33..79c91d44ec1e 100644\n--- a/mm/memory-failure.c\n+++ b/mm/memory-failure.c\n@@ -2379,6 +2379,15 @@ int memory_failure(unsigned long pfn, int flags)\n \t\tgoto unlock_mutex;\n \t}\n \n+\t/*\n+\t * Notify private-node services about the hardware error so they\n+\t * can update internal tracking (e.g., CXL poison lists, stop\n+\t * demoting to failing DIMMs).  This is notification only -- the\n+\t * kernel proceeds with standard hwpoison handling regardless.\n+\t */\n+\tif (unlikely(page_is_private_managed(p)))\n+\t\tfolio_managed_memory_failure(page_folio(p), pfn, flags);\n+\n \t/*\n \t * We need/can do nothing about count=0 pages.\n \t * 1) it's a free page, and therefore in safe hand:\n@@ -2825,6 +2834,12 @@ static int soft_offline_in_use_page(struct page *page)\n \t\treturn 0;\n \t}\n \n+\tif (!folio_managed_allows_migrate(folio)) {\n+\t\tpr_info(\"%#lx: cannot migrate private node folio\\n\", pfn);\n+\t\tfolio_put(folio);\n+\t\treturn -EBUSY;\n+\t}\n+\n \tisolated = isolate_folio_to_list(folio, &pagelist);\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-22-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the ordering of operations in hotplugging memory as N_MEMORY_PRIVATE, explaining that they added a new function to ensure proper ordering: registering the private region first and then hotplugging the memory. They also mentioned that on failure, the private region is unregistered to avoid leaving the node in an inconsistent state.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a new function for drivers to hotplug memory as N_MEMORY_PRIVATE.\n\nThis function combines node_private_region_register() with\n__add_memory_driver_managed() to ensure proper ordering:\n\n1. Register the private region first (sets private node context)\n2. Then hotplug the memory (sets N_MEMORY_PRIVATE)\n3. On failure, unregister the private region to avoid leaving the\n   node in an inconsistent state.\n\nWhen the last of memory is removed, hotplug also removes the private\nnode context. If migration is not supported and the node is still\nonline, fire a warning (likely bug in the driver).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory_hotplug.h |  11 +++\n include/linux/mmzone.h         |  12 ++++\n mm/memory_hotplug.c            | 122 ++++++++++++++++++++++++++++++---\n 3 files changed, 135 insertions(+), 10 deletions(-)\n\ndiff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h\nindex 1f19f08552ea..e5abade9450a 100644\n--- a/include/linux/memory_hotplug.h\n+++ b/include/linux/memory_hotplug.h\n@@ -293,6 +293,7 @@ extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n extern int remove_memory(u64 start, u64 size);\n extern void __remove_memory(u64 start, u64 size);\n extern int offline_and_remove_memory(u64 start, u64 size);\n+extern int offline_and_remove_private_memory(int nid, u64 start, u64 size);\n \n #else\n static inline void try_offline_node(int nid) {}\n@@ -309,6 +310,12 @@ static inline int remove_memory(u64 start, u64 size)\n }\n \n static inline void __remove_memory(u64 start, u64 size) {}\n+\n+static inline int offline_and_remove_private_memory(int nid, u64 start,\n+\t\t\t\t\t\t    u64 size)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n@@ -326,6 +333,10 @@ int __add_memory_driver_managed(int nid, u64 start, u64 size,\n extern int add_memory_driver_managed(int nid, u64 start, u64 size,\n \t\t\t\t     const char *resource_name,\n \t\t\t\t     mhp_t mhp_flags);\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np);\n extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,\n \t\t\t\t   unsigned long nr_pages,\n \t\t\t\t   struct vmem_altmap *altmap, int migratetype,\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 992eb1c5a2c6..cc532b67ad3f 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1524,6 +1524,18 @@ typedef struct pglist_data {\n #endif\n } pg_data_t;\n \n+#ifdef CONFIG_NUMA\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn pgdat->private;\n+}\n+#else\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn false;\n+}\n+#endif\n+\n #define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n #define node_spanned_pages(nid)\t(NODE_DATA(nid)->node_spanned_pages)\n \ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex d2dc527bd5b0..9d72f44a30dc 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1173,8 +1174,7 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tmove_pfn_range_to_zone(zone, pfn, nr_pages, NULL, MIGRATE_MOVABLE,\n \t\t\t       true);\n \n-\tif (!node_state(nid, N_MEMORY)) {\n-\t\t/* Adding memory to the node for the first time */\n+\tif (!node_state(nid, N_MEMORY) && !node_state(nid, N_MEMORY_PRIVATE)) {\n \t\tnode_arg.nid = nid;\n \t\tret = node_notify(NODE_ADDING_FIRST_MEMORY, &node_arg);\n \t\tret = notifier_to_errno(ret);\n@@ -1208,8 +1208,12 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tonline_pages_range(pfn, nr_pages);\n \tadjust_present_page_count(pfn_to_page(pfn), group, nr_pages);\n \n-\tif (node_arg.nid >= 0)\n-\t\tnode_set_state(nid, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (pgdat_is_private(NODE_DATA(nid)))\n+\t\t\tnode_set_state(nid, N_MEMORY_PRIVATE);\n+\t\telse\n+\t\t\tnode_set_state(nid, N_MEMORY);\n+\t}\n \tif (need_zonelists_rebuild)\n \t\tbuild_all_zonelists(NULL);\n \n@@ -1227,8 +1231,14 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t/* reinitialise watermarks and update pcp limits */\n \tinit_per_zone_wmark_min();\n \n-\tkswapd_run(nid);\n-\tkcompactd_run(nid);\n+\t/*\n+\t * Don't start reclaim/compaction daemons for private nodes.\n+\t * Private node services will decide whether to start these services.\n+\t */\n+\tif (!pgdat_is_private(NODE_DATA(nid))) {\n+\t\tkswapd_run(nid);\n+\t\tkcompactd_run(nid);\n+\t}\n \n \tif (node_arg.nid >= 0)\n \t\t/* First memory added successfully. Notify consumers. */\n@@ -1722,6 +1732,54 @@ int add_memory_driver_managed(int nid, u64 start, u64 size,\n }\n EXPORT_SYMBOL_GPL(add_memory_driver_managed);\n \n+/**\n+ * add_private_memory_driver_managed - add driver-managed N_MEMORY_PRIVATE memory\n+ * @nid: NUMA node ID (or memory group ID when MHP_NID_IS_MGID is set)\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ * @resource_name: \"System RAM ($DRIVER)\" format\n+ * @mhp_flags: Memory hotplug flags\n+ * @online_type: MMOP_* online type\n+ * @np: Driver-owned node_private structure (owner, refcount)\n+ *\n+ * Registers node_private first, then hotplugs the memory.\n+ *\n+ * On failure, unregisters the node_private.\n+ */\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np)\n+{\n+\tstruct memory_group *group;\n+\tint real_nid = nid;\n+\tint rc;\n+\n+\tif (!np)\n+\t\treturn -EINVAL;\n+\n+\tif (mhp_flags & MHP_NID_IS_MGID) {\n+\t\tgroup = memory_group_find_by_id(nid);\n+\t\tif (!group)\n+\t\t\treturn -EINVAL;\n+\t\treal_nid = group->nid;\n+\t}\n+\n+\trc = node_private_register(real_nid, np);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\trc = __add_memory_driver_managed(nid, start, size, resource_name,\n+\t\t\t\t\t mhp_flags, online_type);\n+\tif (rc) {\n+\t\tnode_private_unregister(real_nid);\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(add_private_memory_driver_managed);\n+\n /*\n  * Platforms should define arch_get_mappable_range() that provides\n  * maximum possible addressable physical memory range for which the\n@@ -1872,6 +1930,15 @@ static void do_migrate_range(unsigned long start_pfn, unsigned long end_pfn)\n \t\t\tgoto put_folio;\n \t\t}\n \n+\t\t/* Private nodes w/o migration must ensure folios are offline */\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION)) {\n+\t\t\tWARN_ONCE(1, \"hot-unplug on non-migratable node %d pfn %lx\\n\",\n+\t\t\t\t  folio_nid(folio), pfn);\n+\t\t\tpfn = folio_pfn(folio) + folio_nr_pages(folio) - 1;\n+\t\t\tgoto put_folio;\n+\t\t}\n+\n \t\tif (!isolate_folio_to_list(folio, &source)) {\n \t\t\tif (__ratelimit(&migrate_rs)) {\n \t\t\t\tpr_warn(\"failed to isolate pfn %lx\\n\",\n@@ -2014,8 +2081,8 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \n \t/*\n \t * Check whether the node will have no present pages after we offline\n-\t * 'nr_pages' more. If so, we know that the node will become empty, and\n-\t * so we will clear N_MEMORY for it.\n+\t * 'nr_pages' more. If so, send pre-notification for last memory removal.\n+\t * We will clear N_MEMORY(_PRIVATE) if this is the case.\n \t */\n \tif (nr_pages >= pgdat->node_present_pages) {\n \t\tnode_arg.nid = node;\n@@ -2108,8 +2175,12 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * Make sure to mark the node as memory-less before rebuilding the zone\n \t * list. Otherwise this node would still appear in the fallback lists.\n \t */\n-\tif (node_arg.nid >= 0)\n-\t\tnode_clear_state(node, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (node_state(node, N_MEMORY))\n+\t\t\tnode_clear_state(node, N_MEMORY);\n+\t\telse if (node_state(node, N_MEMORY_PRIVATE))\n+\t\t\tnode_clear_state(node, N_MEMORY_PRIVATE);\n+\t}\n \tif (!populated_zone(zone)) {\n \t\tzone_pcp_reset(zone);\n \t\tbuild_all_zonelists(NULL);\n@@ -2461,4 +2532,35 @@ int offline_and_remove_memory(u64 start, u64 size)\n \treturn rc;\n }\n EXPORT_SYMBOL_GPL(offline_and_remove_memory);\n+\n+/**\n+ * offline_and_remove_private_memory - offline, remove, and unregister private memory\n+ * @nid: NUMA node ID of the private memory\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ *\n+ * Counterpart to add_private_memory_driver_managed().  Offlines and removes\n+ * the memory range, then attempts to unregister the node_private.\n+ *\n+ * offline_and_remove_memory() clears N_MEMORY_PRIVATE when the last block\n+ * is offlined, which allows node_private_unregister() to clear the\n+ * pgdat->node_private pointer.  If other private memory ranges remain on\n+ * the node, node_private_unregister() returns -EBUSY (N_MEMORY_PRIVATE\n+ * is still set) and the node_private remains registered.\n+ *\n+ * Return: 0 on full success (memory removed and node_private unregistered),\n+ *         -EBUSY if memory was removed but node still has other private memory,\n+ *         other negative error code if offline/remove failed.\n+ */\n+int offline_and_remove_private_memory(int nid, u64 start, u64 size)\n+{\n+\tint rc;\n+\n+\trc = offline_and_remove_memory(start, size);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\treturn node_private_unregister(nid);\n+}\n+EXPORT_SYMBOL_GPL(offline_and_remove_private_memory);\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-23-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the interaction between CRAM and the standard kernel LRU, explaining that CRAM limits entry by demotion to allow devices to close access under memory pressure, and utilizes write-protect to prevent unbounded writes. The author confirmed that this approach is correct.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a valid concern",
                "provided a clear explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CRAM (Compressed RAM) subsystem that manages folios demoted\nto N_MEMORY_PRIVATE nodes via the standard kernel LRU.\n\nWe limit entry into CRAM by demotion in to provide devices a way for\ndrivers to close access - which allows the system to stabiliz under\nmemory pressure (the device can run out of real memory when compression\nratios drop too far).\n\nWe utilize write-protect to prevent unbounded writes to compressed\nmemory pages, which may cause run-away compression ratio loss without\na reliable way to prevent the degenerate case (cascading poisons).\n\nCRAM provides the bridge between the mm/ private node infrastructure\nand compressed memory hardware.  Folios are aged by kswapd on the\nprivate node and reclaimed to swap when the device signals pressure.\n\nWrite faults trigger promotion back to regular DRAM via the\nops->handle_fault callback.\n\nDevice pressure is communicated via watermark_boost on the private\nnode's zone.\n\nCRAM registers node_private_ops with:\n  - handle_fault:   promotes folio back to DRAM on write\n  - migrate_to:     custom demotion to the CRAM node\n  - folio_migrate:  (no-op)\n  - free_folio:     zeroes pages on free to scrub stale data\n  - reclaim_policy: provides mayswap/writeback/boost overrides\n  - flags: NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n\t   NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE\n           NP_OPS_RECLAIM\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cram.h |  66 ++++++\n mm/Kconfig           |  10 +\n mm/Makefile          |   1 +\n mm/cram.c            | 508 +++++++++++++++++++++++++++++++++++++++++++\n 4 files changed, 585 insertions(+)\n create mode 100644 include/linux/cram.h\n create mode 100644 mm/cram.c\n\ndiff --git a/include/linux/cram.h b/include/linux/cram.h\nnew file mode 100644\nindex 000000000000..a3c10362fd4f\n--- /dev/null\n+++ b/include/linux/cram.h\n@@ -0,0 +1,66 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_CRAM_H\n+#define _LINUX_CRAM_H\n+\n+#include <linux/mm_types.h>\n+\n+struct folio;\n+struct list_head;\n+struct vm_fault;\n+\n+#define CRAM_PRESSURE_MAX\t1000\n+\n+/**\n+ * cram_flush_cb_t - Driver callback invoked when a folio on a private node\n+ *                   is freed (refcount reaches zero).\n+ * @folio: the folio being freed\n+ * @private: opaque driver data passed at registration\n+ *\n+ * Return:\n+ *   0: Flush resolved -- page should return to buddy allocator (e.g., flush\n+ *      record bit was set, meaning this free is from our own flush resolution)\n+ *   1: Page deferred -- driver took a reference, page will be flushed later.\n+ *      Do NOT return to buddy allocator.\n+ *   2: Buffer full -- caller should zero the page and return to buddy.\n+ */\n+typedef int (*cram_flush_cb_t)(struct folio *folio, void *private);\n+\n+#ifdef CONFIG_CRAM\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data);\n+int cram_unregister_private_node(int nid);\n+int cram_unpurge(int nid);\n+void cram_set_pressure(int nid, unsigned int pressure);\n+void cram_clear_pressure(int nid);\n+\n+#else /* !CONFIG_CRAM */\n+\n+static inline int cram_register_private_node(int nid, void *owner,\n+\t\t\t\t\t     cram_flush_cb_t flush_cb,\n+\t\t\t\t\t     void *flush_data)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unregister_private_node(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unpurge(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+}\n+\n+static inline void cram_clear_pressure(int nid)\n+{\n+}\n+\n+#endif /* CONFIG_CRAM */\n+\n+#endif /* _LINUX_CRAM_H */\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex bd0ea5454af8..054462b954d8 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -662,6 +662,16 @@ config MIGRATION\n config DEVICE_MIGRATION\n \tdef_bool MIGRATION && ZONE_DEVICE\n \n+config CRAM\n+\tbool \"Compressed RAM - private node memory management\"\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\tdepends on MEMORY_HOTPLUG\n+\thelp\n+\t  Enables management of N_MEMORY_PRIVATE nodes for compressed RAM\n+\t  and similar use cases. Provides demotion, promotion, and lifecycle\n+\t  management for private memory nodes.\n+\n config ARCH_ENABLE_HUGEPAGE_MIGRATION\n \tbool\n \ndiff --git a/mm/Makefile b/mm/Makefile\nindex 2d0570a16e5b..0e1421512643 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -98,6 +98,7 @@ obj-$(CONFIG_MEMTEST)\t\t+= memtest.o\n obj-$(CONFIG_MIGRATION) += migrate.o\n obj-$(CONFIG_NUMA) += memory-tiers.o\n obj-$(CONFIG_DEVICE_MIGRATION) += migrate_device.o\n+obj-$(CONFIG_CRAM) += cram.o\n obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o\n obj-$(CONFIG_PAGE_COUNTER) += page_counter.o\n obj-$(CONFIG_LIVEUPDATE) += memfd_luo.o\ndiff --git a/mm/cram.c b/mm/cram.c\nnew file mode 100644\nindex 000000000000..6709e61f5b9d\n--- /dev/null\n+++ b/mm/cram.c\n@@ -0,0 +1,508 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * mm/cram.c - Compressed RAM / private node memory management\n+ *\n+ * Copyright 2026 Meta Technologies Inc.\n+ *   Author: Gregory Price <gourry@gourry.net>\n+ *\n+ * Manages folios demoted to N_MEMORY_PRIVATE nodes via the standard kernel\n+ * LRU.  Folios are aged by kswapd on the private node and reclaimed to swap\n+ * (demotion is suppressed for private nodes).  Write faults trigger promotion\n+ * back to regular DRAM via the ops->handle_fault callback.\n+ *\n+ * All reclaim/demotion uses the standard vmscan infrastructure. Device pressure\n+ * is communicated via watermark_boost on the private node's zone.\n+ */\n+\n+#include <linux/atomic.h>\n+#include <linux/cpuset.h>\n+#include <linux/cram.h>\n+#include <linux/errno.h>\n+#include <linux/gfp.h>\n+#include <linux/jiffies.h>\n+#include <linux/highmem.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/list.h>\n+#include <linux/migrate.h>\n+#include <linux/mm.h>\n+#include <linux/huge_mm.h>\n+#include <linux/mmzone.h>\n+#include <linux/mutex.h>\n+#include <linux/nodemask.h>\n+#include <linux/node_private.h>\n+#include <linux/pagemap.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+#include <linux/swap.h>\n+\n+#include \"internal.h\"\n+\n+struct cram_node {\n+\tvoid\t\t*owner;\n+\tbool\t\tpurged;\t\t/* node is being torn down */\n+\tunsigned int\tpressure;\n+\trefcount_t\trefcount;\n+\tcram_flush_cb_t\tflush_cb;\t/* optional driver flush callback */\n+\tvoid\t\t*flush_data;\t/* opaque data for flush_cb */\n+};\n+\n+static struct cram_node *cram_nodes[MAX_NUMNODES];\n+static DEFINE_MUTEX(cram_mutex);\n+\n+static inline bool cram_valid_nid(int nid)\n+{\n+\treturn nid >= 0 && nid < MAX_NUMNODES;\n+}\n+\n+static inline struct cram_node *get_cram_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn NULL;\n+\n+\trcu_read_lock();\n+\tcn = rcu_dereference(cram_nodes[nid]);\n+\tif (cn && !refcount_inc_not_zero(&cn->refcount))\n+\t\tcn = NULL;\n+\trcu_read_unlock();\n+\n+\treturn cn;\n+}\n+\n+static inline void put_cram_node(struct cram_node *cn)\n+{\n+\tif (cn)\n+\t\trefcount_dec(&cn->refcount);\n+}\n+\n+static void cram_zero_folio(struct folio *folio)\n+{\n+\tunsigned int i, nr = folio_nr_pages(folio);\n+\n+\tif (want_init_on_free())\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr; i++)\n+\t\tclear_highpage(folio_page(folio, i));\n+}\n+\n+static bool cram_free_folio_cb(struct folio *folio)\n+{\n+\tint nid = folio_nid(folio);\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\tgoto zero_and_free;\n+\n+\tif (!cn->flush_cb)\n+\t\tgoto zero_and_free_put;\n+\n+\tret = cn->flush_cb(folio, cn->flush_data);\n+\tput_cram_node(cn);\n+\n+\tswitch (ret) {\n+\tcase 0:\n+\t\t/* Flush resolved: return to buddy (already zeroed by device) */\n+\t\treturn false;\n+\tcase 1:\n+\t\t/* Deferred: driver holds a ref, do not free to buddy */\n+\t\treturn true;\n+\tcase 2:\n+\tdefault:\n+\t\t/* Buffer full or unknown: zero locally, return to buddy */\n+\t\tgoto zero_and_free;\n+\t}\n+\n+zero_and_free_put:\n+\tput_cram_node(cn);\n+zero_and_free:\n+\tcram_zero_folio(folio);\n+\treturn false;\n+}\n+\n+static struct folio *alloc_cram_folio(struct folio *src, unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_PRIVATE | __GFP_KSWAPD_RECLAIM |\n+\t\t     __GFP_HIGHMEM | __GFP_MOVABLE |\n+\t\t     __GFP_NOWARN | __GFP_NORETRY;\n+\n+\t/* Stop allocating if backpressure fired mid-batch */\n+\tif (node_private_migration_blocked(nid))\n+\t\treturn NULL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc_node(gfp, order, nid);\n+}\n+\n+static void cram_put_new_folio(struct folio *folio, unsigned long private)\n+{\n+\tcram_zero_folio(folio);\n+\tfolio_put(folio);\n+}\n+\n+/*\n+ * Allocate a DRAM folio for promotion out of a private node.\n+ *\n+ * Unlike alloc_migration_target(), this does NOT strip __GFP_RECLAIM for\n+ * large folios, the generic helper does that because THP allocations are\n+ * opportunistic, but promotion from a private node is mandatory: the page\n+ * MUST move to DRAM or the process cannot make forward progress.\n+ *\n+ * __GFP_RETRY_MAYFAIL tells the allocator to try hard (multiple reclaim\n+ * rounds, wait for writeback) before giving up.\n+ */\n+static struct folio *alloc_cram_promote_folio(struct folio *src,\n+\t\t\t\t\t      unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc(gfp, order, nid, NULL);\n+}\n+\n+static int cram_migrate_to(struct list_head *demote_folios, int to_nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason,\n+\t\t\t   unsigned int *nr_succeeded)\n+{\n+\tstruct cram_node *cn;\n+\tunsigned int nr_success = 0;\n+\tint ret = 0;\n+\n+\tcn = get_cram_node(to_nid);\n+\tif (!cn)\n+\t\treturn -ENODEV;\n+\n+\tif (cn->purged) {\n+\t\tret = -ENODEV;\n+\t\tgoto out;\n+\t}\n+\n+\t/* Block new demotions at maximum pressure */\n+\tif (READ_ONCE(cn->pressure) >= CRAM_PRESSURE_MAX) {\n+\t\tret = -ENOSPC;\n+\t\tgoto out;\n+\t}\n+\n+\tret = migrate_pages(demote_folios, alloc_cram_folio, cram_put_new_folio,\n+\t\t\t    (unsigned long)to_nid, mode, reason,\n+\t\t\t    &nr_success);\n+\n+\t/*\n+\t * migrate_folio_move() calls folio_add_lru() for each migrated\n+\t * folio, but that only adds the folio to a per-CPU batch, \n+\t * PG_lru is not set until the batch is drained.  Drain now so\n+\t * that cram_fault() can isolate these folios immediately.\n+\t *\n+\t * Use lru_add_drain_all() because migrate_pages() may process\n+\t * folios across CPUs, and the local drain might miss batches\n+\t * filled on other CPUs.\n+\t */\n+\tif (nr_success)\n+\t\tlru_add_drain_all();\n+out:\n+\tput_cram_node(cn);\n+\tif (nr_succeeded)\n+\t\t*nr_succeeded = nr_success;\n+\treturn ret;\n+}\n+\n+static void cram_release_ptl(struct vm_fault *vmf, enum pgtable_level level)\n+{\n+\tif (level == PGTABLE_LEVEL_PTE)\n+\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n+\telse\n+\t\tspin_unlock(vmf->ptl);\n+}\n+\n+static vm_fault_t cram_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t     enum pgtable_level level)\n+{\n+\tstruct folio *f, *f2;\n+\tstruct cram_node *cn;\n+\tunsigned int nr_succeeded = 0;\n+\tint nid;\n+\tLIST_HEAD(folios);\n+\n+\tnid = folio_nid(folio);\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn) {\n+\t\tcram_release_ptl(vmf, level);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Isolate from LRU while holding PTL.  This serializes against\n+\t * other CPUs faulting on the same folio: only one CPU can clear\n+\t * PG_lru under the PTL, and it proceeds to migration.  Other\n+\t * CPUs find the folio already isolated and bail out, preventing\n+\t * the refcount pile-up that causes migrate_pages() to fail with\n+\t * -EAGAIN.\n+\t *\n+\t * No explicit folio_get() is needed: the page table entry holds\n+\t * a reference (we still hold PTL), and folio_isolate_lru() takes\n+\t * its own reference.  This matches do_numa_page()'s pattern.\n+\t *\n+\t * PG_lru should already be set: cram_migrate_to() drains per-CPU\n+\t * LRU batches after migration, and the failure path below\n+\t * drains after putback.\n+\t */\n+\tif (!folio_isolate_lru(folio)) {\n+\t\tput_cram_node(cn);\n+\t\tcram_release_ptl(vmf, level);\n+\t\tcond_resched();\n+\t\treturn 0;\n+\t}\n+\n+\t/* Folio isolated, release PTL, proceed to migration */\n+\tcram_release_ptl(vmf, level);\n+\n+\tnode_stat_mod_folio(folio,\n+\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(folio),\n+\t\t\t    folio_nr_pages(folio));\n+\tlist_add(&folio->lru, &folios);\n+\n+\tmigrate_pages(&folios, alloc_cram_promote_folio, NULL,\n+\t\t      (unsigned long)numa_node_id(),\n+\t\t      MIGRATE_SYNC, MR_NUMA_MISPLACED, &nr_succeeded);\n+\n+\t/* Put failed folios back on LRU; retry on next fault */\n+\tlist_for_each_entry_safe(f, f2, &folios, lru) {\n+\t\tlist_del(&f->lru);\n+\t\tnode_stat_mod_folio(f,\n+\t\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(f),\n+\t\t\t\t    -folio_nr_pages(f));\n+\t\tfolio_putback_lru(f);\n+\t}\n+\n+\t/*\n+\t * If migration failed, folio_putback_lru() batched the folio\n+\t * into this CPU's per-CPU LRU cache (PG_lru not yet set).\n+\t * Drain now so the folio is immediately visible on the LRU,\n+\t * the next fault can then isolate it without an IPI storm\n+\t * via lru_add_drain_all().\n+\t *\n+\t * Return VM_FAULT_RETRY after releasing the fault lock so the\n+\t * arch handler retries from scratch.  Without this, returning 0\n+\t * causes a tight livelock: the process immediately re-faults on\n+\t * the same write-protected entry, alloc fails again, and\n+\t * VM_FAULT_OOM eventually leaks out through a stale path.\n+\t * VM_FAULT_RETRY gives the system breathing room to reclaim.\n+\t */\n+\tif (!nr_succeeded) {\n+\t\tlru_add_drain();\n+\t\tcond_resched();\n+\t\tput_cram_node(cn);\n+\t\trelease_fault_lock(vmf);\n+\t\treturn VM_FAULT_RETRY;\n+\t}\n+\n+\tcond_resched();\n+\tput_cram_node(cn);\n+\treturn 0;\n+}\n+\n+static void cram_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static void cram_reclaim_policy(int nid, struct node_reclaim_policy *policy)\n+{\n+\tpolicy->may_swap = true;\n+\tpolicy->may_writepage = true;\n+\tpolicy->managed_watermarks = true;\n+}\n+\n+static vm_fault_t cram_handle_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t    enum pgtable_level level)\n+{\n+\treturn cram_fault(folio, vmf, level);\n+}\n+\n+static const struct node_private_ops cram_ops = {\n+\t.handle_fault\t\t= cram_handle_fault,\n+\t.migrate_to\t\t= cram_migrate_to,\n+\t.folio_migrate\t\t= cram_folio_migrate,\n+\t.free_folio\t\t= cram_free_folio_cb,\n+\t.reclaim_policy\t\t= cram_reclaim_policy,\n+\t.flags\t\t\t= NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n+\t\t\t\t  NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE |\n+\t\t\t\t  NP_OPS_RECLAIM,\n+};\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data)\n+{\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (cn) {\n+\t\tif (cn->owner != owner) {\n+\t\t\tmutex_unlock(&cram_mutex);\n+\t\t\treturn -EBUSY;\n+\t\t}\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn 0;\n+\t}\n+\n+\tcn = kzalloc(sizeof(*cn), GFP_KERNEL);\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENOMEM;\n+\t}\n+\n+\tcn->owner = owner;\n+\tcn->pressure = 0;\n+\tcn->flush_cb = flush_cb;\n+\tcn->flush_data = flush_data;\n+\trefcount_set(&cn->refcount, 1);\n+\n+\tret = node_private_set_ops(nid, &cram_ops);\n+\tif (ret) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\tkfree(cn);\n+\t\treturn ret;\n+\t}\n+\n+\trcu_assign_pointer(cram_nodes[nid], cn);\n+\n+\t/* Start kswapd on the private node for LRU aging and reclaim */\n+\tkswapd_run(nid);\n+\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* Now that ops->migrate_to is set, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_register_private_node);\n+\n+int cram_unregister_private_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tkswapd_stop(nid);\n+\n+\tWARN_ON(node_private_clear_ops(nid, &cram_ops));\n+\trcu_assign_pointer(cram_nodes[nid], NULL);\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* ops->migrate_to cleared, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\n+\tsynchronize_rcu();\n+\twhile (!refcount_dec_if_one(&cn->refcount))\n+\t\tcond_resched();\n+\tkfree(cn);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unregister_private_node);\n+\n+int cram_unpurge(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tcn->purged = false;\n+\n+\tmutex_unlock(&cram_mutex);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unpurge);\n+\n+void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+\tstruct cram_node *cn;\n+\tstruct node_private *np;\n+\tstruct zone *zone;\n+\tunsigned long managed, boost;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\treturn;\n+\n+\tif (pressure > CRAM_PRESSURE_MAX)\n+\t\tpressure = CRAM_PRESSURE_MAX;\n+\n+\tWRITE_ONCE(cn->pressure, pressure);\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\t/* Block demotions only at maximum pressure */\n+\tif (np)\n+\t\tWRITE_ONCE(np->migration_blocked,\n+\t\t\t   pressure >= CRAM_PRESSURE_MAX);\n+\trcu_read_unlock();\n+\n+\tzone = NULL;\n+\tfor (int i = 0; i < MAX_NR_ZONES; i++) {\n+\t\tstruct zone *z = &NODE_DATA(nid)->node_zones[i];\n+\n+\t\tif (zone_managed_pages(z) > 0) {\n+\t\t\tzone = z;\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+\tif (!zone) {\n+\t\tput_cram_node(cn);\n+\t\treturn;\n+\t}\n+\tmanaged = zone_managed_pages(zone);\n+\n+\t/* Boost proportional to pressure. 0:no boost, 1000:full managed */\n+\tboost = (managed * (unsigned long)pressure) / CRAM_PRESSURE_MAX;\n+\tWRITE_ONCE(zone->watermark_boost, boost);\n+\n+\tif (boost) {\n+\t\tset_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);\n+\t\twakeup_kswapd(zone, GFP_KERNEL, 0, ZONE_MOVABLE);\n+\t}\n+\n+\tput_cram_node(cn);\n+}\n+EXPORT_SYMBOL_GPL(cram_set_pressure);\n+\n+void cram_clear_pressure(int nid)\n+{\n+\tcram_set_pressure(nid, 0);\n+}\n+EXPORT_SYMBOL_GPL(cram_clear_pressure);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-24-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the need for a sysram region to directly perform memory hotplug operations, eliminating the intermediate dax_region/dax device layer. The author agrees that this feature is necessary and has implemented it in the new patch, which adds a sysram_regionN device as a child of the CXL region, managing the memory hotplug lifecycle through device add/remove.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreement",
                "implementation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CXL sysram region for direct memory hotplug of CXL RAM regions.\n\nThis region eliminates the intermediate dax_region/dax device layer by\ndirectly performing memory hotplug operations.\n\nKey features:\n- Supports memory tier integration for proper NUMA placement\n- Uses the CXL_SYSRAM_ONLINE_* Kconfig options for default online type\n- Automatically hotplugs memory on probe if online type is configured\n- Will be extended to support private memory nodes in the future\n\nThe driver registers a sysram_regionN device as a child of the CXL\nregion, managing the memory hotplug lifecycle through device add/remove.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/Makefile        |   1 +\n drivers/cxl/core/core.h          |   4 +\n drivers/cxl/core/port.c          |   2 +\n drivers/cxl/core/region_sysram.c | 351 +++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h                |  48 +++++\n 5 files changed, 406 insertions(+)\n create mode 100644 drivers/cxl/core/region_sysram.c\n\ndiff --git a/drivers/cxl/core/Makefile b/drivers/cxl/core/Makefile\nindex d3ec8aea64c5..d7ce52c50810 100644\n--- a/drivers/cxl/core/Makefile\n+++ b/drivers/cxl/core/Makefile\n@@ -18,6 +18,7 @@ cxl_core-$(CONFIG_TRACING) += trace.o\n cxl_core-$(CONFIG_CXL_REGION) += region.o\n cxl_core-$(CONFIG_CXL_REGION) += region_dax.o\n cxl_core-$(CONFIG_CXL_REGION) += region_pmem.o\n+cxl_core-$(CONFIG_CXL_REGION) += region_sysram.o\n cxl_core-$(CONFIG_CXL_MCE) += mce.o\n cxl_core-$(CONFIG_CXL_FEATURES) += features.o\n cxl_core-$(CONFIG_CXL_EDAC_MEM_FEATURES) += edac.o\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 6e1f695fd155..973bbcae43f7 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -35,6 +35,7 @@ extern struct device_attribute dev_attr_delete_region;\n extern struct device_attribute dev_attr_region;\n extern const struct device_type cxl_pmem_region_type;\n extern const struct device_type cxl_dax_region_type;\n+extern const struct device_type cxl_sysram_type;\n extern const struct device_type cxl_region_type;\n \n int cxl_decoder_detach(struct cxl_region *cxlr,\n@@ -46,6 +47,7 @@ int cxl_decoder_detach(struct cxl_region *cxlr,\n #define SET_CXL_REGION_ATTR(x) (&dev_attr_##x.attr),\n #define CXL_PMEM_REGION_TYPE(x) (&cxl_pmem_region_type)\n #define CXL_DAX_REGION_TYPE(x) (&cxl_dax_region_type)\n+#define CXL_SYSRAM_TYPE(x) (&cxl_sysram_type)\n int cxl_region_init(void);\n void cxl_region_exit(void);\n int cxl_get_poison_by_endpoint(struct cxl_port *port);\n@@ -54,6 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\n@@ -88,6 +91,7 @@ static inline void cxl_region_exit(void)\n #define SET_CXL_REGION_ATTR(x)\n #define CXL_PMEM_REGION_TYPE(x) NULL\n #define CXL_DAX_REGION_TYPE(x) NULL\n+#define CXL_SYSRAM_TYPE(x) NULL\n #endif\n \n struct cxl_send_command;\ndiff --git a/drivers/cxl/core/port.c b/drivers/cxl/core/port.c\nindex 5c82e6f32572..d6e82b3c2b64 100644\n--- a/drivers/cxl/core/port.c\n+++ b/drivers/cxl/core/port.c\n@@ -66,6 +66,8 @@ static int cxl_device_id(const struct device *dev)\n \t\treturn CXL_DEVICE_PMEM_REGION;\n \tif (dev->type == CXL_DAX_REGION_TYPE())\n \t\treturn CXL_DEVICE_DAX_REGION;\n+\tif (dev->type == CXL_SYSRAM_TYPE())\n+\t\treturn CXL_DEVICE_SYSRAM;\n \tif (is_cxl_port(dev)) {\n \t\tif (is_cxl_root(to_cxl_port(dev)))\n \t\t\treturn CXL_DEVICE_ROOT;\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nnew file mode 100644\nindex 000000000000..47a415deb352\n--- /dev/null\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -0,0 +1,351 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Sysram Region - Direct memory hotplug for CXL RAM regions\n+ *\n+ * This interface directly performs memory hotplug for CXL RAM regions,\n+ * eliminating the indirection through DAX.\n+ */\n+\n+#include <linux/memory_hotplug.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/memory.h>\n+#include <linux/device.h>\n+#include <linux/slab.h>\n+#include <linux/mm.h>\n+#include <cxlmem.h>\n+#include <cxl.h>\n+#include \"core.h\"\n+\n+static const char *sysram_res_name = \"System RAM (CXL)\";\n+\n+/**\n+ * cxl_region_find_sysram - Find the sysram device associated with a region\n+ * @cxlr: The CXL region\n+ *\n+ * Finds and returns the sysram child device of a CXL region.\n+ * The caller must release the device reference with put_device()\n+ * when done with the returned pointer.\n+ *\n+ * Return: Pointer to cxl_sysram, or NULL if not found\n+ */\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram;\n+\tstruct device *sdev;\n+\tchar sname[32];\n+\n+\tsnprintf(sname, sizeof(sname), \"sysram_region%d\", cxlr->id);\n+\tsdev = device_find_child_by_name(&cxlr->dev, sname);\n+\tif (!sdev)\n+\t\treturn NULL;\n+\n+\tsysram = to_cxl_sysram(sdev);\n+\treturn sysram;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_region_find_sysram, \"CXL\");\n+\n+static int sysram_get_numa_node(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_region_params *p = &cxlr->params;\n+\tint nid;\n+\n+\tnid = phys_to_target_node(p->res->start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(p->res->start);\n+\n+\treturn nid;\n+}\n+\n+static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n+{\n+\tstruct resource *res;\n+\tmhp_t mhp_flags;\n+\tint rc;\n+\n+\tif (sysram->res)\n+\t\treturn -EBUSY;\n+\n+\tres = request_mem_region(sysram->hpa_range.start,\n+\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t sysram->res_name);\n+\tif (!res)\n+\t\treturn -EBUSY;\n+\n+\tsysram->res = res;\n+\n+\t/*\n+\t * Set flags appropriate for System RAM. Leave ..._BUSY clear\n+\t * so that add_memory() can add a child resource.\n+\t */\n+\tres->flags = IORESOURCE_SYSTEM_RAM;\n+\n+\tmhp_flags = MHP_NID_IS_MGID;\n+\n+\t/*\n+\t * Ensure that future kexec'd kernels will not treat\n+\t * this as RAM automatically.\n+\t */\n+\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t online_type);\n+\tif (rc) {\n+\t\tremove_resource(res);\n+\t\tkfree(res);\n+\t\tsysram->res = NULL;\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n+{\n+\tint rc;\n+\n+\tif (!sysram->res)\n+\t\treturn 0;\n+\n+\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t       range_len(&sysram->hpa_range));\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tif (sysram->res) {\n+\t\tremove_resource(sysram->res);\n+\t\tkfree(sysram->res);\n+\t\tsysram->res = NULL;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn sysram_hotplug_remove(sysram);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_offline_and_remove, \"CXL\");\n+\n+static void cxl_sysram_release(struct device *dev)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\n+\tif (sysram->res)\n+\t\tsysram_hotplug_remove(sysram);\n+\n+\tkfree(sysram->res_name);\n+\n+\tif (sysram->mgid >= 0)\n+\t\tmemory_group_unregister(sysram->mgid);\n+\n+\tif (sysram->mtype)\n+\t\tclear_node_memory_type(sysram->numa_node, sysram->mtype);\n+\n+\tkfree(sysram);\n+}\n+\n+static ssize_t hotplug_store(struct device *dev,\n+\t\t\t     struct device_attribute *attr,\n+\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\tint online_type, rc;\n+\n+\tonline_type = mhp_online_type_from_str(buf);\n+\tif (online_type < 0)\n+\t\treturn online_type;\n+\n+\tif (online_type == MMOP_OFFLINE)\n+\t\trc = sysram_hotplug_remove(sysram);\n+\telse\n+\t\trc = sysram_hotplug_add(sysram, online_type);\n+\n+\tif (rc)\n+\t\tdev_warn(dev, \"hotplug %s failed: %d\\n\",\n+\t\t\t online_type == MMOP_OFFLINE ? \"offline\" : \"online\", rc);\n+\n+\treturn rc ? rc : len;\n+}\n+static DEVICE_ATTR_WO(hotplug);\n+\n+static struct attribute *cxl_sysram_attrs[] = {\n+\t&dev_attr_hotplug.attr,\n+\tNULL\n+};\n+\n+static const struct attribute_group cxl_sysram_attribute_group = {\n+\t.attrs = cxl_sysram_attrs,\n+};\n+\n+static const struct attribute_group *cxl_sysram_attribute_groups[] = {\n+\t&cxl_base_attribute_group,\n+\t&cxl_sysram_attribute_group,\n+\tNULL\n+};\n+\n+const struct device_type cxl_sysram_type = {\n+\t.name = \"cxl_sysram\",\n+\t.release = cxl_sysram_release,\n+\t.groups = cxl_sysram_attribute_groups,\n+};\n+\n+static bool is_cxl_sysram(struct device *dev)\n+{\n+\treturn dev->type == &cxl_sysram_type;\n+}\n+\n+struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\tif (dev_WARN_ONCE(dev, !is_cxl_sysram(dev),\n+\t\t\t  \"not a cxl_sysram device\\n\"))\n+\t\treturn NULL;\n+\treturn container_of(dev, struct cxl_sysram, dev);\n+}\n+EXPORT_SYMBOL_NS_GPL(to_cxl_sysram, \"CXL\");\n+\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram)\n+{\n+\treturn &sysram->dev;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_dev, \"CXL\");\n+\n+static struct lock_class_key cxl_sysram_key;\n+\n+static enum mmop cxl_sysram_get_default_online_type(void)\n+{\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_SYSTEM_DEFAULT))\n+\t\treturn mhp_get_default_online_type();\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_MOVABLE))\n+\t\treturn MMOP_ONLINE_MOVABLE;\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_NORMAL))\n+\t\treturn MMOP_ONLINE;\n+\treturn MMOP_OFFLINE;\n+}\n+\n+static struct cxl_sysram *cxl_sysram_alloc(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram __free(kfree) = NULL;\n+\tstruct device *dev;\n+\n+\tsysram = kzalloc(sizeof(*sysram), GFP_KERNEL);\n+\tif (!sysram)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tsysram->online_type = cxl_sysram_get_default_online_type();\n+\tsysram->last_hotplug_cmd = MMOP_OFFLINE;\n+\tsysram->numa_node = -1;\n+\tsysram->mgid = -1;\n+\n+\tdev = &sysram->dev;\n+\tsysram->cxlr = cxlr;\n+\tdevice_initialize(dev);\n+\tlockdep_set_class(&dev->mutex, &cxl_sysram_key);\n+\tdevice_set_pm_not_required(dev);\n+\tdev->parent = &cxlr->dev;\n+\tdev->bus = &cxl_bus_type;\n+\tdev->type = &cxl_sysram_type;\n+\n+\treturn_ptr(sysram);\n+}\n+\n+static void sysram_unregister(void *_sysram)\n+{\n+\tstruct cxl_sysram *sysram = _sysram;\n+\n+\tdevice_unregister(&sysram->dev);\n+}\n+\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+{\n+\tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n+\tstruct memory_dev_type *mtype;\n+\tstruct range hpa_range;\n+\tstruct device *dev;\n+\tint adist = MEMTIER_DEFAULT_LOWTIER_ADISTANCE;\n+\tint numa_node;\n+\tint rc;\n+\n+\trc = cxl_region_get_hpa_range(cxlr, &hpa_range);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\thpa_range = memory_block_align_range(&hpa_range);\n+\tif (hpa_range.start >= hpa_range.end) {\n+\t\tdev_warn(&cxlr->dev, \"region too small after alignment\\n\");\n+\t\treturn -ENOSPC;\n+\t}\n+\n+\tsysram = cxl_sysram_alloc(cxlr);\n+\tif (IS_ERR(sysram))\n+\t\treturn PTR_ERR(sysram);\n+\n+\tsysram->hpa_range = hpa_range;\n+\n+\tsysram->res_name = kasprintf(GFP_KERNEL, \"cxl_sysram%d\", cxlr->id);\n+\tif (!sysram->res_name)\n+\t\treturn -ENOMEM;\n+\n+\t/* Override default online type if caller specified one */\n+\tif (online_type >= 0)\n+\t\tsysram->online_type = online_type;\n+\n+\tdev = &sysram->dev;\n+\n+\trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Setup memory tier before adding device */\n+\tnuma_node = sysram_get_numa_node(cxlr);\n+\tif (numa_node < 0) {\n+\t\tdev_warn(&cxlr->dev, \"rejecting region with invalid node: %d\\n\",\n+\t\t\t numa_node);\n+\t\treturn -EINVAL;\n+\t}\n+\tsysram->numa_node = numa_node;\n+\n+\tmt_calc_adistance(numa_node, &adist);\n+\tmtype = mt_get_memory_type(adist);\n+\tif (IS_ERR(mtype))\n+\t\treturn PTR_ERR(mtype);\n+\tsysram->mtype = mtype;\n+\n+\tinit_node_memory_type(numa_node, mtype);\n+\n+\t/* Register memory group for this region */\n+\trc = memory_group_register_static(numa_node,\n+\t\t\t\t\t  PFN_UP(range_len(&hpa_range)));\n+\tif (rc < 0)\n+\t\treturn rc;\n+\tsysram->mgid = rc;\n+\n+\trc = device_add(dev);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n+\t\tdev_name(dev));\n+\n+\t/*\n+\t * Dynamic capacity regions (DCD) will have memory added later.\n+\t * For static RAM regions, hotplug the entire range now.\n+\t */\n+\tif (cxlr->mode != CXL_PARTMODE_RAM)\n+\t\tgoto out;\n+\n+\t/* If default online_type is a valid online mode, immediately hotplug */\n+\tif (sysram->online_type > MMOP_OFFLINE) {\n+\t\trc = sysram_hotplug_add(sysram, sysram->online_type);\n+\t\tif (rc)\n+\t\t\tdev_warn(dev, \"hotplug failed: %d\\n\", rc);\n+\t\telse\n+\t\t\tsysram->last_hotplug_cmd = sysram->online_type;\n+\t}\n+\n+out:\n+\treturn devm_add_action_or_reset(&cxlr->dev, sysram_unregister,\n+\t\t\t\t\tno_free_ptr(sysram));\n+}\n+EXPORT_SYMBOL_NS_GPL(devm_cxl_add_sysram, \"CXL\");\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex f899f240f229..8e8342fd4fde 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -607,6 +607,34 @@ struct cxl_dax_region {\n \tenum dax_driver_type dax_driver;\n };\n \n+/**\n+ * struct cxl_sysram - CXL SysRAM region for system memory hotplug\n+ * @dev: device for this sysram\n+ * @cxlr: parent cxl_region\n+ * @online_type: Default memory online type for new hotplug ops (MMOP_* value)\n+ * @last_hotplug_cmd: Last hotplug command submitted (MMOP_* value)\n+ * @hpa_range: Host physical address range for the region\n+ * @res_name: Resource name for the memory region\n+ * @res: Memory resource (set when hotplugged)\n+ * @mgid: Memory group id\n+ * @mtype: Memory tier type\n+ * @numa_node: NUMA node for this memory\n+ *\n+ * Device that directly performs memory hotplug for CXL RAM regions.\n+ */\n+struct cxl_sysram {\n+\tstruct device dev;\n+\tstruct cxl_region *cxlr;\n+\tenum mmop online_type;\n+\tint last_hotplug_cmd;\n+\tstruct range hpa_range;\n+\tconst char *res_name;\n+\tstruct resource *res;\n+\tint mgid;\n+\tstruct memory_dev_type *mtype;\n+\tint numa_node;\n+};\n+\n /**\n  * struct cxl_port - logical collection of upstream port devices and\n  *\t\t     downstream port devices to construct a CXL memory\n@@ -807,6 +835,7 @@ DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device\n DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n DEFINE_FREE(put_cxl_dax_region, struct cxl_dax_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n+DEFINE_FREE(put_cxl_sysram, struct cxl_sysram *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n \n int devm_cxl_enumerate_ports(struct cxl_memdev *cxlmd);\n void cxl_bus_rescan(void);\n@@ -889,6 +918,7 @@ void cxl_destroy_region(struct cxl_region *cxlr);\n struct device *cxl_region_dev(struct cxl_region *cxlr);\n enum cxl_partition_mode cxl_region_mode(struct cxl_region *cxlr);\n int cxl_get_region_range(struct cxl_region *cxlr, struct range *range);\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr);\n int cxl_get_committed_regions(struct cxl_memdev *cxlmd,\n \t\t\t      struct cxl_region **regions, int max_regions);\n struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n@@ -936,6 +966,7 @@ void cxl_driver_unregister(struct cxl_driver *cxl_drv);\n #define CXL_DEVICE_PMEM_REGION\t\t7\n #define CXL_DEVICE_DAX_REGION\t\t8\n #define CXL_DEVICE_PMU\t\t\t9\n+#define CXL_DEVICE_SYSRAM\t\t10\n \n #define MODULE_ALIAS_CXL(type) MODULE_ALIAS(\"cxl:t\" __stringify(type) \"*\")\n #define CXL_MODALIAS_FMT \"cxl:t%d\"\n@@ -954,6 +985,10 @@ bool is_cxl_pmem_region(struct device *dev);\n struct cxl_pmem_region *to_cxl_pmem_region(struct device *dev);\n int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n+struct cxl_sysram *to_cxl_sysram(struct device *dev);\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n static inline bool is_cxl_pmem_region(struct device *dev)\n@@ -972,6 +1007,19 @@ static inline struct cxl_dax_region *to_cxl_dax_region(struct device *dev)\n {\n \treturn NULL;\n }\n+static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\treturn NULL;\n+}\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+\t\t\t\t      enum mmop online_type)\n+{\n+\treturn -ENXIO;\n+}\n+static inline int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn -ENXIO;\n+}\n static inline u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint,\n \t\t\t\t\t       u64 spa)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-25-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the cxl_sysram region not supporting N_MEMORY_PRIVATE hotplug, and responded by extending the devm_cxl_add_sysram() function to take an additional 'private' argument, which when set registers the memory as a private node. The author also updated the sysram_hotplug_add() function to use add_private_memory_driver_managed() for private regions.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Extend the cxl_sysram region to support N_MEMORY_PRIVATE hotplug\nvia add_private_memory_driver_managed(). When a caller passes\nprivate=true to devm_cxl_add_sysram(), the memory is registered\nas a private node, isolating it from normal allocations and reclaim.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/core.h          |  2 +-\n drivers/cxl/core/region_sysram.c | 50 +++++++++++++++++++++++++-------\n drivers/cxl/cxl.h                |  9 ++++--\n 3 files changed, 48 insertions(+), 13 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 973bbcae43f7..8ca3d6d41fe4 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -56,7 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nindex 47a415deb352..77aaa52e7332 100644\n--- a/drivers/cxl/core/region_sysram.c\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -85,12 +85,23 @@ static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n \t/*\n \t * Ensure that future kexec'd kernels will not treat\n \t * this as RAM automatically.\n+\t *\n+\t * For private regions, use add_private_memory_driver_managed()\n+\t * to register as N_MEMORY_PRIVATE which isolates the memory from\n+\t * normal allocations and reclaim.\n \t */\n-\trc = __add_memory_driver_managed(sysram->mgid,\n-\t\t\t\t\t sysram->hpa_range.start,\n-\t\t\t\t\t range_len(&sysram->hpa_range),\n-\t\t\t\t\t sysram_res_name, mhp_flags,\n-\t\t\t\t\t online_type);\n+\tif (sysram->private)\n+\t\trc = add_private_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t       sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t       online_type, &sysram->np);\n+\telse\n+\t\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t online_type);\n \tif (rc) {\n \t\tremove_resource(res);\n \t\tkfree(res);\n@@ -108,10 +119,23 @@ static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n \tif (!sysram->res)\n \t\treturn 0;\n \n-\trc = offline_and_remove_memory(sysram->hpa_range.start,\n-\t\t\t\t       range_len(&sysram->hpa_range));\n-\tif (rc)\n-\t\treturn rc;\n+\tif (sysram->private) {\n+\t\trc = offline_and_remove_private_memory(sysram->numa_node,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\t/*\n+\t\t * -EBUSY means memory was removed but node_private_unregister()\n+\t\t * could not complete because other regions share the node.\n+\t\t * Continue to resource cleanup since the memory is gone.\n+\t\t */\n+\t\tif (rc && rc != -EBUSY)\n+\t\t\treturn rc;\n+\t} else {\n+\t\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\t}\n \n \tif (sysram->res) {\n \t\tremove_resource(sysram->res);\n@@ -257,7 +281,8 @@ static void sysram_unregister(void *_sysram)\n \tdevice_unregister(&sysram->dev);\n }\n \n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n+\t\t\tenum mmop online_type)\n {\n \tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n \tstruct memory_dev_type *mtype;\n@@ -291,6 +316,11 @@ int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n \tif (online_type >= 0)\n \t\tsysram->online_type = online_type;\n \n+\t/* Set up private node registration if requested */\n+\tsysram->private = private;\n+\tif (private)\n+\t\tsysram->np.owner = sysram;\n+\n \tdev = &sysram->dev;\n \n \trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 8e8342fd4fde..54e5f9ac59dc 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -10,6 +10,7 @@\n #include <linux/bitops.h>\n #include <linux/log2.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n #include <linux/io.h>\n #include <linux/range.h>\n #include <linux/dax.h>\n@@ -619,6 +620,8 @@ struct cxl_dax_region {\n  * @mgid: Memory group id\n  * @mtype: Memory tier type\n  * @numa_node: NUMA node for this memory\n+ * @private: true if this region uses N_MEMORY_PRIVATE hotplug\n+ * @np: private node registration state (valid when @private is true)\n  *\n  * Device that directly performs memory hotplug for CXL RAM regions.\n  */\n@@ -633,6 +636,8 @@ struct cxl_sysram {\n \tint mgid;\n \tstruct memory_dev_type *mtype;\n \tint numa_node;\n+\tbool private;\n+\tstruct node_private np;\n };\n \n /**\n@@ -987,7 +992,7 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n struct cxl_sysram *to_cxl_sysram(struct device *dev);\n struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n@@ -1011,7 +1016,7 @@ static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n {\n \treturn NULL;\n }\n-static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n \t\t\t\t      enum mmop online_type)\n {\n \treturn -ENXIO;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-26-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the need to move struct migration_target_control to include/linux/migrate.h, agreeing that this is necessary for the driver to use alloc_migration_target() without depending on mm-internal headers.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreement",
                "acknowledgment"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a sample CXL type-3 driver that registers device memory as\nprivate-node NUMA memory reachable only via explicit mempolicy\n(set_mempolicy / mbind).\n\nProbe flow:\n  1. Call cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Look for pre-committed RAM regions; if none exist, create one\n     using cxl_get_hpa_freespace() + cxl_request_dpa() +\n     cxl_create_region()\n  3. Convert the region to sysram via devm_cxl_add_sysram() with\n     private=true and MMOP_ONLINE_MOVABLE\n  4. Register node_private_ops with NP_OPS_MIGRATION | NP_OPS_MEMPOLICY\n     so the node is excluded from default allocations\n\nThe migrate_to callback uses alloc_migration_target() with\n__GFP_THISNODE | __GFP_PRIVATE to keep pages on the target node.\n\nMove struct migration_target_control from mm/internal.h to\ninclude/linux/migrate.h so the driver can use alloc_migration_target()\nwithout depending on mm-internal headers.\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/Kconfig                           |   2 +\n drivers/cxl/Makefile                          |   2 +\n drivers/cxl/type3_drivers/Kconfig             |   2 +\n drivers/cxl/type3_drivers/Makefile            |   2 +\n .../cxl/type3_drivers/cxl_mempolicy/Kconfig   |  16 +\n .../cxl/type3_drivers/cxl_mempolicy/Makefile  |   4 +\n .../type3_drivers/cxl_mempolicy/mempolicy.c   | 297 ++++++++++++++++++\n include/linux/migrate.h                       |   7 +-\n mm/internal.h                                 |   7 -\n 9 files changed, 331 insertions(+), 8 deletions(-)\n create mode 100644 drivers/cxl/type3_drivers/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n\ndiff --git a/drivers/cxl/Kconfig b/drivers/cxl/Kconfig\nindex f99aa7274d12..1648cdeaa0c9 100644\n--- a/drivers/cxl/Kconfig\n+++ b/drivers/cxl/Kconfig\n@@ -278,4 +278,6 @@ config CXL_ATL\n \tdepends on CXL_REGION\n \tdepends on ACPI_PRMT && AMD_NB\n \n+source \"drivers/cxl/type3_drivers/Kconfig\"\n+\n endif\ndiff --git a/drivers/cxl/Makefile b/drivers/cxl/Makefile\nindex 2caa90fa4bf2..94d2b2233bf8 100644\n--- a/drivers/cxl/Makefile\n+++ b/drivers/cxl/Makefile\n@@ -19,3 +19,5 @@ cxl_acpi-y := acpi.o\n cxl_pmem-y := pmem.o security.o\n cxl_mem-y := mem.o\n cxl_pci-y := pci.o\n+\n+obj-y += type3_drivers/\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nnew file mode 100644\nindex 000000000000..369b21763856\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nnew file mode 100644\nindex 000000000000..2b82265ff118\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\nnew file mode 100644\nindex 000000000000..3c45da237b9f\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n@@ -0,0 +1,16 @@\n+config CXL_MEMPOLICY\n+\ttristate \"CXL Private Memory with Mempolicy Support\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\thelp\n+\t  Minimal driver for CXL memory devices that registers memory as\n+\t  N_MEMORY_PRIVATE with mempolicy support.  The memory is isolated\n+\t  from default allocations and can only be reached via explicit\n+\t  mempolicy (set_mempolicy or mbind).\n+\n+\t  No compression, no PTE controls, the memory behaves like normal\n+\t  DRAM but is excluded from fallback allocations.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\nnew file mode 100644\nindex 000000000000..dfb58fc88ad9\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy.o\n+cxl_mempolicy-y := mempolicy.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\nnew file mode 100644\nindex 000000000000..1c19818eb268\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n@@ -0,0 +1,297 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Mempolicy Driver\n+ *\n+ * Minimal driver for CXL memory devices that registers memory as\n+ * N_MEMORY_PRIVATE with mempolicy support but no PTE controls.  The\n+ * memory behaves like normal DRAM but is isolated from default allocations,\n+ * it can only be reached via explicit mempolicy (set_mempolicy/mbind).\n+ *\n+ * Usage:\n+ *   1. Unbind device from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   2. Bind to cxl_mempolicy:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n+ */\n+\n+#include <linux/module.h>\n+#include <linux/pci.h>\n+#include <linux/xarray.h>\n+#include <linux/node_private.h>\n+#include <linux/migrate.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+struct cxl_mempolicy_ctx {\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint nid;\n+};\n+\n+static DEFINE_XARRAY(ctx_xa);\n+\n+static struct cxl_mempolicy_ctx *memdev_to_ctx(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\n+\treturn xa_load(&ctx_xa, (unsigned long)pdev);\n+}\n+\n+static int cxl_mempolicy_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason,\n+\t\t\t\t    unsigned int *nr_succeeded)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE |\n+\t\t\t    __GFP_PRIVATE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, nr_succeeded);\n+}\n+\n+static void cxl_mempolicy_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static const struct node_private_ops cxl_mempolicy_ops = {\n+\t.migrate_to\t= cxl_mempolicy_migrate_to,\n+\t.folio_migrate\t= cxl_mempolicy_folio_migrate,\n+\t.flags = NP_OPS_MIGRATION | NP_OPS_MEMPOLICY,\n+};\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tctx->cxled = cxled;\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\treturn cxlr;\n+}\n+\n+static int setup_private_node(struct cxl_memdev *cxlmd,\n+\t\t\t      struct cxl_region *cxlr)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct range hpa_range;\n+\tint rc;\n+\n+\tdevice_release_driver(cxl_region_dev(cxlr));\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to add sysram: %d\\n\", rc);\n+\t\tif (device_attach(cxl_region_dev(cxlr)) < 0)\n+\t\t\tdev_warn(cxl_region_dev(cxlr),\n+\t\t\t\t \"failed to re-attach driver\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tctx->nid = phys_to_target_node(hpa_range.start);\n+\tif (ctx->nid == NUMA_NO_NODE)\n+\t\tctx->nid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\trc = node_private_set_ops(ctx->nid, &cxl_mempolicy_ops);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to set ops on node %d: %d\\n\", ctx->nid, rc);\n+\t\tctx->nid = NUMA_NO_NODE;\n+\t\treturn rc;\n+\t}\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"node %d registered as private mempolicy memory\\n\", ctx->nid);\n+\treturn 0;\n+}\n+\n+static int cxl_mempolicy_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i;\n+\tint rc;\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"cxl_mempolicy attach: looking for regions\\n\");\n+\n+\t/* Phase 1: look for pre-committed RAM regions */\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) != CXL_PARTMODE_RAM) {\n+\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tcxlr = regions[i];\n+\t\trc = setup_private_node(cxlmd, cxlr);\n+\t\tput_device(cxl_region_dev(cxlr));\n+\t\tif (rc == 0) {\n+\t\t\t/* Release remaining region references */\n+\t\t\tfor (i++; i < nr; i++)\n+\t\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\treturn 0;\n+\t\t}\n+\t}\n+\n+\t/* Phase 2: no committed regions, create one */\n+\tdev_info(&cxlmd->dev,\n+\t\t \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"no RAM capacity: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = setup_private_node(cxlmd, cxlr);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to setup private node: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\t/* Only take ownership of regions we created (Phase 2) */\n+\tmemdev_to_ctx(cxlmd)->cxlr = cxlr;\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_mempolicy_attach = {\n+\t.probe = cxl_mempolicy_attach_probe,\n+};\n+\n+static int cxl_mempolicy_probe(struct pci_dev *pdev,\n+\t\t\t       const struct pci_device_id *id)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probing device\\n\");\n+\n+\tctx = devm_kzalloc(&pdev->dev, sizeof(*ctx), GFP_KERNEL);\n+\tif (!ctx)\n+\t\treturn -ENOMEM;\n+\tctx->nid = NUMA_NO_NODE;\n+\n+\trc = xa_insert(&ctx_xa, (unsigned long)pdev, ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_mempolicy_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_mempolicy_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = xa_erase(&ctx_xa, (unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: removing device\\n\");\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\tif (ctx->nid != NUMA_NO_NODE)\n+\t\tWARN_ON(node_private_clear_ops(ctx->nid, &cxl_mempolicy_ops));\n+\n+\tif (ctx->cxlr) {\n+\t\tcxl_destroy_region(ctx->cxlr);\n+\t\tctx->cxlr = NULL;\n+\t}\n+\n+\tif (ctx->cxled) {\n+\t\tcxl_dpa_free(ctx->cxled);\n+\t\tctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_mempolicy_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_mempolicy_pci_tbl);\n+\n+static struct pci_driver cxl_mempolicy_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_mempolicy_pci_tbl,\n+\t.probe\t\t= cxl_mempolicy_probe,\n+\t.remove\t\t= cxl_mempolicy_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_mempolicy_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Private Memory with Mempolicy Support\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 7b2da3875ff2..1f9fb61f3932 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -10,7 +10,12 @@\n typedef struct folio *new_folio_t(struct folio *folio, unsigned long private);\n typedef void free_folio_t(struct folio *folio, unsigned long private);\n \n-struct migration_target_control;\n+struct migration_target_control {\n+\tint nid;\t\t/* preferred node id */\n+\tnodemask_t *nmask;\n+\tgfp_t gfp_mask;\n+\tenum migrate_reason reason;\n+};\n \n /**\n  * struct movable_operations - Driver page migration\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 64467ca774f1..85cd11189854 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1352,13 +1352,6 @@ extern const struct trace_print_flags gfpflag_names[];\n \n void setup_zone_pageset(struct zone *zone);\n \n-struct migration_target_control {\n-\tint nid;\t\t/* preferred node id */\n-\tnodemask_t *nmask;\n-\tgfp_t gfp_mask;\n-\tenum migrate_reason reason;\n-};\n-\n /*\n  * mm/filemap.c\n  */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-27-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the watermark interrupts, specifically how they adjust CRAM pressure and the dynamic watermark mode. The author explains that the Low watermark interrupt increases pressure while the High watermark interrupt reduces it, and describes the four phases of the dynamic watermark mode with progressively tighter thresholds.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a generic CXL type-3 driver for compressed memory controllers.\n\nThe driver provides an alternative PCI binding that converts CXL\nRAM regions to private-node sysram and registers them with the\nCRAM subsystem for transparent demotion/promotion.\n\nProbe flow:\n  1. cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Discover/convert auto-RAM regions or create a RAM region\n  3. Convert to private-node sysram via devm_cxl_add_sysram()\n  4. Register with CRAM via cram_register_private_node()\n\nPage flush pipeline:\n  When a CRAM folio is freed, the CRAM free_folio   callback buffers\n  it into a per-CPU RCU-protected flush buffer to offload the operation.\n\n  A periodic kthread swaps the per-CPU buffers under RCU, then sends\n  batched Sanitize-Zero commands so the device can zero pages.\n\n  A flush_record bitmap tracks in-flight pages to avoid re-buffering on\n  the second free_folio entry after folio_put().\n\n  Overflow from full buffers is handled by a per-CPU workqueue fallback.\n\nWatermark interrupts:\n  MSI-X vector 12 - delivers \"Low\" watermark interrupts\n  MSI-X vector 13 - delivers \"High\" watermark interrupts\n  This adjusts CRAM pressure:\n\tLow  - increases pressure.\n  \tHigh - reduces pressure.\n\n  A dynamic watermark mode cycles through four phases with\n  progressively tighter thresholds.\n\n  Static watermark mode sets pressure 0 or MAX respectively.\n\nTeardown ordering:\n  pre_teardown  - cram_unregister + retry-loop memory offline\n  post_teardown - kthread stop, drain all flush buffers via CCI\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/type3_drivers/Kconfig             |    1 +\n drivers/cxl/type3_drivers/Makefile            |    1 +\n .../cxl/type3_drivers/cxl_compression/Kconfig |   20 +\n .../type3_drivers/cxl_compression/Makefile    |    4 +\n .../cxl_compression/compression.c             | 1025 +++++++++++++++++\n 5 files changed, 1051 insertions(+)\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/compression.c\n\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nindex 369b21763856..98f73e46730e 100644\n--- a/drivers/cxl/type3_drivers/Kconfig\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\n+source \"drivers/cxl/type3_drivers/cxl_compression/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nindex 2b82265ff118..f5b0766d92af 100644\n--- a/drivers/cxl/type3_drivers/Makefile\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression/\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Kconfig b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\nnew file mode 100644\nindex 000000000000..8c891a48b000\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\n@@ -0,0 +1,20 @@\n+config CXL_COMPRESSION\n+\ttristate \"CXL Compression Memory Driver\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on CRAM\n+\thelp\n+\t  This driver provides an alternative PCI binding for CXL memory\n+\t  devices with compressed memory support. It converts CXL RAM\n+\t  regions to sysram for direct memory hotplug and registers with\n+\t  the CRAM subsystem for transparent compression.\n+\n+\t  Page reclamation uses the standard CXL Media Operations Zero\n+\t  command (opcode 0x4402). If the device does not support it,\n+\t  the driver falls back to inline CPU zeroing.\n+\n+\t  Usage: First unbind the device from cxl_pci, then bind to\n+\t  cxl_compression. The driver will initialize the CXL device and\n+\t  convert any RAM regions to use direct memory hotplug via sysram.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Makefile b/drivers/cxl/type3_drivers/cxl_compression/Makefile\nnew file mode 100644\nindex 000000000000..46f34809bf74\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression.o\n+cxl_compression-y := compression.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/compression.c b/drivers/cxl/type3_drivers/cxl_compression/compression.c\nnew file mode 100644\nindex 000000000000..e4c8b62227e2\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/compression.c\n@@ -0,0 +1,1025 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Compression Driver\n+ *\n+ * This driver provides an alternative binding for CXL memory devices that\n+ * converts all associated RAM regions to sysram_regions for direct memory\n+ * hotplug, bypassing the standard dax region path.\n+ *\n+ * Page reclamation uses the standard CXL Media Operations Zero command\n+ * (opcode 0x4402, class 0x01, subclass 0x01).  Watermark interrupts\n+ * are delivered via separate MSI-X vectors (12 for lthresh, 13 for\n+ * hthresh), injected externally via QMP.\n+ *\n+ * Usage:\n+ *   1. Device initially binds to cxl_pci at boot\n+ *   2. Unbind from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   3. Bind to cxl_compression:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n+ */\n+\n+#include <linux/unaligned.h>\n+#include <linux/io-64-nonatomic-lo-hi.h>\n+#include <linux/module.h>\n+#include <linux/delay.h>\n+#include <linux/sizes.h>\n+#include <linux/mutex.h>\n+#include <linux/list.h>\n+#include <linux/pci.h>\n+#include <linux/io.h>\n+#include <linux/interrupt.h>\n+#include <linux/bitmap.h>\n+#include <linux/highmem.h>\n+#include <linux/workqueue.h>\n+#include <linux/kthread.h>\n+#include <linux/rcupdate.h>\n+#include <linux/percpu.h>\n+#include <linux/sched.h>\n+#include <linux/cram.h>\n+#include <linux/memory_hotplug.h>\n+#include <linux/xarray.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+/*\n+ * Per-device compression context lookup.\n+ *\n+ * pci_set_drvdata() MUST store cxlds because mbox_to_cxlds() uses\n+ * dev_get_drvdata() to recover the cxl_dev_state from the mailbox host\n+ * device.  Storing anything else in pci drvdata breaks every CXL mailbox\n+ * command.  Use an xarray keyed by pci_dev pointer so that multiple\n+ * devices can bind concurrently without colliding.\n+ */\n+static DEFINE_XARRAY(comp_ctx_xa);\n+\n+static struct cxl_compression_ctx *pdev_to_comp_ctx(struct pci_dev *pdev)\n+{\n+\treturn xa_load(&comp_ctx_xa, (unsigned long)pdev);\n+}\n+\n+#define CXL_MEDIA_OP_OPCODE\t\t0x4402\n+#define CXL_MEDIA_OP_CLASS_SANITIZE\t0x01\n+#define CXL_MEDIA_OP_SUBC_ZERO\t\t0x01\n+\n+struct cxl_dpa_range {\n+\t__le64 starting_dpa;\n+\t__le64 length;\n+} __packed;\n+\n+struct cxl_media_op_input {\n+\tu8 media_operation_class;\n+\tu8 media_operation_subclass;\n+\t__le16 reserved;\n+\t__le32 dpa_range_count;\n+\tstruct cxl_dpa_range ranges[];\n+} __packed;\n+\n+#define CXL_CT3_MSIX_LTHRESH\t\t12\n+#define CXL_CT3_MSIX_HTHRESH\t\t13\n+#define CXL_CT3_MSIX_VECTOR_NR\t\t14\n+#define CXL_FLUSH_INTERVAL_DEFAULT_MS\t1000\n+\n+static unsigned int flush_buf_size;\n+module_param(flush_buf_size, uint, 0444);\n+MODULE_PARM_DESC(flush_buf_size,\n+\t\t \"Max DPA ranges per media ops CCI command (0 = use hw max)\");\n+\n+static unsigned int flush_interval_ms = CXL_FLUSH_INTERVAL_DEFAULT_MS;\n+module_param(flush_interval_ms, uint, 0644);\n+MODULE_PARM_DESC(flush_interval_ms,\n+\t\t \"Flush worker interval in ms (default 1000)\");\n+\n+struct cxl_flush_buf {\n+\tunsigned int count;\n+\tunsigned int max;\t\t\t/* max ranges per command */\n+\tstruct cxl_media_op_input *cmd;\t\t/* pre-allocated CCI payload */\n+\tstruct folio **folios;\t\t\t/* parallel folio tracking */\n+};\n+\n+struct cxl_flush_ctx;\n+\n+struct cxl_pcpu_flush {\n+\tstruct cxl_flush_buf __rcu *active;\t/* callback writes here */\n+\tstruct cxl_flush_buf *overflow_spare;\t/* spare for overflow work */\n+\tstruct work_struct overflow_work;\t/* per-CPU overflow flush */\n+\tstruct cxl_flush_ctx *ctx;\t\t/* backpointer */\n+};\n+\n+/**\n+ * struct cxl_flush_ctx - Per-region flush context\n+ * @flush_record: two-level bitmap, 1 bit per 4KB page, tracks in-flight ops\n+ * @flush_record_pages: number of pages in the flush_record array\n+ * @nr_pages: total number of 4KB pages in the region\n+ * @base_pfn: starting PFN of the region (for DPA offset calculation)\n+ * @buf_max: max DPA ranges per CCI command\n+ * @media_ops_supported: true if device supports media operations zero\n+ * @pcpu: per-CPU flush state\n+ * @kthread_spares: array[nr_cpu_ids] of spare buffers for the kthread\n+ * @flush_thread: round-robin kthread\n+ * @mbox: pointer to CXL mailbox for sending CCI commands\n+ * @dev: device for logging\n+ * @nid: NUMA node of the private region\n+ */\n+struct cxl_flush_ctx {\n+\tunsigned long\t**flush_record;\n+\tunsigned int\t flush_record_pages;\n+\tunsigned long\t nr_pages;\n+\tunsigned long\t base_pfn;\n+\tunsigned int\t buf_max;\n+\tbool\t\t media_ops_supported;\n+\tstruct cxl_pcpu_flush __percpu *pcpu;\n+\tstruct cxl_flush_buf **kthread_spares;\n+\tstruct task_struct *flush_thread;\n+\tstruct cxl_mailbox *mbox;\n+\tstruct device\t*dev;\n+\tint\t\t nid;\n+};\n+\n+/* Bits per page-sized bitmap chunk */\n+#define FLUSH_RECORD_BITS_PER_PAGE\t(PAGE_SIZE * BITS_PER_BYTE)\n+#define FLUSH_RECORD_SHIFT\t\t(PAGE_SHIFT + 3)\n+\n+static unsigned long **flush_record_alloc(unsigned long nr_bits,\n+\t\t\t\t\t  unsigned int *nr_pages_out)\n+{\n+\tunsigned int nr_pages = DIV_ROUND_UP(nr_bits, FLUSH_RECORD_BITS_PER_PAGE);\n+\tunsigned long **pages;\n+\tunsigned int i;\n+\n+\tpages = kcalloc(nr_pages, sizeof(*pages), GFP_KERNEL);\n+\tif (!pages)\n+\t\treturn NULL;\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tpages[i] = (unsigned long *)get_zeroed_page(GFP_KERNEL);\n+\t\tif (!pages[i])\n+\t\t\tgoto err;\n+\t}\n+\n+\t*nr_pages_out = nr_pages;\n+\treturn pages;\n+\n+err:\n+\twhile (i--)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+\treturn NULL;\n+}\n+\n+static void flush_record_free(unsigned long **pages, unsigned int nr_pages)\n+{\n+\tunsigned int i;\n+\n+\tif (!pages)\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr_pages; i++)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+}\n+\n+static inline bool flush_record_test_and_clear(unsigned long **pages,\n+\t\t\t\t\t       unsigned long idx)\n+{\n+\treturn test_and_clear_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\t\t\t  pages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static inline void flush_record_set(unsigned long **pages, unsigned long idx)\n+{\n+\tset_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\tpages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static struct cxl_flush_buf *cxl_flush_buf_alloc(unsigned int max, int nid)\n+{\n+\tstruct cxl_flush_buf *buf;\n+\n+\tbuf = kzalloc_node(sizeof(*buf), GFP_KERNEL, nid);\n+\tif (!buf)\n+\t\treturn NULL;\n+\n+\tbuf->max = max;\n+\tbuf->cmd = kvzalloc_node(struct_size(buf->cmd, ranges, max),\n+\t\t\t\t GFP_KERNEL, nid);\n+\tif (!buf->cmd)\n+\t\tgoto err_cmd;\n+\n+\tbuf->folios = kcalloc_node(max, sizeof(struct folio *),\n+\t\t\t\t   GFP_KERNEL, nid);\n+\tif (!buf->folios)\n+\t\tgoto err_folios;\n+\n+\treturn buf;\n+\n+err_folios:\n+\tkvfree(buf->cmd);\n+err_cmd:\n+\tkfree(buf);\n+\treturn NULL;\n+}\n+\n+static void cxl_flush_buf_free(struct cxl_flush_buf *buf)\n+{\n+\tif (!buf)\n+\t\treturn;\n+\tkvfree(buf->cmd);\n+\tkfree(buf->folios);\n+\tkfree(buf);\n+}\n+\n+static inline void cxl_flush_buf_reset(struct cxl_flush_buf *buf)\n+{\n+\tbuf->count = 0;\n+}\n+\n+static void cxl_flush_buf_send(struct cxl_flush_ctx *ctx,\n+\t\t\t       struct cxl_flush_buf *buf)\n+{\n+\tstruct cxl_mbox_cmd mbox_cmd;\n+\tunsigned int count = buf->count;\n+\tunsigned int i;\n+\tint rc;\n+\n+\tif (count == 0)\n+\t\treturn;\n+\n+\tif (!ctx->media_ops_supported) {\n+\t\t/* No device support, zero all folios inline */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t\tgoto release;\n+\t}\n+\n+\tbuf->cmd->media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE;\n+\tbuf->cmd->media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO;\n+\tbuf->cmd->reserved = 0;\n+\tbuf->cmd->dpa_range_count = cpu_to_le32(count);\n+\n+\tmbox_cmd = (struct cxl_mbox_cmd) {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = buf->cmd,\n+\t\t.size_in = struct_size(buf->cmd, ranges, count),\n+\t\t.poll_interval_ms = 1000,\n+\t\t.poll_count = 30,\n+\t};\n+\n+\trc = cxl_internal_send_cmd(ctx->mbox, &mbox_cmd);\n+\tif (rc) {\n+\t\tdev_warn(ctx->dev,\n+\t\t\t \"media ops zero CCI command failed: %d\\n\", rc);\n+\n+\t\t/* Zero all folios inline on failure */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t}\n+\n+release:\n+\tfor (i = 0; i < count; i++)\n+\t\tfolio_put(buf->folios[i]);\n+\n+\tcxl_flush_buf_reset(buf);\n+}\n+\n+static int cxl_compression_flush_cb(struct folio *folio, void *private)\n+{\n+\tstruct cxl_flush_ctx *ctx = private;\n+\tunsigned long pfn = folio_pfn(folio);\n+\tunsigned long idx = pfn - ctx->base_pfn;\n+\tunsigned long nr = folio_nr_pages(folio);\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tunsigned long flags;\n+\tunsigned int pos;\n+\n+\t/* Case (a): flush record bit set, resolution from our media op */\n+\tif (flush_record_test_and_clear(ctx->flush_record, idx))\n+\t\treturn 0;\n+\n+\tdev_dbg_ratelimited(ctx->dev,\n+\t\t\t     \"flush_cb: folio pfn=%lx order=%u idx=%lu cpu=%d\\n\",\n+\t\t\t     pfn, folio_order(folio), idx,\n+\t\t\t     raw_smp_processor_id());\n+\n+\tlocal_irq_save(flags);\n+\trcu_read_lock();\n+\n+\tpcpu = this_cpu_ptr(ctx->pcpu);\n+\tbuf = rcu_dereference(pcpu->active);\n+\n+\tif (unlikely(!buf || buf->count >= buf->max)) {\n+\t\trcu_read_unlock();\n+\t\tlocal_irq_restore(flags);\n+\t\tif (buf)\n+\t\t\tschedule_work_on(raw_smp_processor_id(),\n+\t\t\t\t\t &pcpu->overflow_work);\n+\t\treturn 2;\n+\t}\n+\n+\t/* Case (b): write DPA range directly into pre-formatted CCI buffer */\n+\tfolio_get(folio);\n+\tflush_record_set(ctx->flush_record, idx);\n+\n+\tpos = buf->count;\n+\tbuf->folios[pos] = folio;\n+\tbuf->cmd->ranges[pos].starting_dpa = cpu_to_le64((u64)idx * PAGE_SIZE);\n+\tbuf->cmd->ranges[pos].length = cpu_to_le64((u64)nr * PAGE_SIZE);\n+\tbuf->count = pos + 1;\n+\n+\trcu_read_unlock();\n+\tlocal_irq_restore(flags);\n+\n+\treturn 1;\n+}\n+\n+static int cxl_flush_kthread_fn(void *data)\n+{\n+\tstruct cxl_flush_ctx *ctx = data;\n+\tstruct cxl_flush_buf *dirty;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tint cpu;\n+\tbool any_dirty;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tany_dirty = false;\n+\n+\t\t/* Phase 1: Swap all per-CPU buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tstruct cxl_flush_buf *spare = ctx->kthread_spares[cpu];\n+\n+\t\t\tif (!spare)\n+\t\t\t\tcontinue;\n+\n+\t\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\t\tcxl_flush_buf_reset(spare);\n+\t\t\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\t\t\tctx->kthread_spares[cpu] = dirty;\n+\n+\t\t\tif (dirty && dirty->count > 0) {\n+\t\t\t\tdev_dbg(ctx->dev,\n+\t\t\t\t\t \"flush_kthread: cpu=%d has %u dirty ranges\\n\",\n+\t\t\t\t\t cpu, dirty->count);\n+\t\t\t\tany_dirty = true;\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (!any_dirty)\n+\t\t\tgoto sleep;\n+\n+\t\t/* Phase 2: Single synchronize_rcu for all swaps */\n+\t\tsynchronize_rcu();\n+\n+\t\t/* Phase 3: Send CCI commands for dirty buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tdirty = ctx->kthread_spares[cpu];\n+\t\t\tif (dirty && dirty->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, dirty);\n+\t\t\t/* dirty is now clean, stays as kthread_spares[cpu] */\n+\t\t}\n+\n+sleep:\n+\t\tschedule_timeout_interruptible(\n+\t\t\tmsecs_to_jiffies(flush_interval_ms));\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static void cxl_flush_overflow_work(struct work_struct *work)\n+{\n+\tstruct cxl_pcpu_flush *pcpu =\n+\t\tcontainer_of(work, struct cxl_pcpu_flush, overflow_work);\n+\tstruct cxl_flush_ctx *ctx = pcpu->ctx;\n+\tstruct cxl_flush_buf *dirty, *spare;\n+\tunsigned long flags;\n+\n+\tdev_dbg(ctx->dev, \"flush_overflow: cpu=%d buffer full, flushing\\n\",\n+\t\t raw_smp_processor_id());\n+\n+\tspare = pcpu->overflow_spare;\n+\tif (!spare)\n+\t\treturn;\n+\n+\tcxl_flush_buf_reset(spare);\n+\n+\tlocal_irq_save(flags);\n+\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\tlocal_irq_restore(flags);\n+\n+\tpcpu->overflow_spare = dirty;\n+\n+\tsynchronize_rcu();\n+\tcxl_flush_buf_send(ctx, dirty);\n+}\n+\n+struct cxl_teardown_ctx {\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+};\n+\n+static void cxl_compression_pre_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\n+\tif (!tctx->flush_ctx)\n+\t\treturn;\n+\n+\t/*\n+\t * Unregister the CRAM node before memory goes offline.\n+\t * node_private_clear_ops requires the node_private to still\n+\t * exist, which is destroyed during memory removal.\n+\t */\n+\tcram_unregister_private_node(tctx->nid);\n+\n+\t/*\n+\t * Offline and remove CXL memory with retry.  CXL compressed\n+\t * memory may have pages pinned by in-flight flush operations;\n+\t * keep retrying until they complete.  Once done, sysram->res\n+\t * is NULL so the devm sysram_unregister action that follows\n+\t * will skip the hotplug removal.\n+\t */\n+\tif (tctx->sysram) {\n+\t\tint rc, retries = 0;\n+\n+\t\twhile (true) {\n+\t\t\trc = cxl_sysram_offline_and_remove(tctx->sysram);\n+\t\t\tif (!rc)\n+\t\t\t\tbreak;\n+\t\t\tif (++retries > 60) {\n+\t\t\t\tpr_err(\"cxl_compression: memory offline failed after %d retries, giving up\\n\",\n+\t\t\t\t       retries);\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tpr_info(\"cxl_compression: memory offline failed (%d), retrying...\\n\",\n+\t\t\t\trc);\n+\t\t\tmsleep(1000);\n+\t\t}\n+\t}\n+}\n+\n+static void cxl_compression_post_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\tstruct cxl_flush_ctx *ctx = tctx->flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tint cpu;\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\t/* cram_unregister_private_node already called in pre_teardown */\n+\n+\tif (ctx->flush_thread) {\n+\t\tkthread_stop(ctx->flush_thread);\n+\t\tctx->flush_thread = NULL;\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\tcancel_work_sync(&pcpu->overflow_work);\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tif (buf && buf->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, buf);\n+\n+\t\tif (pcpu->overflow_spare && pcpu->overflow_spare->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares && ctx->kthread_spares[cpu]) {\n+\t\t\tbuf = ctx->kthread_spares[cpu];\n+\t\t\tif (buf->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, buf);\n+\t\t}\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(ctx->kthread_spares[cpu]);\n+\t}\n+\n+\tkfree(ctx->kthread_spares);\n+\tfree_percpu(ctx->pcpu);\n+\tflush_record_free(ctx->flush_record, ctx->flush_record_pages);\n+}\n+\n+/**\n+ * struct cxl_compression_ctx - Per-device context for compression driver\n+ * @mbox: CXL mailbox for issuing CCI commands\n+ * @pdev: PCI device\n+ * @flush_ctx: Flush context for deferred page reclamation\n+ * @tctx: Teardown context for devm actions\n+ * @sysram: Sysram device for offline+remove in remove path\n+ * @nid: NUMA node ID, NUMA_NO_NODE if unset\n+ * @cxlmd: The memdev associated with this context\n+ * @cxlr: Region created by this driver (NULL if pre-existing)\n+ * @cxled: Endpoint decoder with DPA allocated by this driver\n+ * @regions_converted: Number of regions successfully converted\n+ * @media_ops_supported: Device supports media operations zero (0x4402)\n+ */\n+struct cxl_compression_ctx {\n+\tstruct cxl_mailbox *mbox;\n+\tstruct pci_dev *pdev;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+\tstruct cxl_memdev *cxlmd;\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint regions_converted;\n+\tbool media_ops_supported;\n+};\n+\n+/*\n+ * Probe whether the device supports Media Operations Zero (0x4402).\n+ * Send a zero-count command, a conforming device returns SUCCESS,\n+ * a device that doesn't support it returns UNSUPPORTED (-ENXIO).\n+ */\n+static bool cxl_probe_media_ops_zero(struct cxl_mailbox *mbox,\n+\t\t\t\t     struct device *dev)\n+{\n+\tstruct cxl_media_op_input probe = {\n+\t\t.media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE,\n+\t\t.media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO,\n+\t\t.dpa_range_count = 0,\n+\t};\n+\tstruct cxl_mbox_cmd cmd = {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = &probe,\n+\t\t.size_in = sizeof(probe),\n+\t};\n+\tint rc;\n+\n+\trc = cxl_internal_send_cmd(mbox, &cmd);\n+\tif (rc) {\n+\t\tdev_info(dev,\n+\t\t\t \"media operations zero not supported (rc=%d), using inline zeroing\\n\",\n+\t\t\t rc);\n+\t\treturn false;\n+\t}\n+\n+\tdev_info(dev, \"media operations zero (0x4402) supported\\n\");\n+\treturn true;\n+}\n+\n+struct cxl_compression_wm_ctx {\n+\tstruct device *dev;\n+\tint nid;\n+};\n+\n+static irqreturn_t cxl_compression_lthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"lthresh watermark: pressuring node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, CRAM_PRESSURE_MAX);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static irqreturn_t cxl_compression_hthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"hthresh watermark: resuming node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, 0);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static int convert_region_to_sysram(struct cxl_region *cxlr,\n+\t\t\t\t    struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct device *dev = cxl_region_dev(cxlr);\n+\tstruct cxl_compression_wm_ctx *wm_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tresource_size_t region_start, region_size;\n+\tstruct range hpa_range;\n+\tint nid;\n+\tint irq;\n+\tint cpu;\n+\tint rc;\n+\n+\tif (cxl_region_mode(cxlr) != CXL_PARTMODE_RAM) {\n+\t\tdev_dbg(dev, \"skipping non-RAM region (mode=%d)\\n\",\n+\t\t\tcxl_region_mode(cxlr));\n+\t\treturn 0;\n+\t}\n+\n+\tdev_info(dev, \"converting region to sysram\\n\");\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to add sysram region: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\ttctx = devm_kzalloc(dev, sizeof(*tctx), GFP_KERNEL);\n+\tif (!tctx)\n+\t\treturn -ENOMEM;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_post_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Find the sysram child device for pre_teardown */\n+\tcomp_ctx->sysram = cxl_region_find_sysram(cxlr);\n+\tif (comp_ctx->sysram)\n+\t\ttctx->sysram = comp_ctx->sysram;\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tnid = phys_to_target_node(hpa_range.start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\tregion_start = hpa_range.start;\n+\tregion_size = range_len(&hpa_range);\n+\n+\tflush_ctx = devm_kzalloc(dev, sizeof(*flush_ctx), GFP_KERNEL);\n+\tif (!flush_ctx)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->base_pfn = PHYS_PFN(region_start);\n+\tflush_ctx->nr_pages = region_size >> PAGE_SHIFT;\n+\tflush_ctx->flush_record = flush_record_alloc(flush_ctx->nr_pages,\n+\t\t\t\t\t\t     &flush_ctx->flush_record_pages);\n+\tif (!flush_ctx->flush_record)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->mbox = comp_ctx->mbox;\n+\tflush_ctx->dev = dev;\n+\tflush_ctx->nid = nid;\n+\tflush_ctx->media_ops_supported = comp_ctx->media_ops_supported;\n+\n+\t/*\n+\t * Cap buffer at max DPA ranges that fit in one CCI payload.\n+\t * Header is 8 bytes (struct cxl_media_op_input), each range\n+\t * is 16 bytes (struct cxl_dpa_range).  The module parameter\n+\t * flush_buf_size can further limit this (0 = use hw max).\n+\t */\n+\tflush_ctx->buf_max = (flush_ctx->mbox->payload_size -\n+\t\t\t      sizeof(struct cxl_media_op_input)) /\n+\t\t\t     sizeof(struct cxl_dpa_range);\n+\tif (flush_buf_size && flush_buf_size < flush_ctx->buf_max)\n+\t\tflush_ctx->buf_max = flush_buf_size;\n+\tif (flush_ctx->buf_max == 0)\n+\t\tflush_ctx->buf_max = 1;\n+\n+\tdev_info(dev,\n+\t\t \"flush buffer: %u DPA ranges per command (payload %zu bytes, media_ops %s)\\n\",\n+\t\t flush_ctx->buf_max, flush_ctx->mbox->payload_size,\n+\t\t flush_ctx->media_ops_supported ? \"yes\" : \"no\");\n+\n+\tflush_ctx->pcpu = alloc_percpu(struct cxl_pcpu_flush);\n+\tif (!flush_ctx->pcpu)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->kthread_spares = kcalloc(nr_cpu_ids,\n+\t\t\t\t\t    sizeof(struct cxl_flush_buf *),\n+\t\t\t\t\t    GFP_KERNEL);\n+\tif (!flush_ctx->kthread_spares)\n+\t\tgoto err_pcpu_init;\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *active_buf, *overflow_buf, *spare_buf;\n+\n+\t\tactive_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!active_buf)\n+\t\t\tgoto err_pcpu_init;\n+\n+\t\toverflow_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!overflow_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tspare_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!spare_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tcxl_flush_buf_free(overflow_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\t\tpcpu->ctx = flush_ctx;\n+\t\trcu_assign_pointer(pcpu->active, active_buf);\n+\t\tpcpu->overflow_spare = overflow_buf;\n+\t\tINIT_WORK(&pcpu->overflow_work, cxl_flush_overflow_work);\n+\n+\t\tflush_ctx->kthread_spares[cpu] = spare_buf;\n+\t}\n+\n+\tflush_ctx->flush_thread = kthread_create_on_node(\n+\t\tcxl_flush_kthread_fn, flush_ctx, nid, \"cxl-flush/%d\", nid);\n+\tif (IS_ERR(flush_ctx->flush_thread)) {\n+\t\trc = PTR_ERR(flush_ctx->flush_thread);\n+\t\tflush_ctx->flush_thread = NULL;\n+\t\tgoto err_pcpu_init;\n+\t}\n+\twake_up_process(flush_ctx->flush_thread);\n+\n+\trc = cram_register_private_node(nid, cxlr,\n+\t\t\t\t\tcxl_compression_flush_cb, flush_ctx);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to register cram node %d: %d\\n\", nid, rc);\n+\t\tgoto err_pcpu_init;\n+\t}\n+\n+\ttctx->flush_ctx = flush_ctx;\n+\ttctx->nid = nid;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_pre_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcomp_ctx->flush_ctx = flush_ctx;\n+\tcomp_ctx->tctx = tctx;\n+\tcomp_ctx->nid = nid;\n+\n+\t/*\n+\t * Register watermark IRQ handlers on &pdev->dev for\n+\t * MSI-X vector 12 (lthresh) and vector 13 (hthresh).\n+\t */\n+\twm_ctx = devm_kzalloc(&pdev->dev, sizeof(*wm_ctx), GFP_KERNEL);\n+\tif (!wm_ctx)\n+\t\treturn -ENOMEM;\n+\n+\twm_ctx->dev = &pdev->dev;\n+\twm_ctx->nid = nid;\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_LTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_lthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-lthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register lthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_HTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_hthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-hthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register hthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\treturn 0;\n+\n+err_pcpu_init:\n+\tif (flush_ctx->flush_thread)\n+\t\tkthread_stop(flush_ctx->flush_thread);\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *buf;\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (flush_ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(flush_ctx->kthread_spares[cpu]);\n+\t}\n+\tkfree(flush_ctx->kthread_spares);\n+\tfree_percpu(flush_ctx->pcpu);\n+\tflush_record_free(flush_ctx->flush_record, flush_ctx->flush_record_pages);\n+\treturn rc ? rc : -ENOMEM;\n+}\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\tpdev_to_comp_ctx(to_pci_dev(cxlmd->dev.parent))->cxled = cxled;\n+\treturn cxlr;\n+}\n+\n+static int cxl_compression_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i, converted = 0, errors = 0;\n+\tint rc;\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\t/* Probe device for media operations zero support */\n+\tcomp_ctx->media_ops_supported =\n+\t\tcxl_probe_media_ops_zero(comp_ctx->mbox,\n+\t\t\t\t\t &cxlmd->dev);\n+\n+\tdev_info(&cxlmd->dev, \"compression attach: looking for regions\\n\");\n+\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) == CXL_PARTMODE_RAM) {\n+\t\t\trc = convert_region_to_sysram(regions[i], pdev);\n+\t\t\tif (rc)\n+\t\t\t\terrors++;\n+\t\t\telse\n+\t\t\t\tconverted++;\n+\t\t}\n+\t\tput_device(cxl_region_dev(regions[i]));\n+\t}\n+\n+\tif (converted > 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"converted %d regions to sysram (%d errors)\\n\",\n+\t\t\t converted, errors);\n+\t\treturn errors ? -EIO : 0;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"could not create RAM region: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = convert_region_to_sysram(cxlr, pdev);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to convert region to sysram: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tcomp_ctx->cxlr = cxlr;\n+\n+\tdev_info(&cxlmd->dev, \"created and converted region %s to sysram\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_compression_attach = {\n+\t.probe = cxl_compression_attach_probe,\n+};\n+\n+static int cxl_compression_probe(struct pci_dev *pdev,\n+\t\t\t\t const struct pci_device_id *id)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probing device\\n\");\n+\n+\tcomp_ctx = devm_kzalloc(&pdev->dev, sizeof(*comp_ctx), GFP_KERNEL);\n+\tif (!comp_ctx)\n+\t\treturn -ENOMEM;\n+\tcomp_ctx->nid = NUMA_NO_NODE;\n+\tcomp_ctx->pdev = pdev;\n+\n+\trc = xa_insert(&comp_ctx_xa, (unsigned long)pdev, comp_ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_compression_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&comp_ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_compression_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = xa_erase(&comp_ctx_xa,\n+\t\t\t\t\t\t\t(unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: removing device\\n\");\n+\n+\tif (!comp_ctx || comp_ctx->nid == NUMA_NO_NODE)\n+\t\treturn;\n+\n+\t/*\n+\t * Destroy the region, devm actions on the region device handle teardown\n+\t * in registration-reverse order:\n+\t *   1. pre_teardown:  cram_unregister + retry-forever memory offline\n+\t *   2. sysram_unregister: device_unregister (sysram->res is NULL\n+\t *      after pre_teardown, so cxl_sysram_release skips hotplug)\n+\t *   3. post_teardown: kthread stop, flush cleanup\n+\t *\n+\t * PCI MMIO is still live so CCI commands in post_teardown work.\n+\t */\n+\tif (comp_ctx->cxlr) {\n+\t\tcxl_destroy_region(comp_ctx->cxlr);\n+\t\tcomp_ctx->cxlr = NULL;\n+\t}\n+\n+\tif (comp_ctx->cxled) {\n+\t\tcxl_dpa_free(comp_ctx->cxled);\n+\t\tcomp_ctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_compression_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ /* terminate list */ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_compression_pci_tbl);\n+\n+static struct pci_driver cxl_compression_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_compression_pci_tbl,\n+\t.probe\t\t= cxl_compression_probe,\n+\t.remove\t\t= cxl_compression_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_compression_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Compression Memory Driver with SysRAM regions\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "20260222084842.1824063-28-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) expressed concern about adding more special-casing similar to ZONE_DEVICE, specifically mentioning the folio_managed_() functions in mprotect.c and suggesting it's a reasonable topic for discussion.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concern",
                "special-casing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm concerned about adding more special-casing (similar to what we \nalready added for ZONE_DEVICE) all over the place.\n\nLike the whole folio_managed_() stuff in mprotect.c\n\nHaving that said, sounds like a reasonable topic to discuss.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "c10400db-2259-4465-a07e-19d0691101a4@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged a concern about the patch's handling of device-coherent memory and proposed two alternative solutions: reusing zone_device hooks or modifying vma_wants_writenotify() to add a protected/page flag.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "proposed alternative solutions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It's a valid concern - and is why I tried to re-use as many of the\nzone_device hooks as possible.  It does not seem zone_device has quite\nthe same semantics for a case like this, so I had to make something new.\n\nDEVICE_COHERENT injects a temporary swap entry to allow the device to do\na large atomic operation - then the page table is restored and the CPU\nis free to change entries as it pleases.\n\nAnother option would be to add the hook to vma_wants_writenotify()\ninstead of the page table code - and mask MM_CP_TRY_CHANGE_WRITABLE.\n\nThis would require adding a vma flag - or maybe a count of protected /\ndevice pages.\n\nint mprotect_fixup() {\n    ...\n    if (vma_wants_manual_pte_write_upgrade(vma))\n        mm_cp_flags |= MM_CP_TRY_CHANGE_WRITABLE;\n}\n\nbool vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)\n{\n    if (vma->managed_wrprotect)\n        return true;\n}\n\nThat would localize the change in folio_managed_fixup_migration_pte() :\n\nstatic inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n                                                      pte_t pte,\n                                                      pte_t old_pte,\n                                                      struct vm_area_struct *vma)\n{\n    ...\n    } else if (folio_managed_wrprotect(page_folio(new))) {\n        pte = pte_wrprotect(pte);\n+       atomic_inc(&vma->managed_wrprotect);\n    }\n    return pte;\n}\n\nThis would cover both the huge_memory.c and mprotect, and maybe that's\njust generally cleaner? I can try that to see if it actually works.\n\n~Gregory",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "message_id": "aZxqP7J1kOClQUPQ@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that existing hooks can be used for write protection and agreed to remove redundant code from page table walks.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged existing solution",
                "agreed to simplify code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "scratch all this - existing hooks exist for exactly this purpose:\n\n\tcan_change_[pte|pmd]_writable()\n\nSurprised I missed this.\n\nI can clean this up to remove it from the page table walks.\n\nStill valid to question whether we want this, but at least the hook\nlives with other write-protect hooks now.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "aZx7hsVNU0XOCCiG@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Alistair Popple",
              "summary": "Reviewer Alistair Popple expressed concerns that N_MEMORY_PRIVATE may not be the best solution for isolating NUMA nodes and exposing device memory to userspace, suggesting that existing ZONE_DEVICE implementations or a standalone buddy allocator like drm_buddy.c could be used instead.\n\nThe reviewer believes the patch provides a standard interface to userspace for managing device memory, and suggests that the existing NUMA APIs are a reasonable way to achieve this.\n\nReviewer Alistair Popple noted that the patch introduces a new mechanism similar to ZONE_DEVICE's dev_pagemap_ops(), but without explaining why existing mechanisms cannot be extended or modified to provide the same services. Specifically, he pointed out that using NODE_DATA instead of page->pgmap would require pages to be on the LRU, which is problematic for ZONE_DEVICE pages due to their pgmap pointer.\n\nReviewer suggested that the patch is similar to existing ZONE_DEVICE methods and proposed building upon those instead of introducing a new feature set.\n\nReviewer Alistair Popple noted that the implementation duplicates a lot of hooks, similar to those provided by ZONE_DEVICE, and requested further discussion.\n\nReviewer Alistair Popple questioned whether the mm allocator is necessary for private memory allocation, suggesting that a device allocator library could be written or reused from drm_buddy.c.\n\nThe reviewer questioned the patch's approach to handling ZONE_DEVICE pages, suggesting that they are real struct pages and not just a perspective issue, and asked for clarification on the actual limitations being addressed.\n\nReviewer suggested that ZONE_DEVICE_COHERENT could be extended to support the usecase, proposing a couple of extra dev_pagemap_ops and LRU access as sufficient modifications.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "no clear signal",
                "suggested alternative solutions",
                "alternative approach",
                "duplicates_hooks",
                "similar_to_ZONE_DEVICE",
                "questioning",
                "request_for_clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Having had to re-implement entire portions of mm/ in a driver I agree this isn't\nsomething anyone sane should do :-) However aspects of ZONE_DEVICE were added\nprecisely to help with that so I'm not sure N_MEMORY_PRIVATE is the only or best\nway to do that.\n\nBased on our discussion at LPC I believe one of the primary motivators here was\nto re-use the existing mm buddy allocator rather than writing your own. I remain\nto be convinced that alone is justification enough for doing all this - DRM for\nexample already has quite a nice standalone buddy allocator (drm_buddy.c) that\ncould presumably be used, or adapted for use, by any device driver.\n\nThe interesting part of this series (which I have skimmed but not read in\ndetail) is how device memory gets exposed to userspace - this is something that\nexisting ZONE_DEVICE implementations don't address, instead leaving it up to\ndrivers and associated userspace stacks to deal with allocation, migration, etc.\n\n---\n\nThis is I think is one of the key things that should be enabled - providing a\nstandard interface to userspace for managing device memory. The existing NUMA\nAPIs do seem like a reasonable way to do this.\n\n---\n\nOne does not have to squint too hard to see that the above is not so different\nfrom what ZONE_DEVICE provides today via dev_pagemap_ops(). So I think I think\nit would be worth outlining why the existing ZONE_DEVICE mechanism can't be\nextended to provide these kind of services.\n\nThis seems to add a bunch of code just to use NODE_DATA instead of page->pgmap,\nwithout really explaining why just extending dev_pagemap_ops wouldn't work. The\nobvious reason is that if you want to support things like reclaim, compaction,\netc. these pages need to be on the LRU, which is a little bit hard when that\nfield is also used by the pgmap pointer for ZONE_DEVICE pages.\n\nBut it might be good to explore other options for storing the pgmap - for\nexample page_ext could be used.  Or I hear struct page may go away in place of\nfolios any day now, so maybe that gives us space for both :-)\n\n---\n\nThe above also looks pretty similar to the existing ZONE_DEVICE methods for\ndoing this which is another reason to argue for just building up the feature set\nof the existing boondoggle rather than adding another thingymebob.\n\nIt seems the key thing we are looking for is:\n\n1) A userspace API to allocate/manage device memory (ie. move_pages(), mbind(),\netc.)\n\n2) Allowing reclaim/LRU list processing of device memory.\n\n---\n\ndiscussion (hopefully I can make it to LSFMM). Mostly I'm interested in the\nimplementation as this does on the surface seem to sprinkle around and duplicate\na lot of hooks similar to what ZONE_DEVICE already provides.\n\n---\n\nFor basic allocation I agree this is the case. But there's no reason some device\nallocator library couldn't be written. Or in fact as pointed out above reuse the\nalready existing one in drm_buddy.c.  So would be interested to hear arguments\nfor why allocation has to be done by the mm allocator and/or why an allocation\nlibrary wouldn't work here given DRM already has them.\n\n---\n\nZONE_DEVICE pages are in fact real struct pages, but I will concede that\nperspective probably depends on which bits of the mm you play in. The real\nlimitations you seem to be addressing is more around how we get these pages in\nan LRU, or are there other limitations?\n\n---\n\nWhat I'd like to explore is why ZONE_DEVICE_COHERENT couldn't just be extended\nto support your usecase? It seems a couple of extra dev_pagemap_ops and being\nable to go on the LRU would get you there.\n\n - Alistair",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "fzy6f6dpv3oq3ksr2mkst7pz3daeb3buhuvdvcw4633pcl7h6u@mxjgiwpg5acv",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledges that using ZONE_DEVICE is not necessary, as N_MEMORY_PRIVATE can be implemented by modifying existing ZONE_NORMAL behavior. They propose reusing the buddy allocator to manage private memory nodes, eliminating the need for additional complexity in ZONE_DEVICE. This approach would simplify the code and avoid adding more abstraction layers.\n\nThe author explains that the callback similarity between ZONE_DEVICE and private nodes is intentional, as they require the same set of hooks but with different defaults. They argue that extending ZONE_DEVICE into these areas would be cumbersome and inefficient, and that the current implementation is a more elegant solution.\n\nAuthor agrees that NODE_DATA is a better interface than per-page pgmap, and explains that one driver can manage multiple devices with the same numa node if it uses the same owner context.\n\nThe author is addressing concerns about implementing mempolicy support for N_MEMORY_PRIVATE, specifically how to handle ZONE_DEVICE NUMA UAPI and the lack of free lists, managed_pages, or watermarks in Zone Device. The author acknowledges that solving LRU would not be sufficient and presents two options: either put pages in the buddy or add pgmap->device_alloc() callbacks at every allocation site.\n\nAuthor acknowledged that using the buddy allocator is a design choice to simplify mm/ services, and explained that injecting hooks into every surface or modifying get_page_from_freelist would be more complex alternatives.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a fix is needed",
                "proposes an alternative solution",
                "acknowledges fix needed",
                "agreed",
                "explained",
                "presents alternatives",
                "acknowledged"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree that buddy-access alone is insufficient justification, it\nstarted off that way - but if you want mempolicy/NUMA UAPI access,\nit turns into \"Re-use all of MM\" - and that means using the buddy.\n\nI also expected ZONE_DEVICE vs NODE_DATA to be the primary discussion,\n\nI raise replacing it as a thought experiment, but not the proposal.\n\nThe idea that drm/ is going to switch to private nodes is outside the\nrealm of reality, but part of that is because of years of infrastructure\nbuilt on the assumption that re-using mm/ is infeasible.\n\nBut, lets talk about DEVICE_COHERENT\n\n---\n\nDEVICE_COHERENT is the odd-man out among ZONE_DEVICE modes. The others\nuse softleaf entries and don't allow direct mappings.\n\n(DEVICE_PRIVATE sort of does if you squint, but you can also view that\n a bit like PROT_NONE or read-only controls to force migrations).\n\nIf you take DEVICE_COHERENT and:\n\n- Move pgmap out of the struct page (page_ext, NODE_DATA, etc) to free\n  the LRU list_head\n- Put pages in the buddy (free lists, watermarks, managed_pages) or add\n  pgmap->device_alloc() at every allocation callsite / buddy hook\n- Add LRU support (aging, reclaim, compaction)\n- Add isolated gating (new GFP flag and adjusted zonelist filtering)\n- Add new dev_pagemap_ops callbacks for the various mm/ features\n- Audit evey folio_is_zone_device() to distinguish zone device modes\n\n... you've built N_MEMORY_PRIVATE inside ZONE_DEVICE. Except now\npage_zone(page) returns ZONE_DEVICE - so you inherit the wrong\ndefaults at every existing ZONE_DEVICE check. \n\nSkip-sites become things to opt-out of instead of opting into.\n\nYou just end up with\n\nif (folio_is_zone_device(folio))\n    if (folio_is_my_special_zone_device())\n    else ....\n\nand this just generalizes to\n\nif (folio_is_private_managed(folio))\n    folio_managed_my_hooked_operation()\n\nSo you get the same code, but have added more complexity to ZONE_DEVICE.\n\nI don't think that's needed if we just recognize ZONE is the wrong\nabstraction to be operating on.\n\nHonestly, even ZONE_MOVABLE becomes pointless with N_MEMORY_PRIVATE\nif you disallow longterm pinning - because the managing service handles\nallocations (it has to inject GFP_PRIVATE to get access) or selectively\nenables the mm/ services it knows are safe (mempolicy).\n\nEven if you allow longterm pinning, if your service controls what does\nthe pinning it can still be reclaimable - just manually (killing\nprocesses) instead of letting hotplug do it via migration.\n\nIf your service only allocates movable pages - your ZONE_NORMAL is\neffectively ZONE_MOVABLE.  \n\nIn some cases we use ZONE_MOVABLE to prevent the kernel from allocating\nmemory onto devices (like CXL).  This means struct page is forced to\ntake up DRAM or use memmap_on_memory - meaning you lose high-value\ncapacity or sacrifice contiguity (less huge page support).\n\nThis entire problem can evaporate if you can just use ZONE_NORMAL.\n\nThere are a lot of benefits to just re-using the buddy like this.\n\nZones are the wrong abstraction and cause more problems.\n\n---\n\nYou don't have to squint because it was deliberate :]\n\nThe callback similarity is the feature - they're the same logical\noperations.  The difference is the direction of the defaults.\n\nExtending ZONE_DEVICE into these areas requires the same set of hooks,\nplus distinguishing \"old ZONE_DEVICE\" from \"new ZONE_DEVICE\".\n\nWhere there are new injection sites, it's because ZONE_DEVICE opts\nout of ever touching that code in some other silently implied way.\n\nFor example, reclaim/compaction doesn't run because ZONE_DEVICE doesn't\nadd to managed_pages (among other reasons).\n\nYou'd have to go figure out how to hack those things into ZONE_DEVICE \n*and then* opt every *other* ZONE_DEVICE mode *back out*.\n\nSo you still end up with something like this anyway:\n\nstatic inline bool folio_managed_handle_fault(struct folio *folio,\n                                              struct vm_fault *vmf,\n                                              enum pgtable_level level,\n                                              vm_fault_t *ret)\n{\n        /* Zone device pages use swap entries; handled in do_swap_page */\n        if (folio_is_zone_device(folio))\n                return false;\n\n        if (folio_is_private_node(folio))\n\t\t...\n        return false;\n}\n\n---\n\nIf NUMA is the interface we want, then NODE_DATA is the right direction\nregardless of struct page's future or what zone it lives in.\n\nThere's no reason to keep per-page pgmap w/ device-to-node mappings.\n\nYou can have one driver manage multiple devices with the same numa node\nif it uses the same owner context (PFN already differentiates devices).\n\nThe existing code allows for this.\n\n---\n\nOn (1): ZONE_DEVICE NUMA UAPI is harder than it looks from the surface\n\nMuch of the kernel mm/ infrastructure is written on top of the buddy and\nexpects N_MEMORY to be the sole arbiter of \"Where to Acquire Pages\".\n\nMempolicy depends on:\n   - Buddy support or a new alloc hook around the buddy\n\n   - Migration support (mbind() after allocation migrates)\n     - Migration also deeply assumes buddy and LRU support\n\n   - Changing validations on node states\n     - mempolicy checks N_MEMORY membership, so you have to hack\n       N_MEMORY onto ZONE_DEVICE\n       (or teach it about a new node state... N_MEMORY_PRIVATE)\n\n\nGetting mempolicy to work with N_MEMORY_PRIVATE amounts to adding 2\nlines of code in vma_alloc_folio_noprof:\n\nstruct folio *vma_alloc_folio_noprof(gfp_t gfp, int order,\n                                     struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr)\n{\n        if (pol->flags & MPOL_F_PRIVATE)\n                gfp |= __GFP_PRIVATE;\n\n        folio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n\t/* Woo! I faulted a DEVICE PAGE! */\n}\n\nBut this requires the pages to be managed by the buddy.\n\nThe rest of the mempolicy support is around keeping sane nodemasks when\nthings like cpuset.mems rebinds occur and validating you don't end up\nwith private nodes that don't support mempolicy in your nodemask.\n\nYou have to do all of this anyway, but with the added bonus of fighting\nwith the overloaded nature of ZONE_DEVICE at every step.\n\n==========\n\nOn (2): Assume you solve LRU. \n\nZone Device has no free lists, managed_pages, or watermarks.\n\nkswapd can't run, compaction has no targets, vmscan's pressure model\ndoesn't function.  These all come for free when the pages are\nbuddy-managed on a real zone.  Why re-invent the wheel?\n\n==========\n\nSo you really have two options here:\n\na) Put pages in the buddy, or\n\nb) Add pgmap->device_alloc() callbacks at every allocation site that\n   could target a node:\n     - vma_alloc_folio\n     - alloc_migration_target\n     - alloc_demote_folio\n     - alloc_pages_node\n     - alloc_contig_pages\n     - list goes on\n\nOr more likely - hooking get_page_from_freelist.  Which at that\npoint... just use the buddy?  You're already deep in the hot path.\n\n---\n\nUsing the buddy underpins the rest of mm/ services we want to re-use.\n\nThat's basically it.  Otherwise you have to inject hooks into every\nsurface that touches the buddy...\n\n... or in the buddy (get_page_from_freelist), at which point why not\njust use the buddy?\n\n~Gregory",
              "reply_to": "Alistair Popple",
              "message_date": "2026-02-24",
              "message_id": "aZ3BEn_73Rk8Fn7L@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author considered reviewer feedback about introducing N_MEMORY_PRIVATE and decided to explore an alternative approach, checking NODE_DATA(target_nid)->private instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "considering_an_alternative",
                "will_look_at_it"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This gave me something to chew on\n\nI think this can be done without introducing N_MEMORY_PRIVATE and just\nchecking:   NODE_DATA(target_nid)->private\n\nmeaning these nodes can just be N_MEMORY with the same isolations.\n\nI'll look at this a bit more.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "aZ3X3Jni0HZXZMVl@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v2 2/2] cxl/region: Test CXL_DECODER_F_NORMALIZED_ADDRESSING as a bitmask",
          "message_id": "aZzGURaa8aHKqreA@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZzGURaa8aHKqreA@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T21:27:48Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Two patches from Alison Schofield fix issues in the CXL driver by replacing test_bit() with bitmask checks for flags defined as bitmasks.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "Reviewed and tested both patches with a Reviewed-by and Tested-by tag.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "APPROVED"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by",
                "Tested-by"
              ],
              "raw_body": "On Mon, Feb 23, 2026 at 11:13:39AM -0800, Alison Schofield wrote:\n> The CXL decoder flags are defined as bitmasks, not bit indices.\n> Using test_bit() to check them interprets the mask value as a bit\n> index, which is the wrong test.\n> \n> For CXL_DECODER_F_LOCK the test reads beyond the defined bits, causing\n> the test to always return false and allowing resets that should have\n> been blocked.\n> \n> Replace test_bit() with a bitmask check.\n> \n> Fixes: 2230c4bdc412 (\"cxl: Add handling of locked CXL decoder\")\n> Signed-off-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n\nOp, missed the v4 and responded to v1\n\nReviewed-by: Gregory Price <gourry@gourry.net>\nTested-by: Gregory Price <gourry@gourry.net>\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Applied both patches to cxl/fixes.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "APPLIED"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\n\nOn 2/23/26 12:13 PM, Alison Schofield wrote:\n> The CXL decoder flags are defined as bitmasks, not bit indices.\n> Using test_bit() to check them interprets the mask value as a bit\n> index, which is the wrong test.\n> \n> For CXL_DECODER_F_LOCK the test reads beyond the defined bits, causing\n> the test to always return false and allowing resets that should have\n> been blocked.\n> \n> Replace test_bit() with a bitmask check.\n> \n> Fixes: 2230c4bdc412 (\"cxl: Add handling of locked CXL decoder\")\n> Signed-off-by: Alison Schofield <alison.schofield@intel.com>\n\nApplied to cxl/fixes\n8c0025ee29f03d74d34b77127192b9db8457564c\n\n> ---\n> \n> Changes in v2:\n> Split patches to align w Fixes Tags for backport ease\n> Rebased on 7.0-rc1\n> \n>  drivers/cxl/core/hdm.c    | 2 +-\n>  drivers/cxl/core/region.c | 2 +-\n>  2 files changed, 2 insertions(+), 2 deletions(-)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index e3f0c39e6812..c222e98ae736 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -904,7 +904,7 @@ static void cxl_decoder_reset(struct cxl_decoder *cxld)\n>  \tif ((cxld->flags & CXL_DECODER_F_ENABLE) == 0)\n>  \t\treturn;\n>  \n> -\tif (test_bit(CXL_DECODER_F_LOCK, &cxld->flags))\n> +\tif (cxld->flags & CXL_DECODER_F_LOCK)\n>  \t\treturn;\n>  \n>  \tif (port->commit_end == id)\n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index fec37af1dfbf..780ec947ecf2 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -1100,7 +1100,7 @@ static int cxl_rr_assign_decoder(struct cxl_port *port, struct cxl_region *cxlr,\n>  static void cxl_region_setup_flags(struct cxl_region *cxlr,\n>  \t\t\t\t   struct cxl_decoder *cxld)\n>  {\n> -\tif (test_bit(CXL_DECODER_F_LOCK, &cxld->flags)) {\n> +\tif (cxld->flags & CXL_DECODER_F_LOCK) {\n>  \t\tset_bit(CXL_REGION_F_LOCK, &cxlr->flags);\n>  \t\tclear_bit(CXL_REGION_F_NEEDS_RESET, &cxlr->flags);\n>  \t}\n> \n> base-commit: 6de23f81a5e08be8fbf5e8d7e9febc72a5b5f27f\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v2 1/2] cxl: Test CXL_DECODER_F_LOCK as a bitmask",
          "message_id": "aZzGNiMPuU-Jphou@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZzGNiMPuU-Jphou@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T21:27:21Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Two patches to fix incorrect bitmask checks in CXL decoder flags, replacing test_bit() with bitwise AND operations.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "Provided Reviewed-by and Tested-by tags for both patches without raising any concerns or objections.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "APPROVAL"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by",
                "Tested-by"
              ],
              "raw_body": "On Mon, Feb 23, 2026 at 11:13:39AM -0800, Alison Schofield wrote:\n> The CXL decoder flags are defined as bitmasks, not bit indices.\n> Using test_bit() to check them interprets the mask value as a bit\n> index, which is the wrong test.\n> \n> For CXL_DECODER_F_LOCK the test reads beyond the defined bits, causing\n> the test to always return false and allowing resets that should have\n> been blocked.\n> \n> Replace test_bit() with a bitmask check.\n> \n> Fixes: 2230c4bdc412 (\"cxl: Add handling of locked CXL decoder\")\n> Signed-off-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n\nOp, missed the v4 and responded to v1\n\nReviewed-by: Gregory Price <gourry@gourry.net>\nTested-by: Gregory Price <gourry@gourry.net>\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Applied both patches to cxl/fixes without raising any concerns or objections.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "APPROVAL"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\n\nOn 2/23/26 12:13 PM, Alison Schofield wrote:\n> The CXL decoder flags are defined as bitmasks, not bit indices.\n> Using test_bit() to check them interprets the mask value as a bit\n> index, which is the wrong test.\n> \n> For CXL_DECODER_F_LOCK the test reads beyond the defined bits, causing\n> the test to always return false and allowing resets that should have\n> been blocked.\n> \n> Replace test_bit() with a bitmask check.\n> \n> Fixes: 2230c4bdc412 (\"cxl: Add handling of locked CXL decoder\")\n> Signed-off-by: Alison Schofield <alison.schofield@intel.com>\n\nApplied to cxl/fixes\n8c0025ee29f03d74d34b77127192b9db8457564c\n\n> ---\n> \n> Changes in v2:\n> Split patches to align w Fixes Tags for backport ease\n> Rebased on 7.0-rc1\n> \n>  drivers/cxl/core/hdm.c    | 2 +-\n>  drivers/cxl/core/region.c | 2 +-\n>  2 files changed, 2 insertions(+), 2 deletions(-)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index e3f0c39e6812..c222e98ae736 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -904,7 +904,7 @@ static void cxl_decoder_reset(struct cxl_decoder *cxld)\n>  \tif ((cxld->flags & CXL_DECODER_F_ENABLE) == 0)\n>  \t\treturn;\n>  \n> -\tif (test_bit(CXL_DECODER_F_LOCK, &cxld->flags))\n> +\tif (cxld->flags & CXL_DECODER_F_LOCK)\n>  \t\treturn;\n>  \n>  \tif (port->commit_end == id)\n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index fec37af1dfbf..780ec947ecf2 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -1100,7 +1100,7 @@ static int cxl_rr_assign_decoder(struct cxl_port *port, struct cxl_region *cxlr,\n>  static void cxl_region_setup_flags(struct cxl_region *cxlr,\n>  \t\t\t\t   struct cxl_decoder *cxld)\n>  {\n> -\tif (test_bit(CXL_DECODER_F_LOCK, &cxld->flags)) {\n> +\tif (cxld->flags & CXL_DECODER_F_LOCK) {\n>  \t\tset_bit(CXL_REGION_F_LOCK, &cxlr->flags);\n>  \t\tclear_bit(CXL_REGION_F_NEEDS_RESET, &cxlr->flags);\n>  \t}\n> \n> base-commit: 6de23f81a5e08be8fbf5e8d7e9febc72a5b5f27f\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH] cxl: Test decoder flags as bitmasks",
          "message_id": "aZy0EtERdCpGn4gF@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZy0EtERdCpGn4gF@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-23T20:09:57Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Alison Schofield (author)",
              "summary": "Author acknowledged that the original patch needs to be split into two patches due to a dependency and agreed to do so.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "OK - will do, ignore this patch.\n\nIt'll be a 2 patch series because of the dependency, but I believe that\nis still preferred by backport folks.",
              "reply_to": "Dave Jiang",
              "message_date": "2026-02-09",
              "message_id": "aYoLS2u-EJCdOv6K@aschofie-mobl2.lan",
              "analysis_source": "llm"
            },
            {
              "author": "Davidlohr Bueso",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "Alison Schofield",
              "message_date": "2026-02-17",
              "message_id": "20260217222505.p4wguawtfmdlrokg@offworld",
              "analysis_source": "heuristic"
            },
            {
              "author": "Gregory Price",
              "summary": "Gave Reviewed-by, Tested-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by",
                "Tested-by"
              ],
              "raw_body": "",
              "reply_to": "Alison Schofield",
              "message_date": "2026-02-23",
              "message_id": "aZy0EtERdCpGn4gF@gourry-fedora-PF4VCD3F",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Jeff Layton",
      "primary_email": "jlayton@kernel.org",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 0/4] sunrpc: cache infrastructure scalability improvements",
          "message_id": "20260223-sunrpc-cache-v2-0-91fc827c4d33@kernel.org",
          "url": "https://lore.kernel.org/all/20260223-sunrpc-cache-v2-0-91fc827c4d33@kernel.org/",
          "date": "2026-02-23T17:10:17Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series improves the scalability of the sunrpc cache infrastructure by converting a global spinlock and waitqueue to per-cache-detail locks, simplifying the code and reducing contention. It also fixes a pre-existing bug that could cause cache requests to leak, and introduces a new sequence number to help readers track their position in the queue.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about cache_request leak in cache_release() by adding cleanup logic to decrement readers and check if it reached 0 with CACHE_PENDING clear, then dequeue and free the cache_request. A fix is planned for v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "fix_planned"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a reader's file descriptor is closed while in the middle of reading\na cache_request (rp->offset != 0), cache_release() decrements the\nrequest's readers count but never checks whether it should free the\nrequest.\n\nIn cache_read(), when readers drops to 0 and CACHE_PENDING is clear, the\ncache_request is removed from the queue and freed along with its buffer\nand cache_head reference. cache_release() lacks this cleanup.\n\nThe only other path that frees requests with readers == 0 is\ncache_dequeue(), but it runs only when CACHE_PENDING transitions from\nset to clear. If that transition already happened while readers was\nstill non-zero, cache_dequeue() will have skipped the request, and no\nsubsequent call will clean it up.\n\nAdd the same cleanup logic from cache_read() to cache_release(): after\ndecrementing readers, check if it reached 0 with CACHE_PENDING clear,\nand if so, dequeue and free the cache_request.\n\nReported-by: NeilBrown <neilb@ownmail.net>\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n net/sunrpc/cache.c | 26 +++++++++++++++++++++-----\n 1 file changed, 21 insertions(+), 5 deletions(-)\n\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex b82f7cde0c9be6071ee4040150672872e548161d..86b3fd5a429d77f7f917f398a02cb7a5ff8dd1e0 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -1062,14 +1062,25 @@ static int cache_release(struct inode *inode, struct file *filp,\n \tstruct cache_reader *rp = filp->private_data;\n \n \tif (rp) {\n+\t\tstruct cache_request *rq = NULL;\n+\n \t\tspin_lock(&queue_lock);\n \t\tif (rp->offset) {\n \t\t\tstruct cache_queue *cq;\n-\t\t\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t\t\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n+\t\t\tfor (cq = &rp->q; &cq->list != &cd->queue;\n+\t\t\t     cq = list_entry(cq->list.next,\n+\t\t\t\t\t     struct cache_queue, list))\n \t\t\t\tif (!cq->reader) {\n-\t\t\t\t\tcontainer_of(cq, struct cache_request, q)\n-\t\t\t\t\t\t->readers--;\n+\t\t\t\t\tstruct cache_request *cr =\n+\t\t\t\t\t\tcontainer_of(cq,\n+\t\t\t\t\t\tstruct cache_request, q);\n+\t\t\t\t\tcr->readers--;\n+\t\t\t\t\tif (cr->readers == 0 &&\n+\t\t\t\t\t    !test_bit(CACHE_PENDING,\n+\t\t\t\t\t\t      &cr->item->flags)) {\n+\t\t\t\t\t\tlist_del(&cr->q.list);\n+\t\t\t\t\t\trq = cr;\n+\t\t\t\t\t}\n \t\t\t\t\tbreak;\n \t\t\t\t}\n \t\t\trp->offset = 0;\n@@ -1077,9 +1088,14 @@ static int cache_release(struct inode *inode, struct file *filp,\n \t\tlist_del(&rp->q.list);\n \t\tspin_unlock(&queue_lock);\n \n+\t\tif (rq) {\n+\t\t\tcache_put(rq->item, cd);\n+\t\t\tkfree(rq->buf);\n+\t\t\tkfree(rq);\n+\t\t}\n+\n \t\tfilp->private_data = NULL;\n \t\tkfree(rp);\n-\n \t}\n \tif (filp->f_mode & FMODE_WRITE) {\n \t\tatomic_dec(&cd->writers);\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "20260223-sunrpc-cache-v2-1-91fc827c4d33@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about the global queue_lock serializing upcall queue operations across all cache_detail instances. They agreed to convert it to a per-cache-detail spinlock so that different caches no longer contend with each other on queue operations.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The global queue_lock serializes all upcall queue operations across\nevery cache_detail instance. Convert it to a per-cache-detail spinlock\nso that different caches (e.g. auth.unix.ip vs nfsd.fh) no longer\ncontend with each other on queue operations.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h |  1 +\n net/sunrpc/cache.c           | 47 ++++++++++++++++++++++----------------------\n 2 files changed, 24 insertions(+), 24 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex e783132e481ff2593fdc5d323f7b3a08f85d4cd8..3d32dd1f7b05d35562d2064fed69877b3950fb51 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -113,6 +113,7 @@ struct cache_detail {\n \n \t/* fields for communication over channel */\n \tstruct list_head\tqueue;\n+\tspinlock_t\t\tqueue_lock;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex 86b3fd5a429d77f7f917f398a02cb7a5ff8dd1e0..1cfaae488c6c67a9797511804e4bbba16bcc70ae 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -400,6 +400,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)\n {\n \tspin_lock_init(&cd->hash_lock);\n \tINIT_LIST_HEAD(&cd->queue);\n+\tspin_lock_init(&cd->queue_lock);\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -803,8 +804,6 @@ void cache_clean_deferred(void *owner)\n  *\n  */\n \n-static DEFINE_SPINLOCK(queue_lock);\n-\n struct cache_queue {\n \tstruct list_head\tlist;\n \tint\t\t\treader;\t/* if 0, then request */\n@@ -847,7 +846,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tinode_lock(inode); /* protect against multiple concurrent\n \t\t\t      * readers on this file */\n  again:\n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \t/* need to find next request */\n \twhile (rp->q.list.next != &cd->queue &&\n \t       list_entry(rp->q.list.next, struct cache_queue, list)\n@@ -856,7 +855,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\tlist_move(&rp->q.list, next);\n \t}\n \tif (rp->q.list.next == &cd->queue) {\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t\tinode_unlock(inode);\n \t\tWARN_ON_ONCE(rp->offset);\n \t\treturn 0;\n@@ -865,7 +864,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tWARN_ON_ONCE(rq->q.reader);\n \tif (rp->offset == 0)\n \t\trq->readers++;\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \n \tif (rq->len == 0) {\n \t\terr = cache_request(cd, rq);\n@@ -876,9 +875,9 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \n \tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\terr = -EAGAIN;\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t} else {\n \t\tif (rp->offset + count > rq->len)\n \t\t\tcount = rq->len - rp->offset;\n@@ -888,26 +887,26 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trp->offset += count;\n \t\tif (rp->offset >= rq->len) {\n \t\t\trp->offset = 0;\n-\t\t\tspin_lock(&queue_lock);\n+\t\t\tspin_lock(&cd->queue_lock);\n \t\t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t\t}\n \t\terr = 0;\n \t}\n  out:\n \tif (rp->offset == 0) {\n \t\t/* need to release rq */\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\trq->readers--;\n \t\tif (rq->readers == 0 &&\n \t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\t\tlist_del(&rq->q.list);\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t\t\tcache_put(rq->item, cd);\n \t\t\tkfree(rq->buf);\n \t\t\tkfree(rq);\n \t\t} else\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (err == -EAGAIN)\n \t\tgoto again;\n@@ -988,7 +987,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \tif (!rp)\n \t\treturn mask;\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \n \tfor (cq= &rp->q; &cq->list != &cd->queue;\n \t     cq = list_entry(cq->list.next, struct cache_queue, list))\n@@ -996,7 +995,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n \t\t\tbreak;\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \treturn mask;\n }\n \n@@ -1011,7 +1010,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \tif (cmd != FIONREAD || !rp)\n \t\treturn -EINVAL;\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \n \t/* only find the length remaining in current request,\n \t * or the length of the next request\n@@ -1024,7 +1023,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \t\t\tlen = cr->len - rp->offset;\n \t\t\tbreak;\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \n \treturn put_user(len, (int __user *)arg);\n }\n@@ -1046,9 +1045,9 @@ static int cache_open(struct inode *inode, struct file *filp,\n \t\trp->offset = 0;\n \t\trp->q.reader = 1;\n \n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tlist_add(&rp->q.list, &cd->queue);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (filp->f_mode & FMODE_WRITE)\n \t\tatomic_inc(&cd->writers);\n@@ -1064,7 +1063,7 @@ static int cache_release(struct inode *inode, struct file *filp,\n \tif (rp) {\n \t\tstruct cache_request *rq = NULL;\n \n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tif (rp->offset) {\n \t\t\tstruct cache_queue *cq;\n \t\t\tfor (cq = &rp->q; &cq->list != &cd->queue;\n@@ -1086,7 +1085,7 @@ static int cache_release(struct inode *inode, struct file *filp,\n \t\t\trp->offset = 0;\n \t\t}\n \t\tlist_del(&rp->q.list);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \n \t\tif (rq) {\n \t\t\tcache_put(rq->item, cd);\n@@ -1113,7 +1112,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n \tstruct cache_request *cr;\n \tLIST_HEAD(dequeued);\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&detail->queue_lock);\n \tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n \t\tif (!cq->reader) {\n \t\t\tcr = container_of(cq, struct cache_request, q);\n@@ -1126,7 +1125,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n \t\t\t\tcontinue;\n \t\t\tlist_move(&cr->q.list, &dequeued);\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&detail->queue_lock);\n \twhile (!list_empty(&dequeued)) {\n \t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n \t\tlist_del(&cr->q.list);\n@@ -1251,7 +1250,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \tcrq->buf = buf;\n \tcrq->len = 0;\n \tcrq->readers = 0;\n-\tspin_lock(&queue_lock);\n+\tspin_lock(&detail->queue_lock);\n \tif (test_bit(CACHE_PENDING, &h->flags)) {\n \t\tcrq->item = cache_get(h);\n \t\tlist_add_tail(&crq->q.list, &detail->queue);\n@@ -1259,7 +1258,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t} else\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n \t\tret = -EAGAIN;\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&detail->queue_lock);\n \twake_up(&queue_wait);\n \tif (ret == -EAGAIN) {\n \t\tkfree(buf);\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "20260223-sunrpc-cache-v2-2-91fc827c4d33@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author is addressing a concern about the queue_wait waitqueue being global, which wakes pollers on all caches when one cache_detail is woken. The author agrees that this is an issue and has converted it to a per-cache-detail field so only relevant pollers are woken.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged the problem",
                "provided a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The queue_wait waitqueue is currently a file-scoped global, so a\nwake_up for one cache_detail wakes pollers on all caches. Convert it\nto a per-cache-detail field so that only pollers on the relevant cache\nare woken.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h | 2 ++\n net/sunrpc/cache.c           | 7 +++----\n 2 files changed, 5 insertions(+), 4 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex 3d32dd1f7b05d35562d2064fed69877b3950fb51..031379efba24d40f64ce346cf1032261d4b98d05 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -16,6 +16,7 @@\n #include <linux/atomic.h>\n #include <linux/kstrtox.h>\n #include <linux/proc_fs.h>\n+#include <linux/wait.h>\n \n /*\n  * Each cache requires:\n@@ -114,6 +115,7 @@ struct cache_detail {\n \t/* fields for communication over channel */\n \tstruct list_head\tqueue;\n \tspinlock_t\t\tqueue_lock;\n+\twait_queue_head_t\tqueue_wait;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex 1cfaae488c6c67a9797511804e4bbba16bcc70ae..fd02dca1f07afec2f09c591037bac3ea3e8d7e17 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -401,6 +401,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)\n \tspin_lock_init(&cd->hash_lock);\n \tINIT_LIST_HEAD(&cd->queue);\n \tspin_lock_init(&cd->queue_lock);\n+\tinit_waitqueue_head(&cd->queue_wait);\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -970,8 +971,6 @@ static ssize_t cache_write(struct file *filp, const char __user *buf,\n \treturn ret;\n }\n \n-static DECLARE_WAIT_QUEUE_HEAD(queue_wait);\n-\n static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \t\t\t       struct cache_detail *cd)\n {\n@@ -979,7 +978,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \tstruct cache_reader *rp = filp->private_data;\n \tstruct cache_queue *cq;\n \n-\tpoll_wait(filp, &queue_wait, wait);\n+\tpoll_wait(filp, &cd->queue_wait, wait);\n \n \t/* alway allow write */\n \tmask = EPOLLOUT | EPOLLWRNORM;\n@@ -1259,7 +1258,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n \t\tret = -EAGAIN;\n \tspin_unlock(&detail->queue_lock);\n-\twake_up(&queue_wait);\n+\twake_up(&detail->queue_wait);\n \tif (ret == -EAGAIN) {\n \t\tkfree(buf);\n \t\tkfree(crq);\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "20260223-sunrpc-cache-v2-3-91fc827c4d33@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about the complexity of the reader-skipping loops in cache_read, cache_poll, and cache_ioctl by replacing the single interleaved queue with two dedicated lists for upcall requests and open file handles. The readers now track their position via a monotonically increasing sequence number (next_seqno) rather than by their position in the shared list. A new helper function cache_next_request() finds the next request at or after a given seqno, eliminating the need for the cache_queue wrapper struct.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged",
                "agreed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace the single interleaved queue (which mixed cache_request and\ncache_reader entries distinguished by a ->reader flag) with two\ndedicated lists: cd->requests for upcall requests and cd->readers\nfor open file handles.\n\nReaders now track their position via a monotonically increasing\nsequence number (next_seqno) rather than by their position in the\nshared list. Each cache_request is assigned a seqno when enqueued,\nand a new cache_next_request() helper finds the next request at or\nafter a given seqno.\n\nThis eliminates the cache_queue wrapper struct entirely, simplifies\nthe reader-skipping loops in cache_read/cache_poll/cache_ioctl/\ncache_release, and makes the data flow easier to reason about.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h |   4 +-\n net/sunrpc/cache.c           | 143 ++++++++++++++++++-------------------------\n 2 files changed, 62 insertions(+), 85 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex 031379efba24d40f64ce346cf1032261d4b98d05..b1e595c2615bd4be4d9ad19f71a8f4d08bd74a9b 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -113,9 +113,11 @@ struct cache_detail {\n \tint\t\t\tentries;\n \n \t/* fields for communication over channel */\n-\tstruct list_head\tqueue;\n+\tstruct list_head\trequests;\n+\tstruct list_head\treaders;\n \tspinlock_t\t\tqueue_lock;\n \twait_queue_head_t\tqueue_wait;\n+\tu64\t\t\tnext_seqno;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex fd02dca1f07afec2f09c591037bac3ea3e8d7e17..7081c1214e6c3226f8ac82c8bc7ff6c36f598744 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -399,9 +399,11 @@ static struct delayed_work cache_cleaner;\n void sunrpc_init_cache_detail(struct cache_detail *cd)\n {\n \tspin_lock_init(&cd->hash_lock);\n-\tINIT_LIST_HEAD(&cd->queue);\n+\tINIT_LIST_HEAD(&cd->requests);\n+\tINIT_LIST_HEAD(&cd->readers);\n \tspin_lock_init(&cd->queue_lock);\n \tinit_waitqueue_head(&cd->queue_wait);\n+\tcd->next_seqno = 0;\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -796,29 +798,20 @@ void cache_clean_deferred(void *owner)\n  * On read, you get a full request, or block.\n  * On write, an update request is processed.\n  * Poll works if anything to read, and always allows write.\n- *\n- * Implemented by linked list of requests.  Each open file has\n- * a ->private that also exists in this list.  New requests are added\n- * to the end and may wakeup and preceding readers.\n- * New readers are added to the head.  If, on read, an item is found with\n- * CACHE_UPCALLING clear, we free it from the list.\n- *\n  */\n \n-struct cache_queue {\n-\tstruct list_head\tlist;\n-\tint\t\t\treader;\t/* if 0, then request */\n-};\n struct cache_request {\n-\tstruct cache_queue\tq;\n+\tstruct list_head\tlist;\n \tstruct cache_head\t*item;\n-\tchar\t\t\t* buf;\n+\tchar\t\t\t*buf;\n \tint\t\t\tlen;\n \tint\t\t\treaders;\n+\tu64\t\t\tseqno;\n };\n struct cache_reader {\n-\tstruct cache_queue\tq;\n+\tstruct list_head\tlist;\n \tint\t\t\toffset;\t/* if non-0, we have a refcnt on next request */\n+\tu64\t\t\tnext_seqno;\n };\n \n static int cache_request(struct cache_detail *detail,\n@@ -833,6 +826,17 @@ static int cache_request(struct cache_detail *detail,\n \treturn PAGE_SIZE - len;\n }\n \n+static struct cache_request *\n+cache_next_request(struct cache_detail *cd, u64 seqno)\n+{\n+\tstruct cache_request *rq;\n+\n+\tlist_for_each_entry(rq, &cd->requests, list)\n+\t\tif (rq->seqno >= seqno)\n+\t\t\treturn rq;\n+\treturn NULL;\n+}\n+\n static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\t\t  loff_t *ppos, struct cache_detail *cd)\n {\n@@ -849,20 +853,13 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n  again:\n \tspin_lock(&cd->queue_lock);\n \t/* need to find next request */\n-\twhile (rp->q.list.next != &cd->queue &&\n-\t       list_entry(rp->q.list.next, struct cache_queue, list)\n-\t       ->reader) {\n-\t\tstruct list_head *next = rp->q.list.next;\n-\t\tlist_move(&rp->q.list, next);\n-\t}\n-\tif (rp->q.list.next == &cd->queue) {\n+\trq = cache_next_request(cd, rp->next_seqno);\n+\tif (!rq) {\n \t\tspin_unlock(&cd->queue_lock);\n \t\tinode_unlock(inode);\n \t\tWARN_ON_ONCE(rp->offset);\n \t\treturn 0;\n \t}\n-\trq = container_of(rp->q.list.next, struct cache_request, q.list);\n-\tWARN_ON_ONCE(rq->q.reader);\n \tif (rp->offset == 0)\n \t\trq->readers++;\n \tspin_unlock(&cd->queue_lock);\n@@ -876,9 +873,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \n \tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\terr = -EAGAIN;\n-\t\tspin_lock(&cd->queue_lock);\n-\t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\tspin_unlock(&cd->queue_lock);\n+\t\trp->next_seqno = rq->seqno + 1;\n \t} else {\n \t\tif (rp->offset + count > rq->len)\n \t\t\tcount = rq->len - rp->offset;\n@@ -888,9 +883,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trp->offset += count;\n \t\tif (rp->offset >= rq->len) {\n \t\t\trp->offset = 0;\n-\t\t\tspin_lock(&cd->queue_lock);\n-\t\t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\t\tspin_unlock(&cd->queue_lock);\n+\t\t\trp->next_seqno = rq->seqno + 1;\n \t\t}\n \t\terr = 0;\n \t}\n@@ -901,7 +894,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trq->readers--;\n \t\tif (rq->readers == 0 &&\n \t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n-\t\t\tlist_del(&rq->q.list);\n+\t\t\tlist_del(&rq->list);\n \t\t\tspin_unlock(&cd->queue_lock);\n \t\t\tcache_put(rq->item, cd);\n \t\t\tkfree(rq->buf);\n@@ -976,7 +969,6 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n {\n \t__poll_t mask;\n \tstruct cache_reader *rp = filp->private_data;\n-\tstruct cache_queue *cq;\n \n \tpoll_wait(filp, &cd->queue_wait, wait);\n \n@@ -988,12 +980,8 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \n \tspin_lock(&cd->queue_lock);\n \n-\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\tif (!cq->reader) {\n-\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n-\t\t\tbreak;\n-\t\t}\n+\tif (cache_next_request(cd, rp->next_seqno))\n+\t\tmask |= EPOLLIN | EPOLLRDNORM;\n \tspin_unlock(&cd->queue_lock);\n \treturn mask;\n }\n@@ -1004,7 +992,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n {\n \tint len = 0;\n \tstruct cache_reader *rp = filp->private_data;\n-\tstruct cache_queue *cq;\n+\tstruct cache_request *rq;\n \n \tif (cmd != FIONREAD || !rp)\n \t\treturn -EINVAL;\n@@ -1014,14 +1002,9 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \t/* only find the length remaining in current request,\n \t * or the length of the next request\n \t */\n-\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\tif (!cq->reader) {\n-\t\t\tstruct cache_request *cr =\n-\t\t\t\tcontainer_of(cq, struct cache_request, q);\n-\t\t\tlen = cr->len - rp->offset;\n-\t\t\tbreak;\n-\t\t}\n+\trq = cache_next_request(cd, rp->next_seqno);\n+\tif (rq)\n+\t\tlen = rq->len - rp->offset;\n \tspin_unlock(&cd->queue_lock);\n \n \treturn put_user(len, (int __user *)arg);\n@@ -1042,10 +1025,10 @@ static int cache_open(struct inode *inode, struct file *filp,\n \t\t\treturn -ENOMEM;\n \t\t}\n \t\trp->offset = 0;\n-\t\trp->q.reader = 1;\n+\t\trp->next_seqno = 0;\n \n \t\tspin_lock(&cd->queue_lock);\n-\t\tlist_add(&rp->q.list, &cd->queue);\n+\t\tlist_add(&rp->list, &cd->readers);\n \t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (filp->f_mode & FMODE_WRITE)\n@@ -1064,26 +1047,21 @@ static int cache_release(struct inode *inode, struct file *filp,\n \n \t\tspin_lock(&cd->queue_lock);\n \t\tif (rp->offset) {\n-\t\t\tstruct cache_queue *cq;\n-\t\t\tfor (cq = &rp->q; &cq->list != &cd->queue;\n-\t\t\t     cq = list_entry(cq->list.next,\n-\t\t\t\t\t     struct cache_queue, list))\n-\t\t\t\tif (!cq->reader) {\n-\t\t\t\t\tstruct cache_request *cr =\n-\t\t\t\t\t\tcontainer_of(cq,\n-\t\t\t\t\t\tstruct cache_request, q);\n-\t\t\t\t\tcr->readers--;\n-\t\t\t\t\tif (cr->readers == 0 &&\n-\t\t\t\t\t    !test_bit(CACHE_PENDING,\n-\t\t\t\t\t\t      &cr->item->flags)) {\n-\t\t\t\t\t\tlist_del(&cr->q.list);\n-\t\t\t\t\t\trq = cr;\n-\t\t\t\t\t}\n-\t\t\t\t\tbreak;\n+\t\t\tstruct cache_request *cr;\n+\n+\t\t\tcr = cache_next_request(cd, rp->next_seqno);\n+\t\t\tif (cr) {\n+\t\t\t\tcr->readers--;\n+\t\t\t\tif (cr->readers == 0 &&\n+\t\t\t\t    !test_bit(CACHE_PENDING,\n+\t\t\t\t\t      &cr->item->flags)) {\n+\t\t\t\t\tlist_del(&cr->list);\n+\t\t\t\t\trq = cr;\n \t\t\t\t}\n+\t\t\t}\n \t\t\trp->offset = 0;\n \t\t}\n-\t\tlist_del(&rp->q.list);\n+\t\tlist_del(&rp->list);\n \t\tspin_unlock(&cd->queue_lock);\n \n \t\tif (rq) {\n@@ -1107,27 +1085,24 @@ static int cache_release(struct inode *inode, struct file *filp,\n \n static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n {\n-\tstruct cache_queue *cq, *tmp;\n-\tstruct cache_request *cr;\n+\tstruct cache_request *cr, *tmp;\n \tLIST_HEAD(dequeued);\n \n \tspin_lock(&detail->queue_lock);\n-\tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n-\t\tif (!cq->reader) {\n-\t\t\tcr = container_of(cq, struct cache_request, q);\n-\t\t\tif (cr->item != ch)\n-\t\t\t\tcontinue;\n-\t\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n-\t\t\t\t/* Lost a race and it is pending again */\n-\t\t\t\tbreak;\n-\t\t\tif (cr->readers != 0)\n-\t\t\t\tcontinue;\n-\t\t\tlist_move(&cr->q.list, &dequeued);\n-\t\t}\n+\tlist_for_each_entry_safe(cr, tmp, &detail->requests, list) {\n+\t\tif (cr->item != ch)\n+\t\t\tcontinue;\n+\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n+\t\t\t/* Lost a race and it is pending again */\n+\t\t\tbreak;\n+\t\tif (cr->readers != 0)\n+\t\t\tcontinue;\n+\t\tlist_move(&cr->list, &dequeued);\n+\t}\n \tspin_unlock(&detail->queue_lock);\n \twhile (!list_empty(&dequeued)) {\n-\t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n-\t\tlist_del(&cr->q.list);\n+\t\tcr = list_entry(dequeued.next, struct cache_request, list);\n+\t\tlist_del(&cr->list);\n \t\tcache_put(cr->item, detail);\n \t\tkfree(cr->buf);\n \t\tkfree(cr);\n@@ -1245,14 +1220,14 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t\treturn -EAGAIN;\n \t}\n \n-\tcrq->q.reader = 0;\n \tcrq->buf = buf;\n \tcrq->len = 0;\n \tcrq->readers = 0;\n \tspin_lock(&detail->queue_lock);\n \tif (test_bit(CACHE_PENDING, &h->flags)) {\n \t\tcrq->item = cache_get(h);\n-\t\tlist_add_tail(&crq->q.list, &detail->queue);\n+\t\tcrq->seqno = detail->next_seqno++;\n+\t\tlist_add_tail(&crq->list, &detail->requests);\n \t\ttrace_cache_entry_upcall(detail, h);\n \t} else\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "20260223-sunrpc-cache-v2-4-91fc827c4d33@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Chuck Lever",
              "summary": "reviewer noted that the patch does not address the issue of cache_detail->queue being accessed concurrently by multiple threads, and requested a lock be added to protect this access\n\nreviewer noted that the per-cache-detail lock ordering is still a concern, specifically when vswap_free() acquires the spinlock while holding the folio lock, and requested further investigation into this issue",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concurrency",
                "thread safety",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nOn Mon, 23 Feb 2026 12:09:57 -0500, Jeff Layton wrote:\n\n---\n\nApplied to nfsd-testing, replacing v1. Thanks!\n\n[1/4] sunrpc: fix cache_request leak in cache_release\n      commit: dad5f78046759eb5c95970198eb9865550eb6227\n[2/4] sunrpc: convert queue_lock from global spinlock to per-cache-detail lock\n      commit: c94ad34b7ecd5928cf3fdb6ea4fcf6ef55765e97\n[3/4] sunrpc: convert queue_wait from global to per-cache-detail waitqueue\n      commit: 951696964e9c370a5f91d5e3e136d39aa08d912c\n[4/4] sunrpc: split cache_detail queue into request and reader lists\n      commit: 3557b9c71039b2435b383fc57283a0b847b40144\n\n--\nChuck Lever",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-23",
              "message_id": "177188092388.32759.9563088581417995762.b4-ty@oracle.com",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/3] sunrpc: cache infrastructure scalability improvements",
          "message_id": "20260220-sunrpc-cache-v1-0-47d04014c245@kernel.org",
          "url": "https://lore.kernel.org/all/20260220-sunrpc-cache-v1-0-47d04014c245@kernel.org/",
          "date": "2026-02-20T12:26:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-20",
          "patch_summary": "This patch series improves the scalability of the sunrpc cache infrastructure by converting a global spinlock and waitqueue to per-cache-detail locks and waitqueues, and splitting the cache detail queue into two separate lists for requests and readers.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about the global queue_lock serializing upcall queue operations across all cache_detail instances. They agreed to convert it to a per-cache_detail spinlock so that different caches no longer contend with each other on queue operations.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The global queue_lock serializes all upcall queue operations across\nevery cache_detail instance. Convert it to a per-cache_detail spinlock\nso that different caches (e.g. auth.unix.ip vs nfsd.fh) no longer\ncontend with each other on queue operations.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h |  1 +\n net/sunrpc/cache.c           | 47 ++++++++++++++++++++++----------------------\n 2 files changed, 24 insertions(+), 24 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex e783132e481ff2593fdc5d323f7b3a08f85d4cd8..3d32dd1f7b05d35562d2064fed69877b3950fb51 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -113,6 +113,7 @@ struct cache_detail {\n \n \t/* fields for communication over channel */\n \tstruct list_head\tqueue;\n+\tspinlock_t\t\tqueue_lock;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex 7c73d1c39687343db02d1f1423b58213b7a35f42..6add2fe311425dc3aec63efce2c4bed06a3d3ba5 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -400,6 +400,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)\n {\n \tspin_lock_init(&cd->hash_lock);\n \tINIT_LIST_HEAD(&cd->queue);\n+\tspin_lock_init(&cd->queue_lock);\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -803,8 +804,6 @@ void cache_clean_deferred(void *owner)\n  *\n  */\n \n-static DEFINE_SPINLOCK(queue_lock);\n-\n struct cache_queue {\n \tstruct list_head\tlist;\n \tint\t\t\treader;\t/* if 0, then request */\n@@ -847,7 +846,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tinode_lock(inode); /* protect against multiple concurrent\n \t\t\t      * readers on this file */\n  again:\n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \t/* need to find next request */\n \twhile (rp->q.list.next != &cd->queue &&\n \t       list_entry(rp->q.list.next, struct cache_queue, list)\n@@ -856,7 +855,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\tlist_move(&rp->q.list, next);\n \t}\n \tif (rp->q.list.next == &cd->queue) {\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t\tinode_unlock(inode);\n \t\tWARN_ON_ONCE(rp->offset);\n \t\treturn 0;\n@@ -865,7 +864,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tWARN_ON_ONCE(rq->q.reader);\n \tif (rp->offset == 0)\n \t\trq->readers++;\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \n \tif (rq->len == 0) {\n \t\terr = cache_request(cd, rq);\n@@ -876,9 +875,9 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \n \tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\terr = -EAGAIN;\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t} else {\n \t\tif (rp->offset + count > rq->len)\n \t\t\tcount = rq->len - rp->offset;\n@@ -888,26 +887,26 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trp->offset += count;\n \t\tif (rp->offset >= rq->len) {\n \t\t\trp->offset = 0;\n-\t\t\tspin_lock(&queue_lock);\n+\t\t\tspin_lock(&cd->queue_lock);\n \t\t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t\t}\n \t\terr = 0;\n \t}\n  out:\n \tif (rp->offset == 0) {\n \t\t/* need to release rq */\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\trq->readers--;\n \t\tif (rq->readers == 0 &&\n \t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\t\tlist_del(&rq->q.list);\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t\t\tcache_put(rq->item, cd);\n \t\t\tkfree(rq->buf);\n \t\t\tkfree(rq);\n \t\t} else\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (err == -EAGAIN)\n \t\tgoto again;\n@@ -988,7 +987,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \tif (!rp)\n \t\treturn mask;\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \n \tfor (cq= &rp->q; &cq->list != &cd->queue;\n \t     cq = list_entry(cq->list.next, struct cache_queue, list))\n@@ -996,7 +995,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n \t\t\tbreak;\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \treturn mask;\n }\n \n@@ -1011,7 +1010,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \tif (cmd != FIONREAD || !rp)\n \t\treturn -EINVAL;\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \n \t/* only find the length remaining in current request,\n \t * or the length of the next request\n@@ -1024,7 +1023,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \t\t\tlen = cr->len - rp->offset;\n \t\t\tbreak;\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \n \treturn put_user(len, (int __user *)arg);\n }\n@@ -1046,9 +1045,9 @@ static int cache_open(struct inode *inode, struct file *filp,\n \t\trp->offset = 0;\n \t\trp->q.reader = 1;\n \n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tlist_add(&rp->q.list, &cd->queue);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (filp->f_mode & FMODE_WRITE)\n \t\tatomic_inc(&cd->writers);\n@@ -1062,7 +1061,7 @@ static int cache_release(struct inode *inode, struct file *filp,\n \tstruct cache_reader *rp = filp->private_data;\n \n \tif (rp) {\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tif (rp->offset) {\n \t\t\tstruct cache_queue *cq;\n \t\t\tfor (cq= &rp->q; &cq->list != &cd->queue;\n@@ -1075,7 +1074,7 @@ static int cache_release(struct inode *inode, struct file *filp,\n \t\t\trp->offset = 0;\n \t\t}\n \t\tlist_del(&rp->q.list);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \n \t\tfilp->private_data = NULL;\n \t\tkfree(rp);\n@@ -1097,7 +1096,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n \tstruct cache_request *cr;\n \tLIST_HEAD(dequeued);\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&detail->queue_lock);\n \tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n \t\tif (!cq->reader) {\n \t\t\tcr = container_of(cq, struct cache_request, q);\n@@ -1110,7 +1109,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n \t\t\t\tcontinue;\n \t\t\tlist_move(&cr->q.list, &dequeued);\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&detail->queue_lock);\n \twhile (!list_empty(&dequeued)) {\n \t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n \t\tlist_del(&cr->q.list);\n@@ -1235,7 +1234,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \tcrq->buf = buf;\n \tcrq->len = 0;\n \tcrq->readers = 0;\n-\tspin_lock(&queue_lock);\n+\tspin_lock(&detail->queue_lock);\n \tif (test_bit(CACHE_PENDING, &h->flags)) {\n \t\tcrq->item = cache_get(h);\n \t\tlist_add_tail(&crq->q.list, &detail->queue);\n@@ -1243,7 +1242,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t} else\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n \t\tret = -EAGAIN;\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&detail->queue_lock);\n \twake_up(&queue_wait);\n \tif (ret == -EAGAIN) {\n \t\tkfree(buf);\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "20260220-sunrpc-cache-v1-1-47d04014c245@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about the queue_wait waitqueue being global and waking pollers on all caches, explaining that converting it to a per-cache_detail field will only wake pollers on the relevant cache.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged feedback",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The queue_wait waitqueue is currently a file-scoped global, so a\nwake_up for one cache_detail wakes pollers on all caches. Convert it\nto a per-cache_detail field so that only pollers on the relevant cache\nare woken.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h | 2 ++\n net/sunrpc/cache.c           | 7 +++----\n 2 files changed, 5 insertions(+), 4 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex 3d32dd1f7b05d35562d2064fed69877b3950fb51..031379efba24d40f64ce346cf1032261d4b98d05 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -16,6 +16,7 @@\n #include <linux/atomic.h>\n #include <linux/kstrtox.h>\n #include <linux/proc_fs.h>\n+#include <linux/wait.h>\n \n /*\n  * Each cache requires:\n@@ -114,6 +115,7 @@ struct cache_detail {\n \t/* fields for communication over channel */\n \tstruct list_head\tqueue;\n \tspinlock_t\t\tqueue_lock;\n+\twait_queue_head_t\tqueue_wait;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex 6add2fe311425dc3aec63efce2c4bed06a3d3ba5..aef2607b3d7ffb61a42b9ea2ec17947465c026dc 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -401,6 +401,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)\n \tspin_lock_init(&cd->hash_lock);\n \tINIT_LIST_HEAD(&cd->queue);\n \tspin_lock_init(&cd->queue_lock);\n+\tinit_waitqueue_head(&cd->queue_wait);\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -970,8 +971,6 @@ static ssize_t cache_write(struct file *filp, const char __user *buf,\n \treturn ret;\n }\n \n-static DECLARE_WAIT_QUEUE_HEAD(queue_wait);\n-\n static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \t\t\t       struct cache_detail *cd)\n {\n@@ -979,7 +978,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \tstruct cache_reader *rp = filp->private_data;\n \tstruct cache_queue *cq;\n \n-\tpoll_wait(filp, &queue_wait, wait);\n+\tpoll_wait(filp, &cd->queue_wait, wait);\n \n \t/* alway allow write */\n \tmask = EPOLLOUT | EPOLLWRNORM;\n@@ -1243,7 +1242,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n \t\tret = -EAGAIN;\n \tspin_unlock(&detail->queue_lock);\n-\twake_up(&queue_wait);\n+\twake_up(&detail->queue_wait);\n \tif (ret == -EAGAIN) {\n \t\tkfree(buf);\n \t\tkfree(crq);\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "20260220-sunrpc-cache-v1-2-47d04014c245@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about the complexity of the reader-skipping loops in cache_read/cache_poll/cache_ioctl/cache_release by explaining that they have simplified these loops and made the data flow easier to reason about by using two dedicated lists (cd->requests for upcalls and cd->readers for open file handles) and a monotonically increasing sequence number (next_seqno).",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "author provided explanation",
                "author confirmed simplification of code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace the single interleaved queue (which mixed cache_request and\ncache_reader entries distinguished by a ->reader flag) with two\ndedicated lists: cd->requests for upcall requests and cd->readers\nfor open file handles.\n\nReaders now track their position via a monotonically increasing\nsequence number (next_seqno) rather than by their position in the\nshared list. Each cache_request is assigned a seqno when enqueued,\nand a new cache_next_request() helper finds the next request at or\nafter a given seqno.\n\nThis eliminates the cache_queue wrapper struct entirely, simplifies\nthe reader-skipping loops in cache_read/cache_poll/cache_ioctl/\ncache_release, and makes the data flow easier to reason about.\n\nAlso, remove an obsolete comment. CACHE_UPCALLING hasn't existed\nsince before the git era started.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h |   4 +-\n net/sunrpc/cache.c           | 125 ++++++++++++++++++-------------------------\n 2 files changed, 56 insertions(+), 73 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex 031379efba24d40f64ce346cf1032261d4b98d05..b1e595c2615bd4be4d9ad19f71a8f4d08bd74a9b 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -113,9 +113,11 @@ struct cache_detail {\n \tint\t\t\tentries;\n \n \t/* fields for communication over channel */\n-\tstruct list_head\tqueue;\n+\tstruct list_head\trequests;\n+\tstruct list_head\treaders;\n \tspinlock_t\t\tqueue_lock;\n \twait_queue_head_t\tqueue_wait;\n+\tu64\t\t\tnext_seqno;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex aef2607b3d7ffb61a42b9ea2ec17947465c026dc..09389ce8b961fe0cb5a472bcf2d3dd0b3faa13a6 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -399,9 +399,11 @@ static struct delayed_work cache_cleaner;\n void sunrpc_init_cache_detail(struct cache_detail *cd)\n {\n \tspin_lock_init(&cd->hash_lock);\n-\tINIT_LIST_HEAD(&cd->queue);\n+\tINIT_LIST_HEAD(&cd->requests);\n+\tINIT_LIST_HEAD(&cd->readers);\n \tspin_lock_init(&cd->queue_lock);\n \tinit_waitqueue_head(&cd->queue_wait);\n+\tcd->next_seqno = 0;\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -796,29 +798,20 @@ void cache_clean_deferred(void *owner)\n  * On read, you get a full request, or block.\n  * On write, an update request is processed.\n  * Poll works if anything to read, and always allows write.\n- *\n- * Implemented by linked list of requests.  Each open file has\n- * a ->private that also exists in this list.  New requests are added\n- * to the end and may wakeup and preceding readers.\n- * New readers are added to the head.  If, on read, an item is found with\n- * CACHE_UPCALLING clear, we free it from the list.\n- *\n  */\n \n-struct cache_queue {\n-\tstruct list_head\tlist;\n-\tint\t\t\treader;\t/* if 0, then request */\n-};\n struct cache_request {\n-\tstruct cache_queue\tq;\n+\tstruct list_head\tlist;\n \tstruct cache_head\t*item;\n-\tchar\t\t\t* buf;\n+\tchar\t\t\t*buf;\n \tint\t\t\tlen;\n \tint\t\t\treaders;\n+\tu64\t\t\tseqno;\n };\n struct cache_reader {\n-\tstruct cache_queue\tq;\n+\tstruct list_head\tlist;\n \tint\t\t\toffset;\t/* if non-0, we have a refcnt on next request */\n+\tu64\t\t\tnext_seqno;\n };\n \n static int cache_request(struct cache_detail *detail,\n@@ -833,6 +826,17 @@ static int cache_request(struct cache_detail *detail,\n \treturn PAGE_SIZE - len;\n }\n \n+static struct cache_request *\n+cache_next_request(struct cache_detail *cd, u64 seqno)\n+{\n+\tstruct cache_request *rq;\n+\n+\tlist_for_each_entry(rq, &cd->requests, list)\n+\t\tif (rq->seqno >= seqno)\n+\t\t\treturn rq;\n+\treturn NULL;\n+}\n+\n static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\t\t  loff_t *ppos, struct cache_detail *cd)\n {\n@@ -849,20 +853,13 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n  again:\n \tspin_lock(&cd->queue_lock);\n \t/* need to find next request */\n-\twhile (rp->q.list.next != &cd->queue &&\n-\t       list_entry(rp->q.list.next, struct cache_queue, list)\n-\t       ->reader) {\n-\t\tstruct list_head *next = rp->q.list.next;\n-\t\tlist_move(&rp->q.list, next);\n-\t}\n-\tif (rp->q.list.next == &cd->queue) {\n+\trq = cache_next_request(cd, rp->next_seqno);\n+\tif (!rq) {\n \t\tspin_unlock(&cd->queue_lock);\n \t\tinode_unlock(inode);\n \t\tWARN_ON_ONCE(rp->offset);\n \t\treturn 0;\n \t}\n-\trq = container_of(rp->q.list.next, struct cache_request, q.list);\n-\tWARN_ON_ONCE(rq->q.reader);\n \tif (rp->offset == 0)\n \t\trq->readers++;\n \tspin_unlock(&cd->queue_lock);\n@@ -877,7 +874,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\terr = -EAGAIN;\n \t\tspin_lock(&cd->queue_lock);\n-\t\tlist_move(&rp->q.list, &rq->q.list);\n+\t\trp->next_seqno = rq->seqno + 1;\n \t\tspin_unlock(&cd->queue_lock);\n \t} else {\n \t\tif (rp->offset + count > rq->len)\n@@ -889,7 +886,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\tif (rp->offset >= rq->len) {\n \t\t\trp->offset = 0;\n \t\t\tspin_lock(&cd->queue_lock);\n-\t\t\tlist_move(&rp->q.list, &rq->q.list);\n+\t\t\trp->next_seqno = rq->seqno + 1;\n \t\t\tspin_unlock(&cd->queue_lock);\n \t\t}\n \t\terr = 0;\n@@ -901,7 +898,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trq->readers--;\n \t\tif (rq->readers == 0 &&\n \t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n-\t\t\tlist_del(&rq->q.list);\n+\t\t\tlist_del(&rq->list);\n \t\t\tspin_unlock(&cd->queue_lock);\n \t\t\tcache_put(rq->item, cd);\n \t\t\tkfree(rq->buf);\n@@ -976,7 +973,6 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n {\n \t__poll_t mask;\n \tstruct cache_reader *rp = filp->private_data;\n-\tstruct cache_queue *cq;\n \n \tpoll_wait(filp, &cd->queue_wait, wait);\n \n@@ -988,12 +984,8 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \n \tspin_lock(&cd->queue_lock);\n \n-\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\tif (!cq->reader) {\n-\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n-\t\t\tbreak;\n-\t\t}\n+\tif (cache_next_request(cd, rp->next_seqno))\n+\t\tmask |= EPOLLIN | EPOLLRDNORM;\n \tspin_unlock(&cd->queue_lock);\n \treturn mask;\n }\n@@ -1004,7 +996,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n {\n \tint len = 0;\n \tstruct cache_reader *rp = filp->private_data;\n-\tstruct cache_queue *cq;\n+\tstruct cache_request *rq;\n \n \tif (cmd != FIONREAD || !rp)\n \t\treturn -EINVAL;\n@@ -1014,14 +1006,9 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \t/* only find the length remaining in current request,\n \t * or the length of the next request\n \t */\n-\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\tif (!cq->reader) {\n-\t\t\tstruct cache_request *cr =\n-\t\t\t\tcontainer_of(cq, struct cache_request, q);\n-\t\t\tlen = cr->len - rp->offset;\n-\t\t\tbreak;\n-\t\t}\n+\trq = cache_next_request(cd, rp->next_seqno);\n+\tif (rq)\n+\t\tlen = rq->len - rp->offset;\n \tspin_unlock(&cd->queue_lock);\n \n \treturn put_user(len, (int __user *)arg);\n@@ -1042,10 +1029,10 @@ static int cache_open(struct inode *inode, struct file *filp,\n \t\t\treturn -ENOMEM;\n \t\t}\n \t\trp->offset = 0;\n-\t\trp->q.reader = 1;\n+\t\trp->next_seqno = 0;\n \n \t\tspin_lock(&cd->queue_lock);\n-\t\tlist_add(&rp->q.list, &cd->queue);\n+\t\tlist_add(&rp->list, &cd->readers);\n \t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (filp->f_mode & FMODE_WRITE)\n@@ -1062,17 +1049,14 @@ static int cache_release(struct inode *inode, struct file *filp,\n \tif (rp) {\n \t\tspin_lock(&cd->queue_lock);\n \t\tif (rp->offset) {\n-\t\t\tstruct cache_queue *cq;\n-\t\t\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t\t\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\t\t\tif (!cq->reader) {\n-\t\t\t\t\tcontainer_of(cq, struct cache_request, q)\n-\t\t\t\t\t\t->readers--;\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n+\t\t\tstruct cache_request *rq;\n+\n+\t\t\trq = cache_next_request(cd, rp->next_seqno);\n+\t\t\tif (rq)\n+\t\t\t\trq->readers--;\n \t\t\trp->offset = 0;\n \t\t}\n-\t\tlist_del(&rp->q.list);\n+\t\tlist_del(&rp->list);\n \t\tspin_unlock(&cd->queue_lock);\n \n \t\tfilp->private_data = NULL;\n@@ -1091,27 +1075,24 @@ static int cache_release(struct inode *inode, struct file *filp,\n \n static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n {\n-\tstruct cache_queue *cq, *tmp;\n-\tstruct cache_request *cr;\n+\tstruct cache_request *cr, *tmp;\n \tLIST_HEAD(dequeued);\n \n \tspin_lock(&detail->queue_lock);\n-\tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n-\t\tif (!cq->reader) {\n-\t\t\tcr = container_of(cq, struct cache_request, q);\n-\t\t\tif (cr->item != ch)\n-\t\t\t\tcontinue;\n-\t\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n-\t\t\t\t/* Lost a race and it is pending again */\n-\t\t\t\tbreak;\n-\t\t\tif (cr->readers != 0)\n-\t\t\t\tcontinue;\n-\t\t\tlist_move(&cr->q.list, &dequeued);\n-\t\t}\n+\tlist_for_each_entry_safe(cr, tmp, &detail->requests, list) {\n+\t\tif (cr->item != ch)\n+\t\t\tcontinue;\n+\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n+\t\t\t/* Lost a race and it is pending again */\n+\t\t\tbreak;\n+\t\tif (cr->readers != 0)\n+\t\t\tcontinue;\n+\t\tlist_move(&cr->list, &dequeued);\n+\t}\n \tspin_unlock(&detail->queue_lock);\n \twhile (!list_empty(&dequeued)) {\n-\t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n-\t\tlist_del(&cr->q.list);\n+\t\tcr = list_entry(dequeued.next, struct cache_request, list);\n+\t\tlist_del(&cr->list);\n \t\tcache_put(cr->item, detail);\n \t\tkfree(cr->buf);\n \t\tkfree(cr);\n@@ -1229,14 +1210,14 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t\treturn -EAGAIN;\n \t}\n \n-\tcrq->q.reader = 0;\n \tcrq->buf = buf;\n \tcrq->len = 0;\n \tcrq->readers = 0;\n \tspin_lock(&detail->queue_lock);\n \tif (test_bit(CACHE_PENDING, &h->flags)) {\n \t\tcrq->item = cache_get(h);\n-\t\tlist_add_tail(&crq->q.list, &detail->queue);\n+\t\tcrq->seqno = detail->next_seqno++;\n+\t\tlist_add_tail(&crq->list, &detail->requests);\n \t\ttrace_cache_entry_upcall(detail, h);\n \t} else\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "20260220-sunrpc-cache-v1-3-47d04014c245@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Chuck Lever",
              "summary": "Reviewer noted that the patch does not address the issue of cache_detail->seq being accessed without seqlock protection, which could lead to a data corruption bug.\n\nreviewer noted that the patch does not address the issue of cache_detail->queue being accessed concurrently by multiple threads, and requested additional locking to prevent data corruption",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "data corruption",
                "missing lock protection",
                "concern about concurrency",
                "request for additional locking"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nOn Fri, 20 Feb 2026 07:26:02 -0500, Jeff Layton wrote:\n\n---\n\nApplied to nfsd-testing, thanks!\n\n[1/3] sunrpc: convert queue_lock from global spinlock to per-cache_detail lock\n      commit: 8da8f32e9a2702259cdf97e2f8f492ef9c79db65\n[2/3] sunrpc: convert queue_wait from global to per-cache_detail waitqueue\n      commit: 802261d8b58dd2f41a52a0c92776e0fb45619efe\n[3/3] sunrpc: split cache_detail queue into request and reader lists\n      commit: 0eb3d9dc71ada02909e4dfe9cb54e703ec717ed4\n\n--\nChuck Lever",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-20",
              "message_id": "177161622290.3877966.16867844436002593841.b4-ty@oracle.com",
              "analysis_source": "llm"
            },
            {
              "author": "NeilBrown",
              "summary": "reviewer noted that the code should also decrement ->readers and check if it's zero, similar to another part of the codebase, potentially fixing a bug introduced by the patch",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential bug fix",
                "code review"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hmm..  The other place where we decrement ->readers we then check if it\nis zero and if CACHE_PENDING is clear - and do something.\nI suspect we should do that here.\nThis bug (if I'm right and it is a bug) if there before you patch, but\nnow might be a good time to fix it?\n\nThanks.  Nice cleanups.\n\nNeilBrown",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-22",
              "message_id": "177171367423.8396.10176251932730619714@noble.neil.brown.name",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "Author acknowledged a missing patch to address NeilBrown's feedback and plans to add it before the existing patches\n\nAuthor acknowledged that there were issues with the spinlocks in the patch, agreed to fix them in version 2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "plans to add a new patch",
                "acknowledged issues",
                "agreed to fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Good catch. I'll add a patch to fix that preceding these patches, so it\ncan go to stable (if we think that's worthwhile).\n\n---\n\nThanks for the review! I'll send a v2 with the spinlock fixes and the\nabove bugfix.",
              "reply_to": "NeilBrown",
              "message_date": "2026-02-23",
              "message_id": "7c0e019cbf7371bdf47bd7a7c48df132fc5b87fd.camel@kernel.org",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] Add support for empty path in openat and openat2 syscalls",
          "message_id": "44a2111e33631d78aded73e4b79908db6237227f.camel@kernel.org",
          "url": "https://lore.kernel.org/all/44a2111e33631d78aded73e4b79908db6237227f.camel@kernel.org/",
          "date": "2026-02-23T15:28:27Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "Reviewer suggested that the O_EMPTY_PATH flag should be an OPENAT2_* flag instead, citing recent discussion around O_REGULAR flag",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This sounds valuable, but there was recent discussion around the\nO_REGULAR flag that said that we shouldn't be adding new flags to older\nsyscalls [1]. Should this only be an OPENAT2_* flag instead?\n\n[1]: https://lore.kernel.org/linux-fsdevel/20260129-siebzehn-adler-efe74ff8f1a9@brauner/",
              "reply_to": "Jori Koolstra",
              "message_date": "2026-02-23",
              "message_id": "44a2111e33631d78aded73e4b79908db6237227f.camel@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "David Laight",
              "summary": "Reviewer noted that the O_EMPTY_PATH flag is not properly handled in the build_open_flags function, which does not set LOOKUP_EMPTY when O_EMPTY_PATH is specified.\n\nReviewer David Laight questioned the usefulness of allowing empty path in openat and openat2 syscalls for files, citing security concerns and lack of clear use cases.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "flag handling issue",
                "security concern",
                "lack of clear use case"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Mon, 23 Feb 2026 16:16:52 +0100\nJori Koolstra <jkoolstra@xs4all.nl> wrote:\n\n---\n\nWhat do you want the fd for?\nThere are good reasons for wanting an fd for a directory that you don't\nhave access permissions for, but what is the use case for a file?\n\nIt all gets very close to letting you do things that the 'security model'\nshould reject.\n\n\tDavid",
              "reply_to": "Jori Koolstra",
              "message_date": "2026-02-23",
              "message_id": "20260223164511.525762fb@pumpkin",
              "analysis_source": "llm"
            },
            {
              "author": "Christian Brauner",
              "summary": "The reviewer questioned the origin of the O_EMPTY_PATH flag, pointing out that it was taken from a uapi-group list without explicit mention in the commit message.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "inquiry"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Out of curiosity, did you pick this taken from our uapi-group list?\n\nhttps://github.com/uapi-group/kernel-features?tab=readme-ov-file#at_empty_path-support-for-openat-and-openat2\nhttps://github.com/uapi-group/kernel-features/issues/47\n\n?",
              "reply_to": "Jori Koolstra",
              "message_date": "2026-02-24",
              "message_id": "20260224-vorfuhr-spitzen-783550d623a2@brauner",
              "analysis_source": "llm"
            },
            {
              "author": "Jori Koolstra (author)",
              "summary": "Author acknowledged a mistake in their original patch description, stating they should have mentioned the UAPI list, but no fix is planned.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a mistake",
                "no fix planned"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, I took the advice you gave me at FOSDEM to heart :) . I should have maybe\nmentioned that this patch is in reference to the UAPI list, but forgot that in\nthe patch message and did not want to send another email about it.\n\nBest,\nJori.",
              "reply_to": "Christian Brauner",
              "message_date": "2026-02-24",
              "message_id": "1215721492.1952426.1771939986592@kpc.webmail.kpnmail.nl",
              "analysis_source": "llm"
            },
            {
              "author": "Jori Koolstra (author)",
              "summary": "Author is asking for clarification on how to handle the new O_EMPTY_PATH flag, suggesting either filtering it from openat() or adding a RESOLVE_EMPTY flag to resolve options.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "So would you want to filter the O_EMPTY_PATH flag from openat(), or maybe add\na RESOLVE_EMPTY flag to the resolve options?\n\nThanks,\nJori.",
              "reply_to": "Christian Brauner",
              "message_date": "2026-02-24",
              "message_id": "695828658.1952887.1771940100883@kpc.webmail.kpnmail.nl",
              "analysis_source": "llm"
            },
            {
              "author": "Christian Brauner",
              "summary": "The reviewer suggested adding an OPENAT2_EMPTY_PATH flag in the upper 32 bits of the 64-bit flag argument for struct open_how, allowing it to be used only with openat2() and not openat().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, add a OPENAT2_EMPTY_PATH in the upper 32 bit of the 64-bit flag\nargument for struct open_how. Then it cannot be used in openat(). But\nlet's wait a day or so to see whether we have someone that really wants\nto extend this to openat() as well...",
              "reply_to": "Jori Koolstra",
              "message_date": "2026-02-24",
              "message_id": "20260224-einquartieren-lahmen-aa7e0203f917@brauner",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/1] NFSD: Expose callback statistics in /proc/net/rpc/nfsd",
          "message_id": "84bbbe173485c6cbd0af9169e55717be0aa0e367.camel@kernel.org",
          "url": "https://lore.kernel.org/all/84bbbe173485c6cbd0af9169e55717be0aa0e367.camel@kernel.org/",
          "date": "2026-02-23T14:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Chuck Lever",
              "summary": "reviewer noted that the patch introduces a potential deadlock between nfsd4_run_cb() and nfsd4_show_cb_stats() due to shared use of nn->client_lock, and requested the lock be dropped before calling rpc_clnt_show_stats()\n\nReviewer Chuck Lever requested justification for modifying the /proc interface, assuming the intention was to expose the new metrics in an existing NFS administrative tool such as nfsstat.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential deadlock",
                "shared lock",
                "requested justification",
                "assuming intention"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hello Dai!\n\nOn Sat, Feb 21, 2026, at 4:57 PM, Dai Ngo wrote:\n\n---\n\nThe commit message needs to justify why you are modifying a legacy\n/proc interface. I assume it is because you want these metrics to\nappear in an existing NFS administrative tool like nfsstat ?",
              "reply_to": "Dai Ngo",
              "message_date": "2026-02-21",
              "message_id": "8d11898b-9889-43b5-bb96-445870367949@app.fastmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "NeilBrown",
              "summary": "Reviewer NeilBrown questioned the decision to expose system-wide callback statistics, suggesting that per-net-namespace counters would be more accurate and useful.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Why system-wide rather than per-net-namespace?\n\nNeilBrown",
              "reply_to": "Dai Ngo",
              "message_date": "2026-02-22",
              "message_id": "177173152164.8396.12929618094338409157@noble.neil.brown.name",
              "analysis_source": "llm"
            },
            {
              "author": "Dai Ngo (author)",
              "summary": "Author Dai Ngo responded to feedback about displaying NFSd statistics, stating they were unaware that /proc is a legacy interface and asking for guidance on the proper way to display these statistics.\n\nAuthor acknowledged that the existing NFS daemon maintains backchannel statistics, but they are not easily accessible; explained that the current patch adds these stats to /proc/net/rpc/nfsd for developer use only and does not plan to extend nfsstat(8) to report them.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "lack of knowledge",
                "request for clarification",
                "acknowledged existing implementation",
                "explained design choice"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oh I did not know /proc is a legacy interface. What is the proper\nway to display nfsd statistics?\n\n---\n\nI should have made this clearer in the commit message.\n\nnfsd already maintains backchannel statistics, but there's currently\nno convenient way to view them during normal operation - developers\ntypically have to inspect a vmcore.\n\nThis patch addresses that gap by adding the backchannel statistics to\nthe existing output of /proc/net/rpc/nfsd.\n\nAt this time, I don't plan to extend nfsstat(8) to report these statistics,\nas this patch is intended for developer use only.\n\n-Dai",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-22",
              "message_id": "831ee3d3-4d5d-4b63-80e6-51d1e5907666@oracle.com",
              "analysis_source": "llm"
            },
            {
              "author": "Dai Ngo (author)",
              "summary": "Author acknowledged that the callback statistics counters are embedded in a globally defined structure, cb_program, rather than in nfsd_net, and asked for clarification on whether this is the correct terminology.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification requested"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "These counters are currently embedded in cb_program which is defined\nglobally and not in nfsd_net. Am i using the wrong terminology?\n\n-Dai",
              "reply_to": "NeilBrown",
              "message_date": "2026-02-22",
              "message_id": "687f1398-698b-4646-b9d4-24fbe77d7241@oracle.com",
              "analysis_source": "llm"
            },
            {
              "author": "NeilBrown",
              "summary": "Reviewer NeilBrown noted that the patch exports global callback statistics, which he believes is inappropriate and suggested converting the statistics collection to per-net-namespace instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sorry - I hadn't properly processed that these are existing statistics\nand you are only exporting them.\n\nSo I change my comment to: please don't do that.  I don't think it is\nappropriate to export global statistics.  We should only export\nper-net-namespace statistics.\n\nSo place convert the statistics collect to per-net-namespace, then\nexport them.\n\nThanks,\nNeilBrown",
              "reply_to": "Dai Ngo",
              "message_date": "2026-02-23",
              "message_id": "177180401604.8396.3300860214801483447@noble.neil.brown.name",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "reviewer suggested using netlink commands instead of /proc/net/rpc/nfsd to expose callback statistics, citing its extensibility and ease of use compared to proc files",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "We have a new netlink commands for nfsd that give us request status. It\nwould be good to either extend that or add a new command that allows\nstatistics. Netlink is much better suited for this since it's a lot\neasier to extend than dealing with procfiles.",
              "reply_to": "Dai Ngo",
              "message_date": "2026-02-23",
              "message_id": "84bbbe173485c6cbd0af9169e55717be0aa0e367.camel@kernel.org",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 3/3] sunrpc: split cache_detail queue into request and reader lists",
          "message_id": "7c0e019cbf7371bdf47bd7a7c48df132fc5b87fd.camel@kernel.org",
          "url": "https://lore.kernel.org/all/7c0e019cbf7371bdf47bd7a7c48df132fc5b87fd.camel@kernel.org/",
          "date": "2026-02-23T14:08:00Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Jeff Layton (author)",
              "summary": "The author is addressing a concern about contention between different caches on queue operations. They are converting the global queue_lock to a per-cache_detail spinlock, which will serialize upcall queue operations across every cache_detail instance separately.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The global queue_lock serializes all upcall queue operations across\nevery cache_detail instance. Convert it to a per-cache_detail spinlock\nso that different caches (e.g. auth.unix.ip vs nfsd.fh) no longer\ncontend with each other on queue operations.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h |  1 +\n net/sunrpc/cache.c           | 47 ++++++++++++++++++++++----------------------\n 2 files changed, 24 insertions(+), 24 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex e783132e481ff2593fdc5d323f7b3a08f85d4cd8..3d32dd1f7b05d35562d2064fed69877b3950fb51 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -113,6 +113,7 @@ struct cache_detail {\n \n \t/* fields for communication over channel */\n \tstruct list_head\tqueue;\n+\tspinlock_t\t\tqueue_lock;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex 7c73d1c39687343db02d1f1423b58213b7a35f42..6add2fe311425dc3aec63efce2c4bed06a3d3ba5 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -400,6 +400,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)\n {\n \tspin_lock_init(&cd->hash_lock);\n \tINIT_LIST_HEAD(&cd->queue);\n+\tspin_lock_init(&cd->queue_lock);\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -803,8 +804,6 @@ void cache_clean_deferred(void *owner)\n  *\n  */\n \n-static DEFINE_SPINLOCK(queue_lock);\n-\n struct cache_queue {\n \tstruct list_head\tlist;\n \tint\t\t\treader;\t/* if 0, then request */\n@@ -847,7 +846,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tinode_lock(inode); /* protect against multiple concurrent\n \t\t\t      * readers on this file */\n  again:\n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \t/* need to find next request */\n \twhile (rp->q.list.next != &cd->queue &&\n \t       list_entry(rp->q.list.next, struct cache_queue, list)\n@@ -856,7 +855,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\tlist_move(&rp->q.list, next);\n \t}\n \tif (rp->q.list.next == &cd->queue) {\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t\tinode_unlock(inode);\n \t\tWARN_ON_ONCE(rp->offset);\n \t\treturn 0;\n@@ -865,7 +864,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tWARN_ON_ONCE(rq->q.reader);\n \tif (rp->offset == 0)\n \t\trq->readers++;\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \n \tif (rq->len == 0) {\n \t\terr = cache_request(cd, rq);\n@@ -876,9 +875,9 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \n \tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\terr = -EAGAIN;\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t} else {\n \t\tif (rp->offset + count > rq->len)\n \t\t\tcount = rq->len - rp->offset;\n@@ -888,26 +887,26 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trp->offset += count;\n \t\tif (rp->offset >= rq->len) {\n \t\t\trp->offset = 0;\n-\t\t\tspin_lock(&queue_lock);\n+\t\t\tspin_lock(&cd->queue_lock);\n \t\t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t\t}\n \t\terr = 0;\n \t}\n  out:\n \tif (rp->offset == 0) {\n \t\t/* need to release rq */\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\trq->readers--;\n \t\tif (rq->readers == 0 &&\n \t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\t\tlist_del(&rq->q.list);\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t\t\tcache_put(rq->item, cd);\n \t\t\tkfree(rq->buf);\n \t\t\tkfree(rq);\n \t\t} else\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (err == -EAGAIN)\n \t\tgoto again;\n@@ -988,7 +987,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \tif (!rp)\n \t\treturn mask;\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \n \tfor (cq= &rp->q; &cq->list != &cd->queue;\n \t     cq = list_entry(cq->list.next, struct cache_queue, list))\n@@ -996,7 +995,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n \t\t\tbreak;\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \treturn mask;\n }\n \n@@ -1011,7 +1010,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \tif (cmd != FIONREAD || !rp)\n \t\treturn -EINVAL;\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \n \t/* only find the length remaining in current request,\n \t * or the length of the next request\n@@ -1024,7 +1023,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \t\t\tlen = cr->len - rp->offset;\n \t\t\tbreak;\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \n \treturn put_user(len, (int __user *)arg);\n }\n@@ -1046,9 +1045,9 @@ static int cache_open(struct inode *inode, struct file *filp,\n \t\trp->offset = 0;\n \t\trp->q.reader = 1;\n \n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tlist_add(&rp->q.list, &cd->queue);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (filp->f_mode & FMODE_WRITE)\n \t\tatomic_inc(&cd->writers);\n@@ -1062,7 +1061,7 @@ static int cache_release(struct inode *inode, struct file *filp,\n \tstruct cache_reader *rp = filp->private_data;\n \n \tif (rp) {\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tif (rp->offset) {\n \t\t\tstruct cache_queue *cq;\n \t\t\tfor (cq= &rp->q; &cq->list != &cd->queue;\n@@ -1075,7 +1074,7 @@ static int cache_release(struct inode *inode, struct file *filp,\n \t\t\trp->offset = 0;\n \t\t}\n \t\tlist_del(&rp->q.list);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \n \t\tfilp->private_data = NULL;\n \t\tkfree(rp);\n@@ -1097,7 +1096,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n \tstruct cache_request *cr;\n \tLIST_HEAD(dequeued);\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&detail->queue_lock);\n \tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n \t\tif (!cq->reader) {\n \t\t\tcr = container_of(cq, struct cache_request, q);\n@@ -1110,7 +1109,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n \t\t\t\tcontinue;\n \t\t\tlist_move(&cr->q.list, &dequeued);\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&detail->queue_lock);\n \twhile (!list_empty(&dequeued)) {\n \t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n \t\tlist_del(&cr->q.list);\n@@ -1235,7 +1234,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \tcrq->buf = buf;\n \tcrq->len = 0;\n \tcrq->readers = 0;\n-\tspin_lock(&queue_lock);\n+\tspin_lock(&detail->queue_lock);\n \tif (test_bit(CACHE_PENDING, &h->flags)) {\n \t\tcrq->item = cache_get(h);\n \t\tlist_add_tail(&crq->q.list, &detail->queue);\n@@ -1243,7 +1242,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t} else\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n \t\tret = -EAGAIN;\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&detail->queue_lock);\n \twake_up(&queue_wait);\n \tif (ret == -EAGAIN) {\n \t\tkfree(buf);\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "20260220-sunrpc-cache-v1-1-47d04014c245@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author is addressing a concern about the queue_wait waitqueue being global and waking pollers on all caches, instead of just the relevant cache. The author agrees to convert it to a per-cache_detail field so that only pollers on the relevant cache are woken.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "convert"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The queue_wait waitqueue is currently a file-scoped global, so a\nwake_up for one cache_detail wakes pollers on all caches. Convert it\nto a per-cache_detail field so that only pollers on the relevant cache\nare woken.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h | 2 ++\n net/sunrpc/cache.c           | 7 +++----\n 2 files changed, 5 insertions(+), 4 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex 3d32dd1f7b05d35562d2064fed69877b3950fb51..031379efba24d40f64ce346cf1032261d4b98d05 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -16,6 +16,7 @@\n #include <linux/atomic.h>\n #include <linux/kstrtox.h>\n #include <linux/proc_fs.h>\n+#include <linux/wait.h>\n \n /*\n  * Each cache requires:\n@@ -114,6 +115,7 @@ struct cache_detail {\n \t/* fields for communication over channel */\n \tstruct list_head\tqueue;\n \tspinlock_t\t\tqueue_lock;\n+\twait_queue_head_t\tqueue_wait;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex 6add2fe311425dc3aec63efce2c4bed06a3d3ba5..aef2607b3d7ffb61a42b9ea2ec17947465c026dc 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -401,6 +401,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)\n \tspin_lock_init(&cd->hash_lock);\n \tINIT_LIST_HEAD(&cd->queue);\n \tspin_lock_init(&cd->queue_lock);\n+\tinit_waitqueue_head(&cd->queue_wait);\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -970,8 +971,6 @@ static ssize_t cache_write(struct file *filp, const char __user *buf,\n \treturn ret;\n }\n \n-static DECLARE_WAIT_QUEUE_HEAD(queue_wait);\n-\n static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \t\t\t       struct cache_detail *cd)\n {\n@@ -979,7 +978,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \tstruct cache_reader *rp = filp->private_data;\n \tstruct cache_queue *cq;\n \n-\tpoll_wait(filp, &queue_wait, wait);\n+\tpoll_wait(filp, &cd->queue_wait, wait);\n \n \t/* alway allow write */\n \tmask = EPOLLOUT | EPOLLWRNORM;\n@@ -1243,7 +1242,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n \t\tret = -EAGAIN;\n \tspin_unlock(&detail->queue_lock);\n-\twake_up(&queue_wait);\n+\twake_up(&detail->queue_wait);\n \tif (ret == -EAGAIN) {\n \t\tkfree(buf);\n \t\tkfree(crq);\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "20260220-sunrpc-cache-v1-2-47d04014c245@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about the complexity of the reader-skipping loops in cache_read/cache_poll/cache_ioctl/cache_release by explaining that they have simplified these loops and made the data flow easier to reason about by replacing the single interleaved queue with two dedicated lists: cd->requests for upcall requests and cd->readers for open file handles. The author also removed an obsolete comment.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a simplification",
                "removed obsolete code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace the single interleaved queue (which mixed cache_request and\ncache_reader entries distinguished by a ->reader flag) with two\ndedicated lists: cd->requests for upcall requests and cd->readers\nfor open file handles.\n\nReaders now track their position via a monotonically increasing\nsequence number (next_seqno) rather than by their position in the\nshared list. Each cache_request is assigned a seqno when enqueued,\nand a new cache_next_request() helper finds the next request at or\nafter a given seqno.\n\nThis eliminates the cache_queue wrapper struct entirely, simplifies\nthe reader-skipping loops in cache_read/cache_poll/cache_ioctl/\ncache_release, and makes the data flow easier to reason about.\n\nAlso, remove an obsolete comment. CACHE_UPCALLING hasn't existed\nsince before the git era started.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h |   4 +-\n net/sunrpc/cache.c           | 125 ++++++++++++++++++-------------------------\n 2 files changed, 56 insertions(+), 73 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex 031379efba24d40f64ce346cf1032261d4b98d05..b1e595c2615bd4be4d9ad19f71a8f4d08bd74a9b 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -113,9 +113,11 @@ struct cache_detail {\n \tint\t\t\tentries;\n \n \t/* fields for communication over channel */\n-\tstruct list_head\tqueue;\n+\tstruct list_head\trequests;\n+\tstruct list_head\treaders;\n \tspinlock_t\t\tqueue_lock;\n \twait_queue_head_t\tqueue_wait;\n+\tu64\t\t\tnext_seqno;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex aef2607b3d7ffb61a42b9ea2ec17947465c026dc..09389ce8b961fe0cb5a472bcf2d3dd0b3faa13a6 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -399,9 +399,11 @@ static struct delayed_work cache_cleaner;\n void sunrpc_init_cache_detail(struct cache_detail *cd)\n {\n \tspin_lock_init(&cd->hash_lock);\n-\tINIT_LIST_HEAD(&cd->queue);\n+\tINIT_LIST_HEAD(&cd->requests);\n+\tINIT_LIST_HEAD(&cd->readers);\n \tspin_lock_init(&cd->queue_lock);\n \tinit_waitqueue_head(&cd->queue_wait);\n+\tcd->next_seqno = 0;\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -796,29 +798,20 @@ void cache_clean_deferred(void *owner)\n  * On read, you get a full request, or block.\n  * On write, an update request is processed.\n  * Poll works if anything to read, and always allows write.\n- *\n- * Implemented by linked list of requests.  Each open file has\n- * a ->private that also exists in this list.  New requests are added\n- * to the end and may wakeup and preceding readers.\n- * New readers are added to the head.  If, on read, an item is found with\n- * CACHE_UPCALLING clear, we free it from the list.\n- *\n  */\n \n-struct cache_queue {\n-\tstruct list_head\tlist;\n-\tint\t\t\treader;\t/* if 0, then request */\n-};\n struct cache_request {\n-\tstruct cache_queue\tq;\n+\tstruct list_head\tlist;\n \tstruct cache_head\t*item;\n-\tchar\t\t\t* buf;\n+\tchar\t\t\t*buf;\n \tint\t\t\tlen;\n \tint\t\t\treaders;\n+\tu64\t\t\tseqno;\n };\n struct cache_reader {\n-\tstruct cache_queue\tq;\n+\tstruct list_head\tlist;\n \tint\t\t\toffset;\t/* if non-0, we have a refcnt on next request */\n+\tu64\t\t\tnext_seqno;\n };\n \n static int cache_request(struct cache_detail *detail,\n@@ -833,6 +826,17 @@ static int cache_request(struct cache_detail *detail,\n \treturn PAGE_SIZE - len;\n }\n \n+static struct cache_request *\n+cache_next_request(struct cache_detail *cd, u64 seqno)\n+{\n+\tstruct cache_request *rq;\n+\n+\tlist_for_each_entry(rq, &cd->requests, list)\n+\t\tif (rq->seqno >= seqno)\n+\t\t\treturn rq;\n+\treturn NULL;\n+}\n+\n static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\t\t  loff_t *ppos, struct cache_detail *cd)\n {\n@@ -849,20 +853,13 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n  again:\n \tspin_lock(&cd->queue_lock);\n \t/* need to find next request */\n-\twhile (rp->q.list.next != &cd->queue &&\n-\t       list_entry(rp->q.list.next, struct cache_queue, list)\n-\t       ->reader) {\n-\t\tstruct list_head *next = rp->q.list.next;\n-\t\tlist_move(&rp->q.list, next);\n-\t}\n-\tif (rp->q.list.next == &cd->queue) {\n+\trq = cache_next_request(cd, rp->next_seqno);\n+\tif (!rq) {\n \t\tspin_unlock(&cd->queue_lock);\n \t\tinode_unlock(inode);\n \t\tWARN_ON_ONCE(rp->offset);\n \t\treturn 0;\n \t}\n-\trq = container_of(rp->q.list.next, struct cache_request, q.list);\n-\tWARN_ON_ONCE(rq->q.reader);\n \tif (rp->offset == 0)\n \t\trq->readers++;\n \tspin_unlock(&cd->queue_lock);\n@@ -877,7 +874,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\terr = -EAGAIN;\n \t\tspin_lock(&cd->queue_lock);\n-\t\tlist_move(&rp->q.list, &rq->q.list);\n+\t\trp->next_seqno = rq->seqno + 1;\n \t\tspin_unlock(&cd->queue_lock);\n \t} else {\n \t\tif (rp->offset + count > rq->len)\n@@ -889,7 +886,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\tif (rp->offset >= rq->len) {\n \t\t\trp->offset = 0;\n \t\t\tspin_lock(&cd->queue_lock);\n-\t\t\tlist_move(&rp->q.list, &rq->q.list);\n+\t\t\trp->next_seqno = rq->seqno + 1;\n \t\t\tspin_unlock(&cd->queue_lock);\n \t\t}\n \t\terr = 0;\n@@ -901,7 +898,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trq->readers--;\n \t\tif (rq->readers == 0 &&\n \t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n-\t\t\tlist_del(&rq->q.list);\n+\t\t\tlist_del(&rq->list);\n \t\t\tspin_unlock(&cd->queue_lock);\n \t\t\tcache_put(rq->item, cd);\n \t\t\tkfree(rq->buf);\n@@ -976,7 +973,6 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n {\n \t__poll_t mask;\n \tstruct cache_reader *rp = filp->private_data;\n-\tstruct cache_queue *cq;\n \n \tpoll_wait(filp, &cd->queue_wait, wait);\n \n@@ -988,12 +984,8 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \n \tspin_lock(&cd->queue_lock);\n \n-\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\tif (!cq->reader) {\n-\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n-\t\t\tbreak;\n-\t\t}\n+\tif (cache_next_request(cd, rp->next_seqno))\n+\t\tmask |= EPOLLIN | EPOLLRDNORM;\n \tspin_unlock(&cd->queue_lock);\n \treturn mask;\n }\n@@ -1004,7 +996,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n {\n \tint len = 0;\n \tstruct cache_reader *rp = filp->private_data;\n-\tstruct cache_queue *cq;\n+\tstruct cache_request *rq;\n \n \tif (cmd != FIONREAD || !rp)\n \t\treturn -EINVAL;\n@@ -1014,14 +1006,9 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \t/* only find the length remaining in current request,\n \t * or the length of the next request\n \t */\n-\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\tif (!cq->reader) {\n-\t\t\tstruct cache_request *cr =\n-\t\t\t\tcontainer_of(cq, struct cache_request, q);\n-\t\t\tlen = cr->len - rp->offset;\n-\t\t\tbreak;\n-\t\t}\n+\trq = cache_next_request(cd, rp->next_seqno);\n+\tif (rq)\n+\t\tlen = rq->len - rp->offset;\n \tspin_unlock(&cd->queue_lock);\n \n \treturn put_user(len, (int __user *)arg);\n@@ -1042,10 +1029,10 @@ static int cache_open(struct inode *inode, struct file *filp,\n \t\t\treturn -ENOMEM;\n \t\t}\n \t\trp->offset = 0;\n-\t\trp->q.reader = 1;\n+\t\trp->next_seqno = 0;\n \n \t\tspin_lock(&cd->queue_lock);\n-\t\tlist_add(&rp->q.list, &cd->queue);\n+\t\tlist_add(&rp->list, &cd->readers);\n \t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (filp->f_mode & FMODE_WRITE)\n@@ -1062,17 +1049,14 @@ static int cache_release(struct inode *inode, struct file *filp,\n \tif (rp) {\n \t\tspin_lock(&cd->queue_lock);\n \t\tif (rp->offset) {\n-\t\t\tstruct cache_queue *cq;\n-\t\t\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t\t\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\t\t\tif (!cq->reader) {\n-\t\t\t\t\tcontainer_of(cq, struct cache_request, q)\n-\t\t\t\t\t\t->readers--;\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n+\t\t\tstruct cache_request *rq;\n+\n+\t\t\trq = cache_next_request(cd, rp->next_seqno);\n+\t\t\tif (rq)\n+\t\t\t\trq->readers--;\n \t\t\trp->offset = 0;\n \t\t}\n-\t\tlist_del(&rp->q.list);\n+\t\tlist_del(&rp->list);\n \t\tspin_unlock(&cd->queue_lock);\n \n \t\tfilp->private_data = NULL;\n@@ -1091,27 +1075,24 @@ static int cache_release(struct inode *inode, struct file *filp,\n \n static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n {\n-\tstruct cache_queue *cq, *tmp;\n-\tstruct cache_request *cr;\n+\tstruct cache_request *cr, *tmp;\n \tLIST_HEAD(dequeued);\n \n \tspin_lock(&detail->queue_lock);\n-\tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n-\t\tif (!cq->reader) {\n-\t\t\tcr = container_of(cq, struct cache_request, q);\n-\t\t\tif (cr->item != ch)\n-\t\t\t\tcontinue;\n-\t\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n-\t\t\t\t/* Lost a race and it is pending again */\n-\t\t\t\tbreak;\n-\t\t\tif (cr->readers != 0)\n-\t\t\t\tcontinue;\n-\t\t\tlist_move(&cr->q.list, &dequeued);\n-\t\t}\n+\tlist_for_each_entry_safe(cr, tmp, &detail->requests, list) {\n+\t\tif (cr->item != ch)\n+\t\t\tcontinue;\n+\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n+\t\t\t/* Lost a race and it is pending again */\n+\t\t\tbreak;\n+\t\tif (cr->readers != 0)\n+\t\t\tcontinue;\n+\t\tlist_move(&cr->list, &dequeued);\n+\t}\n \tspin_unlock(&detail->queue_lock);\n \twhile (!list_empty(&dequeued)) {\n-\t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n-\t\tlist_del(&cr->q.list);\n+\t\tcr = list_entry(dequeued.next, struct cache_request, list);\n+\t\tlist_del(&cr->list);\n \t\tcache_put(cr->item, detail);\n \t\tkfree(cr->buf);\n \t\tkfree(cr);\n@@ -1229,14 +1210,14 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t\treturn -EAGAIN;\n \t}\n \n-\tcrq->q.reader = 0;\n \tcrq->buf = buf;\n \tcrq->len = 0;\n \tcrq->readers = 0;\n \tspin_lock(&detail->queue_lock);\n \tif (test_bit(CACHE_PENDING, &h->flags)) {\n \t\tcrq->item = cache_get(h);\n-\t\tlist_add_tail(&crq->q.list, &detail->queue);\n+\t\tcrq->seqno = detail->next_seqno++;\n+\t\tlist_add_tail(&crq->list, &detail->requests);\n \t\ttrace_cache_entry_upcall(detail, h);\n \t} else\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "20260220-sunrpc-cache-v1-3-47d04014c245@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Chuck Lever",
              "summary": "Reviewer noted that the patch does not handle the case where a reader is holding a reference to an item in the cache_detail->queue when it is moved from the reader list to the request list, potentially causing a use-after-free error.\n\nReviewer noted that the split of cache_detail queue into request and reader lists may lead to a deadlock scenario if a reader is holding the reader lock and trying to acquire the request lock, which could be held by another thread waiting on the request list.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential use-after-free error",
                "deadlock",
                "locking"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nOn Fri, 20 Feb 2026 07:26:02 -0500, Jeff Layton wrote:\n\n---\n\nApplied to nfsd-testing, thanks!\n\n[1/3] sunrpc: convert queue_lock from global spinlock to per-cache_detail lock\n      commit: 8da8f32e9a2702259cdf97e2f8f492ef9c79db65\n[2/3] sunrpc: convert queue_wait from global to per-cache_detail waitqueue\n      commit: 802261d8b58dd2f41a52a0c92776e0fb45619efe\n[3/3] sunrpc: split cache_detail queue into request and reader lists\n      commit: 0eb3d9dc71ada02909e4dfe9cb54e703ec717ed4\n\n--\nChuck Lever",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-20",
              "message_id": "177161622290.3877966.16867844436002593841.b4-ty@oracle.com",
              "analysis_source": "llm"
            },
            {
              "author": "NeilBrown",
              "summary": "Reviewer noted that the code should decrement ->readers and check if it's zero and CACHE_PENDING is clear, similar to another place in the code, and suggested fixing this potential bug now",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential bug",
                "suggested fix"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hmm..  The other place where we decrement ->readers we then check if it\nis zero and if CACHE_PENDING is clear - and do something.\nI suspect we should do that here.\nThis bug (if I'm right and it is a bug) if there before you patch, but\nnow might be a good time to fix it?\n\nThanks.  Nice cleanups.\n\nNeilBrown",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-22",
              "message_id": "177171367423.8396.10176251932730619714@noble.neil.brown.name",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "Author acknowledged a concern about the order of patches and agreed to add a separate patch to address it before applying the rest.\n\nAuthor acknowledged that there were issues with the spinlock in the patch, agreed to fix them in version 2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Good catch. I'll add a patch to fix that preceding these patches, so it\ncan go to stable (if we think that's worthwhile).\n\n---\n\nThanks for the review! I'll send a v2 with the spinlock fixes and the\nabove bugfix.",
              "reply_to": "NeilBrown",
              "message_date": "2026-02-23",
              "message_id": "7c0e019cbf7371bdf47bd7a7c48df132fc5b87fd.camel@kernel.org",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joanne Koong",
      "primary_email": "joannelkoong@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v1 0/1] iomap: don't mark folio uptodate if read IO has bytes pending",
          "message_id": "20260219003911.344478-1-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260219003911.344478-1-joannelkoong@gmail.com/",
          "date": "2026-02-19T00:41:04Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-19",
          "patch_summary": "This patch fixes a bug where a folio is incorrectly marked as uptodate if there are still bytes pending from a read IO operation. The issue occurs when the read_folio() function is called on a folio size that is larger than the actual file size, and the post-eof blocks are zeroed and marked uptodate before the pending bytes are subtracted. The patch prevents this by not marking the folio uptodate if there are still bytes pending from the read IO operation.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern that marking the folio uptodate in iomap_set_range_uptodate() can be cleared by the XOR semantics used in folio_end_read(). The author agreed to fix this issue by not marking the folio as uptodate if the read IO has bytes pending, and instead setting it uptodate through iomap_end_read()->folio_end_read() in the read completion path.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If a folio has ifs metadata attached to it and the folio is partially\nread in through an async IO helper with the rest of it then being read\nin through post-EOF zeroing or as inline data, and the helper\nsuccessfully finishes the read first, then post-EOF zeroing / reading\ninline will mark the folio as uptodate in iomap_set_range_uptodate().\n\nThis is a problem because when the read completion path later calls\niomap_read_end(), it will call folio_end_read(), which sets the uptodate\nbit using XOR semantics. Calling folio_end_read() on a folio that was\nalready marked uptodate clears the uptodate bit.\n\nFix this by not marking the folio as uptodate if the read IO has bytes\npending. The folio uptodate state will be set in the read completion\npath through iomap_end_read() -> folio_end_read().\n\nReported-by: Wei Gao <wegao@suse.com>\nSuggested-by: Sasha Levin <sashal@kernel.org>\nTested-by: Wei Gao <wegao@suse.com>\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nFixes: b2f35ac4146d (\"iomap: add caller-provided callbacks for read and readahead\")\n---\n fs/iomap/buffered-io.c | 15 ++++++++++++---\n 1 file changed, 12 insertions(+), 3 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 58887513b894..4fc5ce963feb 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -80,18 +80,27 @@ static void iomap_set_range_uptodate(struct folio *folio, size_t off,\n {\n \tstruct iomap_folio_state *ifs = folio->private;\n \tunsigned long flags;\n-\tbool uptodate = true;\n+\tbool mark_uptodate = true;\n \n \tif (folio_test_uptodate(folio))\n \t\treturn;\n \n \tif (ifs) {\n \t\tspin_lock_irqsave(&ifs->state_lock, flags);\n-\t\tuptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n+\t\t/*\n+\t\t * If a read with bytes pending is in progress, we must not call\n+\t\t * folio_mark_uptodate(). The read completion path\n+\t\t * (iomap_read_end()) will call folio_end_read(), which uses XOR\n+\t\t * semantics to set the uptodate bit. If we set it here, the XOR\n+\t\t * in folio_end_read() will clear it, leaving the folio not\n+\t\t * uptodate.\n+\t\t */\n+\t\tmark_uptodate = ifs_set_range_uptodate(folio, ifs, off, len) &&\n+\t\t\t\t!ifs->read_bytes_pending;\n \t\tspin_unlock_irqrestore(&ifs->state_lock, flags);\n \t}\n \n-\tif (uptodate)\n+\tif (mark_uptodate)\n \t\tfolio_mark_uptodate(folio);\n }\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-18",
              "message_id": "20260219003911.344478-2-joannelkoong@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong suggested adding a link to the relevant discussion on linux-fsdevel and CC'ing stable@vger.kernel.org, as the original patch is now in a released kernel.\n\nReviewer Darrick Wong noted that the patch is easy to understand and implement, but requested a test be written for it",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I would add:\n\nLink: https://lore.kernel.org/linux-fsdevel/aYbmy8JdgXwsGaPP@autotest-wegao.qe.prg2.suse.org/\nCc: <stable@vger.kernel.org> # v6.19\n\nsince the recent discussion around this was sort of buried in a\ndifferent thread, and the original patch is now in a released kernel.\n\n---\n\nYeah, that makes sense.  How difficult is this to write up as an fstest?\n\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n\n--D",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "message_id": "20260219024534.GN6467@frogsfrogsfrogs",
              "analysis_source": "llm"
            },
            {
              "author": "Matthew Wilcox",
              "summary": "reviewer expressed frustration that the iomap code has become overly complicated, making it difficult to understand or explain how to fix the issue of marking a folio uptodate when read IO has bytes pending",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "frustration",
                "difficulty understanding"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This isn't \"the xor thing has come back to bite us\".  This is \"the iomap\ncode is now too complicated and I cannot figure out how to explain to\nJoanne that there's really a simple way to do this\".\n\nI'm going to have to set aside my current projects and redo the iomap\nreadahead/read_folio code myself, aren't I?",
              "reply_to": "Darrick Wong",
              "message_date": "2026-02-19",
              "message_id": "aZaQO0jQaZXakwOA@casper.infradead.org",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong expressed confusion about an alternative approach mentioned in the patch description, suggesting that he missed a relevant discussion and requested clarification on what this simpler way is.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "lack of context"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Well you could try explaining to me what that simpler way is?\n\n/me gets the sense he's missing a discussion somewhere...\n\n--D",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-18",
              "message_id": "20260219061101.GO6467@frogsfrogsfrogs",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, but instead provided a link to prior discussion without addressing the specific feedback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "lack of direct response",
                "link to prior discussion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This is the link to the prior discussion\nhttps://lore.kernel.org/linux-fsdevel/20251223223018.3295372-1-sashal@kernel.org/T/#mbd61eaa5fd1e8922caa479720232628e39b8c9da\n\nThanks,\nJoanne",
              "reply_to": "Darrick Wong",
              "message_date": "2026-02-20",
              "message_id": "CAJnrk1aJJqafDkxMypUym6iFQ-HkaSxneOe6Sc746AwrmrDK4Q@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong noted that the read_bytes_pending field has inconsistent behavior across different IO paths, and suggested consolidating the read code into a single function to simplify the logic.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "<willy and I had a chat; this is a clumsy non-AI summary of it>\n\nI started looking at folio read state management in iomap, and made a\nfew observations that (I hope) match what willy's grumpy about.\n\nThere are three ways that iomap can be reading into the pagecache:\na) async ->readahead,\nb) synchronous ->read_folio (page faults), and\nc) synchronous ->read_folio_range (pagecache write).\n\n(Note that (b) can call a different ->read_folio_range than (c), though\nall implementations seem to have the same function)\n\nAll three of these IO paths share the behavior that they try to fill out\nthe folio's contents and set the corresponding folio/ifs uptodate bits\nif that succeeds.  Folio contents can come from anywhere, whether it's:\n\ni) zeroing memory,\nii) copying from an inlinedata buffer, or\niii) asynchronously fetching the contents from somewhere\n\nIn the case of (c) above, if the read fails then we fail the write, and\nif the read succeeds then we start copying to the pagecache.\n\nHowever, (a) and (b) have this additional read_bytes_pending field in\nthe ifs that implements some extra tracking.  AFAICT the purpose of this\nfield is to ensure that we don't call folio_end_read prematurely if\nthere's an async read in progress.  This can happen if iomap_iter\nreturns a negative errno on a partially processed folio, I think?\n\nread_bytes_pending is initialized to the folio_size() at the start of a\nread and subtracted from when parts of the folio are supplied, whether\nthat's synchronous zeroing or asynchronous read ioend completion.  When\nthe field reaches zero, we can then call folio_end_read().\n\nBut then there are twists, like the fact that we only call\niomap_read_init() to set read_bytes_pending if we decide to do an\nasynchronous read.  Or that iomap_read_end and iomap_finish_folio_read\nhave awfully similar code.  I think in the case of (i) and (ii) we also\ndon't touch read_pending_bytes at all, and merely set the uptodate bits?\n\nThis is confusing to me.  It would be more straightforward (I think) if\nwe just did it for all cases instead of adding more conditionals.  IOWs,\nhow hard would it be to consolidate the read code so that there's one\nfunction that iomap calls when it has filled out part of a folio.  Is\nthat possible, even though we shouldn't be calling folio_end_read during\na pagecache write?\n\nAt the end of the day, however, there's a bug in Linus' tree and we need\nto fix it, so Joanne's patch is a sufficient bandaid until we can go\nclean this up.\n\n--D",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-20",
              "message_id": "20260220234521.GA11069@frogsfrogsfrogs",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that the read IO has bytes pending issue also applies to async reads, confirming the problem is more widespread than initially thought.\n\nAuthor clarified that synchronous zeroing does not update read_bytes_pending, explaining the distinction between synchronous and asynchronous read completions.\n\nAuthor Joanne Koong addressed Darrick Wong's concern about consolidating synchronous ->read_folio_range() for buffered writes with the async read logic, explaining that it would add extra overhead and make handling more complicated. She agreed that there are edge cases to consider in the async read path but expressed reservations about manipulating read_bytes_pending from other paths like zeroing and inline reads due to potential race conditions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a broader scope of the issue",
                "clarification",
                "explanation",
                "no clear resolution signal",
                "author expresses reservations"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "b) is async as well. The code for b) and a) are exactly the same (the\nlogic in iomap_read_folio_iter())\n\n---\n\nSynchronous zeroing does not update read_bytes_pending, only async\nread completions do.\n\n---\n\nimo, I don't think the synchronous ->read_folio_range() for buffered\nwrites should be consolidated with the async read logic. If we have\nthe synchronous write path setting read_bytes_pending, that adds extra\noverhead with having to acquire/release the spinlock for every range\nread in. It also makes the handling more complicated (eg now having to\ndifferentiate whether the folio was read in for a read vs. a write).\nSynchronous ->read_folio_range() for buffered writes is extremely\nsimple and self-contained right now and I think it should be kept that\nway.\n\nFor async reads, I agree that there are a bunch of different edge\ncases that arise from i) ii) and iii), and from the fact that a folio\ncould be composed of a mixture of i) ii) and iii).\n\nThe motivation for adding read_bytes_pending was so we could know\nwhich async read finishes last. eg this example scenario: read a 64k\nfolio where the first and last page are not uptodate but everything in\nbetween is\n* ->read_folio_range() for 0 to 4k\n* ->read_folio_range() for 60k to 64k\nThese two async read calls may be two different I/O requests that\ncomplete at different times but only the last finisher should call\nfolio_end_read().\n\nI don't think having the zeroing and inline read paths also\nmanipulating read_bytes_pending helps here. This was discussed a bit\nin [1] but I think it runs into other edge cases / race conditions [2]\nthat would need to be accounted for and makes some paths more\nsuboptimal (eg unnecessary ifs allocations and spinlock acquires). But\nmaybe I'm missing something here and there is a better approach for\ndoing this?\n\nThanks,\nJoanne\n\n[1] https://lore.kernel.org/linux-fsdevel/CAJnrk1YcuhKwbZLo-11=umcTzH_OJ+bdwZq5=XjeJo8gb9e5ig@mail.gmail.com/T/#md09648082a96122ec1e541993872e0c43da5105f\n[2] https://lore.kernel.org/linux-fsdevel/CAJnrk1YcuhKwbZLo-11=umcTzH_OJ+bdwZq5=XjeJo8gb9e5ig@mail.gmail.com/T/#mdc49b649378798fa9e850c9c6914c8c6af5e2895",
              "reply_to": "Darrick Wong",
              "message_date": "2026-02-23",
              "message_id": "CAJnrk1Zk1hHCoC4xaY_KT0m_04CQ=pO6j3e1tGrdj7LTf5BHsA@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that merging the code would be useful, but he hasn't found a good way to do it yet, and expressed concern about the range logic in ->read_folio",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes.  I've been thinking about that on and off, but unfortunately so far\nI've not come up with a good idea how to merge the code.  Doing so would\nbe very useful for many reasons.\n\nThe problem with that isn't really async vs sync; ->read_folio clearly\nshows you you turn underlying asynchronous logic into a synchronous call.\nIt's really about the range logic, where the writer preparation might\nwant to only read the head and the tail segments of a folio.\n\nBut if we can merge that into the main implementation and have a single\ncore implementation we'd be much better off.\n\nAnyone looking for a \"little\" project? :)",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-24",
              "message_id": "aZ3A39jztKdUmWoT@infradead.org",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v1 11/11] io_uring/cmd: set selected buffer index in __io_uring_cmd_done()",
          "message_id": "20260210002852.1394504-12-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260210002852.1394504-12-joannelkoong@gmail.com/",
          "date": "2026-02-10T00:31:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-10",
          "patch_summary": "This patch adds the ability to set the selected buffer index in the __io_uring_cmd_done() function, which is part of the io_uring command handling code. This allows the kernel to keep track of the currently selected buffer when issuing commands through the io_uring interface. The change is a part of a larger series that introduces kernel-managed buffer rings, where the kernel allocates and manages buffers on behalf of applications using io_uring. The patch builds upon previous changes in the series, including support for kernel-managed buffer rings, mmap support, and recycling of buffers.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern that the logic in io_register_pbuf_ring() is too complex and difficult to understand, and refactored it into three separate functions: io_copy_and_validate_buf_reg(), io_alloc_new_buffer_list(), and io_setup_pbuf_ring(). The new functions are designed to be more modular and easier to maintain. The author plans to reuse these helpers in upcoming kernel-managed buffer ring support.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "refactored code",
                "preparatory change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Refactor the logic in io_register_pbuf_ring() into generic helpers:\n- io_copy_and_validate_buf_reg(): Copy out user arg and validate user\n  arg and buffer registration parameters\n- io_alloc_new_buffer_list(): Allocate and initialize a new buffer\n  list for the given buffer group ID\n- io_setup_pbuf_ring(): Sets up the physical buffer ring region and\n  handles memory mapping for provided buffer rings\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport which will need to reuse some of these helpers.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c | 129 +++++++++++++++++++++++++++++++-----------------\n 1 file changed, 85 insertions(+), 44 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 67d4fe576473..850b836f32ee 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -596,55 +596,73 @@ int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags)\n \treturn IOU_COMPLETE;\n }\n \n-int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+static int io_copy_and_validate_buf_reg(const void __user *arg,\n+\t\t\t\t\tstruct io_uring_buf_reg *reg,\n+\t\t\t\t\tunsigned int permitted_flags)\n {\n-\tstruct io_uring_buf_reg reg;\n-\tstruct io_buffer_list *bl;\n-\tstruct io_uring_region_desc rd;\n-\tstruct io_uring_buf_ring *br;\n-\tunsigned long mmap_offset;\n-\tunsigned long ring_size;\n-\tint ret;\n-\n-\tlockdep_assert_held(&ctx->uring_lock);\n-\n-\tif (copy_from_user(&reg, arg, sizeof(reg)))\n+\tif (copy_from_user(reg, arg, sizeof(*reg)))\n \t\treturn -EFAULT;\n-\tif (!mem_is_zero(reg.resv, sizeof(reg.resv)))\n+\n+\tif (!mem_is_zero(reg->resv, sizeof(reg->resv)))\n \t\treturn -EINVAL;\n-\tif (reg.flags & ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC))\n+\tif (reg->flags & ~permitted_flags)\n \t\treturn -EINVAL;\n-\tif (!is_power_of_2(reg.ring_entries))\n+\tif (!is_power_of_2(reg->ring_entries))\n \t\treturn -EINVAL;\n \t/* cannot disambiguate full vs empty due to head/tail size */\n-\tif (reg.ring_entries >= 65536)\n+\tif (reg->ring_entries >= 65536)\n \t\treturn -EINVAL;\n+\treturn 0;\n+}\n \n-\tbl = io_buffer_get_list(ctx, reg.bgid);\n-\tif (bl) {\n+static struct io_buffer_list *\n+io_alloc_new_buffer_list(struct io_ring_ctx *ctx,\n+\t\t\t const struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_buffer_list *list;\n+\n+\tlist = io_buffer_get_list(ctx, reg->bgid);\n+\tif (list) {\n \t\t/* if mapped buffer ring OR classic exists, don't allow */\n-\t\tif (bl->flags & IOBL_BUF_RING || !list_empty(&bl->buf_list))\n-\t\t\treturn -EEXIST;\n-\t\tio_destroy_bl(ctx, bl);\n+\t\tif (list->flags & IOBL_BUF_RING || !list_empty(&list->buf_list))\n+\t\t\treturn ERR_PTR(-EEXIST);\n+\t\tio_destroy_bl(ctx, list);\n \t}\n \n-\tbl = kzalloc(sizeof(*bl), GFP_KERNEL_ACCOUNT);\n-\tif (!bl)\n-\t\treturn -ENOMEM;\n+\tlist = kzalloc(sizeof(*list), GFP_KERNEL_ACCOUNT);\n+\tif (!list)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tlist->nr_entries = reg->ring_entries;\n+\tlist->mask = reg->ring_entries - 1;\n+\tlist->flags = IOBL_BUF_RING;\n+\n+\treturn list;\n+}\n+\n+static int io_setup_pbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t      const struct io_uring_buf_reg *reg,\n+\t\t\t      struct io_buffer_list *bl)\n+{\n+\tstruct io_uring_region_desc rd;\n+\tunsigned long mmap_offset;\n+\tunsigned long ring_size;\n+\tint ret;\n \n-\tmmap_offset = (unsigned long)reg.bgid << IORING_OFF_PBUF_SHIFT;\n-\tring_size = flex_array_size(br, bufs, reg.ring_entries);\n+\tmmap_offset = (unsigned long)reg->bgid << IORING_OFF_PBUF_SHIFT;\n+\tring_size = flex_array_size(bl->buf_ring, bufs, reg->ring_entries);\n \n \tmemset(&rd, 0, sizeof(rd));\n \trd.size = PAGE_ALIGN(ring_size);\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP)) {\n-\t\trd.user_addr = reg.ring_addr;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP)) {\n+\t\trd.user_addr = reg->ring_addr;\n \t\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n \t}\n+\n \tret = io_create_region(ctx, &bl->region, &rd, mmap_offset);\n \tif (ret)\n-\t\tgoto fail;\n-\tbr = io_region_get_ptr(&bl->region);\n+\t\treturn ret;\n+\tbl->buf_ring = io_region_get_ptr(&bl->region);\n \n #ifdef SHM_COLOUR\n \t/*\n@@ -656,25 +674,48 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t * should use IOU_PBUF_RING_MMAP instead, and liburing will handle\n \t * this transparently.\n \t */\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP) &&\n-\t    ((reg.ring_addr | (unsigned long)br) & (SHM_COLOUR - 1))) {\n-\t\tret = -EINVAL;\n-\t\tgoto fail;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP) &&\n+\t    ((reg->ring_addr | (unsigned long)bl->buf_ring) &\n+\t     (SHM_COLOUR - 1))) {\n+\t\tio_free_region(ctx->user, &bl->region);\n+\t\treturn -EINVAL;\n \t}\n #endif\n \n-\tbl->nr_entries = reg.ring_entries;\n-\tbl->mask = reg.ring_entries - 1;\n-\tbl->flags |= IOBL_BUF_RING;\n-\tbl->buf_ring = br;\n-\tif (reg.flags & IOU_PBUF_RING_INC)\n+\tif (reg->flags & IOU_PBUF_RING_INC)\n \t\tbl->flags |= IOBL_INC;\n+\n+\treturn 0;\n+}\n+\n+int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tunsigned int permitted_flags;\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tpermitted_flags = IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC;\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, permitted_flags);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_pbuf_ring(ctx, &reg, bl);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n \tret = io_buffer_add_list(ctx, bl, reg.bgid);\n-\tif (!ret)\n-\t\treturn 0;\n-fail:\n-\tio_free_region(ctx->user, &bl->region);\n-\tkfree(bl);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n \treturn ret;\n }\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "20260210002852.1394504-2-joannelkoong@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the naming consistency of functions related to unregistering buffer rings, agreeing that using the more generic name io_unregister_buf_ring() is better and making preparatory changes for upcoming kernel-managed buffer ring support.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "preparatory change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Use the more generic name io_unregister_buf_ring() as this function will\nbe used for unregistering both provided buffer rings and kernel-managed\nbuffer rings.\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c     | 2 +-\n io_uring/kbuf.h     | 2 +-\n io_uring/register.c | 2 +-\n 3 files changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 850b836f32ee..aa9b70b72db4 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -719,7 +719,7 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \treturn ret;\n }\n \n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n {\n \tstruct io_uring_buf_reg reg;\n \tstruct io_buffer_list *bl;\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex bf15e26520d3..40b44f4fdb15 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -74,7 +74,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags);\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 594b1f2ce875..0882cb34f851 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -841,7 +841,7 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-\t\tret = io_unregister_pbuf_ring(ctx, arg);\n+\t\tret = io_unregister_buf_ring(ctx, arg);\n \t\tbreak;\n \tcase IORING_REGISTER_SYNC_CANCEL:\n \t\tret = -EINVAL;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "20260210002852.1394504-3-joannelkoong@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about the implementation of kernel-managed buffer rings, specifically how to handle the allocation and management of buffers for these rings. The author has provided an explanation of their approach, which involves reusing validation and buffer list allocation helpers from earlier refactoring. They have also added new functions to support kernel-managed buffer rings, including io_setup_kmbuf_ring() and io_register_kmbuf_ring().",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "explanation",
                "implementation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for kernel-managed buffer rings (kmbuf rings), which allow\nthe kernel to allocate and manage the backing buffers for a buffer\nring, rather than requiring the application to provide and manage them.\n\nThis introduces two new registration opcodes:\n- IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring\n- IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring\n\nThe existing io_uring_buf_reg structure is extended with a union to\nsupport both application-provided buffer rings (pbuf) and kernel-managed\nbuffer rings (kmbuf):\n- For pbuf rings: ring_addr specifies the user-provided ring address\n- For kmbuf rings: buf_size specifies the size of each buffer. buf_size\n  must be non-zero and page-aligned.\n\nThe implementation follows the same pattern as pbuf ring registration,\nreusing the validation and buffer list allocation helpers introduced in\nearlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as\nkernel-managed for appropriate handling in the I/O path.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  15 ++++-\n io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-\n io_uring/kbuf.h               |   7 ++-\n io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++\n io_uring/memmap.h             |   4 ++\n io_uring/register.c           |   7 +++\n 6 files changed, 219 insertions(+), 6 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex fc473af6feb4..a0889c1744bd 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -715,6 +715,10 @@ enum io_uring_register_op {\n \t/* register bpf filtering programs */\n \tIORING_REGISTER_BPF_FILTER\t\t= 37,\n \n+\t/* register/unregister kernel-managed ring buffer group */\n+\tIORING_REGISTER_KMBUF_RING\t\t= 38,\n+\tIORING_UNREGISTER_KMBUF_RING\t\t= 39,\n+\n \t/* this goes last */\n \tIORING_REGISTER_LAST,\n \n@@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {\n \tIOU_PBUF_RING_INC\t= 2,\n };\n \n-/* argument for IORING_(UN)REGISTER_PBUF_RING */\n+/* argument for IORING_(UN)REGISTER_PBUF_RING and\n+ * IORING_(UN)REGISTER_KMBUF_RING\n+ */\n struct io_uring_buf_reg {\n-\t__u64\tring_addr;\n+\tunion {\n+\t\t/* used for pbuf rings */\n+\t\t__u64\tring_addr;\n+\t\t/* used for kmbuf rings */\n+\t\t__u32   buf_size;\n+\t};\n \t__u32\tring_entries;\n \t__u16\tbgid;\n \t__u16\tflags;\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex aa9b70b72db4..9bc36451d083 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -427,10 +427,13 @@ static int io_remove_buffers_legacy(struct io_ring_ctx *ctx,\n \n static void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)\n {\n-\tif (bl->flags & IOBL_BUF_RING)\n+\tif (bl->flags & IOBL_BUF_RING) {\n \t\tio_free_region(ctx->user, &bl->region);\n-\telse\n+\t\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\t\tkfree(bl->buf_ring);\n+\t} else {\n \t\tio_remove_buffers_legacy(ctx, bl, -1U);\n+\t}\n \n \tkfree(bl);\n }\n@@ -779,3 +782,77 @@ struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n \t\treturn NULL;\n \treturn &bl->region;\n }\n+\n+static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_buffer_list *bl,\n+\t\t\t       struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_uring_buf_ring *ring;\n+\tunsigned long ring_size;\n+\tvoid *buf_region;\n+\tunsigned int i;\n+\tint ret;\n+\n+\t/* allocate pages for the ring structure */\n+\tring_size = flex_array_size(ring, bufs, bl->nr_entries);\n+\tring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);\n+\tif (!ring)\n+\t\treturn -ENOMEM;\n+\n+\tret = io_create_region_multi_buf(ctx, &bl->region, bl->nr_entries,\n+\t\t\t\t\t reg->buf_size);\n+\tif (ret) {\n+\t\tkfree(ring);\n+\t\treturn ret;\n+\t}\n+\n+\t/* initialize ring buf entries to point to the buffers */\n+\tbuf_region = bl->region.ptr;\n+\tfor (i = 0; i < bl->nr_entries; i++) {\n+\t\tstruct io_uring_buf *buf = &ring->bufs[i];\n+\n+\t\tbuf->addr = (u64)(uintptr_t)buf_region;\n+\t\tbuf->len = reg->buf_size;\n+\t\tbuf->bid = i;\n+\n+\t\tbuf_region += reg->buf_size;\n+\t}\n+\tring->tail = bl->nr_entries;\n+\n+\tbl->buf_ring = ring;\n+\tbl->flags |= IOBL_KERNEL_MANAGED;\n+\n+\treturn 0;\n+}\n+\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, 0);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tif (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_kmbuf_ring(ctx, bl, &reg);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n+\tret = io_buffer_add_list(ctx, bl, reg.bgid);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n+\treturn ret;\n+}\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 40b44f4fdb15..62c80a1ebf03 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -7,9 +7,11 @@\n \n enum {\n \t/* ring mapped provided buffers */\n-\tIOBL_BUF_RING\t= 1,\n+\tIOBL_BUF_RING\t\t= 1,\n \t/* buffers are consumed incrementally rather than always fully */\n-\tIOBL_INC\t= 2,\n+\tIOBL_INC\t\t= 2,\n+\t/* buffers are kernel managed */\n+\tIOBL_KERNEL_MANAGED\t= 4,\n };\n \n struct io_buffer_list {\n@@ -74,6 +76,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 89f56609e50a..8d37e93c0433 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -15,6 +15,28 @@\n #include \"rsrc.h\"\n #include \"zcrx.h\"\n \n+static void release_multi_buf_pages(struct page **pages, unsigned long nr_pages)\n+{\n+\tstruct page *page;\n+\tunsigned int nr, i = 0;\n+\n+\twhile (nr_pages) {\n+\t\tpage = pages[i];\n+\n+\t\tif (!page || WARN_ON_ONCE(page != compound_head(page)))\n+\t\t\treturn;\n+\n+\t\tnr = compound_nr(page);\n+\t\tput_page(page);\n+\n+\t\tif (WARN_ON_ONCE(nr > nr_pages))\n+\t\t\treturn;\n+\n+\t\ti += nr;\n+\t\tnr_pages -= nr;\n+\t}\n+}\n+\n static bool io_mem_alloc_compound(struct page **pages, int nr_pages,\n \t\t\t\t  size_t size, gfp_t gfp)\n {\n@@ -86,6 +108,8 @@ enum {\n \tIO_REGION_F_USER_PROVIDED\t\t= 2,\n \t/* only the first page in the array is ref'ed */\n \tIO_REGION_F_SINGLE_REF\t\t\t= 4,\n+\t/* pages in the array belong to multiple discrete allocations */\n+\tIO_REGION_F_MULTI_BUF\t\t\t= 8,\n };\n \n void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n@@ -98,6 +122,8 @@ void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n \n \t\tif (mr->flags & IO_REGION_F_USER_PROVIDED)\n \t\t\tunpin_user_pages(mr->pages, nr_refs);\n+\t\telse if (mr->flags & IO_REGION_F_MULTI_BUF)\n+\t\t\trelease_multi_buf_pages(mr->pages, nr_refs);\n \t\telse\n \t\t\trelease_pages(mr->pages, nr_refs);\n \n@@ -149,6 +175,54 @@ static int io_region_pin_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+static int io_region_allocate_pages_multi_buf(struct io_mapped_region *mr,\n+\t\t\t\t\t      unsigned int nr_bufs,\n+\t\t\t\t\t      unsigned int buf_size)\n+{\n+\tgfp_t gfp = GFP_USER | __GFP_ACCOUNT | __GFP_ZERO | __GFP_NOWARN;\n+\tstruct page **pages, **cur_pages;\n+\tunsigned int nr_allocated;\n+\tunsigned int buf_pages;\n+\tunsigned int i;\n+\n+\tif (!PAGE_ALIGNED(buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbuf_pages = buf_size >> PAGE_SHIFT;\n+\n+\tpages = kvmalloc_array(mr->nr_pages, sizeof(*pages), gfp);\n+\tif (!pages)\n+\t\treturn -ENOMEM;\n+\n+\tcur_pages = pages;\n+\n+\tfor (i = 0; i < nr_bufs; i++) {\n+\t\tif (io_mem_alloc_compound(cur_pages, buf_pages, buf_size,\n+\t\t\t\t\t  gfp)) {\n+\t\t\tcur_pages += buf_pages;\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tnr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,\n+\t\t\t\t\t\t     buf_pages, cur_pages);\n+\t\tif (nr_allocated != buf_pages) {\n+\t\t\tunsigned int total =\n+\t\t\t\t(cur_pages - pages) + nr_allocated;\n+\n+\t\t\trelease_multi_buf_pages(pages, total);\n+\t\t\tkvfree(pages);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tcur_pages += buf_pages;\n+\t}\n+\n+\tmr->flags |= IO_REGION_F_MULTI_BUF;\n+\tmr->pages = pages;\n+\n+\treturn 0;\n+}\n+\n static int io_region_allocate_pages(struct io_mapped_region *mr,\n \t\t\t\t    struct io_uring_region_desc *reg,\n \t\t\t\t    unsigned long mmap_offset)\n@@ -181,6 +255,43 @@ static int io_region_allocate_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size)\n+{\n+\tunsigned int nr_pages;\n+\tint ret;\n+\n+\tif (WARN_ON_ONCE(mr->pages || mr->ptr || mr->nr_pages))\n+\t\treturn -EFAULT;\n+\n+\tif (WARN_ON_ONCE(!nr_bufs || !buf_size || !PAGE_ALIGNED(buf_size)))\n+\t\treturn -EINVAL;\n+\n+\tif (check_mul_overflow(buf_size >> PAGE_SHIFT, nr_bufs, &nr_pages))\n+\t\treturn -EINVAL;\n+\n+\tif (ctx->user) {\n+\t\tret = __io_account_mem(ctx->user, nr_pages);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\t}\n+\tmr->nr_pages = nr_pages;\n+\n+\tret = io_region_allocate_pages_multi_buf(mr, nr_bufs, buf_size);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\tret = io_region_init_ptr(mr);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\treturn 0;\n+out_free:\n+\tio_free_region(ctx->user, mr);\n+\treturn ret;\n+}\n+\n int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset)\ndiff --git a/io_uring/memmap.h b/io_uring/memmap.h\nindex f4cfbb6b9a1f..3aa1167462ae 100644\n--- a/io_uring/memmap.h\n+++ b/io_uring/memmap.h\n@@ -22,6 +22,10 @@ int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset);\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size);\n+\n static inline void *io_region_get_ptr(struct io_mapped_region *mr)\n {\n \treturn mr->ptr;\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 0882cb34f851..2db8daaf8fde 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -837,7 +837,14 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\t\tbreak;\n \t\tret = io_register_pbuf_ring(ctx, arg);\n \t\tbreak;\n+\tcase IORING_REGISTER_KMBUF_RING:\n+\t\tret = -EINVAL;\n+\t\tif (!arg || nr_args != 1)\n+\t\t\tbreak;\n+\t\tret = io_register_kmbuf_ring(ctx, arg);\n+\t\tbreak;\n \tcase IORING_UNREGISTER_PBUF_RING:\n+\tcase IORING_UNREGISTER_KMBUF_RING:\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "20260210002852.1394504-4-joannelkoong@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the selected buffer index in __io_uring_cmd_done() not being set correctly, explained that the issue was due to the lack of a buffer ID in the io_uring_is_kmbuf_ring() check and promised to fix it by adding a new parameter to io_uring_is_kmbuf_ring()",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a technical issue",
                "promised a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for mmapping kernel-managed buffer rings (kmbuf) to\nuserspace, allowing applications to access the kernel-allocated buffers.\n\nSimilar to application-provided buffer rings (pbuf), kmbuf rings use the\nbuffer group ID encoded in the mmap offset to identify which buffer ring\nto map. The implementation follows the same pattern as pbuf rings.\n\nNew mmap offset constants are introduced:\n  - IORING_OFF_KMBUF_RING (0x88000000): Base offset for kmbuf mappings\n  - IORING_OFF_KMBUF_SHIFT (16): Shift value to encode buffer group ID\n\nThe mmap offset encodes the bgid shifted by IORING_OFF_KMBUF_SHIFT.\nThe io_buf_get_region() helper retrieves the appropriate region.\n\nThis allows userspace to mmap the kernel-allocated buffer region and\naccess the buffers directly.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  2 ++\n io_uring/kbuf.c               | 11 +++++++++--\n io_uring/kbuf.h               |  5 +++--\n io_uring/memmap.c             |  5 ++++-\n 4 files changed, 18 insertions(+), 5 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex a0889c1744bd..42a2812c9922 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -545,6 +545,8 @@ struct io_uring_cqe {\n #define IORING_OFF_SQES\t\t\t0x10000000ULL\n #define IORING_OFF_PBUF_RING\t\t0x80000000ULL\n #define IORING_OFF_PBUF_SHIFT\t\t16\n+#define IORING_OFF_KMBUF_RING\t\t0x88000000ULL\n+#define IORING_OFF_KMBUF_SHIFT\t\t16\n #define IORING_OFF_MMAP_MASK\t\t0xf8000000ULL\n \n /*\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9bc36451d083..ccf5b213087b 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -770,16 +770,23 @@ int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg)\n \treturn 0;\n }\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid)\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed)\n {\n \tstruct io_buffer_list *bl;\n+\tbool is_kernel_managed;\n \n \tlockdep_assert_held(&ctx->mmap_lock);\n \n \tbl = xa_load(&ctx->io_bl_xa, bgid);\n \tif (!bl || !(bl->flags & IOBL_BUF_RING))\n \t\treturn NULL;\n+\n+\tis_kernel_managed = !!(bl->flags & IOBL_KERNEL_MANAGED);\n+\tif (is_kernel_managed != kernel_managed)\n+\t\treturn NULL;\n+\n \treturn &bl->region;\n }\n \ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 62c80a1ebf03..11d165888b8e 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -88,8 +88,9 @@ unsigned int __io_put_kbufs(struct io_kiocb *req, struct io_buffer_list *bl,\n bool io_kbuf_commit(struct io_kiocb *req,\n \t\t    struct io_buffer_list *bl, int len, int nr);\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid);\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed);\n \n static inline bool io_kbuf_recycle_ring(struct io_kiocb *req,\n \t\t\t\t\tstruct io_buffer_list *bl)\ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 8d37e93c0433..916315122323 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -356,7 +356,10 @@ static struct io_mapped_region *io_mmap_get_region(struct io_ring_ctx *ctx,\n \t\treturn &ctx->sq_region;\n \tcase IORING_OFF_PBUF_RING:\n \t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_PBUF_SHIFT;\n-\t\treturn io_pbuf_get_region(ctx, id);\n+\t\treturn io_buf_get_region(ctx, id, false);\n+\tcase IORING_OFF_KMBUF_RING:\n+\t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_KMBUF_SHIFT;\n+\t\treturn io_buf_get_region(ctx, id, true);\n \tcase IORING_MAP_OFF_PARAM_REGION:\n \t\treturn &ctx->param_region;\n \tcase IORING_MAP_OFF_ZCRX_REGION:\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "20260210002852.1394504-5-joannelkoong@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about distinguishing between kernel-managed buffer addresses and negative values in error checking, explaining that the io_br_sel struct needs to be modified to separate address and value fields for this purpose.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Allow kernel-managed buffers to be selected. This requires modifying the\nio_br_sel struct to separate the fields for address and val, since a\nkernel address cannot be distinguished from a negative val when error\nchecking.\n\nAuto-commit any selected kernel-managed buffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring_types.h |  8 ++++----\n io_uring/kbuf.c                | 16 ++++++++++++----\n 2 files changed, 16 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 3e4a82a6f817..36cc2e0346d9 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -93,13 +93,13 @@ struct io_mapped_region {\n  */\n struct io_br_sel {\n \tstruct io_buffer_list *buf_list;\n-\t/*\n-\t * Some selection parts return the user address, others return an error.\n-\t */\n \tunion {\n+\t\t/* for classic/ring provided buffers */\n \t\tvoid __user *addr;\n-\t\tssize_t val;\n+\t\t/* for kernel-managed buffers */\n+\t\tvoid *kaddr;\n \t};\n+\tssize_t val;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex ccf5b213087b..1e8395270227 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -155,7 +155,8 @@ static int io_provided_buffers_select(struct io_kiocb *req, size_t *len,\n \treturn 1;\n }\n \n-static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n+static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n+\t\t\t     unsigned int issue_flags)\n {\n \t/*\n \t* If we came in unlocked, we have no choice but to consume the\n@@ -170,7 +171,11 @@ static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n \tif (issue_flags & IO_URING_F_UNLOCKED)\n \t\treturn true;\n \n-\t/* uring_cmd commits kbuf upfront, no need to auto-commit */\n+\t/* kernel-managed buffers are auto-committed */\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\treturn true;\n+\n+\t/* multishot uring_cmd commits kbuf upfront, no need to auto-commit */\n \tif (!io_file_can_poll(req) && req->opcode != IORING_OP_URING_CMD)\n \t\treturn true;\n \treturn false;\n@@ -200,9 +205,12 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n-\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n+\telse\n+\t\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n \n-\tif (io_should_commit(req, issue_flags)) {\n+\tif (io_should_commit(req, bl, issue_flags)) {\n \t\tio_kbuf_commit(req, sel.buf_list, *len, 1);\n \t\tsel.buf_list = NULL;\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "20260210002852.1394504-6-joannelkoong@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about buffer ring pinning, explaining that the new APIs will prevent userspace from unregistering a buffer ring while it is pinned by the kernel. The author added code to implement these APIs and ensure that a pinned buffer ring cannot be unregistered until explicitly unpinned.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "added code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add kernel APIs to pin and unpin buffer rings, preventing userspace from\nunregistering a buffer ring while it is pinned by the kernel.\n\nThis provides a mechanism for kernel subsystems to safely access buffer\nring contents while ensuring the buffer ring remains valid. A pinned\nbuffer ring cannot be unregistered until explicitly unpinned. On the\nuserspace side, trying to unregister a pinned buffer will return -EBUSY.\n\nThis is a preparatory change for upcoming fuse usage of kernel-managed\nbuffer rings. It is necessary for fuse to pin the buffer ring because\nfuse may need to select a buffer in atomic contexts, which it can only\ndo so by using the underlying buffer list pointer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 17 +++++++++++++\n io_uring/kbuf.c              | 48 ++++++++++++++++++++++++++++++++++++\n io_uring/kbuf.h              |  5 ++++\n 3 files changed, 70 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 375fd048c4cb..702b1903e6ee 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -84,6 +84,10 @@ struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n \t\t\t\t struct io_br_sel *sel, unsigned int issue_flags);\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t    unsigned issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -126,6 +130,19 @@ static inline bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n {\n \treturn true;\n }\n+static inline int io_uring_buf_ring_pin(struct io_uring_cmd *cmd,\n+\t\t\t\t\tunsigned buf_group,\n+\t\t\t\t\tunsigned issue_flags,\n+\t\t\t\t\tstruct io_buffer_list **bl)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n+static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned buf_group,\n+\t\t\t\t\t  unsigned issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 1e8395270227..dee1764ed19f 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -9,6 +9,7 @@\n #include <linux/poll.h>\n #include <linux/vmalloc.h>\n #include <linux/io_uring.h>\n+#include <linux/io_uring/cmd.h>\n \n #include <uapi/linux/io_uring.h>\n \n@@ -237,6 +238,51 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \treturn sel;\n }\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *buffer_list;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbuffer_list = io_buffer_get_list(ctx, buf_group);\n+\tif (buffer_list && (buffer_list->flags & IOBL_BUF_RING)) {\n+\t\tif (unlikely(buffer_list->flags & IOBL_PINNED)) {\n+\t\t\tret = -EALREADY;\n+\t\t} else {\n+\t\t\tbuffer_list->flags |= IOBL_PINNED;\n+\t\t\tret = 0;\n+\t\t\t*bl = buffer_list;\n+\t\t}\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_pin);\n+\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t       unsigned issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (bl && (bl->flags & IOBL_BUF_RING) && (bl->flags & IOBL_PINNED)) {\n+\t\tbl->flags &= ~IOBL_PINNED;\n+\t\tret = 0;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_unpin);\n+\n /* cap it at a reasonable 256, will be one page even for 4K */\n #define PEEK_MAX_IMPORT\t\t256\n \n@@ -747,6 +793,8 @@ int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t\treturn -ENOENT;\n \tif (!(bl->flags & IOBL_BUF_RING))\n \t\treturn -EINVAL;\n+\tif (bl->flags & IOBL_PINNED)\n+\t\treturn -EBUSY;\n \n \tscoped_guard(mutex, &ctx->mmap_lock)\n \t\txa_erase(&ctx->io_bl_xa, bl->bgid);\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 11d165888b8e..781630c2cc10 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -12,6 +12,11 @@ enum {\n \tIOBL_INC\t\t= 2,\n \t/* buffers are kernel managed */\n \tIOBL_KERNEL_MANAGED\t= 4,\n+\t/*\n+\t * buffer ring is pinned and cannot be unregistered by userspace until\n+\t * it has been unpinned\n+\t */\n+\tIOBL_PINNED\t\t= 8,\n };\n \n struct io_buffer_list {\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "20260210002852.1394504-7-joannelkoong@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the implementation of buffer recycling in kernel-managed buffer rings, explained that an interface for buffers to be recycled back into a kernel-managed buffer ring is being added, and confirmed that this is a preparatory patch for fuse over io-uring.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "preparatory"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add an interface for buffers to be recycled back into a kernel-managed\nbuffer ring.\n\nThis is a preparatory patch for fuse over io-uring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 11 +++++++++\n io_uring/kbuf.c              | 44 ++++++++++++++++++++++++++++++++++++\n 2 files changed, 55 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 702b1903e6ee..a488e945f883 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -88,6 +88,10 @@ int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t    unsigned issue_flags);\n+\n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -143,6 +147,13 @@ static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n+\t\t\t\t\t unsigned int buf_group, u64 addr,\n+\t\t\t\t\t unsigned int len, unsigned int bid,\n+\t\t\t\t\t unsigned int issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex dee1764ed19f..17b6178be4ce 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -102,6 +102,50 @@ void io_kbuf_drop_legacy(struct io_kiocb *req)\n \treq->kbuf = NULL;\n }\n \n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags)\n+{\n+\tstruct io_kiocb *req = cmd_to_io_kiocb(cmd);\n+\tstruct io_ring_ctx *ctx = req->ctx;\n+\tstruct io_uring_buf_ring *br;\n+\tstruct io_uring_buf *buf;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tif (WARN_ON_ONCE(req->flags & REQ_F_BUFFERS_COMMIT))\n+\t\treturn ret;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\n+\tif (!bl || WARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING)) ||\n+\t    WARN_ON_ONCE(!(bl->flags & IOBL_KERNEL_MANAGED)))\n+\t\tgoto done;\n+\n+\tbr = bl->buf_ring;\n+\n+\tif (WARN_ON_ONCE((br->tail - bl->head) >= bl->nr_entries))\n+\t\tgoto done;\n+\n+\tbuf = &br->bufs[(br->tail) & bl->mask];\n+\n+\tbuf->addr = addr;\n+\tbuf->len = len;\n+\tbuf->bid = bid;\n+\n+\treq->flags &= ~REQ_F_BUFFER_RING;\n+\n+\tbr->tail++;\n+\tret = 0;\n+\n+done:\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_kmbuf_recycle);\n+\n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)\n {\n \tstruct io_ring_ctx *ctx = req->ctx;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "20260210002852.1394504-8-joannelkoong@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring_is_kmbuf_ring() function, explaining that it returns true if there is a kernel-managed buffer ring at the specified buffer group. The author provided code changes to implement this functionality and stated that these changes are preparatory for upcoming fuse kernel-managed buffer support.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "preparatory"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "io_uring_is_kmbuf_ring() returns true if there is a kernel-managed\nbuffer ring at the specified buffer group.\n\nThis is a preparatory patch for upcoming fuse kernel-managed buffer\nsupport, which needs to ensure the buffer ring registered by the server\nis a kernel-managed buffer ring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h |  9 +++++++++\n io_uring/kbuf.c              | 20 ++++++++++++++++++++\n 2 files changed, 29 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex a488e945f883..04a937f6f4d3 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -92,6 +92,9 @@ int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t   u64 addr, unsigned int len, unsigned int bid,\n \t\t\t   unsigned int issue_flags);\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -154,6 +157,12 @@ static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned int buf_group,\n+\t\t\t\t\t  unsigned int issue_flags)\n+{\n+\treturn false;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 17b6178be4ce..797cc2f0a5e9 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -963,3 +963,23 @@ int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \n \treturn ret;\n }\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tbool is_kmbuf_ring = false;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (likely(bl) && (bl->flags & IOBL_KERNEL_MANAGED)) {\n+\t\tWARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING));\n+\t\tis_kmbuf_ring = true;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn is_kmbuf_ring;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_is_kmbuf_ring);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "20260210002852.1394504-9-joannelkoong@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_ring_buffer_select() function being inaccessible to callers who need it without holding the io_uring mutex. The author agreed that exporting this function is necessary for fuse io-uring, which may need to select a buffer from a kernel-managed bufring in atomic contexts.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "export"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Export io_ring_buffer_select() so that it may be used by callers who\npass in a pinned bufring without needing to grab the io_uring mutex.\n\nThis is a preparatory patch that will be needed by fuse io-uring, which\nwill need to select a buffer from a kernel-managed bufring while the\nuring mutex may already be held by in-progress commits, and may need to\nselect a buffer in atomic contexts.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 14 ++++++++++++++\n io_uring/kbuf.c              |  7 ++++---\n 2 files changed, 18 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 04a937f6f4d3..d4b5943bdeb1 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -95,6 +95,10 @@ int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \n bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t    unsigned int issue_flags);\n+\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -163,6 +167,16 @@ static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n {\n \treturn false;\n }\n+static inline struct io_br_sel io_ring_buffer_select(struct io_kiocb *req,\n+\t\t\t\t\t\t     size_t *len,\n+\t\t\t\t\t\t     struct io_buffer_list *bl,\n+\t\t\t\t\t\t     unsigned int issue_flags)\n+{\n+\tstruct io_br_sel sel = {\n+\t\t.val = -EOPNOTSUPP,\n+\t};\n+\treturn sel;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 797cc2f0a5e9..9a93f10d3214 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -226,9 +226,9 @@ static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n \treturn false;\n }\n \n-static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n-\t\t\t\t\t      struct io_buffer_list *bl,\n-\t\t\t\t\t      unsigned int issue_flags)\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags)\n {\n \tstruct io_uring_buf_ring *br = bl->buf_ring;\n \t__u16 tail, head = bl->head;\n@@ -261,6 +261,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \t}\n \treturn sel;\n }\n+EXPORT_SYMBOL_GPL(io_ring_buffer_select);\n \n struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \t\t\t\t  unsigned buf_group, unsigned int issue_flags)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "20260210002852.1394504-10-joannelkoong@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring_cmd_buffer_select() function not returning the selected buffer's id, and responded by modifying the function to return the id in addition to the address and size.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Return the id of the selected buffer in io_buffer_select(). This is\nneeded for kernel-managed buffer rings to later recycle the selected\nbuffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h   | 2 +-\n include/linux/io_uring_types.h | 2 ++\n io_uring/kbuf.c                | 7 +++++--\n 3 files changed, 8 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex d4b5943bdeb1..94df2bdebe77 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);\n \n /*\n  * Select a buffer from the provided buffer group for multishot uring_cmd.\n- * Returns the selected buffer address and size.\n+ * Returns the selected buffer address, size, and id.\n  */\n struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n \t\t\t\t\t    unsigned buf_group, size_t *len,\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 36cc2e0346d9..5a56bb341337 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -100,6 +100,8 @@ struct io_br_sel {\n \t\tvoid *kaddr;\n \t};\n \tssize_t val;\n+\t/* id of the selected buffer */\n+\tunsigned buf_id;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9a93f10d3214..24c1e34ea23e 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -250,6 +250,7 @@ struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n+\tsel.buf_id = req->buf_index;\n \tif (bl->flags & IOBL_KERNEL_MANAGED)\n \t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n \telse\n@@ -274,10 +275,12 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \n \tbl = io_buffer_get_list(ctx, buf_group);\n \tif (likely(bl)) {\n-\t\tif (bl->flags & IOBL_BUF_RING)\n+\t\tif (bl->flags & IOBL_BUF_RING) {\n \t\t\tsel = io_ring_buffer_select(req, len, bl, issue_flags);\n-\t\telse\n+\t\t} else {\n \t\t\tsel.addr = io_provided_buffer_select(req, len, bl);\n+\t\t\tsel.buf_id = req->buf_index;\n+\t\t}\n \t}\n \tio_ring_submit_unlock(req->ctx, issue_flags);\n \treturn sel;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "20260210002852.1394504-11-joannelkoong@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about indicating which buffer was selected in the completion queue entry, explained that this is needed for fuse to relay the information to userspace, and confirmed that the fix will be included in the patch.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a specific technical issue",
                "confirmed a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When uring_cmd operations select a buffer, the completion queue entry\nshould indicate which buffer was selected.\n\nSet IORING_CQE_F_BUFFER on the completed entry and encode the buffer\nindex if a buffer was selected.\n\nThis will be needed for fuse, which needs to relay to userspace which\nselected buffer contains the data.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/uring_cmd.c | 6 +++++-\n 1 file changed, 5 insertions(+), 1 deletion(-)\n\ndiff --git a/io_uring/uring_cmd.c b/io_uring/uring_cmd.c\nindex ee7b49f47cb5..6d38df1a812d 100644\n--- a/io_uring/uring_cmd.c\n+++ b/io_uring/uring_cmd.c\n@@ -151,6 +151,7 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \t\t       unsigned issue_flags, bool is_cqe32)\n {\n \tstruct io_kiocb *req = cmd_to_io_kiocb(ioucmd);\n+\tu32 cflags = 0;\n \n \tif (WARN_ON_ONCE(req->flags & REQ_F_APOLL_MULTISHOT))\n \t\treturn;\n@@ -160,7 +161,10 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \tif (ret < 0)\n \t\treq_set_fail(req);\n \n-\tio_req_set_res(req, ret, 0);\n+\tif (req->flags & (REQ_F_BUFFER_SELECTED | REQ_F_BUFFER_RING))\n+\t\tcflags |= IORING_CQE_F_BUFFER |\n+\t\t\t(req->buf_index << IORING_CQE_BUFFER_SHIFT);\n+\tio_req_set_res(req, ret, cflags);\n \tif (is_cqe32) {\n \t\tif (req->ctx->flags & IORING_SETUP_CQE_MIXED)\n \t\t\treq->cqe.flags |= IORING_CQE_F_32;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "20260210002852.1394504-12-joannelkoong@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested adding a WARN_ON_ONCE() to prevent int promotion from affecting the calculation of (br->tail - bl->head) >= bl->nr_entries, and noted that this is not a critical issue but rather something to be addressed in the future.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you want:\n\n\tif (WARN_ON_ONCE((__u16)(br->tail - bl->head) >= bl->nr_entries))\n\nhere to avoid int promotion from messing this up if tail has wrapped.\n\nIn general, across the patches for the WARN_ON_ONCE(), it's not a huge\nissue to have a litter of them for now. Hopefully we can prune some of\nthese down the line, however.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "3eb1116b-f48e-4bfd-9a0b-798a147f54ce@kernel.dk",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe questioned the need to set the selected buffer index in __io_uring_cmd_done(), suggesting that the caller could simply use req->buf_index instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm probably missing something here, but why can't the caller just use\nreq->buf_index for this?\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "4e12c801-4d3e-4c49-9a6d-6faba5e05063@kernel.dk",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe requested a branch with all patches and users applied, stating that some helpers require an exposed user for proper judgment.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request_for_additional_context"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Generally looks pretty good - for context, do you have a branch with\nthese patches and the users on top too? Makes it a bit easier for cross\nreferencing, as some of these really do need an exposed user to make a\ngood judgement on the helpers.\n\nI know there's the older series, but I'm assuming the latter patches\nchanged somewhat too, and it'd be nicer to look at a current set rather\nthan go back to the older ones.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "27cebab8-fb11-4199-a668-25aa259ef3b1@kernel.dk",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested refactoring io_pbuf_get_region() to handle kernel-managed buffer rings by adding a new helper function, io_kbuf_get_region(), and checking the bl->flags for IOBL_KERNEL_MANAGED in both functions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor nit",
                "more readable"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "For this, I think just add another helper - leave io_pbuf_get_region()\nand add a bl->flags & IOBL_KERNEL_MANAGED error check in there, and\nadd a io_kbuf_get_region() or similar and have a !(bl->flags &\nIOBL_KERNEL_MANAGED) error check in that one.\n\nThat's easier to read, and there's little reason to avoid duplicating\nthe xa_load() part.\n\nMinor nit, but imho it's more readable that way.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "31ad294f-8a73-4dd5-b303-addec950e96b@kernel.dk",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Jens Axboe suggested using a pointer to struct io_buffer_list *bl instead of passing it by value, and recommended returning an ERR_PTR if the function fails or renaming the parameter to **blret\n\nJens Axboe suggested a more efficient way to check if a buffer ring is both pinned and managed by the kernel, recommending a single bitwise AND operation instead of multiple conditional checks.\n\nReviewer Jens Axboe suggested that the patch should be modified to not enforce a character limit on io_uring strings, as this is acceptable for the io_uring implementation.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "suggested improvement",
                "recommended change",
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Probably use the usual struct io_buffer_list *bl here and either use an\nERR_PTR return, or rename the passed on **bl to **blret or something.\n\n---\n\nUsually done as:\n\n\tif ((bl->flags & (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))\n\nand maybe then just have an earlier\n\n\tif (!bl)\n\t\tgoto err;\n\n---\n\nto avoid making it way too long. For io_uring, it's fine to exceed 80\nchars where it makes sense.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "8826110e-cb5c-4923-99cd-b9f21f536d32@kernel.dk",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer noted that the patch fences itself off from optimizations for huge pages, which can be used when creating a region with user-passed memory\n\nPavel Begunkov noted that io_create_region() should be used instead of a new function in __io_uring_cmd_done(), as it does not introduce any new functionality and violates abstractions; he also suggested stripping buffer allocation from IORING_REGISTER_KMBUF_RING, replacing *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag, or requiring users to register a memory region of appropriate size using IORING_REGISTER_MEM_REGION\n\nReviewer Pavel Begunkov noted that the removal of io_create_region_multi_buf() means that buffer alignment is no longer necessary, and suggested that this could result in wasted memory due to 64KB page sizes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "optimizations",
                "huge pages",
                "requested changes",
                "suggested alternative approaches"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "If you're creating a region, there should be no reason why it\ncan't work with user passed memory. You're fencing yourself off\noptimisations that are already there like huge pages.\n\n---\n\nPlease use io_create_region(), the new function does nothing new\nand only violates abstractions.\n\nProvided buffer rings with kernel addresses could be an interesting\nabstraction, but why is it also responsible for allocating buffers?\nWhat I'd do:\n\n1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.\n2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.\n    Or maybe don't expose it to the user at all and create it from\n    fuse via internal API.\n3. Require the user to register a memory region of appropriate size,\n    see IORING_REGISTER_MEM_REGION, ctx->param_region. Make fuse\n    populating the buffer ring using the memory region.\n\nI wanted to make regions shareable anyway (need it for other purposes),\nI can toss patches for that tomorrow.\n\nA separate question is whether extending buffer rings is the right\napproach as it seems like you're only using it for fuse requests and\nnot for passing buffers to normal requests, but I don't see the\nbig picture here.\n\n---\n\nWith io_create_region_multi_buf() gone, you shouldn't need\nto align every buffer, that could be a lot of wasted memory\n(thinking about 64KB pages).",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "message_id": "89c75fc1-2def-4681-a790-78b12b45478a@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Caleb Mateos",
              "summary": "Reviewer Caleb Mateos noted that the patch's optimization in __io_uring_cmd_done() is unnecessary, as modern compilers will automatically perform this optimization and potentially optimize it further.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "optimization",
                "compiler"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, modern compilers will perform this optimization automatically.\nThey'll even optimize it further to !(~bl->flags &\n(IOBL_BUF_RING|IOBL_PINNED)): https://godbolt.org/z/xGoP4TfhP\n\nBest,\nCaleb",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "message_id": "CADUfDZoiHYKrfb=NxLH=K99ALuDoABCnrOFC4_mZgqvT6qQPXw@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested that the code should follow a common pattern for clarity and readability, citing that the current implementation is easier to read than the original.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes",
                "suggested improvement"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure, it's not about that, it's more about the common way of doing it,\nwhich makes it easier to read for people. FWIW, your example is easier\nto read too than the original.\n\n-- \nJens Axboe",
              "reply_to": "Caleb Mateos",
              "message_date": "2026-02-10",
              "message_id": "b8ed4d3b-efd0-42dc-8628-2a864b050518@kernel.dk",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author is asking a clarifying question about whether there are any optimizations possible with user-allocated buffers that wouldn't be achievable with kernel-allocated buffers, specifically in the context of huge pages.\n\nAuthor Joanne Koong responded to Pavel Begunkov's feedback that __io_uring_cmd_done() should set the selected buffer index, explaining that separate checks are needed between io_create_region() and io_create_region_multi_buf(), and different allocation calls require distinct functions.\n\nAuthor Joanne Koong is responding to feedback about kernel-managed buffer rings, specifically addressing concerns about registering buffers from userspace. She explains that allocating buffers from the kernel-side simplifies interface and lifecycle management, guarantees contiguous page allocation, and avoids complications with user-allocated buffers.\n\nAuthor responded to Pavel Begunkov's feedback by explaining that if kernel-managed buffer rings are squashed into existing pbuf rings, then pbuf rings would need to support pinning, which is necessary for fuse contexts where the uring mutex cannot be grabbed. The author notes they had previously proposed adding pinning to pbuf rings but it was rejected.\n\nAuthor clarified that the term 'normal requests' is specific to fuse's use case and does not apply to io_uring in general.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "question",
                "explanation",
                "asking for clarification",
                "providing explanation",
                "acknowledged a technical requirement",
                "explained reasoning",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Are there any optimizations with user-allocated buffers that wouldn't\nbe possible with kernel-allocated buffers? For huge pages, can't the\nkernel do this as well (eg I see in io_mem_alloc_compound(), it calls\ninto alloc_pages() with order > 0)?\n\n---\n\nThere's separate checks needed between io_create_region() and\nio_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag\nchecking) and different allocation calls (eg\nio_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).\nMaybe I'm misinterpreting your comment (or the code), but I'm not\nseeing how this can just use io_create_region().\n\n---\n\nConceptually, I think it makes the interface and lifecycle management\nsimpler/cleaner. With registering it from userspace, imo there's\nadditional complications with no tangible benefits, eg it's not\nguaranteed that the memory regions registered for the buffers are the\nsame size, with allocating it from the kernel-side we can guarantee\nthat the pages are allocated physically contiguously, userspace setup\nwith user-allocated buffers is less straightforward, etc. In general,\nI'm just not really seeing what advantages there are in allocating the\nbuffers from userspace. Could you elaborate on that part more?\n\n---\n\nIf kmbuf rings are squashed into pbuf rings, then pbuf rings will need\nto support pinning. In fuse, there are some contexts where you can't\ngrab the uring mutex because you're running in atomic context and this\ncan be encountered while recycling the buffer. I originally had a\npatch adding pinning to pbuf rings (to mitigate the overhead of\nregistered buffers lookups) but dropped it when Jens and Caleb didn't\nlike the idea. But for kmbuf rings, pinning will be necessary for\nfuse.\n\n---\n\nWhat are 'normal requests'? For fuse's use case, there are only fuse requests.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "message_id": "CAJnrk1ZZyYmwtzcHAnv2x8rt=ZVsz7CXCVV6jtgMMDZytyxp3A@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that accessing the buffer index from the caller side can be cumbersome and offered alternative solutions, such as introducing a helper function to retrieve the buffer ID.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "offered alternatives"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The caller can, but from the caller side they only have access to the\ncmd so they would need to do something like\n\nstruct io_kiocb *req = cmd_to_iocb_kiocb(ent->cmd);\nbuf_id = req->buf_index;\n\nwhich may be kind of ugly with looking inside io-uring internals.\nMaybe a helper here would be nicer, something like\nio_uring_cmd_buf_id() or io_uring_req_buf_id(). It seemed cleaner to\nme to just return the buf id as part of the io_br_sel struct, but I'm\nhappy to do it another way if you have a preference.\n\nThanks,\nJoanne",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "message_id": "CAJnrk1a419AKBCYf-1fkB8m0u-PwL5RRVZ6Vq9fiqBHqq+GUrA@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that additional changes are needed, specifically the userside changes on top of the patches, and plans to address them in v2 once a discussion with Pavel is resolved.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for further work",
                "plans to revise in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for reviewing the patches. The branch containing the userside\nchanges on top of these patches is in [1]. I'll make the changes you\npointed out in your other comments as part of v2. Once the discussion\nwith Pavel is resolved / figured out with the changes he wants for v2,\nI'll submit v2.\n\nThanks,\nJoanne\n\n[1] https://github.com/joannekoong/linux/commits/fuse_zero_copy/",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "message_id": "CAJnrk1ZmZ_EtQXc5BYqzNxV=Mx3q+K_WnbNTNKpOVugHz0q_1g@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that allocating 1MB in kernel space will not result in a PMD mappable huge page, unlike user space which can allocate 2MB and register the first 1MB for reuse\n\nReviewer Pavel Begunkov suggested that instead of changing io_create_region() to be less strict, the caller should filter arguments to ensure only necessary types are passed.\n\nPavel Begunkov noted that the memmap.c changes in the patch are unnecessary and can be dropped because they only provide contiguous memory within a single buffer, which is already achieved by default io_create_region(). He suggested removing these changes to avoid disabling the usefulness of io_mem_alloc_compound() and to decouple regions from buffer subdivision.\n\nReviewer Pavel Begunkov suggested adding a mechanism to handle user-provided memory for kernel-managed buffer rings, proposing the use of io_create_region() with specific flags and user address information.\n\nThe reviewer suggested separating ring creation from population on the kernel API level, and provided an example of how the fuse kernel module could populate rings without modifying the current layout.\n\nReviewer Pavel Begunkov suggested that instead of introducing new UAPI and internal changes for kernel-managed buffer rings, the existing pbuf implementation could be piggybacked on with a flag to differentiate between them. He proposed setting this flag in __io_uring_cmd_done() if IOU_PBUF_RING_KM is set in flags.\n\nreviewer noted that the patch did not provide buffer rings when pinning the registered buffer table, and suggested an alternative approach where all memory is kept in one larger registered buffer\n\nReviewer Pavel Begunkov expressed concerns that creating many small regions in kernel-managed buffer rings would lead to inefficient memory management, including extra mmap()s, user space overhead, and wasted space for kernel allocations, as well as over-accounting and increased memory footprint for user-provided memory. He also suggested that this approach would limit the ability to free buffers while requests are pending and raised suspicions about ring bound memory lifetimes.\n\nReviewer noted that kernel-managed buffer rings would be particularly useful for operations like read and recv, where the kernel can fill rings without requiring opcode-specific code changes in kbuf.c",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change to allocation size",
                "requested changes",
                "suggested alternative solution",
                "suggested separation",
                "provided alternative implementation",
                "suggested alternative approach",
                "questioning the need for separate UAPI",
                "suggested improvements",
                "no specific request or disagreement mentioned"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, there is handful of differences. To name one, 1MB allocation won't\nget you a PMD mappable huge page, while user space can allocate 2MB,\nregister the first 1MB and reuse the rest for other purposes.\n\n---\n\nIf io_create_region() is too strict, let's discuss that in\nexamples if there are any, but it's likely not a good idea changing\nthat. If it's too lax, filter arguments in the caller. IOW, don't\npass IORING_MEM_REGION_TYPE_USER if it's not used.\n\n---\n\nI saw that and saying that all memmap.c changes can get dropped.\nYou're using it as one big virtually contig kernel memory range then\nchunked into buffers, and that's pretty much what you're getting with\nnormal io_create_region(). I get that you only need it to be\ncontiguous within a single buffer, but that's not what you're doing,\nand it'll be only worse than default io_create_region() e.g.\neffectively disabling any usefulness of io_mem_alloc_compound(),\nand ultimately you don't need to care.\n\nRegions shouldn't know anything about your buffers, how it's\nsubdivided after, etc.\n\n---\n\nstruct io_uring_region_desc rd = {};\ntotal_size = nr_bufs * buf_size;\nrd.size = PAGE_ALIGN(total_size);\nio_create_region(&region, &rd);\n\nAdd something like this for user provided memory:\n\nif (use_user_memory) {\n\trd.user_addr = uaddr;\n\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n}\n\n---\n\nI don't think I follow. I'm saying that it might be interesting\nto separate rings from how and with what they're populated on the\nkernel API level, but the fuse kernel module can do the population\nand get exactly same layout as you currently have:\n\nint fuse_create_ring(size_t region_offset /* user space argument */) {\n\tstruct io_mapped_region *mr = get_mem_region(ctx);\n\t// that can take full control of the ring\n\tring = grab_empty_ring(io_uring_ctx);\n\n\tsize = nr_bufs * buf_size;\n\tif (region_offset + size > get_size(mr)) // + other validation\n\t\treturn error;\n\n\tbuf = mr_get_ptr(mr) + offset;\n\tfor (i = 0; i < nr_bufs; i++) {\n\t\tring_push_buffer(ring, buf, buf_size);\n\t\tbuf += buf_size;\n\t}\n}\n\nfuse might not care, but with empty rings other users will get a\nchannel they can use to do IO (e.g. read requests) using their\nkernel addresses in the future.\n\n---\n\nIt'd change uapi but not internals, you already piggy back it\non pbuf implementation and differentiate with a flag.\n\nIt could basically be:\n\nif (flags & IOU_PBUF_RING_KM)\n\tbl->flags |= IOBL_KERNEL_MANAGED;\n\nPinning can be gated on that flag as well. Pretty likely uapi\nand internals will be a bit cleaner, but that's not a huge deal,\njust don't see why would you roll out a separate set of uapi\n([un]register, offsets, etc.) when essentially it can be treated\nas the same thing.\n\n---\n\nIIRC, you was pinning the registered buffer table and not provided\nbuffer rings? Which would indeed be a bad idea. Thinking about it,\nfwiw, instead of creating multiple registered buffers and trying to\nlock the entire table, you could've kept all memory in one larger\nregistered buffer and pinned only it. It's already refcounted, so\nshouldn't have been much of a problem.\n\n---\n\nTo explain why, I don't think that creating many small regions\nis a good direction going forward. In case of kernel allocation,\nit's extra mmap()s, extra user space management, and wasted space.\nFor user provided memory it's over-accounting and extra memory\nfootprint. It'll also give you better lifecycle guarantees, i.e.\nyou won't be able to free buffers while there are requests for the\ncontext. I'm not so sure about ring bound memory, let's say I have\nmy suspicions, and you'd need to be extra careful about buffer\nlifetimes even after a fuse instance dies.\n\n---\n\nAny kind of read/recv/etc. that can use provided buffers. It's\nwhere kernel memory filled rings would shine, as you'd be able\nto use them together without changing any opcode specific code.\nI.e. not changes in read request implementation, only kbuf.c\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "message_id": "1c657f67-0862-4e13-9c71-7217aeecef61@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that any pages mapped to userspace can be allocated in the kernel, which would allow for a buffer ring that is only mapped read-only into userspace, enabling zero-copy raids if the device requires stable pages for checksumming or raid.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "appreciation for design",
                "positive comment on future implementation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Any pages mapped to userspace can be allocated in the kernel as well.\n\nAnd I really do like this design, because it means we can have a\nbuffer ring that is only mapped read-only into userspace.  That way\nwe can still do zero-copy raids if the device requires stable pages\nfor checksumming or raid.  I was going to implement this as soon\nas this series lands upstream.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "message_id": "aYykILfX_u9-feH-@infradead.org",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong explained that she originally used io_region_allocate_pages(), but it failed due to excessive memory allocation, so she chose to use io_region_allocate_pages_multi_buf() instead to bypass the issue.\n\nAuthor clarifies her understanding of reviewer's feedback, confirming she initially thought user should own and manage buffers, but now realizes kernel can allocate them through IORING_REGISTER_MEM_REGION interface.\n\nAuthor Joanne Koong addressed Pavel Begunkov's feedback about combining kernel-managed buffer rings (kmbufs) and regular pbufs into a single API, explaining that it would make the pbuf API more complex and harder to understand. She agreed to combine the interfaces in v2 unless someone else objects.\n\nAuthor acknowledged that she previously proposed pinning the registered buffer table, not the pbuf ring, and no further action is implied.\n\nThe author is addressing Pavel Begunkov's suggestion that sparse buffers populated by the kernel should be automatically pinned. The author expresses uncertainty about this idea and notes that if implemented, users would need to unregister buffers individually instead of using IORING_UNREGISTER_BUFFERS.\n\nAuthor is addressing a concern about buffer allocation, specifically whether individual buffers should be allocated separately by the kernel. She acknowledges the memory allocation issue and suggests making a change to allocate the region all at once if it's bypassable, but expresses disagreement with allocating separate buffers due to concerns about extra mmaps and userspace management.\n\nAuthor asked for clarification on reviewer's concerns about over-accounting and extra memory footprint in kernel-managed buffer rings.\n\nAuthor is addressing concerns about the API and kernel buffer allocation in the io_uring series. She plans to make changes in v2, including removing the KMBUF_RING API interface, having kernel buffer allocation go through IORING_REGISTER_MEM_REGION, and adding APIs for subsystems to populate a kernel-managed buffer ring with addresses from the registered memory region.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "clarification",
                "explanation",
                "understanding",
                "acknowledged fix is needed",
                "agreed to restructure",
                "acknowledgment of prior mistake",
                "uncertainty",
                "explaining trade-offs",
                "explaining reasoning",
                "clarifying question",
                "acknowledges fix is needed",
                "plans changes in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When I originally implemented it, I had it use\nio_region_allocate_pages() but this fails because it's allocating way\ntoo much memory at once. For fuse's use case, each buffer is usually\nat least 1 MB if not more. Allocating the memory one buffer a time in\nio_region_allocate_pages_multi_buf() bypasses the allocation errors I\nwas seeing. That's the main reason I don't think this can just use\nio_create_region().\n\n---\n\nOh okay, from your first message I (and I think christoph too) thought\nwhat you were saying is that the user should be responsible for\nallocating the buffers with complete ownership over them, and then\njust pass those allocated to the kernel to use. But what you're saying\nis that just use a different way for getting the kernel to allocate\nthe buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am\nI reading this correctly?\n\n---\n\nimo, it looked cleaner as a separate api because it has different\nexpectations and behaviors and squashing kmbuf into the pbuf api makes\nthe pbuf api needlessly more complex. Though I guess from the\nuserspace pov, liburing could have a wrapper that takes care of\nsetting up the pbuf details for kernel-managed pbufs. But in my head,\nhaving pbufs vs. kmbufs makes it clearer what each one does vs regular\npbufs vs. pbufs that are kernel-managed.\n\nEspecially with now having kmbufs go through the ioring mem region\ninterface, it makes things more confusing imo if they're combined, eg\npbufs that are kernel-managed are created empty and then populated\nfrom the kernel side by whatever subsystem is using them. Right now\nthere's only one mem region supported per ring, but in the future if\nthere's the possibility that multiple mem regions can be registered\n(eg if userspace doesn't know upfront what mem region length they'll\nneed), then we should also probably add in a region id param for the\nregistration arg, which if kmbuf rings go through the pbuf ring\nregistration api, is not possible to do.\n\nBut I'm happy to combine the interfaces and go with your suggestion.\nI'll make this change for v2 unless someone else objects.\n\n---\n\nYeah, you're right I misremembered and the objections / patch I\ndropped was pinning the registered buffer table, not the pbuf ring\n\n---\n\nHmm, I'm not sure this idea would work for sparse buffers populated by\nthe kernel, unless those are automatically pinned too but then from\nthe user POV for unregistration they'd need to unregister buffers\nindividually instead of just calling IORING_UNREGISTER_BUFFERS but it\nmight be annoying for them to now need to know which buffers are\npinned vs not. When i benchmarked the fuse code with vs without pinned\nregistered buffers, it didn't seem to make much of a difference\nperformance-wise thankfully, so I just dropped it.\n\n---\n\nTo clarify, is this in reply to why the individual buffers shouldn't\nbe allocated separately by the kernel?\nI added a comment about this above in the discussion about\nio_region_allocate_pages_multi_buf(), and if the memory allocation\nissue I was seeing is bypassable and the region can be allocated all\nat once, I'm happy to make that change. With having the allocation be\nseparate buffers though, I'm not sure I agree that there are extra\nmmaps / userspace management. All the pages across the buffers are\nvmapped together and the userspace just needs to do 1 mmap call for\nthem. On the userspace side, I don't think there's more management\nsince the mmapped address represents the range across all the buffers.\nI'm not seeing how there's wasted space either since the only\nrequirement is that the buffer size is page aligned. I think also\nthere's a higher chance of the entire buffer region being physically\ncontiguous if each buffer is allocated separately vs. all the buffers\nare allocated as 1 region. I don't feel strongly about this either way\nand I'm happy to allocate the entire region at once if that's\npossible.\n\n---\n\nJust out of curiosity, could you elaborate on the over-accounting and\nextra memory footprint? I was under the impression it would be the\nsame since the accounting gets adjusted by the total bytes allocated?\nFor the extra memory footprint, is the extra footprint from the\nmetadata to describe each buffer region, or are you referring to\nsomething else?\n\n---\n\nThanks for your input on the series. To iterate / sum up, these are\nchanges for v2 I'll be making:\n- api-wise from userspace/liburing: get rid of KMBUF_RING api\ninterface and have users go through PBUF_RING api instead with a flag\nindicating the ring is kernel-managed\n- have kernel buffer allocation go through IORING_REGISTER_MEM_REGION\ninstead, which means when the pbuf ring is created and the\nkernel-managed flag is set, the ring will be empty. The memory region\nwill need to be registered before the mmap call to the ring fd.\n- add apis for subsystems to populate a kernel-managed buffer ring\nwith addresses from the registered mem region\n\nDoes this align with your understanding of the conversation as well or\nis there anything I'm missing?\n\nAnd Christoph, do these changes for v2 work for your use case as well?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "message_id": "CAJnrk1YXmxqUnT561-J7seaicxFRJTyJ=F3_MX1rmtAROC6Ybg@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig argued against setting the selected buffer index in __io_uring_cmd_done(), citing a need for kernel-controlled allocation and guaranteeing user processes can only read memory, not write to it.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "opinion diverges from original patch"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm arguing exactly against this.  For my use case I need a setup\nwhere the kernel controls the allocation fully and guarantees user\nprocesses can only read the memory but never write to it.  I'd love\nto be able to piggy back than onto your work.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "message_id": "aY2mdLkqPM0KfPMC@infradead.org",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that using power-of-2 round ups for memory allocations will result in wasted memory, as 1MB allocations will not become 2MB huge pages, and also questioned the handling of 1GB huge pages, suggesting users may be able to make better placement decisions.\n\nThe reviewer suggests that the io_uring uapi should include fields for user-provided memory, making it an optional feature for pbuf rings/regions/etc., and notes that fuse can refuse to bind to buffer rings it doesn't like.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "memory_waste",
                "user_control",
                "reviewer's suggestion is neutral as they are not strongly advocating for a change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "pow2 round ups will waste memory. 1MB allocations will never\nbecome 2MB huge pages. And there is a separate question of\n1GB huge pages. The user can be smarter about all placement\ndecisions.\n\n---\n\nThat's an interesting case. To be clear, user provided memory is\nan optional feature for pbuf rings / regions / etc., and I think\nthe io_uring uapi should leave fields for the feature. However, I\nhave nothing against fuse refusing to bind to buffer rings it\ndoesn't like.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "message_id": "bd488a4e-a856-4fa5-b2bb-427280e6a053@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested modifying IORING_REGISTER_MEM_REGION to support read-only registrations, and proposed adding a new registration flag or rejecting unsupported setups during init.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal",
                "request for further discussion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IORING_REGISTER_MEM_REGION supports both types of allocations. It can\nhave a new registration flag for read-only, and then you either make\nthe bounce avoidance optional or reject binding fuse to unsupported\nsetups during init. Any arguments against that? I need to go over\nJoanne's reply, but I don't see any contradiction in principal with\nyour use case.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "message_id": "809cd04b-007b-46c6-9418-161e757e0e80@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author is clarifying the difference between kernel-managed buffer rings and user-initiated setup of kbuf rings. She explains that if userspace doesn't initiate the setup, IORING_REGISTER_MEM_REGION becomes semantically equivalent to kernel-managed allocation.\n\nAuthor addressed a concern about the complexity and potential over-engineering of the kernel-managed buffer ring interface, suggesting a simpler approach where the user populates the ring through the pbuf interface and adding an optional interface for IORING_REGISTERED_MEM_REGIONS in the future.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation",
                "overkill",
                "over-engineered"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"control the allocation fully\" do you mean for your use case, the\nallocation/setup isn't triggered by userspace but is initiated by the\nkernel (eg user never explicitly registers any kbuf ring, the kernel\njust uses the kbuf ring data structure internally and users can read\nthe buffer contents)? If userspace initiates the setup of the kbuf\nring, going through IORING_REGISTER_MEM_REGION would be semantically\nthe same, except the buffer allocation by the kernel now happens\nbefore the ring is created and then later populated into the ring.\nuserspace would still need to make an mmap call to the region and the\nkernel could enforce that as read-only. But if userspace doesn't\ninitiate the setup, then going through IORING_REGISTER_MEM_REGION gets\nuglier.\n\n---\n\nSo i guess the flow would have to be:\na) user calls io_uring_register_region(&ring, &mem_region_reg) with\nmem_region_reg.region_uptr's size field set to the total buffer size\n(and mem_region_reg.flags read-only bit set if needed)\n     kernel allocates region\nb) user calls mmap() to get the address of the region. If read-only\nbit was set, it gets a read-only address\nc) user calls io_uring_register_buf_ring(&ring, &buf_reg, flags) with\nbuf_reg.flags |= IOU_PBUF_RING_KERNEL_MANAGED\n     kernel creates an empty kernel-managed ring. None of the buffers\nare populated\nd) user tells X subsystem to populate the ring starting from offset Z\nin the registered mem region\ne) on the kernel side, the subsystem populates the ring starting from\noffset Z, filling it up using the buf_size and ring_entries values\nthat the user registered the ring with in c)\n\nTo be completely honest, the more I look at this the more this feels\nlike overkill / over-engineered to me. I get that now the user can do\nthe PMD optimization, but does that actually lead to noticeable\nperformance benefits? It seems especially confusing with them going\nthrough the same pbuf ring interface but having totally different\nexpectations.\n\nWhat about adding a straightforward kmbuf ring that goes through the\npbuf interface (eg the design in this patchset) and then in the future\nadding an interface for pbuf rings (both kernel-managed and\nnon-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if\nusers end up needing/wanting to have their rings populated that way?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "message_id": "CAJnrk1Y6YSw6Rkdh==RfL==n4qEYrrTcdbbS32sBn12jaCoeXg@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "reviewer suggested rounding up buffer allocation to a multiple of PTE levels to mitigate TLB pressure, rather than setting the selected buffer index in __io_uring_cmd_done()\n\nChristoph Hellwig questioned the meaning of 'pbuf' in the patch description, expressing confusion about how it relates to io_uring_register_buffers* and suggesting that web searches have become less useful for understanding io_uring APIs.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested change",
                "alternative solution",
                "confusion",
                "lack of clarity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure.  But if the application cares that much about TLB pressure\nI'd just round up to nice multtiple of PTE levels.\n\n---\n\nCan you clarify what you mean with 'pbuf'?  The only fixed buffer API I\nknow is io_uring_register_buffers* which always takes user provided\nbuffers, so I have a hard time parsing what you're saying there.  But\nthat might just be sign that I'm no expert in io_uring APIs, and that\nweb searches have degraded to the point of not being very useful\nanymore.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "message_id": "aY7QX-BIW-SMJ3h_@infradead.org",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer noted that IORING_REGISTER_MEM_REGION's purpose is unclear, as it is described in both the commit message and public documentation as related to cqs (completion queues), but this seems inconsistent.\n\nreviewer noted that the patch does not address their specific use case of block and file system I/O, which is different from the original fuse over io_uring series",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "inconsistency",
                "unclear",
                "use case mismatch"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IORING_REGISTER_MEM_REGION seems to be all about cqs from both your\ncommit message and the public documentation.  I'm confused.\n\n---\n\nMy use case is not about fuse, but good old block and file system\nI/O.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "message_id": "aY7RA8-65WE6Q9Fv@infradead.org",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that io_uring_register_buffers() only pins memory, allowing applications or other processes to modify it, which can cause issues for file systems and storage devices that need to verify checksums or rebuild data from parity.\n\nreviewer noted that the patch does not address the issue of buffer selection in __io_uring_cmd_done(), and requested a fix",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The idea is that the application tells the kernel that it wants to use\na fixed buffer pool for reads.  Right now the application does this\nusing io_uring_register_buffers().  The problem with that is that\nio_uring_register_buffers ends up just doing a pin of the memory,\nbut the application or, in case of shared memory, someone else could\nstill modify the memory.  If the underlying file system or storage\ndevice needs verify checksums, or worse rebuild data from parity\n(or uncompress), it needs to ensure that the memory it is operating\non can't be modified by someone else.\n\nSo I've been thinking of a version of io_uring_register_buffers where\nthe buffers are not provided by the application, but instead by the\nkernel and mapped into the application address space read-only for\na while, and I thought I could implement this on top of your series,\nbut I have to admit I haven't really looked into the details all\nthat much.\n\n---\n\nYes.  The PMD mapping also is not that relevant.  Both AMD (implicit)\nand ARM (explicit) have optimizations for contiguous PTEs that are\nalmost as valuable.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "message_id": "aY7ScyJOp4zqKJO7@infradead.org",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the provided buffer rings in this series are not clearly distinguished from registered buffers and questioned the need for io_uring to allocate payload memory, suggesting it is inflexible and may lead to kernel crashes if used with other types of requests. He also suggested making the code cleaner and more flexible for fuse's use case without questioning the I/O path.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "infelicities in design",
                "potential for kernel crashes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Registered, aka fixed, buffers are the ones you pass to\nIORING_OP_[READ,WRITE]_FIXED and some other requests. It's normally\ncreated by io_uring_register_buffers*() / IORING_REGISTER_BUFFERS*\nwith user memory, but there are special cases when it's installed\ninternally by other kernel components, e.g. ublk.\nThis series has nothing to do with them, and relevant parts of\nthe discussion here don't mention them either.\n\nProvided buffer rings, a.k.a pbuf rings, IORING_REGISTER_PBUF_RING\nis a kernel-user shared ring. The entries are user buffers\n{uaddr, size}. The user space adds entries, the kernel (io_uring\nrequests) consumes them and issues I/O using the user addresses.\nE.g. you can issue a IORING_OP_RECV request (+IOSQE_BUFFER_SELECT)\nand it'll grab a buffer from the ring instead of using sqe->addr.\n\npbuf rings, IORING_REGISTER_MEM_REGION, completion/submission\nqueues and all other kernel-user rings/etc. are internally based\non so called regions. All of them support both user allocated\nmemory and kernel allocations + mmap.\n\nThis series essentially creates provided buffer rings, where\n1. the ring now contains kernel addresses\n2. the ring itself is in-kernel only and not shared with user space\n3. it also allocates kernel buffers (as a region), populates the ring\n    with them, and allows mapping the buffers into the user space.\n\nFuse is doing both adding (kernel) buffers to the ring and consuming\nthem. At which point it's not clear:\n\n1. Why it even needs io_uring provided buffer rings, it can be all\n    contained in fuse. Maybe it's trying to reuse pbuf ring code as\n    basically an internal memory allocator, but then why expose buffer\n    rings as an io_uring uapi instead of keeping it internally.\n\n    That's also why I mentioned whether those buffers are supposed to\n    be used with other types of io_uring requests like recv, etc.\n\n2. Why making io_uring to allocate payload memory. The answer to which\n    is probably to reuse the region api with mmap and so on. And why\n    payload buffers are inseparably created together with the ring\n    and via a new io_uring uapi.\n\n    And yes, I believe in the current form it's inflexible, it requires\n    a new io_uring uapi. It requires the number of buffers to match\n    the number of ring entries, which are related but not the same\n    thing. You can't easily add more memory as it's bound to the ring\n    object. The buffer memory won't even have same lifetime as the\n    ring object -- allow using that km buffer ring with recv requests\n    and highly likely I'll most likely give you a way to crash the\n    kernel.\n\nBut hey, I'm tired. I don't have any beef here and am only trying\nto make it a bit cleaner and flexible for fuse in the first place\nwithout even questioning the I/O path. If everyone believes\neverything is right, just ask Jens to merge it.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "message_id": "34cf24a3-f7f3-46ed-96be-bf716b2db060@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer expressed concern that the patch introduces using buffer rings for huge payload buffers, which was not their original intention and may lead to memory waste.\n\nreviewer questioned the feasibility of kernel-managed buffer rings without a kernel component returning buffers into the ring, citing that io_uring does not currently support this",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "memory waste",
                "confusion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Think of it as an area of memory for kernel-user communication. Used\nfor syscall parameters passing to avoid copy_from_user, but I added\nit for a bunch of use cases. We'll hopefully get support at some\npoint for passing request arguments like struct iovec. BPF patches\nuse it for communication. I need to respin patches placing SQ/CQ onto\nit (avoid some memory waste).\n\nTbh, I never meant it nor io_uring regions to be used for huge\npayload buffers, but this series already uses regions for that.\n\n---\n\nThen I'm confused. Take a look at the other reply, this series is\nabout buffer rings with kernel memory, it can't work without a kernel\ncomponent returning buffers into the ring, and io_uring doesn't do\nthat. But maybe you're thinking about adding some more elaborate API.\n\nIIUC, Joanne also wants to add support for fuse installing registered\nbuffers, which would allow zero-copy, but those got split out of\nthis series.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "message_id": "df989700-fc4f-4334-a7c5-a6eeb136ab35@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the patch does not handle a specific edge case where the selected buffer index is out of range, and suggested working around this issue by wrapping the code in a loop.\n\nReviewer Pavel Begunkov noted that the patch should disentangle memory allocation from ring creation in the io_uring uapi, and instead move ring population into fuse, where it will be populated by the kernel without user space access to the ring.\n\nReviewer Pavel Begunkov noted that the differences between the two buffer allocation paths are minimal and suggested that they could be handled by a single opcode, but did not strongly object to making them separate opcodes.\n\nThe reviewer noted that without patches using the kernel-managed buffer rings functionality, it is inconvenient to test or verify its correctness. They suggested a control path io-uring command (FUSE_CMD_BIND_BUFFER_RING) to bind a fuse buffer ring to an io_uring region and buf_ring, which would allow passing necessary parameters to the bind_queue function.\n\nreviewer questioned the need for a separate buffer region, suggesting use of IORING_REGISTER_MEM_REGION instead\n\nReviewer noted that when allocating huge pages for buffers, the total allocation size may not be a power of two, potentially leading to wasted space due to alignment requirements, and requested consideration for this scenario.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "workaround",
                "disagreement with current approach",
                "suggestion for alternative solution",
                "no strong opinion",
                "open to alternative",
                "inconvenience",
                "suggested change",
                "potential performance issue",
                "alignment requirement"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Let's fix that then. For now, just work it around by wrapping\ninto a loop.\n\nBtw, I thought you're going to use it for metadata like some\nfuse headers and payloads would be zero copied by installing\nit as registered buffers.\n\n...\n\n---\n\nThe main point is disentangling memory allocation from ring\ncreation in the io_uring uapi, and moving ring population\ninto fuse instead of doing it at creation. And it'll still be\npopulated by the kernel (fuse), user space doesn't have access\nto the ring. IORING_REGISTER_MEM_REGION is just the easiest way\nto achieve that without any extra uapi.\n\n...\n\n---\n\nIt appeared to me that they're different because of special\nregion path and embedded buffer allocations, and otherwise\ndifferences would be minimal. But if you think it's still\nbetter to be made as a separate opcode, I'm not opposing it,\ngo for it.\n\n---\n\nNot having patches using the functionality is inconvenient. How\nfuse looks up the buffer ring from io_uring? I could imagine you\nhave some control path io-uring command:\n\ncase FUSE_CMD_BIND_BUFFER_RING:\n\treturn bind_queue(params);\n\nThen you can pass all necessary parameters to it, pseudo code:\n\nstruct fuse_bind_kmbuf_ring_params {\n\tregion_id;\n\tbuf_ring_id;\n\t...\n};\n\nbind_queue(cmd, struct fuse_bind_kmbuf_ring_params *p)\n{\n\tregion = io_uring_get_region(cmd, p->region_id);\n\t// get exclusive access:\n\tbuf_ring = io_uring_get_buf_ring(cmd, p->buf_ring_id);\n\n\tif (!validate_buf_ring(buf_ring))\n\t\treturn NOTSUPPORTED;\n\n\tio_uring_pin(buf_ring);\n\tfuse_populate_buf_ring(buf_ring, region, ...);\n}\n\nDoes that match expectations? I don't think you even need\nthe ring part exposed as an io_uring uapi, tbh, as it\nstays completely in fuse and doesn't meaningfully interact\nwith the rest of io_uring.\n\n...\n\n---\n\nThat was about an argument for using IORING_REGISTER_MEM_REGION\ninstead a separate region. And it's separate from whether\nbuffers should be bound to the ring.\n\n---\n\nI shouldn't affect you much since you have such large buffers,\nbut imagine the total allocation size is not being pow2, and\nthe kernel allocating it as a single folio. E.g. 3 buffers,\n0.5 MB each, total = 1.5MB, and the kernel allocates a 2MB\nhuge page.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "message_id": "43f34edf-6a34-4afb-b0a3-0d81ec037a96@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer argued that the series does not address registered buffers and suggested separating buffer allocation for io_uring",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There is nothing about registered buffers in this series. And even\nif you try to reuse buffer allocation out of it, it'll come with\na circular buffer you'll have no need for. And I'm pretty much\narguing about separating those for io_uring.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "message_id": "7c241b57-95d4-4d58-8cd3-369751f17df1@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested reusing regions for allocations and mmap()ing, wrapping them into a registered buffer to avoid vmap'ing altogether.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, the easiest solution is to internally reuse regions for\nallocations and mmap()'ing and wrap it into a registered buffer.\nIt just need to make vmap'ing optional as it won't be needed.\n\n-- \nPavel Begunkov",
              "reply_to": "",
              "message_date": "2026-02-13",
              "message_id": "a7d9d3ca-16b1-4299-a7fe-2fc19ca894cb@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that the io_uring uapi should not be tied to specific use cases or requirements, such as uniform buffer sizes, ring size matching buffer count, and buffers being allocated by io_uring. He questioned why these constraints are necessary and suggested that the design should allow for more flexibility, including the ability to add memory at runtime.\n\nreviewer questioned the separation of buffers from rings, expressing uncertainty about the differences between in-kernel buffers with kernel addresses and user-visible buffers with user addresses",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "uncertainty",
                "lack of clear expectations"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, it's mainly about not keeping payload buffers and rings in the same\nobject from the io_uring uapi perspective.\n\n1. If it's an io_uring uapi, it shouldn't be fuse specific or with\na bunch of use case specific expectations attached. Why does it\nrequire all buffers to be uniform in size? Why does it require\nthe ring size to match the number of buffers? Why does it require\nbuffers to be allocated by io_uring in the first place? Maybe some\nsubsystem got memory from somewhere else and wants to do use it\nwith io_uring. Why does it need to know the total size at creation,\nand what would you do if you want to add more memory at runtime\nwhile using the same ring?\n\n2. If it's meant to be fuse specific and _not_ used with other requests\nlike recv/read/etc., then what's the point of having it as an io_uring\nuapi? Which also adds additional trouble like the once you're solving\nwith pinning.\n\nIf it's supposed to be used with other requests, then buffers and\nrings will have different in-kernel lifetime expectations imposed\nby io_uring, so having them together won't even help with\nmanagement.\n\nI have a strong opinion about the memmap.c change. For the\nrest, if you believe it's fine, just send it out and let Jens\ndecide.\n\n---\n\nIt's predicated on separating buffers from rings, see above,\nand assuming that I'm not sure what expectations are different\napart from one being in-kernel with kernel addresses and the\nother user visible with user addresses.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "message_id": "cecca7f8-064b-475e-b887-057891377b87@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to Pavel Begunkov's feedback by agreeing that the circular buffer will be useful for Christoph's use case, which involves differently sized read payloads across requests. The author believes this will reduce memory allocation and enable sharing of buffers across entries.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed with the approach",
                "acknowledged a benefit"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think the circular buffer will be useful for Christoph's use case in\nthe same way it'll be useful for fuse's. The read payload could be\ndifferently sized across requests, so it's a lot of wasted space to\nhave to allocate a buffer large enough to support the max-size request\nper entry in the io_ring. With using a circular buffer, buffers have a\nway to be shared across entries, which means we can significantly\nreduce how much memory needs to be allocated.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "message_id": "CAJnrk1b2BHwBzz+AS7x0WuJSpf98x1xGhf1ys2rm4Ffb0_5TOA@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author agrees that the patch's use case aligns with Christoph Hellwig's, but notes that his buffers need to be read-only, implying that a modification is needed to accommodate this difference.\n\nAuthor addressed Christoph's concern about making the mmap call return a read-only mapping by proposing to add a read-only flag in io_uring_register_buf_ring() and checking it when userspace makes the mmap call, or using IORING_MEM_REGION to allocate memory with a read-only flag. Author is willing to add this patch to the series if needed.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges feedback",
                "implies fix is needed",
                "willingness to add additional patch",
                "proposing alternative solutions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "(resending because I hit reply instead of reply-all)\n\nI think we have the exact same use case, except your buffers need to\nbe read-only. I think your use case benefits from the same memory wins\nwe'll get with incremental buffer consumption, which is the primary\nreason fuse is using a bufring instead of fixed buffers.\n\n---\n\nI think you can and it'll be very easy to do so. All that would be\nneeded is to pass in a read-only flag from the userspace side when it\nregisters the bufring, and then when userspace makes the mmap call to\nthe bufring, the kernel checks if that read-only flag is set on the\nbufring and if so returns a read-only mapping. I'm happy to add that\npatch to this series if that would make things easier for you. The\nio_uring_register_buffers() api registers fixed buffers (which have to\nbe user-allocated memory) so you would need to go through the\nio_uring_register_buf_ring() api once kmbufs are squashed into the\npbuf interface.\n\nWith going through IORING_MEM_REGION, this would work for your use\ncase as well. The user would have to register the mem region with\nio_uring_register_region() and pass in a read-only flag, and then the\nkernel will allocate the memory region. Then userspace would mmap the\nmemory region and on the kernel side, it would set the mapping to be\nread-only. When the kmbufring then gets registered, the buffers in it\nwill be empty. The filesystem will then have to populate the buffers\nin it from the mem region that was previously registered.\n\nThanks,\nJoanne",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "message_id": "CAJnrk1ZnfdY9j1V8ijWx29jaLcuRH46jpNqR1x5E-Zqfz7MXVg@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Bernd Schubert",
              "summary": "Reviewer questioned the usefulness of sharing buffers across io_uring entries, suggesting it would only reduce the ring size and not provide any benefits.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning usefulness",
                "suggested alternative"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Dunno, what we actually want is requests of multiple sizes. Sharing\nbuffers across entries sounds like just reducing the ring size - I\npersonally don't see the point here.\n\n\nThanks,\nBernd",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "message_id": "d9e25d62-d63c-4e09-9607-360c4a847087@bsbernd.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarified that 'sharing buffers across entries' means allowing different parts of a buffer to be used simultaneously by multiple io_uring entries, addressing Bernd's feedback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"sharing buffers across entries\" what I mean is different regions\nof the buffer can now be used concurrently by multiple entries.\n\nThanks,\nJoanne",
              "reply_to": "Bernd Schubert",
              "message_date": "2026-02-13",
              "message_id": "CAJnrk1Ys6_7TuUSvEvWfre0oHCT6NKqdQSHXtRERt-ktHDbMkQ@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing concerns about the need for kernel-managed buffer rings in the context of fuse's use case, specifically to control when buffers get recycled back into the ring. The author explains that this is necessary because the server needs to write data back to the kernel in the same buffer after submitting an sqe, and then the client needs to recycle that buffer so it can be reused for a future cqe.\n\nAuthor acknowledged that the selected buffer index needs to be set in __io_uring_cmd_done() for userspace/server-side operations, and agreed to add this functionality.\n\nAuthor addressed Pavel Begunkov's feedback about using a registered memory region, agreeing it allows optimizations but questioning whether most use cases benefit from them, and suggesting that offering both simple and advanced kernel-managed buffer options is sufficient.\n\nThe author addressed Pavel Begunkov's concern that combining the interface for kernel-managed buffer rings (kmbufs) and user-provided buffer rings (pbufs) through a single uapi is confusing, particularly given the different expectations and behaviors of kmbufs. The author acknowledged their initial opinion but agreed to restructure in v2 by having kmbufs go through the pbuf uapi.\n\nAuthor responded to feedback about having a ring entry with no buffer associated with it, stating that this is similar to existing code and can be fixed by passing the number of buffers from the uapi for kernel-managed pbuf rings.\n\nAuthor responded to feedback from Pavel Begunkov by explaining that adding more memory to the registered memory region is not feasible and that users may need to allocate upfront, which could be challenging in certain scenarios.\n\nAuthor Joanne Koong addressed a concern about the lifetime of buffer memory in relation to the ring object, explaining that the buffers are only freed when the ring itself is freed.\n\nAuthor is addressing Pavel Begunkov's feedback about whether the patch should support both a simple kernel-managed pbuf interface and a more complex one that goes through a registered memory region, and is open to making changes based on reviewer input.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "clarification",
                "explanation",
                "acknowledged a fix is needed",
                "questioning the value of added complexity",
                "suggesting a simpler approach",
                "agreed to restructure",
                "no clear resolution signal",
                "author provides explanation",
                "acknowledged a challenge",
                "provided explanation",
                "open to change",
                "willing to make adjustments"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The most important part and the whole reason fuse needs the buffer\nring to be kernel-managed is because the kernel needs to control when\nbuffers get recycled back into the ring. For fuse's use case, the\nbuffer is used for passing data between the kernel and the server. We\ncan't have the server recycle the buffer because the server writes\nback data to the kernel in that buffer when it submits the sqe. After\nfuse receives the sqe and reads the reply from the server, it then\nneeds to recycle that buffer back into the ring so it can be reused\nfor a future cqe (eg sending a future request).\n\n---\n\nOn the userspace/server side, it uses the buffers for other io-uring\noperations (eg reading or writing the contents from/to a\nlocally-backed file).\n\n---\n\nMy main motivation for this is simplicity. I see (and thanks for\nexplaining) that using a registered mem region allows the use of some\noptimizations (the only one I know of right now is the PMD one you\nmentioned but maybe there's more I'm missing) that could be useful for\nsome workloads, but I don't think (and this could just be my lack of\nunderstanding of what more optimizations there are) most use cases of\nkmbufs benefit from those optimizations, so to me it feels like we're\nadding non-trivial complexity for no noticeable benefit.\n\nI feel like we get the best of both worlds by letting users have both:\nthe simple kernel-managed pbuf where the kernel allocates the buffers\nand the buffers are tied to the lifecycle of the ring, and the more\nadvanced kernel-managed pbuf where buffers are tied to a registered\nmemory region that the subsystem is responsible for later populating\nthe ring with.\n\n---\n\nimo it felt cleaner to have a new uapi for it because kmbufs and pbufs\nhave different expectations and behaviors (eg pbufs only work with\nuser-provided buffers and requires userspace to populate the ring\nbefore using it, whereas for kmbufs the kernel allocates the buffers\nand populates it for you; pbufs require userspace to recycle back the\nbuffer, whereas for kmbufs the kernel is the one in control of\nrecycling) and from the user pov it seemed confusing to have kmbufs as\npart of the pbuf ring uapi, instead of separating it out as a\ndifferent type of ringbuffer with a different expectation and\nbehavior. I was trying to make the point that combining the interface\nif we go with IORING_MEM_REGION gets even more confusing because now\npbufs that are kernel-managed are also empty at initialization and\nonly can point to areas inside a registered mem region and the\nresponsibility of populating it is now on whatever subsystem is using\nit.\n\nI still have this opinion but I also think in general, you likely know\nbetter than I do what kind of io-uring uapi is best for io-uring's\nusers. For v2 I'll have kmbufs go through the pbuf uapi.\n\n---\n\nI'm not really seeing what the purpose of having a ring entry with no\nbuffer associated with it is. In the existing code for non-kernel\nmanaged pbuf rings, there's the same tie between reg->ring_entries\nbeing used as the marker for how many buffers the ring supports. But\nif the number of buffers should be different than the number of ring\nentries, this can be easily fixed by passing in the number of buffers\nfrom the uapi for kernel-managed pbuf rings.\n\n---\n\nTo play devil's advocate, we also can't easily add more memory to the\nmem region once it's been registered. I think there's also a worse\npenalty where the user needs to know upfront how much memory to\nallocate for the mem region for the lifetime of the ring, which imo\nmay be hard to do (eg if a kernel-managed buf ring only needs to be\nregistered for some code paths and not others, the mem region\nregistration would still have to allocate the memory a potential kbuf\nring would use).\n\n---\n\nI'm a bit confused by this part. The buffer memory does have the same\nlifetime as the ring object, no? The buffers only get freed when the\nring itself is freed.\n\n---\n\nI appreciate you looking at this and giving your feedback and insight.\nThank you for doing so. I don't want to merge in something you're\nunhappy with.\n\nAre you open to having support for both a simple kernel-managed pbuf\ninterface and later on if/when the need arises, a kernel-managed pbuf\ninterface that goes through a registered memory region? If the answer\nis no, then I'll make the change to have kmbufs go through the\nregistered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "message_id": "CAJnrk1a+YuPpoLghA01uJhEKrhmrLhQ+5bw2OeeuLG3tG8p6Ew@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that buffer rings are not suitable for storage read/write requests because they immediately bind to a buffer, whereas other types of requests like recv first poll the socket and then take a buffer from the ring. He also pointed out that someone needs to return buffers back into the kernel private ring, which is currently assumed to be handled by the fuse driver but poses a problem for normal rw requests.\n\nReviewer Pavel Begunkov suggested using IORING_MEM_REGION or a standalone registered buffer extension to provide buffers/memory without extra semantics, potentially yielding a finer API.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "problem with current implementation",
                "suggested alternative approach",
                "potential for improved API"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Provided buffer rings are not useful for storage read/write requests\nbecause they bind to a buffer right away, that's in contrast to some\nrecv request, where io_uring will first poll the socket to confirm\nthe data is there, and only then take a buffer from the buffer ring\nand copy into it. With storage rw it makes more sense to specify\nthe buffer directly gain control over where exactly data lands\nIOW, instead of the usual \"read data into a given pointer\" request\nsemantics like what read(2) gives you, buffer rings are rather\n\"read data somewhere and return a pointer to where you placed it\".\n\nAnother problem is that someone needs to return buffers back into\nthe buffer ring, and it's a kernel private ring. For this patchset\nit's assumed the fuse driver is going to be doing that, but there\nis no one for normal rw requests.\n\n---\n\nYes. You only need buffers, and it'll be better to base on sth that\ngives you buffers/memory without extra semantics, i.e.\nIORING_MEM_REGION. Or it can be a standalone registered buffer\nextension, likely reusing regions internally. That might even yield\na finer API.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-18",
              "message_id": "b19e0496-6d3b-4e2b-8853-07848768a553@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer questioned whether kernel-managed buffer rings can be used with other requests, specifically IORING_OP_RECV with IOSQE_BUFFER_SELECT\n\nReviewer Pavel Begunkov commented on the patch, noting that there are two separate issues: (1) making buffers inseparable from buffer rings in the io_uring user API and (2) optionally allowing user memory for buffer creation. He suggests implementing this by passing an argument while creating a region.\n\nreviewer questioned the necessity of making buffer rings an io_uring API, suggesting it could be simpler to implement in fuse or as an implementation detail within io_uring\n\nReviewer Pavel Begunkov noted that the current implementation of kernel-managed buffer rings in io_uring is not reusable and specific to fuse use case, suggesting a middle ground approach where km rings can be registered together with memory as a pure region without buffer notion, allowing fuse to chunk it later.\n\nreviewer noted that the patch introduces a non-generic io_uring uapi, which is assumed to be generic in other parts of the code, and requested clarification on this design decision\n\nThe reviewer noted that the current implementation of __io_uring_cmd_done() only sets the buffer ring depth but does not account for the actual memory allocated by userspace, which could lead to issues if the user allocates more memory than the ring size or vice versa. The reviewer suggests considering dynamic allocation and de-fragmentation mechanisms.\n\nThe reviewer suggested that instead of passing the number of buffers to io_uring, the kernel should allocate a large chunk of memory and let fuse manage the buffer allocation.\n\nReviewer Pavel Begunkov agreed with the patch but noted that adding new memory would require a new mechanism, not necessarily tied to IORING_REGISTER_MEM_REGION.\n\nThe reviewer noted that unregistering a buffer ring does not guarantee the absence of inflight requests using buffers from the ring, and requested synchronization with all other io_uring requests to address this issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "question",
                "clarification",
                "no clear signal",
                "request for implementation",
                "questioning necessity",
                "suggesting alternative approaches",
                "requested changes",
                "design decision",
                "non-generic uapi",
                "agreed",
                "noted",
                "synchronization",
                "inflight requests"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Oops, typo. I was asking whether the buffer rings (not buffers) are\nsupposed to be used with other requests. E.g. submitting a\nIORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying\nyour kernel-managed buffer ring.\n\n---\n\nThere are two separate arguments. The first is about not making buffers\ninseparable from buffer rings in the io_uring user API. Whether it's\nIORING_REGISTER_MEM_REGION or something else is not that important.\nI have no objection if it's a part of fuse instead though, e.g. if\nfuse binds two objects together when you register it with fuse, or even\nif fuse create a buffer ring internally (assuming it doesn't indirectly\nleak into io_uring uapi).\n\nAnd the second was about optionally allowing user memory for buffer\ncreation as you're reusing the region abstraction. You can find pros\nand cons for both modes, and funnily enough, SQ/CQ were first kernel\nallocated and then people asked for backing it by user memory, and IIRC\nit was in the reverse order for pbuf rings.\n\nImplementing this is trivial as well, you just need to pass an argument\nwhile creating a region. All new region users use struct\nio_uring_region_desc for uapi and forward it to io_create_region()\nwithout caring if it's user or kernel allocated memory.\n\n---\n\nThe stress is on why it's an _io_uring_ API. It doesn't matter to me\nwhether it's a separate opcode or not. Currently, buffer rings don't give\nyou anything that can't be pure fuse, and it might be simpler to have\nit implemented in fuse than binding to some io_uring object. Or it could\ncreate buffer rings internally to reuse code but it doesn't become an\nio_uring uapi but rather implementation detail. And that predicates on\nwhether km rings are intended to be used with other / non-fuse requests.\n\n---\n\nI believe the source of disagreement is that you're thinking\nabout how it's going to look like for fuse specifically, and I\nbelieve you that it'll be nicer for the fuse use case. However,\non the other hand it's an io_uring uapi, and if it is an io_uring\nuapi, we need reusable blocks that are not specific to particular\nusers.\n\nIf it km rings has to stay an io_uring uapi, I guess a middle\nground would be to allow registering km rings together with memory,\nbut make it a pure region without a notion of a buffer, and let\nfuse to chunk it. Later, we can make payload memory allocation\noptional.\n\n---\n\nRight, intentionally so, because otherwise it's a fuse uapi that\npretends to be a generic io_uring uapi but it's not because of\nall assumptions in different places.\n\n---\n\nNot really, it tells the buffer ring depth but says nothing about\nhow much memory user space allocated and how it's pushed. It's a\nreasonable default but they could be different. For example, if you\nexpect adding more memory at runtime, you might create the buffer\nring a bit larger. Or when server processing takes a while and you\ncan't recycle until it finishes, you might have more buffers than\nyou need ring entries. Or you might might decide to split buffers\nand as you mentioned incremental consumption, which is an entire\nseparate topic because it doesn't do de-fragmentation and you'd\nneed to have it in fuse, just like user space does with pbufs.\n\n---\n\nMy entire point is that we're making lots of assumptions for io_uring\nuapi, and if it's moved to fuse because it knows better what it\nneeds, it should be a win.\n\nIOW, it sounds better if instead of passing the number of buffers to\nio_uring, you just ask it to create a large chunk of memory, and then\nfuse chunks it up and puts into the ring.\n\n---\n\nI agree, and you'd need something new in either case to add more\nmemory, and it doesn't need to be IORING_REGISTER_MEM_REGION\nspecifically.\n\n---\n\nUnregistering a buffer ring doesn't guarantee that there are no\ninflight requests that are still using buffers that came out of\nthe buffer ring. The fuse driver can wait/terminate its requests\nbefore unregisteration, but allow userspace issued IORING_OP_RECV\nto use this km buffer ring, and you'll need to somehow synchronise\nwith all other io_uring requests.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "message_id": "7a62c5a9-1ac2-4cc2-a22f-e5b0c52dabea@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that a fix is needed for the selected buffer index in __io_uring_cmd_done() and promised to modify it in v2.\n\nAuthor confirmed that kernel-managed buffer rings are intended for use with other io-uring requests, specifically to avoid per-i/o page pinning overhead costs.\n\nAuthor Joanne Koong addressed Pavel Begunkov's feedback on the design of kernel-managed buffer rings, agreeing that having buffers owned by the ring and tied to its lifetime is a more generically useful concept. She proposed modifying the API to allow for dynamic allocation of memory regions and using the registered region's pages array to store associated pages. The author suggested repurposing struct io_uring_sqe fields to include an offset into the registered mem region, adding an IOSQE flag to indicate page lookup from the registered region, and sending the buffer id of the registered mem region through the 'IORING_CQE_F_BUFFER' mechanism.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for fix",
                "promised modification",
                "acknowledged the purpose of the feature",
                "confirmed its intended usage",
                "agreed with feedback",
                "proposed modifications"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sorry, I submitted v2 last night thinking the conversation on this\nthread had died. After reading through your reply, I'll modify v2.\n\n---\n\nYes the buffer rings are intended to be used with other io-uring\nrequests. The ideal scenario is that the user can then do the\nequivalent of IORING_OP_READ/WRITE_FIXED operations on the\nkernel-managed buffers and avoid the per-i/o page pinning overhead\ncosts.\n\n---\n\nI agree 100%. The api we add should be what's best for io-uring, not fuse.\n\nFor the majority of use cases, it seemed to me that having the buffers\nseparated from the buffer rings didn't yield perceptible benefits but\nadded complexity and more restrictions like having to statically know\nup front how big the mem region needs to be across the lifetime of the\nio-uring for anything the io-uring might use the mem region for. It\nseems more generically useful as a concept to have the buffers owned\nby the ring and tied to the lifetime of the ring. I like how with this\ndesign everything is self-contained and multiple subsystems can use it\nwithout having to reimplement functionality locally in the subsystem.\nOn the other hand, I see your point about how it might be something\nusers want in the future if they want complete control over which\nparts of the mem region get used as the backing buffers to do stuff\nlike PMD optimizations.\n\nI think this is a matter of opinion/preference and I think in general\nfor anything io-uring related, yours should take precedence.\n\nWith it going through a mem region, I don't think it should even go\nthrough the \"pbuf ring\" interface then if it's not going to specify\nthe number of entries and buffer sizes upfront, if support is added\nfor io-uring normal requests (eg IORING_OP_READ/WRITE) to use the\nbacking pages from a memory region and if we're able to guarantee that\nthe registered memory region will never be able to be unregistered by\nthe user. I think if we repurpose the\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n};\n\nfields in the struct io_uring_sqe to\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n  __u64 offset; /* offset into registered mem region */\n};\n\nand add some IOSQE_ flag to indicate it should find the pages from the\nregistered mem region, then that should work for normal requests.\nWhere on the kernel side, it looks up the associated pages stored in\nthe io_mapped_region's pages array for the offset passed in.\n\nRight now there's only a uapi to register a memory region and none to\nunregister one. Is it guaranteed that io-uring will never add\nsomething in the future that will let userspace unregister the memory\nregion or at least unregister it while it's being used (eg if we add\nfuture refcounting to it to track active uses of it)?\n\nIf so, then end-to-end, with it going through the mem region, it would\nbe something like:\n* user creates a mem region for the io-uring\n* user mmaps the mem region\n* user passes in offset into region, length of each buffer, and number\nof entries in the ring to the subsystem\n* subsystem creates a locally managed bufring and adds buffers to that\nring from the mem region\n* on the cqe side, it sends the buffer id of the registered mem region\nthrough the same \"IORING_CQE_F_BUFFER |  (buf_id <<\nIORING_CQE_BUFFER_SHIFT)\" mechanism\n\nDoes this design match what you had in mind / prefer?\n\nI think the above works for Christoph's use case too (as his and my\nuse case are the same) but if not, please let me know.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "message_id": "CAJnrk1Y5iTOhj4_RbnR7RJPkr7fFcCdh1gY=3Hm72M91D-SnyQ@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov questioned whether kernel-managed buffer rings (km rings) should be exposed as io_uring uapi, specifically asking if a server or user space program can issue I/O requests that consume buffers/entries from the km ring without fuse kernel code involved. He requested clarification on this point to inform the decision of exposing km rings in the uapi.\n\nReviewer Pavel Begunkov suggested reusing registered buffers instead of introducing a new mechanism for kernel-managed buffer rings, citing efficiency and similarity to zero-copy internally registered buffers as benefits.\n\nReviewer noted that kernel-managed buffer rings would hold page references or require pinning of regions, suggesting a different approach using registered buffers\n\nreviewer suggested adding a liburing helper for the fuse server to avoid dealing with mmap'ing\n\nreviewer expressed conditional approval, requesting confirmation that the patch allows for desired fast path optimizations",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "clarification needed",
                "alternative solution",
                "suggestion",
                "alternative",
                "conditional approval",
                "request for confirmation"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You mention OP_READ_FIXED and below agreed not exposing km rings\nan io_uring uapi, which makes me believe we're still talking about\ndifferent things.\n\nCorrect me if I'm wrong. Currently, only fuse cmds use the buffer\nring itself, I'm not talking about buffer, i.e. fuse cmds consume\nentries from the ring (!!! that's the part I'm interested in), then\nprocess them and tell the server \"this offset in the region has user\ndata to process or should be populated with data\".\n\nNaturally, the server should be able to use the buffers to issue\nsome I/O and process it in other ways, whether it's a normal\nOP_READ to which you pass the user space address (you can since\nit's mmap()'ed by the server) or something else is important but\na separate question than the one I'm trying to understand.\n\nSo I'm asking whether you expect that a server or other user space\nprogram should be able to issue a READ_OP_RECV, READ_OP_READ or any\nother similar request, which would consume buffers/entries from the\nkm ring without any fuse kernel code involved? Do you have some\nuse case for that in mind?\n\nUnderstanding that is the key in deciding whether km rings should\nbe exposed as io_uring uapi or not, regardless of where buffers\nto populate the ring come from.\n\n...\n\n---\n\nSo you already can do all that using the mmap()'ed region user\npointer, and you just want it to be more efficient, right?\nFor that let's just reuse registered buffers, we don't need a\nnew mechanism that needs to be propagated to all request types.\nAnd registered buffer are already optimised for I/O in a bunch\nof ways. And as a bonus, it'll be similar to the zero-copy\ninternally registered buffers if you still plan to add them.\n\nThe simplest way to do that is to create a registered buffer out\nof the mmap'ed region pointer. Pseudo code:\n\n// mmap'ed if it's kernel allocated.\n{region_ptr, region_size} = create_region();\n\nstruct iovec iov;\niov.iov_base = region_ptr;\niov.iov_len = region_size;\nio_uring_register_buffers(ring, &iov, 1);\n\n// later instead of this:\nptr = region_ptr + off;\nio_uring_prep_read(sqe, fd, ptr, ...);\n\n// you use registered buffers as usual:\nio_uring_prep_read_fixed(sqe, fd, off, regbuf_idx, ...);\n\n\nIIRC the registration would fail because it doesn't allow file\nbacked pages, but it should be fine if we know it's io_uring\nregion memory, so that would need to be patched.\n\nThere might be a bunch of other ways you can do that like\ncreate a kernel allocated registered buffer like what Cristoph\nwants, and then register it as a region. Or allow creating\nregistered buffers out of a region. etc.\n\nI wanted to unify registered buffers and regions internally\nat some point, but then drifted away from active io_uring core\ninfrastructure development, so I guess that could've been useful.\n\n---\n\nLet's talk about it when it's needed or something changes, but if\nyou do registered buffers instead as per above, they'll be holding\npage references and or have to pin the region in some other way.\n\n---\n\nFWIW, we should just add a liburing helper, so that fuse server\ndoesn't need to deal with mmap'ing.\n\n---\n\nThat's sounds clean to me _if_ it allows you to achieve all\n(fast path) optimisations you want to have. I hope it does?\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "message_id": "11869d3d-1c40-4d49-a6c2-607fd621bf91@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a question from Pavel Begunkov about whether the concept of kernel-managed buffer rings (kmbuf rings) is fuse-specific and whether it would be useful to optimize for READ_OP_RECV/READ_OP_READ operations directly on the ring. The author agrees that this optimization would be beneficial in certain scenarios, such as network-backed servers with high concurrency and unpredictable latencies, but notes that kmbuf rings are not exclusively fuse-specific and could be useful for other subsystems/users.\n\nAuthor expressed concern about added complexity and potential confusion in the design, specifically questioning the need for kernel-managed buffer rings when memory regions could be used instead.\n\nAuthor addressed Pavel's concern that the caller cannot guarantee the memory region will be registered as a fixed buffer, explaining that this would introduce extra overhead for every I/O operation and suggesting pinning to a registered memory region as an alternative.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a question",
                "provided additional context",
                "questioning",
                "expressed concern",
                "acknowledged a technical issue",
                "provided an explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for clarifying your question. Yes, this would be a useful\noptimization in the future for fuse servers with certain workload\ncharacteristics (eg network-backed servers with high concurrency and\nunpredictable latencies). I don't think the concept of kmbufrings is\nexclusively fuse-specific though (for example, Christoph's use case\nbeing a recent instance); I think other subsystems/users that'll use\nkmbuf rings would also generically find it useful to have the option\nof READ_OP_RECV/READ_OP_READ operating directly on the ring.\n\n---\n\nI feel like this design makes the interface more convoluted and now\nmuddies different concepts together by adding new complexity /\nrelationships between them whereas they were otherwise cleanly\nisolated. Maybe I'm just not seeing/understanding the overarching\nvision for why conceptually it makes sense for them to be tied\ntogether besides as a mechanism to tell io-uring requests where to\ncopy from by reusing what exists for fixed buffer ids. There's more\ncomplexity now on the kernel side (eg having to detect if the buffer\npassed in is kernel-allocated to know whether to pin the pages /\ncharge it against the user's RLIMIT_MEMLOCK limit) but I'm not\nunderstanding what we gain from it. I got the sense from your previous\ncomments that memory regions are the de facto way to go and should be\ndecoupled from other structures, so if that's the case, why doesn't it\nmake sense for io-uring to add native support for using memory regions\nfor io-uring requests? I feel like from the userspace side it makes\nthings more confusing with this extra layer of indirection that now\nhas to go through a fixed buffer.\n\n---\n\nI don't think we can guarantee that the caller will register the\nmemory region as a fixed buffer (eg if it doesn't need/want to use the\nbuffer for normal io-uring requests). On the kernel side, the internal\nbuffer entry uses the kaddr of the registered memory region buffer for\nany memcpys. If it's not guaranteed that registered memory regions\npersist for the lifetime of the ring, there'll have to be extra\noverhead for every I/O (eg grab the io-uring lock, checking if the mem\nregion is still registered, grab a refcount to that mem region, unlock\nthe ring, do the memcpy to the kaddr, then grab the io-uring lock\nagain, decrement the refcount, and unlock). Or I guess we could add\npinning to a registered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "message_id": "CAJnrk1Zr=9RMGpNXpe6=fSDkG2uVijB9qa1vENHpQozB3iPQtg@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed confusion about the connection between kernel-managed buffer rings and a previous request from Christoph to check for mul overflow, use GFP_USER, and change the return type of PTR.\n\nReviewer Pavel Begunkov questioned the exposure of an internal kernel fuse API as an io_uring uapi, suggesting that it may have been discussed previously but was not clear from the patchset\n\nReviewer Pavel Begunkov suggested reusing an existing uapi for buffer management instead of introducing a new one, citing the importance of keeping the I/O path sane and avoiding adding a fourth way to pass buffers.\n\nPavel Begunkov noted that the current design uses regions instead of registered buffers, which he believes would be a better abstraction for copying client's data into user space; he was following the main I/O path and trying to make the setup path more flexible and reusable\n\nreviewer expressed skepticism about the value of introducing a new interface for passing buffers, citing existing alternatives and high bar for adoption\n\nReviewer Pavel Begunkov noted that the io_uring_cmd_done function should not set the selected buffer index because this is a user responsibility, as they can either use OP_READ/etc. with user addresses from mmap()ed regions or register and use OP_READ_FIXED.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "confusion",
                "lack of understanding",
                "requested clarification",
                "expressed uncertainty",
                "requested changes",
                "neutral comment",
                "request for clarification",
                "skepticism",
                "high bar"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sorry, I don't see relevance b/w km rings and what Christoph wants.\nI explained why in some sub-thread, but maybe someone can tell\nwhat I'm missing.\n\n---\n\nYep, it could be, potentially, it's just the patchset doesn't plumb\nit to other requests and uses it within fuse. It's just cases like\nthat always make me wonder, here it was why what is basically an\ninternal kernel fuse API is exposed as an io_uring uapi. Maybe there\nwas a discussion about it I missed?\n\n---\n\nThat would avoid doing a large revamp of uapi and plumbing it\nto each every request type when there is already a uapi that does\nwhat you want, does it well and have lots of things figured out.\nKeeping the I/O path sane is important, io_uring already has 3\ndifferent ways of passing buffers, let's not add a 4th one\nunless it achieves something meaningful.\n\n---\n\nSorry, maybe I wasn't clear. With what I see you're trying to do,\ni.e. copying client's data into user space (server), I think\nregistered buffers would be a better abstraction. However, I just\nwent with your design on top of regions, since it's not the first\niteration of the series and I wasn't following previous ones, and\nIIRC you was already using registered buffers in previous revisions\nbut moved from that for some reason. IOW, I was taking you main I/O\npath and was trying to make the setup path a bit more flexible and\nreusable.\n\n---\n\nThere is a high bar for adding a new interface for passing buffers\nthat needs to be propagated to a good number of request handlers,\nand there is already one that gives you all you need to write\nefficient user space.\n\n---\n\nIt's up to the user (i.e. fuse server) to either use OP_READ/etc. using\nuser addresses that you have in your design from mmap()ing regions, or\nregistering it and using OP_READ_FIXED.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-23",
              "message_id": "94ae832e-209a-4427-925c-d4e2f8217f5a@gmail.com",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v1 1/1] iomap: don't mark folio uptodate if read IO has bytes pending",
          "message_id": "CAJnrk1Zk1hHCoC4xaY_KT0m_04CQ=pO6j3e1tGrdj7LTf5BHsA@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1Zk1hHCoC4xaY_KT0m_04CQ=pO6j3e1tGrdj7LTf5BHsA@mail.gmail.com/",
          "date": "2026-02-23T23:53:28Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the folio uptodate state being cleared when iomap_set_range_uptodate() is called after an async IO helper has successfully finished reading a partially read folio, and then post-EOF zeroing or inline data reads mark the folio as uptodate. The author explained that this is because folio_end_read(), which uses XOR semantics to set the uptodate bit, will clear it if it's already been set by iomap_set_range_uptodate(). To fix this, the author proposed not marking the folio as uptodate if the read IO has bytes pending.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If a folio has ifs metadata attached to it and the folio is partially\nread in through an async IO helper with the rest of it then being read\nin through post-EOF zeroing or as inline data, and the helper\nsuccessfully finishes the read first, then post-EOF zeroing / reading\ninline will mark the folio as uptodate in iomap_set_range_uptodate().\n\nThis is a problem because when the read completion path later calls\niomap_read_end(), it will call folio_end_read(), which sets the uptodate\nbit using XOR semantics. Calling folio_end_read() on a folio that was\nalready marked uptodate clears the uptodate bit.\n\nFix this by not marking the folio as uptodate if the read IO has bytes\npending. The folio uptodate state will be set in the read completion\npath through iomap_end_read() -> folio_end_read().\n\nReported-by: Wei Gao <wegao@suse.com>\nSuggested-by: Sasha Levin <sashal@kernel.org>\nTested-by: Wei Gao <wegao@suse.com>\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nFixes: b2f35ac4146d (\"iomap: add caller-provided callbacks for read and readahead\")\n---\n fs/iomap/buffered-io.c | 15 ++++++++++++---\n 1 file changed, 12 insertions(+), 3 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 58887513b894..4fc5ce963feb 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -80,18 +80,27 @@ static void iomap_set_range_uptodate(struct folio *folio, size_t off,\n {\n \tstruct iomap_folio_state *ifs = folio->private;\n \tunsigned long flags;\n-\tbool uptodate = true;\n+\tbool mark_uptodate = true;\n \n \tif (folio_test_uptodate(folio))\n \t\treturn;\n \n \tif (ifs) {\n \t\tspin_lock_irqsave(&ifs->state_lock, flags);\n-\t\tuptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n+\t\t/*\n+\t\t * If a read with bytes pending is in progress, we must not call\n+\t\t * folio_mark_uptodate(). The read completion path\n+\t\t * (iomap_read_end()) will call folio_end_read(), which uses XOR\n+\t\t * semantics to set the uptodate bit. If we set it here, the XOR\n+\t\t * in folio_end_read() will clear it, leaving the folio not\n+\t\t * uptodate.\n+\t\t */\n+\t\tmark_uptodate = ifs_set_range_uptodate(folio, ifs, off, len) &&\n+\t\t\t\t!ifs->read_bytes_pending;\n \t\tspin_unlock_irqrestore(&ifs->state_lock, flags);\n \t}\n \n-\tif (uptodate)\n+\tif (mark_uptodate)\n \t\tfolio_mark_uptodate(folio);\n }\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-18",
              "message_id": "20260219003911.344478-2-joannelkoong@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong suggested adding a link to a relevant discussion on linux-fsdevel and CC'ing stable@vger.kernel.org for the v6.19 release\n\nReviewer Darrick Wong noted that the patch's impact should be tested with an fstest, and provided a Reviewed-by tag.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I would add:\n\nLink: https://lore.kernel.org/linux-fsdevel/aYbmy8JdgXwsGaPP@autotest-wegao.qe.prg2.suse.org/\nCc: <stable@vger.kernel.org> # v6.19\n\nsince the recent discussion around this was sort of buried in a\ndifferent thread, and the original patch is now in a released kernel.\n\n---\n\nYeah, that makes sense.  How difficult is this to write up as an fstest?\n\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n\n--D",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "message_id": "20260219024534.GN6467@frogsfrogsfrogs",
              "analysis_source": "llm"
            },
            {
              "author": "Matthew Wilcox",
              "summary": "reviewer expressed frustration that the iomap code is overly complicated and difficult to understand, implying it needs a fundamental rework",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This isn't \"the xor thing has come back to bite us\".  This is \"the iomap\ncode is now too complicated and I cannot figure out how to explain to\nJoanne that there's really a simple way to do this\".\n\nI'm going to have to set aside my current projects and redo the iomap\nreadahead/read_folio code myself, aren't I?",
              "reply_to": "Darrick Wong",
              "message_date": "2026-02-19",
              "message_id": "aZaQO0jQaZXakwOA@casper.infradead.org",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong expressed confusion about an alternative approach mentioned in the patch description, indicating that he was not aware of a previous discussion and is seeking clarification.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "lack of context"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Well you could try explaining to me what that simpler way is?\n\n/me gets the sense he's missing a discussion somewhere...\n\n--D",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-18",
              "message_id": "20260219061101.GO6467@frogsfrogsfrogs",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to a request for further context by providing the link to the prior discussion, indicating no specific action or resolution related to the original patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This is the link to the prior discussion\nhttps://lore.kernel.org/linux-fsdevel/20251223223018.3295372-1-sashal@kernel.org/T/#mbd61eaa5fd1e8922caa479720232628e39b8c9da\n\nThanks,\nJoanne",
              "reply_to": "Darrick Wong",
              "message_date": "2026-02-20",
              "message_id": "CAJnrk1aJJqafDkxMypUym6iFQ-HkaSxneOe6Sc746AwrmrDK4Q@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong noted that the read_bytes_pending field in iomap has inconsistent behavior across different IO paths, and suggested consolidating the read code to simplify it. He also mentioned that Joanne's patch is a temporary fix for a bug in Linus' tree.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "<willy and I had a chat; this is a clumsy non-AI summary of it>\n\nI started looking at folio read state management in iomap, and made a\nfew observations that (I hope) match what willy's grumpy about.\n\nThere are three ways that iomap can be reading into the pagecache:\na) async ->readahead,\nb) synchronous ->read_folio (page faults), and\nc) synchronous ->read_folio_range (pagecache write).\n\n(Note that (b) can call a different ->read_folio_range than (c), though\nall implementations seem to have the same function)\n\nAll three of these IO paths share the behavior that they try to fill out\nthe folio's contents and set the corresponding folio/ifs uptodate bits\nif that succeeds.  Folio contents can come from anywhere, whether it's:\n\ni) zeroing memory,\nii) copying from an inlinedata buffer, or\niii) asynchronously fetching the contents from somewhere\n\nIn the case of (c) above, if the read fails then we fail the write, and\nif the read succeeds then we start copying to the pagecache.\n\nHowever, (a) and (b) have this additional read_bytes_pending field in\nthe ifs that implements some extra tracking.  AFAICT the purpose of this\nfield is to ensure that we don't call folio_end_read prematurely if\nthere's an async read in progress.  This can happen if iomap_iter\nreturns a negative errno on a partially processed folio, I think?\n\nread_bytes_pending is initialized to the folio_size() at the start of a\nread and subtracted from when parts of the folio are supplied, whether\nthat's synchronous zeroing or asynchronous read ioend completion.  When\nthe field reaches zero, we can then call folio_end_read().\n\nBut then there are twists, like the fact that we only call\niomap_read_init() to set read_bytes_pending if we decide to do an\nasynchronous read.  Or that iomap_read_end and iomap_finish_folio_read\nhave awfully similar code.  I think in the case of (i) and (ii) we also\ndon't touch read_pending_bytes at all, and merely set the uptodate bits?\n\nThis is confusing to me.  It would be more straightforward (I think) if\nwe just did it for all cases instead of adding more conditionals.  IOWs,\nhow hard would it be to consolidate the read code so that there's one\nfunction that iomap calls when it has filled out part of a folio.  Is\nthat possible, even though we shouldn't be calling folio_end_read during\na pagecache write?\n\nAt the end of the day, however, there's a bug in Linus' tree and we need\nto fix it, so Joanne's patch is a sufficient bandaid until we can go\nclean this up.\n\n--D",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-20",
              "message_id": "20260220234521.GA11069@frogsfrogsfrogs",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author acknowledged that the read IO has bytes pending issue is not unique to synchronous reads, explaining that the code for both synchronous and asynchronous reads (a) and b) are identical.\n\nAuthor clarified that synchronous zeroing does not update read_bytes_pending, explaining that only asynchronous read completions affect this value.\n\nAuthor Joanne Koong addressed Darrick Wong's feedback about consolidating synchronous buffered write logic with async read logic, arguing that it would add unnecessary overhead and complicate handling. She agreed that there are edge cases to consider for async reads but expressed concerns about introducing additional complexity through zeroing and inline read paths.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a nuance",
                "provided clarification",
                "clarification",
                "explanation",
                "acknowledged a concern",
                "pushed back on an approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "b) is async as well. The code for b) and a) are exactly the same (the\nlogic in iomap_read_folio_iter())\n\n---\n\nSynchronous zeroing does not update read_bytes_pending, only async\nread completions do.\n\n---\n\nimo, I don't think the synchronous ->read_folio_range() for buffered\nwrites should be consolidated with the async read logic. If we have\nthe synchronous write path setting read_bytes_pending, that adds extra\noverhead with having to acquire/release the spinlock for every range\nread in. It also makes the handling more complicated (eg now having to\ndifferentiate whether the folio was read in for a read vs. a write).\nSynchronous ->read_folio_range() for buffered writes is extremely\nsimple and self-contained right now and I think it should be kept that\nway.\n\nFor async reads, I agree that there are a bunch of different edge\ncases that arise from i) ii) and iii), and from the fact that a folio\ncould be composed of a mixture of i) ii) and iii).\n\nThe motivation for adding read_bytes_pending was so we could know\nwhich async read finishes last. eg this example scenario: read a 64k\nfolio where the first and last page are not uptodate but everything in\nbetween is\n* ->read_folio_range() for 0 to 4k\n* ->read_folio_range() for 60k to 64k\nThese two async read calls may be two different I/O requests that\ncomplete at different times but only the last finisher should call\nfolio_end_read().\n\nI don't think having the zeroing and inline read paths also\nmanipulating read_bytes_pending helps here. This was discussed a bit\nin [1] but I think it runs into other edge cases / race conditions [2]\nthat would need to be accounted for and makes some paths more\nsuboptimal (eg unnecessary ifs allocations and spinlock acquires). But\nmaybe I'm missing something here and there is a better approach for\ndoing this?\n\nThanks,\nJoanne\n\n[1] https://lore.kernel.org/linux-fsdevel/CAJnrk1YcuhKwbZLo-11=umcTzH_OJ+bdwZq5=XjeJo8gb9e5ig@mail.gmail.com/T/#md09648082a96122ec1e541993872e0c43da5105f\n[2] https://lore.kernel.org/linux-fsdevel/CAJnrk1YcuhKwbZLo-11=umcTzH_OJ+bdwZq5=XjeJo8gb9e5ig@mail.gmail.com/T/#mdc49b649378798fa9e850c9c6914c8c6af5e2895",
              "reply_to": "Darrick Wong",
              "message_date": "2026-02-23",
              "message_id": "CAJnrk1Zk1hHCoC4xaY_KT0m_04CQ=pO6j3e1tGrdj7LTf5BHsA@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that merging the code would be useful, but he hasn't found a good way to do so yet, and expressed concern about the range logic in the current implementation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes.  I've been thinking about that on and off, but unfortunately so far\nI've not come up with a good idea how to merge the code.  Doing so would\nbe very useful for many reasons.\n\nThe problem with that isn't really async vs sync; ->read_folio clearly\nshows you you turn underlying asynchronous logic into a synchronous call.\nIt's really about the range logic, where the writer preparation might\nwant to only read the head and the tail segments of a folio.\n\nBut if we can merge that into the main implementation and have a single\ncore implementation we'd be much better off.\n\nAnyone looking for a \"little\" project? :)",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-24",
              "message_id": "aZ3A39jztKdUmWoT@infradead.org",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    }
  ]
}