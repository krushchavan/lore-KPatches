{
  "date": "2026-02-25",
  "report_file": "2026-02-25_ollama_llama3.1-8b.html",
  "status": "in_progress",
  "last_updated": "2026-02-26 07:14 UTC",
  "llm_backends": [
    [
      "ollama",
      "llama3.1:8b"
    ]
  ],
  "generation_time_seconds": 0.0,
  "developer_reports": [
    {
      "name": "Alexandre Ghiti",
      "primary_email": "alexghiti@rivosinc.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Boris Burkov",
      "primary_email": "boris@bur.io",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 1/1] btrfs: set BTRFS_ROOT_ORPHAN_CLEANUP during subvol create",
          "message_id": "14fc2404e55d99e9d3a4f95e3e825678dc2422a0.1771971643.git.boris@bur.io",
          "url": "https://lore.kernel.org/all/14fc2404e55d99e9d3a4f95e3e825678dc2422a0.1771971643.git.boris@bur.io/",
          "date": "2026-02-24T22:25:39Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-24",
          "patch_summary": "This patch addresses a bug in Btrfs where subvolumes with broken dentries cause issues when deleting or creating new files/subvolumes. The problem arises from the failure of btrfs_orphan_cleanup() to set BTRFS_ROOT_ORPHAN_CLEANUP, leading to negative dentry creation and subsequent errors. The fix involves setting this flag during subvolume creation.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "Identified a potential issue with concurrent orphan cleanup and deletion of inodes. Suggested adding a lock to protect against this scenario.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concurrent deletion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v2 1/1] btrfs: set BTRFS_ROOT_ORPHAN_CLEANUP during subvol create",
          "message_id": "20260225170921.GA682210@zen.localdomain",
          "url": "https://lore.kernel.org/all/20260225170921.GA682210@zen.localdomain/",
          "date": "2026-02-25T17:08:53Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch sets the BTRFS_ROOT_ORPHAN_CLEANUP flag during subvolume creation in btrfs, addressing an issue where orphaned dentries are not properly cleaned up. The problem arises from a race condition between __dentry_kill and iput functions, leading to potential crashes or data corruption. By setting this flag, the patch ensures that orphaned dentries are cleaned up promptly, preventing such issues.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Reviewer noted that the decrement at fs/dcache.c:690 in __dentry_kill() is inside a conditional, which may cause issues with race diagrams and suggested adding more clarity to the callstack context.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Ok, you can add:\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\nThanks.",
              "reply_to": "Boris Burkov",
              "message_date": "2026-02-25",
              "message_id": "CAL3q7H6bVZquyvod=_YjNw1vRBSCQscWSrb5mVEZ1YhLBS8e9Q@mail.gmail.com",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH] btrfs: Fix a bug in try_release_subpage_extent_buffer()",
          "message_id": "20260225202642.GA3307145@zen.localdomain",
          "url": "https://lore.kernel.org/all/20260225202642.GA3307145@zen.localdomain/",
          "date": "2026-02-25T20:25:49Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Boris Burkov",
              "summary": "The reviewer found no issues with the patch and gave it a positive review.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Dmitry Ilvokhin",
      "primary_email": "d@ilvokhin.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 4/4] mm: add tracepoints for zone lock",
          "message_id": "bde161acf827852ef19de51e91caf5c9f7df81bd.1772030186.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/bde161acf827852ef19de51e91caf5c9f7df81bd.1772030186.git.d@ilvokhin.com/",
          "date": "2026-02-25T14:44:17Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, following the mmap_lock tracepoint pattern. The implementation includes lightweight inline helpers that check whether tracing is enabled and call out-of-line helpers when active. When CONFIG_TRACING is disabled, these helpers compile to empty inline stubs.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No comments from the author in this thread.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 3/4] mm: convert compaction to zone lock wrappers",
          "message_id": "9710c3448c6c984164c93d7c6c0283e06ff987bf.1772030186.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/9710c3448c6c984164c93d7c6c0283e06ff987bf.1772030186.git.d@ilvokhin.com/",
          "date": "2026-02-25T14:44:16Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, following the mmap_lock tracepoint pattern. The implementation includes lightweight inline helpers that check whether tracing is enabled and call out-of-line helpers when active. When CONFIG_TRACING is disabled, these helpers compile to empty inline stubs.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No comments from the author in this thread.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 2/4] mm: convert zone lock users to wrappers",
          "message_id": "e5324d64361f86d930d940a5b49235f7996efe53.1772030186.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/e5324d64361f86d930d940a5b49235f7996efe53.1772030186.git.d@ilvokhin.com/",
          "date": "2026-02-25T14:44:16Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, following the mmap_lock tracepoint pattern. The implementation includes lightweight inline helpers that check whether tracing is enabled and call out-of-line helpers when active. When CONFIG_TRACING is disabled, these helpers compile to empty inline stubs.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No comments from the author in this thread.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 1/4] mm: introduce zone lock wrappers",
          "message_id": "5bcc39cd3a227944d0fbe75ff86cdac92b38d4ca.1772030186.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/5bcc39cd3a227944d0fbe75ff86cdac92b38d4ca.1772030186.git.d@ilvokhin.com/",
          "date": "2026-02-25T14:44:15Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, following the mmap_lock tracepoint pattern. The implementation includes lightweight inline helpers that check whether tracing is enabled and call out-of-line helpers when active. When CONFIG_TRACING is disabled, these helpers compile to empty inline stubs.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No comments from the author in this thread.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 0/4] mm: zone lock tracepoint instrumentation",
          "message_id": "cover.1772030186.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/cover.1772030186.git.d@ilvokhin.com/",
          "date": "2026-02-25T14:44:15Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, following the mmap_lock tracepoint pattern. The implementation includes lightweight inline helpers that check whether tracing is enabled and call out-of-line helpers when active. When CONFIG_TRACING is disabled, these helpers compile to empty inline stubs.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No comments from the author in this thread.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 4/4] mm: add tracepoints for zone lock",
          "message_id": "1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, allowing for better visibility into the locking behavior of the system. The implementation follows a lightweight pattern where an inline helper checks whether tracing is enabled and calls an out-of-line helper when necessary. When CONFIG_TRACING is disabled, the helpers compile to empty stubs, ensuring the fast path remains unaffected.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No substantive comments or reviews have been made yet, but the patch series appears to be a straightforward addition of tracepoints for zone lock acquire and release operations. The implementation follows a well-established pattern and is likely to be accepted once reviewed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "WAITING_FOR_REVIEW"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 3/4] mm: convert compaction to zone lock wrappers",
          "message_id": "3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, allowing for better visibility into the locking behavior of the system. The implementation follows a lightweight pattern where an inline helper checks whether tracing is enabled and calls an out-of-line helper when necessary. When CONFIG_TRACING is disabled, the helpers compile to empty stubs, ensuring the fast path remains unaffected.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No substantive comments or reviews have been made yet, but the patch series appears to be a straightforward addition of tracepoints for zone lock acquire and release operations. The implementation follows a well-established pattern and is likely to be accepted once reviewed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "WAITING_FOR_REVIEW"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:10:05PM -0600, Cheatham, Benjamin wrote:\n> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> > Compaction uses compact_lock_irqsave(), which currently operates\n> > on a raw spinlock_t pointer so that it can be used for both\n> > zone->lock and lru_lock. Since zone lock operations are now wrapped,\n> > compact_lock_irqsave() can no longer operate directly on a spinlock_t\n> > when the lock belongs to a zone.\n> > \n> > Introduce struct compact_lock to abstract the underlying lock type. The\n> > structure carries a lock type enum and a union holding either a zone\n> > pointer or a raw spinlock_t pointer, and dispatches to the appropriate\n> > lock/unlock helper.\n> > \n> > No functional change intended.\n> > \n> > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > ---\n> >  mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n> >  1 file changed, 89 insertions(+), 19 deletions(-)\n> > \n> > diff --git a/mm/compaction.c b/mm/compaction.c\n> > index 1e8f8eca318c..1b000d2b95b2 100644\n> > --- a/mm/compaction.c\n> > +++ b/mm/compaction.c\n> > @@ -24,6 +24,7 @@\n> >  #include <linux/page_owner.h>\n> >  #include <linux/psi.h>\n> >  #include <linux/cpuset.h>\n> > +#include <linux/zone_lock.h>\n> >  #include \"internal.h\"\n> >  \n> >  #ifdef CONFIG_COMPACTION\n> > @@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n> >  }\n> >  #endif /* CONFIG_COMPACTION */\n> >  \n> > +enum compact_lock_type {\n> > +\tCOMPACT_LOCK_ZONE,\n> > +\tCOMPACT_LOCK_RAW_SPINLOCK,\n> > +};\n> > +\n> > +struct compact_lock {\n> > +\tenum compact_lock_type type;\n> > +\tunion {\n> > +\t\tstruct zone *zone;\n> > +\t\tspinlock_t *lock; /* Reference to lru lock */\n> > +\t};\n> > +};\n> > +\n> > +static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n> > +\t\t\t\t\t    unsigned long *flags)\n> > +{\n> > +\treturn zone_trylock_irqsave(zone, *flags);\n> > +}\n> > +\n> > +static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n> > +\t\t\t\t\t   unsigned long *flags)\n> > +{\n> > +\treturn spin_trylock_irqsave(lock, *flags);\n> > +}\n> > +\n> > +static bool compact_do_trylock_irqsave(struct compact_lock lock,\n> > +\t\t\t\t       unsigned long *flags)\n> > +{\n> > +\tif (lock.type == COMPACT_LOCK_ZONE)\n> > +\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n> > +\n> > +\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n> > +}\n> \n> Nit: You could remove the helpers above and just do the calls directly in this function, though\n> it would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\n> since they have the __acquires() annotations.\n\nYes, I agree, there is no much value in this wrappers, will remove them,\nthanks!\n\n> > +\n> > +static void compact_do_zone_lock_irqsave(struct zone *zone,\n> > +\t\t\t\t\t unsigned long *flags)\n> > +__acquires(zone->lock)\n> > +{\n> > +\tzone_lock_irqsave(zone, *flags);\n> > +}\n> > +\n> > +static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n> > +\t\t\t\t\tunsigned long *flags)\n> > +__acquires(lock)\n> > +{\n> > +\tspin_lock_irqsave(lock, *flags);\n> > +}\n> > +\n> > +static void compact_do_lock_irqsave(struct compact_lock lock,\n> > +\t\t\t\t    unsigned long *flags)\n> > +{\n> > +\tif (lock.type == COMPACT_LOCK_ZONE) {\n> > +\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n> > +\t\treturn;\n> > +\t}\n> > +\n> > +\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n> \n> You don't need the return statement here (and you shouldn't be returning a value at all).\n\nYes, agree, will fix in v2.\n\n> \n> It may be cleaner to just do an if-else statement here instead.\n> \n> > +}\n> > +\n> >  /*\n> >   * Compaction requires the taking of some coarse locks that are potentially\n> >   * very heavily contended. For async compaction, trylock and record if the\n> > @@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n> >   *\n> >   * Always returns true which makes it easier to track lock state in callers.\n> >   */\n> > -static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n> > -\t\t\t\t\t\tstruct compact_control *cc)\n> > -\t__acquires(lock)\n> > +static bool compact_lock_irqsave(struct compact_lock lock,\n> > +\t\t\t\t unsigned long *flags,\n> > +\t\t\t\t struct compact_control *cc)\n> >  {\n> >  \t/* Track if the lock is contended in async mode */\n> >  \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n> > -\t\tif (spin_trylock_irqsave(lock, *flags))\n> > +\t\tif (compact_do_trylock_irqsave(lock, flags))\n> >  \t\t\treturn true;\n> >  \n> >  \t\tcc->contended = true;\n> >  \t}\n> >  \n> > -\tspin_lock_irqsave(lock, *flags);\n> > +\tcompact_do_lock_irqsave(lock, flags);\n> >  \treturn true;\n> >  }\n> >  \n> > @@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n> >   * Returns true if compaction should abort due to fatal signal pending.\n> >   * Returns false when compaction can continue.\n> >   */\n> > -static bool compact_unlock_should_abort(spinlock_t *lock,\n> > -\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n> > +static bool compact_unlock_should_abort(struct zone *zone,\n> > +\t\t\t\t\tunsigned long flags,\n> > +\t\t\t\t\tbool *locked,\n> > +\t\t\t\t\tstruct compact_control *cc)\n> >  {\n> >  \tif (*locked) {\n> > -\t\tspin_unlock_irqrestore(lock, flags);\n> > +\t\tzone_unlock_irqrestore(zone, flags);\n> \n> I would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\n> didn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\n> and fits better in the last.\n\nThanks for the suggestion. Totally makes sense to me, will do in v2 as well.\n\n> \n> >  \t\t*locked = false;\n> >  \t}\n> >  \n> > @@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n> >  \t\t * contention, to give chance to IRQs. Abort if fatal signal\n> >  \t\t * pending.\n> >  \t\t */\n> > -\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n> > -\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n> > -\t\t\t\t\t\t\t\t&locked, cc))\n> > +\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n> > +\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n> >  \t\t\tbreak;\n> >  \n> >  \t\tnr_scanned++;\n> > @@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n> >  \n> >  \t\t/* If we already hold the lock, we can skip some rechecking. */\n> >  \t\tif (!locked) {\n> > -\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n> > -\t\t\t\t\t\t\t\t&flags, cc);\n> > +\t\t\tstruct compact_lock zol = {\n> > +\t\t\t\t.type = COMPACT_LOCK_ZONE,\n> > +\t\t\t\t.zone = cc->zone,\n> > +\t\t\t};\n> > +\n> > +\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n> >  \n> >  \t\t\t/* Recheck this is a buddy page under lock */\n> >  \t\t\tif (!PageBuddy(page))\n> > @@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n> >  \t}\n> >  \n> >  \tif (locked)\n> > -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> > +\t\tzone_unlock_irqrestore(cc->zone, flags);\n> >  \n> >  \t/*\n> >  \t * Be careful to not go outside of the pageblock.\n> > @@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n> >  \n> >  \t\t/* If we already hold the lock, we can skip some rechecking */\n> >  \t\tif (lruvec != locked) {\n> > +\t\t\tstruct compact_lock zol = {\n> > +\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n> > +\t\t\t\t.lock = &lruvec->lru_lock,\n> > +\t\t\t};\n> > +\n> >  \t\t\tif (locked)\n> >  \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n> >  \n> > -\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n> > +\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n> >  \t\t\tlocked = lruvec;\n> >  \n> >  \t\t\tlruvec_memcg_debug(lruvec, folio);\n> > @@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n> >  \t\tif (!area->nr_free)\n> >  \t\t\tcontinue;\n> >  \n> > -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> > +\t\tzone_lock_irqsave(cc->zone, flags);\n> >  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n> >  \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n> >  \t\t\tunsigned long pfn;\n> > @@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n> >  \t\t\t}\n> >  \t\t}\n> >  \n> > -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> > +\t\tzone_unlock_irqrestore(cc->zone, flags);\n> >  \n> >  \t\t/* Skip fast search if enough freepages isolated */\n> >  \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n> > @@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n> >  \t\tif (!area->nr_free)\n> >  \t\t\tcontinue;\n> >  \n> > -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> > +\t\tzone_lock_irqsave(cc->zone, flags);\n> >  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n> >  \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n> >  \t\t\tunsigned long free_pfn;\n> > @@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n> >  \t\t\t\tbreak;\n> >  \t\t\t}\n> >  \t\t}\n> > -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> > +\t\tzone_unlock_irqrestore(cc->zone, flags);\n> >  \t}\n> >  \n> >  \tcc->total_migrate_scanned += nr_scanned;\n> \n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/4] mm: zone lock tracepoint instrumentation",
          "message_id": "cover.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/cover.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, allowing for better visibility into the locking behavior of the system. The implementation follows a lightweight pattern where an inline helper checks whether tracing is enabled and calls an out-of-line helper when necessary. When CONFIG_TRACING is disabled, the helpers compile to empty stubs, ensuring the fast path remains unaffected.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No substantive comments or reviews have been made yet, but the patch series appears to be a straightforward addition of tracepoints for zone lock acquire and release operations. The implementation follows a well-established pattern and is likely to be accepted once reviewed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "WAITING_FOR_REVIEW"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:\n> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> > Zone lock contention can significantly impact allocation and\n> > reclaim latency, as it is a central synchronization point in\n> > the page allocator and reclaim paths. Improved visibility into\n> > its behavior is therefore important for diagnosing performance\n> > issues in memory-intensive workloads.\n> > \n> > On some production workloads at Meta, we have observed noticeable\n> > zone lock contention. Deeper analysis of lock holders and waiters\n> > is currently difficult with existing instrumentation.\n> > \n> > While generic lock contention_begin/contention_end tracepoints\n> > cover the slow path, they do not provide sufficient visibility\n> > into lock hold times. In particular, the lack of a release-side\n> > event makes it difficult to identify long lock holders and\n> > correlate them with waiters. As a result, distinguishing between\n> > short bursts of contention and pathological long hold times\n> > requires additional instrumentation.\n> > \n> > This patch series adds dedicated tracepoint instrumentation to\n> > zone lock, following the existing mmap_lock tracing model.\n> > \n> > The goal is to enable detailed holder/waiter analysis and lock\n> > hold time measurements without affecting the fast path when\n> > tracing is disabled.\n> > \n> > The series is structured as follows:\n> > \n> >   1. Introduce zone lock wrappers.\n> >   2. Mechanically convert zone lock users to the wrappers.\n> >   3. Convert compaction to use the wrappers (requires minor\n> >      restructuring of compact_lock_irqsave()).\n> >   4. Add zone lock tracepoints.\n> \n> I think you can improve the flow of this series if reorder as follows:\n> \t1. Introduce zone lock wrappers\n> \t4. Add zone lock tracepoints\n> \t2. Mechanically convert zone lock users to the wrappers\n> \t3. Convert compaction to use the wrappers...\n> \n> and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\n> wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\n> patch 1 by the time they get to patch 4.\n\nHi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.\n\n> \n> Thanks,\n> Ben\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 2/4] mm: convert zone lock users to wrappers",
          "message_id": "7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, allowing for better visibility into the locking behavior of the system. The implementation follows a lightweight pattern where an inline helper checks whether tracing is enabled and calls an out-of-line helper when necessary. When CONFIG_TRACING is disabled, the helpers compile to empty stubs, ensuring the fast path remains unaffected.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No substantive comments or reviews have been made yet, but the patch series appears to be a straightforward addition of tracepoints for zone lock acquire and release operations. The implementation follows a well-established pattern and is likely to be accepted once reviewed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "WAITING_FOR_REVIEW"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, allowing for better visibility into the locking behavior of the system. The implementation follows a lightweight pattern where an inline helper checks whether tracing is enabled and calls an out-of-line helper when necessary. When CONFIG_TRACING is disabled, the helpers compile to empty stubs, ensuring the fast path remains unaffected.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No substantive comments or reviews have been made yet, but the patch series appears to be a straightforward addition of tracepoints for zone lock acquire and release operations. The implementation follows a well-established pattern and is likely to be accepted once reviewed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "WAITING_FOR_REVIEW"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > Add thin wrappers around zone lock acquire/release operations. This\n> > prepares the code for future tracepoint instrumentation without\n> > modifying individual call sites.\n> > \n> > Centralizing zone lock operations behind wrappers allows future\n> > instrumentation or debugging hooks to be added without touching\n> > all users.\n> > \n> > No functional change intended. The wrappers are introduced in\n> > preparation for subsequent patches and are not yet used.\n> > \n> > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > ---\n> >  MAINTAINERS               |  1 +\n> >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> >  2 files changed, 39 insertions(+)\n> >  create mode 100644 include/linux/zone_lock.h\n> > \n> > diff --git a/MAINTAINERS b/MAINTAINERS\n> > index b4088f7290be..680c9ae02d7e 100644\n> > --- a/MAINTAINERS\n> > +++ b/MAINTAINERS\n> > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> >  F:\tinclude/linux/ptdump.h\n> >  F:\tinclude/linux/vmpressure.h\n> >  F:\tinclude/linux/vmstat.h\n> > +F:\tinclude/linux/zone_lock.h\n> >  F:\tkernel/fork.c\n> >  F:\tmm/Kconfig\n> >  F:\tmm/debug.c\n> > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > new file mode 100644\n> > index 000000000000..c531e26280e6\n> > --- /dev/null\n> > +++ b/include/linux/zone_lock.h\n> > @@ -0,0 +1,38 @@\n> > +/* SPDX-License-Identifier: GPL-2.0 */\n> > +#ifndef _LINUX_ZONE_LOCK_H\n> > +#define _LINUX_ZONE_LOCK_H\n> > +\n> > +#include <linux/mmzone.h>\n> > +#include <linux/spinlock.h>\n> > +\n> > +static inline void zone_lock_init(struct zone *zone)\n> > +{\n> > +\tspin_lock_init(&zone->lock);\n> > +}\n> > +\n> > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > +do {\t\t\t\t\t\t\t\t\\\n> > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > +} while (0)\n> > +\n> > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > +({\t\t\t\t\t\t\t\t\\\n> > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > +})\n> \n> Any reason you used macros for above two and inlined functions for remaining?\n>\n\nThe reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.\n\n> > +\n> > +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n> > +{\n> > +\tspin_unlock_irqrestore(&zone->lock, flags);\n> > +}\n> > +\n> > +static inline void zone_lock_irq(struct zone *zone)\n> > +{\n> > +\tspin_lock_irq(&zone->lock);\n> > +}\n> > +\n> > +static inline void zone_unlock_irq(struct zone *zone)\n> > +{\n> > +\tspin_unlock_irq(&zone->lock);\n> > +}\n> > +\n> > +#endif /* _LINUX_ZONE_LOCK_H */\n> > -- \n> > 2.47.3\n> > \n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v2] x86/irq: Optimize interrupts decimals printing",
          "message_id": "aZ8vvlwRbkzzpHqo@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aZ8vvlwRbkzzpHqo@shell.ilvokhin.com/",
          "date": "2026-02-25T17:22:12Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch author provided additional data from a production deployment, showing a reduction in CPU cycles spent in the /proc/interrupts read path on machines with different virtual core counts.",
          "analysis_source": "llm",
          "review_comments": []
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Gregory Price",
      "primary_email": "gourry@gourry.net",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[RFC PATCH v4 27/27] cxl: add cxl_compression PCI driver",
          "message_id": "20260222084842.1824063-28-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260222084842.1824063-28-gourry@gourry.net/",
          "date": "2026-02-22T08:50:38Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-22",
          "patch_summary": "This patch introduces a new PCI driver called cxl_compression, which is part of a larger series that adds support for Private Memory Nodes (PMNs) and Compressed RAM. The PMN feature allows for the creation of isolated NUMA nodes, while Compressed RAM enables the compression of memory pages to reduce memory usage. The cxl_compression driver is designed to work with CXL (Compute Express Link) devices, which are used to manage compressed memory. This patch adds the necessary infrastructure to support the cxl_compression driver and allows for the creation of PMNs and Compressed RAM services.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the node_private infrastructure being mutually exclusive with N_MEMORY, explained that it's intended for memory nodes not meant for general consumption, and confirmed that Zonelist construction changes are deferred to a subsequent commit.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY nodes are intended to contain general System RAM. Today, some\ndevice drivers hotplug their memory (marked Specific Purpose or Reserved)\nto get access to mm/ services, but don't intend it for general consumption.\n\nCreate N_MEMORY_PRIVATE for memory nodes whose memory is not intended for\ngeneral consumption. This state is mutually exclusive with N_MEMORY.\n\nAdd the node_private infrastructure for N_MEMORY_PRIVATE nodes:\n\n  - struct node_private: Per-node container stored in NODE_DATA(nid),\n    holding driver callbacks (ops), owner, and refcount.\n\n  - struct node_private_ops: Initial structure with void *reserved\n    placeholder and flags field.  Callbacks will be added by subsequent\n    commits as each consumer is wired up.\n\n  - folio_is_private_node() / page_is_private_node(): check if a\n    folio/page resides on a private node.\n\n  - folio_node_private_ops() / node_private_flags(): retrieve the ops\n    vtable or flags for a folio's node.\n\n  - Registration API: node_private_register()/unregister() for drivers\n    to register callbacks for private nodes. Only one driver callback\n    can be registered per node - attempting to register different ops\n    returns -EBUSY.\n\n  - sysfs attribute exposing N_MEMORY_PRIVATE node state.\n\nZonelist construction changes for private nodes are deferred to a\nsubsequent commit.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 197 ++++++++++++++++++++++++++++++++\n include/linux/mmzone.h       |   4 +\n include/linux/node_private.h | 210 +++++++++++++++++++++++++++++++++++\n include/linux/nodemask.h     |   1 +\n 4 files changed, 412 insertions(+)\n create mode 100644 include/linux/node_private.h\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 00cf4532f121..646dc48a23b5 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -22,6 +22,7 @@\n #include <linux/swap.h>\n #include <linux/slab.h>\n #include <linux/memblock.h>\n+#include <linux/node_private.h>\n \n static const struct bus_type node_subsys = {\n \t.name = \"node\",\n@@ -861,6 +862,198 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n \t\t\t   (void *)&nid, register_mem_block_under_node_hotplug);\n \treturn;\n }\n+\n+static DEFINE_MUTEX(node_private_lock);\n+static bool node_private_initialized;\n+\n+/**\n+ * node_private_register - Register a private node\n+ * @nid: Node identifier\n+ * @np: The node_private structure (driver-allocated, driver-owned)\n+ *\n+ * Register a driver for a private node. Only one driver can register\n+ * per node. If another driver has already registered (with different np),\n+ * -EBUSY is returned. Re-registration with the same np is allowed.\n+ *\n+ * The driver owns the node_private memory and must ensure it remains valid\n+ * until refcount reaches 0 after node_private_unregister().\n+ *\n+ * Returns 0 on success, negative errno on failure.\n+ */\n+int node_private_register(int nid, struct node_private *np)\n+{\n+\tstruct node_private *existing;\n+\tpg_data_t *pgdat;\n+\tint ret = 0;\n+\n+\tif (!np || !node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tif (!node_private_initialized)\n+\t\treturn -ENODEV;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\t/* N_MEMORY_PRIVATE and N_MEMORY are mutually exclusive */\n+\tif (node_state(nid, N_MEMORY)) {\n+\t\tret = -EBUSY;\n+\t\tgoto out;\n+\t}\n+\n+\tpgdat = NODE_DATA(nid);\n+\texisting = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t\t     lockdep_is_held(&node_private_lock));\n+\n+\t/* Only one source my register this node */\n+\tif (existing) {\n+\t\tif (existing != np) {\n+\t\t\tret = -EBUSY;\n+\t\t\tgoto out;\n+\t\t}\n+\t\tgoto out;\n+\t}\n+\n+\trefcount_set(&np->refcount, 1);\n+\tinit_completion(&np->released);\n+\n+\trcu_assign_pointer(pgdat->node_private, np);\n+\tpgdat->private = true;\n+\n+out:\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_register);\n+\n+/**\n+ * node_private_set_ops - Set service callbacks on a registered private node\n+ * @nid: Node identifier\n+ * @ops: Service callbacks and flags (driver-owned, must outlive registration)\n+ *\n+ * Validates flag dependencies and sets the ops on the node's node_private.\n+ * The node must already be registered via node_private_register().\n+ *\n+ * Returns 0 on success, -EINVAL for invalid flag combinations,\n+ * -ENODEV if no node_private is registered on @nid.\n+ */\n+int node_private_set_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!ops)\n+\t\treturn -EINVAL;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse\n+\t\tnp->ops = ops;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_set_ops);\n+\n+/**\n+ * node_private_clear_ops - Clear service callbacks from a private node\n+ * @nid: Node identifier\n+ * @ops: Expected ops pointer (must match current ops)\n+ *\n+ * Clears the ops only if @ops matches the currently registered ops,\n+ * preventing one service from accidentally clearing another's callbacks.\n+ *\n+ * Returns 0 on success, -ENODEV if no node_private is registered,\n+ * -EINVAL if @ops does not match.\n+ */\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse if (np->ops != ops)\n+\t\tret = -EINVAL;\n+\telse\n+\t\tnp->ops = NULL;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_clear_ops);\n+\n+/**\n+ * node_private_unregister - Unregister a private node\n+ * @nid: Node identifier\n+ *\n+ * Unregister the driver from a private node. Only succeeds if all memory\n+ * has been offlined and the node is no longer N_MEMORY_PRIVATE.\n+ * When successful, drops the refcount to 0 indicating the driver can\n+ * free its context.\n+ *\n+ * N_MEMORY_PRIVATE state is cleared by offline_pages() when the last\n+ * memory is offlined, not by this function.\n+ *\n+ * Return: 0 if unregistered, -EBUSY if N_MEMORY_PRIVATE is still set\n+ * (other memory blocks remain on this node).\n+ */\n+int node_private_unregister(int nid)\n+{\n+\tstruct node_private *np;\n+\tpg_data_t *pgdat;\n+\n+\tif (!node_possible(nid))\n+\t\treturn 0;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\tpgdat = NODE_DATA(nid);\n+\tnp = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Only unregister if all memory is offline and N_MEMORY_PRIVATE is\n+\t * cleared. N_MEMORY_PRIVATE is cleared by offline_pages() when the\n+\t * last memory block is offlined.\n+\t */\n+\tif (node_state(nid, N_MEMORY_PRIVATE)) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn -EBUSY;\n+\t}\n+\n+\trcu_assign_pointer(pgdat->node_private, NULL);\n+\tpgdat->private = false;\n+\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\n+\tsynchronize_rcu();\n+\n+\tif (!refcount_dec_and_test(&np->refcount))\n+\t\twait_for_completion(&np->released);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(node_private_unregister);\n+\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n /**\n@@ -959,6 +1152,7 @@ static struct node_attr node_state_attr[] = {\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n \t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\n \t\t\t\t\t   N_GENERIC_INITIATOR),\n@@ -972,6 +1166,7 @@ static struct attribute *node_state_attrs[] = {\n \t&node_state_attr[N_HIGH_MEMORY].attr.attr,\n #endif\n \t&node_state_attr[N_MEMORY].attr.attr,\n+\t&node_state_attr[N_MEMORY_PRIVATE].attr.attr,\n \t&node_state_attr[N_CPU].attr.attr,\n \t&node_state_attr[N_GENERIC_INITIATOR].attr.attr,\n \tNULL\n@@ -1007,5 +1202,7 @@ void __init node_dev_init(void)\n \t\t\tpanic(\"%s() failed to add node: %d\\n\", __func__, ret);\n \t}\n \n+\tnode_private_initialized = true;\n+\n \tregister_memory_blocks_under_nodes();\n }\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex b01cb1e49896..992eb1c5a2c6 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -25,6 +25,8 @@\n #include <linux/zswap.h>\n #include <asm/page.h>\n \n+struct node_private;\n+\n /* Free memory management - zoned buddy allocator.  */\n #ifndef CONFIG_ARCH_FORCE_MAX_ORDER\n #define MAX_PAGE_ORDER 10\n@@ -1514,6 +1516,8 @@ typedef struct pglist_data {\n \tatomic_long_t\t\tvm_stat[NR_VM_NODE_STAT_ITEMS];\n #ifdef CONFIG_NUMA\n \tstruct memory_tier __rcu *memtier;\n+\tstruct node_private __rcu *node_private;\n+\tbool private;\n #endif\n #ifdef CONFIG_MEMORY_FAILURE\n \tstruct memory_failure_stats mf_stats;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nnew file mode 100644\nindex 000000000000..6a70ec39d569\n--- /dev/null\n+++ b/include/linux/node_private.h\n@@ -0,0 +1,210 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_NODE_PRIVATE_H\n+#define _LINUX_NODE_PRIVATE_H\n+\n+#include <linux/completion.h>\n+#include <linux/mm.h>\n+#include <linux/nodemask.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+\n+struct page;\n+struct vm_area_struct;\n+struct vm_fault;\n+\n+/**\n+ * struct node_private_ops - Callbacks for private node services\n+ *\n+ * Services register these callbacks to intercept MM operations that affect\n+ * their private nodes.\n+ *\n+ * Flag bits control which MM subsystems may operate on folios on this node.\n+ *\n+ * The pgdat->node_private pointer is RCU-protected.  Callbacks fall into\n+ * three categories based on their calling context:\n+ *\n+ * Folio-referenced callbacks (RCU released before callback):\n+ *   The caller holds a reference to a folio on the private node, which\n+ *   pins the node's memory online and prevents node_private teardown.\n+ *\n+ * Refcounted callbacks (RCU released before callback):\n+ *   The caller has no folio on the private node (e.g., folios are on a\n+ *   source node being migrated TO this node).  A temporary refcount is\n+ *   taken on node_private under rcu_read_lock to keep the structure (and\n+ *   the service module) alive across the callback.  node_private_unregister\n+ *   waits for all temporary references to drain before returning.\n+ *\n+ * Non-folio callbacks (rcu_read_lock held during callback):\n+ *   No folio reference exists, so rcu_read_lock is held across the\n+ *   callback to prevent node_private from being freed.\n+ *   These callbacks MUST NOT sleep.\n+ *\n+ * @flags: Operation exclusion flags (NP_OPS_* constants).\n+ *\n+ */\n+struct node_private_ops {\n+\tunsigned long flags;\n+};\n+\n+/**\n+ * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n+ *\n+ * This structure is allocated by the driver and passed to node_private_register().\n+ * The driver owns the memory and must ensure it remains valid until after\n+ * node_private_unregister() returns with the reference count dropped to 0.\n+ *\n+ * @owner: Opaque driver identifier\n+ * @refcount: Reference count (1 = registered; temporary refs for non-folio\n+ *\t\tcallbacks that may sleep; 0 = fully released)\n+ * @released: Signaled when refcount drops to 0; unregister waits on this\n+ * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ */\n+struct node_private {\n+\tvoid *owner;\n+\trefcount_t refcount;\n+\tstruct completion released;\n+\tconst struct node_private_ops *ops;\n+};\n+\n+#ifdef CONFIG_NUMA\n+\n+#include <linux/mmzone.h>\n+\n+/**\n+ * folio_is_private_node - Check if folio is on an N_MEMORY_PRIVATE node\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio resides on a private node.\n+ */\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn node_state(folio_nid(folio), N_MEMORY_PRIVATE);\n+}\n+\n+/**\n+ * page_is_private_node - Check if page is on an N_MEMORY_PRIVATE node\n+ * @page: The page to check\n+ *\n+ * Returns true if the page resides on a private node.\n+ */\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\tconst struct node_private_ops *ops;\n+\tstruct node_private *np;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(folio_nid(folio))->node_private);\n+\tops = np ? np->ops : NULL;\n+\trcu_read_unlock();\n+\n+\treturn ops;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\tstruct node_private *np;\n+\tunsigned long flags;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tflags = (np && np->ops) ? np->ops->flags : 0;\n+\trcu_read_unlock();\n+\n+\treturn flags;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn node_private_flags(folio_nid(f)) & flag;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn node_private_flags(nid) & flag;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn node_private_flags(zone_to_nid(z)) & flag;\n+}\n+\n+#else /* !CONFIG_NUMA */\n+\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn false;\n+}\n+\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn false;\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\treturn NULL;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+#endif /* CONFIG_NUMA */\n+\n+#if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\n+\n+int node_private_register(int nid, struct node_private *np);\n+int node_private_unregister(int nid);\n+int node_private_set_ops(int nid, const struct node_private_ops *ops);\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n+\n+#else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n+\n+static inline int node_private_register(int nid, struct node_private *np)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_unregister(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline int node_private_set_ops(int nid,\n+\t\t\t\t       const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_clear_ops(int nid,\n+\t\t\t\t\t const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+#endif /* CONFIG_NUMA && CONFIG_MEMORY_HOTPLUG */\n+\n+#endif /* _LINUX_NODE_PRIVATE_H */\ndiff --git a/include/linux/nodemask.h b/include/linux/nodemask.h\nindex bd38648c998d..c9bcfd5a9a06 100644\n--- a/include/linux/nodemask.h\n+++ b/include/linux/nodemask.h\n@@ -391,6 +391,7 @@ enum node_states {\n \tN_HIGH_MEMORY = N_NORMAL_MEMORY,\n #endif\n \tN_MEMORY,\t\t/* The node has memory(regular, high, movable) */\n+\tN_MEMORY_PRIVATE,\t/* The node's memory is private */\n \tN_CPU,\t\t/* The node has one or more cpus */\n \tN_GENERIC_INITIATOR,\t/* The node has one or more Generic Initiators */\n \tNR_NODE_STATES\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about general allocations landing on private nodes without explicit permission by introducing __GFP_PRIVATE and updating cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE nodes unless this flag is set.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY_PRIVATE nodes hold device-managed memory that should not be\nused for general allocations. Without a gating mechanism, any allocation\ncould land on a private node if it appears in the task's mems_allowed.\n\nIntroduce __GFP_PRIVATE that explicitly opts in to allocation from\nN_MEMORY_PRIVATE nodes.\n\nAdd the GFP_PRIVATE compound mask (__GFP_PRIVATE | __GFP_THISNODE)\nfor callers that explicitly target private nodes to help prevent\nfallback allocations from DRAM.\n\nUpdate cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE\nnodes unless __GFP_PRIVATE is set.\n\nIn interrupt context, only N_MEMORY nodes are valid.\n\nUpdate cpuset_handle_hotplug() to include N_MEMORY_PRIVATE nodes in\nthe effective mems set, allowing cgroup-level control over private\nnode access.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/gfp_types.h      | 15 +++++++++++++--\n include/trace/events/mmflags.h |  4 ++--\n kernel/cgroup/cpuset.c         | 32 ++++++++++++++++++++++++++++----\n 3 files changed, 43 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/gfp_types.h b/include/linux/gfp_types.h\nindex 3de43b12209e..ac375f9a0fc2 100644\n--- a/include/linux/gfp_types.h\n+++ b/include/linux/gfp_types.h\n@@ -33,7 +33,7 @@ enum {\n \t___GFP_IO_BIT,\n \t___GFP_FS_BIT,\n \t___GFP_ZERO_BIT,\n-\t___GFP_UNUSED_BIT,\t/* 0x200u unused */\n+\t___GFP_PRIVATE_BIT,\n \t___GFP_DIRECT_RECLAIM_BIT,\n \t___GFP_KSWAPD_RECLAIM_BIT,\n \t___GFP_WRITE_BIT,\n@@ -69,7 +69,7 @@ enum {\n #define ___GFP_IO\t\tBIT(___GFP_IO_BIT)\n #define ___GFP_FS\t\tBIT(___GFP_FS_BIT)\n #define ___GFP_ZERO\t\tBIT(___GFP_ZERO_BIT)\n-/* 0x200u unused */\n+#define ___GFP_PRIVATE\t\tBIT(___GFP_PRIVATE_BIT)\n #define ___GFP_DIRECT_RECLAIM\tBIT(___GFP_DIRECT_RECLAIM_BIT)\n #define ___GFP_KSWAPD_RECLAIM\tBIT(___GFP_KSWAPD_RECLAIM_BIT)\n #define ___GFP_WRITE\t\tBIT(___GFP_WRITE_BIT)\n@@ -139,6 +139,11 @@ enum {\n  * %__GFP_ACCOUNT causes the allocation to be accounted to kmemcg.\n  *\n  * %__GFP_NO_OBJ_EXT causes slab allocation to have no object extension.\n+ *\n+ * %__GFP_PRIVATE allows allocation from N_MEMORY_PRIVATE nodes (e.g., compressed\n+ * memory, accelerator memory). Without this flag, allocations are restricted\n+ * to N_MEMORY nodes only. Used by migration/demotion paths when explicitly\n+ * targeting private nodes.\n  */\n #define __GFP_RECLAIMABLE ((__force gfp_t)___GFP_RECLAIMABLE)\n #define __GFP_WRITE\t((__force gfp_t)___GFP_WRITE)\n@@ -146,6 +151,7 @@ enum {\n #define __GFP_THISNODE\t((__force gfp_t)___GFP_THISNODE)\n #define __GFP_ACCOUNT\t((__force gfp_t)___GFP_ACCOUNT)\n #define __GFP_NO_OBJ_EXT   ((__force gfp_t)___GFP_NO_OBJ_EXT)\n+#define __GFP_PRIVATE\t((__force gfp_t)___GFP_PRIVATE)\n \n /**\n  * DOC: Watermark modifiers\n@@ -367,6 +373,10 @@ enum {\n  * available and will not wake kswapd/kcompactd on failure. The _LIGHT\n  * version does not attempt reclaim/compaction at all and is by default used\n  * in page fault path, while the non-light is used by khugepaged.\n+ *\n+ * %GFP_PRIVATE adds %__GFP_THISNODE by default to prevent any fallback\n+ * allocations to other nodes, given that the caller was already attempting\n+ * to access driver-managed memory explicitly.\n  */\n #define GFP_ATOMIC\t(__GFP_HIGH|__GFP_KSWAPD_RECLAIM)\n #define GFP_KERNEL\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n@@ -382,5 +392,6 @@ enum {\n #define GFP_TRANSHUGE_LIGHT\t((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \\\n \t\t\t __GFP_NOMEMALLOC | __GFP_NOWARN) & ~__GFP_RECLAIM)\n #define GFP_TRANSHUGE\t(GFP_TRANSHUGE_LIGHT | __GFP_DIRECT_RECLAIM)\n+#define GFP_PRIVATE\t(__GFP_PRIVATE | __GFP_THISNODE)\n \n #endif /* __LINUX_GFP_TYPES_H */\ndiff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h\nindex a6e5a44c9b42..f042cd848451 100644\n--- a/include/trace/events/mmflags.h\n+++ b/include/trace/events/mmflags.h\n@@ -37,7 +37,8 @@\n \tTRACE_GFP_EM(HARDWALL)\t\t\t\\\n \tTRACE_GFP_EM(THISNODE)\t\t\t\\\n \tTRACE_GFP_EM(ACCOUNT)\t\t\t\\\n-\tTRACE_GFP_EM(ZEROTAGS)\n+\tTRACE_GFP_EM(ZEROTAGS)\t\t\t\\\n+\tTRACE_GFP_EM(PRIVATE)\n \n #ifdef CONFIG_KASAN_HW_TAGS\n # define TRACE_GFP_FLAGS_KASAN\t\t\t\\\n@@ -73,7 +74,6 @@\n TRACE_GFP_FLAGS\n \n /* Just in case these are ever used */\n-TRACE_DEFINE_ENUM(___GFP_UNUSED_BIT);\n TRACE_DEFINE_ENUM(___GFP_LAST_BIT);\n \n #define gfpflag_string(flag) {(__force unsigned long)flag, #flag}\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 473aa9261e16..1a597f0c7c6c 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -444,21 +444,32 @@ static void guarantee_active_cpus(struct task_struct *tsk,\n }\n \n /*\n- * Return in *pmask the portion of a cpusets's mems_allowed that\n+ * Return in *pmask the portion of a cpuset's mems_allowed that\n  * are online, with memory.  If none are online with memory, walk\n  * up the cpuset hierarchy until we find one that does have some\n  * online mems.  The top cpuset always has some mems online.\n  *\n  * One way or another, we guarantee to return some non-empty subset\n- * of node_states[N_MEMORY].\n+ * of node_states[N_MEMORY].  N_MEMORY_PRIVATE nodes from the\n+ * original cpuset are preserved, but only N_MEMORY nodes are\n+ * pulled from ancestors.\n  *\n  * Call with callback_lock or cpuset_mutex held.\n  */\n static void guarantee_online_mems(struct cpuset *cs, nodemask_t *pmask)\n {\n+\tstruct cpuset *orig_cs = cs;\n+\tint nid;\n+\n \twhile (!nodes_intersects(cs->effective_mems, node_states[N_MEMORY]))\n \t\tcs = parent_cs(cs);\n+\n \tnodes_and(*pmask, cs->effective_mems, node_states[N_MEMORY]);\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_isset(nid, orig_cs->effective_mems))\n+\t\t\tnode_set(nid, *pmask);\n+\t}\n }\n \n /**\n@@ -4075,7 +4086,9 @@ static void cpuset_handle_hotplug(void)\n \n \t/* fetch the available cpus/mems and find out which changed how */\n \tcpumask_copy(&new_cpus, cpu_active_mask);\n-\tnew_mems = node_states[N_MEMORY];\n+\n+\t/* Include N_MEMORY_PRIVATE so cpuset controls access the same way */\n+\tnodes_or(new_mems, node_states[N_MEMORY], node_states[N_MEMORY_PRIVATE]);\n \n \t/*\n \t * If subpartitions_cpus is populated, it is likely that the check\n@@ -4488,10 +4501,21 @@ bool cpuset_node_allowed(struct cgroup *cgroup, int nid)\n  * __alloc_pages() will include all nodes.  If the slab allocator\n  * is passed an offline node, it will fall back to the local node.\n  * See kmem_cache_alloc_node().\n+ *\n+ *\n+ * Private nodes aren't eligible for these allocations, so skip them.\n+ * guarantee_online_mems guaranttes at least one N_MEMORY node is set.\n  */\n static int cpuset_spread_node(int *rotor)\n {\n-\treturn *rotor = next_node_in(*rotor, current->mems_allowed);\n+\tint node;\n+\n+\tdo {\n+\t\tnode = next_node_in(*rotor, current->mems_allowed);\n+\t\t*rotor = node;\n+\t} while (node_state(node, N_MEMORY_PRIVATE));\n+\n+\treturn node;\n }\n \n /**\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern that the open-coded cpuset filtering in mm/ does not account for N_MEMORY_PRIVATE nodes on systems without cpusets, which can lead to private-node zones leaking into allocation paths. The author added a new helper function numa_zone_allowed() and replaced the open-coded patterns with it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Various locations in mm/ open-code cpuset filtering with:\n\n  cpusets_enabled() && ALLOC_CPUSET && !__cpuset_zone_allowed()\n\nThis pattern does not account for N_MEMORY_PRIVATE nodes on systems\nwithout cpusets, so private-node zones can leak into allocation\npaths that should only see general-purpose memory.\n\nAdd numa_zone_allowed() which consolidates zone filtering. It checks\ncpuset membership when cpusets are enabled, and otherwise gates\nN_MEMORY_PRIVATE zones behind __GFP_PRIVATE globally.\n\nReplace the open-coded patterns in mm/ with the new helper.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/compaction.c |  6 ++----\n mm/hugetlb.c    |  2 +-\n mm/internal.h   |  7 +++++++\n mm/page_alloc.c | 31 ++++++++++++++++++++-----------\n mm/slub.c       |  3 ++-\n 5 files changed, 32 insertions(+), 17 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..6a65145b03d8 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -2829,10 +2829,8 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tenum compact_result status;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 51273baec9e5..f2b914ab5910 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -1353,7 +1353,7 @@ static struct folio *dequeue_hugetlb_folio_nodemask(struct hstate *h, gfp_t gfp_\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {\n \t\tstruct folio *folio;\n \n-\t\tif (!cpuset_zone_allowed(zone, gfp_mask))\n+\t\tif (!numa_zone_alloc_allowed(ALLOC_CPUSET, zone, gfp_mask))\n \t\t\tcontinue;\n \t\t/*\n \t\t * no need to ask again on the same node. Pool is node rather than\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 23ee14790227..97023748e6a9 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t      gfp_t gfp_mask);\n #else\n #define node_reclaim_mode 0\n \n@@ -1218,6 +1220,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t\t     gfp_t gfp_mask)\n+{\n+\treturn true;\n+}\n #endif\n \n static inline bool node_reclaim_enabled(void)\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 2facee0805da..47f2619d3840 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3690,6 +3690,21 @@ static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n \treturn node_distance(zone_to_nid(local_zone), zone_to_nid(zone)) <=\n \t\t\t\tnode_reclaim_distance;\n }\n+\n+/* Returns true if allocation from this zone is permitted */\n+bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone, gfp_t gfp_mask)\n+{\n+\t/* Gate N_MEMORY_PRIVATE zones behind __GFP_PRIVATE */\n+\tif (!(gfp_mask & __GFP_PRIVATE) &&\n+\t    node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn false;\n+\n+\t/* If cpusets is being used, check mems_allowed */\n+\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET))\n+\t\treturn cpuset_zone_allowed(zone, gfp_mask);\n+\n+\treturn true;\n+}\n #else\t/* CONFIG_NUMA */\n static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n {\n@@ -3781,10 +3796,8 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\tstruct page *page;\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \t\t/*\n \t\t * When allocating a page cache page for writing, we\n \t\t * want to get it from a node that is within its dirty\n@@ -4585,10 +4598,8 @@ should_reclaim_retry(gfp_t gfp_mask, unsigned order,\n \t\tunsigned long min_wmark = min_wmark_pages(zone);\n \t\tbool wmark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tavailable = reclaimable = zone_reclaimable_pages(zone);\n \t\tavailable += zone_page_state_snapshot(zone, NR_FREE_PAGES);\n@@ -5084,10 +5095,8 @@ unsigned long alloc_pages_bulk_noprof(gfp_t gfp, int preferred_nid,\n \tfor_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&\n-\t\t    !__cpuset_zone_allowed(zone, gfp)) {\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp))\n \t\t\tcontinue;\n-\t\t}\n \n \t\tif (nr_online_nodes > 1 && zone != zonelist_zone(ac.preferred_zoneref) &&\n \t\t    zone_to_nid(zone) != zonelist_node_idx(ac.preferred_zoneref)) {\ndiff --git a/mm/slub.c b/mm/slub.c\nindex 861592ac5425..e4bd6ede81d1 100644\n--- a/mm/slub.c\n+++ b/mm/slub.c\n@@ -3595,7 +3595,8 @@ static struct slab *get_any_partial(struct kmem_cache *s,\n \n \t\t\tn = get_node(s, zone_to_nid(zone));\n \n-\t\t\tif (n && cpuset_zone_allowed(zone, pc->flags) &&\n+\t\t\tif (n && numa_zone_alloc_allowed(ALLOC_CPUSET, zone,\n+\t\t\t\t\t\t   pc->flags) &&\n \t\t\t\t\tn->nr_partial > s->min_partial) {\n \t\t\t\tslab = get_partial_node(s, n, pc);\n \t\t\t\tif (slab) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about N_MEMORY fallback lists including N_MEMORY_PRIVATE nodes, explaining that this would allow allocations from private nodes in some scenarios and cause unnecessary iterations over ineligible nodes. The author provided a patch to fix the issue by adding private nodes as fallbacks for kernel allocations on behalf of the private node.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY fallback lists should not include N_MEMORY_PRIVATE nodes, at\nworst this would allow allocation from them in some scenarios, and at\nbest it causes iterations over nodes that aren't eligible.\n\nPrivate node primary fallback lists do include N_MEMORY nodes so\nkernel/slab allocations made on behalf of the private node can\nfall back to DRAM when __GFP_PRIVATE is not set.\n\nThe nofallback list contains only the node's own zones, restricting\n__GFP_THISNODE allocations to the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/page_alloc.c | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 47f2619d3840..5a1b35421d78 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5683,6 +5683,26 @@ static void build_zonelists(pg_data_t *pgdat)\n \tlocal_node = pgdat->node_id;\n \tprev_node = local_node;\n \n+\t/*\n+\t * Private nodes need N_MEMORY nodes as fallback for kernel allocations\n+\t * (e.g., slab objects allocated on behalf of this node).\n+\t */\n+\tif (node_state(local_node, N_MEMORY_PRIVATE)) {\n+\t\tnode_order[nr_nodes++] = local_node;\n+\t\tnode_set(local_node, used_mask);\n+\n+\t\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0)\n+\t\t\tnode_order[nr_nodes++] = node;\n+\n+\t\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n+\t\tbuild_thisnode_zonelists(pgdat);\n+\t\tpr_info(\"Fallback order for Node %d (private):\", local_node);\n+\t\tfor (node = 0; node < nr_nodes; node++)\n+\t\t\tpr_cont(\" %d\", node_order[node]);\n+\t\tpr_cont(\"\\n\");\n+\t\treturn;\n+\t}\n+\n \tmemset(node_order, 0, sizeof(node_order));\n \twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author addressed a concern about the need for a unified predicate to exclude both N_MEMORY_PRIVATE and ZONE_DEVICE folios from MM operations, and provided a patch that adds the folio_is_private_managed() function to achieve this.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix is needed",
                "provided a patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Multiple mm/ subsystems already skip operations for ZONE_DEVICE folios,\nand N_MEMORY_PRIVATE folios share the checkpoints for ZONE_DEVICE pages.\n\nAdd folio_is_private_managed() as a unified predicate that returns true\nfor folios on N_MEMORY_PRIVATE nodes or in ZONE_DEVICE.\n\nThis predicate replaces folio_is_zone_device at skip sites where both\nfolio types should be excluded from an MM operation.\n\nAt some locations, explicit zone_device vs private_node checks are more\nappropriate when the operations between the two fundamentally differ.\n\nThe !CONFIG_NUMA stubs fall through to folio_is_zone_device() only,\npreserving existing behavior when NUMA is disabled.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 6a70ec39d569..7687a4cf990c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -92,6 +92,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio) || folio_is_private_node(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n@@ -146,6 +156,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn false;\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about mlocking private node folios, explaining that they should not be locked and citing the existing folio_is_zone_device check as sufficient to handle this case. The author extended this check to include private nodes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "addressed_concern",
                "explained_reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nmlocked.  The existing folio_is_zone_device check is already correctly\nplaced to handle this - simply extend it for private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/mlock.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/mlock.c b/mm/mlock.c\nindex 2f699c3497a5..c56159253e45 100644\n--- a/mm/mlock.c\n+++ b/mm/mlock.c\n@@ -25,6 +25,7 @@\n #include <linux/memcontrol.h>\n #include <linux/mm_inline.h>\n #include <linux/secretmem.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -366,7 +367,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (is_huge_zero_pmd(*pmd))\n \t\t\tgoto out;\n \t\tfolio = pmd_folio(*pmd);\n-\t\tif (folio_is_zone_device(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)))\n \t\t\tgoto out;\n \t\tif (vma->vm_flags & VM_LOCKED)\n \t\t\tmlock_folio(folio);\n@@ -386,7 +387,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (!pte_present(ptent))\n \t\t\tcontinue;\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\tstep = folio_mlock_step(folio, pte, addr, end);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author acknowledged a concern that madvise and pageout operations should not interfere with device driver-managed private node folios, agreed to extend the zone_device check to cover private nodes, and made corresponding changes to mm/madvise.c.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "agreed to make changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nsubjectto madvise cold/pageout/free operations that would interfere\nwith the driver's memory management.\n\nExtend the existing zone_device check to cover private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/madvise.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/madvise.c b/mm/madvise.c\nindex b617b1be0f53..3aac105e840b 100644\n--- a/mm/madvise.c\n+++ b/mm/madvise.c\n@@ -32,6 +32,7 @@\n #include <linux/leafops.h>\n #include <linux/shmem_fs.h>\n #include <linux/mmu_notifier.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlb.h>\n \n@@ -475,7 +476,7 @@ static int madvise_cold_or_pageout_pte_range(pmd_t *pmd,\n \t\t\tcontinue;\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n@@ -704,7 +705,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,\n \t\t}\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private node folios participating in KSM merging by default, agreeing that this can interfere with driver operations. The author extended existing checks to exclude private node folios from KSM merging.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not participate in KSM merging by default.\nThe driver manages the memory lifecycle and KSM's page sharing can\ninterfere with driver operations.\n\nExtend the existing zone_device checks in get_mergeable_page and\nksm_next_page_pmd_entry to cover private node folios as well.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/ksm.c | 9 ++++++---\n 1 file changed, 6 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/ksm.c b/mm/ksm.c\nindex 2d89a7c8b4eb..c48e95a6fff9 100644\n--- a/mm/ksm.c\n+++ b/mm/ksm.c\n@@ -40,6 +40,7 @@\n #include <linux/oom.h>\n #include <linux/numa.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include \"internal.h\"\n@@ -808,7 +809,7 @@ static struct page *get_mergeable_page(struct ksm_rmap_item *rmap_item)\n \n \tfolio = folio_walk_start(&fw, vma, addr, 0);\n \tif (folio) {\n-\t\tif (!folio_is_zone_device(folio) &&\n+\t\tif (!folio_is_private_managed(folio) &&\n \t\t    folio_test_anon(folio)) {\n \t\t\tfolio_get(folio);\n \t\t\tpage = fw.page;\n@@ -2521,7 +2522,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\t\tgoto not_found_unlock;\n \t\t\tfolio = page_folio(page);\n \n-\t\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t\t    !folio_test_anon(folio))\n \t\t\t\tgoto not_found_unlock;\n \n \t\t\tpage += ((addr & (PMD_SIZE - 1)) >> PAGE_SHIFT);\n@@ -2545,7 +2547,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\tcontinue;\n \t\tfolio = page_folio(page);\n \n-\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t    !folio_test_anon(folio))\n \t\t\tcontinue;\n \t\tgoto found_unlock;\n \t}\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about collapse operations on private nodes potentially promoting pages to local nodes and inverting LRU order, agreeing that handling this like zone_device is the best approach for now.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "A collapse operation allocates a new large folio and migrates the\nsmaller folios into it.  This is an issue for private nodes:\n\n  1. The private node service may not support migration\n  2. Collapse may promotes pages from the private node to a local node,\n     which may result in an LRU inversion that defeats memory tiering.\n\nHandle this just like zone_device for now.\n\nIt may be possible to support this later for some private node services\nthat report explicit support for collapse (and migration).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/khugepaged.c | 7 ++++---\n 1 file changed, 4 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/khugepaged.c b/mm/khugepaged.c\nindex 97d1b2824386..36f6bc5da53c 100644\n--- a/mm/khugepaged.c\n+++ b/mm/khugepaged.c\n@@ -21,6 +21,7 @@\n #include <linux/shmem_fs.h>\n #include <linux/dax.h>\n #include <linux/ksm.h>\n+#include <linux/node_private.h>\n #include <linux/pgalloc.h>\n \n #include <asm/tlb.h>\n@@ -571,7 +572,7 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,\n \t\t\tgoto out;\n \t\t}\n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out;\n \t\t}\n@@ -1323,7 +1324,7 @@ static int hpage_collapse_scan_pmd(struct mm_struct *mm,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out_unmap;\n \t\t}\n@@ -1575,7 +1576,7 @@ int collapse_pte_mapped_thp(struct mm_struct *mm, unsigned long addr,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, ptent);\n-\t\tif (WARN_ON_ONCE(page && is_zone_device_page(page)))\n+\t\tif (WARN_ON_ONCE(page && page_is_private_managed(page)))\n \t\t\tpage = NULL;\n \t\t/*\n \t\t * Note that uprobe, debugger, or MAP_PRIVATE may change the\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about cleanup when a folio's refcount drops to zero, explaining that the service may need to perform cleanup before the page returns to the buddy allocator. They added a new function `folio_managed_on_free()` to wrap both zone_device and private node semantics for this operation. The function will return true if the folio is fully handled (zone_device) or false if the callback ran but the folio should continue through the normal free path (private_node).",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "added new function to address concern"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a folio's refcount drops to zero, the service may need to perform\ncleanup before the page returns to the buddy allocator (e.g. zeroing\npages to scrub stale compressed data / release compression ratio).\n\nAdd folio_managed_on_free() to wrap both zone_device and private node\nsemantics for this operation since they are the same.\n\nOne difference between zone_device and private node folios:\n  - private nodes may choose to either take a reference and return true\n    (\"handled\"), or return false to return it back to the buddy.\n\n  - zone_device returns the page to the buddy (always returns true)\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 30 ++++++++++++++++++++++++++++++\n mm/swap.c                    | 21 ++++++++++-----------\n 3 files changed, 46 insertions(+), 11 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7687a4cf990c..09ea7c4cb13c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -39,10 +39,16 @@ struct vm_fault;\n  *   callback to prevent node_private from being freed.\n  *   These callbacks MUST NOT sleep.\n  *\n+ * @free_folio: Called when a folio refcount drops to 0\n+ *   [folio-referenced callback]\n+ *   Returns: true if handled (skip return to buddy)\n+ *            false if no op (return to buddy)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n+\tbool (*free_folio)(struct folio *folio);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 97023748e6a9..658da41cdb8e 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1412,6 +1412,36 @@ int numa_migrate_check(struct folio *folio, struct vm_fault *vmf,\n void free_zone_device_folio(struct folio *folio);\n int migrate_device_coherent_folio(struct folio *folio);\n \n+/**\n+ * folio_managed_on_free - Notify managed-memory service that folio\n+ *                         refcount reached zero.\n+ * @folio: the folio being freed\n+ *\n+ * Returns true if the folio is fully handled (zone_device -- caller\n+ * must return immediately).  Returns false if the callback ran but\n+ * the folio should continue through the normal free path\n+ * (private_node -- pages go back to buddy).\n+ *\n+ * Returns false for normal folios (no-op).\n+ */\n+static inline bool folio_managed_on_free(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio)) {\n+\t\tfree_zone_device_folio(folio);\n+\t\treturn true;\n+\t}\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->free_folio) {\n+\t\t\tif (ops->free_folio(folio))\n+\t\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/swap.c b/mm/swap.c\nindex 2260dcd2775e..dca306e1ae6d 100644\n--- a/mm/swap.c\n+++ b/mm/swap.c\n@@ -37,6 +37,7 @@\n #include <linux/page_idle.h>\n #include <linux/local_lock.h>\n #include <linux/buffer_head.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -96,10 +97,9 @@ static void page_cache_release(struct folio *folio)\n \n void __folio_put(struct folio *folio)\n {\n-\tif (unlikely(folio_is_zone_device(folio))) {\n-\t\tfree_zone_device_folio(folio);\n-\t\treturn;\n-\t}\n+\tif (unlikely(folio_is_private_managed(folio)))\n+\t\tif (folio_managed_on_free(folio))\n+\t\t\treturn;\n \n \tif (folio_test_hugetlb(folio)) {\n \t\tfree_huge_folio(folio);\n@@ -961,19 +961,18 @@ void folios_put_refs(struct folio_batch *folios, unsigned int *refs)\n \t\tif (is_huge_zero_folio(folio))\n \t\t\tcontinue;\n \n-\t\tif (folio_is_zone_device(folio)) {\n+\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n+\t\t\tcontinue;\n+\n+\t\tif (unlikely(folio_is_private_managed(folio))) {\n \t\t\tif (lruvec) {\n \t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n \t\t\t\tlruvec = NULL;\n \t\t\t}\n-\t\t\tif (folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\t\tfree_zone_device_folio(folio);\n-\t\t\tcontinue;\n+\t\t\tif (folio_managed_on_free(folio))\n+\t\t\t\tcontinue;\n \t\t}\n \n-\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\tcontinue;\n-\n \t\t/* hugetlb has its own memcg */\n \t\tif (folio_test_hugetlb(folio)) {\n \t\t\tif (lruvec) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about notifying private node services when a THP folio is split by adding an optional callback to the ops struct and updating the folio_split path in huge_memory.c. The author confirmed that this change will be included in the next version of the patch series.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "confirmed a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private node services may need to update internal metadata when\na THP folio is split.  ZONE_DEVICE already has a split callback via\npgmap->ops; private nodes can provide the same capability.\n\nJust like zone_device, some private node services may want to know\nabout a folio being split.  Add this optional callback to the ops\nstruct and add a wrapper for zone_device and private node callback\ndispatch to be consolidated.\n\nWire this into __folio_split() where the zone_device check was made.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 33 +++++++++++++++++++++++++++++++++\n mm/huge_memory.c             |  6 ++++--\n 2 files changed, 37 insertions(+), 2 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 09ea7c4cb13c..f9dd2d25c8a5 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -3,6 +3,7 @@\n #define _LINUX_NODE_PRIVATE_H\n \n #include <linux/completion.h>\n+#include <linux/memremap.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -44,11 +45,19 @@ struct vm_fault;\n  *   Returns: true if handled (skip return to buddy)\n  *            false if no op (return to buddy)\n  *\n+ * @folio_split: Notification that a folio on this private node is being split.\n+ *    [folio-referenced callback]\n+ *     Called from the folio split path via folio_managed_split_cb().\n+ *     @folio is the original folio; @new_folio is the newly created folio,\n+ *     or NULL when called for the final (original) folio after all sub-folios\n+ *     have been split off.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n+\tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n \tunsigned long flags;\n };\n \n@@ -150,6 +159,24 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn node_private_flags(zone_to_nid(z)) & flag;\n }\n \n+static inline void node_private_split_cb(struct folio *folio,\n+\t\t\t\t\t struct folio *new_folio)\n+{\n+\tconst struct node_private_ops *ops = folio_node_private_ops(folio);\n+\n+\tif (ops && ops->folio_split)\n+\t\tops->folio_split(folio, new_folio);\n+}\n+\n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+\telse if (folio_is_private_node(original_folio))\n+\t\tnode_private_split_cb(original_folio, new_folio);\n+}\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -198,6 +225,12 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn false;\n }\n \n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+}\n #endif /* CONFIG_NUMA */\n \n #if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 40cf59301c21..2ecae494291a 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -24,6 +24,7 @@\n #include <linux/freezer.h>\n #include <linux/mman.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/debugfs.h>\n #include <linux/migrate.h>\n@@ -3850,7 +3851,7 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \n \t\t\tnext = folio_next(new_folio);\n \n-\t\t\tzone_device_private_split_cb(folio, new_folio);\n+\t\t\tfolio_managed_split_cb(folio, new_folio);\n \n \t\t\tfolio_ref_unfreeze(new_folio,\n \t\t\t\t\t   folio_cache_ref_count(new_folio) + 1);\n@@ -3889,7 +3890,8 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \t\t\tfolio_put_refs(new_folio, nr_pages);\n \t\t}\n \n-\t\tzone_device_private_split_cb(folio, NULL);\n+\t\tfolio_managed_split_cb(folio, NULL);\n+\n \t\t/*\n \t\t * Unfreeze @folio only after all page cache entries, which\n \t\t * used to point to it, have been updated with new folios.\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about user-driven migration to private nodes, explaining that ZONE_DEVICE always rejects it but private nodes should be able to opt in. They added the NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper to dispatch migration requests, allowing migrate_pages syscall to target private nodes.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "added new functionality",
                "acknowledged reviewer feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services may want to support user-driven migration\n(migrate_pages syscall, mbind) to allow data movement between regular\nand private nodes.\n\nZONE_DEVICE always rejects user migration, but private nodes should\nbe able to opt in.\n\nAdd NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper that\ndispatches migration requests.  Private nodes can either set the flag\nand provide a custom migrate_to callback for driver-managed migration.\n\nIn migrate_to_node(), allows GFP_PRIVATE when the destination node\nsupports NP_OPS_MIGRATION, enabling migrate_pages syscall to target\nprivate nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |   4 ++\n include/linux/migrate.h      |  10 +++\n include/linux/node_private.h | 122 +++++++++++++++++++++++++++++++++++\n mm/damon/paddr.c             |   3 +\n mm/internal.h                |  24 +++++++\n mm/mempolicy.c               |  10 +--\n mm/migrate.c                 |  49 ++++++++++----\n mm/rmap.c                    |   4 +-\n 8 files changed, 206 insertions(+), 20 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 646dc48a23b5..e587f5781135 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -949,6 +949,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \tif (!node_possible(nid))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MIGRATION) &&\n+\t    (!ops->migrate_to || !ops->folio_migrate))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 26ca00c325d9..7b2da3875ff2 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -71,6 +71,9 @@ void folio_migrate_flags(struct folio *newfolio, struct folio *folio);\n int folio_migrate_mapping(struct address_space *mapping,\n \t\tstruct folio *newfolio, struct folio *folio, int extra_count);\n int set_movable_ops(const struct movable_operations *ops, enum pagetype type);\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason);\n \n #else\n \n@@ -96,6 +99,13 @@ static inline int set_movable_ops(const struct movable_operations *ops, enum pag\n {\n \treturn -ENOSYS;\n }\n+static inline int migrate_folios_to_node(struct list_head *folios,\n+\t\t\t\t\t\t  int nid,\n+\t\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t\t  enum migrate_reason reason)\n+{\n+\treturn -ENOSYS;\n+}\n \n #endif /* CONFIG_MIGRATION */\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex f9dd2d25c8a5..0c5be1ee6e60 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -4,6 +4,7 @@\n \n #include <linux/completion.h>\n #include <linux/memremap.h>\n+#include <linux/migrate_mode.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -52,15 +53,40 @@ struct vm_fault;\n  *     or NULL when called for the final (original) folio after all sub-folios\n  *     have been split off.\n  *\n+ * @migrate_to: Migrate folios TO this node.\n+ *\t[refcounted callback]\n+ *\tReturns: 0 on full success, >0 = number of folios that failed to\n+ *\t\t migrate, <0 = error.  Matches migrate_pages() semantics.\n+ *\t\t @nr_succeeded is set to the number of successfully migrated\n+ *\t\t folios (may be NULL if caller doesn't need it).\n+ *\n+ * @folio_migrate: Post-migration notification that a folio on this private node\n+ *    changed physical location (on the same node or a different node).\n+ *    [folio-referenced callback]\n+ *     Called from migrate_folio_move() after data has been copied but before\n+ *     migration entries are replaced with real PTEs.  Both @src and @dst are\n+ *     locked.  Faults block in migration_entry_wait() until\n+ *     remove_migration_ptes() runs, so the service can safely update\n+ *     PFN-based metadata (compression tables, device page tables, DMA\n+ *     mappings, etc.) before any access through the page tables.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n \tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n+\tint (*migrate_to)(struct list_head *folios, int nid,\n+\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t  unsigned int *nr_succeeded);\n+\tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tunsigned long flags;\n };\n \n+/* Allow user/kernel migration; requires migrate_to and folio_migrate */\n+#define NP_OPS_MIGRATION\t\tBIT(0)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\n@@ -177,6 +203,81 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n \t\tnode_private_split_cb(original_folio, new_folio);\n }\n \n+#ifdef CONFIG_MEMORY_HOTPLUG\n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn -ENOENT;\n+\treturn node_private_has_flag(folio_nid(folio), NP_OPS_MIGRATION) ?\n+\t       folio_nid(folio) : -ENOENT;\n+}\n+\n+/**\n+ * folio_managed_allows_migrate - Check if a managed folio supports migration\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio can be migrated.  For zone_device folios, only\n+ * device_private and device_coherent support migration.  For private node\n+ * folios, migration requires NP_OPS_MIGRATION.  Normal folios always\n+ * return true.\n+ */\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\tif (folio_is_private_node(folio))\n+\t\treturn folio_private_flags(folio, NP_OPS_MIGRATION);\n+\treturn true;\n+}\n+\n+/**\n+ * node_private_migrate_to - Attempt service-specific migration to a private node\n+ * @folios: list of folios to migrate (may sleep)\n+ * @nid: target node\n+ * @mode: migration mode (MIGRATE_ASYNC, MIGRATE_SYNC, etc.)\n+ * @reason: migration reason (MR_DEMOTION, MR_SYSCALL, etc.)\n+ * @nr_succeeded: optional output for number of successfully migrated folios\n+ *\n+ * If @nid is an N_MEMORY_PRIVATE node with a migrate_to callback,\n+ * invokes the callback and returns the result with migrate_pages()\n+ * semantics (0 = full success, >0 = failure count, <0 = error).\n+ * Returns -ENODEV if the node is not private or the service is being\n+ * torn down.\n+ *\n+ * The source folios are on other nodes, so they do not pin the target\n+ * node's node_private.  A temporary refcount is taken under rcu_read_lock\n+ * to keep node_private (and the service module) alive across the callback.\n+ */\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\tint (*fn)(struct list_head *, int, enum migrate_mode,\n+\t\t  enum migrate_reason, unsigned int *);\n+\tstruct node_private *np;\n+\tint ret;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (!np || !np->ops || !np->ops->migrate_to ||\n+\t    !refcount_inc_not_zero(&np->refcount)) {\n+\t\trcu_read_unlock();\n+\t\treturn -ENODEV;\n+\t}\n+\tfn = np->ops->migrate_to;\n+\trcu_read_unlock();\n+\n+\tret = fn(folios, nid, mode, reason, nr_succeeded);\n+\n+\tif (refcount_dec_and_test(&np->refcount))\n+\t\tcomplete(&np->released);\n+\n+\treturn ret;\n+}\n+#endif /* CONFIG_MEMORY_HOTPLUG */\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -242,6 +343,27 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\treturn -ENOENT;\n+}\n+\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\treturn true;\n+}\n+\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\treturn -ENODEV;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/damon/paddr.c b/mm/damon/paddr.c\nindex 07a8aead439e..532b8e2c62b0 100644\n--- a/mm/damon/paddr.c\n+++ b/mm/damon/paddr.c\n@@ -277,6 +277,9 @@ static unsigned long damon_pa_migrate(struct damon_region *r,\n \t\telse\n \t\t\t*sz_filter_passed += folio_size(folio) / addr_unit;\n \n+\t\tif (!folio_managed_allows_migrate(folio))\n+\t\t\tgoto put_folio;\n+\n \t\tif (!folio_isolate_lru(folio))\n \t\t\tgoto put_folio;\n \t\tlist_add(&folio->lru, &folio_list);\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 658da41cdb8e..6ab4679fe943 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1442,6 +1442,30 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/**\n+ * folio_managed_migrate_notify - Notify service that a folio changed location\n+ * @src: the old folio (about to be freed)\n+ * @dst: the new folio (data already copied, migration entries still in place)\n+ *\n+ * Called from migrate_folio_move() after data has been copied but before\n+ * remove_migration_ptes() installs real PTEs pointing to @dst.  While\n+ * migration entries are in place, faults block in migration_entry_wait(),\n+ * so the service can safely update PFN-based metadata before any access\n+ * through the page tables.  Both @src and @dst are locked.\n+ */\n+static inline void folio_managed_migrate_notify(struct folio *src,\n+\t\t\t\t\t\tstruct folio *dst)\n+{\n+\tconst struct node_private_ops *ops;\n+\n+\tif (!folio_is_private_node(src))\n+\t\treturn;\n+\n+\tops = folio_node_private_ops(src);\n+\tif (ops && ops->folio_migrate)\n+\t\tops->folio_migrate(src, dst);\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 68a98ba57882..2b0f9762d171 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -111,6 +111,7 @@\n #include <linux/mmu_notifier.h>\n #include <linux/printk.h>\n #include <linux/leafops.h>\n+#include <linux/node_private.h>\n #include <linux/gcd.h>\n \n #include <asm/tlbflush.h>\n@@ -1282,11 +1283,6 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tLIST_HEAD(pagelist);\n \tlong nr_failed;\n \tlong err = 0;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = dest,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n \tnodes_clear(nmask);\n \tnode_set(source, nmask);\n@@ -1311,8 +1307,8 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tmmap_read_unlock(mm);\n \n \tif (!list_empty(&pagelist)) {\n-\t\terr = migrate_pages(&pagelist, alloc_migration_target, NULL,\n-\t\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\t\terr = migrate_folios_to_node(&pagelist, dest, MIGRATE_SYNC,\n+\t\t\t\t\t     MR_SYSCALL);\n \t\tif (err)\n \t\t\tputback_movable_pages(&pagelist);\n \t}\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 5169f9717f60..a54d4af04df3 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -43,6 +43,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/memory-tiers.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1387,6 +1388,8 @@ static int migrate_folio_move(free_folio_t put_new_folio, unsigned long private,\n \tif (old_page_state & PAGE_WAS_MLOCKED)\n \t\tlru_add_drain();\n \n+\tfolio_managed_migrate_notify(src, dst);\n+\n \tif (old_page_state & PAGE_WAS_MAPPED)\n \t\tremove_migration_ptes(src, dst, 0);\n \n@@ -2165,6 +2168,7 @@ int migrate_pages(struct list_head *from, new_folio_t get_new_folio,\n \n \treturn rc_gather;\n }\n+EXPORT_SYMBOL_GPL(migrate_pages);\n \n struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n {\n@@ -2204,6 +2208,31 @@ struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n \n \treturn __folio_alloc(gfp_mask, order, nid, mtc->nmask);\n }\n+EXPORT_SYMBOL_GPL(alloc_migration_target);\n+\n+static int __migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, NULL);\n+}\n+\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason)\n+{\n+\tif (node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_private_migrate_to(folios, nid, mode,\n+\t\t\t\t\t       reason, NULL);\n+\treturn __migrate_folios_to_node(folios, nid, mode, reason);\n+}\n \n #ifdef CONFIG_NUMA\n \n@@ -2221,14 +2250,8 @@ static int store_status(int __user *status, int start, int value, int nr)\n static int do_move_pages_to_node(struct list_head *pagelist, int node)\n {\n \tint err;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = node,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n-\terr = migrate_pages(pagelist, alloc_migration_target, NULL,\n-\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\terr = migrate_folios_to_node(pagelist, node, MIGRATE_SYNC, MR_SYSCALL);\n \tif (err)\n \t\tputback_movable_pages(pagelist);\n \treturn err;\n@@ -2240,7 +2263,7 @@ static int __add_folio_for_migration(struct folio *folio, int node,\n \tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\treturn -EFAULT;\n \n-\tif (folio_is_zone_device(folio))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn -ENOENT;\n \n \tif (folio_nid(folio) == node)\n@@ -2364,7 +2387,8 @@ static int do_pages_move(struct mm_struct *mm, nodemask_t task_nodes,\n \t\terr = -ENODEV;\n \t\tif (node < 0 || node >= MAX_NUMNODES)\n \t\t\tgoto out_flush;\n-\t\tif (!node_state(node, N_MEMORY))\n+\t\tif (!node_state(node, N_MEMORY) &&\n+\t\t    !node_state(node, N_MEMORY_PRIVATE))\n \t\t\tgoto out_flush;\n \n \t\terr = -EACCES;\n@@ -2449,8 +2473,8 @@ static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n \t\tif (folio) {\n \t\t\tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\t\t\terr = -EFAULT;\n-\t\t\telse if (folio_is_zone_device(folio))\n-\t\t\t\terr = -ENOENT;\n+\t\t\telse if (unlikely(folio_is_private_managed(folio)))\n+\t\t\t\terr = folio_managed_allows_user_migrate(folio);\n \t\t\telse\n \t\t\t\terr = folio_nid(folio);\n \t\t\tfolio_walk_end(&fw, vma);\n@@ -2660,6 +2684,9 @@ int migrate_misplaced_folio_prepare(struct folio *folio,\n \tint nr_pages = folio_nr_pages(folio);\n \tpg_data_t *pgdat = NODE_DATA(node);\n \n+\tif (!folio_managed_allows_migrate(folio))\n+\t\treturn -ENOENT;\n+\n \tif (folio_is_file_lru(folio)) {\n \t\t/*\n \t\t * Do not migrate file folios that are mapped in multiple\ndiff --git a/mm/rmap.c b/mm/rmap.c\nindex f955f02d570e..805f9ceb82f3 100644\n--- a/mm/rmap.c\n+++ b/mm/rmap.c\n@@ -72,6 +72,7 @@\n #include <linux/backing-dev.h>\n #include <linux/page_idle.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/userfaultfd_k.h>\n #include <linux/mm_inline.h>\n #include <linux/oom.h>\n@@ -2616,8 +2617,7 @@ void try_to_migrate(struct folio *folio, enum ttu_flags flags)\n \t\t\t\t\tTTU_SYNC | TTU_BATCH_FLUSH)))\n \t\treturn;\n \n-\tif (folio_is_zone_device(folio) &&\n-\t    (!folio_is_device_private(folio) && !folio_is_device_coherent(folio)))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn;\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about allowing userland to directly allocate from private nodes via set_mempolicy() and mbind(), but not wanting those nodes as normal allocable system memory in the fallback lists. The author added a flag NP_OPS_MEMPOLICY requiring NP_OPS_MIGRATION, updated sysfs 'has_memory' attribute, and modified mempolicy migration sites to use __GFP_PRIVATE.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private nodes want userland to directly allocate from the node\nvia set_mempolicy() and mbind() - but don't want that node as normal\nallocable system memory in the fallback lists.\n\nAdd NP_OPS_MEMPOLICY flag requiring NP_OPS_MIGRATION (since mbind can\ndrive migrations).  Only allow private nodes in policy nodemasks if\nall private nodes in the mask support NP_OPS_MEMPOLICY. This prevents\n__GFP_PRIVATE from unlocking nodes without NP_OPS_MEMPOLICY support.\n\nAdd __GFP_PRIVATE to mempolicy migration sites so moves to opted-in\nprivate nodes succeed.\n\nUpdate the sysfs \"has_memory\" attribute to include N_MEMORY_PRIVATE\nnodes with NP_OPS_MEMPOLICY set, allowing existing numactl userland\ntools to work without modification.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c            | 22 +++++++++++++-\n include/linux/node_private.h   | 40 +++++++++++++++++++++++++\n include/uapi/linux/mempolicy.h |  1 +\n mm/mempolicy.c                 | 54 ++++++++++++++++++++++++++++++----\n mm/page_alloc.c                |  5 ++++\n 5 files changed, 116 insertions(+), 6 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex e587f5781135..c08b5a948779 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -953,6 +953,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (!ops->migrate_to || !ops->folio_migrate))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\n@@ -1145,6 +1149,21 @@ static ssize_t show_node_state(struct device *dev,\n \t\t\t  nodemask_pr_args(&node_states[na->state]));\n }\n \n+/* has_memory includes N_MEMORY + N_MEMORY_PRIVATE that support mempolicy. */\n+static ssize_t show_has_memory(struct device *dev,\n+\t\t\t       struct device_attribute *attr, char *buf)\n+{\n+\tnodemask_t mask = node_states[N_MEMORY];\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_set(nid, mask);\n+\t}\n+\n+\treturn sysfs_emit(buf, \"%*pbl\\n\", nodemask_pr_args(&mask));\n+}\n+\n #define _NODE_ATTR(name, state) \\\n \t{ __ATTR(name, 0444, show_node_state, NULL), state }\n \n@@ -1155,7 +1174,8 @@ static struct node_attr node_state_attr[] = {\n #ifdef CONFIG_HIGHMEM\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n-\t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY] = { __ATTR(has_memory, 0444, show_has_memory, NULL),\n+\t\t       N_MEMORY },\n \t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 0c5be1ee6e60..e9b58afa366b 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -86,6 +86,8 @@ struct node_private_ops {\n \n /* Allow user/kernel migration; requires migrate_to and folio_migrate */\n #define NP_OPS_MIGRATION\t\tBIT(0)\n+/* Allow mempolicy-directed allocation and mbind migration to this node */\n+#define NP_OPS_MEMPOLICY\t\tBIT(1)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -276,6 +278,34 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \n \treturn ret;\n }\n+\n+static inline bool node_mpol_eligible(int nid)\n+{\n+\tbool ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_state(nid, N_MEMORY);\n+\n+\trcu_read_lock();\n+\tret = node_private_has_flag(nid, NP_OPS_MEMPOLICY);\n+\trcu_read_unlock();\n+\treturn ret;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\tint nid;\n+\tbool eligible = false;\n+\n+\tfor_each_node_mask(nid, *nodes) {\n+\t\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\t\tcontinue;\n+\t\tif (!node_mpol_eligible(nid))\n+\t\t\treturn false;\n+\t\teligible = true;\n+\t}\n+\treturn eligible;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -364,6 +394,16 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \treturn -ENODEV;\n }\n \n+static inline bool node_mpol_eligible(int nid)\n+{\n+\treturn false;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/include/uapi/linux/mempolicy.h b/include/uapi/linux/mempolicy.h\nindex 8fbbe613611a..b606eae983c8 100644\n--- a/include/uapi/linux/mempolicy.h\n+++ b/include/uapi/linux/mempolicy.h\n@@ -64,6 +64,7 @@ enum {\n #define MPOL_F_SHARED  (1 << 0)\t/* identify shared policies */\n #define MPOL_F_MOF\t(1 << 3) /* this policy wants migrate on fault */\n #define MPOL_F_MORON\t(1 << 4) /* Migrate On protnone Reference On Node */\n+#define MPOL_F_PRIVATE\t(1 << 5) /* policy targets private node; use __GFP_PRIVATE */\n \n /*\n  * Enabling zone reclaim means the page allocator will attempt to fulfill\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 2b0f9762d171..8ac014950e88 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -406,8 +406,6 @@ static int mpol_new_preferred(struct mempolicy *pol, const nodemask_t *nodes)\n static int mpol_set_nodemask(struct mempolicy *pol,\n \t\t     const nodemask_t *nodes, struct nodemask_scratch *nsc)\n {\n-\tint ret;\n-\n \t/*\n \t * Default (pol==NULL) resp. local memory policies are not a\n \t * subject of any remapping. They also do not need any special\n@@ -416,9 +414,12 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \tif (!pol || pol->mode == MPOL_LOCAL)\n \t\treturn 0;\n \n-\t/* Check N_MEMORY */\n+\t/* Check N_MEMORY and N_MEMORY_PRIVATE*/\n \tnodes_and(nsc->mask1,\n \t\t  cpuset_current_mems_allowed, node_states[N_MEMORY]);\n+\tnodes_and(nsc->mask2, cpuset_current_mems_allowed,\n+\t\t  node_states[N_MEMORY_PRIVATE]);\n+\tnodes_or(nsc->mask1, nsc->mask1, nsc->mask2);\n \n \tVM_BUG_ON(!nodes);\n \n@@ -432,8 +433,13 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \telse\n \t\tpol->w.cpuset_mems_allowed = cpuset_current_mems_allowed;\n \n-\tret = mpol_ops[pol->mode].create(pol, &nsc->mask2);\n-\treturn ret;\n+\t/* All private nodes in the mask must have NP_OPS_MEMPOLICY. */\n+\tif (nodes_private_mpol_allowed(&nsc->mask2))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse if (nodes_intersects(nsc->mask2, node_states[N_MEMORY_PRIVATE]))\n+\t\treturn -EINVAL;\n+\n+\treturn mpol_ops[pol->mode].create(pol, &nsc->mask2);\n }\n \n /*\n@@ -500,6 +506,7 @@ static void mpol_rebind_default(struct mempolicy *pol, const nodemask_t *nodes)\n static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n {\n \tnodemask_t tmp;\n+\tint nid;\n \n \tif (pol->flags & MPOL_F_STATIC_NODES)\n \t\tnodes_and(tmp, pol->w.user_nodemask, *nodes);\n@@ -514,6 +521,21 @@ static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n \tif (nodes_empty(tmp))\n \t\ttmp = *nodes;\n \n+\t/*\n+\t * Drop private nodes that don't have mempolicy support.\n+\t * cpusets guarantees at least one N_MEMORY node in effective_mems\n+\t * and mems_allowed, so dropping private nodes here is safe.\n+\t */\n+\tfor_each_node_mask(nid, tmp) {\n+\t\tif (node_state(nid, N_MEMORY_PRIVATE) &&\n+\t\t    !node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_clear(nid, tmp);\n+\t}\n+\tif (nodes_intersects(tmp, node_states[N_MEMORY_PRIVATE]))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse\n+\t\tpol->flags &= ~MPOL_F_PRIVATE;\n+\n \tpol->nodes = tmp;\n }\n \n@@ -661,6 +683,9 @@ static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)\n \t}\n \tif (!queue_folio_required(folio, qp))\n \t\treturn;\n+\tif (folio_is_private_node(folio) &&\n+\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\treturn;\n \tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||\n \t    !vma_migratable(walk->vma) ||\n \t    !migrate_folio_add(folio, qp->pagelist, qp->flags))\n@@ -717,6 +742,9 @@ static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n \t\tif (!folio || folio_is_zone_device(folio))\n \t\t\tcontinue;\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\t\tcontinue;\n \t\tif (folio_test_large(folio) && max_nr != 1)\n \t\t\tnr = folio_pte_batch(folio, pte, ptent, max_nr);\n \t\t/*\n@@ -1451,6 +1479,9 @@ static struct folio *alloc_migration_target_by_mpol(struct folio *src,\n \telse\n \t\tgfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL | __GFP_COMP;\n \n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \treturn folio_alloc_mpol(gfp, order, pol, ilx, nid);\n }\n #else\n@@ -2280,6 +2311,15 @@ static nodemask_t *policy_nodemask(gfp_t gfp, struct mempolicy *pol,\n \t\t\tnodemask = &pol->nodes;\n \t\tif (pol->home_node != NUMA_NO_NODE)\n \t\t\t*nid = pol->home_node;\n+\t\telse if ((pol->flags & MPOL_F_PRIVATE) &&\n+\t\t\t !node_isset(*nid, pol->nodes)) {\n+\t\t\t/*\n+\t\t\t * Private nodes are not in N_MEMORY nodes' zonelists.\n+\t\t\t * When the preferred nid (usually numa_node_id()) can't\n+\t\t\t * reach the policy nodes, start from a policy node.\n+\t\t\t */\n+\t\t\t*nid = first_node(pol->nodes);\n+\t\t}\n \t\t/*\n \t\t * __GFP_THISNODE shouldn't even be used with the bind policy\n \t\t * because we might easily break the expectation to stay on the\n@@ -2533,6 +2573,10 @@ struct folio *vma_alloc_folio_noprof(gfp_t gfp, int order, struct vm_area_struct\n \t\tgfp |= __GFP_NOWARN;\n \n \tpol = get_vma_policy(vma, addr, order, &ilx);\n+\n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \tfolio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n \tmpol_cond_put(pol);\n \treturn folio;\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 5a1b35421d78..ec6c1f8e85d8 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3849,8 +3849,13 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\t * if another process has NUMA bindings and is causing\n \t\t * kswapd wakeups on only some nodes. Avoid accidental\n \t\t * \"node_reclaim_mode\"-like behavior in this case.\n+\t\t *\n+\t\t * Nodes without kswapd (some private nodes) are never\n+\t\t * skipped - this causes some mempolicies to silently\n+\t\t * fall back to DRAM even if the node is eligible.\n \t\t */\n \t\tif (skip_kswapd_nodes &&\n+\t\t    zone->zone_pgdat->kswapd &&\n \t\t    !waitqueue_active(&zone->zone_pgdat->kswapd_wait)) {\n \t\t\tskipped_kswapd_nodes = true;\n \t\t\tcontinue;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private nodes being used as demotion targets in the memory-tiers subsystem, agreeing that they should be added to the demotion target mask and implementing backpressure support to allow vmscan to fall back to swap. The author also acknowledged that the current demotion logic induces LRU inversions and suggested re-doing it to allow less fallback and kick kswapd instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The memory-tier subsystem needs to know which private nodes should\nappear as demotion targets.\n\nAdd NP_OPS_DEMOTION (BIT(2)):\n   Node can be added as a demotion target by memory-tiers.\n\nAdd demotion backpressure support so private nodes can reject\nnew demotions cleanly, allowing vmscan to fall back to swap.\n\nIn the demotion path, try demotion to private nodes invididually,\nthen clear private nodes from the demotion target mask until a\nnon-private node is found, then fall back to the remaining mask.\nThis prevents LRU inversion while still allowing forward progress.\n\nThis is the closest match to the current behavior without making\nprivate nodes inaccessible or preventing forward progress. We\nshould probably completely re-do the demotion logic to allow less\nfallback and kick kswapd instead - right now we induce LRU\ninversions by simply falling back to any node in the demotion list.\n\nAdd memory_tier_refresh_demotion() export for services to trigger\nre-evaluation of demotion targets after changing their flags.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory-tiers.h |  9 +++++++\n include/linux/node_private.h | 22 +++++++++++++++++\n mm/internal.h                |  7 ++++++\n mm/memory-tiers.c            | 46 ++++++++++++++++++++++++++++++++----\n mm/page_alloc.c              | 12 +++++++---\n mm/vmscan.c                  | 30 ++++++++++++++++++++++-\n 6 files changed, 117 insertions(+), 9 deletions(-)\n\ndiff --git a/include/linux/memory-tiers.h b/include/linux/memory-tiers.h\nindex 3e1159f6762c..e1476432e359 100644\n--- a/include/linux/memory-tiers.h\n+++ b/include/linux/memory-tiers.h\n@@ -58,6 +58,7 @@ struct memory_dev_type *mt_get_memory_type(int adist);\n int next_demotion_node(int node);\n void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets);\n bool node_is_toptier(int node);\n+void memory_tier_refresh_demotion(void);\n #else\n static inline int next_demotion_node(int node)\n {\n@@ -73,6 +74,10 @@ static inline bool node_is_toptier(int node)\n {\n \treturn true;\n }\n+\n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n #endif\n \n #else\n@@ -106,6 +111,10 @@ static inline bool node_is_toptier(int node)\n \treturn true;\n }\n \n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n+\n static inline int register_mt_adistance_algorithm(struct notifier_block *nb)\n {\n \treturn 0;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e9b58afa366b..e254e36056cd 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -88,6 +88,8 @@ struct node_private_ops {\n #define NP_OPS_MIGRATION\t\tBIT(0)\n /* Allow mempolicy-directed allocation and mbind migration to this node */\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n+/* Node participates as a demotion target in memory-tiers */\n+#define NP_OPS_DEMOTION\t\t\tBIT(2)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -101,12 +103,14 @@ struct node_private_ops {\n  *\t\tcallbacks that may sleep; 0 = fully released)\n  * @released: Signaled when refcount drops to 0; unregister waits on this\n  * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ * @migration_blocked: Service signals migrations should pause\n  */\n struct node_private {\n \tvoid *owner;\n \trefcount_t refcount;\n \tstruct completion released;\n \tconst struct node_private_ops *ops;\n+\tbool migration_blocked;\n };\n \n #ifdef CONFIG_NUMA\n@@ -306,6 +310,19 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \t}\n \treturn eligible;\n }\n+\n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\tstruct node_private *np;\n+\tbool blocked;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tblocked = np && READ_ONCE(np->migration_blocked);\n+\trcu_read_unlock();\n+\n+\treturn blocked;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -404,6 +421,11 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \treturn false;\n }\n \n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 6ab4679fe943..5950e20d4023 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t  const nodemask_t *candidates);\n extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t      gfp_t gfp_mask);\n #else\n@@ -1220,6 +1222,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t\t const nodemask_t *candidates)\n+{\n+\treturn NUMA_NO_NODE;\n+}\n static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t\t     gfp_t gfp_mask)\n {\ndiff --git a/mm/memory-tiers.c b/mm/memory-tiers.c\nindex 9c742e18e48f..434190fdc078 100644\n--- a/mm/memory-tiers.c\n+++ b/mm/memory-tiers.c\n@@ -3,6 +3,7 @@\n #include <linux/lockdep.h>\n #include <linux/sysfs.h>\n #include <linux/kobject.h>\n+#include <linux/node_private.h>\n #include <linux/memory.h>\n #include <linux/memory-tiers.h>\n #include <linux/notifier.h>\n@@ -380,6 +381,8 @@ static void disable_all_demotion_targets(void)\n \t\tif (memtier)\n \t\t\tmemtier->lower_tier_mask = NODE_MASK_NONE;\n \t}\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE)\n+\t\tnode_demotion[node].preferred = NODE_MASK_NONE;\n \t/*\n \t * Ensure that the \"disable\" is visible across the system.\n \t * Readers will see either a combination of before+disable\n@@ -421,6 +424,7 @@ static void establish_demotion_targets(void)\n \tint target = NUMA_NO_NODE, node;\n \tint distance, best_distance;\n \tnodemask_t tier_nodes, lower_tier;\n+\tnodemask_t all_memory;\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n@@ -429,6 +433,13 @@ static void establish_demotion_targets(void)\n \n \tdisable_all_demotion_targets();\n \n+\t/* Include private nodes that have opted in to demotion. */\n+\tall_memory = node_states[N_MEMORY];\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(node, NP_OPS_DEMOTION))\n+\t\t\tnode_set(node, all_memory);\n+\t}\n+\n \tfor_each_node_state(node, N_MEMORY) {\n \t\tbest_distance = -1;\n \t\tnd = &node_demotion[node];\n@@ -442,12 +453,12 @@ static void establish_demotion_targets(void)\n \t\tmemtier = list_next_entry(memtier, list);\n \t\ttier_nodes = get_memtier_nodemask(memtier);\n \t\t/*\n-\t\t * find_next_best_node, use 'used' nodemask as a skip list.\n+\t\t * find_next_best_node_in, use 'used' nodemask as a skip list.\n \t\t * Add all memory nodes except the selected memory tier\n \t\t * nodelist to skip list so that we find the best node from the\n \t\t * memtier nodelist.\n \t\t */\n-\t\tnodes_andnot(tier_nodes, node_states[N_MEMORY], tier_nodes);\n+\t\tnodes_andnot(tier_nodes, all_memory, tier_nodes);\n \n \t\t/*\n \t\t * Find all the nodes in the memory tier node list of same best distance.\n@@ -455,7 +466,8 @@ static void establish_demotion_targets(void)\n \t\t * in the preferred mask when allocating pages during demotion.\n \t\t */\n \t\tdo {\n-\t\t\ttarget = find_next_best_node(node, &tier_nodes);\n+\t\t\ttarget = find_next_best_node_in(node, &tier_nodes,\n+\t\t\t\t\t\t\t&all_memory);\n \t\t\tif (target == NUMA_NO_NODE)\n \t\t\t\tbreak;\n \n@@ -495,7 +507,7 @@ static void establish_demotion_targets(void)\n \t * allocation to a set of nodes that is closer the above selected\n \t * preferred node.\n \t */\n-\tlower_tier = node_states[N_MEMORY];\n+\tlower_tier = all_memory;\n \tlist_for_each_entry(memtier, &memory_tiers, list) {\n \t\t/*\n \t\t * Keep removing current tier from lower_tier nodes,\n@@ -542,7 +554,7 @@ static struct memory_tier *set_node_memory_tier(int node)\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n-\tif (!node_state(node, N_MEMORY))\n+\tif (!node_state(node, N_MEMORY) && !node_state(node, N_MEMORY_PRIVATE))\n \t\treturn ERR_PTR(-EINVAL);\n \n \tmt_calc_adistance(node, &adist);\n@@ -865,6 +877,30 @@ int mt_calc_adistance(int node, int *adist)\n }\n EXPORT_SYMBOL_GPL(mt_calc_adistance);\n \n+/**\n+ * memory_tier_refresh_demotion() - Re-establish demotion targets\n+ *\n+ * Called by services after registering or unregistering ops->migrate_to on\n+ * a private node, so that establish_demotion_targets() picks up the change.\n+ */\n+void memory_tier_refresh_demotion(void)\n+{\n+\tint nid;\n+\n+\tmutex_lock(&memory_tier_lock);\n+\t/*\n+\t * Ensure private nodes are registered with a tier, otherwise\n+\t * they won't show up in any node's demotion targets nodemask.\n+\t */\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (!__node_get_memory_tier(nid))\n+\t\t\tset_node_memory_tier(nid);\n+\t}\n+\testablish_demotion_targets();\n+\tmutex_unlock(&memory_tier_lock);\n+}\n+EXPORT_SYMBOL_GPL(memory_tier_refresh_demotion);\n+\n static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\t\t\t\t      unsigned long action, void *_arg)\n {\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex ec6c1f8e85d8..e272dfdc6b00 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5589,7 +5589,8 @@ static int node_load[MAX_NUMNODES];\n  *\n  * Return: node id of the found node or %NUMA_NO_NODE if no node is found.\n  */\n-int find_next_best_node(int node, nodemask_t *used_node_mask)\n+int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t   const nodemask_t *candidates)\n {\n \tint n, val;\n \tint min_val = INT_MAX;\n@@ -5599,12 +5600,12 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \t * Use the local node if we haven't already, but for memoryless local\n \t * node, we should skip it and fall back to other nodes.\n \t */\n-\tif (!node_isset(node, *used_node_mask) && node_state(node, N_MEMORY)) {\n+\tif (!node_isset(node, *used_node_mask) && node_isset(node, *candidates)) {\n \t\tnode_set(node, *used_node_mask);\n \t\treturn node;\n \t}\n \n-\tfor_each_node_state(n, N_MEMORY) {\n+\tfor_each_node_mask(n, *candidates) {\n \n \t\t/* Don't want a node to appear more than once */\n \t\tif (node_isset(n, *used_node_mask))\n@@ -5636,6 +5637,11 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \treturn best_node;\n }\n \n+int find_next_best_node(int node, nodemask_t *used_node_mask)\n+{\n+\treturn find_next_best_node_in(node, used_node_mask,\n+\t\t\t\t      &node_states[N_MEMORY]);\n+}\n \n /*\n  * Build zonelists ordered by node and zones within node.\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 6113be4d3519..0f534428ea88 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -355,6 +356,10 @@ static bool can_demote(int nid, struct scan_control *sc,\n \tif (demotion_nid == NUMA_NO_NODE)\n \t\treturn false;\n \n+\t/* Don't demote when the target's service signals backpressure */\n+\tif (node_private_migration_blocked(demotion_nid))\n+\t\treturn false;\n+\n \t/* If demotion node isn't in the cgroup's mems_allowed, fall back */\n \treturn mem_cgroup_node_allowed(memcg, demotion_nid);\n }\n@@ -1022,8 +1027,10 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \t\t\t\t     struct pglist_data *pgdat)\n {\n \tint target_nid = next_demotion_node(pgdat->node_id);\n-\tunsigned int nr_succeeded;\n+\tint first_nid = target_nid;\n+\tunsigned int nr_succeeded = 0;\n \tnodemask_t allowed_mask;\n+\tint ret;\n \n \tstruct migration_target_control mtc = {\n \t\t/*\n@@ -1046,6 +1053,27 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \n \tnode_get_allowed_targets(pgdat, &allowed_mask);\n \n+\t/* Try private node targets until we find non-private node */\n+\twhile (node_state(target_nid, N_MEMORY_PRIVATE)) {\n+\t\tunsigned int nr = 0;\n+\n+\t\tret = node_private_migrate_to(demote_folios, target_nid,\n+\t\t\t\t\t      MIGRATE_ASYNC, MR_DEMOTION,\n+\t\t\t\t\t      &nr);\n+\t\tnr_succeeded += nr;\n+\t\tif (ret == 0 || list_empty(demote_folios))\n+\t\t\treturn nr_succeeded;\n+\n+\t\ttarget_nid = next_node_in(target_nid, allowed_mask);\n+\t\tif (target_nid == first_nid)\n+\t\t\treturn nr_succeeded;\n+\t\tif (!node_state(target_nid, N_MEMORY_PRIVATE))\n+\t\t\tbreak;\n+\t}\n+\n+\t/* target_nid is a non-private node; use standard migration */\n+\tmtc.nid = target_nid;\n+\n \t/* Demotion ignores all cpuset and mempolicy settings */\n \tmigrate_pages(demote_folios, alloc_demote_folio, NULL,\n \t\t      (unsigned long)&mtc, MIGRATE_ASYNC, MR_DEMOTION,\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about write faults on private nodes by adding a new operation flag NP_OPS_PROTECT_WRITE and modifying several functions to prevent PTEs from being upgraded to writable when the node is write-protected.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Services that intercept write faults (e.g., for promotion tracking)\nneed PTEs to stay read-only. This requires preventing mprotect\nfrom silently upgrade the PTE, bypassing the service's handle_fault\ncallback.\n\nAdd NP_OPS_PROTECT_WRITE and folio_managed_wrprotect().\n\nIn change_pte_range() and change_huge_pmd(), suppress PTE write-upgrade\nwhen MM_CP_TRY_CHANGE_WRITABLE is sees the folio is write-protected.\n\nIn handle_pte_fault() and do_huge_pmd_wp_page(), dispatch to the node's\nops->handle_fault callback when set, allowing the service to handle write\nfaults with promotion or other custom logic.\n\nNP_OPS_MEMPOLICY is incompatible with NP_OPS_PROTECT_WRITE to avoid the\nfootgun of binding a writable VMA to a write-protected node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++\n include/linux/node_private.h | 22 ++++++++\n mm/huge_memory.c             | 17 ++++++-\n mm/internal.h                | 99 ++++++++++++++++++++++++++++++++++++\n mm/memory.c                  | 15 ++++++\n mm/migrate.c                 | 14 +----\n mm/mprotect.c                |  4 +-\n 7 files changed, 159 insertions(+), 16 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex c08b5a948779..a4955b9b5b93 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -957,6 +957,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    (ops->flags & NP_OPS_PROTECT_WRITE))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e254e36056cd..27d6e5d84e61 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -70,6 +70,24 @@ struct vm_fault;\n  *     PFN-based metadata (compression tables, device page tables, DMA\n  *     mappings, etc.) before any access through the page tables.\n  *\n+ * @handle_fault: Handle fault on folio on this private node.\n+ *   [folio-referenced callback, PTL held on entry]\n+ *\n+ *   Called from handle_pte_fault() (PTE level) or do_huge_pmd_wp_page()\n+ *   (PMD level) after lock acquisition and entry verification.\n+ *   @folio is the faulting folio, @level indicates the page table level.\n+ *\n+ *   For PGTABLE_LEVEL_PTE: vmf->pte is mapped and vmf->ptl is the\n+ *   PTE lock.  Release via pte_unmap_unlock(vmf->pte, vmf->ptl).\n+ *\n+ *   For PGTABLE_LEVEL_PMD: vmf->pte is NULL and vmf->ptl is the\n+ *   PMD lock.  Release via spin_unlock(vmf->ptl).\n+ *\n+ *   The callback MUST release PTL on ALL paths.\n+ *   The caller will NOT touch the page table entry after this returns.\n+ *\n+ *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -81,6 +99,8 @@ struct node_private_ops {\n \t\t\t\t  enum migrate_reason reason,\n \t\t\t\t  unsigned int *nr_succeeded);\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n+\tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t   enum pgtable_level level);\n \tunsigned long flags;\n };\n \n@@ -90,6 +110,8 @@ struct node_private_ops {\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n /* Node participates as a demotion target in memory-tiers */\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n+/* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n+#define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 2ecae494291a..d9ba6593244d 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -2063,12 +2063,14 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tstruct page *page;\n \tunsigned long haddr = vmf->address & HPAGE_PMD_MASK;\n \tpmd_t orig_pmd = vmf->orig_pmd;\n+\tvm_fault_t ret;\n+\n \n \tvmf->ptl = pmd_lockptr(vma->vm_mm, vmf->pmd);\n \tVM_BUG_ON_VMA(!vma->anon_vma, vma);\n \n \tif (is_huge_zero_pmd(orig_pmd)) {\n-\t\tvm_fault_t ret = do_huge_zero_wp_pmd(vmf);\n+\t\tret = do_huge_zero_wp_pmd(vmf);\n \n \t\tif (!(ret & VM_FAULT_FALLBACK))\n \t\t\treturn ret;\n@@ -2088,6 +2090,13 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tfolio = page_folio(page);\n \tVM_BUG_ON_PAGE(!PageHead(page), page);\n \n+\t/* Private-managed write-protect: let the service handle the fault */\n+\tif (unlikely(folio_is_private_managed(folio))) {\n+\t\tif (folio_managed_handle_fault(folio, vmf,\n+\t\t\t\t\t      PGTABLE_LEVEL_PMD, &ret))\n+\t\t\treturn ret;\n+\t}\n+\n \t/* Early check when only holding the PT lock. */\n \tif (PageAnonExclusive(page))\n \t\tgoto reuse;\n@@ -2633,7 +2642,8 @@ int change_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,\n \n \t/* See change_pte_range(). */\n \tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) && !pmd_write(entry) &&\n-\t    can_change_pmd_writable(vma, addr, entry))\n+\t    can_change_pmd_writable(vma, addr, entry) &&\n+\t    !folio_managed_wrprotect(pmd_folio(entry)))\n \t\tentry = pmd_mkwrite(entry, vma);\n \n \tret = HPAGE_PMD_NR;\n@@ -4943,6 +4953,9 @@ void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n \tif (folio_test_dirty(folio) && softleaf_is_migration_dirty(entry))\n \t\tpmde = pmd_mkdirty(pmde);\n \n+\tif (folio_managed_wrprotect(folio))\n+\t\tpmde = pmd_wrprotect(pmde);\n+\n \tif (folio_is_device_private(folio)) {\n \t\tswp_entry_t entry;\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 5950e20d4023..ae4ff86e8dc6 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -11,6 +11,7 @@\n #include <linux/khugepaged.h>\n #include <linux/mm.h>\n #include <linux/mm_inline.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/pagewalk.h>\n #include <linux/rmap.h>\n@@ -18,6 +19,7 @@\n #include <linux/leafops.h>\n #include <linux/swap_cgroup.h>\n #include <linux/tracepoint-defs.h>\n+#include <linux/node_private.h>\n \n /* Internal core VMA manipulation functions. */\n #include \"vma.h\"\n@@ -1449,6 +1451,103 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/*\n+ * folio_managed_handle_fault - Dispatch fault on managed-memory folio\n+ * @folio: the faulting folio (must not be NULL)\n+ * @vmf: the vm_fault descriptor (PTL held: vmf->ptl locked)\n+ * @level: page table level (PGTABLE_LEVEL_PTE or PGTABLE_LEVEL_PMD)\n+ * @ret: output fault result if handled\n+ *\n+ * Called with PTL held.  If a handle_fault callback exists, it is invoked\n+ * with PTL still held.  The callback is responsible for releasing PTL on\n+ * all paths.\n+ *\n+ * Returns true if the service handled the fault (PTL released by callback,\n+ * caller returns *ret).  Returns false if no handler exists (PTL still held,\n+ * caller continues with normal fault handling).\n+ */\n+static inline bool folio_managed_handle_fault(struct folio *folio,\n+\t\t\t\t\t      struct vm_fault *vmf,\n+\t\t\t\t\t      enum pgtable_level level,\n+\t\t\t\t\t      vm_fault_t *ret)\n+{\n+\t/* Zone device pages use swap entries; handled in do_swap_page */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->handle_fault) {\n+\t\t\t*ret = ops->handle_fault(folio, vmf, level);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n+/**\n+ * folio_managed_wrprotect - Should this folio's mappings stay write-protected?\n+ * @folio: the folio to check\n+ *\n+ * Returns true if the folio is on a private node with NP_OPS_PROTECT_WRITE,\n+ * meaning page table entries (PTE or PMD) should not be made writable.\n+ * Write faults are intercepted by the service's handle_fault callback\n+ * to promote the folio to DRAM.\n+ *\n+ * Used by:\n+ *   - change_pte_range() / change_huge_pmd(): prevent mprotect write-upgrade\n+ *   - remove_migration_pte() / remove_migration_pmd(): strip write after migration\n+ *   - do_huge_pmd_wp_page(): dispatch to fault handler instead of reuse\n+ */\n+static inline bool folio_managed_wrprotect(struct folio *folio)\n+{\n+\treturn unlikely(folio_is_private_node(folio) &&\n+\t\t\tfolio_private_flags(folio, NP_OPS_PROTECT_WRITE));\n+}\n+\n+/**\n+ * folio_managed_fixup_migration_pte - Fixup PTE after migration for\n+ *                                     managed memory pages.\n+ * @new: the destination page\n+ * @pte: the PTE being installed (normal PTE built by caller)\n+ * @old_pte: the original PTE (before migration, for swap entry flags)\n+ * @vma: the VMA\n+ *\n+ * For MEMORY_DEVICE_PRIVATE pages: replaces the PTE with a device-private\n+ * swap entry, preserving soft_dirty and uffd_wp from old_pte.\n+ *\n+ * For N_MEMORY_PRIVATE pages with NP_OPS_PROTECT_WRITE: strips the write\n+ * bit so the next write triggers the fault handler for promotion.\n+ *\n+ * For normal pages: returns pte unmodified.\n+ */\n+static inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n+\t\t\t\t\t\t      pte_t pte,\n+\t\t\t\t\t\t      pte_t old_pte,\n+\t\t\t\t\t\t      struct vm_area_struct *vma)\n+{\n+\tif (unlikely(is_device_private_page(new))) {\n+\t\tsoftleaf_t entry;\n+\n+\t\tif (pte_write(pte))\n+\t\t\tentry = make_writable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\telse\n+\t\t\tentry = make_readable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\tpte = softleaf_to_pte(entry);\n+\t\tif (pte_swp_soft_dirty(old_pte))\n+\t\t\tpte = pte_swp_mksoft_dirty(pte);\n+\t\tif (pte_swp_uffd_wp(old_pte))\n+\t\t\tpte = pte_swp_mkuffd_wp(pte);\n+\t} else if (folio_managed_wrprotect(page_folio(new))) {\n+\t\tpte = pte_wrprotect(pte);\n+\t}\n+\treturn pte;\n+}\n+\n /**\n  * folio_managed_migrate_notify - Notify service that a folio changed location\n  * @src: the old folio (about to be freed)\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 2a55edc48a65..0f78988befef 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -6079,6 +6079,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n \t */\n+\tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n+\t\twritable = false;\n+\t\tignore_writable = true;\n+\t}\n \tif (folio && folio_test_large(folio))\n \t\tnuma_rebuild_large_mapping(vmf, vma, folio, pte, ignore_writable,\n \t\t\t\t\t   pte_write_upgrade);\n@@ -6228,6 +6232,7 @@ static void fix_spurious_fault(struct vm_fault *vmf,\n  */\n static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n {\n+\tstruct folio *folio;\n \tpte_t entry;\n \n \tif (unlikely(pmd_none(*vmf->pmd))) {\n@@ -6284,6 +6289,16 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n \t\tupdate_mmu_tlb(vmf->vma, vmf->address, vmf->pte);\n \t\tgoto unlock;\n \t}\n+\n+\tfolio = vm_normal_folio(vmf->vma, vmf->address, entry);\n+\tif (unlikely(folio && folio_is_private_managed(folio))) {\n+\t\tvm_fault_t fault_ret;\n+\n+\t\tif (folio_managed_handle_fault(folio, vmf, PGTABLE_LEVEL_PTE,\n+\t\t\t\t\t       &fault_ret))\n+\t\t\treturn fault_ret;\n+\t}\n+\n \tif (vmf->flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {\n \t\tif (!pte_write(entry))\n \t\t\treturn do_wp_page(vmf);\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex a54d4af04df3..f632e8b03504 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -398,19 +398,7 @@ static bool remove_migration_pte(struct folio *folio,\n \t\tif (folio_test_anon(folio) && !softleaf_is_migration_read(entry))\n \t\t\trmap_flags |= RMAP_EXCLUSIVE;\n \n-\t\tif (unlikely(is_device_private_page(new))) {\n-\t\t\tif (pte_write(pte))\n-\t\t\t\tentry = make_writable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\telse\n-\t\t\t\tentry = make_readable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\tpte = softleaf_to_pte(entry);\n-\t\t\tif (pte_swp_soft_dirty(old_pte))\n-\t\t\t\tpte = pte_swp_mksoft_dirty(pte);\n-\t\t\tif (pte_swp_uffd_wp(old_pte))\n-\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n-\t\t}\n+\t\tpte = folio_managed_fixup_migration_pte(new, pte, old_pte, vma);\n \n #ifdef CONFIG_HUGETLB_PAGE\n \t\tif (folio_test_hugetlb(folio)) {\ndiff --git a/mm/mprotect.c b/mm/mprotect.c\nindex 283889e4f1ce..830be609bc24 100644\n--- a/mm/mprotect.c\n+++ b/mm/mprotect.c\n@@ -30,6 +30,7 @@\n #include <linux/mm_inline.h>\n #include <linux/pgtable.h>\n #include <linux/userfaultfd_k.h>\n+#include <linux/node_private.h>\n #include <uapi/linux/mman.h>\n #include <asm/cacheflush.h>\n #include <asm/mmu_context.h>\n@@ -290,7 +291,8 @@ static long change_pte_range(struct mmu_gather *tlb,\n \t\t\t * COW or special handling is required.\n \t\t\t */\n \t\t\tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) &&\n-\t\t\t     !pte_write(ptent))\n+\t\t\t     !pte_write(ptent) &&\n+\t\t\t     !(folio && folio_managed_wrprotect(folio)))\n \t\t\t\tset_write_prot_commit_flush_ptes(vma, folio, page,\n \t\t\t\taddr, pte, oldpte, ptent, nr_ptes, tlb);\n \t\t\telse\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the reclaim policy for private nodes in boosted reclaim mode, explaining that it needs to allow swap and writepage operations. They proposed adding a reclaim_policy callback to struct node_private_ops and a struct node_reclaim_policy to configure these policies. The author also added zone_reclaim_allowed() to filter private nodes that have not opted into reclaim.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix is needed",
                "proposed changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services that drive kswapd via watermark_boost need\ncontrol over the reclaim policy.  There are three problems:\n\n1) Boosted reclaim suppresses may_swap and may_writepage.  When\n   demotion is not possible, swap is the only evict path, so kswapd\n   cannot make progress and pages are stranded.\n\n2) __setup_per_zone_wmarks() unconditionally zeros watermark_boost,\n   killing the service's pressure signal.\n\n3) Not all private nodes want reclaim to touch their pages.\n\nAdd a reclaim_policy callback to struct node_private_ops and a\nstruct node_reclaim_policy with:\n\n  - active:             set by the helper when a callback was invoked\n  - may_swap:           allow swap writeback during boosted reclaim\n  - may_writepage:      allow writepage during boosted reclaim\n  - managed_watermarks: service owns watermark_boost lifecycle\n\nWe do not allow disabling swap/writepage, as core MM may have\nexplicitly enabled them on a non-boosted pass.\n\nWe only allow enablign swap/writepage, so that the supression during\na boost can be overridden.  This allows a device to force evictions\neven when the system otherwise would not percieve pressure.\n\nThis is important for a service like compressed RAM, as device capacity\nmay differ from reported capacity, and device may want to relieve real\npressure (poor compression ratio) as opposed to percieved pressure\n(i.e. how many pages are in use).\n\nAdd zone_reclaim_allowed() to filter private nodes that have not\nopted into reclaim.\n\nRegular nodes fall through to cpuset_zone_allowed() unchanged.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 28 ++++++++++++++++++++++++++++\n mm/internal.h                | 36 ++++++++++++++++++++++++++++++++++++\n mm/page_alloc.c              | 11 ++++++++++-\n mm/vmscan.c                  | 25 +++++++++++++++++++++++--\n 4 files changed, 97 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 27d6e5d84e61..34be52383255 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -14,6 +14,24 @@ struct page;\n struct vm_area_struct;\n struct vm_fault;\n \n+/**\n+ * struct node_reclaim_policy - Reclaim policy overrides for private nodes\n+ * @active: set by node_private_reclaim_policy() when a callback was invoked\n+ * @may_swap: allow swap writeback during boosted reclaim\n+ * @may_writepage: allow writepage during boosted reclaim\n+ * @managed_watermarks: service owns watermark_boost lifecycle; kswapd must\n+ *                      not clear it after boosted reclaim\n+ *\n+ * Passed to the reclaim_policy callback so each private node service can\n+ * inject its own reclaim policy before kswapd runs boosted reclaim.\n+ */\n+struct node_reclaim_policy {\n+\tbool active;\n+\tbool may_swap;\n+\tbool may_writepage;\n+\tbool managed_watermarks;\n+};\n+\n /**\n  * struct node_private_ops - Callbacks for private node services\n  *\n@@ -88,6 +106,13 @@ struct vm_fault;\n  *\n  *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n  *\n+ * @reclaim_policy: Configure reclaim policy for boosted reclaim.\n+ *   [called hodling rcu_read_lock, MUST NOT sleep]\n+ *   Called by kswapd before boosted reclaim to let the service override\n+ *   may_swap / may_writepage.  If provided, the service also owns the\n+ *   watermark_boost lifecycle (kswapd will not clear it).\n+ *   If NULL, normal boost policy applies.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -101,6 +126,7 @@ struct node_private_ops {\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n+\tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n \tunsigned long flags;\n };\n \n@@ -112,6 +138,8 @@ struct node_private_ops {\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n /* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n+/* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n+#define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/internal.h b/mm/internal.h\nindex ae4ff86e8dc6..db32cb2d7a29 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1572,6 +1572,42 @@ static inline void folio_managed_migrate_notify(struct folio *src,\n \t\tops->folio_migrate(src, dst);\n }\n \n+/**\n+ * node_private_reclaim_policy - invoke the service's reclaim policy callback\n+ * @nid: NUMA node id\n+ * @policy: reclaim policy struct to fill in\n+ *\n+ * Called by kswapd before boosted reclaim.  Zeroes @policy, then if the\n+ * private node service provides a reclaim_policy callback, invokes it\n+ * and sets policy->active to true.\n+ */\n+#ifdef CONFIG_NUMA\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tstruct node_private *np;\n+\n+\tmemset(policy, 0, sizeof(*policy));\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (np && np->ops && np->ops->reclaim_policy) {\n+\t\tnp->ops->reclaim_policy(nid, policy);\n+\t\tpolicy->active = true;\n+\t}\n+\trcu_read_unlock();\n+}\n+#else\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tmemset(policy, 0, sizeof(*policy));\n+}\n+#endif\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e272dfdc6b00..9692048ab5fb 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -55,6 +55,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/node_private.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -6437,6 +6438,8 @@ static void __setup_per_zone_wmarks(void)\n \tunsigned long lowmem_pages = 0;\n \tstruct zone *zone;\n \tunsigned long flags;\n+\tstruct node_reclaim_policy rp;\n+\tint prev_nid = NUMA_NO_NODE;\n \n \t/* Calculate total number of !ZONE_HIGHMEM and !ZONE_MOVABLE pages */\n \tfor_each_zone(zone) {\n@@ -6446,6 +6449,7 @@ static void __setup_per_zone_wmarks(void)\n \n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n+\t\tint nid = zone_to_nid(zone);\n \n \t\tspin_lock_irqsave(&zone->lock, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n@@ -6482,7 +6486,12 @@ static void __setup_per_zone_wmarks(void)\n \t\t\t    mult_frac(zone_managed_pages(zone),\n \t\t\t\t      watermark_scale_factor, 10000));\n \n-\t\tzone->watermark_boost = 0;\n+\t\tif (nid != prev_nid) {\n+\t\t\tnode_private_reclaim_policy(nid, &rp);\n+\t\t\tprev_nid = nid;\n+\t\t}\n+\t\tif (!rp.managed_watermarks)\n+\t\t\tzone->watermark_boost = 0;\n \t\tzone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_HIGH] = low_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 0f534428ea88..07de666c1276 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -73,6 +73,13 @@\n #define CREATE_TRACE_POINTS\n #include <trace/events/vmscan.h>\n \n+static inline bool zone_reclaim_allowed(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn zone_private_flags(zone, NP_OPS_RECLAIM);\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n struct scan_control {\n \t/* How many pages shrink_list() should reclaim */\n \tunsigned long nr_to_reclaim;\n@@ -6274,7 +6281,7 @@ static void shrink_zones(struct zonelist *zonelist, struct scan_control *sc)\n \t\t * to global LRU.\n \t\t */\n \t\tif (!cgroup_reclaim(sc)) {\n-\t\t\tif (!cpuset_zone_allowed(zone,\n+\t\t\tif (!zone_reclaim_allowed(zone,\n \t\t\t\t\t\t GFP_KERNEL | __GFP_HARDWALL))\n \t\t\t\tcontinue;\n \n@@ -6992,6 +6999,7 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \tunsigned long zone_boosts[MAX_NR_ZONES] = { 0, };\n \tbool boosted;\n \tstruct zone *zone;\n+\tstruct node_reclaim_policy policy;\n \tstruct scan_control sc = {\n \t\t.gfp_mask = GFP_KERNEL,\n \t\t.order = order,\n@@ -7016,6 +7024,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t}\n \tboosted = nr_boost_reclaim;\n \n+\t/* Query/cache private node reclaim policy once per balance() */\n+\tnode_private_reclaim_policy(pgdat->node_id, &policy);\n+\n restart:\n \tset_reclaim_active(pgdat, highest_zoneidx);\n \tsc.priority = DEF_PRIORITY;\n@@ -7083,6 +7094,12 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\tsc.may_writepage = !laptop_mode && !nr_boost_reclaim;\n \t\tsc.may_swap = !nr_boost_reclaim;\n \n+\t\t/* Private nodes may enable swap/writepage when using boost */\n+\t\tif (policy.active) {\n+\t\t\tsc.may_swap |= policy.may_swap;\n+\t\t\tsc.may_writepage |= policy.may_writepage;\n+\t\t}\n+\n \t\t/*\n \t\t * Do some background aging, to give pages a chance to be\n \t\t * referenced before reclaiming. All pages are rotated\n@@ -7176,6 +7193,10 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\t\tif (!zone_boosts[i])\n \t\t\t\tcontinue;\n \n+\t\t\t/* Some private nodes may own the\\ boost lifecycle */\n+\t\t\tif (policy.managed_watermarks)\n+\t\t\t\tcontinue;\n+\n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n \t\t\tspin_lock_irqsave(&zone->lock, flags);\n@@ -7406,7 +7427,7 @@ void wakeup_kswapd(struct zone *zone, gfp_t gfp_flags, int order,\n \tif (!managed_zone(zone))\n \t\treturn;\n \n-\tif (!cpuset_zone_allowed(zone, gfp_flags))\n+\tif (!zone_reclaim_allowed(zone, gfp_flags))\n \t\treturn;\n \n \tpgdat = zone->zone_pgdat;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed the concern that the OOM killer may select an undeserving victim if it doesn't know whether killing a task can actually free memory on a private node. The author introduced NP_OPS_OOM_ELIGIBLE and helpers to check if a private node is OOM-eligible, and updated constrained_alloc() to use these checks.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The OOM killer must know whether killing a task can actually free\nmemory such that pressure is reduced.\n\nA private node only contributes to relieving pressure if it participates\nin both reclaim and demotion. Without this check, the check, the OOM\nkiller may select an undeserving victim.\n\nIntroduce NP_OPS_OOM_ELIGIBLE and helpers node_oom_eligible() and\nzone_oom_eligible().\n\nReplace cpuset_mems_allowed_intersects() in oom_cpuset_eligible()\nwith oom_mems_intersect() that iterates N_MEMORY nodes and skips\nineligible private nodes.\n\nUpdate constrained_alloc() to use zone_oom_eligible() for constraint\ndetection and node_oom_eligible() to exclude ineligible nodes from\ntotalpages accounting.\n\nRemove cpuset_mems_allowed_intersects() as it has no remaining callers.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cpuset.h       |  9 -------\n include/linux/node_private.h |  3 +++\n kernel/cgroup/cpuset.c       | 17 ------------\n mm/oom_kill.c                | 52 ++++++++++++++++++++++++++++++++----\n 4 files changed, 50 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/cpuset.h b/include/linux/cpuset.h\nindex 7b2f3f6b68a9..53ccfb00b277 100644\n--- a/include/linux/cpuset.h\n+++ b/include/linux/cpuset.h\n@@ -97,9 +97,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-extern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t  const struct task_struct *tsk2);\n-\n #ifdef CONFIG_CPUSETS_V1\n #define cpuset_memory_pressure_bump() \t\t\t\t\\\n \tdo {\t\t\t\t\t\t\t\\\n@@ -241,12 +238,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-static inline int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t\t const struct task_struct *tsk2)\n-{\n-\treturn 1;\n-}\n-\n static inline void cpuset_memory_pressure_bump(void) {}\n \n static inline void cpuset_task_status_allowed(struct seq_file *m,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34be52383255..34d862f09e24 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -141,6 +141,9 @@ struct node_private_ops {\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n+/* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n+#define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 1a597f0c7c6c..29789d544fd5 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -4530,23 +4530,6 @@ int cpuset_mem_spread_node(void)\n \treturn cpuset_spread_node(&current->cpuset_mem_spread_rotor);\n }\n \n-/**\n- * cpuset_mems_allowed_intersects - Does @tsk1's mems_allowed intersect @tsk2's?\n- * @tsk1: pointer to task_struct of some task.\n- * @tsk2: pointer to task_struct of some other task.\n- *\n- * Description: Return true if @tsk1's mems_allowed intersects the\n- * mems_allowed of @tsk2.  Used by the OOM killer to determine if\n- * one of the task's memory usage might impact the memory available\n- * to the other.\n- **/\n-\n-int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t   const struct task_struct *tsk2)\n-{\n-\treturn nodes_intersects(tsk1->mems_allowed, tsk2->mems_allowed);\n-}\n-\n /**\n  * cpuset_print_current_mems_allowed - prints current's cpuset and mems_allowed\n  *\ndiff --git a/mm/oom_kill.c b/mm/oom_kill.c\nindex 5eb11fbba704..cd0d65ccd1e8 100644\n--- a/mm/oom_kill.c\n+++ b/mm/oom_kill.c\n@@ -74,7 +74,45 @@ static inline bool is_memcg_oom(struct oom_control *oc)\n \treturn oc->memcg != NULL;\n }\n \n+/* Private nodes are only eligible if they support both reclaim and demotion */\n+static inline bool node_oom_eligible(int nid)\n+{\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn (node_private_flags(nid) & NP_OPS_OOM_ELIGIBLE) ==\n+\t\tNP_OPS_OOM_ELIGIBLE;\n+}\n+\n+static inline bool zone_oom_eligible(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (!node_oom_eligible(zone_to_nid(zone)))\n+\t\treturn false;\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n #ifdef CONFIG_NUMA\n+/*\n+ * Killing a task can only relieve system pressure if freed memory can be\n+ * demoted there and reclaim can operate on the node's pages, so we\n+ * omit private nodes that aren't eligible.\n+ */\n+static bool oom_mems_intersect(const struct task_struct *tsk1,\n+\t\t\t       const struct task_struct *tsk2)\n+{\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (!node_isset(nid, tsk1->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_isset(nid, tsk2->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_oom_eligible(nid))\n+\t\t\tcontinue;\n+\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n /**\n  * oom_cpuset_eligible() - check task eligibility for kill\n  * @start: task struct of which task to consider\n@@ -107,9 +145,10 @@ static bool oom_cpuset_eligible(struct task_struct *start,\n \t\t} else {\n \t\t\t/*\n \t\t\t * This is not a mempolicy constrained oom, so only\n-\t\t\t * check the mems of tsk's cpuset.\n+\t\t\t * check the mems of tsk's cpuset, excluding private\n+\t\t\t * nodes that do not participate in kernel reclaim.\n \t\t\t */\n-\t\t\tret = cpuset_mems_allowed_intersects(current, tsk);\n+\t\t\tret = oom_mems_intersect(current, tsk);\n \t\t}\n \t\tif (ret)\n \t\t\tbreak;\n@@ -291,16 +330,19 @@ static enum oom_constraint constrained_alloc(struct oom_control *oc)\n \t\treturn CONSTRAINT_MEMORY_POLICY;\n \t}\n \n-\t/* Check this allocation failure is caused by cpuset's wall function */\n+\t/* Check this allocation failure is caused by cpuset or private node constraints */\n \tfor_each_zone_zonelist_nodemask(zone, z, oc->zonelist,\n \t\t\thighest_zoneidx, oc->nodemask)\n-\t\tif (!cpuset_zone_allowed(zone, oc->gfp_mask))\n+\t\tif (!zone_oom_eligible(zone, oc->gfp_mask))\n \t\t\tcpuset_limited = true;\n \n \tif (cpuset_limited) {\n \t\toc->totalpages = total_swap_pages;\n-\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed)\n+\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed) {\n+\t\t\tif (!node_oom_eligible(nid))\n+\t\t\t\tcontinue;\n \t\t\toc->totalpages += node_present_pages(nid);\n+\t\t}\n \t\treturn CONSTRAINT_CPUSET;\n \t}\n \treturn CONSTRAINT_NONE;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private nodes engaging in NUMA balancing faults by introducing an opt-in method (NP_OPS_NUMA_BALANCING) and adding a helper function to filter for private nodes that have opted in. The author also added code to enforce write-protection on failed or skipped migrations.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "added new functionality"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Not all private nodes may wish to engage in NUMA balancing faults.\n\nAdd the NP_OPS_NUMA_BALANCING flag (BIT(5)) as an opt-in method.\n\nIntroduce folio_managed_allows_numa() helper:\n   ZONE_DEVICE folios always return false (never NUMA-scanned)\n   NP_OPS_NUMA_BALANCING filters for private nodes\n\nIn do_numa_page(), if a private-node folio with NP_OPS_PROTECT_WRITE\nis still on its node after a failed/skipped migration, enforce\nwrite-protection so the next write triggers handle_fault.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h | 16 ++++++++++++++++\n mm/memory.c                  | 11 +++++++++++\n mm/mempolicy.c               |  5 ++++-\n 4 files changed, 35 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex a4955b9b5b93..88aaac45e814 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -961,6 +961,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (ops->flags & NP_OPS_PROTECT_WRITE))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_NUMA_BALANCING) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34d862f09e24..5ac60db1f044 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -140,6 +140,8 @@ struct node_private_ops {\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n+/* Allow NUMA balancing to scan and migrate folios on this node */\n+#define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n@@ -263,6 +265,15 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n }\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\tif (!folio_is_private_managed(folio))\n+\t\treturn true;\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\treturn folio_private_flags(folio, NP_OPS_NUMA_BALANCING);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \tif (folio_is_zone_device(folio))\n@@ -443,6 +454,11 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\treturn !folio_is_zone_device(folio);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \treturn -ENOENT;\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 0f78988befef..88a581baae40 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -78,6 +78,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/pgalloc.h>\n #include <linux/uaccess.h>\n+#include <linux/node_private.h>\n \n #include <trace/events/kmem.h>\n \n@@ -6041,6 +6042,12 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \tif (!folio || folio_is_zone_device(folio))\n \t\tgoto out_map;\n \n+\t/*\n+\t * We do not need to check private-node folios here because the private\n+\t * memory service either never opted in to NUMA balancing, or it did\n+\t * and we need to restore private PTE controls on the failure path.\n+\t */\n+\n \tnid = folio_nid(folio);\n \tnr_pages = folio_nr_pages(folio);\n \n@@ -6078,6 +6085,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t/*\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n+\t *\n+\t * If the folio is still on a private node with NP_OPS_PROTECT_WRITE,\n+\t * enforce write-protection so the next write triggers handle_fault.\n+\t * This covers migration-failed and migration-skipped paths.\n \t */\n \tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n \t\twritable = false;\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 8ac014950e88..8a3a9916ab59 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -861,7 +861,10 @@ bool folio_can_map_prot_numa(struct folio *folio, struct vm_area_struct *vma,\n {\n \tint nid;\n \n-\tif (!folio || folio_is_zone_device(folio) || folio_test_ksm(folio))\n+\tif (!folio || folio_test_ksm(folio))\n+\t\treturn false;\n+\n+\tif (unlikely(!folio_managed_allows_numa(folio)))\n \t\treturn false;\n \n \t/* Also skip shared copy-on-write folios */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about compaction on private nodes, explaining that it requires migration and services may have PFN-based metadata to update. They added a folio_migrate callback, zone_supports_compaction function, and filtered three direct compaction zone loops. The service is responsible for starting kcompactd on its node.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node zones should not be compacted unless the service explicitly\nopts in - as compaction requires migration and services may have\nPFN-based metadata that needs updating.\n\nAdd a folio_migrate callback which fires from migrate_folio_move() for\neach relocated folio before faults are unblocked.\n\nAdd zone_supports_compaction() which returns true for normal zones and\nchecks NP_OPS_COMPACTION for N_MEMORY_PRIVATE zones.\n\nFilter three direct compaction zone loops:\n  - compaction_zonelist_suitable() (reclaimer eligibility)\n  - try_to_compact_pages()         (direct compaction)\n  - compact_node()                 (proactive/manual compaction)\n\nkcompactd paths are intentionally unfiltered -- the service is\nresponsible for starting kcompactd on its node.\n\nNP_OPS_COMPACTION requires NP_OPS_MIGRATION.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h |  2 ++\n mm/compaction.c              | 26 ++++++++++++++++++++++++++\n 3 files changed, 32 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 88aaac45e814..da523aca18fa 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -965,6 +965,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_COMPACTION) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 5ac60db1f044..fe0336773ddb 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -142,6 +142,8 @@ struct node_private_ops {\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n /* Allow NUMA balancing to scan and migrate folios on this node */\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n+/* Allow compaction to run on the node.  Service must start kcompactd. */\n+#define NP_OPS_COMPACTION\t\tBIT(6)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 6a65145b03d8..d8532b957ec6 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,9 +24,26 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/node_private.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n+\n+/*\n+ * Private node zones require NP_OPS_COMPACTION to opt in.  Normal zones\n+ * always support compaction.\n+ */\n+static inline bool zone_supports_compaction(struct zone *zone)\n+{\n+#ifdef CONFIG_NUMA\n+\tif (!node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn zone_private_flags(zone, NP_OPS_COMPACTION);\n+#else\n+\treturn true;\n+#endif\n+}\n+\n /*\n  * Fragmentation score check interval for proactive compaction purposes.\n  */\n@@ -2443,6 +2460,9 @@ bool compaction_zonelist_suitable(struct alloc_context *ac, int order,\n \t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tunsigned long available;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\t/*\n \t\t * Do not consider all the reclaimable memory because we do not\n \t\t * want to trash just for a single high order allocation which\n@@ -2832,6 +2852,9 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\n \t\t\trc = max_t(enum compact_result, COMPACT_DEFERRED, rc);\n@@ -2906,6 +2929,9 @@ static int compact_node(pg_data_t *pgdat, bool proactive)\n \t\tif (!populated_zone(zone))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (fatal_signal_pending(current))\n \t\t\treturn -EINTR;\n \n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private node folios being longterm-pinnable by default, explaining that this would prevent services from controlling the memory during pinning. They added an NP_OPS_LONGTERM_PIN flag for services to opt-in and modified the folio_is_longterm_pinnable() function in mm.h to check for this flag.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not be longterm-pinnable by default.\nA pinned folio is frozen in place, no migration, compaction, or\nreclaim, so the service loses control for the duration of the pin.\n\nSome services may depend on hot-unplugability and must disallow\nlongterm pinning.  Others (accelerators with shared CPU-device state)\nneed pinning to work.\n\nAdd NP_OPS_LONGTERM_PIN flag for services to opt in with. Hook into\nfolio_is_longterm_pinnable() in mm.h, which all GUP callers\nout-of-line helper, node_private_allows_longterm_pin(),  called\nonly for N_MEMORY_PRIVATE nodes.\n\nWithout the flag: folio_is_longterm_pinnable() returns false, migration\nfails (no __GFP_PRIVATE in GFP mask) and pin_user_pages(FOLL_LONGTERM)\nreturns -ENOMEM.\n\nWith the flag: pin succeeds and the folio stays on the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 15 +++++++++++++++\n include/linux/mm.h           | 22 ++++++++++++++++++++++\n include/linux/node_private.h |  2 ++\n 3 files changed, 39 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex da523aca18fa..5d2487fd54f4 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -866,6 +866,21 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n static DEFINE_MUTEX(node_private_lock);\n static bool node_private_initialized;\n \n+/**\n+ * node_private_allows_longterm_pin - Check if a private node allows longterm pinning\n+ * @nid: Node identifier\n+ *\n+ * Out-of-line helper for folio_is_longterm_pinnable() since mm.h cannot\n+ * include node_private.h (circular dependency).\n+ *\n+ * Returns true if the node has NP_OPS_LONGTERM_PIN set.\n+ */\n+bool node_private_allows_longterm_pin(int nid)\n+{\n+\treturn node_private_has_flag(nid, NP_OPS_LONGTERM_PIN);\n+}\n+EXPORT_SYMBOL_GPL(node_private_allows_longterm_pin);\n+\n /**\n  * node_private_register - Register a private node\n  * @nid: Node identifier\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex fb1819ad42c3..9088fd08aeb9 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -2192,6 +2192,13 @@ static inline bool is_zero_folio(const struct folio *folio)\n \n /* MIGRATE_CMA and ZONE_MOVABLE do not allow pin folios */\n #ifdef CONFIG_MIGRATION\n+\n+#ifdef CONFIG_NUMA\n+bool node_private_allows_longterm_pin(int nid);\n+#else\n+static inline bool node_private_allows_longterm_pin(int nid) { return false; }\n+#endif\n+\n static inline bool folio_is_longterm_pinnable(struct folio *folio)\n {\n #ifdef CONFIG_CMA\n@@ -2215,6 +2222,21 @@ static inline bool folio_is_longterm_pinnable(struct folio *folio)\n \tif (folio_is_fsdax(folio))\n \t\treturn false;\n \n+\t/*\n+\t * Private node folios are not longterm pinnable by default.\n+\t * Services that support pinning opt in via NP_OPS_LONGTERM_PIN.\n+\t * node_private_allows_longterm_pin() is out-of-line because\n+\t * node_private.h includes mm.h (circular dependency).\n+\t *\n+\t * Guarded by CONFIG_NUMA because on !CONFIG_NUMA the single-node\n+\t * node_state() stub returns true for node 0, which would make\n+\t * all folios non-pinnable via the false-returning stub.\n+\t */\n+#ifdef CONFIG_NUMA\n+\tif (node_state(folio_nid(folio), N_MEMORY_PRIVATE))\n+\t\treturn node_private_allows_longterm_pin(folio_nid(folio));\n+#endif\n+\n \t/* Otherwise, non-movable zone folios can be pinned. */\n \treturn !folio_is_zone_movable(folio);\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex fe0336773ddb..7a7438fb9eda 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -144,6 +144,8 @@ struct node_private_ops {\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n /* Allow compaction to run on the node.  Service must start kcompactd. */\n #define NP_OPS_COMPACTION\t\tBIT(6)\n+/* Allow longterm DMA pinning (RDMA, VFIO, etc.) of folios on this node */\n+#define NP_OPS_LONGTERM_PIN\t\tBIT(7)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about notifying private-node services of hardware errors on their nodes by adding a memory_failure callback to struct node_private_ops, which will be called after TestSetPageHWPoison succeeds and before get_hwpoison_page. The kernel always proceeds with standard hwpoison handling for online pages.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "added callback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a void memory_failure notification callback to struct\nnode_private_ops so services managing N_MEMORY_PRIVATE nodes notified\nwhen a page on their node experiences a hardware error.\n\nThe callback is notification only -- the kernel always proceeds with\nstandard hwpoison handling for online pages.\n\nThe notification hook fires after TestSetPageHWPoison succeeds and\nbefore get_hwpoison_page giving the service a chance to clean up.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 16 ++++++++++++++++\n mm/memory-failure.c          | 15 +++++++++++++++\n 3 files changed, 37 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7a7438fb9eda..d2669f68ac20 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -113,6 +113,10 @@ struct node_reclaim_policy {\n  *   watermark_boost lifecycle (kswapd will not clear it).\n  *   If NULL, normal boost policy applies.\n  *\n+ * @memory_failure: Notification of hardware error on a page on this node.\n+ *   [folio-referenced callback]\n+ *   Notification only, kernel always handles the failure.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -127,6 +131,8 @@ struct node_private_ops {\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n \tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n+\tvoid (*memory_failure)(struct folio *folio, unsigned long pfn,\n+\t\t\t       int mf_flags);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex db32cb2d7a29..64467ca774f1 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1608,6 +1608,22 @@ static inline void node_private_reclaim_policy(int nid,\n }\n #endif\n \n+static inline void folio_managed_memory_failure(struct folio *folio,\n+\t\t\t\t\t\tunsigned long pfn,\n+\t\t\t\t\t\tint mf_flags)\n+{\n+\t/* Zone device pages handle memory failure via dev_pagemap_ops */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn;\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->memory_failure)\n+\t\t\tops->memory_failure(folio, pfn, mf_flags);\n+\t}\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/memory-failure.c b/mm/memory-failure.c\nindex c80c2907da33..79c91d44ec1e 100644\n--- a/mm/memory-failure.c\n+++ b/mm/memory-failure.c\n@@ -2379,6 +2379,15 @@ int memory_failure(unsigned long pfn, int flags)\n \t\tgoto unlock_mutex;\n \t}\n \n+\t/*\n+\t * Notify private-node services about the hardware error so they\n+\t * can update internal tracking (e.g., CXL poison lists, stop\n+\t * demoting to failing DIMMs).  This is notification only -- the\n+\t * kernel proceeds with standard hwpoison handling regardless.\n+\t */\n+\tif (unlikely(page_is_private_managed(p)))\n+\t\tfolio_managed_memory_failure(page_folio(p), pfn, flags);\n+\n \t/*\n \t * We need/can do nothing about count=0 pages.\n \t * 1) it's a free page, and therefore in safe hand:\n@@ -2825,6 +2834,12 @@ static int soft_offline_in_use_page(struct page *page)\n \t\treturn 0;\n \t}\n \n+\tif (!folio_managed_allows_migrate(folio)) {\n+\t\tpr_info(\"%#lx: cannot migrate private node folio\\n\", pfn);\n+\t\tfolio_put(folio);\n+\t\treturn -EBUSY;\n+\t}\n+\n \tisolated = isolate_folio_to_list(folio, &pagelist);\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the ordering of registering private regions and hotplugging memory, explaining that their new function combines these two steps to ensure proper ordering. The function first registers the private region, then hotplugs the memory, and on failure, unregisters the private region. They also added checks for migration support and online status when removing the last of memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a new function for drivers to hotplug memory as N_MEMORY_PRIVATE.\n\nThis function combines node_private_region_register() with\n__add_memory_driver_managed() to ensure proper ordering:\n\n1. Register the private region first (sets private node context)\n2. Then hotplug the memory (sets N_MEMORY_PRIVATE)\n3. On failure, unregister the private region to avoid leaving the\n   node in an inconsistent state.\n\nWhen the last of memory is removed, hotplug also removes the private\nnode context. If migration is not supported and the node is still\nonline, fire a warning (likely bug in the driver).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory_hotplug.h |  11 +++\n include/linux/mmzone.h         |  12 ++++\n mm/memory_hotplug.c            | 122 ++++++++++++++++++++++++++++++---\n 3 files changed, 135 insertions(+), 10 deletions(-)\n\ndiff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h\nindex 1f19f08552ea..e5abade9450a 100644\n--- a/include/linux/memory_hotplug.h\n+++ b/include/linux/memory_hotplug.h\n@@ -293,6 +293,7 @@ extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n extern int remove_memory(u64 start, u64 size);\n extern void __remove_memory(u64 start, u64 size);\n extern int offline_and_remove_memory(u64 start, u64 size);\n+extern int offline_and_remove_private_memory(int nid, u64 start, u64 size);\n \n #else\n static inline void try_offline_node(int nid) {}\n@@ -309,6 +310,12 @@ static inline int remove_memory(u64 start, u64 size)\n }\n \n static inline void __remove_memory(u64 start, u64 size) {}\n+\n+static inline int offline_and_remove_private_memory(int nid, u64 start,\n+\t\t\t\t\t\t    u64 size)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n@@ -326,6 +333,10 @@ int __add_memory_driver_managed(int nid, u64 start, u64 size,\n extern int add_memory_driver_managed(int nid, u64 start, u64 size,\n \t\t\t\t     const char *resource_name,\n \t\t\t\t     mhp_t mhp_flags);\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np);\n extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,\n \t\t\t\t   unsigned long nr_pages,\n \t\t\t\t   struct vmem_altmap *altmap, int migratetype,\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 992eb1c5a2c6..cc532b67ad3f 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1524,6 +1524,18 @@ typedef struct pglist_data {\n #endif\n } pg_data_t;\n \n+#ifdef CONFIG_NUMA\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn pgdat->private;\n+}\n+#else\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn false;\n+}\n+#endif\n+\n #define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n #define node_spanned_pages(nid)\t(NODE_DATA(nid)->node_spanned_pages)\n \ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex d2dc527bd5b0..9d72f44a30dc 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1173,8 +1174,7 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tmove_pfn_range_to_zone(zone, pfn, nr_pages, NULL, MIGRATE_MOVABLE,\n \t\t\t       true);\n \n-\tif (!node_state(nid, N_MEMORY)) {\n-\t\t/* Adding memory to the node for the first time */\n+\tif (!node_state(nid, N_MEMORY) && !node_state(nid, N_MEMORY_PRIVATE)) {\n \t\tnode_arg.nid = nid;\n \t\tret = node_notify(NODE_ADDING_FIRST_MEMORY, &node_arg);\n \t\tret = notifier_to_errno(ret);\n@@ -1208,8 +1208,12 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tonline_pages_range(pfn, nr_pages);\n \tadjust_present_page_count(pfn_to_page(pfn), group, nr_pages);\n \n-\tif (node_arg.nid >= 0)\n-\t\tnode_set_state(nid, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (pgdat_is_private(NODE_DATA(nid)))\n+\t\t\tnode_set_state(nid, N_MEMORY_PRIVATE);\n+\t\telse\n+\t\t\tnode_set_state(nid, N_MEMORY);\n+\t}\n \tif (need_zonelists_rebuild)\n \t\tbuild_all_zonelists(NULL);\n \n@@ -1227,8 +1231,14 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t/* reinitialise watermarks and update pcp limits */\n \tinit_per_zone_wmark_min();\n \n-\tkswapd_run(nid);\n-\tkcompactd_run(nid);\n+\t/*\n+\t * Don't start reclaim/compaction daemons for private nodes.\n+\t * Private node services will decide whether to start these services.\n+\t */\n+\tif (!pgdat_is_private(NODE_DATA(nid))) {\n+\t\tkswapd_run(nid);\n+\t\tkcompactd_run(nid);\n+\t}\n \n \tif (node_arg.nid >= 0)\n \t\t/* First memory added successfully. Notify consumers. */\n@@ -1722,6 +1732,54 @@ int add_memory_driver_managed(int nid, u64 start, u64 size,\n }\n EXPORT_SYMBOL_GPL(add_memory_driver_managed);\n \n+/**\n+ * add_private_memory_driver_managed - add driver-managed N_MEMORY_PRIVATE memory\n+ * @nid: NUMA node ID (or memory group ID when MHP_NID_IS_MGID is set)\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ * @resource_name: \"System RAM ($DRIVER)\" format\n+ * @mhp_flags: Memory hotplug flags\n+ * @online_type: MMOP_* online type\n+ * @np: Driver-owned node_private structure (owner, refcount)\n+ *\n+ * Registers node_private first, then hotplugs the memory.\n+ *\n+ * On failure, unregisters the node_private.\n+ */\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np)\n+{\n+\tstruct memory_group *group;\n+\tint real_nid = nid;\n+\tint rc;\n+\n+\tif (!np)\n+\t\treturn -EINVAL;\n+\n+\tif (mhp_flags & MHP_NID_IS_MGID) {\n+\t\tgroup = memory_group_find_by_id(nid);\n+\t\tif (!group)\n+\t\t\treturn -EINVAL;\n+\t\treal_nid = group->nid;\n+\t}\n+\n+\trc = node_private_register(real_nid, np);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\trc = __add_memory_driver_managed(nid, start, size, resource_name,\n+\t\t\t\t\t mhp_flags, online_type);\n+\tif (rc) {\n+\t\tnode_private_unregister(real_nid);\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(add_private_memory_driver_managed);\n+\n /*\n  * Platforms should define arch_get_mappable_range() that provides\n  * maximum possible addressable physical memory range for which the\n@@ -1872,6 +1930,15 @@ static void do_migrate_range(unsigned long start_pfn, unsigned long end_pfn)\n \t\t\tgoto put_folio;\n \t\t}\n \n+\t\t/* Private nodes w/o migration must ensure folios are offline */\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION)) {\n+\t\t\tWARN_ONCE(1, \"hot-unplug on non-migratable node %d pfn %lx\\n\",\n+\t\t\t\t  folio_nid(folio), pfn);\n+\t\t\tpfn = folio_pfn(folio) + folio_nr_pages(folio) - 1;\n+\t\t\tgoto put_folio;\n+\t\t}\n+\n \t\tif (!isolate_folio_to_list(folio, &source)) {\n \t\t\tif (__ratelimit(&migrate_rs)) {\n \t\t\t\tpr_warn(\"failed to isolate pfn %lx\\n\",\n@@ -2014,8 +2081,8 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \n \t/*\n \t * Check whether the node will have no present pages after we offline\n-\t * 'nr_pages' more. If so, we know that the node will become empty, and\n-\t * so we will clear N_MEMORY for it.\n+\t * 'nr_pages' more. If so, send pre-notification for last memory removal.\n+\t * We will clear N_MEMORY(_PRIVATE) if this is the case.\n \t */\n \tif (nr_pages >= pgdat->node_present_pages) {\n \t\tnode_arg.nid = node;\n@@ -2108,8 +2175,12 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * Make sure to mark the node as memory-less before rebuilding the zone\n \t * list. Otherwise this node would still appear in the fallback lists.\n \t */\n-\tif (node_arg.nid >= 0)\n-\t\tnode_clear_state(node, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (node_state(node, N_MEMORY))\n+\t\t\tnode_clear_state(node, N_MEMORY);\n+\t\telse if (node_state(node, N_MEMORY_PRIVATE))\n+\t\t\tnode_clear_state(node, N_MEMORY_PRIVATE);\n+\t}\n \tif (!populated_zone(zone)) {\n \t\tzone_pcp_reset(zone);\n \t\tbuild_all_zonelists(NULL);\n@@ -2461,4 +2532,35 @@ int offline_and_remove_memory(u64 start, u64 size)\n \treturn rc;\n }\n EXPORT_SYMBOL_GPL(offline_and_remove_memory);\n+\n+/**\n+ * offline_and_remove_private_memory - offline, remove, and unregister private memory\n+ * @nid: NUMA node ID of the private memory\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ *\n+ * Counterpart to add_private_memory_driver_managed().  Offlines and removes\n+ * the memory range, then attempts to unregister the node_private.\n+ *\n+ * offline_and_remove_memory() clears N_MEMORY_PRIVATE when the last block\n+ * is offlined, which allows node_private_unregister() to clear the\n+ * pgdat->node_private pointer.  If other private memory ranges remain on\n+ * the node, node_private_unregister() returns -EBUSY (N_MEMORY_PRIVATE\n+ * is still set) and the node_private remains registered.\n+ *\n+ * Return: 0 on full success (memory removed and node_private unregistered),\n+ *         -EBUSY if memory was removed but node still has other private memory,\n+ *         other negative error code if offline/remove failed.\n+ */\n+int offline_and_remove_private_memory(int nid, u64 start, u64 size)\n+{\n+\tint rc;\n+\n+\trc = offline_and_remove_memory(start, size);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\treturn node_private_unregister(nid);\n+}\n+EXPORT_SYMBOL_GPL(offline_and_remove_private_memory);\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the lack of locking in the swapoff path, acknowledged that the per-vswap spinlock needs to be dropped before calling try_to_unmap(), and agreed to restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CRAM (Compressed RAM) subsystem that manages folios demoted\nto N_MEMORY_PRIVATE nodes via the standard kernel LRU.\n\nWe limit entry into CRAM by demotion in to provide devices a way for\ndrivers to close access - which allows the system to stabiliz under\nmemory pressure (the device can run out of real memory when compression\nratios drop too far).\n\nWe utilize write-protect to prevent unbounded writes to compressed\nmemory pages, which may cause run-away compression ratio loss without\na reliable way to prevent the degenerate case (cascading poisons).\n\nCRAM provides the bridge between the mm/ private node infrastructure\nand compressed memory hardware.  Folios are aged by kswapd on the\nprivate node and reclaimed to swap when the device signals pressure.\n\nWrite faults trigger promotion back to regular DRAM via the\nops->handle_fault callback.\n\nDevice pressure is communicated via watermark_boost on the private\nnode's zone.\n\nCRAM registers node_private_ops with:\n  - handle_fault:   promotes folio back to DRAM on write\n  - migrate_to:     custom demotion to the CRAM node\n  - folio_migrate:  (no-op)\n  - free_folio:     zeroes pages on free to scrub stale data\n  - reclaim_policy: provides mayswap/writeback/boost overrides\n  - flags: NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n\t   NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE\n           NP_OPS_RECLAIM\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cram.h |  66 ++++++\n mm/Kconfig           |  10 +\n mm/Makefile          |   1 +\n mm/cram.c            | 508 +++++++++++++++++++++++++++++++++++++++++++\n 4 files changed, 585 insertions(+)\n create mode 100644 include/linux/cram.h\n create mode 100644 mm/cram.c\n\ndiff --git a/include/linux/cram.h b/include/linux/cram.h\nnew file mode 100644\nindex 000000000000..a3c10362fd4f\n--- /dev/null\n+++ b/include/linux/cram.h\n@@ -0,0 +1,66 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_CRAM_H\n+#define _LINUX_CRAM_H\n+\n+#include <linux/mm_types.h>\n+\n+struct folio;\n+struct list_head;\n+struct vm_fault;\n+\n+#define CRAM_PRESSURE_MAX\t1000\n+\n+/**\n+ * cram_flush_cb_t - Driver callback invoked when a folio on a private node\n+ *                   is freed (refcount reaches zero).\n+ * @folio: the folio being freed\n+ * @private: opaque driver data passed at registration\n+ *\n+ * Return:\n+ *   0: Flush resolved -- page should return to buddy allocator (e.g., flush\n+ *      record bit was set, meaning this free is from our own flush resolution)\n+ *   1: Page deferred -- driver took a reference, page will be flushed later.\n+ *      Do NOT return to buddy allocator.\n+ *   2: Buffer full -- caller should zero the page and return to buddy.\n+ */\n+typedef int (*cram_flush_cb_t)(struct folio *folio, void *private);\n+\n+#ifdef CONFIG_CRAM\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data);\n+int cram_unregister_private_node(int nid);\n+int cram_unpurge(int nid);\n+void cram_set_pressure(int nid, unsigned int pressure);\n+void cram_clear_pressure(int nid);\n+\n+#else /* !CONFIG_CRAM */\n+\n+static inline int cram_register_private_node(int nid, void *owner,\n+\t\t\t\t\t     cram_flush_cb_t flush_cb,\n+\t\t\t\t\t     void *flush_data)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unregister_private_node(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unpurge(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+}\n+\n+static inline void cram_clear_pressure(int nid)\n+{\n+}\n+\n+#endif /* CONFIG_CRAM */\n+\n+#endif /* _LINUX_CRAM_H */\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex bd0ea5454af8..054462b954d8 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -662,6 +662,16 @@ config MIGRATION\n config DEVICE_MIGRATION\n \tdef_bool MIGRATION && ZONE_DEVICE\n \n+config CRAM\n+\tbool \"Compressed RAM - private node memory management\"\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\tdepends on MEMORY_HOTPLUG\n+\thelp\n+\t  Enables management of N_MEMORY_PRIVATE nodes for compressed RAM\n+\t  and similar use cases. Provides demotion, promotion, and lifecycle\n+\t  management for private memory nodes.\n+\n config ARCH_ENABLE_HUGEPAGE_MIGRATION\n \tbool\n \ndiff --git a/mm/Makefile b/mm/Makefile\nindex 2d0570a16e5b..0e1421512643 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -98,6 +98,7 @@ obj-$(CONFIG_MEMTEST)\t\t+= memtest.o\n obj-$(CONFIG_MIGRATION) += migrate.o\n obj-$(CONFIG_NUMA) += memory-tiers.o\n obj-$(CONFIG_DEVICE_MIGRATION) += migrate_device.o\n+obj-$(CONFIG_CRAM) += cram.o\n obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o\n obj-$(CONFIG_PAGE_COUNTER) += page_counter.o\n obj-$(CONFIG_LIVEUPDATE) += memfd_luo.o\ndiff --git a/mm/cram.c b/mm/cram.c\nnew file mode 100644\nindex 000000000000..6709e61f5b9d\n--- /dev/null\n+++ b/mm/cram.c\n@@ -0,0 +1,508 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * mm/cram.c - Compressed RAM / private node memory management\n+ *\n+ * Copyright 2026 Meta Technologies Inc.\n+ *   Author: Gregory Price <gourry@gourry.net>\n+ *\n+ * Manages folios demoted to N_MEMORY_PRIVATE nodes via the standard kernel\n+ * LRU.  Folios are aged by kswapd on the private node and reclaimed to swap\n+ * (demotion is suppressed for private nodes).  Write faults trigger promotion\n+ * back to regular DRAM via the ops->handle_fault callback.\n+ *\n+ * All reclaim/demotion uses the standard vmscan infrastructure. Device pressure\n+ * is communicated via watermark_boost on the private node's zone.\n+ */\n+\n+#include <linux/atomic.h>\n+#include <linux/cpuset.h>\n+#include <linux/cram.h>\n+#include <linux/errno.h>\n+#include <linux/gfp.h>\n+#include <linux/jiffies.h>\n+#include <linux/highmem.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/list.h>\n+#include <linux/migrate.h>\n+#include <linux/mm.h>\n+#include <linux/huge_mm.h>\n+#include <linux/mmzone.h>\n+#include <linux/mutex.h>\n+#include <linux/nodemask.h>\n+#include <linux/node_private.h>\n+#include <linux/pagemap.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+#include <linux/swap.h>\n+\n+#include \"internal.h\"\n+\n+struct cram_node {\n+\tvoid\t\t*owner;\n+\tbool\t\tpurged;\t\t/* node is being torn down */\n+\tunsigned int\tpressure;\n+\trefcount_t\trefcount;\n+\tcram_flush_cb_t\tflush_cb;\t/* optional driver flush callback */\n+\tvoid\t\t*flush_data;\t/* opaque data for flush_cb */\n+};\n+\n+static struct cram_node *cram_nodes[MAX_NUMNODES];\n+static DEFINE_MUTEX(cram_mutex);\n+\n+static inline bool cram_valid_nid(int nid)\n+{\n+\treturn nid >= 0 && nid < MAX_NUMNODES;\n+}\n+\n+static inline struct cram_node *get_cram_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn NULL;\n+\n+\trcu_read_lock();\n+\tcn = rcu_dereference(cram_nodes[nid]);\n+\tif (cn && !refcount_inc_not_zero(&cn->refcount))\n+\t\tcn = NULL;\n+\trcu_read_unlock();\n+\n+\treturn cn;\n+}\n+\n+static inline void put_cram_node(struct cram_node *cn)\n+{\n+\tif (cn)\n+\t\trefcount_dec(&cn->refcount);\n+}\n+\n+static void cram_zero_folio(struct folio *folio)\n+{\n+\tunsigned int i, nr = folio_nr_pages(folio);\n+\n+\tif (want_init_on_free())\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr; i++)\n+\t\tclear_highpage(folio_page(folio, i));\n+}\n+\n+static bool cram_free_folio_cb(struct folio *folio)\n+{\n+\tint nid = folio_nid(folio);\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\tgoto zero_and_free;\n+\n+\tif (!cn->flush_cb)\n+\t\tgoto zero_and_free_put;\n+\n+\tret = cn->flush_cb(folio, cn->flush_data);\n+\tput_cram_node(cn);\n+\n+\tswitch (ret) {\n+\tcase 0:\n+\t\t/* Flush resolved: return to buddy (already zeroed by device) */\n+\t\treturn false;\n+\tcase 1:\n+\t\t/* Deferred: driver holds a ref, do not free to buddy */\n+\t\treturn true;\n+\tcase 2:\n+\tdefault:\n+\t\t/* Buffer full or unknown: zero locally, return to buddy */\n+\t\tgoto zero_and_free;\n+\t}\n+\n+zero_and_free_put:\n+\tput_cram_node(cn);\n+zero_and_free:\n+\tcram_zero_folio(folio);\n+\treturn false;\n+}\n+\n+static struct folio *alloc_cram_folio(struct folio *src, unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_PRIVATE | __GFP_KSWAPD_RECLAIM |\n+\t\t     __GFP_HIGHMEM | __GFP_MOVABLE |\n+\t\t     __GFP_NOWARN | __GFP_NORETRY;\n+\n+\t/* Stop allocating if backpressure fired mid-batch */\n+\tif (node_private_migration_blocked(nid))\n+\t\treturn NULL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc_node(gfp, order, nid);\n+}\n+\n+static void cram_put_new_folio(struct folio *folio, unsigned long private)\n+{\n+\tcram_zero_folio(folio);\n+\tfolio_put(folio);\n+}\n+\n+/*\n+ * Allocate a DRAM folio for promotion out of a private node.\n+ *\n+ * Unlike alloc_migration_target(), this does NOT strip __GFP_RECLAIM for\n+ * large folios, the generic helper does that because THP allocations are\n+ * opportunistic, but promotion from a private node is mandatory: the page\n+ * MUST move to DRAM or the process cannot make forward progress.\n+ *\n+ * __GFP_RETRY_MAYFAIL tells the allocator to try hard (multiple reclaim\n+ * rounds, wait for writeback) before giving up.\n+ */\n+static struct folio *alloc_cram_promote_folio(struct folio *src,\n+\t\t\t\t\t      unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc(gfp, order, nid, NULL);\n+}\n+\n+static int cram_migrate_to(struct list_head *demote_folios, int to_nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason,\n+\t\t\t   unsigned int *nr_succeeded)\n+{\n+\tstruct cram_node *cn;\n+\tunsigned int nr_success = 0;\n+\tint ret = 0;\n+\n+\tcn = get_cram_node(to_nid);\n+\tif (!cn)\n+\t\treturn -ENODEV;\n+\n+\tif (cn->purged) {\n+\t\tret = -ENODEV;\n+\t\tgoto out;\n+\t}\n+\n+\t/* Block new demotions at maximum pressure */\n+\tif (READ_ONCE(cn->pressure) >= CRAM_PRESSURE_MAX) {\n+\t\tret = -ENOSPC;\n+\t\tgoto out;\n+\t}\n+\n+\tret = migrate_pages(demote_folios, alloc_cram_folio, cram_put_new_folio,\n+\t\t\t    (unsigned long)to_nid, mode, reason,\n+\t\t\t    &nr_success);\n+\n+\t/*\n+\t * migrate_folio_move() calls folio_add_lru() for each migrated\n+\t * folio, but that only adds the folio to a per-CPU batch, \n+\t * PG_lru is not set until the batch is drained.  Drain now so\n+\t * that cram_fault() can isolate these folios immediately.\n+\t *\n+\t * Use lru_add_drain_all() because migrate_pages() may process\n+\t * folios across CPUs, and the local drain might miss batches\n+\t * filled on other CPUs.\n+\t */\n+\tif (nr_success)\n+\t\tlru_add_drain_all();\n+out:\n+\tput_cram_node(cn);\n+\tif (nr_succeeded)\n+\t\t*nr_succeeded = nr_success;\n+\treturn ret;\n+}\n+\n+static void cram_release_ptl(struct vm_fault *vmf, enum pgtable_level level)\n+{\n+\tif (level == PGTABLE_LEVEL_PTE)\n+\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n+\telse\n+\t\tspin_unlock(vmf->ptl);\n+}\n+\n+static vm_fault_t cram_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t     enum pgtable_level level)\n+{\n+\tstruct folio *f, *f2;\n+\tstruct cram_node *cn;\n+\tunsigned int nr_succeeded = 0;\n+\tint nid;\n+\tLIST_HEAD(folios);\n+\n+\tnid = folio_nid(folio);\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn) {\n+\t\tcram_release_ptl(vmf, level);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Isolate from LRU while holding PTL.  This serializes against\n+\t * other CPUs faulting on the same folio: only one CPU can clear\n+\t * PG_lru under the PTL, and it proceeds to migration.  Other\n+\t * CPUs find the folio already isolated and bail out, preventing\n+\t * the refcount pile-up that causes migrate_pages() to fail with\n+\t * -EAGAIN.\n+\t *\n+\t * No explicit folio_get() is needed: the page table entry holds\n+\t * a reference (we still hold PTL), and folio_isolate_lru() takes\n+\t * its own reference.  This matches do_numa_page()'s pattern.\n+\t *\n+\t * PG_lru should already be set: cram_migrate_to() drains per-CPU\n+\t * LRU batches after migration, and the failure path below\n+\t * drains after putback.\n+\t */\n+\tif (!folio_isolate_lru(folio)) {\n+\t\tput_cram_node(cn);\n+\t\tcram_release_ptl(vmf, level);\n+\t\tcond_resched();\n+\t\treturn 0;\n+\t}\n+\n+\t/* Folio isolated, release PTL, proceed to migration */\n+\tcram_release_ptl(vmf, level);\n+\n+\tnode_stat_mod_folio(folio,\n+\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(folio),\n+\t\t\t    folio_nr_pages(folio));\n+\tlist_add(&folio->lru, &folios);\n+\n+\tmigrate_pages(&folios, alloc_cram_promote_folio, NULL,\n+\t\t      (unsigned long)numa_node_id(),\n+\t\t      MIGRATE_SYNC, MR_NUMA_MISPLACED, &nr_succeeded);\n+\n+\t/* Put failed folios back on LRU; retry on next fault */\n+\tlist_for_each_entry_safe(f, f2, &folios, lru) {\n+\t\tlist_del(&f->lru);\n+\t\tnode_stat_mod_folio(f,\n+\t\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(f),\n+\t\t\t\t    -folio_nr_pages(f));\n+\t\tfolio_putback_lru(f);\n+\t}\n+\n+\t/*\n+\t * If migration failed, folio_putback_lru() batched the folio\n+\t * into this CPU's per-CPU LRU cache (PG_lru not yet set).\n+\t * Drain now so the folio is immediately visible on the LRU,\n+\t * the next fault can then isolate it without an IPI storm\n+\t * via lru_add_drain_all().\n+\t *\n+\t * Return VM_FAULT_RETRY after releasing the fault lock so the\n+\t * arch handler retries from scratch.  Without this, returning 0\n+\t * causes a tight livelock: the process immediately re-faults on\n+\t * the same write-protected entry, alloc fails again, and\n+\t * VM_FAULT_OOM eventually leaks out through a stale path.\n+\t * VM_FAULT_RETRY gives the system breathing room to reclaim.\n+\t */\n+\tif (!nr_succeeded) {\n+\t\tlru_add_drain();\n+\t\tcond_resched();\n+\t\tput_cram_node(cn);\n+\t\trelease_fault_lock(vmf);\n+\t\treturn VM_FAULT_RETRY;\n+\t}\n+\n+\tcond_resched();\n+\tput_cram_node(cn);\n+\treturn 0;\n+}\n+\n+static void cram_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static void cram_reclaim_policy(int nid, struct node_reclaim_policy *policy)\n+{\n+\tpolicy->may_swap = true;\n+\tpolicy->may_writepage = true;\n+\tpolicy->managed_watermarks = true;\n+}\n+\n+static vm_fault_t cram_handle_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t    enum pgtable_level level)\n+{\n+\treturn cram_fault(folio, vmf, level);\n+}\n+\n+static const struct node_private_ops cram_ops = {\n+\t.handle_fault\t\t= cram_handle_fault,\n+\t.migrate_to\t\t= cram_migrate_to,\n+\t.folio_migrate\t\t= cram_folio_migrate,\n+\t.free_folio\t\t= cram_free_folio_cb,\n+\t.reclaim_policy\t\t= cram_reclaim_policy,\n+\t.flags\t\t\t= NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n+\t\t\t\t  NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE |\n+\t\t\t\t  NP_OPS_RECLAIM,\n+};\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data)\n+{\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (cn) {\n+\t\tif (cn->owner != owner) {\n+\t\t\tmutex_unlock(&cram_mutex);\n+\t\t\treturn -EBUSY;\n+\t\t}\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn 0;\n+\t}\n+\n+\tcn = kzalloc(sizeof(*cn), GFP_KERNEL);\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENOMEM;\n+\t}\n+\n+\tcn->owner = owner;\n+\tcn->pressure = 0;\n+\tcn->flush_cb = flush_cb;\n+\tcn->flush_data = flush_data;\n+\trefcount_set(&cn->refcount, 1);\n+\n+\tret = node_private_set_ops(nid, &cram_ops);\n+\tif (ret) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\tkfree(cn);\n+\t\treturn ret;\n+\t}\n+\n+\trcu_assign_pointer(cram_nodes[nid], cn);\n+\n+\t/* Start kswapd on the private node for LRU aging and reclaim */\n+\tkswapd_run(nid);\n+\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* Now that ops->migrate_to is set, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_register_private_node);\n+\n+int cram_unregister_private_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tkswapd_stop(nid);\n+\n+\tWARN_ON(node_private_clear_ops(nid, &cram_ops));\n+\trcu_assign_pointer(cram_nodes[nid], NULL);\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* ops->migrate_to cleared, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\n+\tsynchronize_rcu();\n+\twhile (!refcount_dec_if_one(&cn->refcount))\n+\t\tcond_resched();\n+\tkfree(cn);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unregister_private_node);\n+\n+int cram_unpurge(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tcn->purged = false;\n+\n+\tmutex_unlock(&cram_mutex);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unpurge);\n+\n+void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+\tstruct cram_node *cn;\n+\tstruct node_private *np;\n+\tstruct zone *zone;\n+\tunsigned long managed, boost;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\treturn;\n+\n+\tif (pressure > CRAM_PRESSURE_MAX)\n+\t\tpressure = CRAM_PRESSURE_MAX;\n+\n+\tWRITE_ONCE(cn->pressure, pressure);\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\t/* Block demotions only at maximum pressure */\n+\tif (np)\n+\t\tWRITE_ONCE(np->migration_blocked,\n+\t\t\t   pressure >= CRAM_PRESSURE_MAX);\n+\trcu_read_unlock();\n+\n+\tzone = NULL;\n+\tfor (int i = 0; i < MAX_NR_ZONES; i++) {\n+\t\tstruct zone *z = &NODE_DATA(nid)->node_zones[i];\n+\n+\t\tif (zone_managed_pages(z) > 0) {\n+\t\t\tzone = z;\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+\tif (!zone) {\n+\t\tput_cram_node(cn);\n+\t\treturn;\n+\t}\n+\tmanaged = zone_managed_pages(zone);\n+\n+\t/* Boost proportional to pressure. 0:no boost, 1000:full managed */\n+\tboost = (managed * (unsigned long)pressure) / CRAM_PRESSURE_MAX;\n+\tWRITE_ONCE(zone->watermark_boost, boost);\n+\n+\tif (boost) {\n+\t\tset_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);\n+\t\twakeup_kswapd(zone, GFP_KERNEL, 0, ZONE_MOVABLE);\n+\t}\n+\n+\tput_cram_node(cn);\n+}\n+EXPORT_SYMBOL_GPL(cram_set_pressure);\n+\n+void cram_clear_pressure(int nid)\n+{\n+\tcram_set_pressure(nid, 0);\n+}\n+EXPORT_SYMBOL_GPL(cram_clear_pressure);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the need for a sysram region to directly perform memory hotplug operations, which would eliminate the intermediate dax_region/dax device layer. The author agrees that this feature is necessary and explains how it will work, including its key features such as supporting memory tier integration and automatically hotplugging memory on probe if online type is configured.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed to implement a new feature",
                "explained the benefits of the feature"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CXL sysram region for direct memory hotplug of CXL RAM regions.\n\nThis region eliminates the intermediate dax_region/dax device layer by\ndirectly performing memory hotplug operations.\n\nKey features:\n- Supports memory tier integration for proper NUMA placement\n- Uses the CXL_SYSRAM_ONLINE_* Kconfig options for default online type\n- Automatically hotplugs memory on probe if online type is configured\n- Will be extended to support private memory nodes in the future\n\nThe driver registers a sysram_regionN device as a child of the CXL\nregion, managing the memory hotplug lifecycle through device add/remove.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/Makefile        |   1 +\n drivers/cxl/core/core.h          |   4 +\n drivers/cxl/core/port.c          |   2 +\n drivers/cxl/core/region_sysram.c | 351 +++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h                |  48 +++++\n 5 files changed, 406 insertions(+)\n create mode 100644 drivers/cxl/core/region_sysram.c\n\ndiff --git a/drivers/cxl/core/Makefile b/drivers/cxl/core/Makefile\nindex d3ec8aea64c5..d7ce52c50810 100644\n--- a/drivers/cxl/core/Makefile\n+++ b/drivers/cxl/core/Makefile\n@@ -18,6 +18,7 @@ cxl_core-$(CONFIG_TRACING) += trace.o\n cxl_core-$(CONFIG_CXL_REGION) += region.o\n cxl_core-$(CONFIG_CXL_REGION) += region_dax.o\n cxl_core-$(CONFIG_CXL_REGION) += region_pmem.o\n+cxl_core-$(CONFIG_CXL_REGION) += region_sysram.o\n cxl_core-$(CONFIG_CXL_MCE) += mce.o\n cxl_core-$(CONFIG_CXL_FEATURES) += features.o\n cxl_core-$(CONFIG_CXL_EDAC_MEM_FEATURES) += edac.o\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 6e1f695fd155..973bbcae43f7 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -35,6 +35,7 @@ extern struct device_attribute dev_attr_delete_region;\n extern struct device_attribute dev_attr_region;\n extern const struct device_type cxl_pmem_region_type;\n extern const struct device_type cxl_dax_region_type;\n+extern const struct device_type cxl_sysram_type;\n extern const struct device_type cxl_region_type;\n \n int cxl_decoder_detach(struct cxl_region *cxlr,\n@@ -46,6 +47,7 @@ int cxl_decoder_detach(struct cxl_region *cxlr,\n #define SET_CXL_REGION_ATTR(x) (&dev_attr_##x.attr),\n #define CXL_PMEM_REGION_TYPE(x) (&cxl_pmem_region_type)\n #define CXL_DAX_REGION_TYPE(x) (&cxl_dax_region_type)\n+#define CXL_SYSRAM_TYPE(x) (&cxl_sysram_type)\n int cxl_region_init(void);\n void cxl_region_exit(void);\n int cxl_get_poison_by_endpoint(struct cxl_port *port);\n@@ -54,6 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\n@@ -88,6 +91,7 @@ static inline void cxl_region_exit(void)\n #define SET_CXL_REGION_ATTR(x)\n #define CXL_PMEM_REGION_TYPE(x) NULL\n #define CXL_DAX_REGION_TYPE(x) NULL\n+#define CXL_SYSRAM_TYPE(x) NULL\n #endif\n \n struct cxl_send_command;\ndiff --git a/drivers/cxl/core/port.c b/drivers/cxl/core/port.c\nindex 5c82e6f32572..d6e82b3c2b64 100644\n--- a/drivers/cxl/core/port.c\n+++ b/drivers/cxl/core/port.c\n@@ -66,6 +66,8 @@ static int cxl_device_id(const struct device *dev)\n \t\treturn CXL_DEVICE_PMEM_REGION;\n \tif (dev->type == CXL_DAX_REGION_TYPE())\n \t\treturn CXL_DEVICE_DAX_REGION;\n+\tif (dev->type == CXL_SYSRAM_TYPE())\n+\t\treturn CXL_DEVICE_SYSRAM;\n \tif (is_cxl_port(dev)) {\n \t\tif (is_cxl_root(to_cxl_port(dev)))\n \t\t\treturn CXL_DEVICE_ROOT;\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nnew file mode 100644\nindex 000000000000..47a415deb352\n--- /dev/null\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -0,0 +1,351 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Sysram Region - Direct memory hotplug for CXL RAM regions\n+ *\n+ * This interface directly performs memory hotplug for CXL RAM regions,\n+ * eliminating the indirection through DAX.\n+ */\n+\n+#include <linux/memory_hotplug.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/memory.h>\n+#include <linux/device.h>\n+#include <linux/slab.h>\n+#include <linux/mm.h>\n+#include <cxlmem.h>\n+#include <cxl.h>\n+#include \"core.h\"\n+\n+static const char *sysram_res_name = \"System RAM (CXL)\";\n+\n+/**\n+ * cxl_region_find_sysram - Find the sysram device associated with a region\n+ * @cxlr: The CXL region\n+ *\n+ * Finds and returns the sysram child device of a CXL region.\n+ * The caller must release the device reference with put_device()\n+ * when done with the returned pointer.\n+ *\n+ * Return: Pointer to cxl_sysram, or NULL if not found\n+ */\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram;\n+\tstruct device *sdev;\n+\tchar sname[32];\n+\n+\tsnprintf(sname, sizeof(sname), \"sysram_region%d\", cxlr->id);\n+\tsdev = device_find_child_by_name(&cxlr->dev, sname);\n+\tif (!sdev)\n+\t\treturn NULL;\n+\n+\tsysram = to_cxl_sysram(sdev);\n+\treturn sysram;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_region_find_sysram, \"CXL\");\n+\n+static int sysram_get_numa_node(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_region_params *p = &cxlr->params;\n+\tint nid;\n+\n+\tnid = phys_to_target_node(p->res->start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(p->res->start);\n+\n+\treturn nid;\n+}\n+\n+static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n+{\n+\tstruct resource *res;\n+\tmhp_t mhp_flags;\n+\tint rc;\n+\n+\tif (sysram->res)\n+\t\treturn -EBUSY;\n+\n+\tres = request_mem_region(sysram->hpa_range.start,\n+\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t sysram->res_name);\n+\tif (!res)\n+\t\treturn -EBUSY;\n+\n+\tsysram->res = res;\n+\n+\t/*\n+\t * Set flags appropriate for System RAM. Leave ..._BUSY clear\n+\t * so that add_memory() can add a child resource.\n+\t */\n+\tres->flags = IORESOURCE_SYSTEM_RAM;\n+\n+\tmhp_flags = MHP_NID_IS_MGID;\n+\n+\t/*\n+\t * Ensure that future kexec'd kernels will not treat\n+\t * this as RAM automatically.\n+\t */\n+\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t online_type);\n+\tif (rc) {\n+\t\tremove_resource(res);\n+\t\tkfree(res);\n+\t\tsysram->res = NULL;\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n+{\n+\tint rc;\n+\n+\tif (!sysram->res)\n+\t\treturn 0;\n+\n+\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t       range_len(&sysram->hpa_range));\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tif (sysram->res) {\n+\t\tremove_resource(sysram->res);\n+\t\tkfree(sysram->res);\n+\t\tsysram->res = NULL;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn sysram_hotplug_remove(sysram);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_offline_and_remove, \"CXL\");\n+\n+static void cxl_sysram_release(struct device *dev)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\n+\tif (sysram->res)\n+\t\tsysram_hotplug_remove(sysram);\n+\n+\tkfree(sysram->res_name);\n+\n+\tif (sysram->mgid >= 0)\n+\t\tmemory_group_unregister(sysram->mgid);\n+\n+\tif (sysram->mtype)\n+\t\tclear_node_memory_type(sysram->numa_node, sysram->mtype);\n+\n+\tkfree(sysram);\n+}\n+\n+static ssize_t hotplug_store(struct device *dev,\n+\t\t\t     struct device_attribute *attr,\n+\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\tint online_type, rc;\n+\n+\tonline_type = mhp_online_type_from_str(buf);\n+\tif (online_type < 0)\n+\t\treturn online_type;\n+\n+\tif (online_type == MMOP_OFFLINE)\n+\t\trc = sysram_hotplug_remove(sysram);\n+\telse\n+\t\trc = sysram_hotplug_add(sysram, online_type);\n+\n+\tif (rc)\n+\t\tdev_warn(dev, \"hotplug %s failed: %d\\n\",\n+\t\t\t online_type == MMOP_OFFLINE ? \"offline\" : \"online\", rc);\n+\n+\treturn rc ? rc : len;\n+}\n+static DEVICE_ATTR_WO(hotplug);\n+\n+static struct attribute *cxl_sysram_attrs[] = {\n+\t&dev_attr_hotplug.attr,\n+\tNULL\n+};\n+\n+static const struct attribute_group cxl_sysram_attribute_group = {\n+\t.attrs = cxl_sysram_attrs,\n+};\n+\n+static const struct attribute_group *cxl_sysram_attribute_groups[] = {\n+\t&cxl_base_attribute_group,\n+\t&cxl_sysram_attribute_group,\n+\tNULL\n+};\n+\n+const struct device_type cxl_sysram_type = {\n+\t.name = \"cxl_sysram\",\n+\t.release = cxl_sysram_release,\n+\t.groups = cxl_sysram_attribute_groups,\n+};\n+\n+static bool is_cxl_sysram(struct device *dev)\n+{\n+\treturn dev->type == &cxl_sysram_type;\n+}\n+\n+struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\tif (dev_WARN_ONCE(dev, !is_cxl_sysram(dev),\n+\t\t\t  \"not a cxl_sysram device\\n\"))\n+\t\treturn NULL;\n+\treturn container_of(dev, struct cxl_sysram, dev);\n+}\n+EXPORT_SYMBOL_NS_GPL(to_cxl_sysram, \"CXL\");\n+\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram)\n+{\n+\treturn &sysram->dev;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_dev, \"CXL\");\n+\n+static struct lock_class_key cxl_sysram_key;\n+\n+static enum mmop cxl_sysram_get_default_online_type(void)\n+{\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_SYSTEM_DEFAULT))\n+\t\treturn mhp_get_default_online_type();\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_MOVABLE))\n+\t\treturn MMOP_ONLINE_MOVABLE;\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_NORMAL))\n+\t\treturn MMOP_ONLINE;\n+\treturn MMOP_OFFLINE;\n+}\n+\n+static struct cxl_sysram *cxl_sysram_alloc(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram __free(kfree) = NULL;\n+\tstruct device *dev;\n+\n+\tsysram = kzalloc(sizeof(*sysram), GFP_KERNEL);\n+\tif (!sysram)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tsysram->online_type = cxl_sysram_get_default_online_type();\n+\tsysram->last_hotplug_cmd = MMOP_OFFLINE;\n+\tsysram->numa_node = -1;\n+\tsysram->mgid = -1;\n+\n+\tdev = &sysram->dev;\n+\tsysram->cxlr = cxlr;\n+\tdevice_initialize(dev);\n+\tlockdep_set_class(&dev->mutex, &cxl_sysram_key);\n+\tdevice_set_pm_not_required(dev);\n+\tdev->parent = &cxlr->dev;\n+\tdev->bus = &cxl_bus_type;\n+\tdev->type = &cxl_sysram_type;\n+\n+\treturn_ptr(sysram);\n+}\n+\n+static void sysram_unregister(void *_sysram)\n+{\n+\tstruct cxl_sysram *sysram = _sysram;\n+\n+\tdevice_unregister(&sysram->dev);\n+}\n+\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+{\n+\tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n+\tstruct memory_dev_type *mtype;\n+\tstruct range hpa_range;\n+\tstruct device *dev;\n+\tint adist = MEMTIER_DEFAULT_LOWTIER_ADISTANCE;\n+\tint numa_node;\n+\tint rc;\n+\n+\trc = cxl_region_get_hpa_range(cxlr, &hpa_range);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\thpa_range = memory_block_align_range(&hpa_range);\n+\tif (hpa_range.start >= hpa_range.end) {\n+\t\tdev_warn(&cxlr->dev, \"region too small after alignment\\n\");\n+\t\treturn -ENOSPC;\n+\t}\n+\n+\tsysram = cxl_sysram_alloc(cxlr);\n+\tif (IS_ERR(sysram))\n+\t\treturn PTR_ERR(sysram);\n+\n+\tsysram->hpa_range = hpa_range;\n+\n+\tsysram->res_name = kasprintf(GFP_KERNEL, \"cxl_sysram%d\", cxlr->id);\n+\tif (!sysram->res_name)\n+\t\treturn -ENOMEM;\n+\n+\t/* Override default online type if caller specified one */\n+\tif (online_type >= 0)\n+\t\tsysram->online_type = online_type;\n+\n+\tdev = &sysram->dev;\n+\n+\trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Setup memory tier before adding device */\n+\tnuma_node = sysram_get_numa_node(cxlr);\n+\tif (numa_node < 0) {\n+\t\tdev_warn(&cxlr->dev, \"rejecting region with invalid node: %d\\n\",\n+\t\t\t numa_node);\n+\t\treturn -EINVAL;\n+\t}\n+\tsysram->numa_node = numa_node;\n+\n+\tmt_calc_adistance(numa_node, &adist);\n+\tmtype = mt_get_memory_type(adist);\n+\tif (IS_ERR(mtype))\n+\t\treturn PTR_ERR(mtype);\n+\tsysram->mtype = mtype;\n+\n+\tinit_node_memory_type(numa_node, mtype);\n+\n+\t/* Register memory group for this region */\n+\trc = memory_group_register_static(numa_node,\n+\t\t\t\t\t  PFN_UP(range_len(&hpa_range)));\n+\tif (rc < 0)\n+\t\treturn rc;\n+\tsysram->mgid = rc;\n+\n+\trc = device_add(dev);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n+\t\tdev_name(dev));\n+\n+\t/*\n+\t * Dynamic capacity regions (DCD) will have memory added later.\n+\t * For static RAM regions, hotplug the entire range now.\n+\t */\n+\tif (cxlr->mode != CXL_PARTMODE_RAM)\n+\t\tgoto out;\n+\n+\t/* If default online_type is a valid online mode, immediately hotplug */\n+\tif (sysram->online_type > MMOP_OFFLINE) {\n+\t\trc = sysram_hotplug_add(sysram, sysram->online_type);\n+\t\tif (rc)\n+\t\t\tdev_warn(dev, \"hotplug failed: %d\\n\", rc);\n+\t\telse\n+\t\t\tsysram->last_hotplug_cmd = sysram->online_type;\n+\t}\n+\n+out:\n+\treturn devm_add_action_or_reset(&cxlr->dev, sysram_unregister,\n+\t\t\t\t\tno_free_ptr(sysram));\n+}\n+EXPORT_SYMBOL_NS_GPL(devm_cxl_add_sysram, \"CXL\");\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex f899f240f229..8e8342fd4fde 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -607,6 +607,34 @@ struct cxl_dax_region {\n \tenum dax_driver_type dax_driver;\n };\n \n+/**\n+ * struct cxl_sysram - CXL SysRAM region for system memory hotplug\n+ * @dev: device for this sysram\n+ * @cxlr: parent cxl_region\n+ * @online_type: Default memory online type for new hotplug ops (MMOP_* value)\n+ * @last_hotplug_cmd: Last hotplug command submitted (MMOP_* value)\n+ * @hpa_range: Host physical address range for the region\n+ * @res_name: Resource name for the memory region\n+ * @res: Memory resource (set when hotplugged)\n+ * @mgid: Memory group id\n+ * @mtype: Memory tier type\n+ * @numa_node: NUMA node for this memory\n+ *\n+ * Device that directly performs memory hotplug for CXL RAM regions.\n+ */\n+struct cxl_sysram {\n+\tstruct device dev;\n+\tstruct cxl_region *cxlr;\n+\tenum mmop online_type;\n+\tint last_hotplug_cmd;\n+\tstruct range hpa_range;\n+\tconst char *res_name;\n+\tstruct resource *res;\n+\tint mgid;\n+\tstruct memory_dev_type *mtype;\n+\tint numa_node;\n+};\n+\n /**\n  * struct cxl_port - logical collection of upstream port devices and\n  *\t\t     downstream port devices to construct a CXL memory\n@@ -807,6 +835,7 @@ DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device\n DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n DEFINE_FREE(put_cxl_dax_region, struct cxl_dax_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n+DEFINE_FREE(put_cxl_sysram, struct cxl_sysram *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n \n int devm_cxl_enumerate_ports(struct cxl_memdev *cxlmd);\n void cxl_bus_rescan(void);\n@@ -889,6 +918,7 @@ void cxl_destroy_region(struct cxl_region *cxlr);\n struct device *cxl_region_dev(struct cxl_region *cxlr);\n enum cxl_partition_mode cxl_region_mode(struct cxl_region *cxlr);\n int cxl_get_region_range(struct cxl_region *cxlr, struct range *range);\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr);\n int cxl_get_committed_regions(struct cxl_memdev *cxlmd,\n \t\t\t      struct cxl_region **regions, int max_regions);\n struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n@@ -936,6 +966,7 @@ void cxl_driver_unregister(struct cxl_driver *cxl_drv);\n #define CXL_DEVICE_PMEM_REGION\t\t7\n #define CXL_DEVICE_DAX_REGION\t\t8\n #define CXL_DEVICE_PMU\t\t\t9\n+#define CXL_DEVICE_SYSRAM\t\t10\n \n #define MODULE_ALIAS_CXL(type) MODULE_ALIAS(\"cxl:t\" __stringify(type) \"*\")\n #define CXL_MODALIAS_FMT \"cxl:t%d\"\n@@ -954,6 +985,10 @@ bool is_cxl_pmem_region(struct device *dev);\n struct cxl_pmem_region *to_cxl_pmem_region(struct device *dev);\n int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n+struct cxl_sysram *to_cxl_sysram(struct device *dev);\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n static inline bool is_cxl_pmem_region(struct device *dev)\n@@ -972,6 +1007,19 @@ static inline struct cxl_dax_region *to_cxl_dax_region(struct device *dev)\n {\n \treturn NULL;\n }\n+static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\treturn NULL;\n+}\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+\t\t\t\t      enum mmop online_type)\n+{\n+\treturn -ENXIO;\n+}\n+static inline int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn -ENXIO;\n+}\n static inline u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint,\n \t\t\t\t\t       u64 spa)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private memory regions being isolated from normal allocations and reclaim by adding support for N_MEMORY_PRIVATE hotplug via add_private_memory_driver_managed(). They modified the cxl_sysram region to register as a private node when private=true is passed to devm_cxl_add_sysram(), allowing callers to isolate their memory. A fix is planned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "planned a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Extend the cxl_sysram region to support N_MEMORY_PRIVATE hotplug\nvia add_private_memory_driver_managed(). When a caller passes\nprivate=true to devm_cxl_add_sysram(), the memory is registered\nas a private node, isolating it from normal allocations and reclaim.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/core.h          |  2 +-\n drivers/cxl/core/region_sysram.c | 50 +++++++++++++++++++++++++-------\n drivers/cxl/cxl.h                |  9 ++++--\n 3 files changed, 48 insertions(+), 13 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 973bbcae43f7..8ca3d6d41fe4 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -56,7 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nindex 47a415deb352..77aaa52e7332 100644\n--- a/drivers/cxl/core/region_sysram.c\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -85,12 +85,23 @@ static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n \t/*\n \t * Ensure that future kexec'd kernels will not treat\n \t * this as RAM automatically.\n+\t *\n+\t * For private regions, use add_private_memory_driver_managed()\n+\t * to register as N_MEMORY_PRIVATE which isolates the memory from\n+\t * normal allocations and reclaim.\n \t */\n-\trc = __add_memory_driver_managed(sysram->mgid,\n-\t\t\t\t\t sysram->hpa_range.start,\n-\t\t\t\t\t range_len(&sysram->hpa_range),\n-\t\t\t\t\t sysram_res_name, mhp_flags,\n-\t\t\t\t\t online_type);\n+\tif (sysram->private)\n+\t\trc = add_private_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t       sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t       online_type, &sysram->np);\n+\telse\n+\t\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t online_type);\n \tif (rc) {\n \t\tremove_resource(res);\n \t\tkfree(res);\n@@ -108,10 +119,23 @@ static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n \tif (!sysram->res)\n \t\treturn 0;\n \n-\trc = offline_and_remove_memory(sysram->hpa_range.start,\n-\t\t\t\t       range_len(&sysram->hpa_range));\n-\tif (rc)\n-\t\treturn rc;\n+\tif (sysram->private) {\n+\t\trc = offline_and_remove_private_memory(sysram->numa_node,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\t/*\n+\t\t * -EBUSY means memory was removed but node_private_unregister()\n+\t\t * could not complete because other regions share the node.\n+\t\t * Continue to resource cleanup since the memory is gone.\n+\t\t */\n+\t\tif (rc && rc != -EBUSY)\n+\t\t\treturn rc;\n+\t} else {\n+\t\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\t}\n \n \tif (sysram->res) {\n \t\tremove_resource(sysram->res);\n@@ -257,7 +281,8 @@ static void sysram_unregister(void *_sysram)\n \tdevice_unregister(&sysram->dev);\n }\n \n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n+\t\t\tenum mmop online_type)\n {\n \tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n \tstruct memory_dev_type *mtype;\n@@ -291,6 +316,11 @@ int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n \tif (online_type >= 0)\n \t\tsysram->online_type = online_type;\n \n+\t/* Set up private node registration if requested */\n+\tsysram->private = private;\n+\tif (private)\n+\t\tsysram->np.owner = sysram;\n+\n \tdev = &sysram->dev;\n \n \trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 8e8342fd4fde..54e5f9ac59dc 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -10,6 +10,7 @@\n #include <linux/bitops.h>\n #include <linux/log2.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n #include <linux/io.h>\n #include <linux/range.h>\n #include <linux/dax.h>\n@@ -619,6 +620,8 @@ struct cxl_dax_region {\n  * @mgid: Memory group id\n  * @mtype: Memory tier type\n  * @numa_node: NUMA node for this memory\n+ * @private: true if this region uses N_MEMORY_PRIVATE hotplug\n+ * @np: private node registration state (valid when @private is true)\n  *\n  * Device that directly performs memory hotplug for CXL RAM regions.\n  */\n@@ -633,6 +636,8 @@ struct cxl_sysram {\n \tint mgid;\n \tstruct memory_dev_type *mtype;\n \tint numa_node;\n+\tbool private;\n+\tstruct node_private np;\n };\n \n /**\n@@ -987,7 +992,7 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n struct cxl_sysram *to_cxl_sysram(struct device *dev);\n struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n@@ -1011,7 +1016,7 @@ static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n {\n \treturn NULL;\n }\n-static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n \t\t\t\t      enum mmop online_type)\n {\n \treturn -ENXIO;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the driver's interaction with the migration target control, explaining that they moved struct migration_target_control to include/linux/migrate.h so the driver can use alloc_migration_target() without depending on mm-internal headers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a sample CXL type-3 driver that registers device memory as\nprivate-node NUMA memory reachable only via explicit mempolicy\n(set_mempolicy / mbind).\n\nProbe flow:\n  1. Call cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Look for pre-committed RAM regions; if none exist, create one\n     using cxl_get_hpa_freespace() + cxl_request_dpa() +\n     cxl_create_region()\n  3. Convert the region to sysram via devm_cxl_add_sysram() with\n     private=true and MMOP_ONLINE_MOVABLE\n  4. Register node_private_ops with NP_OPS_MIGRATION | NP_OPS_MEMPOLICY\n     so the node is excluded from default allocations\n\nThe migrate_to callback uses alloc_migration_target() with\n__GFP_THISNODE | __GFP_PRIVATE to keep pages on the target node.\n\nMove struct migration_target_control from mm/internal.h to\ninclude/linux/migrate.h so the driver can use alloc_migration_target()\nwithout depending on mm-internal headers.\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/Kconfig                           |   2 +\n drivers/cxl/Makefile                          |   2 +\n drivers/cxl/type3_drivers/Kconfig             |   2 +\n drivers/cxl/type3_drivers/Makefile            |   2 +\n .../cxl/type3_drivers/cxl_mempolicy/Kconfig   |  16 +\n .../cxl/type3_drivers/cxl_mempolicy/Makefile  |   4 +\n .../type3_drivers/cxl_mempolicy/mempolicy.c   | 297 ++++++++++++++++++\n include/linux/migrate.h                       |   7 +-\n mm/internal.h                                 |   7 -\n 9 files changed, 331 insertions(+), 8 deletions(-)\n create mode 100644 drivers/cxl/type3_drivers/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n\ndiff --git a/drivers/cxl/Kconfig b/drivers/cxl/Kconfig\nindex f99aa7274d12..1648cdeaa0c9 100644\n--- a/drivers/cxl/Kconfig\n+++ b/drivers/cxl/Kconfig\n@@ -278,4 +278,6 @@ config CXL_ATL\n \tdepends on CXL_REGION\n \tdepends on ACPI_PRMT && AMD_NB\n \n+source \"drivers/cxl/type3_drivers/Kconfig\"\n+\n endif\ndiff --git a/drivers/cxl/Makefile b/drivers/cxl/Makefile\nindex 2caa90fa4bf2..94d2b2233bf8 100644\n--- a/drivers/cxl/Makefile\n+++ b/drivers/cxl/Makefile\n@@ -19,3 +19,5 @@ cxl_acpi-y := acpi.o\n cxl_pmem-y := pmem.o security.o\n cxl_mem-y := mem.o\n cxl_pci-y := pci.o\n+\n+obj-y += type3_drivers/\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nnew file mode 100644\nindex 000000000000..369b21763856\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nnew file mode 100644\nindex 000000000000..2b82265ff118\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\nnew file mode 100644\nindex 000000000000..3c45da237b9f\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n@@ -0,0 +1,16 @@\n+config CXL_MEMPOLICY\n+\ttristate \"CXL Private Memory with Mempolicy Support\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\thelp\n+\t  Minimal driver for CXL memory devices that registers memory as\n+\t  N_MEMORY_PRIVATE with mempolicy support.  The memory is isolated\n+\t  from default allocations and can only be reached via explicit\n+\t  mempolicy (set_mempolicy or mbind).\n+\n+\t  No compression, no PTE controls, the memory behaves like normal\n+\t  DRAM but is excluded from fallback allocations.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\nnew file mode 100644\nindex 000000000000..dfb58fc88ad9\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy.o\n+cxl_mempolicy-y := mempolicy.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\nnew file mode 100644\nindex 000000000000..1c19818eb268\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n@@ -0,0 +1,297 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Mempolicy Driver\n+ *\n+ * Minimal driver for CXL memory devices that registers memory as\n+ * N_MEMORY_PRIVATE with mempolicy support but no PTE controls.  The\n+ * memory behaves like normal DRAM but is isolated from default allocations,\n+ * it can only be reached via explicit mempolicy (set_mempolicy/mbind).\n+ *\n+ * Usage:\n+ *   1. Unbind device from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   2. Bind to cxl_mempolicy:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n+ */\n+\n+#include <linux/module.h>\n+#include <linux/pci.h>\n+#include <linux/xarray.h>\n+#include <linux/node_private.h>\n+#include <linux/migrate.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+struct cxl_mempolicy_ctx {\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint nid;\n+};\n+\n+static DEFINE_XARRAY(ctx_xa);\n+\n+static struct cxl_mempolicy_ctx *memdev_to_ctx(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\n+\treturn xa_load(&ctx_xa, (unsigned long)pdev);\n+}\n+\n+static int cxl_mempolicy_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason,\n+\t\t\t\t    unsigned int *nr_succeeded)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE |\n+\t\t\t    __GFP_PRIVATE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, nr_succeeded);\n+}\n+\n+static void cxl_mempolicy_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static const struct node_private_ops cxl_mempolicy_ops = {\n+\t.migrate_to\t= cxl_mempolicy_migrate_to,\n+\t.folio_migrate\t= cxl_mempolicy_folio_migrate,\n+\t.flags = NP_OPS_MIGRATION | NP_OPS_MEMPOLICY,\n+};\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tctx->cxled = cxled;\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\treturn cxlr;\n+}\n+\n+static int setup_private_node(struct cxl_memdev *cxlmd,\n+\t\t\t      struct cxl_region *cxlr)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct range hpa_range;\n+\tint rc;\n+\n+\tdevice_release_driver(cxl_region_dev(cxlr));\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to add sysram: %d\\n\", rc);\n+\t\tif (device_attach(cxl_region_dev(cxlr)) < 0)\n+\t\t\tdev_warn(cxl_region_dev(cxlr),\n+\t\t\t\t \"failed to re-attach driver\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tctx->nid = phys_to_target_node(hpa_range.start);\n+\tif (ctx->nid == NUMA_NO_NODE)\n+\t\tctx->nid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\trc = node_private_set_ops(ctx->nid, &cxl_mempolicy_ops);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to set ops on node %d: %d\\n\", ctx->nid, rc);\n+\t\tctx->nid = NUMA_NO_NODE;\n+\t\treturn rc;\n+\t}\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"node %d registered as private mempolicy memory\\n\", ctx->nid);\n+\treturn 0;\n+}\n+\n+static int cxl_mempolicy_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i;\n+\tint rc;\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"cxl_mempolicy attach: looking for regions\\n\");\n+\n+\t/* Phase 1: look for pre-committed RAM regions */\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) != CXL_PARTMODE_RAM) {\n+\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tcxlr = regions[i];\n+\t\trc = setup_private_node(cxlmd, cxlr);\n+\t\tput_device(cxl_region_dev(cxlr));\n+\t\tif (rc == 0) {\n+\t\t\t/* Release remaining region references */\n+\t\t\tfor (i++; i < nr; i++)\n+\t\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\treturn 0;\n+\t\t}\n+\t}\n+\n+\t/* Phase 2: no committed regions, create one */\n+\tdev_info(&cxlmd->dev,\n+\t\t \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"no RAM capacity: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = setup_private_node(cxlmd, cxlr);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to setup private node: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\t/* Only take ownership of regions we created (Phase 2) */\n+\tmemdev_to_ctx(cxlmd)->cxlr = cxlr;\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_mempolicy_attach = {\n+\t.probe = cxl_mempolicy_attach_probe,\n+};\n+\n+static int cxl_mempolicy_probe(struct pci_dev *pdev,\n+\t\t\t       const struct pci_device_id *id)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probing device\\n\");\n+\n+\tctx = devm_kzalloc(&pdev->dev, sizeof(*ctx), GFP_KERNEL);\n+\tif (!ctx)\n+\t\treturn -ENOMEM;\n+\tctx->nid = NUMA_NO_NODE;\n+\n+\trc = xa_insert(&ctx_xa, (unsigned long)pdev, ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_mempolicy_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_mempolicy_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = xa_erase(&ctx_xa, (unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: removing device\\n\");\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\tif (ctx->nid != NUMA_NO_NODE)\n+\t\tWARN_ON(node_private_clear_ops(ctx->nid, &cxl_mempolicy_ops));\n+\n+\tif (ctx->cxlr) {\n+\t\tcxl_destroy_region(ctx->cxlr);\n+\t\tctx->cxlr = NULL;\n+\t}\n+\n+\tif (ctx->cxled) {\n+\t\tcxl_dpa_free(ctx->cxled);\n+\t\tctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_mempolicy_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_mempolicy_pci_tbl);\n+\n+static struct pci_driver cxl_mempolicy_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_mempolicy_pci_tbl,\n+\t.probe\t\t= cxl_mempolicy_probe,\n+\t.remove\t\t= cxl_mempolicy_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_mempolicy_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Private Memory with Mempolicy Support\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 7b2da3875ff2..1f9fb61f3932 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -10,7 +10,12 @@\n typedef struct folio *new_folio_t(struct folio *folio, unsigned long private);\n typedef void free_folio_t(struct folio *folio, unsigned long private);\n \n-struct migration_target_control;\n+struct migration_target_control {\n+\tint nid;\t\t/* preferred node id */\n+\tnodemask_t *nmask;\n+\tgfp_t gfp_mask;\n+\tenum migrate_reason reason;\n+};\n \n /**\n  * struct movable_operations - Driver page migration\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 64467ca774f1..85cd11189854 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1352,13 +1352,6 @@ extern const struct trace_print_flags gfpflag_names[];\n \n void setup_zone_pageset(struct zone *zone);\n \n-struct migration_target_control {\n-\tint nid;\t\t/* preferred node id */\n-\tnodemask_t *nmask;\n-\tgfp_t gfp_mask;\n-\tenum migrate_reason reason;\n-};\n-\n /*\n  * mm/filemap.c\n  */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the cxl_compression driver's page reclamation using the CXL Media Operations Zero command (opcode 0x4402). The author explains that if the device does not support this command, the driver falls back to inline CPU zeroing.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a generic CXL type-3 driver for compressed memory controllers.\n\nThe driver provides an alternative PCI binding that converts CXL\nRAM regions to private-node sysram and registers them with the\nCRAM subsystem for transparent demotion/promotion.\n\nProbe flow:\n  1. cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Discover/convert auto-RAM regions or create a RAM region\n  3. Convert to private-node sysram via devm_cxl_add_sysram()\n  4. Register with CRAM via cram_register_private_node()\n\nPage flush pipeline:\n  When a CRAM folio is freed, the CRAM free_folio   callback buffers\n  it into a per-CPU RCU-protected flush buffer to offload the operation.\n\n  A periodic kthread swaps the per-CPU buffers under RCU, then sends\n  batched Sanitize-Zero commands so the device can zero pages.\n\n  A flush_record bitmap tracks in-flight pages to avoid re-buffering on\n  the second free_folio entry after folio_put().\n\n  Overflow from full buffers is handled by a per-CPU workqueue fallback.\n\nWatermark interrupts:\n  MSI-X vector 12 - delivers \"Low\" watermark interrupts\n  MSI-X vector 13 - delivers \"High\" watermark interrupts\n  This adjusts CRAM pressure:\n\tLow  - increases pressure.\n  \tHigh - reduces pressure.\n\n  A dynamic watermark mode cycles through four phases with\n  progressively tighter thresholds.\n\n  Static watermark mode sets pressure 0 or MAX respectively.\n\nTeardown ordering:\n  pre_teardown  - cram_unregister + retry-loop memory offline\n  post_teardown - kthread stop, drain all flush buffers via CCI\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/type3_drivers/Kconfig             |    1 +\n drivers/cxl/type3_drivers/Makefile            |    1 +\n .../cxl/type3_drivers/cxl_compression/Kconfig |   20 +\n .../type3_drivers/cxl_compression/Makefile    |    4 +\n .../cxl_compression/compression.c             | 1025 +++++++++++++++++\n 5 files changed, 1051 insertions(+)\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/compression.c\n\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nindex 369b21763856..98f73e46730e 100644\n--- a/drivers/cxl/type3_drivers/Kconfig\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\n+source \"drivers/cxl/type3_drivers/cxl_compression/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nindex 2b82265ff118..f5b0766d92af 100644\n--- a/drivers/cxl/type3_drivers/Makefile\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression/\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Kconfig b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\nnew file mode 100644\nindex 000000000000..8c891a48b000\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\n@@ -0,0 +1,20 @@\n+config CXL_COMPRESSION\n+\ttristate \"CXL Compression Memory Driver\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on CRAM\n+\thelp\n+\t  This driver provides an alternative PCI binding for CXL memory\n+\t  devices with compressed memory support. It converts CXL RAM\n+\t  regions to sysram for direct memory hotplug and registers with\n+\t  the CRAM subsystem for transparent compression.\n+\n+\t  Page reclamation uses the standard CXL Media Operations Zero\n+\t  command (opcode 0x4402). If the device does not support it,\n+\t  the driver falls back to inline CPU zeroing.\n+\n+\t  Usage: First unbind the device from cxl_pci, then bind to\n+\t  cxl_compression. The driver will initialize the CXL device and\n+\t  convert any RAM regions to use direct memory hotplug via sysram.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Makefile b/drivers/cxl/type3_drivers/cxl_compression/Makefile\nnew file mode 100644\nindex 000000000000..46f34809bf74\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression.o\n+cxl_compression-y := compression.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/compression.c b/drivers/cxl/type3_drivers/cxl_compression/compression.c\nnew file mode 100644\nindex 000000000000..e4c8b62227e2\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/compression.c\n@@ -0,0 +1,1025 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Compression Driver\n+ *\n+ * This driver provides an alternative binding for CXL memory devices that\n+ * converts all associated RAM regions to sysram_regions for direct memory\n+ * hotplug, bypassing the standard dax region path.\n+ *\n+ * Page reclamation uses the standard CXL Media Operations Zero command\n+ * (opcode 0x4402, class 0x01, subclass 0x01).  Watermark interrupts\n+ * are delivered via separate MSI-X vectors (12 for lthresh, 13 for\n+ * hthresh), injected externally via QMP.\n+ *\n+ * Usage:\n+ *   1. Device initially binds to cxl_pci at boot\n+ *   2. Unbind from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   3. Bind to cxl_compression:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n+ */\n+\n+#include <linux/unaligned.h>\n+#include <linux/io-64-nonatomic-lo-hi.h>\n+#include <linux/module.h>\n+#include <linux/delay.h>\n+#include <linux/sizes.h>\n+#include <linux/mutex.h>\n+#include <linux/list.h>\n+#include <linux/pci.h>\n+#include <linux/io.h>\n+#include <linux/interrupt.h>\n+#include <linux/bitmap.h>\n+#include <linux/highmem.h>\n+#include <linux/workqueue.h>\n+#include <linux/kthread.h>\n+#include <linux/rcupdate.h>\n+#include <linux/percpu.h>\n+#include <linux/sched.h>\n+#include <linux/cram.h>\n+#include <linux/memory_hotplug.h>\n+#include <linux/xarray.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+/*\n+ * Per-device compression context lookup.\n+ *\n+ * pci_set_drvdata() MUST store cxlds because mbox_to_cxlds() uses\n+ * dev_get_drvdata() to recover the cxl_dev_state from the mailbox host\n+ * device.  Storing anything else in pci drvdata breaks every CXL mailbox\n+ * command.  Use an xarray keyed by pci_dev pointer so that multiple\n+ * devices can bind concurrently without colliding.\n+ */\n+static DEFINE_XARRAY(comp_ctx_xa);\n+\n+static struct cxl_compression_ctx *pdev_to_comp_ctx(struct pci_dev *pdev)\n+{\n+\treturn xa_load(&comp_ctx_xa, (unsigned long)pdev);\n+}\n+\n+#define CXL_MEDIA_OP_OPCODE\t\t0x4402\n+#define CXL_MEDIA_OP_CLASS_SANITIZE\t0x01\n+#define CXL_MEDIA_OP_SUBC_ZERO\t\t0x01\n+\n+struct cxl_dpa_range {\n+\t__le64 starting_dpa;\n+\t__le64 length;\n+} __packed;\n+\n+struct cxl_media_op_input {\n+\tu8 media_operation_class;\n+\tu8 media_operation_subclass;\n+\t__le16 reserved;\n+\t__le32 dpa_range_count;\n+\tstruct cxl_dpa_range ranges[];\n+} __packed;\n+\n+#define CXL_CT3_MSIX_LTHRESH\t\t12\n+#define CXL_CT3_MSIX_HTHRESH\t\t13\n+#define CXL_CT3_MSIX_VECTOR_NR\t\t14\n+#define CXL_FLUSH_INTERVAL_DEFAULT_MS\t1000\n+\n+static unsigned int flush_buf_size;\n+module_param(flush_buf_size, uint, 0444);\n+MODULE_PARM_DESC(flush_buf_size,\n+\t\t \"Max DPA ranges per media ops CCI command (0 = use hw max)\");\n+\n+static unsigned int flush_interval_ms = CXL_FLUSH_INTERVAL_DEFAULT_MS;\n+module_param(flush_interval_ms, uint, 0644);\n+MODULE_PARM_DESC(flush_interval_ms,\n+\t\t \"Flush worker interval in ms (default 1000)\");\n+\n+struct cxl_flush_buf {\n+\tunsigned int count;\n+\tunsigned int max;\t\t\t/* max ranges per command */\n+\tstruct cxl_media_op_input *cmd;\t\t/* pre-allocated CCI payload */\n+\tstruct folio **folios;\t\t\t/* parallel folio tracking */\n+};\n+\n+struct cxl_flush_ctx;\n+\n+struct cxl_pcpu_flush {\n+\tstruct cxl_flush_buf __rcu *active;\t/* callback writes here */\n+\tstruct cxl_flush_buf *overflow_spare;\t/* spare for overflow work */\n+\tstruct work_struct overflow_work;\t/* per-CPU overflow flush */\n+\tstruct cxl_flush_ctx *ctx;\t\t/* backpointer */\n+};\n+\n+/**\n+ * struct cxl_flush_ctx - Per-region flush context\n+ * @flush_record: two-level bitmap, 1 bit per 4KB page, tracks in-flight ops\n+ * @flush_record_pages: number of pages in the flush_record array\n+ * @nr_pages: total number of 4KB pages in the region\n+ * @base_pfn: starting PFN of the region (for DPA offset calculation)\n+ * @buf_max: max DPA ranges per CCI command\n+ * @media_ops_supported: true if device supports media operations zero\n+ * @pcpu: per-CPU flush state\n+ * @kthread_spares: array[nr_cpu_ids] of spare buffers for the kthread\n+ * @flush_thread: round-robin kthread\n+ * @mbox: pointer to CXL mailbox for sending CCI commands\n+ * @dev: device for logging\n+ * @nid: NUMA node of the private region\n+ */\n+struct cxl_flush_ctx {\n+\tunsigned long\t**flush_record;\n+\tunsigned int\t flush_record_pages;\n+\tunsigned long\t nr_pages;\n+\tunsigned long\t base_pfn;\n+\tunsigned int\t buf_max;\n+\tbool\t\t media_ops_supported;\n+\tstruct cxl_pcpu_flush __percpu *pcpu;\n+\tstruct cxl_flush_buf **kthread_spares;\n+\tstruct task_struct *flush_thread;\n+\tstruct cxl_mailbox *mbox;\n+\tstruct device\t*dev;\n+\tint\t\t nid;\n+};\n+\n+/* Bits per page-sized bitmap chunk */\n+#define FLUSH_RECORD_BITS_PER_PAGE\t(PAGE_SIZE * BITS_PER_BYTE)\n+#define FLUSH_RECORD_SHIFT\t\t(PAGE_SHIFT + 3)\n+\n+static unsigned long **flush_record_alloc(unsigned long nr_bits,\n+\t\t\t\t\t  unsigned int *nr_pages_out)\n+{\n+\tunsigned int nr_pages = DIV_ROUND_UP(nr_bits, FLUSH_RECORD_BITS_PER_PAGE);\n+\tunsigned long **pages;\n+\tunsigned int i;\n+\n+\tpages = kcalloc(nr_pages, sizeof(*pages), GFP_KERNEL);\n+\tif (!pages)\n+\t\treturn NULL;\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tpages[i] = (unsigned long *)get_zeroed_page(GFP_KERNEL);\n+\t\tif (!pages[i])\n+\t\t\tgoto err;\n+\t}\n+\n+\t*nr_pages_out = nr_pages;\n+\treturn pages;\n+\n+err:\n+\twhile (i--)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+\treturn NULL;\n+}\n+\n+static void flush_record_free(unsigned long **pages, unsigned int nr_pages)\n+{\n+\tunsigned int i;\n+\n+\tif (!pages)\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr_pages; i++)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+}\n+\n+static inline bool flush_record_test_and_clear(unsigned long **pages,\n+\t\t\t\t\t       unsigned long idx)\n+{\n+\treturn test_and_clear_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\t\t\t  pages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static inline void flush_record_set(unsigned long **pages, unsigned long idx)\n+{\n+\tset_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\tpages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static struct cxl_flush_buf *cxl_flush_buf_alloc(unsigned int max, int nid)\n+{\n+\tstruct cxl_flush_buf *buf;\n+\n+\tbuf = kzalloc_node(sizeof(*buf), GFP_KERNEL, nid);\n+\tif (!buf)\n+\t\treturn NULL;\n+\n+\tbuf->max = max;\n+\tbuf->cmd = kvzalloc_node(struct_size(buf->cmd, ranges, max),\n+\t\t\t\t GFP_KERNEL, nid);\n+\tif (!buf->cmd)\n+\t\tgoto err_cmd;\n+\n+\tbuf->folios = kcalloc_node(max, sizeof(struct folio *),\n+\t\t\t\t   GFP_KERNEL, nid);\n+\tif (!buf->folios)\n+\t\tgoto err_folios;\n+\n+\treturn buf;\n+\n+err_folios:\n+\tkvfree(buf->cmd);\n+err_cmd:\n+\tkfree(buf);\n+\treturn NULL;\n+}\n+\n+static void cxl_flush_buf_free(struct cxl_flush_buf *buf)\n+{\n+\tif (!buf)\n+\t\treturn;\n+\tkvfree(buf->cmd);\n+\tkfree(buf->folios);\n+\tkfree(buf);\n+}\n+\n+static inline void cxl_flush_buf_reset(struct cxl_flush_buf *buf)\n+{\n+\tbuf->count = 0;\n+}\n+\n+static void cxl_flush_buf_send(struct cxl_flush_ctx *ctx,\n+\t\t\t       struct cxl_flush_buf *buf)\n+{\n+\tstruct cxl_mbox_cmd mbox_cmd;\n+\tunsigned int count = buf->count;\n+\tunsigned int i;\n+\tint rc;\n+\n+\tif (count == 0)\n+\t\treturn;\n+\n+\tif (!ctx->media_ops_supported) {\n+\t\t/* No device support, zero all folios inline */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t\tgoto release;\n+\t}\n+\n+\tbuf->cmd->media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE;\n+\tbuf->cmd->media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO;\n+\tbuf->cmd->reserved = 0;\n+\tbuf->cmd->dpa_range_count = cpu_to_le32(count);\n+\n+\tmbox_cmd = (struct cxl_mbox_cmd) {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = buf->cmd,\n+\t\t.size_in = struct_size(buf->cmd, ranges, count),\n+\t\t.poll_interval_ms = 1000,\n+\t\t.poll_count = 30,\n+\t};\n+\n+\trc = cxl_internal_send_cmd(ctx->mbox, &mbox_cmd);\n+\tif (rc) {\n+\t\tdev_warn(ctx->dev,\n+\t\t\t \"media ops zero CCI command failed: %d\\n\", rc);\n+\n+\t\t/* Zero all folios inline on failure */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t}\n+\n+release:\n+\tfor (i = 0; i < count; i++)\n+\t\tfolio_put(buf->folios[i]);\n+\n+\tcxl_flush_buf_reset(buf);\n+}\n+\n+static int cxl_compression_flush_cb(struct folio *folio, void *private)\n+{\n+\tstruct cxl_flush_ctx *ctx = private;\n+\tunsigned long pfn = folio_pfn(folio);\n+\tunsigned long idx = pfn - ctx->base_pfn;\n+\tunsigned long nr = folio_nr_pages(folio);\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tunsigned long flags;\n+\tunsigned int pos;\n+\n+\t/* Case (a): flush record bit set, resolution from our media op */\n+\tif (flush_record_test_and_clear(ctx->flush_record, idx))\n+\t\treturn 0;\n+\n+\tdev_dbg_ratelimited(ctx->dev,\n+\t\t\t     \"flush_cb: folio pfn=%lx order=%u idx=%lu cpu=%d\\n\",\n+\t\t\t     pfn, folio_order(folio), idx,\n+\t\t\t     raw_smp_processor_id());\n+\n+\tlocal_irq_save(flags);\n+\trcu_read_lock();\n+\n+\tpcpu = this_cpu_ptr(ctx->pcpu);\n+\tbuf = rcu_dereference(pcpu->active);\n+\n+\tif (unlikely(!buf || buf->count >= buf->max)) {\n+\t\trcu_read_unlock();\n+\t\tlocal_irq_restore(flags);\n+\t\tif (buf)\n+\t\t\tschedule_work_on(raw_smp_processor_id(),\n+\t\t\t\t\t &pcpu->overflow_work);\n+\t\treturn 2;\n+\t}\n+\n+\t/* Case (b): write DPA range directly into pre-formatted CCI buffer */\n+\tfolio_get(folio);\n+\tflush_record_set(ctx->flush_record, idx);\n+\n+\tpos = buf->count;\n+\tbuf->folios[pos] = folio;\n+\tbuf->cmd->ranges[pos].starting_dpa = cpu_to_le64((u64)idx * PAGE_SIZE);\n+\tbuf->cmd->ranges[pos].length = cpu_to_le64((u64)nr * PAGE_SIZE);\n+\tbuf->count = pos + 1;\n+\n+\trcu_read_unlock();\n+\tlocal_irq_restore(flags);\n+\n+\treturn 1;\n+}\n+\n+static int cxl_flush_kthread_fn(void *data)\n+{\n+\tstruct cxl_flush_ctx *ctx = data;\n+\tstruct cxl_flush_buf *dirty;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tint cpu;\n+\tbool any_dirty;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tany_dirty = false;\n+\n+\t\t/* Phase 1: Swap all per-CPU buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tstruct cxl_flush_buf *spare = ctx->kthread_spares[cpu];\n+\n+\t\t\tif (!spare)\n+\t\t\t\tcontinue;\n+\n+\t\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\t\tcxl_flush_buf_reset(spare);\n+\t\t\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\t\t\tctx->kthread_spares[cpu] = dirty;\n+\n+\t\t\tif (dirty && dirty->count > 0) {\n+\t\t\t\tdev_dbg(ctx->dev,\n+\t\t\t\t\t \"flush_kthread: cpu=%d has %u dirty ranges\\n\",\n+\t\t\t\t\t cpu, dirty->count);\n+\t\t\t\tany_dirty = true;\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (!any_dirty)\n+\t\t\tgoto sleep;\n+\n+\t\t/* Phase 2: Single synchronize_rcu for all swaps */\n+\t\tsynchronize_rcu();\n+\n+\t\t/* Phase 3: Send CCI commands for dirty buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tdirty = ctx->kthread_spares[cpu];\n+\t\t\tif (dirty && dirty->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, dirty);\n+\t\t\t/* dirty is now clean, stays as kthread_spares[cpu] */\n+\t\t}\n+\n+sleep:\n+\t\tschedule_timeout_interruptible(\n+\t\t\tmsecs_to_jiffies(flush_interval_ms));\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static void cxl_flush_overflow_work(struct work_struct *work)\n+{\n+\tstruct cxl_pcpu_flush *pcpu =\n+\t\tcontainer_of(work, struct cxl_pcpu_flush, overflow_work);\n+\tstruct cxl_flush_ctx *ctx = pcpu->ctx;\n+\tstruct cxl_flush_buf *dirty, *spare;\n+\tunsigned long flags;\n+\n+\tdev_dbg(ctx->dev, \"flush_overflow: cpu=%d buffer full, flushing\\n\",\n+\t\t raw_smp_processor_id());\n+\n+\tspare = pcpu->overflow_spare;\n+\tif (!spare)\n+\t\treturn;\n+\n+\tcxl_flush_buf_reset(spare);\n+\n+\tlocal_irq_save(flags);\n+\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\tlocal_irq_restore(flags);\n+\n+\tpcpu->overflow_spare = dirty;\n+\n+\tsynchronize_rcu();\n+\tcxl_flush_buf_send(ctx, dirty);\n+}\n+\n+struct cxl_teardown_ctx {\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+};\n+\n+static void cxl_compression_pre_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\n+\tif (!tctx->flush_ctx)\n+\t\treturn;\n+\n+\t/*\n+\t * Unregister the CRAM node before memory goes offline.\n+\t * node_private_clear_ops requires the node_private to still\n+\t * exist, which is destroyed during memory removal.\n+\t */\n+\tcram_unregister_private_node(tctx->nid);\n+\n+\t/*\n+\t * Offline and remove CXL memory with retry.  CXL compressed\n+\t * memory may have pages pinned by in-flight flush operations;\n+\t * keep retrying until they complete.  Once done, sysram->res\n+\t * is NULL so the devm sysram_unregister action that follows\n+\t * will skip the hotplug removal.\n+\t */\n+\tif (tctx->sysram) {\n+\t\tint rc, retries = 0;\n+\n+\t\twhile (true) {\n+\t\t\trc = cxl_sysram_offline_and_remove(tctx->sysram);\n+\t\t\tif (!rc)\n+\t\t\t\tbreak;\n+\t\t\tif (++retries > 60) {\n+\t\t\t\tpr_err(\"cxl_compression: memory offline failed after %d retries, giving up\\n\",\n+\t\t\t\t       retries);\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tpr_info(\"cxl_compression: memory offline failed (%d), retrying...\\n\",\n+\t\t\t\trc);\n+\t\t\tmsleep(1000);\n+\t\t}\n+\t}\n+}\n+\n+static void cxl_compression_post_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\tstruct cxl_flush_ctx *ctx = tctx->flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tint cpu;\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\t/* cram_unregister_private_node already called in pre_teardown */\n+\n+\tif (ctx->flush_thread) {\n+\t\tkthread_stop(ctx->flush_thread);\n+\t\tctx->flush_thread = NULL;\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\tcancel_work_sync(&pcpu->overflow_work);\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tif (buf && buf->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, buf);\n+\n+\t\tif (pcpu->overflow_spare && pcpu->overflow_spare->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares && ctx->kthread_spares[cpu]) {\n+\t\t\tbuf = ctx->kthread_spares[cpu];\n+\t\t\tif (buf->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, buf);\n+\t\t}\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(ctx->kthread_spares[cpu]);\n+\t}\n+\n+\tkfree(ctx->kthread_spares);\n+\tfree_percpu(ctx->pcpu);\n+\tflush_record_free(ctx->flush_record, ctx->flush_record_pages);\n+}\n+\n+/**\n+ * struct cxl_compression_ctx - Per-device context for compression driver\n+ * @mbox: CXL mailbox for issuing CCI commands\n+ * @pdev: PCI device\n+ * @flush_ctx: Flush context for deferred page reclamation\n+ * @tctx: Teardown context for devm actions\n+ * @sysram: Sysram device for offline+remove in remove path\n+ * @nid: NUMA node ID, NUMA_NO_NODE if unset\n+ * @cxlmd: The memdev associated with this context\n+ * @cxlr: Region created by this driver (NULL if pre-existing)\n+ * @cxled: Endpoint decoder with DPA allocated by this driver\n+ * @regions_converted: Number of regions successfully converted\n+ * @media_ops_supported: Device supports media operations zero (0x4402)\n+ */\n+struct cxl_compression_ctx {\n+\tstruct cxl_mailbox *mbox;\n+\tstruct pci_dev *pdev;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+\tstruct cxl_memdev *cxlmd;\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint regions_converted;\n+\tbool media_ops_supported;\n+};\n+\n+/*\n+ * Probe whether the device supports Media Operations Zero (0x4402).\n+ * Send a zero-count command, a conforming device returns SUCCESS,\n+ * a device that doesn't support it returns UNSUPPORTED (-ENXIO).\n+ */\n+static bool cxl_probe_media_ops_zero(struct cxl_mailbox *mbox,\n+\t\t\t\t     struct device *dev)\n+{\n+\tstruct cxl_media_op_input probe = {\n+\t\t.media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE,\n+\t\t.media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO,\n+\t\t.dpa_range_count = 0,\n+\t};\n+\tstruct cxl_mbox_cmd cmd = {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = &probe,\n+\t\t.size_in = sizeof(probe),\n+\t};\n+\tint rc;\n+\n+\trc = cxl_internal_send_cmd(mbox, &cmd);\n+\tif (rc) {\n+\t\tdev_info(dev,\n+\t\t\t \"media operations zero not supported (rc=%d), using inline zeroing\\n\",\n+\t\t\t rc);\n+\t\treturn false;\n+\t}\n+\n+\tdev_info(dev, \"media operations zero (0x4402) supported\\n\");\n+\treturn true;\n+}\n+\n+struct cxl_compression_wm_ctx {\n+\tstruct device *dev;\n+\tint nid;\n+};\n+\n+static irqreturn_t cxl_compression_lthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"lthresh watermark: pressuring node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, CRAM_PRESSURE_MAX);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static irqreturn_t cxl_compression_hthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"hthresh watermark: resuming node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, 0);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static int convert_region_to_sysram(struct cxl_region *cxlr,\n+\t\t\t\t    struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct device *dev = cxl_region_dev(cxlr);\n+\tstruct cxl_compression_wm_ctx *wm_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tresource_size_t region_start, region_size;\n+\tstruct range hpa_range;\n+\tint nid;\n+\tint irq;\n+\tint cpu;\n+\tint rc;\n+\n+\tif (cxl_region_mode(cxlr) != CXL_PARTMODE_RAM) {\n+\t\tdev_dbg(dev, \"skipping non-RAM region (mode=%d)\\n\",\n+\t\t\tcxl_region_mode(cxlr));\n+\t\treturn 0;\n+\t}\n+\n+\tdev_info(dev, \"converting region to sysram\\n\");\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to add sysram region: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\ttctx = devm_kzalloc(dev, sizeof(*tctx), GFP_KERNEL);\n+\tif (!tctx)\n+\t\treturn -ENOMEM;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_post_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Find the sysram child device for pre_teardown */\n+\tcomp_ctx->sysram = cxl_region_find_sysram(cxlr);\n+\tif (comp_ctx->sysram)\n+\t\ttctx->sysram = comp_ctx->sysram;\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tnid = phys_to_target_node(hpa_range.start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\tregion_start = hpa_range.start;\n+\tregion_size = range_len(&hpa_range);\n+\n+\tflush_ctx = devm_kzalloc(dev, sizeof(*flush_ctx), GFP_KERNEL);\n+\tif (!flush_ctx)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->base_pfn = PHYS_PFN(region_start);\n+\tflush_ctx->nr_pages = region_size >> PAGE_SHIFT;\n+\tflush_ctx->flush_record = flush_record_alloc(flush_ctx->nr_pages,\n+\t\t\t\t\t\t     &flush_ctx->flush_record_pages);\n+\tif (!flush_ctx->flush_record)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->mbox = comp_ctx->mbox;\n+\tflush_ctx->dev = dev;\n+\tflush_ctx->nid = nid;\n+\tflush_ctx->media_ops_supported = comp_ctx->media_ops_supported;\n+\n+\t/*\n+\t * Cap buffer at max DPA ranges that fit in one CCI payload.\n+\t * Header is 8 bytes (struct cxl_media_op_input), each range\n+\t * is 16 bytes (struct cxl_dpa_range).  The module parameter\n+\t * flush_buf_size can further limit this (0 = use hw max).\n+\t */\n+\tflush_ctx->buf_max = (flush_ctx->mbox->payload_size -\n+\t\t\t      sizeof(struct cxl_media_op_input)) /\n+\t\t\t     sizeof(struct cxl_dpa_range);\n+\tif (flush_buf_size && flush_buf_size < flush_ctx->buf_max)\n+\t\tflush_ctx->buf_max = flush_buf_size;\n+\tif (flush_ctx->buf_max == 0)\n+\t\tflush_ctx->buf_max = 1;\n+\n+\tdev_info(dev,\n+\t\t \"flush buffer: %u DPA ranges per command (payload %zu bytes, media_ops %s)\\n\",\n+\t\t flush_ctx->buf_max, flush_ctx->mbox->payload_size,\n+\t\t flush_ctx->media_ops_supported ? \"yes\" : \"no\");\n+\n+\tflush_ctx->pcpu = alloc_percpu(struct cxl_pcpu_flush);\n+\tif (!flush_ctx->pcpu)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->kthread_spares = kcalloc(nr_cpu_ids,\n+\t\t\t\t\t    sizeof(struct cxl_flush_buf *),\n+\t\t\t\t\t    GFP_KERNEL);\n+\tif (!flush_ctx->kthread_spares)\n+\t\tgoto err_pcpu_init;\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *active_buf, *overflow_buf, *spare_buf;\n+\n+\t\tactive_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!active_buf)\n+\t\t\tgoto err_pcpu_init;\n+\n+\t\toverflow_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!overflow_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tspare_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!spare_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tcxl_flush_buf_free(overflow_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\t\tpcpu->ctx = flush_ctx;\n+\t\trcu_assign_pointer(pcpu->active, active_buf);\n+\t\tpcpu->overflow_spare = overflow_buf;\n+\t\tINIT_WORK(&pcpu->overflow_work, cxl_flush_overflow_work);\n+\n+\t\tflush_ctx->kthread_spares[cpu] = spare_buf;\n+\t}\n+\n+\tflush_ctx->flush_thread = kthread_create_on_node(\n+\t\tcxl_flush_kthread_fn, flush_ctx, nid, \"cxl-flush/%d\", nid);\n+\tif (IS_ERR(flush_ctx->flush_thread)) {\n+\t\trc = PTR_ERR(flush_ctx->flush_thread);\n+\t\tflush_ctx->flush_thread = NULL;\n+\t\tgoto err_pcpu_init;\n+\t}\n+\twake_up_process(flush_ctx->flush_thread);\n+\n+\trc = cram_register_private_node(nid, cxlr,\n+\t\t\t\t\tcxl_compression_flush_cb, flush_ctx);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to register cram node %d: %d\\n\", nid, rc);\n+\t\tgoto err_pcpu_init;\n+\t}\n+\n+\ttctx->flush_ctx = flush_ctx;\n+\ttctx->nid = nid;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_pre_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcomp_ctx->flush_ctx = flush_ctx;\n+\tcomp_ctx->tctx = tctx;\n+\tcomp_ctx->nid = nid;\n+\n+\t/*\n+\t * Register watermark IRQ handlers on &pdev->dev for\n+\t * MSI-X vector 12 (lthresh) and vector 13 (hthresh).\n+\t */\n+\twm_ctx = devm_kzalloc(&pdev->dev, sizeof(*wm_ctx), GFP_KERNEL);\n+\tif (!wm_ctx)\n+\t\treturn -ENOMEM;\n+\n+\twm_ctx->dev = &pdev->dev;\n+\twm_ctx->nid = nid;\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_LTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_lthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-lthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register lthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_HTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_hthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-hthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register hthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\treturn 0;\n+\n+err_pcpu_init:\n+\tif (flush_ctx->flush_thread)\n+\t\tkthread_stop(flush_ctx->flush_thread);\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *buf;\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (flush_ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(flush_ctx->kthread_spares[cpu]);\n+\t}\n+\tkfree(flush_ctx->kthread_spares);\n+\tfree_percpu(flush_ctx->pcpu);\n+\tflush_record_free(flush_ctx->flush_record, flush_ctx->flush_record_pages);\n+\treturn rc ? rc : -ENOMEM;\n+}\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\tpdev_to_comp_ctx(to_pci_dev(cxlmd->dev.parent))->cxled = cxled;\n+\treturn cxlr;\n+}\n+\n+static int cxl_compression_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i, converted = 0, errors = 0;\n+\tint rc;\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\t/* Probe device for media operations zero support */\n+\tcomp_ctx->media_ops_supported =\n+\t\tcxl_probe_media_ops_zero(comp_ctx->mbox,\n+\t\t\t\t\t &cxlmd->dev);\n+\n+\tdev_info(&cxlmd->dev, \"compression attach: looking for regions\\n\");\n+\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) == CXL_PARTMODE_RAM) {\n+\t\t\trc = convert_region_to_sysram(regions[i], pdev);\n+\t\t\tif (rc)\n+\t\t\t\terrors++;\n+\t\t\telse\n+\t\t\t\tconverted++;\n+\t\t}\n+\t\tput_device(cxl_region_dev(regions[i]));\n+\t}\n+\n+\tif (converted > 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"converted %d regions to sysram (%d errors)\\n\",\n+\t\t\t converted, errors);\n+\t\treturn errors ? -EIO : 0;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"could not create RAM region: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = convert_region_to_sysram(cxlr, pdev);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to convert region to sysram: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tcomp_ctx->cxlr = cxlr;\n+\n+\tdev_info(&cxlmd->dev, \"created and converted region %s to sysram\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_compression_attach = {\n+\t.probe = cxl_compression_attach_probe,\n+};\n+\n+static int cxl_compression_probe(struct pci_dev *pdev,\n+\t\t\t\t const struct pci_device_id *id)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probing device\\n\");\n+\n+\tcomp_ctx = devm_kzalloc(&pdev->dev, sizeof(*comp_ctx), GFP_KERNEL);\n+\tif (!comp_ctx)\n+\t\treturn -ENOMEM;\n+\tcomp_ctx->nid = NUMA_NO_NODE;\n+\tcomp_ctx->pdev = pdev;\n+\n+\trc = xa_insert(&comp_ctx_xa, (unsigned long)pdev, comp_ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_compression_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&comp_ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_compression_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = xa_erase(&comp_ctx_xa,\n+\t\t\t\t\t\t\t(unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: removing device\\n\");\n+\n+\tif (!comp_ctx || comp_ctx->nid == NUMA_NO_NODE)\n+\t\treturn;\n+\n+\t/*\n+\t * Destroy the region, devm actions on the region device handle teardown\n+\t * in registration-reverse order:\n+\t *   1. pre_teardown:  cram_unregister + retry-forever memory offline\n+\t *   2. sysram_unregister: device_unregister (sysram->res is NULL\n+\t *      after pre_teardown, so cxl_sysram_release skips hotplug)\n+\t *   3. post_teardown: kthread stop, flush cleanup\n+\t *\n+\t * PCI MMIO is still live so CCI commands in post_teardown work.\n+\t */\n+\tif (comp_ctx->cxlr) {\n+\t\tcxl_destroy_region(comp_ctx->cxlr);\n+\t\tcomp_ctx->cxlr = NULL;\n+\t}\n+\n+\tif (comp_ctx->cxled) {\n+\t\tcxl_dpa_free(comp_ctx->cxled);\n+\t\tcomp_ctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_compression_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ /* terminate list */ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_compression_pci_tbl);\n+\n+static struct pci_driver cxl_compression_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_compression_pci_tbl,\n+\t.probe\t\t= cxl_compression_probe,\n+\t.remove\t\t= cxl_compression_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_compression_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Compression Memory Driver with SysRAM regions\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David expressed concern about adding special-casing for private memory nodes, similar to ZONE_DEVICE, and suggested discussing the topic further.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concern",
                "special-casing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm concerned about adding more special-casing (similar to what we \nalready added for ZONE_DEVICE) all over the place.\n\nLike the whole folio_managed_() stuff in mprotect.c\n\nHaving that said, sounds like a reasonable topic to discuss.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged a concern about the semantics of zone_device hooks and proposed two alternative solutions: reusing vma_wants_writenotify() or adding a new vma flag to track protected/device pages.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "proposed alternative solutions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It's a valid concern - and is why I tried to re-use as many of the\nzone_device hooks as possible.  It does not seem zone_device has quite\nthe same semantics for a case like this, so I had to make something new.\n\nDEVICE_COHERENT injects a temporary swap entry to allow the device to do\na large atomic operation - then the page table is restored and the CPU\nis free to change entries as it pleases.\n\nAnother option would be to add the hook to vma_wants_writenotify()\ninstead of the page table code - and mask MM_CP_TRY_CHANGE_WRITABLE.\n\nThis would require adding a vma flag - or maybe a count of protected /\ndevice pages.\n\nint mprotect_fixup() {\n    ...\n    if (vma_wants_manual_pte_write_upgrade(vma))\n        mm_cp_flags |= MM_CP_TRY_CHANGE_WRITABLE;\n}\n\nbool vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)\n{\n    if (vma->managed_wrprotect)\n        return true;\n}\n\nThat would localize the change in folio_managed_fixup_migration_pte() :\n\nstatic inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n                                                      pte_t pte,\n                                                      pte_t old_pte,\n                                                      struct vm_area_struct *vma)\n{\n    ...\n    } else if (folio_managed_wrprotect(page_folio(new))) {\n        pte = pte_wrprotect(pte);\n+       atomic_inc(&vma->managed_wrprotect);\n    }\n    return pte;\n}\n\nThis would cover both the huge_memory.c and mprotect, and maybe that's\njust generally cleaner? I can try that to see if it actually works.\n\n~Gregory",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that existing hooks can be used for write protection and agreed to remove redundant code from page table walks.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged existing solution",
                "agreed to simplify"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "scratch all this - existing hooks exist for exactly this purpose:\n\n\tcan_change_[pte|pmd]_writable()\n\nSurprised I missed this.\n\nI can clean this up to remove it from the page table walks.\n\nStill valid to question whether we want this, but at least the hook\nlives with other write-protect hooks now.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alistair Popple",
              "summary": "Reviewer Alistair Popple expressed concerns that N_MEMORY_PRIVATE may not be the best solution for reusing the existing mm buddy allocator and suggested considering alternative approaches such as adapting DRM's standalone buddy allocator, while also noting that device memory exposure to userspace is an interesting aspect of the series.\n\nThe reviewer agrees that the patch provides a standard interface to userspace for managing device memory and suggests using the existing NUMA APIs as a reasonable approach.\n\nReviewer Alistair Popple noted that the proposed cxl_compression driver is similar to ZONE_DEVICE and questioned why it cannot be extended instead of duplicating code, pointing out a potential lock ordering issue with reclaim paths when using pgmap for ZONE_DEVICE pages. He suggested exploring alternative storage options such as page_ext or considering the future replacement of struct page with folios.\n\nReviewer suggested that the cxl_compression PCI driver is similar to existing ZONE_DEVICE methods and proposed building on those instead of introducing a new feature set.\n\nReviewer Alistair Popple noted that the implementation duplicates a lot of hooks, similar to those provided by ZONE_DEVICE, and requested further discussion on this aspect.\n\nReviewer questioned whether allocation must be handled by the mm allocator, suggesting that a device allocator library could be written or reused from drm_buddy.c\n\nThe reviewer questioned the characterization of ZONE_DEVICE pages as not being real struct pages, suggesting that perspective on this may vary depending on one's role in the mm subsystem, and asked for clarification on what limitations are actually being addressed.\n\nReviewer suggested that ZONE_DEVICE_COHERENT could be extended to support the use case, proposing a couple of extra dev_pagemap_ops and LRU access as a potential solution.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "alternative solutions",
                "agreement",
                "endorsement",
                "suggested alternatives",
                "duplicates hooks",
                "similar to ZONE_DEVICE",
                "clarification requested",
                "questioning characterization"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Having had to re-implement entire portions of mm/ in a driver I agree this isn't\nsomething anyone sane should do :-) However aspects of ZONE_DEVICE were added\nprecisely to help with that so I'm not sure N_MEMORY_PRIVATE is the only or best\nway to do that.\n\nBased on our discussion at LPC I believe one of the primary motivators here was\nto re-use the existing mm buddy allocator rather than writing your own. I remain\nto be convinced that alone is justification enough for doing all this - DRM for\nexample already has quite a nice standalone buddy allocator (drm_buddy.c) that\ncould presumably be used, or adapted for use, by any device driver.\n\nThe interesting part of this series (which I have skimmed but not read in\ndetail) is how device memory gets exposed to userspace - this is something that\nexisting ZONE_DEVICE implementations don't address, instead leaving it up to\ndrivers and associated userspace stacks to deal with allocation, migration, etc.\n\n---\n\nThis is I think is one of the key things that should be enabled - providing a\nstandard interface to userspace for managing device memory. The existing NUMA\nAPIs do seem like a reasonable way to do this.\n\n---\n\nOne does not have to squint too hard to see that the above is not so different\nfrom what ZONE_DEVICE provides today via dev_pagemap_ops(). So I think I think\nit would be worth outlining why the existing ZONE_DEVICE mechanism can't be\nextended to provide these kind of services.\n\nThis seems to add a bunch of code just to use NODE_DATA instead of page->pgmap,\nwithout really explaining why just extending dev_pagemap_ops wouldn't work. The\nobvious reason is that if you want to support things like reclaim, compaction,\netc. these pages need to be on the LRU, which is a little bit hard when that\nfield is also used by the pgmap pointer for ZONE_DEVICE pages.\n\nBut it might be good to explore other options for storing the pgmap - for\nexample page_ext could be used.  Or I hear struct page may go away in place of\nfolios any day now, so maybe that gives us space for both :-)\n\n---\n\nThe above also looks pretty similar to the existing ZONE_DEVICE methods for\ndoing this which is another reason to argue for just building up the feature set\nof the existing boondoggle rather than adding another thingymebob.\n\nIt seems the key thing we are looking for is:\n\n1) A userspace API to allocate/manage device memory (ie. move_pages(), mbind(),\netc.)\n\n2) Allowing reclaim/LRU list processing of device memory.\n\n---\n\ndiscussion (hopefully I can make it to LSFMM). Mostly I'm interested in the\nimplementation as this does on the surface seem to sprinkle around and duplicate\na lot of hooks similar to what ZONE_DEVICE already provides.\n\n---\n\nFor basic allocation I agree this is the case. But there's no reason some device\nallocator library couldn't be written. Or in fact as pointed out above reuse the\nalready existing one in drm_buddy.c.  So would be interested to hear arguments\nfor why allocation has to be done by the mm allocator and/or why an allocation\nlibrary wouldn't work here given DRM already has them.\n\n---\n\nZONE_DEVICE pages are in fact real struct pages, but I will concede that\nperspective probably depends on which bits of the mm you play in. The real\nlimitations you seem to be addressing is more around how we get these pages in\nan LRU, or are there other limitations?\n\n---\n\nWhat I'd like to explore is why ZONE_DEVICE_COHERENT couldn't just be extended\nto support your usecase? It seems a couple of extra dev_pagemap_ops and being\nable to go on the LRU would get you there.\n\n - Alistair",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author acknowledges that using ZONE_DEVICE is insufficient for N_MEMORY_PRIVATE, as it introduces unnecessary complexity. They propose reusing the buddy allocator instead, which would simplify the implementation and eliminate issues related to zones.\n\nThe author explains that the callback similarity between ZONE_DEVICE and private nodes is intentional, as they require the same set of hooks but with different defaults. They argue that extending ZONE_DEVICE into these areas would be cumbersome and inefficient, and that the current implementation is a more straightforward solution.\n\nAuthor addressed a concern about the per-page pgmap and device-to-node mappings, agreeing that NODE_DATA is the right direction regardless of struct page's future or zone it lives in.\n\nThe author acknowledges that implementing mempolicy support for N_MEMORY_PRIVATE is more complex than initially thought, explaining that it requires adding code to vma_alloc_folio_noprof and dealing with ZONE_DEVICE's overloaded nature. They suggest two options: putting pages in the buddy or adding pgmap->device_alloc() callbacks at every allocation site.\n\nAuthor acknowledged reviewer's concern about reusing mm/ services and explained that using the buddy underpins the rest of these services, making it a more efficient approach.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a fix is needed",
                "proposes an alternative solution",
                "acknowledges feedback",
                "provides explanation",
                "agreed with reviewer's suggestion",
                "provided explanation",
                "acknowledges complexity",
                "suggests two options",
                "acknowledged",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree that buddy-access alone is insufficient justification, it\nstarted off that way - but if you want mempolicy/NUMA UAPI access,\nit turns into \"Re-use all of MM\" - and that means using the buddy.\n\nI also expected ZONE_DEVICE vs NODE_DATA to be the primary discussion,\n\nI raise replacing it as a thought experiment, but not the proposal.\n\nThe idea that drm/ is going to switch to private nodes is outside the\nrealm of reality, but part of that is because of years of infrastructure\nbuilt on the assumption that re-using mm/ is infeasible.\n\nBut, lets talk about DEVICE_COHERENT\n\n---\n\nDEVICE_COHERENT is the odd-man out among ZONE_DEVICE modes. The others\nuse softleaf entries and don't allow direct mappings.\n\n(DEVICE_PRIVATE sort of does if you squint, but you can also view that\n a bit like PROT_NONE or read-only controls to force migrations).\n\nIf you take DEVICE_COHERENT and:\n\n- Move pgmap out of the struct page (page_ext, NODE_DATA, etc) to free\n  the LRU list_head\n- Put pages in the buddy (free lists, watermarks, managed_pages) or add\n  pgmap->device_alloc() at every allocation callsite / buddy hook\n- Add LRU support (aging, reclaim, compaction)\n- Add isolated gating (new GFP flag and adjusted zonelist filtering)\n- Add new dev_pagemap_ops callbacks for the various mm/ features\n- Audit evey folio_is_zone_device() to distinguish zone device modes\n\n... you've built N_MEMORY_PRIVATE inside ZONE_DEVICE. Except now\npage_zone(page) returns ZONE_DEVICE - so you inherit the wrong\ndefaults at every existing ZONE_DEVICE check. \n\nSkip-sites become things to opt-out of instead of opting into.\n\nYou just end up with\n\nif (folio_is_zone_device(folio))\n    if (folio_is_my_special_zone_device())\n    else ....\n\nand this just generalizes to\n\nif (folio_is_private_managed(folio))\n    folio_managed_my_hooked_operation()\n\nSo you get the same code, but have added more complexity to ZONE_DEVICE.\n\nI don't think that's needed if we just recognize ZONE is the wrong\nabstraction to be operating on.\n\nHonestly, even ZONE_MOVABLE becomes pointless with N_MEMORY_PRIVATE\nif you disallow longterm pinning - because the managing service handles\nallocations (it has to inject GFP_PRIVATE to get access) or selectively\nenables the mm/ services it knows are safe (mempolicy).\n\nEven if you allow longterm pinning, if your service controls what does\nthe pinning it can still be reclaimable - just manually (killing\nprocesses) instead of letting hotplug do it via migration.\n\nIf your service only allocates movable pages - your ZONE_NORMAL is\neffectively ZONE_MOVABLE.  \n\nIn some cases we use ZONE_MOVABLE to prevent the kernel from allocating\nmemory onto devices (like CXL).  This means struct page is forced to\ntake up DRAM or use memmap_on_memory - meaning you lose high-value\ncapacity or sacrifice contiguity (less huge page support).\n\nThis entire problem can evaporate if you can just use ZONE_NORMAL.\n\nThere are a lot of benefits to just re-using the buddy like this.\n\nZones are the wrong abstraction and cause more problems.\n\n---\n\nYou don't have to squint because it was deliberate :]\n\nThe callback similarity is the feature - they're the same logical\noperations.  The difference is the direction of the defaults.\n\nExtending ZONE_DEVICE into these areas requires the same set of hooks,\nplus distinguishing \"old ZONE_DEVICE\" from \"new ZONE_DEVICE\".\n\nWhere there are new injection sites, it's because ZONE_DEVICE opts\nout of ever touching that code in some other silently implied way.\n\nFor example, reclaim/compaction doesn't run because ZONE_DEVICE doesn't\nadd to managed_pages (among other reasons).\n\nYou'd have to go figure out how to hack those things into ZONE_DEVICE \n*and then* opt every *other* ZONE_DEVICE mode *back out*.\n\nSo you still end up with something like this anyway:\n\nstatic inline bool folio_managed_handle_fault(struct folio *folio,\n                                              struct vm_fault *vmf,\n                                              enum pgtable_level level,\n                                              vm_fault_t *ret)\n{\n        /* Zone device pages use swap entries; handled in do_swap_page */\n        if (folio_is_zone_device(folio))\n                return false;\n\n        if (folio_is_private_node(folio))\n\t\t...\n        return false;\n}\n\n---\n\nIf NUMA is the interface we want, then NODE_DATA is the right direction\nregardless of struct page's future or what zone it lives in.\n\nThere's no reason to keep per-page pgmap w/ device-to-node mappings.\n\nYou can have one driver manage multiple devices with the same numa node\nif it uses the same owner context (PFN already differentiates devices).\n\nThe existing code allows for this.\n\n---\n\nOn (1): ZONE_DEVICE NUMA UAPI is harder than it looks from the surface\n\nMuch of the kernel mm/ infrastructure is written on top of the buddy and\nexpects N_MEMORY to be the sole arbiter of \"Where to Acquire Pages\".\n\nMempolicy depends on:\n   - Buddy support or a new alloc hook around the buddy\n\n   - Migration support (mbind() after allocation migrates)\n     - Migration also deeply assumes buddy and LRU support\n\n   - Changing validations on node states\n     - mempolicy checks N_MEMORY membership, so you have to hack\n       N_MEMORY onto ZONE_DEVICE\n       (or teach it about a new node state... N_MEMORY_PRIVATE)\n\n\nGetting mempolicy to work with N_MEMORY_PRIVATE amounts to adding 2\nlines of code in vma_alloc_folio_noprof:\n\nstruct folio *vma_alloc_folio_noprof(gfp_t gfp, int order,\n                                     struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr)\n{\n        if (pol->flags & MPOL_F_PRIVATE)\n                gfp |= __GFP_PRIVATE;\n\n        folio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n\t/* Woo! I faulted a DEVICE PAGE! */\n}\n\nBut this requires the pages to be managed by the buddy.\n\nThe rest of the mempolicy support is around keeping sane nodemasks when\nthings like cpuset.mems rebinds occur and validating you don't end up\nwith private nodes that don't support mempolicy in your nodemask.\n\nYou have to do all of this anyway, but with the added bonus of fighting\nwith the overloaded nature of ZONE_DEVICE at every step.\n\n==========\n\nOn (2): Assume you solve LRU. \n\nZone Device has no free lists, managed_pages, or watermarks.\n\nkswapd can't run, compaction has no targets, vmscan's pressure model\ndoesn't function.  These all come for free when the pages are\nbuddy-managed on a real zone.  Why re-invent the wheel?\n\n==========\n\nSo you really have two options here:\n\na) Put pages in the buddy, or\n\nb) Add pgmap->device_alloc() callbacks at every allocation site that\n   could target a node:\n     - vma_alloc_folio\n     - alloc_migration_target\n     - alloc_demote_folio\n     - alloc_pages_node\n     - alloc_contig_pages\n     - list goes on\n\nOr more likely - hooking get_page_from_freelist.  Which at that\npoint... just use the buddy?  You're already deep in the hot path.\n\n---\n\nUsing the buddy underpins the rest of mm/ services we want to re-use.\n\nThat's basically it.  Otherwise you have to inject hooks into every\nsurface that touches the buddy...\n\n... or in the buddy (get_page_from_freelist), at which point why not\njust use the buddy?\n\n~Gregory",
              "reply_to": "Alistair Popple",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author considered reviewer's suggestion to simplify patch by removing N_MEMORY_PRIVATE and instead checking NODE_DATA(target_nid)->private, agreeing to explore this alternative.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "considering alternative approach",
                "agrees to look at it more"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This gave me something to chew on\n\nI think this can be done without introducing N_MEMORY_PRIVATE and just\nchecking:   NODE_DATA(target_nid)->private\n\nmeaning these nodes can just be N_MEMORY with the same isolations.\n\nI'll look at this a bit more.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC][RFC PATCH v4 00/27] Private Memory Nodes (w/ Compressed RAM)",
          "message_id": "aZ-MnVVNGG_cOvxE@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ-MnVVNGG_cOvxE@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-25T23:58:27Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Private Memory Nodes (w/ Compressed RAM) patch series, focusing on memory management and allocation for specific services.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "Expressed concerns about generalizing DRM's infrastructure and suggested exploring alternative approaches for memory management. Proposed using Private-CMA to create service-dedicated memory pools directly from DRAM.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement over generalization",
                "suggestion to explore alternative approaches"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v5 3/3] hw/cxl: Add a performant (and correct) path for the non interleaved cases",
          "message_id": "aZ9c0yYcvKf5a0j7@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ9c0yYcvKf5a0j7@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-25T20:35:02Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "A patch to add a performant and correct path for non-interleaved cases in the CXL hardware driver.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "Identified a potential issue with the patch being flagged by kreview, but considered it reasonable given other examples in the tree. Mentioned that they will try to test the other commits soon.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "kreview flagged this, seems reasonable given other examples in the tree"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alireza Sanaee",
              "summary": "Confirmed that the patch's approach is not correct and suggested using the unparent API to fix the issue. Offered to send another revision with this change.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "This was actually my question as well, but I tested with a scenario in which I created a region with its alias..."
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "Gregory Price",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v4 0/3] hw/cxl: Add a performant (and correct) path for the non interleaved cases",
          "message_id": "aZ8pZXR5Jv4_uB4q@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ8pZXR5Jv4_uB4q@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-25T16:55:17Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch author is revising their submission due to a merge conflict issue.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "Identified a merge conflict issue with the patch, suggested double-checking the actual merge base.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "Alireza Sanaee",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC][RFC PATCH v4 00/27] Private Memory Nodes (w/ Compressed RAM)",
          "message_id": "aZ8KjxdPS077aFcq@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ8KjxdPS077aFcq@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-25T14:43:33Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Private Memory Nodes (w/ Compressed RAM) patch series, focusing on memory management and allocation for specific services.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "Expressed concerns about generalizing DRM's infrastructure and suggested exploring alternative approaches for memory management. Proposed using Private-CMA to create service-dedicated memory pools directly from DRAM.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement over generalization",
                "suggestion to explore alternative approaches"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/2] cxl/region: fix region leak when attach_target fails in cxl_add_to_region",
          "message_id": "aZ5TfEeqULEGXo4N@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ5TfEeqULEGXo4N@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-25T01:42:23Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch fixes a region leak in the CXL driver when attaching a target fails. When `attach_target` returns an error, the auto-discovered region remains registered and consumes HPA resources without ever reaching a committed state. The patch tracks whether the region was created by checking the return value of `cxl_add_to_region`, and if it was not created successfully, it calls `drop_region` to unregister the region and release the HPA resource. This prevents subsequent region creation attempts from failing due to reserved HPA ranges.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "Author addressed a concern about device_attach() being called on auto-discovered regions when a custom attach callback is present. They explained that this can lead to dax memory being left online due to dax_kmem refusing to offline during its remove path. The author agreed to skip device_attach() in such cases, with the custom attach callback responsible for setting up the region afterwards.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a CXL memdev has a custom attach callback, cxl_add_to_region()\nshould not call device_attach() on the auto-discovered region.\n\nThe default device_attach() binds the dax driver, which may online\nmemory via dax_kmem.  The custom attach callback then has to tear down\nthe dax stack to convert the region to sysram, but dax_kmem refuses to\noffline memory during its remove path, leaving regions stuck online.\n\nSkip device_attach() when cxlmd->attach is set.  The attach callback\nis responsible for setting up the region after auto-discovery completes\n(e.g. adding it as sysram directly).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/region.c | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 276046d49f88..e5edeabd9262 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -3971,6 +3971,12 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n \t}\n \n \tif (attach) {\n+\t\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n+\n+\t\t/* Skip device_attach if memdev has is own attach callback */\n+\t\tif (cxlmd->attach)\n+\t\t\treturn 0;\n+\n \t\t/*\n \t\t * If device_attach() fails the range may still be active via\n \t\t * the platform-firmware memory map, otherwise the driver for\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author is pushing back against the review, indicating that the patch should be disregarded because it uses a function introduced by another developer, and instead pointing to an alternative solution.",
              "sentiment": "CONTENTIOUS",
              "sentiment_signals": [
                "pushing back",
                "disregard this patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "BAH - disregard this patch, it uses drop_region which is introduced by\nAlejandro here:\n\nhttps://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/",
              "reply_to": "",
              "message_date": "2026-02-21",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alison Schofield",
              "summary": "Reviewer noted that the patch's current implementation drops the region only when attach_target() fails immediately after creation, but not if it fails later in the process, and suggested revisiting a previous patch that unregisters auto-created regions on assembly failure.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I see you dropping this, perhaps just for the moment, because\nthe drop_region() you wanted to use is not available yet.\n\nThis looks a lot like \n\thttps://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/\n\tcxl/region: Unregister auto-created region when assembly fails\n\tWhen auto-created region assembly fails the region remains registered\n\tbut disabled. The region continues to reserve its memory resource,\n\tpreventing DAX from registering the memory.\n\tUnregister the region on assembly failure to release the resource.\n\nAnd the review comments on that one, or at least on that thread in\ngeneral, was to leave all the broken things in place.\nI didn't agree with that, and hope to see this version move ahead\nwhen you have the drop_region you need.\n\n-- Alison",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that the patch is not currently useful due to lack of usage, but did not commit to revising or removing it.\n\nThe author acknowledged that the patch addresses a specific issue related to auto-regions and manually created regions in a narrow failure scenario where two devices unbind/bind cycle at the same time, but emphasized it's a rare occurrence.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a limitation",
                "did not promise a fix",
                "acknowledged",
                "emphasized"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah it's not a particularly useful cleanup in the current\ninfrastructure because nothing actually uses this pattern (yet).\n\n---\n\nThe important note here is the difference between auto-regions and\nmanually created regions.  For auto-regions, you might have another\nendpoint show up looking for the partially created region - and then\njust go off and create it anyway because it thinks it was first.\n\nBut in my driver, i'm explicitly converting these auto-regions into\nother things, and if that fails it causes *all other* region creation to\nfail - even if it wasn't actually dependent on that original region.\n\nThis is only an issue if you have two devices unbind/bind cycling at\nthe same time - i.e.\n\n   echo 0000:d0:00.00 > cxl_pci/unbind\n   echo 0000:e0:00.00 > cxl_pci/unbind\n   echo 0000:d0:00.00 > mydriver/bind\n   echo 0000:e0:00.00 > mydriver/bind\n\nIf the platform has pre-programmed and locked the decoders, and one of\nthe two devices fails to probe and leaves a hanging partially\ncreated region, the other device will fail too.\n\nIt's a pretty narrow failure scenario.\n\n~Gregory",
              "reply_to": "Alison Schofield",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alison Schofield",
              "summary": "Reviewer noted that the patch's fix will eventually lead to another failure due to a similar issue, and requested clarification on why this specific case is handled differently.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "clarification needed"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That's by design, and that'll eventually fail too.\n\nBut - is see how your case is different. Thanks for the explanation.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer noted that the fix is good but may not provide sufficient debugging/tracing information in case of failure, and suggested further work on the region creation changes",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "further work required",
                "debugging/tracing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Feel free to add it to this series. I have started to send individual \nseries as you know but the part changing the region creation will \nrequire more work than the already sent.\n\nAbout this fix, it looks good to me, although I have to admit I'm a bit \nlost after following the discussion Allison points to. If we want to \nkeep the state of failure for forensics, not sure if the \ndebugging/tracing or default error info in this case will be enough.\n\nIn any case:\n\nReviewed-by: Alejandro Lucero <alucerop@amd.com>",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that keeping objects around causes more issues, expressed uncertainty about the frequency of the problem but did not commit to addressing it",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "lack of commitment"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah i don't quite follow the want to keep the objects around, it seems\nto cause more issues than it solves - but then i also don't think this\nis going to be a particularly common problem\n\n~Gregory",
              "reply_to": "Alejandro Palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v2 3/3] cxl: Move pci generic code",
          "message_id": "aZ8cp4_xhEspZxwX@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ8cp4_xhEspZxwX@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-25T16:01:08Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch proposes to remove a redundant call to cxl_pci_find_port() in the cxl_rcrb_get_comp_regs() function, simplifying the code and eliminating unnecessary port lookups.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "Raised a question about whether the redundant port lookup is actually necessary and suggested removing it to simplify the code.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "QUESTION"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v2 1/3] cxl: support Type2 when initializing cxl_dev_state",
          "message_id": "aZ8Ynok74P7QyDNU@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ8Ynok74P7QyDNU@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-25T15:43:29Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch proposes to remove a redundant call to cxl_pci_find_port() in the cxl_rcrb_get_comp_regs() function, simplifying the code and eliminating unnecessary port lookups.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "Raised a question about whether the redundant port lookup is actually necessary and suggested removing it to simplify the code.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "QUESTION"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v2 2/3] cxl: export internal structs for external Type2 drivers",
          "message_id": "aZ8WuBEAYBNIpSDY@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ8WuBEAYBNIpSDY@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-25T15:35:24Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch proposes to remove a redundant call to cxl_pci_find_port() in the cxl_rcrb_get_comp_regs() function, simplifying the code and eliminating unnecessary port lookups.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "Raised a question about whether the redundant port lookup is actually necessary and suggested removing it to simplify the code.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "QUESTION"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Jeff Layton",
      "primary_email": "jlayton@kernel.org",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v2 1/1] NFSD: move accumulated callback ops to per-net namespace",
          "message_id": "4b7553c45a9255bf5569a908d3aa7f18350e3312.camel@kernel.org",
          "url": "https://lore.kernel.org/all/4b7553c45a9255bf5569a908d3aa7f18350e3312.camel@kernel.org/",
          "date": "2026-02-25T12:40:32Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch moves accumulated callback operations to per-net namespace, addressing a bug where the nrvers field was incorrectly calculated.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "Identified a double-put bug where nrvers was incorrectly calculated due to an out-of-date code snippet. Suggested using ARRAY_SIZE(nn->nfsd_cb_versions) instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dai Ngo",
              "summary": "Confirmed the bug and suggested that it be fixed in version 3.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\nOn 2/25/26 4:40 AM, Jeff Layton wrote:\n> I had Claude take a look and it came up with this:\n>\n> Moves the callback RPC program, version, and stats structures from\n> global statics into struct nfsd_net so that each network namespace\n> gets its own callback counters and program definition.\n\nClause describes it much more precise than I did, pretty soon I'll\nbe out a job :). Fix in v3.\n\n>\n>> diff --git a/fs/nfsd/nfs4callback.c b/fs/nfsd/nfs4callback.c\n>> index aea8bdd2fdc49..aad4276b2f1bc 100644\n>> --- a/fs/nfsd/nfs4callback.c\n>> +++ b/fs/nfsd/nfs4callback.c\n> [ ... ]\n>\n>> +int nfsd_net_cb_stats_init(struct nfsd_net *nn)\n>> +{\n>> +     nn->nfsd_cb_version4.counts = kzalloc_objs(unsigned int,\n>> +                     ARRAY_SIZE(nfs4_cb_procedures), GFP_KERNEL);\n>> +     if (!nn->nfsd_cb_version4.counts)\n>> +             return -ENOMEM;\n> [ ... ]\n>\n>> +     nn->nfsd_cb_program.name = \"nfs4_cb\";\n>> +     nn->nfsd_cb_program.number = NFS4_CALLBACK;\n>> +     nn->nfsd_cb_program.nrvers = sizeof(struct rpc_version *) * 2;\n>                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n>\n> Should this be ARRAY_SIZE(nn->nfsd_cb_versions) instead?  The nrvers\n> field is documented as \"number of versions\" in struct rpc_program.\n> The old code used ARRAY_SIZE(nfs_cb_version) which evaluated to 2.\n>\n> sizeof(struct rpc_version *) is 8 on 64-bit, making nrvers 16 here\n> while the nfsd_cb_versions array only has 2 elements.  The version\n> bounds check in rpc_new_client() compares args->version against\n> program->nrvers, and rpc_proc_show() iterates from 0 to nrvers - 1\n> accessing program->version[i].\n\nYes, it's a bug. Fix in v3.\n\nThanks,\n-Dai\n\n>\n>> +     nn->nfsd_cb_program.version = &nn->nfsd_cb_versions[0];\n>> +     nn->nfsd_cb_program.pipe_dir_name = \"nfsd4_cb\";\n>> +     nn->nfsd_cb_program.stats = &nn->nfsd_cb_stat;\n>> +     nn->nfsd_cb_stat.program = &nn->nfsd_cb_program;\n>> +\n>> +     return 0;\n>> +}\n>                                                                                 \n\n",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v7 3/3] NFSD: Sign filehandles",
          "message_id": "c64ca6ae0f2d948f42b454c87ebcc58edee8bc3c.camel@kernel.org",
          "url": "https://lore.kernel.org/all/c64ca6ae0f2d948f42b454c87ebcc58edee8bc3c.camel@kernel.org/",
          "date": "2026-02-25T12:10:48Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch fixes a build failure issue caused by a function being defined in multiple places, which is not allowed in C. The problem arises from the fact that the NFSD code has been modified to sign filehandles, but this change introduced duplicate definitions of a function. The approach taken was to remove the redundant definition and ensure that the function is only declared once.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Benjamin Coddington",
              "summary": "reviewer admitted to not regenerating the patch after updating the code, and promised to resend it",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I did that thing again where I fixed the tree and didn't regenerate the patch.  I will resend this.\n\nBen",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-25",
              "message_id": "025AD576-CBDD-4477-916F-7397585351AA@hammerspace.com",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joanne Koong",
      "primary_email": "joannelkoong@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v4 19/25] fuse: add io-uring kernel-managed buffer ring",
          "message_id": "CAJnrk1b6z2oar_Zw89N275zfyU2+oZJwtozSdTPFw49x38FCOA@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1b6z2oar_Zw89N275zfyU2+oZJwtozSdTPFw49x38FCOA@mail.gmail.com/",
          "date": "2026-02-25T23:42:43Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch adds kernel-managed buffer ring support for io-uring in the FUSE filesystem.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joanne Koong",
              "summary": "Raised questions about the handling of 0-byte payloads and the order of fuse_uring_headers_cleanup() and fuse_uring_get_next_fuse_req().",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    }
  ]
}