{
  "date": "2026-02-20",
  "report_file": "2026-02-20_ollama_llama3.1-8b.html",
  "llm_backends": [
    [
      "ollama",
      "llama3.1:8b"
    ]
  ],
  "generation_time_seconds": 27728.77006340027,
  "developer_reports": [
    {
      "name": "Alexandre Ghiti",
      "primary_email": "alexghiti@rivosinc.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Boris Burkov",
      "primary_email": "boris@bur.io",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/1] btrfs: set BTRFS_ROOT_ORPHAN_CLEANUP during subvol create",
          "message_id": "718a3b0c2275324b9e287af7e4434f55a4a45901.1771529877.git.boris@bur.io",
          "url": "https://lore.kernel.org/all/718a3b0c2275324b9e287af7e4434f55a4a45901.1771529877.git.boris@bur.io/",
          "date": "2026-02-19T19:38:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-19",
          "patch_summary": "This patch addresses an issue where subvolumes with broken dentries cause problems when trying to delete or create new files/subvolumes over them. The problem is caused by a race condition between orphan cleanup and delayed iputs, leading to ENOENT errors. To fix this, the patch sets BTRFS_ROOT_ORPHAN_CLEANUP during subvolume creation, ensuring that orphan cleanup is run before other operations can interfere with it.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Reviewer questioned the wording of 'first does' and requested that the function name be fully qualified as btrfs_orphan_cleanup()",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification_request",
                "nitpick"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "What do you mean by \"first does\"? The first call to btrfs_orphan_cleanup()?\n\nAlso please use full function name, orphan_cleanup() -> btrfs_orphan_cleanup()",
              "reply_to": "Boris Burkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Filipe Manana",
              "summary": "The reviewer questioned the patch's description of a race condition between writeback and unlink, pointing out that the igrab() call actually increases the inode count from 1 to 2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested correction"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I find this confusing:   1 -> 1 + N ?\n\nShouldn't it be 1 -> 2, meaning the igrab() increased i_count from 1 to 2?",
              "reply_to": "Boris Burkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Filipe Manana",
              "summary": "Reviewer Filipe Manana requested a detailed explanation of where the decrement of parent->d_lockref.count occurs, specifically asking for the full call chain.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request_for_clarity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Where does this decrement of parent->d_lockref.count happens exactly?\nI don't see it immediately in iput(), or iput_final(). Please put the\nfull call chain.",
              "reply_to": "Boris Burkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Filipe Manana",
              "summary": "Reviewer noted that the patch is reasonable and looks good, providing a positive assessment.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Looks reasonable to me.\n\nThanks!",
              "reply_to": "Boris Burkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Dmitry Ilvokhin",
      "primary_email": "d@ilvokhin.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 4/4] mm: add tracepoints for zone lock",
          "message_id": "1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch series introduces a new abstraction for zone lock operations, adding thin wrappers around acquire and release operations. The goal is to prepare the code for future tracepoint instrumentation without modifying individual call sites. Centralizing zone lock operations behind these wrappers allows for easier addition of debugging hooks or other instrumentation in the future.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about direct zone lock acquire/release operations being replaced with the newly introduced wrappers. The author confirms that the changes are purely mechanical substitutions and no functional change is intended, but notes that the compaction path will be handled separately in a following patch due to additional non-trivial modifications.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about the lack of visibility into zone lock behavior, specifically the inability to identify long lock holders and correlate them with waiters. They explain that generic lock contention tracepoints are insufficient for this purpose and propose adding dedicated tracepoint instrumentation to the zone lock, following the existing mmap_lock tracing model. The author mentions that they have observed noticeable zone lock contention on production workloads at Meta and believe improved visibility is necessary for diagnosing performance issues in memory-intensive workloads.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a need for better instrumentation",
                "explained their reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern that the zone lock wrappers would break compact_lock_irqsave() by introducing struct compact_lock to abstract the underlying lock type, which carries a lock type enum and a union holding either a zone pointer or a raw spinlock_t pointer. The structure dispatches to the appropriate lock/unlock helper based on the lock type.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "introduced new abstraction",
                "dispatches to appropriate helper"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the need to drop the per-vswap spinlock before calling try_to_unmap() in the swapoff path, and agreed to restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series of patches to improve flow, specifically introducing zone lock wrappers and tracepoints together before converting users to the wrappers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls in this function, citing parity with compact helpers as a reason.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the function should not return a value and suggested replacing it with an if-else statement.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer Cheatham suggested moving certain wrapper changes, including the zone lock wrappers, to a later patch, arguing they are not relevant to the current patch and fit better in the last one.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested_reordering"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the patch series order, suggesting to squash patches 1 and 2 together, but noted that this is just a matter of personal preference.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "personal preference"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 3/4] mm: convert compaction to zone lock wrappers",
          "message_id": "3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch converts the compaction code to use zone lock wrappers, which are thin layers around the original zone lock operations. The goal is to prepare for future instrumentation or debugging without modifying individual call sites. The wrappers allow for centralizing zone lock operations and adding hooks in the future without affecting all users.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about mechanical substitutions of direct zone lock acquire/release operations with the newly introduced wrappers, acknowledging that locking semantics and ordering remain unchanged.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "unchanged"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about the abstraction of compact_lock_irqsave() away from raw spinlock_t, and is open to feedback on whether using a small tagged struct or splitting helpers is preferred.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "open_to_feedback",
                "asking_for_preferred_alternative"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed the issue that compaction's compact_lock_irqsave() function cannot operate directly on a spinlock_t pointer when the lock belongs to a zone, as it is now wrapped in a zone lock wrapper. The author introduced a struct compact_lock to abstract the underlying lock type and dispatch to the appropriate lock/unlock helper, with no functional change intended.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock acquire/release operations, explaining that the implementation follows the mmap_lock tracepoint pattern and that the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the patch series to introduce zone lock wrappers and tracepoints together before mechanically converting users, citing improved flow and reduced likelihood of forgetting previous changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "improved flow"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing zone lock wrappers and making direct calls in this function, citing parity with compact_do_lock_irqsave() helpers as a reason.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the function should not return a value and suggested replacing it with an if-else statement",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "You don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer Cheatham suggested moving zone lock wrapper changes that don't use compact_* to a later patch, arguing they aren't relevant to the current patch and fit better in the final one.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change",
                "suggested reordering"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to convert compaction to zone lock wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2) together instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "nit"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/4] mm: zone lock tracepoint instrumentation",
          "message_id": "cover.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/cover.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations, allowing for future tracepoint instrumentation without modifying individual call sites. The wrappers are centralizing zone lock operations, making it easier to add debugging hooks or instrumentation in the future. This is a preparatory step with no functional changes intended at this time.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about direct zone lock acquire/release operations being replaced with the newly introduced wrappers. The author confirms that the changes are purely mechanical substitutions, with no functional change intended, and that locking semantics and ordering remain unchanged.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing concerns about the effectiveness of generic lock contention tracepoints in diagnosing performance issues related to zone lock contention. They explain that these tracepoints lack visibility into lock hold times and make it difficult to identify long lock holders, which can be addressed by adding dedicated tracepoint instrumentation as proposed in this patch series. The author acknowledges the need for additional instrumentation and reiterates their goal of enabling detailed holder/waiter analysis and lock hold time measurements without affecting the fast path when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledges need for additional instrumentation",
                "reiterates goal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the zone lock wrappers interfering with compact_lock_irqsave() by introducing a struct compact_lock to abstract the underlying lock type, which will allow both zone locks and raw spinlocks to be used with the function.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no functional change intended",
                "abstracting lock type"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the fast path being affected when tracing is disabled, explaining that the implementation follows the mmap_lock tracepoint pattern and the fast path remains unaffected.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to improve flow, proposing a sequence of introducing zone lock wrappers, adding tracepoints, mechanically converting users, and converting compaction to use the wrappers, potentially squashing patches 1 and 4.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls in the function, citing parity with compact helpers as a reason.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer Cheatham pointed out that the function should not return a value and suggested replacing it with an if-else statement.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "You don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer Cheatham suggested moving wrapper changes that don't use compact_* to the last patch, arguing they are not relevant to this patch's additions and fit better in the final patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change",
                "minor suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggested change to squash patches (1) and (2), stating it's just a matter of personal taste, but also mentioned that the current series ordering is good as is.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "personal opinion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 2/4] mm: convert zone lock users to wrappers",
          "message_id": "7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch converts zone lock users to wrappers, which are thin layers around the original zone lock acquire and release operations. The goal is to prepare for future tracepoint instrumentation without modifying individual call sites. By centralizing zone lock operations behind wrappers, it becomes easier to add debugging hooks or instrumentation in the future without affecting existing code.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, and stated that the compaction path is left unchanged for now due to additional non-trivial modifications.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about the need for more detailed instrumentation of zone lock behavior, specifically to enable analysis of long lock holders and correlating them with waiters. They explain that generic tracepoints are insufficient and propose adding dedicated tracepoint instrumentation following the existing mmap_lock tracing model. The author also mentions restructuring compaction code to use the wrappers and requests feedback on an abstraction choice.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "request for feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the zone lock wrappers interfering with compact_lock_irqsave() by introducing a struct compact_lock to abstract the underlying lock type, which will allow for both zone and raw spinlock operations.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "introduction of new abstraction",
                "acknowledgment of potential issue"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock wrappers, explaining that the implementation follows the mmap_lock tracepoint pattern and that the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series of patches to improve flow, recommending that zone lock wrappers and tracepoints be introduced together before mechanically converting users to the wrappers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the function should not return a value and suggested replacing it with an if-else statement",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "You don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer Cheatham suggested moving zone lock wrapper changes that don't use compact_* to a later patch, arguing they aren't relevant to this patch's additions and fit better in the final patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change",
                "minor"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the patch, stating that it doesn't improve anything and is merely a matter of personal preference, but suggested squashing patches (1) and (2) together as an alternative",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "personal preference"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations in the Linux kernel. The goal is to centralize these operations, making it easier to add future instrumentation or debugging hooks without modifying individual call sites. No functional changes are intended at this time, as the wrappers are introduced in preparation for subsequent patches.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about direct zone lock acquire/release operations being replaced with the newly introduced wrappers. The changes are purely mechanical substitutions, and no functional change is intended. Locking semantics and ordering remain unchanged. The compaction path is left unchanged for now and will be handled separately in a subsequent patch due to additional non-trivial modifications.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged the need for mechanical substitutions",
                "no functional change intended"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing concerns about the lack of visibility into zone lock behavior, specifically lock hold times and long lock holders. They explain that their patch series aims to add dedicated tracepoint instrumentation to improve diagnostics in memory-intensive workloads. The author mentions that they have observed noticeable zone lock contention on production workloads at Meta and are seeking feedback on their approach.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging a problem",
                "seeking feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern that the introduction of zone lock wrappers would break compaction code, which uses compact_lock_irqsave() to operate on raw spinlocks. The author introduced a new struct compact_lock to abstract the underlying lock type and dispatch to the appropriate lock/unlock helper, ensuring compatibility with both zone locks and raw spinlocks.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "introduced new abstraction",
                "dispatches to correct lock helper"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock acquire/release operations, explaining that the implementation follows the mmap_lock tracepoint pattern and that the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series of patches to improve flow, specifically introducing zone lock wrappers and tracepoints together in a single patch before converting users to the wrappers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the zone_lock_irqsave() macro should not return a value and suggested replacing it with an if-else statement.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer Cheatham suggested moving zone lock wrapper changes that don't use compact_* functions to a later patch, as they are not directly relevant to the current patch's functionality and would fit better in the last patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change",
                "suggested reordering"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to introduce zone lock wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2) together instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "personal preference"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Gregory Price",
      "primary_email": "gourry@gourry.net",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 0/3] pull region-specific logic into new files",
          "message_id": "20260211204206.2171525-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260211204206.2171525-1-gourry@gourry.net/",
          "date": "2026-02-11T20:42:11Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch refactors the cxl_region management code in region.c to separate logic for pmem and dax regions into new files, region_pmem.c and region_dax.c respectively. This improves code organization, reduces development conflicts, and clarifies where changes occur. Additionally, a cleanup.h fixup is included to tidy up existing functions.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the region-specific logic in core/region.c being overloaded, and responded by moving pmem region logic from region.c into region_pmem.c to make it clear that this code only applies to pmem regions. The author confirmed that no functional changes were made.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "confirmed no functional changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "core/region.c is overloaded with per-region control logic (pmem, dax).\nMove pmem region logic from region.c into region_pmem.c to make it\nclear that this code only applies to pmem regions.\n\nNo functional changes.\n\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Fabio M. De Francesco <fabio.m.de.francesco@linux.intel.com>\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/Makefile      |   1 +\n drivers/cxl/core/core.h        |   1 +\n drivers/cxl/core/region.c      | 184 --------------------------------\n drivers/cxl/core/region_pmem.c | 189 +++++++++++++++++++++++++++++++++\n 4 files changed, 191 insertions(+), 184 deletions(-)\n create mode 100644 drivers/cxl/core/region_pmem.c\n\ndiff --git a/drivers/cxl/core/Makefile b/drivers/cxl/core/Makefile\nindex a639a9499972..d1484a0e5eb4 100644\n--- a/drivers/cxl/core/Makefile\n+++ b/drivers/cxl/core/Makefile\n@@ -16,6 +16,7 @@ cxl_core-y += pmu.o\n cxl_core-y += cdat.o\n cxl_core-$(CONFIG_TRACING) += trace.o\n cxl_core-$(CONFIG_CXL_REGION) += region.o\n+cxl_core-$(CONFIG_CXL_REGION) += region_pmem.o\n cxl_core-$(CONFIG_CXL_MCE) += mce.o\n cxl_core-$(CONFIG_CXL_FEATURES) += features.o\n cxl_core-$(CONFIG_CXL_EDAC_MEM_FEATURES) += edac.o\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 007b8aff0238..ced65a779a09 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -50,6 +50,7 @@ int cxl_get_poison_by_endpoint(struct cxl_port *port);\n struct cxl_region *cxl_dpa_to_region(const struct cxl_memdev *cxlmd, u64 dpa);\n u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n+int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex bd4c4a4a27da..1c322318d70e 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2757,46 +2757,6 @@ static ssize_t delete_region_store(struct device *dev,\n }\n DEVICE_ATTR_WO(delete_region);\n \n-static void cxl_pmem_region_release(struct device *dev)\n-{\n-\tstruct cxl_pmem_region *cxlr_pmem = to_cxl_pmem_region(dev);\n-\tint i;\n-\n-\tfor (i = 0; i < cxlr_pmem->nr_mappings; i++) {\n-\t\tstruct cxl_memdev *cxlmd = cxlr_pmem->mapping[i].cxlmd;\n-\n-\t\tput_device(&cxlmd->dev);\n-\t}\n-\n-\tkfree(cxlr_pmem);\n-}\n-\n-static const struct attribute_group *cxl_pmem_region_attribute_groups[] = {\n-\t&cxl_base_attribute_group,\n-\tNULL,\n-};\n-\n-const struct device_type cxl_pmem_region_type = {\n-\t.name = \"cxl_pmem_region\",\n-\t.release = cxl_pmem_region_release,\n-\t.groups = cxl_pmem_region_attribute_groups,\n-};\n-\n-bool is_cxl_pmem_region(struct device *dev)\n-{\n-\treturn dev->type == &cxl_pmem_region_type;\n-}\n-EXPORT_SYMBOL_NS_GPL(is_cxl_pmem_region, \"CXL\");\n-\n-struct cxl_pmem_region *to_cxl_pmem_region(struct device *dev)\n-{\n-\tif (dev_WARN_ONCE(dev, !is_cxl_pmem_region(dev),\n-\t\t\t  \"not a cxl_pmem_region device\\n\"))\n-\t\treturn NULL;\n-\treturn container_of(dev, struct cxl_pmem_region, dev);\n-}\n-EXPORT_SYMBOL_NS_GPL(to_cxl_pmem_region, \"CXL\");\n-\n struct cxl_poison_context {\n \tstruct cxl_port *port;\n \tint part;\n@@ -3432,64 +3392,6 @@ static int region_offset_to_dpa_result(struct cxl_region *cxlr, u64 offset,\n \treturn -ENXIO;\n }\n \n-static struct lock_class_key cxl_pmem_region_key;\n-\n-static int cxl_pmem_region_alloc(struct cxl_region *cxlr)\n-{\n-\tstruct cxl_region_params *p = &cxlr->params;\n-\tstruct cxl_nvdimm_bridge *cxl_nvb;\n-\tstruct device *dev;\n-\tint i;\n-\n-\tguard(rwsem_read)(&cxl_rwsem.region);\n-\tif (p->state != CXL_CONFIG_COMMIT)\n-\t\treturn -ENXIO;\n-\n-\tstruct cxl_pmem_region *cxlr_pmem __free(kfree) =\n-\t\tkzalloc(struct_size(cxlr_pmem, mapping, p->nr_targets), GFP_KERNEL);\n-\tif (!cxlr_pmem)\n-\t\treturn -ENOMEM;\n-\n-\tcxlr_pmem->hpa_range.start = p->res->start;\n-\tcxlr_pmem->hpa_range.end = p->res->end;\n-\n-\t/* Snapshot the region configuration underneath the cxl_rwsem.region */\n-\tcxlr_pmem->nr_mappings = p->nr_targets;\n-\tfor (i = 0; i < p->nr_targets; i++) {\n-\t\tstruct cxl_endpoint_decoder *cxled = p->targets[i];\n-\t\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n-\t\tstruct cxl_pmem_region_mapping *m = &cxlr_pmem->mapping[i];\n-\n-\t\t/*\n-\t\t * Regions never span CXL root devices, so by definition the\n-\t\t * bridge for one device is the same for all.\n-\t\t */\n-\t\tif (i == 0) {\n-\t\t\tcxl_nvb = cxl_find_nvdimm_bridge(cxlmd->endpoint);\n-\t\t\tif (!cxl_nvb)\n-\t\t\t\treturn -ENODEV;\n-\t\t\tcxlr->cxl_nvb = cxl_nvb;\n-\t\t}\n-\t\tm->cxlmd = cxlmd;\n-\t\tget_device(&cxlmd->dev);\n-\t\tm->start = cxled->dpa_res->start;\n-\t\tm->size = resource_size(cxled->dpa_res);\n-\t\tm->position = i;\n-\t}\n-\n-\tdev = &cxlr_pmem->dev;\n-\tdevice_initialize(dev);\n-\tlockdep_set_class(&dev->mutex, &cxl_pmem_region_key);\n-\tdevice_set_pm_not_required(dev);\n-\tdev->parent = &cxlr->dev;\n-\tdev->bus = &cxl_bus_type;\n-\tdev->type = &cxl_pmem_region_type;\n-\tcxlr_pmem->cxlr = cxlr;\n-\tcxlr->cxlr_pmem = no_free_ptr(cxlr_pmem);\n-\n-\treturn 0;\n-}\n-\n static void cxl_dax_region_release(struct device *dev)\n {\n \tstruct cxl_dax_region *cxlr_dax = to_cxl_dax_region(dev);\n@@ -3553,92 +3455,6 @@ static struct cxl_dax_region *cxl_dax_region_alloc(struct cxl_region *cxlr)\n \treturn cxlr_dax;\n }\n \n-static void cxlr_pmem_unregister(void *_cxlr_pmem)\n-{\n-\tstruct cxl_pmem_region *cxlr_pmem = _cxlr_pmem;\n-\tstruct cxl_region *cxlr = cxlr_pmem->cxlr;\n-\tstruct cxl_nvdimm_bridge *cxl_nvb = cxlr->cxl_nvb;\n-\n-\t/*\n-\t * Either the bridge is in ->remove() context under the device_lock(),\n-\t * or cxlr_release_nvdimm() is cancelling the bridge's release action\n-\t * for @cxlr_pmem and doing it itself (while manually holding the bridge\n-\t * lock).\n-\t */\n-\tdevice_lock_assert(&cxl_nvb->dev);\n-\tcxlr->cxlr_pmem = NULL;\n-\tcxlr_pmem->cxlr = NULL;\n-\tdevice_unregister(&cxlr_pmem->dev);\n-}\n-\n-static void cxlr_release_nvdimm(void *_cxlr)\n-{\n-\tstruct cxl_region *cxlr = _cxlr;\n-\tstruct cxl_nvdimm_bridge *cxl_nvb = cxlr->cxl_nvb;\n-\n-\tscoped_guard(device, &cxl_nvb->dev) {\n-\t\tif (cxlr->cxlr_pmem)\n-\t\t\tdevm_release_action(&cxl_nvb->dev, cxlr_pmem_unregister,\n-\t\t\t\t\t    cxlr->cxlr_pmem);\n-\t}\n-\tcxlr->cxl_nvb = NULL;\n-\tput_device(&cxl_nvb->dev);\n-}\n-\n-/**\n- * devm_cxl_add_pmem_region() - add a cxl_region-to-nd_region bridge\n- * @cxlr: parent CXL region for this pmem region bridge device\n- *\n- * Return: 0 on success negative error code on failure.\n- */\n-static int devm_cxl_add_pmem_region(struct cxl_region *cxlr)\n-{\n-\tstruct cxl_pmem_region *cxlr_pmem;\n-\tstruct cxl_nvdimm_bridge *cxl_nvb;\n-\tstruct device *dev;\n-\tint rc;\n-\n-\trc = cxl_pmem_region_alloc(cxlr);\n-\tif (rc)\n-\t\treturn rc;\n-\tcxlr_pmem = cxlr->cxlr_pmem;\n-\tcxl_nvb = cxlr->cxl_nvb;\n-\n-\tdev = &cxlr_pmem->dev;\n-\trc = dev_set_name(dev, \"pmem_region%d\", cxlr->id);\n-\tif (rc)\n-\t\tgoto err;\n-\n-\trc = device_add(dev);\n-\tif (rc)\n-\t\tgoto err;\n-\n-\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n-\t\tdev_name(dev));\n-\n-\tscoped_guard(device, &cxl_nvb->dev) {\n-\t\tif (cxl_nvb->dev.driver)\n-\t\t\trc = devm_add_action_or_reset(&cxl_nvb->dev,\n-\t\t\t\t\t\t      cxlr_pmem_unregister,\n-\t\t\t\t\t\t      cxlr_pmem);\n-\t\telse\n-\t\t\trc = -ENXIO;\n-\t}\n-\n-\tif (rc)\n-\t\tgoto err_bridge;\n-\n-\t/* @cxlr carries a reference on @cxl_nvb until cxlr_release_nvdimm */\n-\treturn devm_add_action_or_reset(&cxlr->dev, cxlr_release_nvdimm, cxlr);\n-\n-err:\n-\tput_device(dev);\n-err_bridge:\n-\tput_device(&cxl_nvb->dev);\n-\tcxlr->cxl_nvb = NULL;\n-\treturn rc;\n-}\n-\n static void cxlr_dax_unregister(void *_cxlr_dax)\n {\n \tstruct cxl_dax_region *cxlr_dax = _cxlr_dax;\ndiff --git a/drivers/cxl/core/region_pmem.c b/drivers/cxl/core/region_pmem.c\nnew file mode 100644\nindex 000000000000..f800407566d3\n--- /dev/null\n+++ b/drivers/cxl/core/region_pmem.c\n@@ -0,0 +1,189 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2022 Intel Corporation. All rights reserved. */\n+#include <linux/device.h>\n+#include <linux/slab.h>\n+#include <cxlmem.h>\n+#include <cxl.h>\n+#include \"core.h\"\n+\n+static void cxl_pmem_region_release(struct device *dev)\n+{\n+\tstruct cxl_pmem_region *cxlr_pmem = to_cxl_pmem_region(dev);\n+\tint i;\n+\n+\tfor (i = 0; i < cxlr_pmem->nr_mappings; i++) {\n+\t\tstruct cxl_memdev *cxlmd = cxlr_pmem->mapping[i].cxlmd;\n+\n+\t\tput_device(&cxlmd->dev);\n+\t}\n+\n+\tkfree(cxlr_pmem);\n+}\n+\n+static const struct attribute_group *cxl_pmem_region_attribute_groups[] = {\n+\t&cxl_base_attribute_group,\n+\tNULL\n+};\n+\n+const struct device_type cxl_pmem_region_type = {\n+\t.name = \"cxl_pmem_region\",\n+\t.release = cxl_pmem_region_release,\n+\t.groups = cxl_pmem_region_attribute_groups,\n+};\n+bool is_cxl_pmem_region(struct device *dev)\n+{\n+\treturn dev->type == &cxl_pmem_region_type;\n+}\n+EXPORT_SYMBOL_NS_GPL(is_cxl_pmem_region, \"CXL\");\n+\n+struct cxl_pmem_region *to_cxl_pmem_region(struct device *dev)\n+{\n+\tif (dev_WARN_ONCE(dev, !is_cxl_pmem_region(dev),\n+\t\t\t\t\"not a cxl_pmem_region device\\n\"))\n+\t\treturn NULL;\n+\treturn container_of(dev, struct cxl_pmem_region, dev);\n+}\n+EXPORT_SYMBOL_NS_GPL(to_cxl_pmem_region, \"CXL\");\n+static struct lock_class_key cxl_pmem_region_key;\n+\n+static int cxl_pmem_region_alloc(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_region_params *p = &cxlr->params;\n+\tstruct cxl_nvdimm_bridge *cxl_nvb;\n+\tstruct device *dev;\n+\tint i;\n+\n+\tguard(rwsem_read)(&cxl_rwsem.region);\n+\tif (p->state != CXL_CONFIG_COMMIT)\n+\t\treturn -ENXIO;\n+\n+\tstruct cxl_pmem_region *cxlr_pmem __free(kfree) =\n+\t\tkzalloc(struct_size(cxlr_pmem, mapping, p->nr_targets), GFP_KERNEL);\n+\tif (!cxlr_pmem)\n+\t\treturn -ENOMEM;\n+\n+\tcxlr_pmem->hpa_range.start = p->res->start;\n+\tcxlr_pmem->hpa_range.end = p->res->end;\n+\n+\t/* Snapshot the region configuration underneath the cxl_rwsem.region */\n+\tcxlr_pmem->nr_mappings = p->nr_targets;\n+\tfor (i = 0; i < p->nr_targets; i++) {\n+\t\tstruct cxl_endpoint_decoder *cxled = p->targets[i];\n+\t\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n+\t\tstruct cxl_pmem_region_mapping *m = &cxlr_pmem->mapping[i];\n+\n+\t\t/*\n+\t\t * Regions never span CXL root devices, so by definition the\n+\t\t * bridge for one device is the same for all.\n+\t\t */\n+\t\tif (i == 0) {\n+\t\t\tcxl_nvb = cxl_find_nvdimm_bridge(cxlmd->endpoint);\n+\t\t\tif (!cxl_nvb)\n+\t\t\t\treturn -ENODEV;\n+\t\t\tcxlr->cxl_nvb = cxl_nvb;\n+\t\t}\n+\t\tm->cxlmd = cxlmd;\n+\t\tget_device(&cxlmd->dev);\n+\t\tm->start = cxled->dpa_res->start;\n+\t\tm->size = resource_size(cxled->dpa_res);\n+\t\tm->position = i;\n+\t}\n+\n+\tdev = &cxlr_pmem->dev;\n+\tdevice_initialize(dev);\n+\tlockdep_set_class(&dev->mutex, &cxl_pmem_region_key);\n+\tdevice_set_pm_not_required(dev);\n+\tdev->parent = &cxlr->dev;\n+\tdev->bus = &cxl_bus_type;\n+\tdev->type = &cxl_pmem_region_type;\n+\tcxlr_pmem->cxlr = cxlr;\n+\tcxlr->cxlr_pmem = no_free_ptr(cxlr_pmem);\n+\n+\treturn 0;\n+}\n+\n+static void cxlr_pmem_unregister(void *_cxlr_pmem)\n+{\n+\tstruct cxl_pmem_region *cxlr_pmem = _cxlr_pmem;\n+\tstruct cxl_region *cxlr = cxlr_pmem->cxlr;\n+\tstruct cxl_nvdimm_bridge *cxl_nvb = cxlr->cxl_nvb;\n+\n+\t/*\n+\t * Either the bridge is in ->remove() context under the device_lock(),\n+\t * or cxlr_release_nvdimm() is cancelling the bridge's release action\n+\t * for @cxlr_pmem and doing it itself (while manually holding the bridge\n+\t * lock).\n+\t */\n+\tdevice_lock_assert(&cxl_nvb->dev);\n+\tcxlr->cxlr_pmem = NULL;\n+\tcxlr_pmem->cxlr = NULL;\n+\tdevice_unregister(&cxlr_pmem->dev);\n+}\n+\n+static void cxlr_release_nvdimm(void *_cxlr)\n+{\n+\tstruct cxl_region *cxlr = _cxlr;\n+\tstruct cxl_nvdimm_bridge *cxl_nvb = cxlr->cxl_nvb;\n+\n+\tscoped_guard(device, &cxl_nvb->dev) {\n+\t\tif (cxlr->cxlr_pmem)\n+\t\t\tdevm_release_action(&cxl_nvb->dev, cxlr_pmem_unregister,\n+\t\t\t\t\tcxlr->cxlr_pmem);\n+\t}\n+\tcxlr->cxl_nvb = NULL;\n+\tput_device(&cxl_nvb->dev);\n+}\n+\n+/**\n+ * devm_cxl_add_pmem_region() - add a cxl_region-to-nd_region bridge\n+ * @cxlr: parent CXL region for this pmem region bridge device\n+ *\n+ * Return: 0 on success negative error code on failure.\n+ */\n+int devm_cxl_add_pmem_region(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_pmem_region *cxlr_pmem;\n+\tstruct cxl_nvdimm_bridge *cxl_nvb;\n+\tstruct device *dev;\n+\tint rc;\n+\n+\trc = cxl_pmem_region_alloc(cxlr);\n+\tif (rc)\n+\t\treturn rc;\n+\tcxlr_pmem = cxlr->cxlr_pmem;\n+\tcxl_nvb = cxlr->cxl_nvb;\n+\n+\tdev = &cxlr_pmem->dev;\n+\trc = dev_set_name(dev, \"pmem_region%d\", cxlr->id);\n+\tif (rc)\n+\t\tgoto err;\n+\n+\trc = device_add(dev);\n+\tif (rc)\n+\t\tgoto err;\n+\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n+\t\t\tdev_name(dev));\n+\n+\tscoped_guard(device, &cxl_nvb->dev) {\n+\t\tif (cxl_nvb->dev.driver)\n+\t\t\trc = devm_add_action_or_reset(&cxl_nvb->dev,\n+\t\t\t\t\tcxlr_pmem_unregister,\n+\t\t\t\t\tcxlr_pmem);\n+\t\telse\n+\t\t\trc = -ENXIO;\n+\t}\n+\n+\tif (rc)\n+\t\tgoto err_bridge;\n+\n+\t/* @cxlr carries a reference on @cxl_nvb until cxlr_release_nvdimm */\n+\treturn devm_add_action_or_reset(&cxlr->dev, cxlr_release_nvdimm, cxlr);\n+\n+err:\n+\tput_device(dev);\n+err_bridge:\n+\tput_device(&cxl_nvb->dev);\n+\tcxlr->cxl_nvb = NULL;\n+\treturn rc;\n+}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the overloading of region control logic in core/region.c by moving CXL DAX region device logic into region_dax.c, confirming that no functional changes were made.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "no functional changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "core/region.c is overloaded with per-region control logic (pmem, dax).\nMove CXL DAX region device logic from region.c into region_dax.c.\n\nNo functional changes.\n\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Fabio M. De Francesco <fabio.m.de.francesco@linux.intel.com>\nAcked-by: Davidlohr Bueso <dave@stgolabs.net>\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/Makefile     |   1 +\n drivers/cxl/core/core.h       |   1 +\n drivers/cxl/core/region.c     |  99 ------------------------------\n drivers/cxl/core/region_dax.c | 109 ++++++++++++++++++++++++++++++++++\n 4 files changed, 111 insertions(+), 99 deletions(-)\n create mode 100644 drivers/cxl/core/region_dax.c\n\ndiff --git a/drivers/cxl/core/Makefile b/drivers/cxl/core/Makefile\nindex d1484a0e5eb4..d3ec8aea64c5 100644\n--- a/drivers/cxl/core/Makefile\n+++ b/drivers/cxl/core/Makefile\n@@ -16,6 +16,7 @@ cxl_core-y += pmu.o\n cxl_core-y += cdat.o\n cxl_core-$(CONFIG_TRACING) += trace.o\n cxl_core-$(CONFIG_CXL_REGION) += region.o\n+cxl_core-$(CONFIG_CXL_REGION) += region_dax.o\n cxl_core-$(CONFIG_CXL_REGION) += region_pmem.o\n cxl_core-$(CONFIG_CXL_MCE) += mce.o\n cxl_core-$(CONFIG_CXL_FEATURES) += features.o\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex ced65a779a09..a89e0d6d9e7b 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -50,6 +50,7 @@ int cxl_get_poison_by_endpoint(struct cxl_port *port);\n struct cxl_region *cxl_dpa_to_region(const struct cxl_memdev *cxlmd, u64 dpa);\n u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n+int devm_cxl_add_dax_region(struct cxl_region *cxlr);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n \n #else\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 1c322318d70e..0c37caa60f2a 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -3392,105 +3392,6 @@ static int region_offset_to_dpa_result(struct cxl_region *cxlr, u64 offset,\n \treturn -ENXIO;\n }\n \n-static void cxl_dax_region_release(struct device *dev)\n-{\n-\tstruct cxl_dax_region *cxlr_dax = to_cxl_dax_region(dev);\n-\n-\tkfree(cxlr_dax);\n-}\n-\n-static const struct attribute_group *cxl_dax_region_attribute_groups[] = {\n-\t&cxl_base_attribute_group,\n-\tNULL,\n-};\n-\n-const struct device_type cxl_dax_region_type = {\n-\t.name = \"cxl_dax_region\",\n-\t.release = cxl_dax_region_release,\n-\t.groups = cxl_dax_region_attribute_groups,\n-};\n-\n-static bool is_cxl_dax_region(struct device *dev)\n-{\n-\treturn dev->type == &cxl_dax_region_type;\n-}\n-\n-struct cxl_dax_region *to_cxl_dax_region(struct device *dev)\n-{\n-\tif (dev_WARN_ONCE(dev, !is_cxl_dax_region(dev),\n-\t\t\t  \"not a cxl_dax_region device\\n\"))\n-\t\treturn NULL;\n-\treturn container_of(dev, struct cxl_dax_region, dev);\n-}\n-EXPORT_SYMBOL_NS_GPL(to_cxl_dax_region, \"CXL\");\n-\n-static struct lock_class_key cxl_dax_region_key;\n-\n-static struct cxl_dax_region *cxl_dax_region_alloc(struct cxl_region *cxlr)\n-{\n-\tstruct cxl_region_params *p = &cxlr->params;\n-\tstruct cxl_dax_region *cxlr_dax;\n-\tstruct device *dev;\n-\n-\tguard(rwsem_read)(&cxl_rwsem.region);\n-\tif (p->state != CXL_CONFIG_COMMIT)\n-\t\treturn ERR_PTR(-ENXIO);\n-\n-\tcxlr_dax = kzalloc(sizeof(*cxlr_dax), GFP_KERNEL);\n-\tif (!cxlr_dax)\n-\t\treturn ERR_PTR(-ENOMEM);\n-\n-\tcxlr_dax->hpa_range.start = p->res->start;\n-\tcxlr_dax->hpa_range.end = p->res->end;\n-\n-\tdev = &cxlr_dax->dev;\n-\tcxlr_dax->cxlr = cxlr;\n-\tdevice_initialize(dev);\n-\tlockdep_set_class(&dev->mutex, &cxl_dax_region_key);\n-\tdevice_set_pm_not_required(dev);\n-\tdev->parent = &cxlr->dev;\n-\tdev->bus = &cxl_bus_type;\n-\tdev->type = &cxl_dax_region_type;\n-\n-\treturn cxlr_dax;\n-}\n-\n-static void cxlr_dax_unregister(void *_cxlr_dax)\n-{\n-\tstruct cxl_dax_region *cxlr_dax = _cxlr_dax;\n-\n-\tdevice_unregister(&cxlr_dax->dev);\n-}\n-\n-static int devm_cxl_add_dax_region(struct cxl_region *cxlr)\n-{\n-\tstruct cxl_dax_region *cxlr_dax;\n-\tstruct device *dev;\n-\tint rc;\n-\n-\tcxlr_dax = cxl_dax_region_alloc(cxlr);\n-\tif (IS_ERR(cxlr_dax))\n-\t\treturn PTR_ERR(cxlr_dax);\n-\n-\tdev = &cxlr_dax->dev;\n-\trc = dev_set_name(dev, \"dax_region%d\", cxlr->id);\n-\tif (rc)\n-\t\tgoto err;\n-\n-\trc = device_add(dev);\n-\tif (rc)\n-\t\tgoto err;\n-\n-\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n-\t\tdev_name(dev));\n-\n-\treturn devm_add_action_or_reset(&cxlr->dev, cxlr_dax_unregister,\n-\t\t\t\t\tcxlr_dax);\n-err:\n-\tput_device(dev);\n-\treturn rc;\n-}\n-\n static int match_root_decoder(struct device *dev, const void *data)\n {\n \tconst struct range *r1, *r2 = data;\ndiff --git a/drivers/cxl/core/region_dax.c b/drivers/cxl/core/region_dax.c\nnew file mode 100644\nindex 000000000000..c8dd2bd1d9b9\n--- /dev/null\n+++ b/drivers/cxl/core/region_dax.c\n@@ -0,0 +1,109 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/*\n+ * Copyright(c) 2022 Intel Corporation. All rights reserved.\n+ * Copyright(c) 2026 Meta Technologies Inc. All rights reserved.\n+ */\n+#include <linux/device.h>\n+#include <linux/slab.h>\n+#include <cxlmem.h>\n+#include <cxl.h>\n+#include \"core.h\"\n+\n+static void cxl_dax_region_release(struct device *dev)\n+{\n+\tstruct cxl_dax_region *cxlr_dax = to_cxl_dax_region(dev);\n+\n+\tkfree(cxlr_dax);\n+}\n+\n+static const struct attribute_group *cxl_dax_region_attribute_groups[] = {\n+\t&cxl_base_attribute_group,\n+\tNULL\n+};\n+\n+const struct device_type cxl_dax_region_type = {\n+\t.name = \"cxl_dax_region\",\n+\t.release = cxl_dax_region_release,\n+\t.groups = cxl_dax_region_attribute_groups,\n+};\n+\n+static bool is_cxl_dax_region(struct device *dev)\n+{\n+\treturn dev->type == &cxl_dax_region_type;\n+}\n+\n+struct cxl_dax_region *to_cxl_dax_region(struct device *dev)\n+{\n+\tif (dev_WARN_ONCE(dev, !is_cxl_dax_region(dev),\n+\t\t\t  \"not a cxl_dax_region device\\n\"))\n+\t\treturn NULL;\n+\treturn container_of(dev, struct cxl_dax_region, dev);\n+}\n+EXPORT_SYMBOL_NS_GPL(to_cxl_dax_region, \"CXL\");\n+\n+static struct lock_class_key cxl_dax_region_key;\n+\n+static struct cxl_dax_region *cxl_dax_region_alloc(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_region_params *p = &cxlr->params;\n+\tstruct cxl_dax_region *cxlr_dax;\n+\tstruct device *dev;\n+\n+\tguard(rwsem_read)(&cxl_rwsem.region);\n+\tif (p->state != CXL_CONFIG_COMMIT)\n+\t\treturn ERR_PTR(-ENXIO);\n+\n+\tcxlr_dax = kzalloc(sizeof(*cxlr_dax), GFP_KERNEL);\n+\tif (!cxlr_dax)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tcxlr_dax->hpa_range.start = p->res->start;\n+\tcxlr_dax->hpa_range.end = p->res->end;\n+\n+\tdev = &cxlr_dax->dev;\n+\tcxlr_dax->cxlr = cxlr;\n+\tdevice_initialize(dev);\n+\tlockdep_set_class(&dev->mutex, &cxl_dax_region_key);\n+\tdevice_set_pm_not_required(dev);\n+\tdev->parent = &cxlr->dev;\n+\tdev->bus = &cxl_bus_type;\n+\tdev->type = &cxl_dax_region_type;\n+\n+\treturn cxlr_dax;\n+}\n+\n+static void cxlr_dax_unregister(void *_cxlr_dax)\n+{\n+\tstruct cxl_dax_region *cxlr_dax = _cxlr_dax;\n+\n+\tdevice_unregister(&cxlr_dax->dev);\n+}\n+\n+int devm_cxl_add_dax_region(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_dax_region *cxlr_dax;\n+\tstruct device *dev;\n+\tint rc;\n+\n+\tcxlr_dax = cxl_dax_region_alloc(cxlr);\n+\tif (IS_ERR(cxlr_dax))\n+\t\treturn PTR_ERR(cxlr_dax);\n+\n+\tdev = &cxlr_dax->dev;\n+\trc = dev_set_name(dev, \"dax_region%d\", cxlr->id);\n+\tif (rc)\n+\t\tgoto err;\n+\n+\trc = device_add(dev);\n+\tif (rc)\n+\t\tgoto err;\n+\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n+\t\tdev_name(dev));\n+\n+\treturn devm_add_action_or_reset(&cxlr->dev, cxlr_dax_unregister,\n+\t\t\t\t\tcxlr_dax);\n+err:\n+\tput_device(dev);\n+\treturn rc;\n+}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the use of gotos in the function, acknowledging that it's not a functional change but rather a cleanup effort. They've made changes to remove the gotos and added a comment indicating no functional change intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged feedback",
                "made changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Cleanup the gotos in the function.\n\nNo functional change intended.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/region_dax.c | 21 ++++++++-------------\n drivers/cxl/cxl.h             |  1 +\n 2 files changed, 9 insertions(+), 13 deletions(-)\n\ndiff --git a/drivers/cxl/core/region_dax.c b/drivers/cxl/core/region_dax.c\nindex c8dd2bd1d9b9..49907c6c7620 100644\n--- a/drivers/cxl/core/region_dax.c\n+++ b/drivers/cxl/core/region_dax.c\n@@ -81,29 +81,24 @@ static void cxlr_dax_unregister(void *_cxlr_dax)\n \n int devm_cxl_add_dax_region(struct cxl_region *cxlr)\n {\n-\tstruct cxl_dax_region *cxlr_dax;\n-\tstruct device *dev;\n+\tstruct cxl_dax_region *cxlr_dax __free(put_cxl_dax_region) = NULL;\n \tint rc;\n \n \tcxlr_dax = cxl_dax_region_alloc(cxlr);\n \tif (IS_ERR(cxlr_dax))\n \t\treturn PTR_ERR(cxlr_dax);\n \n-\tdev = &cxlr_dax->dev;\n-\trc = dev_set_name(dev, \"dax_region%d\", cxlr->id);\n+\trc = dev_set_name(&cxlr_dax->dev, \"dax_region%d\", cxlr->id);\n \tif (rc)\n-\t\tgoto err;\n+\t\treturn rc;\n \n-\trc = device_add(dev);\n+\trc = device_add(&cxlr_dax->dev);\n \tif (rc)\n-\t\tgoto err;\n+\t\treturn rc;\n \n-\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n-\t\tdev_name(dev));\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(cxlr_dax->dev.parent),\n+\t\tdev_name(&cxlr_dax->dev));\n \n \treturn devm_add_action_or_reset(&cxlr->dev, cxlr_dax_unregister,\n-\t\t\t\t\tcxlr_dax);\n-err:\n-\tput_device(dev);\n-\treturn rc;\n+\t\t\t\t\tno_free_ptr(cxlr_dax));\n }\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 04c673e7cdb0..0b59008ea45a 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -803,6 +803,7 @@ DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n+DEFINE_FREE(put_cxl_dax_region, struct cxl_dax_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n \n int devm_cxl_enumerate_ports(struct cxl_memdev *cxlmd);\n void cxl_bus_rescan(void);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer Dave Jiang suggested keeping a local variable that is currently being removed and reused in the patch, citing its repeated use as a reason for retention.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Given that this local var is used multiple times, maybe we should keep it?\n\nDJ",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged the need to re-spin the patch after reviewer feedback and agreed to do so",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged",
                "re-spin"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Fair, will re-spin claned up\n\n~Gregory",
              "reply_to": "Dave Jiang",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v3 3/3] cxl/core: use cleanup.h for devm_cxl_add_dax_region",
          "message_id": "aZfv6qQR5DoZ7Chp@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZfv6qQR5DoZ7Chp@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-20T05:23:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the per-region control logic in region.c being overloaded, specifically for pmem regions. They moved the pmem region logic from region.c into region_pmem.c to make it clear that this code only applies to pmem regions. No functional changes were made.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "made a change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "core/region.c is overloaded with per-region control logic (pmem, dax).\nMove pmem region logic from region.c into region_pmem.c to make it\nclear that this code only applies to pmem regions.\n\nNo functional changes.\n\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Fabio M. De Francesco <fabio.m.de.francesco@linux.intel.com>\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/Makefile      |   1 +\n drivers/cxl/core/core.h        |   1 +\n drivers/cxl/core/region.c      | 184 --------------------------------\n drivers/cxl/core/region_pmem.c | 189 +++++++++++++++++++++++++++++++++\n 4 files changed, 191 insertions(+), 184 deletions(-)\n create mode 100644 drivers/cxl/core/region_pmem.c\n\ndiff --git a/drivers/cxl/core/Makefile b/drivers/cxl/core/Makefile\nindex a639a9499972..d1484a0e5eb4 100644\n--- a/drivers/cxl/core/Makefile\n+++ b/drivers/cxl/core/Makefile\n@@ -16,6 +16,7 @@ cxl_core-y += pmu.o\n cxl_core-y += cdat.o\n cxl_core-$(CONFIG_TRACING) += trace.o\n cxl_core-$(CONFIG_CXL_REGION) += region.o\n+cxl_core-$(CONFIG_CXL_REGION) += region_pmem.o\n cxl_core-$(CONFIG_CXL_MCE) += mce.o\n cxl_core-$(CONFIG_CXL_FEATURES) += features.o\n cxl_core-$(CONFIG_CXL_EDAC_MEM_FEATURES) += edac.o\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 007b8aff0238..ced65a779a09 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -50,6 +50,7 @@ int cxl_get_poison_by_endpoint(struct cxl_port *port);\n struct cxl_region *cxl_dpa_to_region(const struct cxl_memdev *cxlmd, u64 dpa);\n u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n+int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex bd4c4a4a27da..1c322318d70e 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2757,46 +2757,6 @@ static ssize_t delete_region_store(struct device *dev,\n }\n DEVICE_ATTR_WO(delete_region);\n \n-static void cxl_pmem_region_release(struct device *dev)\n-{\n-\tstruct cxl_pmem_region *cxlr_pmem = to_cxl_pmem_region(dev);\n-\tint i;\n-\n-\tfor (i = 0; i < cxlr_pmem->nr_mappings; i++) {\n-\t\tstruct cxl_memdev *cxlmd = cxlr_pmem->mapping[i].cxlmd;\n-\n-\t\tput_device(&cxlmd->dev);\n-\t}\n-\n-\tkfree(cxlr_pmem);\n-}\n-\n-static const struct attribute_group *cxl_pmem_region_attribute_groups[] = {\n-\t&cxl_base_attribute_group,\n-\tNULL,\n-};\n-\n-const struct device_type cxl_pmem_region_type = {\n-\t.name = \"cxl_pmem_region\",\n-\t.release = cxl_pmem_region_release,\n-\t.groups = cxl_pmem_region_attribute_groups,\n-};\n-\n-bool is_cxl_pmem_region(struct device *dev)\n-{\n-\treturn dev->type == &cxl_pmem_region_type;\n-}\n-EXPORT_SYMBOL_NS_GPL(is_cxl_pmem_region, \"CXL\");\n-\n-struct cxl_pmem_region *to_cxl_pmem_region(struct device *dev)\n-{\n-\tif (dev_WARN_ONCE(dev, !is_cxl_pmem_region(dev),\n-\t\t\t  \"not a cxl_pmem_region device\\n\"))\n-\t\treturn NULL;\n-\treturn container_of(dev, struct cxl_pmem_region, dev);\n-}\n-EXPORT_SYMBOL_NS_GPL(to_cxl_pmem_region, \"CXL\");\n-\n struct cxl_poison_context {\n \tstruct cxl_port *port;\n \tint part;\n@@ -3432,64 +3392,6 @@ static int region_offset_to_dpa_result(struct cxl_region *cxlr, u64 offset,\n \treturn -ENXIO;\n }\n \n-static struct lock_class_key cxl_pmem_region_key;\n-\n-static int cxl_pmem_region_alloc(struct cxl_region *cxlr)\n-{\n-\tstruct cxl_region_params *p = &cxlr->params;\n-\tstruct cxl_nvdimm_bridge *cxl_nvb;\n-\tstruct device *dev;\n-\tint i;\n-\n-\tguard(rwsem_read)(&cxl_rwsem.region);\n-\tif (p->state != CXL_CONFIG_COMMIT)\n-\t\treturn -ENXIO;\n-\n-\tstruct cxl_pmem_region *cxlr_pmem __free(kfree) =\n-\t\tkzalloc(struct_size(cxlr_pmem, mapping, p->nr_targets), GFP_KERNEL);\n-\tif (!cxlr_pmem)\n-\t\treturn -ENOMEM;\n-\n-\tcxlr_pmem->hpa_range.start = p->res->start;\n-\tcxlr_pmem->hpa_range.end = p->res->end;\n-\n-\t/* Snapshot the region configuration underneath the cxl_rwsem.region */\n-\tcxlr_pmem->nr_mappings = p->nr_targets;\n-\tfor (i = 0; i < p->nr_targets; i++) {\n-\t\tstruct cxl_endpoint_decoder *cxled = p->targets[i];\n-\t\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n-\t\tstruct cxl_pmem_region_mapping *m = &cxlr_pmem->mapping[i];\n-\n-\t\t/*\n-\t\t * Regions never span CXL root devices, so by definition the\n-\t\t * bridge for one device is the same for all.\n-\t\t */\n-\t\tif (i == 0) {\n-\t\t\tcxl_nvb = cxl_find_nvdimm_bridge(cxlmd->endpoint);\n-\t\t\tif (!cxl_nvb)\n-\t\t\t\treturn -ENODEV;\n-\t\t\tcxlr->cxl_nvb = cxl_nvb;\n-\t\t}\n-\t\tm->cxlmd = cxlmd;\n-\t\tget_device(&cxlmd->dev);\n-\t\tm->start = cxled->dpa_res->start;\n-\t\tm->size = resource_size(cxled->dpa_res);\n-\t\tm->position = i;\n-\t}\n-\n-\tdev = &cxlr_pmem->dev;\n-\tdevice_initialize(dev);\n-\tlockdep_set_class(&dev->mutex, &cxl_pmem_region_key);\n-\tdevice_set_pm_not_required(dev);\n-\tdev->parent = &cxlr->dev;\n-\tdev->bus = &cxl_bus_type;\n-\tdev->type = &cxl_pmem_region_type;\n-\tcxlr_pmem->cxlr = cxlr;\n-\tcxlr->cxlr_pmem = no_free_ptr(cxlr_pmem);\n-\n-\treturn 0;\n-}\n-\n static void cxl_dax_region_release(struct device *dev)\n {\n \tstruct cxl_dax_region *cxlr_dax = to_cxl_dax_region(dev);\n@@ -3553,92 +3455,6 @@ static struct cxl_dax_region *cxl_dax_region_alloc(struct cxl_region *cxlr)\n \treturn cxlr_dax;\n }\n \n-static void cxlr_pmem_unregister(void *_cxlr_pmem)\n-{\n-\tstruct cxl_pmem_region *cxlr_pmem = _cxlr_pmem;\n-\tstruct cxl_region *cxlr = cxlr_pmem->cxlr;\n-\tstruct cxl_nvdimm_bridge *cxl_nvb = cxlr->cxl_nvb;\n-\n-\t/*\n-\t * Either the bridge is in ->remove() context under the device_lock(),\n-\t * or cxlr_release_nvdimm() is cancelling the bridge's release action\n-\t * for @cxlr_pmem and doing it itself (while manually holding the bridge\n-\t * lock).\n-\t */\n-\tdevice_lock_assert(&cxl_nvb->dev);\n-\tcxlr->cxlr_pmem = NULL;\n-\tcxlr_pmem->cxlr = NULL;\n-\tdevice_unregister(&cxlr_pmem->dev);\n-}\n-\n-static void cxlr_release_nvdimm(void *_cxlr)\n-{\n-\tstruct cxl_region *cxlr = _cxlr;\n-\tstruct cxl_nvdimm_bridge *cxl_nvb = cxlr->cxl_nvb;\n-\n-\tscoped_guard(device, &cxl_nvb->dev) {\n-\t\tif (cxlr->cxlr_pmem)\n-\t\t\tdevm_release_action(&cxl_nvb->dev, cxlr_pmem_unregister,\n-\t\t\t\t\t    cxlr->cxlr_pmem);\n-\t}\n-\tcxlr->cxl_nvb = NULL;\n-\tput_device(&cxl_nvb->dev);\n-}\n-\n-/**\n- * devm_cxl_add_pmem_region() - add a cxl_region-to-nd_region bridge\n- * @cxlr: parent CXL region for this pmem region bridge device\n- *\n- * Return: 0 on success negative error code on failure.\n- */\n-static int devm_cxl_add_pmem_region(struct cxl_region *cxlr)\n-{\n-\tstruct cxl_pmem_region *cxlr_pmem;\n-\tstruct cxl_nvdimm_bridge *cxl_nvb;\n-\tstruct device *dev;\n-\tint rc;\n-\n-\trc = cxl_pmem_region_alloc(cxlr);\n-\tif (rc)\n-\t\treturn rc;\n-\tcxlr_pmem = cxlr->cxlr_pmem;\n-\tcxl_nvb = cxlr->cxl_nvb;\n-\n-\tdev = &cxlr_pmem->dev;\n-\trc = dev_set_name(dev, \"pmem_region%d\", cxlr->id);\n-\tif (rc)\n-\t\tgoto err;\n-\n-\trc = device_add(dev);\n-\tif (rc)\n-\t\tgoto err;\n-\n-\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n-\t\tdev_name(dev));\n-\n-\tscoped_guard(device, &cxl_nvb->dev) {\n-\t\tif (cxl_nvb->dev.driver)\n-\t\t\trc = devm_add_action_or_reset(&cxl_nvb->dev,\n-\t\t\t\t\t\t      cxlr_pmem_unregister,\n-\t\t\t\t\t\t      cxlr_pmem);\n-\t\telse\n-\t\t\trc = -ENXIO;\n-\t}\n-\n-\tif (rc)\n-\t\tgoto err_bridge;\n-\n-\t/* @cxlr carries a reference on @cxl_nvb until cxlr_release_nvdimm */\n-\treturn devm_add_action_or_reset(&cxlr->dev, cxlr_release_nvdimm, cxlr);\n-\n-err:\n-\tput_device(dev);\n-err_bridge:\n-\tput_device(&cxl_nvb->dev);\n-\tcxlr->cxl_nvb = NULL;\n-\treturn rc;\n-}\n-\n static void cxlr_dax_unregister(void *_cxlr_dax)\n {\n \tstruct cxl_dax_region *cxlr_dax = _cxlr_dax;\ndiff --git a/drivers/cxl/core/region_pmem.c b/drivers/cxl/core/region_pmem.c\nnew file mode 100644\nindex 000000000000..f800407566d3\n--- /dev/null\n+++ b/drivers/cxl/core/region_pmem.c\n@@ -0,0 +1,189 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2022 Intel Corporation. All rights reserved. */\n+#include <linux/device.h>\n+#include <linux/slab.h>\n+#include <cxlmem.h>\n+#include <cxl.h>\n+#include \"core.h\"\n+\n+static void cxl_pmem_region_release(struct device *dev)\n+{\n+\tstruct cxl_pmem_region *cxlr_pmem = to_cxl_pmem_region(dev);\n+\tint i;\n+\n+\tfor (i = 0; i < cxlr_pmem->nr_mappings; i++) {\n+\t\tstruct cxl_memdev *cxlmd = cxlr_pmem->mapping[i].cxlmd;\n+\n+\t\tput_device(&cxlmd->dev);\n+\t}\n+\n+\tkfree(cxlr_pmem);\n+}\n+\n+static const struct attribute_group *cxl_pmem_region_attribute_groups[] = {\n+\t&cxl_base_attribute_group,\n+\tNULL\n+};\n+\n+const struct device_type cxl_pmem_region_type = {\n+\t.name = \"cxl_pmem_region\",\n+\t.release = cxl_pmem_region_release,\n+\t.groups = cxl_pmem_region_attribute_groups,\n+};\n+bool is_cxl_pmem_region(struct device *dev)\n+{\n+\treturn dev->type == &cxl_pmem_region_type;\n+}\n+EXPORT_SYMBOL_NS_GPL(is_cxl_pmem_region, \"CXL\");\n+\n+struct cxl_pmem_region *to_cxl_pmem_region(struct device *dev)\n+{\n+\tif (dev_WARN_ONCE(dev, !is_cxl_pmem_region(dev),\n+\t\t\t\t\"not a cxl_pmem_region device\\n\"))\n+\t\treturn NULL;\n+\treturn container_of(dev, struct cxl_pmem_region, dev);\n+}\n+EXPORT_SYMBOL_NS_GPL(to_cxl_pmem_region, \"CXL\");\n+static struct lock_class_key cxl_pmem_region_key;\n+\n+static int cxl_pmem_region_alloc(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_region_params *p = &cxlr->params;\n+\tstruct cxl_nvdimm_bridge *cxl_nvb;\n+\tstruct device *dev;\n+\tint i;\n+\n+\tguard(rwsem_read)(&cxl_rwsem.region);\n+\tif (p->state != CXL_CONFIG_COMMIT)\n+\t\treturn -ENXIO;\n+\n+\tstruct cxl_pmem_region *cxlr_pmem __free(kfree) =\n+\t\tkzalloc(struct_size(cxlr_pmem, mapping, p->nr_targets), GFP_KERNEL);\n+\tif (!cxlr_pmem)\n+\t\treturn -ENOMEM;\n+\n+\tcxlr_pmem->hpa_range.start = p->res->start;\n+\tcxlr_pmem->hpa_range.end = p->res->end;\n+\n+\t/* Snapshot the region configuration underneath the cxl_rwsem.region */\n+\tcxlr_pmem->nr_mappings = p->nr_targets;\n+\tfor (i = 0; i < p->nr_targets; i++) {\n+\t\tstruct cxl_endpoint_decoder *cxled = p->targets[i];\n+\t\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n+\t\tstruct cxl_pmem_region_mapping *m = &cxlr_pmem->mapping[i];\n+\n+\t\t/*\n+\t\t * Regions never span CXL root devices, so by definition the\n+\t\t * bridge for one device is the same for all.\n+\t\t */\n+\t\tif (i == 0) {\n+\t\t\tcxl_nvb = cxl_find_nvdimm_bridge(cxlmd->endpoint);\n+\t\t\tif (!cxl_nvb)\n+\t\t\t\treturn -ENODEV;\n+\t\t\tcxlr->cxl_nvb = cxl_nvb;\n+\t\t}\n+\t\tm->cxlmd = cxlmd;\n+\t\tget_device(&cxlmd->dev);\n+\t\tm->start = cxled->dpa_res->start;\n+\t\tm->size = resource_size(cxled->dpa_res);\n+\t\tm->position = i;\n+\t}\n+\n+\tdev = &cxlr_pmem->dev;\n+\tdevice_initialize(dev);\n+\tlockdep_set_class(&dev->mutex, &cxl_pmem_region_key);\n+\tdevice_set_pm_not_required(dev);\n+\tdev->parent = &cxlr->dev;\n+\tdev->bus = &cxl_bus_type;\n+\tdev->type = &cxl_pmem_region_type;\n+\tcxlr_pmem->cxlr = cxlr;\n+\tcxlr->cxlr_pmem = no_free_ptr(cxlr_pmem);\n+\n+\treturn 0;\n+}\n+\n+static void cxlr_pmem_unregister(void *_cxlr_pmem)\n+{\n+\tstruct cxl_pmem_region *cxlr_pmem = _cxlr_pmem;\n+\tstruct cxl_region *cxlr = cxlr_pmem->cxlr;\n+\tstruct cxl_nvdimm_bridge *cxl_nvb = cxlr->cxl_nvb;\n+\n+\t/*\n+\t * Either the bridge is in ->remove() context under the device_lock(),\n+\t * or cxlr_release_nvdimm() is cancelling the bridge's release action\n+\t * for @cxlr_pmem and doing it itself (while manually holding the bridge\n+\t * lock).\n+\t */\n+\tdevice_lock_assert(&cxl_nvb->dev);\n+\tcxlr->cxlr_pmem = NULL;\n+\tcxlr_pmem->cxlr = NULL;\n+\tdevice_unregister(&cxlr_pmem->dev);\n+}\n+\n+static void cxlr_release_nvdimm(void *_cxlr)\n+{\n+\tstruct cxl_region *cxlr = _cxlr;\n+\tstruct cxl_nvdimm_bridge *cxl_nvb = cxlr->cxl_nvb;\n+\n+\tscoped_guard(device, &cxl_nvb->dev) {\n+\t\tif (cxlr->cxlr_pmem)\n+\t\t\tdevm_release_action(&cxl_nvb->dev, cxlr_pmem_unregister,\n+\t\t\t\t\tcxlr->cxlr_pmem);\n+\t}\n+\tcxlr->cxl_nvb = NULL;\n+\tput_device(&cxl_nvb->dev);\n+}\n+\n+/**\n+ * devm_cxl_add_pmem_region() - add a cxl_region-to-nd_region bridge\n+ * @cxlr: parent CXL region for this pmem region bridge device\n+ *\n+ * Return: 0 on success negative error code on failure.\n+ */\n+int devm_cxl_add_pmem_region(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_pmem_region *cxlr_pmem;\n+\tstruct cxl_nvdimm_bridge *cxl_nvb;\n+\tstruct device *dev;\n+\tint rc;\n+\n+\trc = cxl_pmem_region_alloc(cxlr);\n+\tif (rc)\n+\t\treturn rc;\n+\tcxlr_pmem = cxlr->cxlr_pmem;\n+\tcxl_nvb = cxlr->cxl_nvb;\n+\n+\tdev = &cxlr_pmem->dev;\n+\trc = dev_set_name(dev, \"pmem_region%d\", cxlr->id);\n+\tif (rc)\n+\t\tgoto err;\n+\n+\trc = device_add(dev);\n+\tif (rc)\n+\t\tgoto err;\n+\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n+\t\t\tdev_name(dev));\n+\n+\tscoped_guard(device, &cxl_nvb->dev) {\n+\t\tif (cxl_nvb->dev.driver)\n+\t\t\trc = devm_add_action_or_reset(&cxl_nvb->dev,\n+\t\t\t\t\tcxlr_pmem_unregister,\n+\t\t\t\t\tcxlr_pmem);\n+\t\telse\n+\t\t\trc = -ENXIO;\n+\t}\n+\n+\tif (rc)\n+\t\tgoto err_bridge;\n+\n+\t/* @cxlr carries a reference on @cxl_nvb until cxlr_release_nvdimm */\n+\treturn devm_add_action_or_reset(&cxlr->dev, cxlr_release_nvdimm, cxlr);\n+\n+err:\n+\tput_device(dev);\n+err_bridge:\n+\tput_device(&cxl_nvb->dev);\n+\tcxlr->cxl_nvb = NULL;\n+\treturn rc;\n+}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the per-region control logic in region.c being overloaded, specifically for CXL DAX regions. The author agreed to move the CXL DAX region device logic from region.c into region_dax.c, confirming that no functional changes are needed.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "core/region.c is overloaded with per-region control logic (pmem, dax).\nMove CXL DAX region device logic from region.c into region_dax.c.\n\nNo functional changes.\n\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Fabio M. De Francesco <fabio.m.de.francesco@linux.intel.com>\nAcked-by: Davidlohr Bueso <dave@stgolabs.net>\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/Makefile     |   1 +\n drivers/cxl/core/core.h       |   1 +\n drivers/cxl/core/region.c     |  99 ------------------------------\n drivers/cxl/core/region_dax.c | 109 ++++++++++++++++++++++++++++++++++\n 4 files changed, 111 insertions(+), 99 deletions(-)\n create mode 100644 drivers/cxl/core/region_dax.c\n\ndiff --git a/drivers/cxl/core/Makefile b/drivers/cxl/core/Makefile\nindex d1484a0e5eb4..d3ec8aea64c5 100644\n--- a/drivers/cxl/core/Makefile\n+++ b/drivers/cxl/core/Makefile\n@@ -16,6 +16,7 @@ cxl_core-y += pmu.o\n cxl_core-y += cdat.o\n cxl_core-$(CONFIG_TRACING) += trace.o\n cxl_core-$(CONFIG_CXL_REGION) += region.o\n+cxl_core-$(CONFIG_CXL_REGION) += region_dax.o\n cxl_core-$(CONFIG_CXL_REGION) += region_pmem.o\n cxl_core-$(CONFIG_CXL_MCE) += mce.o\n cxl_core-$(CONFIG_CXL_FEATURES) += features.o\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex ced65a779a09..a89e0d6d9e7b 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -50,6 +50,7 @@ int cxl_get_poison_by_endpoint(struct cxl_port *port);\n struct cxl_region *cxl_dpa_to_region(const struct cxl_memdev *cxlmd, u64 dpa);\n u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n+int devm_cxl_add_dax_region(struct cxl_region *cxlr);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n \n #else\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 1c322318d70e..0c37caa60f2a 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -3392,105 +3392,6 @@ static int region_offset_to_dpa_result(struct cxl_region *cxlr, u64 offset,\n \treturn -ENXIO;\n }\n \n-static void cxl_dax_region_release(struct device *dev)\n-{\n-\tstruct cxl_dax_region *cxlr_dax = to_cxl_dax_region(dev);\n-\n-\tkfree(cxlr_dax);\n-}\n-\n-static const struct attribute_group *cxl_dax_region_attribute_groups[] = {\n-\t&cxl_base_attribute_group,\n-\tNULL,\n-};\n-\n-const struct device_type cxl_dax_region_type = {\n-\t.name = \"cxl_dax_region\",\n-\t.release = cxl_dax_region_release,\n-\t.groups = cxl_dax_region_attribute_groups,\n-};\n-\n-static bool is_cxl_dax_region(struct device *dev)\n-{\n-\treturn dev->type == &cxl_dax_region_type;\n-}\n-\n-struct cxl_dax_region *to_cxl_dax_region(struct device *dev)\n-{\n-\tif (dev_WARN_ONCE(dev, !is_cxl_dax_region(dev),\n-\t\t\t  \"not a cxl_dax_region device\\n\"))\n-\t\treturn NULL;\n-\treturn container_of(dev, struct cxl_dax_region, dev);\n-}\n-EXPORT_SYMBOL_NS_GPL(to_cxl_dax_region, \"CXL\");\n-\n-static struct lock_class_key cxl_dax_region_key;\n-\n-static struct cxl_dax_region *cxl_dax_region_alloc(struct cxl_region *cxlr)\n-{\n-\tstruct cxl_region_params *p = &cxlr->params;\n-\tstruct cxl_dax_region *cxlr_dax;\n-\tstruct device *dev;\n-\n-\tguard(rwsem_read)(&cxl_rwsem.region);\n-\tif (p->state != CXL_CONFIG_COMMIT)\n-\t\treturn ERR_PTR(-ENXIO);\n-\n-\tcxlr_dax = kzalloc(sizeof(*cxlr_dax), GFP_KERNEL);\n-\tif (!cxlr_dax)\n-\t\treturn ERR_PTR(-ENOMEM);\n-\n-\tcxlr_dax->hpa_range.start = p->res->start;\n-\tcxlr_dax->hpa_range.end = p->res->end;\n-\n-\tdev = &cxlr_dax->dev;\n-\tcxlr_dax->cxlr = cxlr;\n-\tdevice_initialize(dev);\n-\tlockdep_set_class(&dev->mutex, &cxl_dax_region_key);\n-\tdevice_set_pm_not_required(dev);\n-\tdev->parent = &cxlr->dev;\n-\tdev->bus = &cxl_bus_type;\n-\tdev->type = &cxl_dax_region_type;\n-\n-\treturn cxlr_dax;\n-}\n-\n-static void cxlr_dax_unregister(void *_cxlr_dax)\n-{\n-\tstruct cxl_dax_region *cxlr_dax = _cxlr_dax;\n-\n-\tdevice_unregister(&cxlr_dax->dev);\n-}\n-\n-static int devm_cxl_add_dax_region(struct cxl_region *cxlr)\n-{\n-\tstruct cxl_dax_region *cxlr_dax;\n-\tstruct device *dev;\n-\tint rc;\n-\n-\tcxlr_dax = cxl_dax_region_alloc(cxlr);\n-\tif (IS_ERR(cxlr_dax))\n-\t\treturn PTR_ERR(cxlr_dax);\n-\n-\tdev = &cxlr_dax->dev;\n-\trc = dev_set_name(dev, \"dax_region%d\", cxlr->id);\n-\tif (rc)\n-\t\tgoto err;\n-\n-\trc = device_add(dev);\n-\tif (rc)\n-\t\tgoto err;\n-\n-\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n-\t\tdev_name(dev));\n-\n-\treturn devm_add_action_or_reset(&cxlr->dev, cxlr_dax_unregister,\n-\t\t\t\t\tcxlr_dax);\n-err:\n-\tput_device(dev);\n-\treturn rc;\n-}\n-\n static int match_root_decoder(struct device *dev, const void *data)\n {\n \tconst struct range *r1, *r2 = data;\ndiff --git a/drivers/cxl/core/region_dax.c b/drivers/cxl/core/region_dax.c\nnew file mode 100644\nindex 000000000000..c8dd2bd1d9b9\n--- /dev/null\n+++ b/drivers/cxl/core/region_dax.c\n@@ -0,0 +1,109 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/*\n+ * Copyright(c) 2022 Intel Corporation. All rights reserved.\n+ * Copyright(c) 2026 Meta Technologies Inc. All rights reserved.\n+ */\n+#include <linux/device.h>\n+#include <linux/slab.h>\n+#include <cxlmem.h>\n+#include <cxl.h>\n+#include \"core.h\"\n+\n+static void cxl_dax_region_release(struct device *dev)\n+{\n+\tstruct cxl_dax_region *cxlr_dax = to_cxl_dax_region(dev);\n+\n+\tkfree(cxlr_dax);\n+}\n+\n+static const struct attribute_group *cxl_dax_region_attribute_groups[] = {\n+\t&cxl_base_attribute_group,\n+\tNULL\n+};\n+\n+const struct device_type cxl_dax_region_type = {\n+\t.name = \"cxl_dax_region\",\n+\t.release = cxl_dax_region_release,\n+\t.groups = cxl_dax_region_attribute_groups,\n+};\n+\n+static bool is_cxl_dax_region(struct device *dev)\n+{\n+\treturn dev->type == &cxl_dax_region_type;\n+}\n+\n+struct cxl_dax_region *to_cxl_dax_region(struct device *dev)\n+{\n+\tif (dev_WARN_ONCE(dev, !is_cxl_dax_region(dev),\n+\t\t\t  \"not a cxl_dax_region device\\n\"))\n+\t\treturn NULL;\n+\treturn container_of(dev, struct cxl_dax_region, dev);\n+}\n+EXPORT_SYMBOL_NS_GPL(to_cxl_dax_region, \"CXL\");\n+\n+static struct lock_class_key cxl_dax_region_key;\n+\n+static struct cxl_dax_region *cxl_dax_region_alloc(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_region_params *p = &cxlr->params;\n+\tstruct cxl_dax_region *cxlr_dax;\n+\tstruct device *dev;\n+\n+\tguard(rwsem_read)(&cxl_rwsem.region);\n+\tif (p->state != CXL_CONFIG_COMMIT)\n+\t\treturn ERR_PTR(-ENXIO);\n+\n+\tcxlr_dax = kzalloc(sizeof(*cxlr_dax), GFP_KERNEL);\n+\tif (!cxlr_dax)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tcxlr_dax->hpa_range.start = p->res->start;\n+\tcxlr_dax->hpa_range.end = p->res->end;\n+\n+\tdev = &cxlr_dax->dev;\n+\tcxlr_dax->cxlr = cxlr;\n+\tdevice_initialize(dev);\n+\tlockdep_set_class(&dev->mutex, &cxl_dax_region_key);\n+\tdevice_set_pm_not_required(dev);\n+\tdev->parent = &cxlr->dev;\n+\tdev->bus = &cxl_bus_type;\n+\tdev->type = &cxl_dax_region_type;\n+\n+\treturn cxlr_dax;\n+}\n+\n+static void cxlr_dax_unregister(void *_cxlr_dax)\n+{\n+\tstruct cxl_dax_region *cxlr_dax = _cxlr_dax;\n+\n+\tdevice_unregister(&cxlr_dax->dev);\n+}\n+\n+int devm_cxl_add_dax_region(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_dax_region *cxlr_dax;\n+\tstruct device *dev;\n+\tint rc;\n+\n+\tcxlr_dax = cxl_dax_region_alloc(cxlr);\n+\tif (IS_ERR(cxlr_dax))\n+\t\treturn PTR_ERR(cxlr_dax);\n+\n+\tdev = &cxlr_dax->dev;\n+\trc = dev_set_name(dev, \"dax_region%d\", cxlr->id);\n+\tif (rc)\n+\t\tgoto err;\n+\n+\trc = device_add(dev);\n+\tif (rc)\n+\t\tgoto err;\n+\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n+\t\tdev_name(dev));\n+\n+\treturn devm_add_action_or_reset(&cxlr->dev, cxlr_dax_unregister,\n+\t\t\t\t\tcxlr_dax);\n+err:\n+\tput_device(dev);\n+\treturn rc;\n+}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the use of goto statements in the function, acknowledging that it's not a functional change but rather a cleanup effort.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a non-functional change",
                "no fix planned"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Cleanup the gotos in the function.\n\nNo functional change intended.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/region_dax.c | 21 ++++++++-------------\n drivers/cxl/cxl.h             |  1 +\n 2 files changed, 9 insertions(+), 13 deletions(-)\n\ndiff --git a/drivers/cxl/core/region_dax.c b/drivers/cxl/core/region_dax.c\nindex c8dd2bd1d9b9..49907c6c7620 100644\n--- a/drivers/cxl/core/region_dax.c\n+++ b/drivers/cxl/core/region_dax.c\n@@ -81,29 +81,24 @@ static void cxlr_dax_unregister(void *_cxlr_dax)\n \n int devm_cxl_add_dax_region(struct cxl_region *cxlr)\n {\n-\tstruct cxl_dax_region *cxlr_dax;\n-\tstruct device *dev;\n+\tstruct cxl_dax_region *cxlr_dax __free(put_cxl_dax_region) = NULL;\n \tint rc;\n \n \tcxlr_dax = cxl_dax_region_alloc(cxlr);\n \tif (IS_ERR(cxlr_dax))\n \t\treturn PTR_ERR(cxlr_dax);\n \n-\tdev = &cxlr_dax->dev;\n-\trc = dev_set_name(dev, \"dax_region%d\", cxlr->id);\n+\trc = dev_set_name(&cxlr_dax->dev, \"dax_region%d\", cxlr->id);\n \tif (rc)\n-\t\tgoto err;\n+\t\treturn rc;\n \n-\trc = device_add(dev);\n+\trc = device_add(&cxlr_dax->dev);\n \tif (rc)\n-\t\tgoto err;\n+\t\treturn rc;\n \n-\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n-\t\tdev_name(dev));\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(cxlr_dax->dev.parent),\n+\t\tdev_name(&cxlr_dax->dev));\n \n \treturn devm_add_action_or_reset(&cxlr->dev, cxlr_dax_unregister,\n-\t\t\t\t\tcxlr_dax);\n-err:\n-\tput_device(dev);\n-\treturn rc;\n+\t\t\t\t\tno_free_ptr(cxlr_dax));\n }\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 04c673e7cdb0..0b59008ea45a 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -803,6 +803,7 @@ DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n+DEFINE_FREE(put_cxl_dax_region, struct cxl_dax_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n \n int devm_cxl_enumerate_ports(struct cxl_memdev *cxlmd);\n void cxl_bus_rescan(void);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer suggested keeping a local variable that is used multiple times to improve code readability and maintainability",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Given that this local var is used multiple times, maybe we should keep it?\n\nDJ",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledges that the patch needs further revision and agrees to re-spin it",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a need for revision",
                "agreed to re-spin"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Fair, will re-spin claned up\n\n~Gregory",
              "reply_to": "Dave Jiang",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Jeff Layton",
      "primary_email": "jlayton@kernel.org",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/3] sunrpc: cache infrastructure scalability improvements",
          "message_id": "20260220-sunrpc-cache-v1-0-47d04014c245@kernel.org",
          "url": "https://lore.kernel.org/all/20260220-sunrpc-cache-v1-0-47d04014c245@kernel.org/",
          "date": "2026-02-20T12:26:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series improves the scalability of the sunrpc cache infrastructure by converting a global spinlock and waitqueue to per-cache-detail locks and waitqueues, reducing contention between different caches. The patches also split the cache detail queue into separate lists for readers and requests, simplifying the code and enabling future implementation of netlink upcalls.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about contention between different caches on queue operations by converting the global spinlock to per-cache_detail spinlocks, which will resolve the issue.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The global queue_lock serializes all upcall queue operations across\nevery cache_detail instance. Convert it to a per-cache_detail spinlock\nso that different caches (e.g. auth.unix.ip vs nfsd.fh) no longer\ncontend with each other on queue operations.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h |  1 +\n net/sunrpc/cache.c           | 47 ++++++++++++++++++++++----------------------\n 2 files changed, 24 insertions(+), 24 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex e783132e481ff2593fdc5d323f7b3a08f85d4cd8..3d32dd1f7b05d35562d2064fed69877b3950fb51 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -113,6 +113,7 @@ struct cache_detail {\n \n \t/* fields for communication over channel */\n \tstruct list_head\tqueue;\n+\tspinlock_t\t\tqueue_lock;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex 7c73d1c39687343db02d1f1423b58213b7a35f42..6add2fe311425dc3aec63efce2c4bed06a3d3ba5 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -400,6 +400,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)\n {\n \tspin_lock_init(&cd->hash_lock);\n \tINIT_LIST_HEAD(&cd->queue);\n+\tspin_lock_init(&cd->queue_lock);\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -803,8 +804,6 @@ void cache_clean_deferred(void *owner)\n  *\n  */\n \n-static DEFINE_SPINLOCK(queue_lock);\n-\n struct cache_queue {\n \tstruct list_head\tlist;\n \tint\t\t\treader;\t/* if 0, then request */\n@@ -847,7 +846,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tinode_lock(inode); /* protect against multiple concurrent\n \t\t\t      * readers on this file */\n  again:\n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \t/* need to find next request */\n \twhile (rp->q.list.next != &cd->queue &&\n \t       list_entry(rp->q.list.next, struct cache_queue, list)\n@@ -856,7 +855,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\tlist_move(&rp->q.list, next);\n \t}\n \tif (rp->q.list.next == &cd->queue) {\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t\tinode_unlock(inode);\n \t\tWARN_ON_ONCE(rp->offset);\n \t\treturn 0;\n@@ -865,7 +864,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tWARN_ON_ONCE(rq->q.reader);\n \tif (rp->offset == 0)\n \t\trq->readers++;\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \n \tif (rq->len == 0) {\n \t\terr = cache_request(cd, rq);\n@@ -876,9 +875,9 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \n \tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\terr = -EAGAIN;\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t} else {\n \t\tif (rp->offset + count > rq->len)\n \t\t\tcount = rq->len - rp->offset;\n@@ -888,26 +887,26 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trp->offset += count;\n \t\tif (rp->offset >= rq->len) {\n \t\t\trp->offset = 0;\n-\t\t\tspin_lock(&queue_lock);\n+\t\t\tspin_lock(&cd->queue_lock);\n \t\t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t\t}\n \t\terr = 0;\n \t}\n  out:\n \tif (rp->offset == 0) {\n \t\t/* need to release rq */\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\trq->readers--;\n \t\tif (rq->readers == 0 &&\n \t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\t\tlist_del(&rq->q.list);\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t\t\tcache_put(rq->item, cd);\n \t\t\tkfree(rq->buf);\n \t\t\tkfree(rq);\n \t\t} else\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (err == -EAGAIN)\n \t\tgoto again;\n@@ -988,7 +987,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \tif (!rp)\n \t\treturn mask;\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \n \tfor (cq= &rp->q; &cq->list != &cd->queue;\n \t     cq = list_entry(cq->list.next, struct cache_queue, list))\n@@ -996,7 +995,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n \t\t\tbreak;\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \treturn mask;\n }\n \n@@ -1011,7 +1010,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \tif (cmd != FIONREAD || !rp)\n \t\treturn -EINVAL;\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \n \t/* only find the length remaining in current request,\n \t * or the length of the next request\n@@ -1024,7 +1023,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \t\t\tlen = cr->len - rp->offset;\n \t\t\tbreak;\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \n \treturn put_user(len, (int __user *)arg);\n }\n@@ -1046,9 +1045,9 @@ static int cache_open(struct inode *inode, struct file *filp,\n \t\trp->offset = 0;\n \t\trp->q.reader = 1;\n \n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tlist_add(&rp->q.list, &cd->queue);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (filp->f_mode & FMODE_WRITE)\n \t\tatomic_inc(&cd->writers);\n@@ -1062,7 +1061,7 @@ static int cache_release(struct inode *inode, struct file *filp,\n \tstruct cache_reader *rp = filp->private_data;\n \n \tif (rp) {\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tif (rp->offset) {\n \t\t\tstruct cache_queue *cq;\n \t\t\tfor (cq= &rp->q; &cq->list != &cd->queue;\n@@ -1075,7 +1074,7 @@ static int cache_release(struct inode *inode, struct file *filp,\n \t\t\trp->offset = 0;\n \t\t}\n \t\tlist_del(&rp->q.list);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \n \t\tfilp->private_data = NULL;\n \t\tkfree(rp);\n@@ -1097,7 +1096,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n \tstruct cache_request *cr;\n \tLIST_HEAD(dequeued);\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&detail->queue_lock);\n \tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n \t\tif (!cq->reader) {\n \t\t\tcr = container_of(cq, struct cache_request, q);\n@@ -1110,7 +1109,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n \t\t\t\tcontinue;\n \t\t\tlist_move(&cr->q.list, &dequeued);\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&detail->queue_lock);\n \twhile (!list_empty(&dequeued)) {\n \t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n \t\tlist_del(&cr->q.list);\n@@ -1235,7 +1234,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \tcrq->buf = buf;\n \tcrq->len = 0;\n \tcrq->readers = 0;\n-\tspin_lock(&queue_lock);\n+\tspin_lock(&detail->queue_lock);\n \tif (test_bit(CACHE_PENDING, &h->flags)) {\n \t\tcrq->item = cache_get(h);\n \t\tlist_add_tail(&crq->q.list, &detail->queue);\n@@ -1243,7 +1242,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t} else\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n \t\tret = -EAGAIN;\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&detail->queue_lock);\n \twake_up(&queue_wait);\n \tif (ret == -EAGAIN) {\n \t\tkfree(buf);\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author is addressing a concern about the queue_wait waitqueue being global and waking pollers on all caches, instead of just the relevant one. The author agrees to convert it to a per-cache_detail field so that only pollers on the relevant cache are woken.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The queue_wait waitqueue is currently a file-scoped global, so a\nwake_up for one cache_detail wakes pollers on all caches. Convert it\nto a per-cache_detail field so that only pollers on the relevant cache\nare woken.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h | 2 ++\n net/sunrpc/cache.c           | 7 +++----\n 2 files changed, 5 insertions(+), 4 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex 3d32dd1f7b05d35562d2064fed69877b3950fb51..031379efba24d40f64ce346cf1032261d4b98d05 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -16,6 +16,7 @@\n #include <linux/atomic.h>\n #include <linux/kstrtox.h>\n #include <linux/proc_fs.h>\n+#include <linux/wait.h>\n \n /*\n  * Each cache requires:\n@@ -114,6 +115,7 @@ struct cache_detail {\n \t/* fields for communication over channel */\n \tstruct list_head\tqueue;\n \tspinlock_t\t\tqueue_lock;\n+\twait_queue_head_t\tqueue_wait;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex 6add2fe311425dc3aec63efce2c4bed06a3d3ba5..aef2607b3d7ffb61a42b9ea2ec17947465c026dc 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -401,6 +401,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)\n \tspin_lock_init(&cd->hash_lock);\n \tINIT_LIST_HEAD(&cd->queue);\n \tspin_lock_init(&cd->queue_lock);\n+\tinit_waitqueue_head(&cd->queue_wait);\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -970,8 +971,6 @@ static ssize_t cache_write(struct file *filp, const char __user *buf,\n \treturn ret;\n }\n \n-static DECLARE_WAIT_QUEUE_HEAD(queue_wait);\n-\n static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \t\t\t       struct cache_detail *cd)\n {\n@@ -979,7 +978,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \tstruct cache_reader *rp = filp->private_data;\n \tstruct cache_queue *cq;\n \n-\tpoll_wait(filp, &queue_wait, wait);\n+\tpoll_wait(filp, &cd->queue_wait, wait);\n \n \t/* alway allow write */\n \tmask = EPOLLOUT | EPOLLWRNORM;\n@@ -1243,7 +1242,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n \t\tret = -EAGAIN;\n \tspin_unlock(&detail->queue_lock);\n-\twake_up(&queue_wait);\n+\twake_up(&detail->queue_wait);\n \tif (ret == -EAGAIN) {\n \t\tkfree(buf);\n \t\tkfree(crq);\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about the shared list of requests and readers in the sunrpc cache infrastructure. They replaced this single interleaved queue with two dedicated lists: cd->requests for upcall requests and cd->readers for open file handles, eliminating the need for the ->reader flag. The sequence number (next_seqno) is now used to track position instead of the shared list. This change simplifies the reader-skipping loops in various functions and makes data flow easier to reason about.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a design improvement",
                "agreed with the approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace the single interleaved queue (which mixed cache_request and\ncache_reader entries distinguished by a ->reader flag) with two\ndedicated lists: cd->requests for upcall requests and cd->readers\nfor open file handles.\n\nReaders now track their position via a monotonically increasing\nsequence number (next_seqno) rather than by their position in the\nshared list. Each cache_request is assigned a seqno when enqueued,\nand a new cache_next_request() helper finds the next request at or\nafter a given seqno.\n\nThis eliminates the cache_queue wrapper struct entirely, simplifies\nthe reader-skipping loops in cache_read/cache_poll/cache_ioctl/\ncache_release, and makes the data flow easier to reason about.\n\nAlso, remove an obsolete comment. CACHE_UPCALLING hasn't existed\nsince before the git era started.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h |   4 +-\n net/sunrpc/cache.c           | 125 ++++++++++++++++++-------------------------\n 2 files changed, 56 insertions(+), 73 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex 031379efba24d40f64ce346cf1032261d4b98d05..b1e595c2615bd4be4d9ad19f71a8f4d08bd74a9b 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -113,9 +113,11 @@ struct cache_detail {\n \tint\t\t\tentries;\n \n \t/* fields for communication over channel */\n-\tstruct list_head\tqueue;\n+\tstruct list_head\trequests;\n+\tstruct list_head\treaders;\n \tspinlock_t\t\tqueue_lock;\n \twait_queue_head_t\tqueue_wait;\n+\tu64\t\t\tnext_seqno;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex aef2607b3d7ffb61a42b9ea2ec17947465c026dc..09389ce8b961fe0cb5a472bcf2d3dd0b3faa13a6 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -399,9 +399,11 @@ static struct delayed_work cache_cleaner;\n void sunrpc_init_cache_detail(struct cache_detail *cd)\n {\n \tspin_lock_init(&cd->hash_lock);\n-\tINIT_LIST_HEAD(&cd->queue);\n+\tINIT_LIST_HEAD(&cd->requests);\n+\tINIT_LIST_HEAD(&cd->readers);\n \tspin_lock_init(&cd->queue_lock);\n \tinit_waitqueue_head(&cd->queue_wait);\n+\tcd->next_seqno = 0;\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -796,29 +798,20 @@ void cache_clean_deferred(void *owner)\n  * On read, you get a full request, or block.\n  * On write, an update request is processed.\n  * Poll works if anything to read, and always allows write.\n- *\n- * Implemented by linked list of requests.  Each open file has\n- * a ->private that also exists in this list.  New requests are added\n- * to the end and may wakeup and preceding readers.\n- * New readers are added to the head.  If, on read, an item is found with\n- * CACHE_UPCALLING clear, we free it from the list.\n- *\n  */\n \n-struct cache_queue {\n-\tstruct list_head\tlist;\n-\tint\t\t\treader;\t/* if 0, then request */\n-};\n struct cache_request {\n-\tstruct cache_queue\tq;\n+\tstruct list_head\tlist;\n \tstruct cache_head\t*item;\n-\tchar\t\t\t* buf;\n+\tchar\t\t\t*buf;\n \tint\t\t\tlen;\n \tint\t\t\treaders;\n+\tu64\t\t\tseqno;\n };\n struct cache_reader {\n-\tstruct cache_queue\tq;\n+\tstruct list_head\tlist;\n \tint\t\t\toffset;\t/* if non-0, we have a refcnt on next request */\n+\tu64\t\t\tnext_seqno;\n };\n \n static int cache_request(struct cache_detail *detail,\n@@ -833,6 +826,17 @@ static int cache_request(struct cache_detail *detail,\n \treturn PAGE_SIZE - len;\n }\n \n+static struct cache_request *\n+cache_next_request(struct cache_detail *cd, u64 seqno)\n+{\n+\tstruct cache_request *rq;\n+\n+\tlist_for_each_entry(rq, &cd->requests, list)\n+\t\tif (rq->seqno >= seqno)\n+\t\t\treturn rq;\n+\treturn NULL;\n+}\n+\n static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\t\t  loff_t *ppos, struct cache_detail *cd)\n {\n@@ -849,20 +853,13 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n  again:\n \tspin_lock(&cd->queue_lock);\n \t/* need to find next request */\n-\twhile (rp->q.list.next != &cd->queue &&\n-\t       list_entry(rp->q.list.next, struct cache_queue, list)\n-\t       ->reader) {\n-\t\tstruct list_head *next = rp->q.list.next;\n-\t\tlist_move(&rp->q.list, next);\n-\t}\n-\tif (rp->q.list.next == &cd->queue) {\n+\trq = cache_next_request(cd, rp->next_seqno);\n+\tif (!rq) {\n \t\tspin_unlock(&cd->queue_lock);\n \t\tinode_unlock(inode);\n \t\tWARN_ON_ONCE(rp->offset);\n \t\treturn 0;\n \t}\n-\trq = container_of(rp->q.list.next, struct cache_request, q.list);\n-\tWARN_ON_ONCE(rq->q.reader);\n \tif (rp->offset == 0)\n \t\trq->readers++;\n \tspin_unlock(&cd->queue_lock);\n@@ -877,7 +874,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\terr = -EAGAIN;\n \t\tspin_lock(&cd->queue_lock);\n-\t\tlist_move(&rp->q.list, &rq->q.list);\n+\t\trp->next_seqno = rq->seqno + 1;\n \t\tspin_unlock(&cd->queue_lock);\n \t} else {\n \t\tif (rp->offset + count > rq->len)\n@@ -889,7 +886,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\tif (rp->offset >= rq->len) {\n \t\t\trp->offset = 0;\n \t\t\tspin_lock(&cd->queue_lock);\n-\t\t\tlist_move(&rp->q.list, &rq->q.list);\n+\t\t\trp->next_seqno = rq->seqno + 1;\n \t\t\tspin_unlock(&cd->queue_lock);\n \t\t}\n \t\terr = 0;\n@@ -901,7 +898,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trq->readers--;\n \t\tif (rq->readers == 0 &&\n \t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n-\t\t\tlist_del(&rq->q.list);\n+\t\t\tlist_del(&rq->list);\n \t\t\tspin_unlock(&cd->queue_lock);\n \t\t\tcache_put(rq->item, cd);\n \t\t\tkfree(rq->buf);\n@@ -976,7 +973,6 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n {\n \t__poll_t mask;\n \tstruct cache_reader *rp = filp->private_data;\n-\tstruct cache_queue *cq;\n \n \tpoll_wait(filp, &cd->queue_wait, wait);\n \n@@ -988,12 +984,8 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \n \tspin_lock(&cd->queue_lock);\n \n-\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\tif (!cq->reader) {\n-\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n-\t\t\tbreak;\n-\t\t}\n+\tif (cache_next_request(cd, rp->next_seqno))\n+\t\tmask |= EPOLLIN | EPOLLRDNORM;\n \tspin_unlock(&cd->queue_lock);\n \treturn mask;\n }\n@@ -1004,7 +996,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n {\n \tint len = 0;\n \tstruct cache_reader *rp = filp->private_data;\n-\tstruct cache_queue *cq;\n+\tstruct cache_request *rq;\n \n \tif (cmd != FIONREAD || !rp)\n \t\treturn -EINVAL;\n@@ -1014,14 +1006,9 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \t/* only find the length remaining in current request,\n \t * or the length of the next request\n \t */\n-\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\tif (!cq->reader) {\n-\t\t\tstruct cache_request *cr =\n-\t\t\t\tcontainer_of(cq, struct cache_request, q);\n-\t\t\tlen = cr->len - rp->offset;\n-\t\t\tbreak;\n-\t\t}\n+\trq = cache_next_request(cd, rp->next_seqno);\n+\tif (rq)\n+\t\tlen = rq->len - rp->offset;\n \tspin_unlock(&cd->queue_lock);\n \n \treturn put_user(len, (int __user *)arg);\n@@ -1042,10 +1029,10 @@ static int cache_open(struct inode *inode, struct file *filp,\n \t\t\treturn -ENOMEM;\n \t\t}\n \t\trp->offset = 0;\n-\t\trp->q.reader = 1;\n+\t\trp->next_seqno = 0;\n \n \t\tspin_lock(&cd->queue_lock);\n-\t\tlist_add(&rp->q.list, &cd->queue);\n+\t\tlist_add(&rp->list, &cd->readers);\n \t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (filp->f_mode & FMODE_WRITE)\n@@ -1062,17 +1049,14 @@ static int cache_release(struct inode *inode, struct file *filp,\n \tif (rp) {\n \t\tspin_lock(&cd->queue_lock);\n \t\tif (rp->offset) {\n-\t\t\tstruct cache_queue *cq;\n-\t\t\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t\t\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\t\t\tif (!cq->reader) {\n-\t\t\t\t\tcontainer_of(cq, struct cache_request, q)\n-\t\t\t\t\t\t->readers--;\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n+\t\t\tstruct cache_request *rq;\n+\n+\t\t\trq = cache_next_request(cd, rp->next_seqno);\n+\t\t\tif (rq)\n+\t\t\t\trq->readers--;\n \t\t\trp->offset = 0;\n \t\t}\n-\t\tlist_del(&rp->q.list);\n+\t\tlist_del(&rp->list);\n \t\tspin_unlock(&cd->queue_lock);\n \n \t\tfilp->private_data = NULL;\n@@ -1091,27 +1075,24 @@ static int cache_release(struct inode *inode, struct file *filp,\n \n static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n {\n-\tstruct cache_queue *cq, *tmp;\n-\tstruct cache_request *cr;\n+\tstruct cache_request *cr, *tmp;\n \tLIST_HEAD(dequeued);\n \n \tspin_lock(&detail->queue_lock);\n-\tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n-\t\tif (!cq->reader) {\n-\t\t\tcr = container_of(cq, struct cache_request, q);\n-\t\t\tif (cr->item != ch)\n-\t\t\t\tcontinue;\n-\t\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n-\t\t\t\t/* Lost a race and it is pending again */\n-\t\t\t\tbreak;\n-\t\t\tif (cr->readers != 0)\n-\t\t\t\tcontinue;\n-\t\t\tlist_move(&cr->q.list, &dequeued);\n-\t\t}\n+\tlist_for_each_entry_safe(cr, tmp, &detail->requests, list) {\n+\t\tif (cr->item != ch)\n+\t\t\tcontinue;\n+\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n+\t\t\t/* Lost a race and it is pending again */\n+\t\t\tbreak;\n+\t\tif (cr->readers != 0)\n+\t\t\tcontinue;\n+\t\tlist_move(&cr->list, &dequeued);\n+\t}\n \tspin_unlock(&detail->queue_lock);\n \twhile (!list_empty(&dequeued)) {\n-\t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n-\t\tlist_del(&cr->q.list);\n+\t\tcr = list_entry(dequeued.next, struct cache_request, list);\n+\t\tlist_del(&cr->list);\n \t\tcache_put(cr->item, detail);\n \t\tkfree(cr->buf);\n \t\tkfree(cr);\n@@ -1229,14 +1210,14 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t\treturn -EAGAIN;\n \t}\n \n-\tcrq->q.reader = 0;\n \tcrq->buf = buf;\n \tcrq->len = 0;\n \tcrq->readers = 0;\n \tspin_lock(&detail->queue_lock);\n \tif (test_bit(CACHE_PENDING, &h->flags)) {\n \t\tcrq->item = cache_get(h);\n-\t\tlist_add_tail(&crq->q.list, &detail->queue);\n+\t\tcrq->seqno = detail->next_seqno++;\n+\t\tlist_add_tail(&crq->list, &detail->requests);\n \t\ttrace_cache_entry_upcall(detail, h);\n \t} else\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chuck Lever",
              "summary": "reviewer noted that the patch does not address the issue of cache_detail->seq being accessed without seq_lock held, which could lead to a race condition",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "race condition",
                "missing lock"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nOn Fri, 20 Feb 2026 07:26:02 -0500, Jeff Layton wrote:",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chuck Lever",
              "summary": "reviewer noted that the per-cache_detail spinlock is not properly dropped before calling try_to_unmap(), potentially causing a lock ordering violation",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential deadlock",
                "locking issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Applied to nfsd-testing, thanks!\n\n[1/3] sunrpc: convert queue_lock from global spinlock to per-cache_detail lock\n      commit: 8da8f32e9a2702259cdf97e2f8f492ef9c79db65\n[2/3] sunrpc: convert queue_wait from global to per-cache_detail waitqueue\n      commit: 802261d8b58dd2f41a52a0c92776e0fb45619efe\n[3/3] sunrpc: split cache_detail queue into request and reader lists\n      commit: 0eb3d9dc71ada02909e4dfe9cb54e703ec717ed4\n\n--\nChuck Lever",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "NeilBrown",
              "summary": "Reviewer NeilBrown noted that the code should also decrement ->readers and check if it's zero, similar to another place in the code, and suggested fixing this potential bug now",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential bug",
                "requested fix"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hmm..  The other place where we decrement ->readers we then check if it\nis zero and if CACHE_PENDING is clear - and do something.\nI suspect we should do that here.\nThis bug (if I'm right and it is a bug) if there before you patch, but\nnow might be a good time to fix it?\n\nThanks.  Nice cleanups.\n\nNeilBrown",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-22",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v1 1/2] NFSD: Defer sub-object cleanup in export put callbacks",
          "message_id": "ae5f1ee0c43eda94f86bc60b1b223c86e0f24805.camel@kernel.org",
          "url": "https://lore.kernel.org/all/ae5f1ee0c43eda94f86bc60b1b223c86e0f24805.camel@kernel.org/",
          "date": "2026-02-20T15:50:11Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Chuck Lever (author)",
              "summary": "The author addressed the concern that svc_export_put() and expkey_put() call path_put() and auth_domain_put() before the RCU grace period, which can lead to a NULL pointer dereference in d_path(). The author agreed to restructure the code to use queue_rcu_work(), which defers the callback until after the RCU grace period and executes it in process context where sleeping is permitted. A dedicated workqueue is used for the shutdown drain to only NFSD export release work items.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "restructured"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nsvc_export_put() calls path_put() and auth_domain_put() immediately\nwhen the last reference drops, before the RCU grace period. RCU\nreaders in e_show() and c_show() access both ex_path (via\nseq_path/d_path) and ex_client->name (via seq_escape) without\nholding a reference. If cache_clean removes the entry and drops the\nlast reference concurrently, the sub-objects are freed while still\nin use, producing a NULL pointer dereference in d_path.\n\nCommit 2530766492ec (\"nfsd: fix UAF when access ex_uuid or\nex_stats\") moved kfree of ex_uuid and ex_stats into the\ncall_rcu callback, but left path_put() and auth_domain_put() running\nbefore the grace period because both may sleep and call_rcu\ncallbacks execute in softirq context.\n\nReplace call_rcu/kfree_rcu with queue_rcu_work(), which defers the\ncallback until after the RCU grace period and executes it in process\ncontext where sleeping is permitted. This allows path_put() and\nauth_domain_put() to be moved into the deferred callback alongside\nthe other resource releases. Apply the same fix to expkey_put(),\nwhich has the identical pattern with ek_path and ek_client.\n\nA dedicated workqueue scopes the shutdown drain to only NFSD\nexport release work items; flushing the shared\nsystem_unbound_wq would stall on unrelated work from other\nsubsystems. nfsd_export_shutdown() uses rcu_barrier() followed\nby flush_workqueue() to ensure all deferred release callbacks\ncomplete before the export caches are destroyed.\n\nReported-by: Misbah Anjum N <misanjum@linux.ibm.com>\nCloses: https://lore.kernel.org/linux-nfs/dcd371d3a95815a84ba7de52cef447b8@linux.ibm.com/\nFixes: c224edca7af0 (\"nfsd: no need get cache ref when protected by rcu\")\nFixes: 1b10f0b603c0 (\"SUNRPC: no need get cache ref when protected by rcu\")\nSigned-off-by: Chuck Lever <chuck.lever@oracle.com>\n---\n fs/nfsd/export.c | 63 +++++++++++++++++++++++++++++++++++++++++-------\n fs/nfsd/export.h |  7 ++++--\n fs/nfsd/nfsctl.c |  8 +++++-\n 3 files changed, 66 insertions(+), 12 deletions(-)\n\ndiff --git a/fs/nfsd/export.c b/fs/nfsd/export.c\nindex 04b18f0f402f..53fe66784ed2 100644\n--- a/fs/nfsd/export.c\n+++ b/fs/nfsd/export.c\n@@ -36,19 +36,30 @@\n  * second map contains a reference to the entry in the first map.\n  */\n \n+static struct workqueue_struct *nfsd_export_wq;\n+\n #define\tEXPKEY_HASHBITS\t\t8\n #define\tEXPKEY_HASHMAX\t\t(1 << EXPKEY_HASHBITS)\n #define\tEXPKEY_HASHMASK\t\t(EXPKEY_HASHMAX -1)\n \n-static void expkey_put(struct kref *ref)\n+static void expkey_release(struct work_struct *work)\n {\n-\tstruct svc_expkey *key = container_of(ref, struct svc_expkey, h.ref);\n+\tstruct svc_expkey *key = container_of(to_rcu_work(work),\n+\t\t\t\t\t      struct svc_expkey, ek_rwork);\n \n \tif (test_bit(CACHE_VALID, &key->h.flags) &&\n \t    !test_bit(CACHE_NEGATIVE, &key->h.flags))\n \t\tpath_put(&key->ek_path);\n \tauth_domain_put(key->ek_client);\n-\tkfree_rcu(key, ek_rcu);\n+\tkfree(key);\n+}\n+\n+static void expkey_put(struct kref *ref)\n+{\n+\tstruct svc_expkey *key = container_of(ref, struct svc_expkey, h.ref);\n+\n+\tINIT_RCU_WORK(&key->ek_rwork, expkey_release);\n+\tqueue_rcu_work(nfsd_export_wq, &key->ek_rwork);\n }\n \n static int expkey_upcall(struct cache_detail *cd, struct cache_head *h)\n@@ -353,11 +364,13 @@ static void export_stats_destroy(struct export_stats *stats)\n \t\t\t\t\t    EXP_STATS_COUNTERS_NUM);\n }\n \n-static void svc_export_release(struct rcu_head *rcu_head)\n+static void svc_export_release(struct work_struct *work)\n {\n-\tstruct svc_export *exp = container_of(rcu_head, struct svc_export,\n-\t\t\tex_rcu);\n+\tstruct svc_export *exp = container_of(to_rcu_work(work),\n+\t\t\t\t\t      struct svc_export, ex_rwork);\n \n+\tpath_put(&exp->ex_path);\n+\tauth_domain_put(exp->ex_client);\n \tnfsd4_fslocs_free(&exp->ex_fslocs);\n \texport_stats_destroy(exp->ex_stats);\n \tkfree(exp->ex_stats);\n@@ -369,9 +382,8 @@ static void svc_export_put(struct kref *ref)\n {\n \tstruct svc_export *exp = container_of(ref, struct svc_export, h.ref);\n \n-\tpath_put(&exp->ex_path);\n-\tauth_domain_put(exp->ex_client);\n-\tcall_rcu(&exp->ex_rcu, svc_export_release);\n+\tINIT_RCU_WORK(&exp->ex_rwork, svc_export_release);\n+\tqueue_rcu_work(nfsd_export_wq, &exp->ex_rwork);\n }\n \n static int svc_export_upcall(struct cache_detail *cd, struct cache_head *h)\n@@ -1481,6 +1493,36 @@ const struct seq_operations nfs_exports_op = {\n \t.show\t= e_show,\n };\n \n+/**\n+ * nfsd_export_wq_init - allocate the export release workqueue\n+ *\n+ * Called once at module load. The workqueue runs deferred svc_export and\n+ * svc_expkey release work scheduled by queue_rcu_work() in the cache put\n+ * callbacks.\n+ *\n+ * Return values:\n+ *   %0: workqueue allocated\n+ *   %-ENOMEM: allocation failed\n+ */\n+int nfsd_export_wq_init(void)\n+{\n+\tnfsd_export_wq = alloc_workqueue(\"nfsd_export\", WQ_UNBOUND, 0);\n+\tif (!nfsd_export_wq)\n+\t\treturn -ENOMEM;\n+\treturn 0;\n+}\n+\n+/**\n+ * nfsd_export_wq_shutdown - drain and free the export release workqueue\n+ *\n+ * Called once at module unload. Per-namespace teardown in\n+ * nfsd_export_shutdown() has already drained all deferred work.\n+ */\n+void nfsd_export_wq_shutdown(void)\n+{\n+\tdestroy_workqueue(nfsd_export_wq);\n+}\n+\n /*\n  * Initialize the exports module.\n  */\n@@ -1542,6 +1584,9 @@ nfsd_export_shutdown(struct net *net)\n \n \tcache_unregister_net(nn->svc_expkey_cache, net);\n \tcache_unregister_net(nn->svc_export_cache, net);\n+\t/* Drain deferred export and expkey release work. */\n+\trcu_barrier();\n+\tflush_workqueue(nfsd_export_wq);\n \tcache_destroy_net(nn->svc_expkey_cache, net);\n \tcache_destroy_net(nn->svc_export_cache, net);\n \tsvcauth_unix_purge(net);\ndiff --git a/fs/nfsd/export.h b/fs/nfsd/export.h\nindex d2b09cd76145..b05399374574 100644\n--- a/fs/nfsd/export.h\n+++ b/fs/nfsd/export.h\n@@ -7,6 +7,7 @@\n \n #include <linux/sunrpc/cache.h>\n #include <linux/percpu_counter.h>\n+#include <linux/workqueue.h>\n #include <uapi/linux/nfsd/export.h>\n #include <linux/nfs4.h>\n \n@@ -75,7 +76,7 @@ struct svc_export {\n \tu32\t\t\tex_layout_types;\n \tstruct nfsd4_deviceid_map *ex_devid_map;\n \tstruct cache_detail\t*cd;\n-\tstruct rcu_head\t\tex_rcu;\n+\tstruct rcu_work\t\tex_rwork;\n \tunsigned long\t\tex_xprtsec_modes;\n \tstruct export_stats\t*ex_stats;\n };\n@@ -92,7 +93,7 @@ struct svc_expkey {\n \tu32\t\t\tek_fsid[6];\n \n \tstruct path\t\tek_path;\n-\tstruct rcu_head\t\tek_rcu;\n+\tstruct rcu_work\t\tek_rwork;\n };\n \n #define EX_ISSYNC(exp)\t\t(!((exp)->ex_flags & NFSEXP_ASYNC))\n@@ -110,6 +111,8 @@ __be32 check_nfsd_access(struct svc_export *exp, struct svc_rqst *rqstp,\n /*\n  * Function declarations\n  */\n+int\t\t\tnfsd_export_wq_init(void);\n+void\t\t\tnfsd_export_wq_shutdown(void);\n int\t\t\tnfsd_export_init(struct net *);\n void\t\t\tnfsd_export_shutdown(struct net *);\n void\t\t\tnfsd_export_flush(struct net *);\ndiff --git a/fs/nfsd/nfsctl.c b/fs/nfsd/nfsctl.c\nindex 664a3275c511..4166f59908f4 100644\n--- a/fs/nfsd/nfsctl.c\n+++ b/fs/nfsd/nfsctl.c\n@@ -2308,9 +2308,12 @@ static int __init init_nfsd(void)\n \tif (retval)\n \t\tgoto out_free_pnfs;\n \tnfsd_lockd_init();\t/* lockd->nfsd callbacks */\n+\tretval = nfsd_export_wq_init();\n+\tif (retval)\n+\t\tgoto out_free_lockd;\n \tretval = register_pernet_subsys(&nfsd_net_ops);\n \tif (retval < 0)\n-\t\tgoto out_free_lockd;\n+\t\tgoto out_free_export_wq;\n \tretval = register_cld_notifier();\n \tif (retval)\n \t\tgoto out_free_subsys;\n@@ -2339,6 +2342,8 @@ static int __init init_nfsd(void)\n \tunregister_cld_notifier();\n out_free_subsys:\n \tunregister_pernet_subsys(&nfsd_net_ops);\n+out_free_export_wq:\n+\tnfsd_export_wq_shutdown();\n out_free_lockd:\n \tnfsd_lockd_shutdown();\n \tnfsd_drc_slab_free();\n@@ -2359,6 +2364,7 @@ static void __exit exit_nfsd(void)\n \tnfsd4_destroy_laundry_wq();\n \tunregister_cld_notifier();\n \tunregister_pernet_subsys(&nfsd_net_ops);\n+\tnfsd_export_wq_shutdown();\n \tnfsd_drc_slab_free();\n \tnfsd_lockd_shutdown();\n \tnfsd4_free_slabs();\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Chuck Lever (author)",
              "summary": "The author addressed a concern about the /proc/fs/nfs/exports proc entry being freed while still-open file descriptors exist, explaining that holding a reference on the struct net for the lifetime of the open file descriptor prevents nfsd_net_exit() from running and thus prevents nfsd_export_shutdown() from freeing the cache. The author confirmed this fix is sufficient to address the issue.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a specific technical concern",
                "confirmed the proposed solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nThe /proc/fs/nfs/exports proc entry is created at module init\nand persists for the module's lifetime. exports_proc_open()\ncaptures the caller's current network namespace and stores\nits svc_export_cache in seq->private, but takes no reference\non the namespace. If the namespace is subsequently torn down\n(e.g. container destruction after the opener does setns() to a\ndifferent namespace), nfsd_net_exit() calls nfsd_export_shutdown()\nwhich frees the cache. Subsequent reads on the still-open fd\ndereference the freed cache_detail, walking a freed hash table.\n\nHold a reference on the struct net for the lifetime of the open\nfile descriptor. This prevents nfsd_net_exit() from running --\nand thus prevents nfsd_export_shutdown() from freeing the cache\n-- while any exports fd is open. cache_detail already stores\nits net pointer (cd->net, set by cache_create_net()), so\nexports_release() can retrieve it without additional per-file\nstorage.\n\nReported-by: Misbah Anjum N <misanjum@linux.ibm.com>\nCloses: https://lore.kernel.org/linux-nfs/dcd371d3a95815a84ba7de52cef447b8@linux.ibm.com/\nFixes: 96d851c4d28d (\"nfsd: use proper net while reading \"exports\" file\")\nSigned-off-by: Chuck Lever <chuck.lever@oracle.com>\n---\n fs/nfsd/nfsctl.c | 14 ++++++++++++--\n 1 file changed, 12 insertions(+), 2 deletions(-)\n\ndiff --git a/fs/nfsd/nfsctl.c b/fs/nfsd/nfsctl.c\nindex 4166f59908f4..3d5a676e1d14 100644\n--- a/fs/nfsd/nfsctl.c\n+++ b/fs/nfsd/nfsctl.c\n@@ -149,9 +149,19 @@ static int exports_net_open(struct net *net, struct file *file)\n \n \tseq = file->private_data;\n \tseq->private = nn->svc_export_cache;\n+\tget_net(net);\n \treturn 0;\n }\n \n+static int exports_release(struct inode *inode, struct file *file)\n+{\n+\tstruct seq_file *seq = file->private_data;\n+\tstruct cache_detail *cd = seq->private;\n+\n+\tput_net(cd->net);\n+\treturn seq_release(inode, file);\n+}\n+\n static int exports_nfsd_open(struct inode *inode, struct file *file)\n {\n \treturn exports_net_open(inode->i_sb->s_fs_info, file);\n@@ -161,7 +171,7 @@ static const struct file_operations exports_nfsd_operations = {\n \t.open\t\t= exports_nfsd_open,\n \t.read\t\t= seq_read,\n \t.llseek\t\t= seq_lseek,\n-\t.release\t= seq_release,\n+\t.release\t= exports_release,\n };\n \n static int export_features_show(struct seq_file *m, void *v)\n@@ -1376,7 +1386,7 @@ static const struct proc_ops exports_proc_ops = {\n \t.proc_open\t= exports_proc_open,\n \t.proc_read\t= seq_read,\n \t.proc_lseek\t= seq_lseek,\n-\t.proc_release\t= seq_release,\n+\t.proc_release\t= exports_release,\n };\n \n static int create_proc_exports_entry(void)\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that the patch does not address the original crashes reported on PowerPC systems, as it only contains compile and regression tests.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "test coverage",
                "regression testing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Looks good.\n\nReviwed-by: Jeff Layton <jlayton@kernel.org>",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-20",
              "analysis_source": "heuristic"
            },
            {
              "author": "NeilBrown",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-22",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v1 2/2] NFSD: Hold net reference for the lifetime of /proc/fs/nfs/exports fd",
          "message_id": "2fa166bf4183cbc049350dc892eeb6656d9ed081.camel@kernel.org",
          "url": "https://lore.kernel.org/all/2fa166bf4183cbc049350dc892eeb6656d9ed081.camel@kernel.org/",
          "date": "2026-02-20T15:52:23Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Chuck Lever (author)",
              "summary": "The author addressed concerns about concurrent access to sub-objects while the last reference is being dropped, explaining that RCU readers in e_show() and c_show() access ex_path and ex_client->name without holding a reference. The author agreed to restructure the code using queue_rcu_work(), which defers the callback until after the RCU grace period and executes it in process context where sleeping is permitted.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nsvc_export_put() calls path_put() and auth_domain_put() immediately\nwhen the last reference drops, before the RCU grace period. RCU\nreaders in e_show() and c_show() access both ex_path (via\nseq_path/d_path) and ex_client->name (via seq_escape) without\nholding a reference. If cache_clean removes the entry and drops the\nlast reference concurrently, the sub-objects are freed while still\nin use, producing a NULL pointer dereference in d_path.\n\nCommit 2530766492ec (\"nfsd: fix UAF when access ex_uuid or\nex_stats\") moved kfree of ex_uuid and ex_stats into the\ncall_rcu callback, but left path_put() and auth_domain_put() running\nbefore the grace period because both may sleep and call_rcu\ncallbacks execute in softirq context.\n\nReplace call_rcu/kfree_rcu with queue_rcu_work(), which defers the\ncallback until after the RCU grace period and executes it in process\ncontext where sleeping is permitted. This allows path_put() and\nauth_domain_put() to be moved into the deferred callback alongside\nthe other resource releases. Apply the same fix to expkey_put(),\nwhich has the identical pattern with ek_path and ek_client.\n\nA dedicated workqueue scopes the shutdown drain to only NFSD\nexport release work items; flushing the shared\nsystem_unbound_wq would stall on unrelated work from other\nsubsystems. nfsd_export_shutdown() uses rcu_barrier() followed\nby flush_workqueue() to ensure all deferred release callbacks\ncomplete before the export caches are destroyed.\n\nReported-by: Misbah Anjum N <misanjum@linux.ibm.com>\nCloses: https://lore.kernel.org/linux-nfs/dcd371d3a95815a84ba7de52cef447b8@linux.ibm.com/\nFixes: c224edca7af0 (\"nfsd: no need get cache ref when protected by rcu\")\nFixes: 1b10f0b603c0 (\"SUNRPC: no need get cache ref when protected by rcu\")\nSigned-off-by: Chuck Lever <chuck.lever@oracle.com>\n---\n fs/nfsd/export.c | 63 +++++++++++++++++++++++++++++++++++++++++-------\n fs/nfsd/export.h |  7 ++++--\n fs/nfsd/nfsctl.c |  8 +++++-\n 3 files changed, 66 insertions(+), 12 deletions(-)\n\ndiff --git a/fs/nfsd/export.c b/fs/nfsd/export.c\nindex 04b18f0f402f..53fe66784ed2 100644\n--- a/fs/nfsd/export.c\n+++ b/fs/nfsd/export.c\n@@ -36,19 +36,30 @@\n  * second map contains a reference to the entry in the first map.\n  */\n \n+static struct workqueue_struct *nfsd_export_wq;\n+\n #define\tEXPKEY_HASHBITS\t\t8\n #define\tEXPKEY_HASHMAX\t\t(1 << EXPKEY_HASHBITS)\n #define\tEXPKEY_HASHMASK\t\t(EXPKEY_HASHMAX -1)\n \n-static void expkey_put(struct kref *ref)\n+static void expkey_release(struct work_struct *work)\n {\n-\tstruct svc_expkey *key = container_of(ref, struct svc_expkey, h.ref);\n+\tstruct svc_expkey *key = container_of(to_rcu_work(work),\n+\t\t\t\t\t      struct svc_expkey, ek_rwork);\n \n \tif (test_bit(CACHE_VALID, &key->h.flags) &&\n \t    !test_bit(CACHE_NEGATIVE, &key->h.flags))\n \t\tpath_put(&key->ek_path);\n \tauth_domain_put(key->ek_client);\n-\tkfree_rcu(key, ek_rcu);\n+\tkfree(key);\n+}\n+\n+static void expkey_put(struct kref *ref)\n+{\n+\tstruct svc_expkey *key = container_of(ref, struct svc_expkey, h.ref);\n+\n+\tINIT_RCU_WORK(&key->ek_rwork, expkey_release);\n+\tqueue_rcu_work(nfsd_export_wq, &key->ek_rwork);\n }\n \n static int expkey_upcall(struct cache_detail *cd, struct cache_head *h)\n@@ -353,11 +364,13 @@ static void export_stats_destroy(struct export_stats *stats)\n \t\t\t\t\t    EXP_STATS_COUNTERS_NUM);\n }\n \n-static void svc_export_release(struct rcu_head *rcu_head)\n+static void svc_export_release(struct work_struct *work)\n {\n-\tstruct svc_export *exp = container_of(rcu_head, struct svc_export,\n-\t\t\tex_rcu);\n+\tstruct svc_export *exp = container_of(to_rcu_work(work),\n+\t\t\t\t\t      struct svc_export, ex_rwork);\n \n+\tpath_put(&exp->ex_path);\n+\tauth_domain_put(exp->ex_client);\n \tnfsd4_fslocs_free(&exp->ex_fslocs);\n \texport_stats_destroy(exp->ex_stats);\n \tkfree(exp->ex_stats);\n@@ -369,9 +382,8 @@ static void svc_export_put(struct kref *ref)\n {\n \tstruct svc_export *exp = container_of(ref, struct svc_export, h.ref);\n \n-\tpath_put(&exp->ex_path);\n-\tauth_domain_put(exp->ex_client);\n-\tcall_rcu(&exp->ex_rcu, svc_export_release);\n+\tINIT_RCU_WORK(&exp->ex_rwork, svc_export_release);\n+\tqueue_rcu_work(nfsd_export_wq, &exp->ex_rwork);\n }\n \n static int svc_export_upcall(struct cache_detail *cd, struct cache_head *h)\n@@ -1481,6 +1493,36 @@ const struct seq_operations nfs_exports_op = {\n \t.show\t= e_show,\n };\n \n+/**\n+ * nfsd_export_wq_init - allocate the export release workqueue\n+ *\n+ * Called once at module load. The workqueue runs deferred svc_export and\n+ * svc_expkey release work scheduled by queue_rcu_work() in the cache put\n+ * callbacks.\n+ *\n+ * Return values:\n+ *   %0: workqueue allocated\n+ *   %-ENOMEM: allocation failed\n+ */\n+int nfsd_export_wq_init(void)\n+{\n+\tnfsd_export_wq = alloc_workqueue(\"nfsd_export\", WQ_UNBOUND, 0);\n+\tif (!nfsd_export_wq)\n+\t\treturn -ENOMEM;\n+\treturn 0;\n+}\n+\n+/**\n+ * nfsd_export_wq_shutdown - drain and free the export release workqueue\n+ *\n+ * Called once at module unload. Per-namespace teardown in\n+ * nfsd_export_shutdown() has already drained all deferred work.\n+ */\n+void nfsd_export_wq_shutdown(void)\n+{\n+\tdestroy_workqueue(nfsd_export_wq);\n+}\n+\n /*\n  * Initialize the exports module.\n  */\n@@ -1542,6 +1584,9 @@ nfsd_export_shutdown(struct net *net)\n \n \tcache_unregister_net(nn->svc_expkey_cache, net);\n \tcache_unregister_net(nn->svc_export_cache, net);\n+\t/* Drain deferred export and expkey release work. */\n+\trcu_barrier();\n+\tflush_workqueue(nfsd_export_wq);\n \tcache_destroy_net(nn->svc_expkey_cache, net);\n \tcache_destroy_net(nn->svc_export_cache, net);\n \tsvcauth_unix_purge(net);\ndiff --git a/fs/nfsd/export.h b/fs/nfsd/export.h\nindex d2b09cd76145..b05399374574 100644\n--- a/fs/nfsd/export.h\n+++ b/fs/nfsd/export.h\n@@ -7,6 +7,7 @@\n \n #include <linux/sunrpc/cache.h>\n #include <linux/percpu_counter.h>\n+#include <linux/workqueue.h>\n #include <uapi/linux/nfsd/export.h>\n #include <linux/nfs4.h>\n \n@@ -75,7 +76,7 @@ struct svc_export {\n \tu32\t\t\tex_layout_types;\n \tstruct nfsd4_deviceid_map *ex_devid_map;\n \tstruct cache_detail\t*cd;\n-\tstruct rcu_head\t\tex_rcu;\n+\tstruct rcu_work\t\tex_rwork;\n \tunsigned long\t\tex_xprtsec_modes;\n \tstruct export_stats\t*ex_stats;\n };\n@@ -92,7 +93,7 @@ struct svc_expkey {\n \tu32\t\t\tek_fsid[6];\n \n \tstruct path\t\tek_path;\n-\tstruct rcu_head\t\tek_rcu;\n+\tstruct rcu_work\t\tek_rwork;\n };\n \n #define EX_ISSYNC(exp)\t\t(!((exp)->ex_flags & NFSEXP_ASYNC))\n@@ -110,6 +111,8 @@ __be32 check_nfsd_access(struct svc_export *exp, struct svc_rqst *rqstp,\n /*\n  * Function declarations\n  */\n+int\t\t\tnfsd_export_wq_init(void);\n+void\t\t\tnfsd_export_wq_shutdown(void);\n int\t\t\tnfsd_export_init(struct net *);\n void\t\t\tnfsd_export_shutdown(struct net *);\n void\t\t\tnfsd_export_flush(struct net *);\ndiff --git a/fs/nfsd/nfsctl.c b/fs/nfsd/nfsctl.c\nindex 664a3275c511..4166f59908f4 100644\n--- a/fs/nfsd/nfsctl.c\n+++ b/fs/nfsd/nfsctl.c\n@@ -2308,9 +2308,12 @@ static int __init init_nfsd(void)\n \tif (retval)\n \t\tgoto out_free_pnfs;\n \tnfsd_lockd_init();\t/* lockd->nfsd callbacks */\n+\tretval = nfsd_export_wq_init();\n+\tif (retval)\n+\t\tgoto out_free_lockd;\n \tretval = register_pernet_subsys(&nfsd_net_ops);\n \tif (retval < 0)\n-\t\tgoto out_free_lockd;\n+\t\tgoto out_free_export_wq;\n \tretval = register_cld_notifier();\n \tif (retval)\n \t\tgoto out_free_subsys;\n@@ -2339,6 +2342,8 @@ static int __init init_nfsd(void)\n \tunregister_cld_notifier();\n out_free_subsys:\n \tunregister_pernet_subsys(&nfsd_net_ops);\n+out_free_export_wq:\n+\tnfsd_export_wq_shutdown();\n out_free_lockd:\n \tnfsd_lockd_shutdown();\n \tnfsd_drc_slab_free();\n@@ -2359,6 +2364,7 @@ static void __exit exit_nfsd(void)\n \tnfsd4_destroy_laundry_wq();\n \tunregister_cld_notifier();\n \tunregister_pernet_subsys(&nfsd_net_ops);\n+\tnfsd_export_wq_shutdown();\n \tnfsd_drc_slab_free();\n \tnfsd_lockd_shutdown();\n \tnfsd4_free_slabs();\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Chuck Lever (author)",
              "summary": "The author addressed a concern about the /proc/fs/nfs/exports fd being closed while its net reference is still held, which causes a crash when trying to access the freed cache_detail. The author explained that holding a reference on the struct net for the lifetime of the open file descriptor prevents nfsd_net_exit() from running and thus prevents nfsd_export_shutdown() from freeing the cache. A fix is planned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "planned a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nThe /proc/fs/nfs/exports proc entry is created at module init\nand persists for the module's lifetime. exports_proc_open()\ncaptures the caller's current network namespace and stores\nits svc_export_cache in seq->private, but takes no reference\non the namespace. If the namespace is subsequently torn down\n(e.g. container destruction after the opener does setns() to a\ndifferent namespace), nfsd_net_exit() calls nfsd_export_shutdown()\nwhich frees the cache. Subsequent reads on the still-open fd\ndereference the freed cache_detail, walking a freed hash table.\n\nHold a reference on the struct net for the lifetime of the open\nfile descriptor. This prevents nfsd_net_exit() from running --\nand thus prevents nfsd_export_shutdown() from freeing the cache\n-- while any exports fd is open. cache_detail already stores\nits net pointer (cd->net, set by cache_create_net()), so\nexports_release() can retrieve it without additional per-file\nstorage.\n\nReported-by: Misbah Anjum N <misanjum@linux.ibm.com>\nCloses: https://lore.kernel.org/linux-nfs/dcd371d3a95815a84ba7de52cef447b8@linux.ibm.com/\nFixes: 96d851c4d28d (\"nfsd: use proper net while reading \"exports\" file\")\nSigned-off-by: Chuck Lever <chuck.lever@oracle.com>\n---\n fs/nfsd/nfsctl.c | 14 ++++++++++++--\n 1 file changed, 12 insertions(+), 2 deletions(-)\n\ndiff --git a/fs/nfsd/nfsctl.c b/fs/nfsd/nfsctl.c\nindex 4166f59908f4..3d5a676e1d14 100644\n--- a/fs/nfsd/nfsctl.c\n+++ b/fs/nfsd/nfsctl.c\n@@ -149,9 +149,19 @@ static int exports_net_open(struct net *net, struct file *file)\n \n \tseq = file->private_data;\n \tseq->private = nn->svc_export_cache;\n+\tget_net(net);\n \treturn 0;\n }\n \n+static int exports_release(struct inode *inode, struct file *file)\n+{\n+\tstruct seq_file *seq = file->private_data;\n+\tstruct cache_detail *cd = seq->private;\n+\n+\tput_net(cd->net);\n+\treturn seq_release(inode, file);\n+}\n+\n static int exports_nfsd_open(struct inode *inode, struct file *file)\n {\n \treturn exports_net_open(inode->i_sb->s_fs_info, file);\n@@ -161,7 +171,7 @@ static const struct file_operations exports_nfsd_operations = {\n \t.open\t\t= exports_nfsd_open,\n \t.read\t\t= seq_read,\n \t.llseek\t\t= seq_lseek,\n-\t.release\t= seq_release,\n+\t.release\t= exports_release,\n };\n \n static int export_features_show(struct seq_file *m, void *v)\n@@ -1376,7 +1386,7 @@ static const struct proc_ops exports_proc_ops = {\n \t.proc_open\t= exports_proc_open,\n \t.proc_read\t= seq_read,\n \t.proc_lseek\t= seq_lseek,\n-\t.proc_release\t= seq_release,\n+\t.proc_release\t= exports_release,\n };\n \n static int create_proc_exports_entry(void)\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that the patch description does not mention any crashes on PowerPC systems, and requested someone with a PowerPC system to test whether the patches actually address the reported crashes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested testing on specific hardware"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Looks good.\n\nReviwed-by: Jeff Layton <jlayton@kernel.org>",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-20",
              "analysis_source": "heuristic"
            },
            {
              "author": "NeilBrown",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-22",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joanne Koong",
      "primary_email": "joannelkoong@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] io_uring/rsrc: clean up buffer cloning arg validation (for 6.18-stable tree)",
          "message_id": "CAJnrk1YA9hk5Mv0BXFe+TcWLXsNLpWtcA-gy+k03zDt4f0z7zg@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1YA9hk5Mv0BXFe+TcWLXsNLpWtcA-gy+k03zDt4f0z7zg@mail.gmail.com/",
          "date": "2026-02-20T18:20:08Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch cleans up the buffer cloning argument validation in io_uring/rsrc to fix a dependency issue for commit 5b804b8f1e0d in the 6.18-stable tree.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jens Axboe",
              "summary": "Approved the patch as it fixes a dependency issue for commit 5b804b8f1e0d in the 6.18-stable tree.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "approved"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On 2/20/26 11:19 AM, Joanne Koong wrote:\n> Commit id upstream: b8201b50e403815f941d1c6581a27fdbfe7d0fd4\n> (\"io_uring/rsrc: clean up buffer cloning arg validation\")\n> Link to the patch:\n> https://lore.kernel.org/io-uring/20251204215116.2642044-1-joannelkoong@gmail.com/#t\n> Kernel version to apply it to: 6.18-stable tree\n> \n> Hi stable@,\n> \n> Chris Mason recently detected that this patch is a required dependency\n> for commit 5b804b8f1e0d (\"io_uring/rsrc: fix lost entries after cloned\n> range\") in the 6.18-stable tree [1]. Without this patch, the changes\n> in commit 5b804b8f1e0d use an incorrect value for nbufs when it\n> assigns \"i = nbufs\" [2].\n> \n> Could you please apply this patch to the 6.18-stable tree as a\n> dependency fix needed for commit 5b804b8f1e0d?\t\n> \n> Thanks,\n> Joanne\n> \n> [1] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=linux-6.18.y&id=5b804b8f1e0d66413774d43f7a4b78bba0ca6272\n> [2] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/rsrc.c?h=linux-6.18.y#n1252.\n\nFWIW, this is approved on my end. CC Greg.\n\n\n-- \nJens Axboe\n\n\n",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-22",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v1 0/1] iomap: don't mark folio uptodate if read IO has bytes pending",
          "message_id": "20260219003911.344478-1-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260219003911.344478-1-joannelkoong@gmail.com/",
          "date": "2026-02-19T00:41:04Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-19",
          "patch_summary": "This patch prevents a folio from being marked as uptodate if there are still bytes pending in the read IO operation. This fix addresses an issue where a folio could be incorrectly marked as uptodate due to a race condition between marking ranges uptodate and subtracting pending bytes. The approach taken is to modify the iomap code to check for pending bytes before marking the folio uptodate.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern that marking the folio uptodate in iomap_set_range_uptodate() when read IO has bytes pending can lead to the uptodate bit being cleared by folio_end_read(). The author agreed to fix this by not marking the folio uptodate in such cases and instead setting it through iomap_end_read()->folio_end_read().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If a folio has ifs metadata attached to it and the folio is partially\nread in through an async IO helper with the rest of it then being read\nin through post-EOF zeroing or as inline data, and the helper\nsuccessfully finishes the read first, then post-EOF zeroing / reading\ninline will mark the folio as uptodate in iomap_set_range_uptodate().\n\nThis is a problem because when the read completion path later calls\niomap_read_end(), it will call folio_end_read(), which sets the uptodate\nbit using XOR semantics. Calling folio_end_read() on a folio that was\nalready marked uptodate clears the uptodate bit.\n\nFix this by not marking the folio as uptodate if the read IO has bytes\npending. The folio uptodate state will be set in the read completion\npath through iomap_end_read() -> folio_end_read().\n\nReported-by: Wei Gao <wegao@suse.com>\nSuggested-by: Sasha Levin <sashal@kernel.org>\nTested-by: Wei Gao <wegao@suse.com>\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nFixes: b2f35ac4146d (\"iomap: add caller-provided callbacks for read and readahead\")\n---\n fs/iomap/buffered-io.c | 15 ++++++++++++---\n 1 file changed, 12 insertions(+), 3 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 58887513b894..4fc5ce963feb 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -80,18 +80,27 @@ static void iomap_set_range_uptodate(struct folio *folio, size_t off,\n {\n \tstruct iomap_folio_state *ifs = folio->private;\n \tunsigned long flags;\n-\tbool uptodate = true;\n+\tbool mark_uptodate = true;\n \n \tif (folio_test_uptodate(folio))\n \t\treturn;\n \n \tif (ifs) {\n \t\tspin_lock_irqsave(&ifs->state_lock, flags);\n-\t\tuptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n+\t\t/*\n+\t\t * If a read with bytes pending is in progress, we must not call\n+\t\t * folio_mark_uptodate(). The read completion path\n+\t\t * (iomap_read_end()) will call folio_end_read(), which uses XOR\n+\t\t * semantics to set the uptodate bit. If we set it here, the XOR\n+\t\t * in folio_end_read() will clear it, leaving the folio not\n+\t\t * uptodate.\n+\t\t */\n+\t\tmark_uptodate = ifs_set_range_uptodate(folio, ifs, off, len) &&\n+\t\t\t\t!ifs->read_bytes_pending;\n \t\tspin_unlock_irqrestore(&ifs->state_lock, flags);\n \t}\n \n-\tif (uptodate)\n+\tif (mark_uptodate)\n \t\tfolio_mark_uptodate(folio);\n }\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong suggested adding a link to a relevant discussion on linux-fsdevel and CC'ing stable@vger.kernel.org for the v6.19 release",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I would add:\n\nLink: https://lore.kernel.org/linux-fsdevel/aYbmy8JdgXwsGaPP@autotest-wegao.qe.prg2.suse.org/\nCc: <stable@vger.kernel.org> # v6.19\n\nsince the recent discussion around this was sort of buried in a\ndifferent thread, and the original patch is now in a released kernel.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong asked how difficult it would be to write a test for the patch, indicating that he understands and agrees with the change.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreement",
                "request-for-testing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yeah, that makes sense.  How difficult is this to write up as an fstest?\n\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n\n--D",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Matthew Wilcox",
              "summary": "Reviewer Matthew Wilcox expressed frustration that the iomap code has become overly complex, making it difficult to understand and explain how to fix the issue of marking a folio uptodate when read IO has bytes pending.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "frustration",
                "difficulty understanding"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This isn't \"the xor thing has come back to bite us\".  This is \"the iomap\ncode is now too complicated and I cannot figure out how to explain to\nJoanne that there's really a simple way to do this\".\n\nI'm going to have to set aside my current projects and redo the iomap\nreadahead/read_folio code myself, aren't I?",
              "reply_to": "Darrick Wong",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong expressed confusion about an alternative approach mentioned in a previous discussion, implying that he was not aware of the context and requested clarification on what this simpler way is.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "lack of understanding"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Well you could try explaining to me what that simpler way is?\n\n/me gets the sense he's missing a discussion somewhere...\n\n--D",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), but instead of addressing the technical concern, she provided a link to an unrelated prior discussion.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged the issue",
                "provided no explanation or fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This is the link to the prior discussion\nhttps://lore.kernel.org/linux-fsdevel/20251223223018.3295372-1-sashal@kernel.org/T/#mbd61eaa5fd1e8922caa479720232628e39b8c9da\n\nThanks,\nJoanne",
              "reply_to": "Darrick Wong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong noted that the read_bytes_pending field in iomap has inconsistent behavior across different IO paths, and suggested consolidating the read code into a single function to simplify the logic.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "<willy and I had a chat; this is a clumsy non-AI summary of it>\n\nI started looking at folio read state management in iomap, and made a\nfew observations that (I hope) match what willy's grumpy about.\n\nThere are three ways that iomap can be reading into the pagecache:\na) async ->readahead,\nb) synchronous ->read_folio (page faults), and\nc) synchronous ->read_folio_range (pagecache write).\n\n(Note that (b) can call a different ->read_folio_range than (c), though\nall implementations seem to have the same function)\n\nAll three of these IO paths share the behavior that they try to fill out\nthe folio's contents and set the corresponding folio/ifs uptodate bits\nif that succeeds.  Folio contents can come from anywhere, whether it's:\n\ni) zeroing memory,\nii) copying from an inlinedata buffer, or\niii) asynchronously fetching the contents from somewhere\n\nIn the case of (c) above, if the read fails then we fail the write, and\nif the read succeeds then we start copying to the pagecache.\n\nHowever, (a) and (b) have this additional read_bytes_pending field in\nthe ifs that implements some extra tracking.  AFAICT the purpose of this\nfield is to ensure that we don't call folio_end_read prematurely if\nthere's an async read in progress.  This can happen if iomap_iter\nreturns a negative errno on a partially processed folio, I think?\n\nread_bytes_pending is initialized to the folio_size() at the start of a\nread and subtracted from when parts of the folio are supplied, whether\nthat's synchronous zeroing or asynchronous read ioend completion.  When\nthe field reaches zero, we can then call folio_end_read().\n\nBut then there are twists, like the fact that we only call\niomap_read_init() to set read_bytes_pending if we decide to do an\nasynchronous read.  Or that iomap_read_end and iomap_finish_folio_read\nhave awfully similar code.  I think in the case of (i) and (ii) we also\ndon't touch read_pending_bytes at all, and merely set the uptodate bits?\n\nThis is confusing to me.  It would be more straightforward (I think) if\nwe just did it for all cases instead of adding more conditionals.  IOWs,\nhow hard would it be to consolidate the read code so that there's one\nfunction that iomap calls when it has filled out part of a folio.  Is\nthat possible, even though we shouldn't be calling folio_end_read during\na pagecache write?\n\nAt the end of the day, however, there's a bug in Linus' tree and we need\nto fix it, so Joanne's patch is a sufficient bandaid until we can go\nclean this up.\n\n--D",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v1 11/11] io_uring/cmd: set selected buffer index in __io_uring_cmd_done()",
          "message_id": "20260210002852.1394504-12-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260210002852.1394504-12-joannelkoong@gmail.com/",
          "date": "2026-02-10T00:31:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-10",
          "patch_summary": "This patch modifies the __io_uring_cmd_done() function to set the selected buffer index in io_uring commands. This is part of a larger series that introduces kernel-managed buffer rings, where the kernel allocates and manages buffers on behalf of applications. The goal is to simplify buffer management for applications using io_uring. The patch builds upon previous changes that added support for kernel-managed buffer rings and recycling API. It ensures that the correct buffer index is set in the command, enabling proper handling of buffer selection and reuse.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about the logic in io_register_pbuf_ring() being overly complex and difficult to follow, which was raised by reviewer feedback. The author has refactored the logic into three new helper functions: io_copy_and_validate_buf_reg(), io_alloc_new_buffer_list(), and io_setup_pbuf_ring(). These helpers will be used to simplify the registration process for kernel-managed buffer rings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "refactored code",
                "new helper functions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Refactor the logic in io_register_pbuf_ring() into generic helpers:\n- io_copy_and_validate_buf_reg(): Copy out user arg and validate user\n  arg and buffer registration parameters\n- io_alloc_new_buffer_list(): Allocate and initialize a new buffer\n  list for the given buffer group ID\n- io_setup_pbuf_ring(): Sets up the physical buffer ring region and\n  handles memory mapping for provided buffer rings\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport which will need to reuse some of these helpers.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c | 129 +++++++++++++++++++++++++++++++-----------------\n 1 file changed, 85 insertions(+), 44 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 67d4fe576473..850b836f32ee 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -596,55 +596,73 @@ int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags)\n \treturn IOU_COMPLETE;\n }\n \n-int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+static int io_copy_and_validate_buf_reg(const void __user *arg,\n+\t\t\t\t\tstruct io_uring_buf_reg *reg,\n+\t\t\t\t\tunsigned int permitted_flags)\n {\n-\tstruct io_uring_buf_reg reg;\n-\tstruct io_buffer_list *bl;\n-\tstruct io_uring_region_desc rd;\n-\tstruct io_uring_buf_ring *br;\n-\tunsigned long mmap_offset;\n-\tunsigned long ring_size;\n-\tint ret;\n-\n-\tlockdep_assert_held(&ctx->uring_lock);\n-\n-\tif (copy_from_user(&reg, arg, sizeof(reg)))\n+\tif (copy_from_user(reg, arg, sizeof(*reg)))\n \t\treturn -EFAULT;\n-\tif (!mem_is_zero(reg.resv, sizeof(reg.resv)))\n+\n+\tif (!mem_is_zero(reg->resv, sizeof(reg->resv)))\n \t\treturn -EINVAL;\n-\tif (reg.flags & ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC))\n+\tif (reg->flags & ~permitted_flags)\n \t\treturn -EINVAL;\n-\tif (!is_power_of_2(reg.ring_entries))\n+\tif (!is_power_of_2(reg->ring_entries))\n \t\treturn -EINVAL;\n \t/* cannot disambiguate full vs empty due to head/tail size */\n-\tif (reg.ring_entries >= 65536)\n+\tif (reg->ring_entries >= 65536)\n \t\treturn -EINVAL;\n+\treturn 0;\n+}\n \n-\tbl = io_buffer_get_list(ctx, reg.bgid);\n-\tif (bl) {\n+static struct io_buffer_list *\n+io_alloc_new_buffer_list(struct io_ring_ctx *ctx,\n+\t\t\t const struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_buffer_list *list;\n+\n+\tlist = io_buffer_get_list(ctx, reg->bgid);\n+\tif (list) {\n \t\t/* if mapped buffer ring OR classic exists, don't allow */\n-\t\tif (bl->flags & IOBL_BUF_RING || !list_empty(&bl->buf_list))\n-\t\t\treturn -EEXIST;\n-\t\tio_destroy_bl(ctx, bl);\n+\t\tif (list->flags & IOBL_BUF_RING || !list_empty(&list->buf_list))\n+\t\t\treturn ERR_PTR(-EEXIST);\n+\t\tio_destroy_bl(ctx, list);\n \t}\n \n-\tbl = kzalloc(sizeof(*bl), GFP_KERNEL_ACCOUNT);\n-\tif (!bl)\n-\t\treturn -ENOMEM;\n+\tlist = kzalloc(sizeof(*list), GFP_KERNEL_ACCOUNT);\n+\tif (!list)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tlist->nr_entries = reg->ring_entries;\n+\tlist->mask = reg->ring_entries - 1;\n+\tlist->flags = IOBL_BUF_RING;\n+\n+\treturn list;\n+}\n+\n+static int io_setup_pbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t      const struct io_uring_buf_reg *reg,\n+\t\t\t      struct io_buffer_list *bl)\n+{\n+\tstruct io_uring_region_desc rd;\n+\tunsigned long mmap_offset;\n+\tunsigned long ring_size;\n+\tint ret;\n \n-\tmmap_offset = (unsigned long)reg.bgid << IORING_OFF_PBUF_SHIFT;\n-\tring_size = flex_array_size(br, bufs, reg.ring_entries);\n+\tmmap_offset = (unsigned long)reg->bgid << IORING_OFF_PBUF_SHIFT;\n+\tring_size = flex_array_size(bl->buf_ring, bufs, reg->ring_entries);\n \n \tmemset(&rd, 0, sizeof(rd));\n \trd.size = PAGE_ALIGN(ring_size);\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP)) {\n-\t\trd.user_addr = reg.ring_addr;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP)) {\n+\t\trd.user_addr = reg->ring_addr;\n \t\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n \t}\n+\n \tret = io_create_region(ctx, &bl->region, &rd, mmap_offset);\n \tif (ret)\n-\t\tgoto fail;\n-\tbr = io_region_get_ptr(&bl->region);\n+\t\treturn ret;\n+\tbl->buf_ring = io_region_get_ptr(&bl->region);\n \n #ifdef SHM_COLOUR\n \t/*\n@@ -656,25 +674,48 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t * should use IOU_PBUF_RING_MMAP instead, and liburing will handle\n \t * this transparently.\n \t */\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP) &&\n-\t    ((reg.ring_addr | (unsigned long)br) & (SHM_COLOUR - 1))) {\n-\t\tret = -EINVAL;\n-\t\tgoto fail;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP) &&\n+\t    ((reg->ring_addr | (unsigned long)bl->buf_ring) &\n+\t     (SHM_COLOUR - 1))) {\n+\t\tio_free_region(ctx->user, &bl->region);\n+\t\treturn -EINVAL;\n \t}\n #endif\n \n-\tbl->nr_entries = reg.ring_entries;\n-\tbl->mask = reg.ring_entries - 1;\n-\tbl->flags |= IOBL_BUF_RING;\n-\tbl->buf_ring = br;\n-\tif (reg.flags & IOU_PBUF_RING_INC)\n+\tif (reg->flags & IOU_PBUF_RING_INC)\n \t\tbl->flags |= IOBL_INC;\n+\n+\treturn 0;\n+}\n+\n+int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tunsigned int permitted_flags;\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tpermitted_flags = IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC;\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, permitted_flags);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_pbuf_ring(ctx, &reg, bl);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n \tret = io_buffer_add_list(ctx, bl, reg.bgid);\n-\tif (!ret)\n-\t\treturn 0;\n-fail:\n-\tio_free_region(ctx->user, &bl->region);\n-\tkfree(bl);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n \treturn ret;\n }\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the naming consistency of io_unregister_pbuf_ring() and its upcoming use for kernel-managed buffer rings, agreed to rename it to io_unregister_buf_ring() as a preparatory change.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "preparatory"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Use the more generic name io_unregister_buf_ring() as this function will\nbe used for unregistering both provided buffer rings and kernel-managed\nbuffer rings.\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c     | 2 +-\n io_uring/kbuf.h     | 2 +-\n io_uring/register.c | 2 +-\n 3 files changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 850b836f32ee..aa9b70b72db4 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -719,7 +719,7 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \treturn ret;\n }\n \n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n {\n \tstruct io_uring_buf_reg reg;\n \tstruct io_buffer_list *bl;\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex bf15e26520d3..40b44f4fdb15 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -74,7 +74,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags);\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 594b1f2ce875..0882cb34f851 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -841,7 +841,7 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-\t\tret = io_unregister_pbuf_ring(ctx, arg);\n+\t\tret = io_unregister_buf_ring(ctx, arg);\n \t\tbreak;\n \tcase IORING_REGISTER_SYNC_CANCEL:\n \t\tret = -EINVAL;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the implementation of kernel-managed buffer rings, explaining that they reuse validation and buffer list allocation helpers from earlier refactoring to support both application-provided and kernel-managed buffer rings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for kernel-managed buffer rings (kmbuf rings), which allow\nthe kernel to allocate and manage the backing buffers for a buffer\nring, rather than requiring the application to provide and manage them.\n\nThis introduces two new registration opcodes:\n- IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring\n- IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring\n\nThe existing io_uring_buf_reg structure is extended with a union to\nsupport both application-provided buffer rings (pbuf) and kernel-managed\nbuffer rings (kmbuf):\n- For pbuf rings: ring_addr specifies the user-provided ring address\n- For kmbuf rings: buf_size specifies the size of each buffer. buf_size\n  must be non-zero and page-aligned.\n\nThe implementation follows the same pattern as pbuf ring registration,\nreusing the validation and buffer list allocation helpers introduced in\nearlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as\nkernel-managed for appropriate handling in the I/O path.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  15 ++++-\n io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-\n io_uring/kbuf.h               |   7 ++-\n io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++\n io_uring/memmap.h             |   4 ++\n io_uring/register.c           |   7 +++\n 6 files changed, 219 insertions(+), 6 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex fc473af6feb4..a0889c1744bd 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -715,6 +715,10 @@ enum io_uring_register_op {\n \t/* register bpf filtering programs */\n \tIORING_REGISTER_BPF_FILTER\t\t= 37,\n \n+\t/* register/unregister kernel-managed ring buffer group */\n+\tIORING_REGISTER_KMBUF_RING\t\t= 38,\n+\tIORING_UNREGISTER_KMBUF_RING\t\t= 39,\n+\n \t/* this goes last */\n \tIORING_REGISTER_LAST,\n \n@@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {\n \tIOU_PBUF_RING_INC\t= 2,\n };\n \n-/* argument for IORING_(UN)REGISTER_PBUF_RING */\n+/* argument for IORING_(UN)REGISTER_PBUF_RING and\n+ * IORING_(UN)REGISTER_KMBUF_RING\n+ */\n struct io_uring_buf_reg {\n-\t__u64\tring_addr;\n+\tunion {\n+\t\t/* used for pbuf rings */\n+\t\t__u64\tring_addr;\n+\t\t/* used for kmbuf rings */\n+\t\t__u32   buf_size;\n+\t};\n \t__u32\tring_entries;\n \t__u16\tbgid;\n \t__u16\tflags;\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex aa9b70b72db4..9bc36451d083 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -427,10 +427,13 @@ static int io_remove_buffers_legacy(struct io_ring_ctx *ctx,\n \n static void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)\n {\n-\tif (bl->flags & IOBL_BUF_RING)\n+\tif (bl->flags & IOBL_BUF_RING) {\n \t\tio_free_region(ctx->user, &bl->region);\n-\telse\n+\t\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\t\tkfree(bl->buf_ring);\n+\t} else {\n \t\tio_remove_buffers_legacy(ctx, bl, -1U);\n+\t}\n \n \tkfree(bl);\n }\n@@ -779,3 +782,77 @@ struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n \t\treturn NULL;\n \treturn &bl->region;\n }\n+\n+static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_buffer_list *bl,\n+\t\t\t       struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_uring_buf_ring *ring;\n+\tunsigned long ring_size;\n+\tvoid *buf_region;\n+\tunsigned int i;\n+\tint ret;\n+\n+\t/* allocate pages for the ring structure */\n+\tring_size = flex_array_size(ring, bufs, bl->nr_entries);\n+\tring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);\n+\tif (!ring)\n+\t\treturn -ENOMEM;\n+\n+\tret = io_create_region_multi_buf(ctx, &bl->region, bl->nr_entries,\n+\t\t\t\t\t reg->buf_size);\n+\tif (ret) {\n+\t\tkfree(ring);\n+\t\treturn ret;\n+\t}\n+\n+\t/* initialize ring buf entries to point to the buffers */\n+\tbuf_region = bl->region.ptr;\n+\tfor (i = 0; i < bl->nr_entries; i++) {\n+\t\tstruct io_uring_buf *buf = &ring->bufs[i];\n+\n+\t\tbuf->addr = (u64)(uintptr_t)buf_region;\n+\t\tbuf->len = reg->buf_size;\n+\t\tbuf->bid = i;\n+\n+\t\tbuf_region += reg->buf_size;\n+\t}\n+\tring->tail = bl->nr_entries;\n+\n+\tbl->buf_ring = ring;\n+\tbl->flags |= IOBL_KERNEL_MANAGED;\n+\n+\treturn 0;\n+}\n+\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, 0);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tif (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_kmbuf_ring(ctx, bl, &reg);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n+\tret = io_buffer_add_list(ctx, bl, reg.bgid);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n+\treturn ret;\n+}\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 40b44f4fdb15..62c80a1ebf03 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -7,9 +7,11 @@\n \n enum {\n \t/* ring mapped provided buffers */\n-\tIOBL_BUF_RING\t= 1,\n+\tIOBL_BUF_RING\t\t= 1,\n \t/* buffers are consumed incrementally rather than always fully */\n-\tIOBL_INC\t= 2,\n+\tIOBL_INC\t\t= 2,\n+\t/* buffers are kernel managed */\n+\tIOBL_KERNEL_MANAGED\t= 4,\n };\n \n struct io_buffer_list {\n@@ -74,6 +76,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 89f56609e50a..8d37e93c0433 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -15,6 +15,28 @@\n #include \"rsrc.h\"\n #include \"zcrx.h\"\n \n+static void release_multi_buf_pages(struct page **pages, unsigned long nr_pages)\n+{\n+\tstruct page *page;\n+\tunsigned int nr, i = 0;\n+\n+\twhile (nr_pages) {\n+\t\tpage = pages[i];\n+\n+\t\tif (!page || WARN_ON_ONCE(page != compound_head(page)))\n+\t\t\treturn;\n+\n+\t\tnr = compound_nr(page);\n+\t\tput_page(page);\n+\n+\t\tif (WARN_ON_ONCE(nr > nr_pages))\n+\t\t\treturn;\n+\n+\t\ti += nr;\n+\t\tnr_pages -= nr;\n+\t}\n+}\n+\n static bool io_mem_alloc_compound(struct page **pages, int nr_pages,\n \t\t\t\t  size_t size, gfp_t gfp)\n {\n@@ -86,6 +108,8 @@ enum {\n \tIO_REGION_F_USER_PROVIDED\t\t= 2,\n \t/* only the first page in the array is ref'ed */\n \tIO_REGION_F_SINGLE_REF\t\t\t= 4,\n+\t/* pages in the array belong to multiple discrete allocations */\n+\tIO_REGION_F_MULTI_BUF\t\t\t= 8,\n };\n \n void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n@@ -98,6 +122,8 @@ void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n \n \t\tif (mr->flags & IO_REGION_F_USER_PROVIDED)\n \t\t\tunpin_user_pages(mr->pages, nr_refs);\n+\t\telse if (mr->flags & IO_REGION_F_MULTI_BUF)\n+\t\t\trelease_multi_buf_pages(mr->pages, nr_refs);\n \t\telse\n \t\t\trelease_pages(mr->pages, nr_refs);\n \n@@ -149,6 +175,54 @@ static int io_region_pin_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+static int io_region_allocate_pages_multi_buf(struct io_mapped_region *mr,\n+\t\t\t\t\t      unsigned int nr_bufs,\n+\t\t\t\t\t      unsigned int buf_size)\n+{\n+\tgfp_t gfp = GFP_USER | __GFP_ACCOUNT | __GFP_ZERO | __GFP_NOWARN;\n+\tstruct page **pages, **cur_pages;\n+\tunsigned int nr_allocated;\n+\tunsigned int buf_pages;\n+\tunsigned int i;\n+\n+\tif (!PAGE_ALIGNED(buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbuf_pages = buf_size >> PAGE_SHIFT;\n+\n+\tpages = kvmalloc_array(mr->nr_pages, sizeof(*pages), gfp);\n+\tif (!pages)\n+\t\treturn -ENOMEM;\n+\n+\tcur_pages = pages;\n+\n+\tfor (i = 0; i < nr_bufs; i++) {\n+\t\tif (io_mem_alloc_compound(cur_pages, buf_pages, buf_size,\n+\t\t\t\t\t  gfp)) {\n+\t\t\tcur_pages += buf_pages;\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tnr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,\n+\t\t\t\t\t\t     buf_pages, cur_pages);\n+\t\tif (nr_allocated != buf_pages) {\n+\t\t\tunsigned int total =\n+\t\t\t\t(cur_pages - pages) + nr_allocated;\n+\n+\t\t\trelease_multi_buf_pages(pages, total);\n+\t\t\tkvfree(pages);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tcur_pages += buf_pages;\n+\t}\n+\n+\tmr->flags |= IO_REGION_F_MULTI_BUF;\n+\tmr->pages = pages;\n+\n+\treturn 0;\n+}\n+\n static int io_region_allocate_pages(struct io_mapped_region *mr,\n \t\t\t\t    struct io_uring_region_desc *reg,\n \t\t\t\t    unsigned long mmap_offset)\n@@ -181,6 +255,43 @@ static int io_region_allocate_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size)\n+{\n+\tunsigned int nr_pages;\n+\tint ret;\n+\n+\tif (WARN_ON_ONCE(mr->pages || mr->ptr || mr->nr_pages))\n+\t\treturn -EFAULT;\n+\n+\tif (WARN_ON_ONCE(!nr_bufs || !buf_size || !PAGE_ALIGNED(buf_size)))\n+\t\treturn -EINVAL;\n+\n+\tif (check_mul_overflow(buf_size >> PAGE_SHIFT, nr_bufs, &nr_pages))\n+\t\treturn -EINVAL;\n+\n+\tif (ctx->user) {\n+\t\tret = __io_account_mem(ctx->user, nr_pages);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\t}\n+\tmr->nr_pages = nr_pages;\n+\n+\tret = io_region_allocate_pages_multi_buf(mr, nr_bufs, buf_size);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\tret = io_region_init_ptr(mr);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\treturn 0;\n+out_free:\n+\tio_free_region(ctx->user, mr);\n+\treturn ret;\n+}\n+\n int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset)\ndiff --git a/io_uring/memmap.h b/io_uring/memmap.h\nindex f4cfbb6b9a1f..3aa1167462ae 100644\n--- a/io_uring/memmap.h\n+++ b/io_uring/memmap.h\n@@ -22,6 +22,10 @@ int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset);\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size);\n+\n static inline void *io_region_get_ptr(struct io_mapped_region *mr)\n {\n \treturn mr->ptr;\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 0882cb34f851..2db8daaf8fde 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -837,7 +837,14 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\t\tbreak;\n \t\tret = io_register_pbuf_ring(ctx, arg);\n \t\tbreak;\n+\tcase IORING_REGISTER_KMBUF_RING:\n+\t\tret = -EINVAL;\n+\t\tif (!arg || nr_args != 1)\n+\t\t\tbreak;\n+\t\tret = io_register_kmbuf_ring(ctx, arg);\n+\t\tbreak;\n \tcase IORING_UNREGISTER_PBUF_RING:\n+\tcase IORING_UNREGISTER_KMBUF_RING:\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the buffer selection implementation in __io_uring_cmd_done() and explained that they are setting the selected buffer index as intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for mmapping kernel-managed buffer rings (kmbuf) to\nuserspace, allowing applications to access the kernel-allocated buffers.\n\nSimilar to application-provided buffer rings (pbuf), kmbuf rings use the\nbuffer group ID encoded in the mmap offset to identify which buffer ring\nto map. The implementation follows the same pattern as pbuf rings.\n\nNew mmap offset constants are introduced:\n  - IORING_OFF_KMBUF_RING (0x88000000): Base offset for kmbuf mappings\n  - IORING_OFF_KMBUF_SHIFT (16): Shift value to encode buffer group ID\n\nThe mmap offset encodes the bgid shifted by IORING_OFF_KMBUF_SHIFT.\nThe io_buf_get_region() helper retrieves the appropriate region.\n\nThis allows userspace to mmap the kernel-allocated buffer region and\naccess the buffers directly.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  2 ++\n io_uring/kbuf.c               | 11 +++++++++--\n io_uring/kbuf.h               |  5 +++--\n io_uring/memmap.c             |  5 ++++-\n 4 files changed, 18 insertions(+), 5 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex a0889c1744bd..42a2812c9922 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -545,6 +545,8 @@ struct io_uring_cqe {\n #define IORING_OFF_SQES\t\t\t0x10000000ULL\n #define IORING_OFF_PBUF_RING\t\t0x80000000ULL\n #define IORING_OFF_PBUF_SHIFT\t\t16\n+#define IORING_OFF_KMBUF_RING\t\t0x88000000ULL\n+#define IORING_OFF_KMBUF_SHIFT\t\t16\n #define IORING_OFF_MMAP_MASK\t\t0xf8000000ULL\n \n /*\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9bc36451d083..ccf5b213087b 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -770,16 +770,23 @@ int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg)\n \treturn 0;\n }\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid)\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed)\n {\n \tstruct io_buffer_list *bl;\n+\tbool is_kernel_managed;\n \n \tlockdep_assert_held(&ctx->mmap_lock);\n \n \tbl = xa_load(&ctx->io_bl_xa, bgid);\n \tif (!bl || !(bl->flags & IOBL_BUF_RING))\n \t\treturn NULL;\n+\n+\tis_kernel_managed = !!(bl->flags & IOBL_KERNEL_MANAGED);\n+\tif (is_kernel_managed != kernel_managed)\n+\t\treturn NULL;\n+\n \treturn &bl->region;\n }\n \ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 62c80a1ebf03..11d165888b8e 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -88,8 +88,9 @@ unsigned int __io_put_kbufs(struct io_kiocb *req, struct io_buffer_list *bl,\n bool io_kbuf_commit(struct io_kiocb *req,\n \t\t    struct io_buffer_list *bl, int len, int nr);\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid);\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed);\n \n static inline bool io_kbuf_recycle_ring(struct io_kiocb *req,\n \t\t\t\t\tstruct io_buffer_list *bl)\ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 8d37e93c0433..916315122323 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -356,7 +356,10 @@ static struct io_mapped_region *io_mmap_get_region(struct io_ring_ctx *ctx,\n \t\treturn &ctx->sq_region;\n \tcase IORING_OFF_PBUF_RING:\n \t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_PBUF_SHIFT;\n-\t\treturn io_pbuf_get_region(ctx, id);\n+\t\treturn io_buf_get_region(ctx, id, false);\n+\tcase IORING_OFF_KMBUF_RING:\n+\t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_KMBUF_SHIFT;\n+\t\treturn io_buf_get_region(ctx, id, true);\n \tcase IORING_MAP_OFF_PARAM_REGION:\n \t\treturn &ctx->param_region;\n \tcase IORING_MAP_OFF_ZCRX_REGION:\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about distinguishing between kernel-managed buffer addresses and negative values in error checking, modified the io_br_sel struct to separate address and value fields for kernel-managed buffers, and added auto-commit logic for selected kernel-managed buffers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "added code changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Allow kernel-managed buffers to be selected. This requires modifying the\nio_br_sel struct to separate the fields for address and val, since a\nkernel address cannot be distinguished from a negative val when error\nchecking.\n\nAuto-commit any selected kernel-managed buffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring_types.h |  8 ++++----\n io_uring/kbuf.c                | 16 ++++++++++++----\n 2 files changed, 16 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 3e4a82a6f817..36cc2e0346d9 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -93,13 +93,13 @@ struct io_mapped_region {\n  */\n struct io_br_sel {\n \tstruct io_buffer_list *buf_list;\n-\t/*\n-\t * Some selection parts return the user address, others return an error.\n-\t */\n \tunion {\n+\t\t/* for classic/ring provided buffers */\n \t\tvoid __user *addr;\n-\t\tssize_t val;\n+\t\t/* for kernel-managed buffers */\n+\t\tvoid *kaddr;\n \t};\n+\tssize_t val;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex ccf5b213087b..1e8395270227 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -155,7 +155,8 @@ static int io_provided_buffers_select(struct io_kiocb *req, size_t *len,\n \treturn 1;\n }\n \n-static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n+static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n+\t\t\t     unsigned int issue_flags)\n {\n \t/*\n \t* If we came in unlocked, we have no choice but to consume the\n@@ -170,7 +171,11 @@ static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n \tif (issue_flags & IO_URING_F_UNLOCKED)\n \t\treturn true;\n \n-\t/* uring_cmd commits kbuf upfront, no need to auto-commit */\n+\t/* kernel-managed buffers are auto-committed */\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\treturn true;\n+\n+\t/* multishot uring_cmd commits kbuf upfront, no need to auto-commit */\n \tif (!io_file_can_poll(req) && req->opcode != IORING_OP_URING_CMD)\n \t\treturn true;\n \treturn false;\n@@ -200,9 +205,12 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n-\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n+\telse\n+\t\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n \n-\tif (io_should_commit(req, issue_flags)) {\n+\tif (io_should_commit(req, bl, issue_flags)) {\n \t\tio_kbuf_commit(req, sel.buf_list, *len, 1);\n \t\tsel.buf_list = NULL;\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about buffer ring pinning and unpinning, explaining that the new APIs will prevent userspace from unregistering a buffer ring while it is pinned by the kernel. The author stated that this is necessary for fuse to safely access buffer ring contents in atomic contexts.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "preparatory change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add kernel APIs to pin and unpin buffer rings, preventing userspace from\nunregistering a buffer ring while it is pinned by the kernel.\n\nThis provides a mechanism for kernel subsystems to safely access buffer\nring contents while ensuring the buffer ring remains valid. A pinned\nbuffer ring cannot be unregistered until explicitly unpinned. On the\nuserspace side, trying to unregister a pinned buffer will return -EBUSY.\n\nThis is a preparatory change for upcoming fuse usage of kernel-managed\nbuffer rings. It is necessary for fuse to pin the buffer ring because\nfuse may need to select a buffer in atomic contexts, which it can only\ndo so by using the underlying buffer list pointer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 17 +++++++++++++\n io_uring/kbuf.c              | 48 ++++++++++++++++++++++++++++++++++++\n io_uring/kbuf.h              |  5 ++++\n 3 files changed, 70 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 375fd048c4cb..702b1903e6ee 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -84,6 +84,10 @@ struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n \t\t\t\t struct io_br_sel *sel, unsigned int issue_flags);\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t    unsigned issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -126,6 +130,19 @@ static inline bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n {\n \treturn true;\n }\n+static inline int io_uring_buf_ring_pin(struct io_uring_cmd *cmd,\n+\t\t\t\t\tunsigned buf_group,\n+\t\t\t\t\tunsigned issue_flags,\n+\t\t\t\t\tstruct io_buffer_list **bl)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n+static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned buf_group,\n+\t\t\t\t\t  unsigned issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 1e8395270227..dee1764ed19f 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -9,6 +9,7 @@\n #include <linux/poll.h>\n #include <linux/vmalloc.h>\n #include <linux/io_uring.h>\n+#include <linux/io_uring/cmd.h>\n \n #include <uapi/linux/io_uring.h>\n \n@@ -237,6 +238,51 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \treturn sel;\n }\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *buffer_list;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbuffer_list = io_buffer_get_list(ctx, buf_group);\n+\tif (buffer_list && (buffer_list->flags & IOBL_BUF_RING)) {\n+\t\tif (unlikely(buffer_list->flags & IOBL_PINNED)) {\n+\t\t\tret = -EALREADY;\n+\t\t} else {\n+\t\t\tbuffer_list->flags |= IOBL_PINNED;\n+\t\t\tret = 0;\n+\t\t\t*bl = buffer_list;\n+\t\t}\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_pin);\n+\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t       unsigned issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (bl && (bl->flags & IOBL_BUF_RING) && (bl->flags & IOBL_PINNED)) {\n+\t\tbl->flags &= ~IOBL_PINNED;\n+\t\tret = 0;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_unpin);\n+\n /* cap it at a reasonable 256, will be one page even for 4K */\n #define PEEK_MAX_IMPORT\t\t256\n \n@@ -747,6 +793,8 @@ int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t\treturn -ENOENT;\n \tif (!(bl->flags & IOBL_BUF_RING))\n \t\treturn -EINVAL;\n+\tif (bl->flags & IOBL_PINNED)\n+\t\treturn -EBUSY;\n \n \tscoped_guard(mutex, &ctx->mmap_lock)\n \t\txa_erase(&ctx->io_bl_xa, bl->bgid);\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 11d165888b8e..781630c2cc10 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -12,6 +12,11 @@ enum {\n \tIOBL_INC\t\t= 2,\n \t/* buffers are kernel managed */\n \tIOBL_KERNEL_MANAGED\t= 4,\n+\t/*\n+\t * buffer ring is pinned and cannot be unregistered by userspace until\n+\t * it has been unpinned\n+\t */\n+\tIOBL_PINNED\t\t= 8,\n };\n \n struct io_buffer_list {\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the implementation of buffer recycling for kernel-managed buffer rings, explained that an interface is being added to recycle buffers back into a kernel-managed buffer ring, and confirmed that this is preparatory work for fuse over io-uring.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "preparatory patch",
                "interface addition"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add an interface for buffers to be recycled back into a kernel-managed\nbuffer ring.\n\nThis is a preparatory patch for fuse over io-uring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 11 +++++++++\n io_uring/kbuf.c              | 44 ++++++++++++++++++++++++++++++++++++\n 2 files changed, 55 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 702b1903e6ee..a488e945f883 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -88,6 +88,10 @@ int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t    unsigned issue_flags);\n+\n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -143,6 +147,13 @@ static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n+\t\t\t\t\t unsigned int buf_group, u64 addr,\n+\t\t\t\t\t unsigned int len, unsigned int bid,\n+\t\t\t\t\t unsigned int issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex dee1764ed19f..17b6178be4ce 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -102,6 +102,50 @@ void io_kbuf_drop_legacy(struct io_kiocb *req)\n \treq->kbuf = NULL;\n }\n \n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags)\n+{\n+\tstruct io_kiocb *req = cmd_to_io_kiocb(cmd);\n+\tstruct io_ring_ctx *ctx = req->ctx;\n+\tstruct io_uring_buf_ring *br;\n+\tstruct io_uring_buf *buf;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tif (WARN_ON_ONCE(req->flags & REQ_F_BUFFERS_COMMIT))\n+\t\treturn ret;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\n+\tif (!bl || WARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING)) ||\n+\t    WARN_ON_ONCE(!(bl->flags & IOBL_KERNEL_MANAGED)))\n+\t\tgoto done;\n+\n+\tbr = bl->buf_ring;\n+\n+\tif (WARN_ON_ONCE((br->tail - bl->head) >= bl->nr_entries))\n+\t\tgoto done;\n+\n+\tbuf = &br->bufs[(br->tail) & bl->mask];\n+\n+\tbuf->addr = addr;\n+\tbuf->len = len;\n+\tbuf->bid = bid;\n+\n+\treq->flags &= ~REQ_F_BUFFER_RING;\n+\n+\tbr->tail++;\n+\tret = 0;\n+\n+done:\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_kmbuf_recycle);\n+\n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)\n {\n \tstruct io_ring_ctx *ctx = req->ctx;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring_is_kmbuf_ring() function, explaining that it returns true if there is a kernel-managed buffer ring at the specified buffer group. The author provided code changes to implement this functionality and clarified its purpose as a preparatory patch for upcoming fuse kernel-managed buffer support.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "io_uring_is_kmbuf_ring() returns true if there is a kernel-managed\nbuffer ring at the specified buffer group.\n\nThis is a preparatory patch for upcoming fuse kernel-managed buffer\nsupport, which needs to ensure the buffer ring registered by the server\nis a kernel-managed buffer ring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h |  9 +++++++++\n io_uring/kbuf.c              | 20 ++++++++++++++++++++\n 2 files changed, 29 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex a488e945f883..04a937f6f4d3 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -92,6 +92,9 @@ int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t   u64 addr, unsigned int len, unsigned int bid,\n \t\t\t   unsigned int issue_flags);\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -154,6 +157,12 @@ static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned int buf_group,\n+\t\t\t\t\t  unsigned int issue_flags)\n+{\n+\treturn false;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 17b6178be4ce..797cc2f0a5e9 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -963,3 +963,23 @@ int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \n \treturn ret;\n }\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tbool is_kmbuf_ring = false;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (likely(bl) && (bl->flags & IOBL_KERNEL_MANAGED)) {\n+\t\tWARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING));\n+\t\tis_kmbuf_ring = true;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn is_kmbuf_ring;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_is_kmbuf_ring);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about the io_uring/cmd: set selected buffer index in __io_uring_cmd_done() patch, specifically that it doesn't export io_ring_buffer_select(). The author agrees to make this change and has already done so by adding an EXPORT_SYMBOL_GPL macro. This preparatory patch will be needed for fuse io-uring, which requires selecting a buffer from a kernel-managed bufring while the uring mutex may already be held.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed to make change",
                "already implemented fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Export io_ring_buffer_select() so that it may be used by callers who\npass in a pinned bufring without needing to grab the io_uring mutex.\n\nThis is a preparatory patch that will be needed by fuse io-uring, which\nwill need to select a buffer from a kernel-managed bufring while the\nuring mutex may already be held by in-progress commits, and may need to\nselect a buffer in atomic contexts.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 14 ++++++++++++++\n io_uring/kbuf.c              |  7 ++++---\n 2 files changed, 18 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 04a937f6f4d3..d4b5943bdeb1 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -95,6 +95,10 @@ int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \n bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t    unsigned int issue_flags);\n+\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -163,6 +167,16 @@ static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n {\n \treturn false;\n }\n+static inline struct io_br_sel io_ring_buffer_select(struct io_kiocb *req,\n+\t\t\t\t\t\t     size_t *len,\n+\t\t\t\t\t\t     struct io_buffer_list *bl,\n+\t\t\t\t\t\t     unsigned int issue_flags)\n+{\n+\tstruct io_br_sel sel = {\n+\t\t.val = -EOPNOTSUPP,\n+\t};\n+\treturn sel;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 797cc2f0a5e9..9a93f10d3214 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -226,9 +226,9 @@ static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n \treturn false;\n }\n \n-static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n-\t\t\t\t\t      struct io_buffer_list *bl,\n-\t\t\t\t\t      unsigned int issue_flags)\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags)\n {\n \tstruct io_uring_buf_ring *br = bl->buf_ring;\n \t__u16 tail, head = bl->head;\n@@ -261,6 +261,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \t}\n \treturn sel;\n }\n+EXPORT_SYMBOL_GPL(io_ring_buffer_select);\n \n struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \t\t\t\t  unsigned buf_group, unsigned int issue_flags)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about returning the selected buffer's id in io_uring_cmd_buffer_select(). They modified the function to return the selected buffer address, size, and id, and also updated the io_ring_buffer_select() and io_buffer_select() functions to set the buf_id field of the struct io_br_sel. The author confirmed that this change is needed for kernel-managed buffer rings to recycle the selected buffer.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix",
                "confirmed the issue is resolved"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Return the id of the selected buffer in io_buffer_select(). This is\nneeded for kernel-managed buffer rings to later recycle the selected\nbuffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h   | 2 +-\n include/linux/io_uring_types.h | 2 ++\n io_uring/kbuf.c                | 7 +++++--\n 3 files changed, 8 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex d4b5943bdeb1..94df2bdebe77 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);\n \n /*\n  * Select a buffer from the provided buffer group for multishot uring_cmd.\n- * Returns the selected buffer address and size.\n+ * Returns the selected buffer address, size, and id.\n  */\n struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n \t\t\t\t\t    unsigned buf_group, size_t *len,\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 36cc2e0346d9..5a56bb341337 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -100,6 +100,8 @@ struct io_br_sel {\n \t\tvoid *kaddr;\n \t};\n \tssize_t val;\n+\t/* id of the selected buffer */\n+\tunsigned buf_id;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9a93f10d3214..24c1e34ea23e 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -250,6 +250,7 @@ struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n+\tsel.buf_id = req->buf_index;\n \tif (bl->flags & IOBL_KERNEL_MANAGED)\n \t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n \telse\n@@ -274,10 +275,12 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \n \tbl = io_buffer_get_list(ctx, buf_group);\n \tif (likely(bl)) {\n-\t\tif (bl->flags & IOBL_BUF_RING)\n+\t\tif (bl->flags & IOBL_BUF_RING) {\n \t\t\tsel = io_ring_buffer_select(req, len, bl, issue_flags);\n-\t\telse\n+\t\t} else {\n \t\t\tsel.addr = io_provided_buffer_select(req, len, bl);\n+\t\t\tsel.buf_id = req->buf_index;\n+\t\t}\n \t}\n \tio_ring_submit_unlock(req->ctx, issue_flags);\n \treturn sel;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about indicating which buffer was selected in the completion queue entry, explained that this is needed for fuse to relay the information to userspace, and confirmed that they will set IORING_CQE_F_BUFFER on the completed entry along with encoding the buffer index.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a specific concern",
                "confirmed a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When uring_cmd operations select a buffer, the completion queue entry\nshould indicate which buffer was selected.\n\nSet IORING_CQE_F_BUFFER on the completed entry and encode the buffer\nindex if a buffer was selected.\n\nThis will be needed for fuse, which needs to relay to userspace which\nselected buffer contains the data.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/uring_cmd.c | 6 +++++-\n 1 file changed, 5 insertions(+), 1 deletion(-)\n\ndiff --git a/io_uring/uring_cmd.c b/io_uring/uring_cmd.c\nindex ee7b49f47cb5..6d38df1a812d 100644\n--- a/io_uring/uring_cmd.c\n+++ b/io_uring/uring_cmd.c\n@@ -151,6 +151,7 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \t\t       unsigned issue_flags, bool is_cqe32)\n {\n \tstruct io_kiocb *req = cmd_to_io_kiocb(ioucmd);\n+\tu32 cflags = 0;\n \n \tif (WARN_ON_ONCE(req->flags & REQ_F_APOLL_MULTISHOT))\n \t\treturn;\n@@ -160,7 +161,10 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \tif (ret < 0)\n \t\treq_set_fail(req);\n \n-\tio_req_set_res(req, ret, 0);\n+\tif (req->flags & (REQ_F_BUFFER_SELECTED | REQ_F_BUFFER_RING))\n+\t\tcflags |= IORING_CQE_F_BUFFER |\n+\t\t\t(req->buf_index << IORING_CQE_BUFFER_SHIFT);\n+\tio_req_set_res(req, ret, cflags);\n \tif (is_cqe32) {\n \t\tif (req->ctx->flags & IORING_SETUP_CQE_MIXED)\n \t\t\treq->cqe.flags |= IORING_CQE_F_32;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested adding a WARN_ON_ONCE() to prevent int promotion from affecting the calculation of (br->tail - bl->head) >= bl->nr_entries, and noted that this is not a critical issue but rather something to be addressed in future patches.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes",
                "acknowledged as non-critical"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you want:\n\n\tif (WARN_ON_ONCE((__u16)(br->tail - bl->head) >= bl->nr_entries))\n\nhere to avoid int promotion from messing this up if tail has wrapped.\n\nIn general, across the patches for the WARN_ON_ONCE(), it's not a huge\nissue to have a litter of them for now. Hopefully we can prune some of\nthese down the line, however.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe questioned the necessity of setting the selected buffer index in __io_uring_cmd_done(), suggesting that req->buf_index could be used instead, but acknowledged he might be missing something.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "acknowledging ignorance"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm probably missing something here, but why can't the caller just use\nreq->buf_index for this?\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe requested that Joanne Koong provide a branch with all patches and user code for easier cross-referencing, as some of the changes need an exposed user to make a good judgment on the helpers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request",
                "suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Generally looks pretty good - for context, do you have a branch with\nthese patches and the users on top too? Makes it a bit easier for cross\nreferencing, as some of these really do need an exposed user to make a\ngood judgement on the helpers.\n\nI know there's the older series, but I'm assuming the latter patches\nchanged somewhat too, and it'd be nicer to look at a current set rather\nthan go back to the older ones.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested refactoring io_pbuf_get_region() to handle kernel-managed buffer rings by adding a new helper function, io_kbuf_get_region(), and checking the bl->flags for IOBL_KERNEL_MANAGED in both functions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes",
                "minor nit"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "For this, I think just add another helper - leave io_pbuf_get_region()\nand add a bl->flags & IOBL_KERNEL_MANAGED error check in there, and\nadd a io_kbuf_get_region() or similar and have a !(bl->flags &\nIOBL_KERNEL_MANAGED) error check in that one.\n\nThat's easier to read, and there's little reason to avoid duplicating\nthe xa_load() part.\n\nMinor nit, but imho it's more readable that way.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested using a pointer to struct io_buffer_list instead of a double pointer, and recommended returning an ERR_PTR value or renaming the passed pointer to indicate its return type.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "code suggestion"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Probably use the usual struct io_buffer_list *bl here and either use an\nERR_PTR return, or rename the passed on **bl to **blret or something.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Jens Axboe suggested a more efficient way to check if a buffer is both in a ring and pinned, and proposed adding an early return for the case where bl is NULL.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested improvement",
                "proposed alternative"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Usually done as:\n\n\tif ((bl->flags & (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))\n\nand maybe then just have an earlier\n\n\tif (!bl)\n\t\tgoto err;",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe requested that the patch be shortened by removing a comment that exceeds the 80 character limit, stating that for io_uring it is acceptable to exceed this limit where necessary.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested change",
                "clarified coding style"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "to avoid making it way too long. For io_uring, it's fine to exceed 80\nchars where it makes sense.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer noted that the patch fences itself off from existing optimizations, such as huge page support, by not allowing regions to work with user-passed memory",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If you're creating a region, there should be no reason why it\ncan't work with user passed memory. You're fencing yourself off\noptimisations that are already there like huge pages.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that io_create_region() should be used instead of a new function in __io_uring_cmd_done(), as it violates abstractions and does not introduce any new functionality. He suggested stripping buffer allocation from IORING_REGISTER_KMBUF_RING, replacing *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag, or requiring the user to register a memory region of appropriate size.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Please use io_create_region(), the new function does nothing new\nand only violates abstractions.\n\nProvided buffer rings with kernel addresses could be an interesting\nabstraction, but why is it also responsible for allocating buffers?\nWhat I'd do:\n\n1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.\n2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.\n    Or maybe don't expose it to the user at all and create it from\n    fuse via internal API.\n3. Require the user to register a memory region of appropriate size,\n    see IORING_REGISTER_MEM_REGION, ctx->param_region. Make fuse\n    populating the buffer ring using the memory region.\n\nI wanted to make regions shareable anyway (need it for other purposes),\nI can toss patches for that tomorrow.\n\nA separate question is whether extending buffer rings is the right\napproach as it seems like you're only using it for fuse requests and\nnot for passing buffers to normal requests, but I don't see the\nbig picture here.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the removal of io_create_region_multi_buf() means there is no longer a need to align every buffer, which could result in wasted memory due to 64KB page sizes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "wasted memory",
                "64KB pages"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "With io_create_region_multi_buf() gone, you shouldn't need\nto align every buffer, that could be a lot of wasted memory\n(thinking about 64KB pages).",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Caleb Mateos",
              "summary": "Reviewer Caleb Mateos noted that the patch's optimization in __io_uring_cmd_done() is unnecessary, as modern compilers can automatically perform this optimization and even further optimize it to !(~bl->flags & (IOBL_BUF_RING|IOBL_PINNED))",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "optimization",
                "compiler"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "FWIW, modern compilers will perform this optimization automatically.\nThey'll even optimize it further to !(~bl->flags &\n(IOBL_BUF_RING|IOBL_PINNED)): https://godbolt.org/z/xGoP4TfhP\n\nBest,\nCaleb",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "reviewer suggested that the patch should follow a common coding convention to improve readability",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "improvement suggestion",
                "readability"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure, it's not about that, it's more about the common way of doing it,\nwhich makes it easier to read for people. FWIW, your example is easier\nto read too than the original.\n\n-- \nJens Axboe",
              "reply_to": "Caleb Mateos",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to a question about whether kernel-allocated buffers would be able to optimize huge pages, explaining that the kernel can already do this in io_mem_alloc_compound()",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question",
                "explaining existing behavior"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Are there any optimizations with user-allocated buffers that wouldn't\nbe possible with kernel-allocated buffers? For huge pages, can't the\nkernel do this as well (eg I see in io_mem_alloc_compound(), it calls\ninto alloc_pages() with order > 0)?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that separate checks are needed between io_create_region() and io_create_region_multi_buf(), disagreed with the suggestion to use only io_create_region().",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "There's separate checks needed between io_create_region() and\nio_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag\nchecking) and different allocation calls (eg\nio_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).\nMaybe I'm misinterpreting your comment (or the code), but I'm not\nseeing how this can just use io_create_region().",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong is addressing feedback about the benefits of kernel-managed buffer rings, specifically questioning the advantages of registering buffers from userspace. She asks for elaboration on this point and provides her own reasoning that kernel-managed allocation simplifies interface and lifecycle management.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "requesting clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Conceptually, I think it makes the interface and lifecycle management\nsimpler/cleaner. With registering it from userspace, imo there's\nadditional complications with no tangible benefits, eg it's not\nguaranteed that the memory regions registered for the buffers are the\nsame size, with allocating it from the kernel-side we can guarantee\nthat the pages are allocated physically contiguously, userspace setup\nwith user-allocated buffers is less straightforward, etc. In general,\nI'm just not really seeing what advantages there are in allocating the\nbuffers from userspace. Could you elaborate on that part more?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern about squashing kernel-managed buffer rings into existing pbuf rings, explaining that pbuf rings would need to support pinning and citing a previously dropped patch for this purpose.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical challenge",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If kmbuf rings are squashed into pbuf rings, then pbuf rings will need\nto support pinning. In fuse, there are some contexts where you can't\ngrab the uring mutex because you're running in atomic context and this\ncan be encountered while recycling the buffer. I originally had a\npatch adding pinning to pbuf rings (to mitigate the overhead of\nregistered buffers lookups) but dropped it when Jens and Caleb didn't\nlike the idea. But for kmbuf rings, pinning will be necessary for\nfuse.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to Pavel Begunkov's question about what constitutes a 'normal request' in the context of io_uring, explaining that for fuse's use case, there are only fuse requests.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "What are 'normal requests'? For fuse's use case, there are only fuse requests.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that accessing the buffer index from the io_kiocb pointer is not ideal and proposed an alternative solution, such as introducing a helper function to retrieve the buffer ID.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a potential issue",
                "proposed an alternative"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The caller can, but from the caller side they only have access to the\ncmd so they would need to do something like\n\nstruct io_kiocb *req = cmd_to_iocb_kiocb(ent->cmd);\nbuf_id = req->buf_index;\n\nwhich may be kind of ugly with looking inside io-uring internals.\nMaybe a helper here would be nicer, something like\nio_uring_cmd_buf_id() or io_uring_req_buf_id(). It seemed cleaner to\nme to just return the buf id as part of the io_br_sel struct, but I'm\nhappy to do it another way if you have a preference.\n\nThanks,\nJoanne",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged the need for changes in v2, promising to address reviewer feedback once discussion with Pavel is resolved.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for changes",
                "promised fix in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for reviewing the patches. The branch containing the userside\nchanges on top of these patches is in [1]. I'll make the changes you\npointed out in your other comments as part of v2. Once the discussion\nwith Pavel is resolved / figured out with the changes he wants for v2,\nI'll submit v2.\n\nThanks,\nJoanne\n\n[1] https://github.com/joannekoong/linux/commits/fuse_zero_copy/",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that allocating 1MB in kernel space will not result in a PMD mappable huge page, unlike user space which can allocate 2MB and register the first 1MB for reuse",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change",
                "technical concern"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, there is handful of differences. To name one, 1MB allocation won't\nget you a PMD mappable huge page, while user space can allocate 2MB,\nregister the first 1MB and reuse the rest for other purposes.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested that instead of changing io_create_region() to be less strict, the caller should filter arguments and only pass IORING_MEM_REGION_TYPE_USER when it's actually used.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "If io_create_region() is too strict, let's discuss that in\nexamples if there are any, but it's likely not a good idea changing\nthat. If it's too lax, filter arguments in the caller. IOW, don't\npass IORING_MEM_REGION_TYPE_USER if it's not used.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that the memmap.c changes in this patch can be dropped because they are equivalent to using io_create_region() and suggested removing them to avoid potential issues with io_mem_alloc_compound().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I saw that and saying that all memmap.c changes can get dropped.\nYou're using it as one big virtually contig kernel memory range then\nchunked into buffers, and that's pretty much what you're getting with\nnormal io_create_region(). I get that you only need it to be\ncontiguous within a single buffer, but that's not what you're doing,\nand it'll be only worse than default io_create_region() e.g.\neffectively disabling any usefulness of io_mem_alloc_compound(),\nand ultimately you don't need to care.\n\nRegions shouldn't know anything about your buffers, how it's\nsubdivided after, etc.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested adding a check for user-provided memory and modifying the io_create_region() call to include the user address and flags when allocating a region from user memory",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "struct io_uring_region_desc rd = {};\ntotal_size = nr_bufs * buf_size;\nrd.size = PAGE_ALIGN(total_size);\nio_create_region(&region, &rd);\n\nAdd something like this for user provided memory:\n\nif (use_user_memory) {\n\trd.user_addr = uaddr;\n\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n}",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov suggested separating ring population from kernel API, noting that fuse kernel module could populate rings and achieve same layout as current implementation, but expressed uncertainty about his own understanding of the issue.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "lack of clarity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think I follow. I'm saying that it might be interesting\nto separate rings from how and with what they're populated on the\nkernel API level, but the fuse kernel module can do the population\nand get exactly same layout as you currently have:\n\nint fuse_create_ring(size_t region_offset /* user space argument */) {\n\tstruct io_mapped_region *mr = get_mem_region(ctx);\n\t// that can take full control of the ring\n\tring = grab_empty_ring(io_uring_ctx);\n\n\tsize = nr_bufs * buf_size;\n\tif (region_offset + size > get_size(mr)) // + other validation\n\t\treturn error;\n\n\tbuf = mr_get_ptr(mr) + offset;\n\tfor (i = 0; i < nr_bufs; i++) {\n\t\tring_push_buffer(ring, buf, buf_size);\n\t\tbuf += buf_size;\n\t}\n}\n\nfuse might not care, but with empty rings other users will get a\nchannel they can use to do IO (e.g. read requests) using their\nkernel addresses in the future.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested simplifying the io_uring buffer ring implementation by introducing a flag to indicate kernel-managed buffers, rather than introducing separate uapi and internal APIs for kernel-managed buffer rings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested improvement",
                "no strong opinion"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It'd change uapi but not internals, you already piggy back it\non pbuf implementation and differentiate with a flag.\n\nIt could basically be:\n\nif (flags & IOU_PBUF_RING_KM)\n\tbl->flags |= IOBL_KERNEL_MANAGED;\n\nPinning can be gated on that flag as well. Pretty likely uapi\nand internals will be a bit cleaner, but that's not a huge deal,\njust don't see why would you roll out a separate set of uapi\n([un]register, offsets, etc.) when essentially it can be treated\nas the same thing.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer noted that the patch did not provide buffer rings when pinning the registered buffer table, suggesting an alternative approach where all memory is kept in one larger registered buffer and pinned only once",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IIRC, you was pinning the registered buffer table and not provided\nbuffer rings? Which would indeed be a bad idea. Thinking about it,\nfwiw, instead of creating multiple registered buffers and trying to\nlock the entire table, you could've kept all memory in one larger\nregistered buffer and pinned only it. It's already refcounted, so\nshouldn't have been much of a problem.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov expressed concerns that creating many small regions for kernel-managed buffer rings would lead to extra mmap()s, user space management, and wasted space, as well as over-accounting and increased memory footprint for user-provided memory. He also noted potential issues with buffer lifetimes after a fuse instance dies.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "expressed concerns"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "To explain why, I don't think that creating many small regions\nis a good direction going forward. In case of kernel allocation,\nit's extra mmap()s, extra user space management, and wasted space.\nFor user provided memory it's over-accounting and extra memory\nfootprint. It'll also give you better lifecycle guarantees, i.e.\nyou won't be able to free buffers while there are requests for the\ncontext. I'm not so sure about ring bound memory, let's say I have\nmy suspicions, and you'd need to be extra careful about buffer\nlifetimes even after a fuse instance dies.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that kernel-managed buffer rings are particularly useful for operations like read and recv, where the kernel can fill the buffers without requiring opcode-specific code changes in kbuf.c",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Any kind of read/recv/etc. that can use provided buffers. It's\nwhere kernel memory filled rings would shine, as you'd be able\nto use them together without changing any opcode specific code.\nI.e. not changes in read request implementation, only kbuf.c\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that any pages mapped to userspace can be allocated in the kernel, allowing for a buffer ring that is only mapped read-only into userspace, which enables zero-copy raids if the device requires stable pages for checksumming or raid.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Any pages mapped to userspace can be allocated in the kernel as well.\n\nAnd I really do like this design, because it means we can have a\nbuffer ring that is only mapped read-only into userspace.  That way\nwe can still do zero-copy raids if the device requires stable pages\nfor checksumming or raid.  I was going to implement this as soon\nas this series lands upstream.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to feedback about using io_region_allocate_pages(), explaining that it fails due to allocating too much memory at once, and instead chose to use io_region_allocate_pages_multi_buf() to bypass allocation errors.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical issue",
                "provided explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When I originally implemented it, I had it use\nio_region_allocate_pages() but this fails because it's allocating way\ntoo much memory at once. For fuse's use case, each buffer is usually\nat least 1 MB if not more. Allocating the memory one buffer a time in\nio_region_allocate_pages_multi_buf() bypasses the allocation errors I\nwas seeing. That's the main reason I don't think this can just use\nio_create_region().",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarifies her understanding of Pavel's feedback, asking if she has correctly interpreted his suggestion that the kernel should allocate buffers through IORING_REGISTER_MEM_REGION.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oh okay, from your first message I (and I think christoph too) thought\nwhat you were saying is that the user should be responsible for\nallocating the buffers with complete ownership over them, and then\njust pass those allocated to the kernel to use. But what you're saying\nis that just use a different way for getting the kernel to allocate\nthe buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am\nI reading this correctly?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern that combining kernel-managed buffer rings (kmbufs) into the existing pbuf API would make it overly complex, and instead suggested keeping them separate to maintain clarity in user-space code. The author agreed to restructure the interfaces for v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "imo, it looked cleaner as a separate api because it has different\nexpectations and behaviors and squashing kmbuf into the pbuf api makes\nthe pbuf api needlessly more complex. Though I guess from the\nuserspace pov, liburing could have a wrapper that takes care of\nsetting up the pbuf details for kernel-managed pbufs. But in my head,\nhaving pbufs vs. kmbufs makes it clearer what each one does vs regular\npbufs vs. pbufs that are kernel-managed.\n\nEspecially with now having kmbufs go through the ioring mem region\ninterface, it makes things more confusing imo if they're combined, eg\npbufs that are kernel-managed are created empty and then populated\nfrom the kernel side by whatever subsystem is using them. Right now\nthere's only one mem region supported per ring, but in the future if\nthere's the possibility that multiple mem regions can be registered\n(eg if userspace doesn't know upfront what mem region length they'll\nneed), then we should also probably add in a region id param for the\nregistration arg, which if kmbuf rings go through the pbuf ring\nregistration api, is not possible to do.\n\nBut I'm happy to combine the interfaces and go with your suggestion.\nI'll make this change for v2 unless someone else objects.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that they previously misunderstood the issue as being about pinning the pbuf ring, when in fact it was about pinning the registered buffer table.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "acknowledgment of misunderstanding"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah, you're right I misremembered and the objections / patch I\ndropped was pinning the registered buffer table, not the pbuf ring",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to feedback about sparse buffers populated by the kernel, expressing uncertainty and potential drawbacks to pinning them, but ultimately deciding not to implement this feature due to performance implications.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "potential drawbacks"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hmm, I'm not sure this idea would work for sparse buffers populated by\nthe kernel, unless those are automatically pinned too but then from\nthe user POV for unregistration they'd need to unregister buffers\nindividually instead of just calling IORING_UNREGISTER_BUFFERS but it\nmight be annoying for them to now need to know which buffers are\npinned vs not. When i benchmarked the fuse code with vs without pinned\nregistered buffers, it didn't seem to make much of a difference\nperformance-wise thankfully, so I just dropped it.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing concerns about separate buffer allocation, explaining that userspace only needs one mmap call and doesn't see wasted space or extra management. She suggests allocating the entire region at once if possible.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explaining"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "To clarify, is this in reply to why the individual buffers shouldn't\nbe allocated separately by the kernel?\nI added a comment about this above in the discussion about\nio_region_allocate_pages_multi_buf(), and if the memory allocation\nissue I was seeing is bypassable and the region can be allocated all\nat once, I'm happy to make that change. With having the allocation be\nseparate buffers though, I'm not sure I agree that there are extra\nmmaps / userspace management. All the pages across the buffers are\nvmapped together and the userspace just needs to do 1 mmap call for\nthem. On the userspace side, I don't think there's more management\nsince the mmapped address represents the range across all the buffers.\nI'm not seeing how there's wasted space either since the only\nrequirement is that the buffer size is page aligned. I think also\nthere's a higher chance of the entire buffer region being physically\ncontiguous if each buffer is allocated separately vs. all the buffers\nare allocated as 1 region. I don't feel strongly about this either way\nand I'm happy to allocate the entire region at once if that's\npossible.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author asks for clarification on reviewer's concerns about over-accounting and extra memory footprint in kernel-managed buffer rings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question",
                "request for explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Just out of curiosity, could you elaborate on the over-accounting and\nextra memory footprint? I was under the impression it would be the\nsame since the accounting gets adjusted by the total bytes allocated?\nFor the extra memory footprint, is the extra footprint from the\nmetadata to describe each buffer region, or are you referring to\nsomething else?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author is addressing concerns about the API and buffer allocation in the kernel-managed buffer rings patch series. She plans to make changes in v2, including removing the KMBUF_RING API interface, having kernel buffer allocation go through IORING_REGISTER_MEM_REGION, and adding APIs for subsystems to populate a kernel-managed buffer ring with addresses from the registered memory region.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "planned changes in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for your input on the series. To iterate / sum up, these are\nchanges for v2 I'll be making:\n- api-wise from userspace/liburing: get rid of KMBUF_RING api\ninterface and have users go through PBUF_RING api instead with a flag\nindicating the ring is kernel-managed\n- have kernel buffer allocation go through IORING_REGISTER_MEM_REGION\ninstead, which means when the pbuf ring is created and the\nkernel-managed flag is set, the ring will be empty. The memory region\nwill need to be registered before the mmap call to the ring fd.\n- add apis for subsystems to populate a kernel-managed buffer ring\nwith addresses from the registered mem region\n\nDoes this align with your understanding of the conversation as well or\nis there anything I'm missing?\n\nAnd Christoph, do these changes for v2 work for your use case as well?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig expressed concerns that the current implementation does not meet his use case requirements, where the kernel fully controls buffer allocation and guarantees user processes can only read from the memory without writing to it.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm arguing exactly against this.  For my use case I need a setup\nwhere the kernel controls the allocation fully and guarantees user\nprocesses can only read the memory but never write to it.  I'd love\nto be able to piggy back than onto your work.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer pointed out that using pow2 round ups for memory allocation will result in wasted memory, specifically citing the example of 1MB allocations not being able to utilize 2MB huge pages, and suggested that users should be allowed to make their own placement decisions",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "pow2 round ups will waste memory. 1MB allocations will never\nbecome 2MB huge pages. And there is a separate question of\n1GB huge pages. The user can be smarter about all placement\ndecisions.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer suggests that the io_uring uapi should include fields for optional user-provided memory, and is neutral about fuse refusing to bind to buffer rings it doesn't like.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That's an interesting case. To be clear, user provided memory is\nan optional feature for pbuf rings / regions / etc., and I think\nthe io_uring uapi should leave fields for the feature. However, I\nhave nothing against fuse refusing to bind to buffer rings it\ndoesn't like.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested modifying IORING_REGISTER_MEM_REGION to support read-only registrations, and proposed adding a new registration flag or rejecting unsupported setups during initialization.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request for clarification",
                "no clear objection"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IORING_REGISTER_MEM_REGION supports both types of allocations. It can\nhave a new registration flag for read-only, and then you either make\nthe bounce avoidance optional or reject binding fuse to unsupported\nsetups during init. Any arguments against that? I need to go over\nJoanne's reply, but I don't see any contradiction in principal with\nyour use case.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author is clarifying the difference between kernel-managed buffer rings and user-initiated setup of kbuf rings, explaining that in the former case, userspace doesn't explicitly register any kbuf ring and the kernel uses it internally.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"control the allocation fully\" do you mean for your use case, the\nallocation/setup isn't triggered by userspace but is initiated by the\nkernel (eg user never explicitly registers any kbuf ring, the kernel\njust uses the kbuf ring data structure internally and users can read\nthe buffer contents)? If userspace initiates the setup of the kbuf\nring, going through IORING_REGISTER_MEM_REGION would be semantically\nthe same, except the buffer allocation by the kernel now happens\nbefore the ring is created and then later populated into the ring.\nuserspace would still need to make an mmap call to the region and the\nkernel could enforce that as read-only. But if userspace doesn't\ninitiate the setup, then going through IORING_REGISTER_MEM_REGION gets\nuglier.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged a potential over-engineering issue in the current design, suggesting an alternative approach where a straightforward kernel-managed buffer ring goes through the pbuf interface and a future interface for pbuf rings to go through IORING_REGISTERED_MEM_REGIONS.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "overkill",
                "over-engineered"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "So i guess the flow would have to be:\na) user calls io_uring_register_region(&ring, &mem_region_reg) with\nmem_region_reg.region_uptr's size field set to the total buffer size\n(and mem_region_reg.flags read-only bit set if needed)\n     kernel allocates region\nb) user calls mmap() to get the address of the region. If read-only\nbit was set, it gets a read-only address\nc) user calls io_uring_register_buf_ring(&ring, &buf_reg, flags) with\nbuf_reg.flags |= IOU_PBUF_RING_KERNEL_MANAGED\n     kernel creates an empty kernel-managed ring. None of the buffers\nare populated\nd) user tells X subsystem to populate the ring starting from offset Z\nin the registered mem region\ne) on the kernel side, the subsystem populates the ring starting from\noffset Z, filling it up using the buf_size and ring_entries values\nthat the user registered the ring with in c)\n\nTo be completely honest, the more I look at this the more this feels\nlike overkill / over-engineered to me. I get that now the user can do\nthe PMD optimization, but does that actually lead to noticeable\nperformance benefits? It seems especially confusing with them going\nthrough the same pbuf ring interface but having totally different\nexpectations.\n\nWhat about adding a straightforward kmbuf ring that goes through the\npbuf interface (eg the design in this patchset) and then in the future\nadding an interface for pbuf rings (both kernel-managed and\nnon-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if\nusers end up needing/wanting to have their rings populated that way?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer noted that instead of optimizing for buffer selection, the patch should focus on reducing TLB pressure by rounding up to a multiple of PTE levels, and suggested this as an alternative solution.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "alternative_solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sure.  But if the application cares that much about TLB pressure\nI'd just round up to nice multtiple of PTE levels.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig questioned the meaning of 'pbuf' in the patch, expressing confusion about its relation to io_uring_register_buffers* and suggesting that it may be a sign of his own lack of expertise in io_uring APIs.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "lack of understanding"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Can you clarify what you mean with 'pbuf'?  The only fixed buffer API I\nknow is io_uring_register_buffers* which always takes user provided\nbuffers, so I have a hard time parsing what you're saying there.  But\nthat might just be sign that I'm no expert in io_uring APIs, and that\nweb searches have degraded to the point of not being very useful\nanymore.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "reviewer expressed confusion over the purpose of IORING_REGISTER_MEM_REGION, citing a mismatch between the commit message and public documentation",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "mismatch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "IORING_REGISTER_MEM_REGION seems to be all about cqs from both your\ncommit message and the public documentation.  I'm confused.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "reviewer noted that the patch does not address their specific use case of block and file system I/O",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "use case mismatch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "My use case is not about fuse, but good old block and file system\nI/O.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that io_uring_register_buffers() only pins memory, allowing applications or other processes to modify it, which can cause issues for file systems and storage devices that need to verify checksums or rebuild data from parity.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The idea is that the application tells the kernel that it wants to use\na fixed buffer pool for reads.  Right now the application does this\nusing io_uring_register_buffers().  The problem with that is that\nio_uring_register_buffers ends up just doing a pin of the memory,\nbut the application or, in case of shared memory, someone else could\nstill modify the memory.  If the underlying file system or storage\ndevice needs verify checksums, or worse rebuild data from parity\n(or uncompress), it needs to ensure that the memory it is operating\non can't be modified by someone else.\n\nSo I've been thinking of a version of io_uring_register_buffers where\nthe buffers are not provided by the application, but instead by the\nkernel and mapped into the application address space read-only for\na while, and I thought I could implement this on top of your series,\nbut I have to admit I haven't really looked into the details all\nthat much.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer noted that the patch's optimization for PMD mappings is not relevant, as both AMD and ARM architectures have their own optimizations for contiguous PTEs.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "optimization",
                "architecture"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes.  The PMD mapping also is not that relevant.  Both AMD (implicit)\nand ARM (explicit) have optimizations for contiguous PTEs that are\nalmost as valuable.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that the patch introduces a new io_uring uapi for kernel-managed buffer rings, which is inflexible and requires a new uapi. They expressed concerns about the lifetime of the buffer memory not matching the ring object and suggested making it more flexible to allow using the km buffer ring with other types of io_uring requests.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "inflexibility",
                "lifetime mismatch"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Registered, aka fixed, buffers are the ones you pass to\nIORING_OP_[READ,WRITE]_FIXED and some other requests. It's normally\ncreated by io_uring_register_buffers*() / IORING_REGISTER_BUFFERS*\nwith user memory, but there are special cases when it's installed\ninternally by other kernel components, e.g. ublk.\nThis series has nothing to do with them, and relevant parts of\nthe discussion here don't mention them either.\n\nProvided buffer rings, a.k.a pbuf rings, IORING_REGISTER_PBUF_RING\nis a kernel-user shared ring. The entries are user buffers\n{uaddr, size}. The user space adds entries, the kernel (io_uring\nrequests) consumes them and issues I/O using the user addresses.\nE.g. you can issue a IORING_OP_RECV request (+IOSQE_BUFFER_SELECT)\nand it'll grab a buffer from the ring instead of using sqe->addr.\n\npbuf rings, IORING_REGISTER_MEM_REGION, completion/submission\nqueues and all other kernel-user rings/etc. are internally based\non so called regions. All of them support both user allocated\nmemory and kernel allocations + mmap.\n\nThis series essentially creates provided buffer rings, where\n1. the ring now contains kernel addresses\n2. the ring itself is in-kernel only and not shared with user space\n3. it also allocates kernel buffers (as a region), populates the ring\n    with them, and allows mapping the buffers into the user space.\n\nFuse is doing both adding (kernel) buffers to the ring and consuming\nthem. At which point it's not clear:\n\n1. Why it even needs io_uring provided buffer rings, it can be all\n    contained in fuse. Maybe it's trying to reuse pbuf ring code as\n    basically an internal memory allocator, but then why expose buffer\n    rings as an io_uring uapi instead of keeping it internally.\n\n    That's also why I mentioned whether those buffers are supposed to\n    be used with other types of io_uring requests like recv, etc.\n\n2. Why making io_uring to allocate payload memory. The answer to which\n    is probably to reuse the region api with mmap and so on. And why\n    payload buffers are inseparably created together with the ring\n    and via a new io_uring uapi.\n\n    And yes, I believe in the current form it's inflexible, it requires\n    a new io_uring uapi. It requires the number of buffers to match\n    the number of ring entries, which are related but not the same\n    thing. You can't easily add more memory as it's bound to the ring\n    object. The buffer memory won't even have same lifetime as the\n    ring object -- allow using that km buffer ring with recv requests\n    and highly likely I'll most likely give you a way to crash the\n    kernel.\n\nBut hey, I'm tired. I don't have any beef here and am only trying\nto make it a bit cleaner and flexible for fuse in the first place\nwithout even questioning the I/O path. If everyone believes\neverything is right, just ask Jens to merge it.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer expressed concern that the patch introduces using buffer rings for huge payload buffers, which was not their original intention.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Think of it as an area of memory for kernel-user communication. Used\nfor syscall parameters passing to avoid copy_from_user, but I added\nit for a bunch of use cases. We'll hopefully get support at some\npoint for passing request arguments like struct iovec. BPF patches\nuse it for communication. I need to respin patches placing SQ/CQ onto\nit (avoid some memory waste).\n\nTbh, I never meant it nor io_uring regions to be used for huge\npayload buffers, but this series already uses regions for that.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed confusion about how the kernel-managed buffer rings can work without a kernel component returning buffers into the ring, noting that io_uring does not currently do this and suggesting that maybe an additional API is being considered.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "questioning"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Then I'm confused. Take a look at the other reply, this series is\nabout buffer rings with kernel memory, it can't work without a kernel\ncomponent returning buffers into the ring, and io_uring doesn't do\nthat. But maybe you're thinking about adding some more elaborate API.\n\nIIUC, Joanne also wants to add support for fuse installing registered\nbuffers, which would allow zero-copy, but those got split out of\nthis series.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the patch's approach to setting the selected buffer index in __io_uring_cmd_done() has a potential issue, suggesting a workaround by wrapping it into a loop, and expressed surprise at not seeing this used for metadata like fuse headers and payloads.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "surprise"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Let's fix that then. For now, just work it around by wrapping\ninto a loop.\n\nBtw, I thought you're going to use it for metadata like some\nfuse headers and payloads would be zero copied by installing\nit as registered buffers.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the patch should disentangle memory allocation from ring creation in the io_uring uapi and move ring population into fuse, instead of doing it at creation, to prevent user space from accessing the ring.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The main point is disentangling memory allocation from ring\ncreation in the io_uring uapi, and moving ring population\ninto fuse instead of doing it at creation. And it'll still be\npopulated by the kernel (fuse), user space doesn't have access\nto the ring. IORING_REGISTER_MEM_REGION is just the easiest way\nto achieve that without any extra uapi.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the differences between two buffer allocation paths are significant and suggested that they should be handled differently, but did not strongly object to making them a separate opcode.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no strong disagreement",
                "open to alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It appeared to me that they're different because of special\nregion path and embedded buffer allocations, and otherwise\ndifferences would be minimal. But if you think it's still\nbetter to be made as a separate opcode, I'm not opposing it,\ngo for it.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer questioned how fuse looks up buffer rings from io_uring and suggested a control path io-uring command to bind buffer rings, proposing a pseudo-code implementation for this command.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "inconvenience",
                "questioning"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Not having patches using the functionality is inconvenient. How\nfuse looks up the buffer ring from io_uring? I could imagine you\nhave some control path io-uring command:\n\ncase FUSE_CMD_BIND_BUFFER_RING:\n\treturn bind_queue(params);\n\nThen you can pass all necessary parameters to it, pseudo code:\n\nstruct fuse_bind_kmbuf_ring_params {\n\tregion_id;\n\tbuf_ring_id;\n\t...\n};\n\nbind_queue(cmd, struct fuse_bind_kmbuf_ring_params *p)\n{\n\tregion = io_uring_get_region(cmd, p->region_id);\n\t// get exclusive access:\n\tbuf_ring = io_uring_get_buf_ring(cmd, p->buf_ring_id);\n\n\tif (!validate_buf_ring(buf_ring))\n\t\treturn NOTSUPPORTED;\n\n\tio_uring_pin(buf_ring);\n\tfuse_populate_buf_ring(buf_ring, region, ...);\n}\n\nDoes that match expectations? I don't think you even need\nthe ring part exposed as an io_uring uapi, tbh, as it\nstays completely in fuse and doesn't meaningfully interact\nwith the rest of io_uring.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer noted that the patch does not address his previous comment on using IORING_REGISTER_MEM_REGION instead of a separate region, which is unrelated to whether buffers should be bound to the ring",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That was about an argument for using IORING_REGISTER_MEM_REGION\ninstead a separate region. And it's separate from whether\nbuffers should be bound to the ring.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that when allocating huge pages for non-power-of-2 buffer sizes, the kernel may allocate a larger huge page than necessary, wasting memory, and requested further investigation into this issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential memory waste",
                "requested further investigation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I shouldn't affect you much since you have such large buffers,\nbut imagine the total allocation size is not being pow2, and\nthe kernel allocating it as a single folio. E.g. 3 buffers,\n0.5 MB each, total = 1.5MB, and the kernel allocates a 2MB\nhuge page.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer argued that the patch series does not address registered buffers and suggested separating buffer allocation for io_uring, expressing disagreement with the current approach",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "disagreement",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There is nothing about registered buffers in this series. And even\nif you try to reuse buffer allocation out of it, it'll come with\na circular buffer you'll have no need for. And I'm pretty much\narguing about separating those for io_uring.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested reusing regions for allocations and mmap() operations, wrapping them into a registered buffer, and making vmap'ing optional to avoid unnecessary overhead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, the easiest solution is to internally reuse regions for\nallocations and mmap()'ing and wrap it into a registered buffer.\nIt just need to make vmap'ing optional as it won't be needed.\n\n-- \nPavel Begunkov",
              "reply_to": "",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed concerns that the io_uring uapi should not be tied to fuse-specific requirements, such as uniform buffer sizes, matching ring size, and kernel-allocated buffers. He questioned the need for these constraints and suggested that the uapi should be more flexible to accommodate different use cases.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, it's mainly about not keeping payload buffers and rings in the same\nobject from the io_uring uapi perspective.\n\n1. If it's an io_uring uapi, it shouldn't be fuse specific or with\na bunch of use case specific expectations attached. Why does it\nrequire all buffers to be uniform in size? Why does it require\nthe ring size to match the number of buffers? Why does it require\nbuffers to be allocated by io_uring in the first place? Maybe some\nsubsystem got memory from somewhere else and wants to do use it\nwith io_uring. Why does it need to know the total size at creation,\nand what would you do if you want to add more memory at runtime\nwhile using the same ring?\n\n2. If it's meant to be fuse specific and _not_ used with other requests\nlike recv/read/etc., then what's the point of having it as an io_uring\nuapi? Which also adds additional trouble like the once you're solving\nwith pinning.\n\nIf it's supposed to be used with other requests, then buffers and\nrings will have different in-kernel lifetime expectations imposed\nby io_uring, so having them together won't even help with\nmanagement.\n\nI have a strong opinion about the memmap.c change. For the\nrest, if you believe it's fine, just send it out and let Jens\ndecide.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer questioned the separation of buffers from rings, citing uncertainty about differing expectations for in-kernel vs user-visible buffers",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "lack of clear expectation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It's predicated on separating buffers from rings, see above,\nand assuming that I'm not sure what expectations are different\napart from one being in-kernel with kernel addresses and the\nother user visible with user addresses.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to Pavel Begunkov's feedback by explaining that the circular buffer will be useful for Christoph's use case, which involves differently sized read payloads across requests, and thus reduce memory allocation.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a benefit of the patch",
                "explained technical reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think the circular buffer will be useful for Christoph's use case in\nthe same way it'll be useful for fuse's. The read payload could be\ndifferently sized across requests, so it's a lot of wasted space to\nhave to allocate a buffer large enough to support the max-size request\nper entry in the io_ring. With using a circular buffer, buffers have a\nway to be shared across entries, which means we can significantly\nreduce how much memory needs to be allocated.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that Christoph Hellwig's use case requires read-only buffers and agrees that it benefits from incremental buffer consumption, which is the primary reason for kernel-managed buffer rings.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreement",
                "acknowledgment"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "(resending because I hit reply instead of reply-all)\n\nI think we have the exact same use case, except your buffers need to\nbe read-only. I think your use case benefits from the same memory wins\nwe'll get with incremental buffer consumption, which is the primary\nreason fuse is using a bufring instead of fixed buffers.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Christoph's concern about how to handle read-only mappings for kernel-managed buffer rings, suggesting a solution involving passing a read-only flag from userspace and checking it when mapping the buffers. She offered to add this patch to the series if needed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "offer to add additional patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think you can and it'll be very easy to do so. All that would be\nneeded is to pass in a read-only flag from the userspace side when it\nregisters the bufring, and then when userspace makes the mmap call to\nthe bufring, the kernel checks if that read-only flag is set on the\nbufring and if so returns a read-only mapping. I'm happy to add that\npatch to this series if that would make things easier for you. The\nio_uring_register_buffers() api registers fixed buffers (which have to\nbe user-allocated memory) so you would need to go through the\nio_uring_register_buf_ring() api once kmbufs are squashed into the\npbuf interface.\n\nWith going through IORING_MEM_REGION, this would work for your use\ncase as well. The user would have to register the mem region with\nio_uring_register_region() and pass in a read-only flag, and then the\nkernel will allocate the memory region. Then userspace would mmap the\nmemory region and on the kernel side, it would set the mapping to be\nread-only. When the kmbufring then gets registered, the buffers in it\nwill be empty. The filesystem will then have to populate the buffers\nin it from the mem region that was previously registered.\n\nThanks,\nJoanne",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Bernd Schubert",
              "summary": "Reviewer Bernd Schubert expressed skepticism about the kernel-managed buffer rings feature, questioning its usefulness and suggesting that it may be equivalent to reducing ring size, and requested clarification on the actual goal of this feature.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "skepticism",
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Dunno, what we actually want is requests of multiple sizes. Sharing\nbuffers across entries sounds like just reducing the ring size - I\npersonally don't see the point here.\n\n\nThanks,\nBernd",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarified that 'sharing buffers across entries' means allowing different parts of a buffer to be accessed simultaneously by multiple io_uring entries, in response to reviewer feedback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"sharing buffers across entries\" what I mean is different regions\nof the buffer can now be used concurrently by multiple entries.\n\nThanks,\nJoanne",
              "reply_to": "Bernd Schubert",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed Pavel Begunkov's concern about the need for kernel-managed buffer rings, explaining that fuse requires the kernel to control when buffers get recycled back into the ring due to its use case of passing data between the kernel and server. The author emphasized the importance of this feature for fuse's functionality.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "explained reasoning",
                "emphasized importance"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The most important part and the whole reason fuse needs the buffer\nring to be kernel-managed is because the kernel needs to control when\nbuffers get recycled back into the ring. For fuse's use case, the\nbuffer is used for passing data between the kernel and the server. We\ncan't have the server recycle the buffer because the server writes\nback data to the kernel in that buffer when it submits the sqe. After\nfuse receives the sqe and reads the reply from the server, it then\nneeds to recycle that buffer back into the ring so it can be reused\nfor a future cqe (eg sending a future request).",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that the selected buffer index needs to be set in __io_uring_cmd_done() for userspace/server side operations, and agreed to add this logic.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to add logic"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On the userspace/server side, it uses the buffers for other io-uring\noperations (eg reading or writing the contents from/to a\nlocally-backed file).",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong responded to feedback from Pavel Begunkov, acknowledging that using a registered mem region allows optimizations like PMD, but expressing concern that most use cases of kmbufs do not benefit from these optimizations and thus feel that adding complexity for no noticeable benefit is unnecessary. She suggests keeping both simple kernel-managed pbufs and more advanced ones tied to registered memory regions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging feedback",
                "expressing concern about added complexity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "My main motivation for this is simplicity. I see (and thanks for\nexplaining) that using a registered mem region allows the use of some\noptimizations (the only one I know of right now is the PMD one you\nmentioned but maybe there's more I'm missing) that could be useful for\nsome workloads, but I don't think (and this could just be my lack of\nunderstanding of what more optimizations there are) most use cases of\nkmbufs benefit from those optimizations, so to me it feels like we're\nadding non-trivial complexity for no noticeable benefit.\n\nI feel like we get the best of both worlds by letting users have both:\nthe simple kernel-managed pbuf where the kernel allocates the buffers\nand the buffers are tied to the lifecycle of the ring, and the more\nadvanced kernel-managed pbuf where buffers are tied to a registered\nmemory region that the subsystem is responsible for later populating\nthe ring with.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern that combining kernel-managed buffer rings (kmbufs) and user-provided buffer rings (pbufs) in a single API is confusing, particularly when using IORING_MEM_REGION. The author acknowledges this point but still prefers to separate the two interfaces, agreeing to restructure kmbufs to go through the pbuf uapi in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a fix is needed",
                "agrees to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "imo it felt cleaner to have a new uapi for it because kmbufs and pbufs\nhave different expectations and behaviors (eg pbufs only work with\nuser-provided buffers and requires userspace to populate the ring\nbefore using it, whereas for kmbufs the kernel allocates the buffers\nand populates it for you; pbufs require userspace to recycle back the\nbuffer, whereas for kmbufs the kernel is the one in control of\nrecycling) and from the user pov it seemed confusing to have kmbufs as\npart of the pbuf ring uapi, instead of separating it out as a\ndifferent type of ringbuffer with a different expectation and\nbehavior. I was trying to make the point that combining the interface\nif we go with IORING_MEM_REGION gets even more confusing because now\npbufs that are kernel-managed are also empty at initialization and\nonly can point to areas inside a registered mem region and the\nresponsibility of populating it is now on whatever subsystem is using\nit.\n\nI still have this opinion but I also think in general, you likely know\nbetter than I do what kind of io-uring uapi is best for io-uring's\nusers. For v2 I'll have kmbufs go through the pbuf uapi.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to feedback that existing code for non-kernel managed pbuf rings has a ring entry marker, and suggested this could be fixed by passing the number of buffers from uapi for kernel-managed pbuf rings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm not really seeing what the purpose of having a ring entry with no\nbuffer associated with it is. In the existing code for non-kernel\nmanaged pbuf rings, there's the same tie between reg->ring_entries\nbeing used as the marker for how many buffers the ring supports. But\nif the number of buffers should be different than the number of ring\nentries, this can be easily fixed by passing in the number of buffers\nfrom the uapi for kernel-managed pbuf rings.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that there are limitations in kernel-managed buffer rings, specifically regarding dynamic allocation of memory and upfront knowledge of required memory size.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged limitations",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "To play devil's advocate, we also can't easily add more memory to the\nmem region once it's been registered. I think there's also a worse\npenalty where the user needs to know upfront how much memory to\nallocate for the mem region for the lifetime of the ring, which imo\nmay be hard to do (eg if a kernel-managed buf ring only needs to be\nregistered for some code paths and not others, the mem region\nregistration would still have to allocate the memory a potential kbuf\nring would use).",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern that the selected buffer index in __io_uring_cmd_done() needs to be set, and responded by explaining that the buffer memory has the same lifetime as the ring object, which is freed when the ring itself is freed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm a bit confused by this part. The buffer memory does have the same\nlifetime as the ring object, no? The buffers only get freed when the\nring itself is freed.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author asked if reviewer is open to having both a simple kernel-managed pbuf interface and one that goes through registered memory regions, indicating willingness to make changes based on feedback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "willingness to make changes",
                "asking for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I appreciate you looking at this and giving your feedback and insight.\nThank you for doing so. I don't want to merge in something you're\nunhappy with.\n\nAre you open to having support for both a simple kernel-managed pbuf\ninterface and later on if/when the need arises, a kernel-managed pbuf\ninterface that goes through a registered memory region? If the answer\nis no, then I'll make the change to have kmbufs go through the\nregistered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that buffer rings are not suitable for storage read/write requests because they bind to a buffer immediately, whereas other types of requests like recv allow the kernel to first poll the socket and then take a buffer from the ring. The reviewer also pointed out that someone needs to return buffers back into the private kernel ring, which is currently assumed to be handled by the fuse driver but poses a problem for normal rw requests.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Provided buffer rings are not useful for storage read/write requests\nbecause they bind to a buffer right away, that's in contrast to some\nrecv request, where io_uring will first poll the socket to confirm\nthe data is there, and only then take a buffer from the buffer ring\nand copy into it. With storage rw it makes more sense to specify\nthe buffer directly gain control over where exactly data lands\nIOW, instead of the usual \"read data into a given pointer\" request\nsemantics like what read(2) gives you, buffer rings are rather\n\"read data somewhere and return a pointer to where you placed it\".\n\nAnother problem is that someone needs to return buffers back into\nthe buffer ring, and it's a kernel private ring. For this patchset\nit's assumed the fuse driver is going to be doing that, but there\nis no one for normal rw requests.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested using IORING_MEM_REGION or a standalone registered buffer extension to provide buffers/memory without extra semantics, potentially yielding a finer API.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes. You only need buffers, and it'll be better to base on sth that\ngives you buffers/memory without extra semantics, i.e.\nIORING_MEM_REGION. Or it can be a standalone registered buffer\nextension, likely reusing regions internally. That might even yield\na finer API.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer questioned whether kernel-managed buffer rings can be used with other requests, specifically IORING_OP_RECV with IOSQE_BUFFER_SELECT set",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "question",
                "clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Oops, typo. I was asking whether the buffer rings (not buffers) are\nsupposed to be used with other requests. E.g. submitting a\nIORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying\nyour kernel-managed buffer ring.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov raised two separate concerns: first, he suggested separating buffers from buffer rings in the io_uring user API, and second, he proposed allowing optional user memory for buffer creation by reusing the region abstraction.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no objection",
                "pros and cons"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There are two separate arguments. The first is about not making buffers\ninseparable from buffer rings in the io_uring user API. Whether it's\nIORING_REGISTER_MEM_REGION or something else is not that important.\nI have no objection if it's a part of fuse instead though, e.g. if\nfuse binds two objects together when you register it with fuse, or even\nif fuse create a buffer ring internally (assuming it doesn't indirectly\nleak into io_uring uapi).\n\nAnd the second was about optionally allowing user memory for buffer\ncreation as you're reusing the region abstraction. You can find pros\nand cons for both modes, and funnily enough, SQ/CQ were first kernel\nallocated and then people asked for backing it by user memory, and IIRC\nit was in the reverse order for pbuf rings.\n\nImplementing this is trivial as well, you just need to pass an argument\nwhile creating a region. All new region users use struct\nio_uring_region_desc for uapi and forward it to io_create_region()\nwithout caring if it's user or kernel allocated memory.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov questioned the necessity of making buffer ring management an io_uring API, suggesting it could be simpler to implement in fuse or as an internal implementation detail",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning necessity",
                "suggesting alternative approaches"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The stress is on why it's an _io_uring_ API. It doesn't matter to me\nwhether it's a separate opcode or not. Currently, buffer rings don't give\nyou anything that can't be pure fuse, and it might be simpler to have\nit implemented in fuse than binding to some io_uring object. Or it could\ncreate buffer rings internally to reuse code but it doesn't become an\nio_uring uapi but rather implementation detail. And that predicates on\nwhether km rings are intended to be used with other / non-fuse requests.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer believes that the current implementation of kernel-managed buffer rings in io_uring is not reusable for other users and suggests allowing registration of km rings together with memory as a pure region without a notion of a buffer, to be chunked by fuse later.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "middle ground suggested"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I believe the source of disagreement is that you're thinking\nabout how it's going to look like for fuse specifically, and I\nbelieve you that it'll be nicer for the fuse use case. However,\non the other hand it's an io_uring uapi, and if it is an io_uring\nuapi, we need reusable blocks that are not specific to particular\nusers.\n\nIf it km rings has to stay an io_uring uapi, I guess a middle\nground would be to allow registering km rings together with memory,\nbut make it a pure region without a notion of a buffer, and let\nfuse to chunk it. Later, we can make payload memory allocation\noptional.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer noted that the patch introduces a non-generic io_uring interface by assuming fuse-specific behavior, and requested that the code be revised to avoid these assumptions",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "assumptions in different places"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right, intentionally so, because otherwise it's a fuse uapi that\npretends to be a generic io_uring uapi but it's not because of\nall assumptions in different places.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that the current implementation of __io_uring_cmd_done() only sets the buffer ring depth but does not account for the actual memory allocated by userspace, which could lead to issues such as running out of buffers or inefficient use of memory. The reviewer suggested considering factors like runtime memory addition, recycling, and incremental consumption when determining the buffer ring size.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Not really, it tells the buffer ring depth but says nothing about\nhow much memory user space allocated and how it's pushed. It's a\nreasonable default but they could be different. For example, if you\nexpect adding more memory at runtime, you might create the buffer\nring a bit larger. Or when server processing takes a while and you\ncan't recycle until it finishes, you might have more buffers than\nyou need ring entries. Or you might might decide to split buffers\nand as you mentioned incremental consumption, which is an entire\nseparate topic because it doesn't do de-fragmentation and you'd\nneed to have it in fuse, just like user space does with pbufs.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer suggested rethinking io_uring uapi by having it allocate a large block of memory and letting fuse manage buffer allocation",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear technical objection or suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "My entire point is that we're making lots of assumptions for io_uring\nuapi, and if it's moved to fuse because it knows better what it\nneeds, it should be a win.\n\nIOW, it sounds better if instead of passing the number of buffers to\nio_uring, you just ask it to create a large chunk of memory, and then\nfuse chunks it up and puts into the ring.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov agreed with the patch but noted that adding new memory would require a new mechanism, not necessarily tied to IORING_REGISTER_MEM_REGION.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "agreement",
                "request for additional consideration"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I agree, and you'd need something new in either case to add more\nmemory, and it doesn't need to be IORING_REGISTER_MEM_REGION\nspecifically.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that unregistering a buffer ring does not guarantee there are no inflight requests using buffers from the ring, and requested synchronization with all other io_uring requests to prevent issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "synchronization",
                "inflight requests"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Unregistering a buffer ring doesn't guarantee that there are no\ninflight requests that are still using buffers that came out of\nthe buffer ring. The fuse driver can wait/terminate its requests\nbefore unregisteration, but allow userspace issued IORING_OP_RECV\nto use this km buffer ring, and you'll need to somehow synchronise\nwith all other io_uring requests.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that a fix is needed for the selected buffer index in __io_uring_cmd_done() and promised to modify it in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for modification",
                "promised to modify in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sorry, I submitted v2 last night thinking the conversation on this\nthread had died. After reading through your reply, I'll modify v2.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong addressed Pavel Begunkov's concern about the buffer rings being used for other io-uring requests, explaining that they are intended to be used in conjunction with IORING_OP_READ/WRITE_FIXED operations to avoid per-i/o page pinning overhead costs.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes the buffer rings are intended to be used with other io-uring\nrequests. The ideal scenario is that the user can then do the\nequivalent of IORING_OP_READ/WRITE_FIXED operations on the\nkernel-managed buffers and avoid the per-i/o page pinning overhead\ncosts.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong addressed Pavel Begunkov's feedback that the io_uring buffer rings should be decoupled from the backing buffers. She agreed that having the buffers owned by the ring is more generically useful and self-contained, but acknowledged that users might want control over which parts of the memory region get used as backing buffers in the future. Joanne proposed a design where normal requests use the registered memory region's pages array to find associated pages for the offset passed in, and suggested repurposing struct io_uring_sqe fields to include an offset into the registered mem region.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a design trade-off",
                "asked for clarification on future refcounting"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree 100%. The api we add should be what's best for io-uring, not fuse.\n\nFor the majority of use cases, it seemed to me that having the buffers\nseparated from the buffer rings didn't yield perceptible benefits but\nadded complexity and more restrictions like having to statically know\nup front how big the mem region needs to be across the lifetime of the\nio-uring for anything the io-uring might use the mem region for. It\nseems more generically useful as a concept to have the buffers owned\nby the ring and tied to the lifetime of the ring. I like how with this\ndesign everything is self-contained and multiple subsystems can use it\nwithout having to reimplement functionality locally in the subsystem.\nOn the other hand, I see your point about how it might be something\nusers want in the future if they want complete control over which\nparts of the mem region get used as the backing buffers to do stuff\nlike PMD optimizations.\n\nI think this is a matter of opinion/preference and I think in general\nfor anything io-uring related, yours should take precedence.\n\nWith it going through a mem region, I don't think it should even go\nthrough the \"pbuf ring\" interface then if it's not going to specify\nthe number of entries and buffer sizes upfront, if support is added\nfor io-uring normal requests (eg IORING_OP_READ/WRITE) to use the\nbacking pages from a memory region and if we're able to guarantee that\nthe registered memory region will never be able to be unregistered by\nthe user. I think if we repurpose the\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n};\n\nfields in the struct io_uring_sqe to\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n  __u64 offset; /* offset into registered mem region */\n};\n\nand add some IOSQE_ flag to indicate it should find the pages from the\nregistered mem region, then that should work for normal requests.\nWhere on the kernel side, it looks up the associated pages stored in\nthe io_mapped_region's pages array for the offset passed in.\n\nRight now there's only a uapi to register a memory region and none to\nunregister one. Is it guaranteed that io-uring will never add\nsomething in the future that will let userspace unregister the memory\nregion or at least unregister it while it's being used (eg if we add\nfuture refcounting to it to track active uses of it)?\n\nIf so, then end-to-end, with it going through the mem region, it would\nbe something like:\n* user creates a mem region for the io-uring\n* user mmaps the mem region\n* user passes in offset into region, length of each buffer, and number\nof entries in the ring to the subsystem\n* subsystem creates a locally managed bufring and adds buffers to that\nring from the mem region\n* on the cqe side, it sends the buffer id of the registered mem region\nthrough the same \"IORING_CQE_F_BUFFER |  (buf_id <<\nIORING_CQE_BUFFER_SHIFT)\" mechanism\n\nDoes this design match what you had in mind / prefer?\n\nI think the above works for Christoph's use case too (as his and my\nuse case are the same) but if not, please let me know.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov questioned whether a server or user space program can issue I/O requests that consume buffers/entries from kernel-managed buffer rings without involving fuse kernel code, and asked for clarification on the expected use case.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "unclear expectations",
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You mention OP_READ_FIXED and below agreed not exposing km rings\nan io_uring uapi, which makes me believe we're still talking about\ndifferent things.\n\nCorrect me if I'm wrong. Currently, only fuse cmds use the buffer\nring itself, I'm not talking about buffer, i.e. fuse cmds consume\nentries from the ring (!!! that's the part I'm interested in), then\nprocess them and tell the server \"this offset in the region has user\ndata to process or should be populated with data\".\n\nNaturally, the server should be able to use the buffers to issue\nsome I/O and process it in other ways, whether it's a normal\nOP_READ to which you pass the user space address (you can since\nit's mmap()'ed by the server) or something else is important but\na separate question than the one I'm trying to understand.\n\nSo I'm asking whether you expect that a server or other user space\nprogram should be able to issue a READ_OP_RECV, READ_OP_READ or any\nother similar request, which would consume buffers/entries from the\nkm ring without any fuse kernel code involved? Do you have some\nuse case for that in mind?\n\nUnderstanding that is the key in deciding whether km rings should\nbe exposed as io_uring uapi or not, regardless of where buffers\nto populate the ring come from.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested reusing registered buffers instead of introducing a new mechanism for kernel-managed buffer rings, citing efficiency and optimization benefits. He provided pseudo-code to create a registered buffer from the mmap'ed region pointer and proposed patching io_uring_register_buffers() to allow file-backed pages.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "So you already can do all that using the mmap()'ed region user\npointer, and you just want it to be more efficient, right?\nFor that let's just reuse registered buffers, we don't need a\nnew mechanism that needs to be propagated to all request types.\nAnd registered buffer are already optimised for I/O in a bunch\nof ways. And as a bonus, it'll be similar to the zero-copy\ninternally registered buffers if you still plan to add them.\n\nThe simplest way to do that is to create a registered buffer out\nof the mmap'ed region pointer. Pseudo code:\n\n// mmap'ed if it's kernel allocated.\n{region_ptr, region_size} = create_region();\n\nstruct iovec iov;\niov.iov_base = region_ptr;\niov.iov_len = region_size;\nio_uring_register_buffers(ring, &iov, 1);\n\n// later instead of this:\nptr = region_ptr + off;\nio_uring_prep_read(sqe, fd, ptr, ...);\n\n// you use registered buffers as usual:\nio_uring_prep_read_fixed(sqe, fd, off, regbuf_idx, ...);\n\n\nIIRC the registration would fail because it doesn't allow file\nbacked pages, but it should be fine if we know it's io_uring\nregion memory, so that would need to be patched.\n\nThere might be a bunch of other ways you can do that like\ncreate a kernel allocated registered buffer like what Cristoph\nwants, and then register it as a region. Or allow creating\nregistered buffers out of a region. etc.\n\nI wanted to unify registered buffers and regions internally\nat some point, but then drifted away from active io_uring core\ninfrastructure development, so I guess that could've been useful.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that kernel-managed buffer rings may hold page references or require pinning of regions, which could lead to issues if registered buffers are used instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "pinning requirement"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Let's talk about it when it's needed or something changes, but if\nyou do registered buffers instead as per above, they'll be holding\npage references and or have to pin the region in some other way.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer suggested adding a liburing helper to handle mmap'ing for the fuse server, eliminating the need for it to directly manage memory",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggestion",
                "alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, we should just add a liburing helper, so that fuse server\ndoesn't need to deal with mmap'ing.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer expressed conditional approval, requesting confirmation that the patch allows for desired fast path optimizations",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "conditional approval",
                "request for confirmation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That's sounds clean to me _if_ it allows you to achieve all\n(fast path) optimisations you want to have. I hope it does?\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about the applicability of kernel-managed buffer rings beyond fuse servers, acknowledging that other subsystems or users could benefit from this optimization.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a potential use case",
                "identified additional potential beneficiaries"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for clarifying your question. Yes, this would be a useful\noptimization in the future for fuse servers with certain workload\ncharacteristics (eg network-backed servers with high concurrency and\nunpredictable latencies). I don't think the concept of kmbufrings is\nexclusively fuse-specific though (for example, Christoph's use case\nbeing a recent instance); I think other subsystems/users that'll use\nkmbuf rings would also generically find it useful to have the option\nof READ_OP_RECV/READ_OP_READ operating directly on the ring.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author expressed concerns about added complexity and potential confusion in the design, questioning the need for kernel-managed buffer rings when memory regions could be used instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "added complexity",
                "potential confusion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I feel like this design makes the interface more convoluted and now\nmuddies different concepts together by adding new complexity /\nrelationships between them whereas they were otherwise cleanly\nisolated. Maybe I'm just not seeing/understanding the overarching\nvision for why conceptually it makes sense for them to be tied\ntogether besides as a mechanism to tell io-uring requests where to\ncopy from by reusing what exists for fixed buffer ids. There's more\ncomplexity now on the kernel side (eg having to detect if the buffer\npassed in is kernel-allocated to know whether to pin the pages /\ncharge it against the user's RLIMIT_MEMLOCK limit) but I'm not\nunderstanding what we gain from it. I got the sense from your previous\ncomments that memory regions are the de facto way to go and should be\ndecoupled from other structures, so if that's the case, why doesn't it\nmake sense for io-uring to add native support for using memory regions\nfor io-uring requests? I feel like from the userspace side it makes\nthings more confusing with this extra layer of indirection that now\nhas to go through a fixed buffer.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern that the caller cannot guarantee registering the memory region as a fixed buffer, and explained that this would introduce extra overhead for every I/O operation. Author did not promise to fix the issue but instead presented an alternative solution of adding pinning to registered memory regions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "alternative solution proposed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't think we can guarantee that the caller will register the\nmemory region as a fixed buffer (eg if it doesn't need/want to use the\nbuffer for normal io-uring requests). On the kernel side, the internal\nbuffer entry uses the kaddr of the registered memory region buffer for\nany memcpys. If it's not guaranteed that registered memory regions\npersist for the lifetime of the ring, there'll have to be extra\noverhead for every I/O (eg grab the io-uring lock, checking if the mem\nregion is still registered, grab a refcount to that mem region, unlock\nthe ring, do the memcpy to the kaddr, then grab the io-uring lock\nagain, decrement the refcount, and unlock). Or I guess we could add\npinning to a registered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v1 1/1] iomap: don't mark folio uptodate if read IO has bytes pending",
          "message_id": "CAJnrk1aJJqafDkxMypUym6iFQ-HkaSxneOe6Sc746AwrmrDK4Q@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1aJJqafDkxMypUym6iFQ-HkaSxneOe6Sc746AwrmrDK4Q@mail.gmail.com/",
          "date": "2026-02-20T22:13:46Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about the folio uptodate state being cleared when iomap_set_range_uptodate() marks it as uptodate, and then folio_end_read() uses XOR semantics to set it again. The author explains that this is because the read completion path calls folio_end_read(), which clears the uptodate bit if it was already set. To fix this, the author proposes not marking the folio as uptodate if the read IO has bytes pending.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If a folio has ifs metadata attached to it and the folio is partially\nread in through an async IO helper with the rest of it then being read\nin through post-EOF zeroing or as inline data, and the helper\nsuccessfully finishes the read first, then post-EOF zeroing / reading\ninline will mark the folio as uptodate in iomap_set_range_uptodate().\n\nThis is a problem because when the read completion path later calls\niomap_read_end(), it will call folio_end_read(), which sets the uptodate\nbit using XOR semantics. Calling folio_end_read() on a folio that was\nalready marked uptodate clears the uptodate bit.\n\nFix this by not marking the folio as uptodate if the read IO has bytes\npending. The folio uptodate state will be set in the read completion\npath through iomap_end_read() -> folio_end_read().\n\nReported-by: Wei Gao <wegao@suse.com>\nSuggested-by: Sasha Levin <sashal@kernel.org>\nTested-by: Wei Gao <wegao@suse.com>\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nFixes: b2f35ac4146d (\"iomap: add caller-provided callbacks for read and readahead\")\n---\n fs/iomap/buffered-io.c | 15 ++++++++++++---\n 1 file changed, 12 insertions(+), 3 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 58887513b894..4fc5ce963feb 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -80,18 +80,27 @@ static void iomap_set_range_uptodate(struct folio *folio, size_t off,\n {\n \tstruct iomap_folio_state *ifs = folio->private;\n \tunsigned long flags;\n-\tbool uptodate = true;\n+\tbool mark_uptodate = true;\n \n \tif (folio_test_uptodate(folio))\n \t\treturn;\n \n \tif (ifs) {\n \t\tspin_lock_irqsave(&ifs->state_lock, flags);\n-\t\tuptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n+\t\t/*\n+\t\t * If a read with bytes pending is in progress, we must not call\n+\t\t * folio_mark_uptodate(). The read completion path\n+\t\t * (iomap_read_end()) will call folio_end_read(), which uses XOR\n+\t\t * semantics to set the uptodate bit. If we set it here, the XOR\n+\t\t * in folio_end_read() will clear it, leaving the folio not\n+\t\t * uptodate.\n+\t\t */\n+\t\tmark_uptodate = ifs_set_range_uptodate(folio, ifs, off, len) &&\n+\t\t\t\t!ifs->read_bytes_pending;\n \t\tspin_unlock_irqrestore(&ifs->state_lock, flags);\n \t}\n \n-\tif (uptodate)\n+\tif (mark_uptodate)\n \t\tfolio_mark_uptodate(folio);\n }\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong suggested adding a link to a relevant discussion on linux-fsdevel and Cc'ing stable@vger.kernel.org for the v6.19 release, indicating that the patch is now in a released kernel.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested additional information",
                "acknowledged the patch's status"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I would add:\n\nLink: https://lore.kernel.org/linux-fsdevel/aYbmy8JdgXwsGaPP@autotest-wegao.qe.prg2.suse.org/\nCc: <stable@vger.kernel.org> # v6.19\n\nsince the recent discussion around this was sort of buried in a\ndifferent thread, and the original patch is now in a released kernel.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer asked how difficult it would be to write a test for the patch, indicating that the code change makes sense",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yeah, that makes sense.  How difficult is this to write up as an fstest?\n\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n\n--D",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Matthew Wilcox",
              "summary": "reviewer expressed frustration with the complexity of the iomap code, stating it's too complicated to explain a simple solution to Joanne",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "frustration",
                "complexity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This isn't \"the xor thing has come back to bite us\".  This is \"the iomap\ncode is now too complicated and I cannot figure out how to explain to\nJoanne that there's really a simple way to do this\".\n\nI'm going to have to set aside my current projects and redo the iomap\nreadahead/read_folio code myself, aren't I?",
              "reply_to": "Darrick Wong",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong expressed confusion about an alternative approach mentioned in a previous discussion, and requested clarification on what that simpler way is.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "lack of clarity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Well you could try explaining to me what that simpler way is?\n\n/me gets the sense he's missing a discussion somewhere...\n\n--D",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to feedback from Darrick Wong by providing a link to the prior discussion, indicating that they are not addressing any specific technical concerns or providing additional information.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "lack of response",
                "no explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This is the link to the prior discussion\nhttps://lore.kernel.org/linux-fsdevel/20251223223018.3295372-1-sashal@kernel.org/T/#mbd61eaa5fd1e8922caa479720232628e39b8c9da\n\nThanks,\nJoanne",
              "reply_to": "Darrick Wong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong noted that the read_bytes_pending field in iomap has inconsistent behavior across different IO paths, and suggested consolidating the read code to simplify it. He also mentioned a bug in Linus' tree that needs fixing, but considered Joanne's patch as a sufficient temporary fix.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "<willy and I had a chat; this is a clumsy non-AI summary of it>\n\nI started looking at folio read state management in iomap, and made a\nfew observations that (I hope) match what willy's grumpy about.\n\nThere are three ways that iomap can be reading into the pagecache:\na) async ->readahead,\nb) synchronous ->read_folio (page faults), and\nc) synchronous ->read_folio_range (pagecache write).\n\n(Note that (b) can call a different ->read_folio_range than (c), though\nall implementations seem to have the same function)\n\nAll three of these IO paths share the behavior that they try to fill out\nthe folio's contents and set the corresponding folio/ifs uptodate bits\nif that succeeds.  Folio contents can come from anywhere, whether it's:\n\ni) zeroing memory,\nii) copying from an inlinedata buffer, or\niii) asynchronously fetching the contents from somewhere\n\nIn the case of (c) above, if the read fails then we fail the write, and\nif the read succeeds then we start copying to the pagecache.\n\nHowever, (a) and (b) have this additional read_bytes_pending field in\nthe ifs that implements some extra tracking.  AFAICT the purpose of this\nfield is to ensure that we don't call folio_end_read prematurely if\nthere's an async read in progress.  This can happen if iomap_iter\nreturns a negative errno on a partially processed folio, I think?\n\nread_bytes_pending is initialized to the folio_size() at the start of a\nread and subtracted from when parts of the folio are supplied, whether\nthat's synchronous zeroing or asynchronous read ioend completion.  When\nthe field reaches zero, we can then call folio_end_read().\n\nBut then there are twists, like the fact that we only call\niomap_read_init() to set read_bytes_pending if we decide to do an\nasynchronous read.  Or that iomap_read_end and iomap_finish_folio_read\nhave awfully similar code.  I think in the case of (i) and (ii) we also\ndon't touch read_pending_bytes at all, and merely set the uptodate bits?\n\nThis is confusing to me.  It would be more straightforward (I think) if\nwe just did it for all cases instead of adding more conditionals.  IOWs,\nhow hard would it be to consolidate the read code so that there's one\nfunction that iomap calls when it has filled out part of a folio.  Is\nthat possible, even though we shouldn't be calling folio_end_read during\na pagecache write?\n\nAt the end of the day, however, there's a bug in Linus' tree and we need\nto fix it, so Joanne's patch is a sufficient bandaid until we can go\nclean this up.\n\n--D",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [syzbot] [iomap?] WARNING in ifs_free",
          "message_id": "CAJnrk1bk7jN8SfHny9nVWZZS6tP8bnQbMZHTCuFma6-YuMugAg@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1bk7jN8SfHny9nVWZZS6tP8bnQbMZHTCuFma6-YuMugAg@mail.gmail.com/",
          "date": "2026-02-20T00:47:10Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "syzbot (author)",
              "summary": "The author is responding to feedback about the swapoff path needing to drop the per-vswap spinlock before calling try_to_unmap(). The author found a reproducer for the issue and provided additional information, but did not address the specific concern about locking.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "additional information provided"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "syzbot has found a reproducer for the following issue on:\n\nHEAD commit:    2b7a25df823d Merge tag 'mm-nonmm-stable-2026-02-18-19-56' ..\ngit tree:       upstream\nconsole output: https://syzkaller.appspot.com/x/log.txt?x=10c21722580000\nkernel config:  https://syzkaller.appspot.com/x/.config?x=65722f41f7edc17e\ndashboard link: https://syzkaller.appspot.com/bug?extid=d3a62bea0e61f9d121da\ncompiler:       Debian clang version 21.1.8 (++20251221033036+2078da43e25a-1~exp1~20251221153213.50), Debian LLD 21.1.8\nsyz repro:      https://syzkaller.appspot.com/x/repro.syz?x=1501dc02580000\nC reproducer:   https://syzkaller.appspot.com/x/repro.c?x=1357f652580000\n\nDownloadable assets:\ndisk image (non-bootable): https://storage.googleapis.com/syzbot-assets/d900f083ada3/non_bootable_disk-2b7a25df.raw.xz\nvmlinux: https://storage.googleapis.com/syzbot-assets/f3a54d09b17c/vmlinux-2b7a25df.xz\nkernel image: https://storage.googleapis.com/syzbot-assets/fb704901bce5/bzImage-2b7a25df.xz\nmounted in repro: https://storage.googleapis.com/syzbot-assets/b778b9903de5/mount_0.gz\n\nIMPORTANT: if you fix the issue, please add the following tag to the commit:\nReported-by: syzbot+d3a62bea0e61f9d121da@syzkaller.appspotmail.com\n\n------------[ cut here ]------------\nifs_is_fully_uptodate(folio, ifs) != folio_test_uptodate(folio)\nWARNING: fs/iomap/buffered-io.c:256 at ifs_free+0x358/0x420 fs/iomap/buffered-io.c:255, CPU#0: syz-executor/5453\nModules linked in:\nCPU: 0 UID: 0 PID: 5453 Comm: syz-executor Not tainted syzkaller #0 PREEMPT(full) \nHardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 1.16.3-debian-1.16.3-2 04/01/2014\nRIP: 0010:ifs_free+0x358/0x420 fs/iomap/buffered-io.c:255\nCode: 41 5f 5d e9 7a fb bd ff e8 45 5a 5e ff 90 0f 0b 90 e9 d0 fe ff ff e8 37 5a 5e ff 90 0f 0b 90 e9 0a ff ff ff e8 29 5a 5e ff 90 <0f> 0b 90 eb c3 44 89 e1 80 e1 07 80 c1 03 38 c1 0f 8c 06 fe ff ff\nRSP: 0018:ffffc9000dfcf688 EFLAGS: 00010293\nRAX: ffffffff82674207 RBX: 0000000000000008 RCX: ffff88801f834900\nRDX: 0000000000000000 RSI: 0000000000000008 RDI: 0000000000000000\nRBP: 000000008267bc01 R08: ffffea00010fb747 R09: 1ffffd400021f6e8\nR10: dffffc0000000000 R11: fffff9400021f6e9 R12: ffff888051c7da44\nR13: ffff888051c7da00 R14: ffffea00010fb740 R15: 1ffffd400021f6e9\nFS:  0000555586def500(0000) GS:ffff88808ca5b000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 0000555586e0aa28 CR3: 00000000591fe000 CR4: 0000000000352ef0\nCall Trace:\n <TASK>\n folio_invalidate mm/truncate.c:140 [inline]\n truncate_cleanup_folio+0xcb/0x190 mm/truncate.c:160\n truncate_inode_pages_range+0x2ce/0xe30 mm/truncate.c:404\n ntfs_evict_inode+0x19/0x40 fs/ntfs3/inode.c:1861\n evict+0x61e/0xb10 fs/inode.c:846\n dispose_list fs/inode.c:888 [inline]\n evict_inodes+0x75a/0x7f0 fs/inode.c:942\n generic_shutdown_super+0xaa/0x2d0 fs/super.c:632\n kill_block_super+0x44/0x90 fs/super.c:1725\n ntfs3_kill_sb+0x44/0x1c0 fs/ntfs3/super.c:1889\n deactivate_locked_super+0xbc/0x130 fs/super.c:476\n cleanup_mnt+0x437/0x4d0 fs/namespace.c:1312\n task_work_run+0x1d9/0x270 kernel/task_work.c:233\n resume_user_mode_work include/linux/resume_user_mode.h:50 [inline]\n __exit_to_user_mode_loop kernel/entry/common.c:67 [inline]\n exit_to_user_mode_loop+0xed/0x480 kernel/entry/common.c:98\n __exit_to_user_mode_prepare include/linux/irq-entry-common.h:226 [inline]\n syscall_exit_to_user_mode_prepare include/linux/irq-entry-common.h:256 [inline]\n syscall_exit_to_user_mode include/linux/entry-common.h:325 [inline]\n do_syscall_64+0x32d/0xf80 arch/x86/entry/syscall_64.c:100\n entry_SYSCALL_64_after_hwframe+0x77/0x7f\nRIP: 0033:0x7fb0f859d897\nCode: a2 c7 05 5c ee 24 00 00 00 00 00 eb 96 e8 e1 12 00 00 90 31 f6 e9 09 00 00 00 66 0f 1f 84 00 00 00 00 00 b8 a6 00 00 00 0f 05 <48> 3d 00 f0 ff ff 77 01 c3 48 c7 c2 e8 ff ff ff f7 d8 64 89 02 b8\nRSP: 002b:00007ffd23732b28 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6\nRAX: 0000000000000000 RBX: 00007fb0f8631ef0 RCX: 00007fb0f859d897\nRDX: 0000000000000000 RSI: 0000000000000009 RDI: 00007ffd23732be0\nRBP: 00007ffd23732be0 R08: 00007ffd23733be0 R09: 00000000ffffffff\nR10: 0000000000000000 R11: 0000000000000246 R12: 00007ffd23733c70\nR13: 00007fb0f8631ef0 R14: 000000000001b126 R15: 00007ffd23733cb0\n </TASK>\n\n\n---\nIf you want syzbot to run the reproducer, reply with:\n#syz test: git://repo/address.git branch-or-commit-hash\nIf you attach or paste a git patch, syzbot will apply it before testing.",
              "reply_to": "",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong",
              "summary": "Reviewer noted that ifs_free() is called while holding the folio lock, but it also acquires the per-vswap spinlock, creating a potential deadlock scenario when reclaim paths are executed.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential deadlock",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Thu, Feb 19, 2026 at 12:51PM syzbot\n<syzbot+d3a62bea0e61f9d121da@syzkaller.appspotmail.com> wrote:",
              "reply_to": "syzbot",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong",
              "summary": "The reviewer, Joanne Koong, noted that the ifs bitmap is not updated when a folio is marked uptodate by ntfs3 for compressed files, which leads to a false WARN_ON_ONCE condition in ifs_free. She suggested either removing the WARN_ON_ONCE or making iomap_set_range_uptodate() a public API callable by subsystems.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I ran the repro locally to see if it's the same issue fixed by [1] but\nthis is a different unrelated issue.\n\nThe folio is uptodate but the ifs uptodate bitmap is not reflected as\nfully uptodate. I think this is because ntfs3 handles writes for\ncompressed files through its own interface that doesn't go through\niomap where it calls folio_mark_uptodate() but the ifs bitmap doesn't\nget updated. fuse-blk servers that operate in writethrough mode run\ninto something like this as well [2].\n\nThis doesn't lead to any data corruption issues. Should we get rid of\nthe  WARN_ON_ONCE(ifs_is_fully_uptodate(folio, ifs) !=\nfolio_test_uptodate(folio))? The alternative is to make a modified\nversion of the functionality in \"iomap_set_range_uptodate()\" a public\napi callable by subsystems.\n\nThanks,\nJoanne\n\n[1] https://lore.kernel.org/linux-fsdevel/20260219003911.344478-1-joannelkoong@gmail.com/\n[2] https://lore.kernel.org/linux-fsdevel/20251223223018.3295372-2-sashal@kernel.org/",
              "reply_to": "syzbot",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer expressed frustration that ntfs3 has copied and pasted large chunks of the buffered read code without explanation, and requested they explain their actions instead of having them 'papered over'.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'd honestly rather have ntfs3 come along and explain what their\ndoing.  They've copy and pasted large chunks of the buffered\nread code for now reason, which already annoys me and I'd rather\nnot paper over random misuses.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Johannes Weiner",
      "primary_email": "hannes@cmpxchg.org",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/2] mm: vmalloc: streamline vmalloc memory accounting",
          "message_id": "20260220191035.3703800-1-hannes@cmpxchg.org",
          "url": "https://lore.kernel.org/all/20260220191035.3703800-1-hannes@cmpxchg.org/",
          "date": "2026-02-20T19:10:37Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch streamlines memory accounting for vmalloc by replacing a custom atomic counter with a vmstat counter, which provides per-node data and prepares the system for cleaning up memcg (memory cgroup) accounting.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Johannes Weiner (author)",
              "summary": "The author is addressing a concern about the removal of the custom memcg counter for vmalloc memory accounting. They eliminated the custom counter and replaced it with a single consolidated accounting call in the vmalloc code, which results in fewer changes to the vmalloc code.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Eliminates the custom memcg counter and results in a single,\nconsolidated accounting call in vmalloc code.\n\nSigned-off-by: Johannes Weiner <hannes@cmpxchg.org>\n---\n include/linux/memcontrol.h |  1 -\n mm/memcontrol.c            |  4 ++--\n mm/vmalloc.c               | 16 ++++------------\n 3 files changed, 6 insertions(+), 15 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 67f154de10bc..c7cc4e50e59a 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -35,7 +35,6 @@ enum memcg_stat_item {\n \tMEMCG_SWAP = NR_VM_NODE_STAT_ITEMS,\n \tMEMCG_SOCK,\n \tMEMCG_PERCPU_B,\n-\tMEMCG_VMALLOC,\n \tMEMCG_KMEM,\n \tMEMCG_ZSWAP_B,\n \tMEMCG_ZSWAPPED,\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 129eed3ff5bb..fef5bdd887e0 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -317,6 +317,7 @@ static const unsigned int memcg_node_stat_items[] = {\n \tNR_SHMEM_THPS,\n \tNR_FILE_THPS,\n \tNR_ANON_THPS,\n+\tNR_VMALLOC,\n \tNR_KERNEL_STACK_KB,\n \tNR_PAGETABLE,\n \tNR_SECONDARY_PAGETABLE,\n@@ -339,7 +340,6 @@ static const unsigned int memcg_stat_items[] = {\n \tMEMCG_SWAP,\n \tMEMCG_SOCK,\n \tMEMCG_PERCPU_B,\n-\tMEMCG_VMALLOC,\n \tMEMCG_KMEM,\n \tMEMCG_ZSWAP_B,\n \tMEMCG_ZSWAPPED,\n@@ -1359,7 +1359,7 @@ static const struct memory_stat memory_stats[] = {\n \t{ \"sec_pagetables\",\t\tNR_SECONDARY_PAGETABLE\t\t},\n \t{ \"percpu\",\t\t\tMEMCG_PERCPU_B\t\t\t},\n \t{ \"sock\",\t\t\tMEMCG_SOCK\t\t\t},\n-\t{ \"vmalloc\",\t\t\tMEMCG_VMALLOC\t\t\t},\n+\t{ \"vmalloc\",\t\t\tNR_VMALLOC\t\t\t},\n \t{ \"shmem\",\t\t\tNR_SHMEM\t\t\t},\n #ifdef CONFIG_ZSWAP\n \t{ \"zswap\",\t\t\tMEMCG_ZSWAP_B\t\t\t},\ndiff --git a/mm/vmalloc.c b/mm/vmalloc.c\nindex a49a46de9c4f..8773bc0c4734 100644\n--- a/mm/vmalloc.c\n+++ b/mm/vmalloc.c\n@@ -3446,9 +3446,6 @@ void vfree(const void *addr)\n \n \tif (unlikely(vm->flags & VM_FLUSH_RESET_PERMS))\n \t\tvm_reset_perms(vm);\n-\t/* All pages of vm should be charged to same memcg, so use first one. */\n-\tif (vm->nr_pages && !(vm->flags & VM_MAP_PUT_PAGES))\n-\t\tmod_memcg_page_state(vm->pages[0], MEMCG_VMALLOC, -vm->nr_pages);\n \tfor (i = 0; i < vm->nr_pages; i++) {\n \t\tstruct page *page = vm->pages[i];\n \n@@ -3458,7 +3455,7 @@ void vfree(const void *addr)\n \t\t * can be freed as an array of order-0 allocations\n \t\t */\n \t\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n-\t\t\tdec_node_page_state(page, NR_VMALLOC);\n+\t\t\tmod_lruvec_page_state(page, NR_VMALLOC, -1);\n \t\t__free_page(page);\n \t\tcond_resched();\n \t}\n@@ -3649,7 +3646,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n \t\t\tcontinue;\n \t\t}\n \n-\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n+\t\tmod_lruvec_page_state(page, NR_VMALLOC, 1 << large_order);\n \n \t\tsplit_page(page, large_order);\n \t\tfor (i = 0; i < (1U << large_order); i++)\n@@ -3696,7 +3693,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n \t\t\t\t\t\t\tpages + nr_allocated);\n \n \t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n-\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n+\t\t\t\tmod_lruvec_page_state(pages[i], NR_VMALLOC, 1);\n \n \t\t\tnr_allocated += nr;\n \n@@ -3722,7 +3719,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n \t\tif (unlikely(!page))\n \t\t\tbreak;\n \n-\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n+\t\tmod_lruvec_page_state(page, NR_VMALLOC, 1 << order);\n \n \t\t/*\n \t\t * High-order allocations must be able to be treated as\n@@ -3866,11 +3863,6 @@ static void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,\n \t\t\tvmalloc_gfp_adjust(gfp_mask, page_order), node,\n \t\t\tpage_order, nr_small_pages, area->pages);\n \n-\t/* All pages of vm should be charged to same memcg, so use first one. */\n-\tif (gfp_mask & __GFP_ACCOUNT && area->nr_pages)\n-\t\tmod_memcg_page_state(area->pages[0], MEMCG_VMALLOC,\n-\t\t\t\t     area->nr_pages);\n-\n \t/*\n \t * If not enough pages were obtained to accomplish an\n \t * allocation request, free them via vfree() if any.\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer pointed out that the patch does not handle the case where the vmalloc counter is not initialized, which could lead to incorrect memory accounting.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "missing initialization check",
                "potential for incorrect memory accounting"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n[...]",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt pointed out that the function mod_node_page_state() requires a struct pglist_data pointer as its first parameter, and suggested using page_pgdat(page) to obtain this pointer.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "mod_node_page_state() takes 'struct pglist_data *pgdat', you need to use\npage_pgdat(page) as first param.",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt approved the patch after suggested fixes were applied and added their Acked-by tag.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK",
                "APPROVAL"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "Same here.\n\nWith above fixes, you can add:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-20",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] Improving MGLRU",
          "message_id": "aZim2hT0nNjcRYVG@cmpxchg.org",
          "url": "https://lore.kernel.org/all/aZim2hT0nNjcRYVG@cmpxchg.org/",
          "date": "2026-02-20T18:24:35Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Johannes Weiner",
              "summary": "reviewer questioned the generality of Kairui's proposed solution for MGLRU, pointing out that it performs better with zswap but worse with disk swap, and suggested that a notion of backend speed is needed to balance anon and file reclaim",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Are you referring to refaults on the page cache side, or swapins?\n\nLast time we evaluated MGLRU on Meta workloads, we noticed that it\ntends to do better with zswap, but worse with disk swap. It seemed to\njust prefer reclaiming anon, period.\n\nFor the balancing between anon and file to work well in all\nsituations, it needs to have a notion of backend speed and factor in\nthe respective cost of misses on each side.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Reviewer noted that MGLRU's special casing of anonymous pages could lead to poor performance in workloads with large anon sets, as it assumes every refaulting anon page is hot and would fall apart when locality is low.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On the face of it, both of these sounds problematic to me. Why are\nanon pages special cased?\n\nThe cost of reclaiming a page is:\n\n    reuse frequency * cost of a miss\n\nThe *type* of the page is not all that meaningful for workload\nperformance. The wait time is qualitatively the same.\n\nIf you assume every refaulting anon is hot, it'll fall apart when the\nanon set is huge and has little locality.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song (author)",
              "summary": "Author acknowledged that MGLRU performs worse in some workloads without swap, specifically citing MongoDB as an example. They attributed this issue to the PID protection mechanism being too passive and the force protection in sort_folio being too aggressive. The author agreed that this is a fixable issue using their proposed approach, which they claim outperforms Active/Inactive LRU in all known cases.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "A bit more than that. When there is no swap, MGLRU still performs\nworse in some workloads like MongoDB. From what I've noticed that's\nbecause the PID protection is a bit too passive, and there is a force\nprotection in sort_folio which sometimes seems too aggressive.\nActive/Inactive LRU will just move a folio to head if it's accessed\ntwice while in RAM, but MGLRU won't do so, as result hotter file\nfolios are evicted equally as the colder one until the PID gets\ntriggered, or still gets protected even if it hasn't been used for a\nwhile. And by the time PID finally gets triggered, the workload might\nhas changed. This is fixable using the approach I mentioned though,\nand it seems to be better than the Active/Inactive in all our known\ncases after that, whether that is a good fix worth discussion.\n\nI also notice Ridong has a series to apply a \"heat\" based reclaim,\nwhich also looks interesting.",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song (author)",
              "summary": "Author acknowledged a concern about MGLRU ignoring shadow distance for re-activation, but noted that it's not considered a problem by users and performance looks good. They proposed alternative approaches to restore the old behavior or further optimize it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "proposed alternative approaches"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sorry I didn't make it clear. For MGLRU currently it already ignored\nthe shadow distance for re-activation. And yeah, basically all anons\nare activated on fault, which turns out to be quite nice? None MGLRU\nusers considered that as a problem and in fact the performance looks\ngood.\n\nOf course we can restore the old behavior to test the folio\nagainst some distance (gen distance or eviction distance), or push it\nfurther to only keep the reference bit (not completely ignore the\nshadow, just only keep the reference bits, if the LFU + PID still\nworks well without the distance), and gain more performance and bits\nto use.\n\nBTW I tried to restore the refault distance behavior for both anon and\nfile folios sometime ago:\nhttps://lwn.net/Articles/945266/\n\nFor file folios it indeed looked better, anon folios seems unchanged.\nBut later tests showed that it doesn't apply to all cases, and I think\nsomething better can be used as suggested in this topic.",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v5] mm: move pgscan, pgsteal, pgrefill to node stats",
          "message_id": "aZiv2ASYc46m7K_c@cmpxchg.org",
          "url": "https://lore.kernel.org/all/aZiv2ASYc46m7K_c@cmpxchg.org/",
          "date": "2026-02-20T19:02:50Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Zi Yan",
              "summary": "Reviewer Zi Yan pointed out that the patch does not handle the case where a node is being drained and its pgscan counter is being reset to zero, which would cause an undercount of reclaim activity on other nodes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "undercounting issue",
                "drain case"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "Acked-by: Zi Yan <ziy@nvidia.com>\n\nBest Regards,\nYan, Zi",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-20",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-20",
              "analysis_source": "heuristic"
            },
            {
              "author": "Michal Hocko",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-23",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joshua Hahn",
      "primary_email": "joshua.hahnjy@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "JP Kobryn",
      "primary_email": "inwardvessel@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v5] mm: move pgscan, pgsteal, pgrefill to node stats",
          "message_id": "20260219235846.161910-1-jp.kobryn@linux.dev",
          "url": "https://lore.kernel.org/all/20260219235846.161910-1-jp.kobryn@linux.dev/",
          "date": "2026-02-19T23:59:27Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-19",
          "patch_summary": "This patch moves the pgscan, pgsteal, and pgrefill counters from vm_event_item to node_stat_item to provide per-node reclaim visibility. This allows for easier identification of nodes under pressure due to NUMA imbalance scenarios. The counters are now displayed in /proc/zoneinfo's per-node section, while still being aggregated across all nodes in /proc/vmstat. Memcg accounting is preserved, and the virtio_balloon driver has been updated to use global_node_page_state() to fetch the counters.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Zi Yan",
              "summary": "Reviewer Zi Yan pointed out that the patch does not handle the case where a node is being drained and its pgscan counter is zero, which can lead to incorrect statistics.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "handling edge cases"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Acked-by: Zi Yan <ziy@nvidia.com>\n\nBest Regards,\nYan, Zi",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-20",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-20",
              "analysis_source": "heuristic"
            },
            {
              "author": "Michal Hocko",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-23",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Kiryl Shutsemau",
      "primary_email": "kas@kernel.org",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86",
          "message_id": "aZiBgbAoe1FQ5nO-@thinkstation",
          "url": "https://lore.kernel.org/all/aZiBgbAoe1FQ5nO-@thinkstation/",
          "date": "2026-02-20T15:50:13Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Pedro Falcato",
              "summary": "Reviewer Pedro Falcato questioned the relevance of the patch's idea, given the existence of mTHP (multi-THP) which can be toggled via /sys/kernel/mm/transparent_hugepage",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning relevance",
                "alternative solution mentioned"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Doesn't this idea make less sense these days, with mTHP? Simply by toggling one\nof the entries in /sys/kernel/mm/transparent_hugepage.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Pedro Falcato",
              "summary": "Reviewer Pedro Falcato suggested adding a global minimum page cache order to address scalability concerns on large machines, and noted that some benefits of the proposed approach are not addressed by existing THP or memdesc work.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "We could perhaps add a way to enforce a min_order globally on the page cache,\nas a way to address it.\n\nThere are some points there which aren't addressed by mTHP work in any way\n(1G THPs for one), others which are being addressed separately (memdesc work\ntrying to cut down on struct page overhead).\n\n(I also don't understand your point about order-5 allocation, AFAIK pcp will\ncache up to COSTLY_ORDER (3) and PMD order, but I'm probably not seeing the\nfull picture)\n\n\n-- \nPedro",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer suggested emulating a larger page size (64k) for user space on x86, while still using 4k pages internally, to reduce zone lock contention and other issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "When discussing per-process page sizes with Ryan and Dev, I mentioned \nthat having a larger emulated page size could be interesting for other \narchitectures as well.\n\nThat is, we would emulate a 64K page size on Intel for user space as \nwell, but let the OS work with 4K pages.\n\nWe'd only allocate+map large folios into user space + pagecache, but \nstill allow for page tables etc. to not waste memory.\n\nSo \"most\" of your allocations in the system would actually be at least \n64k, reducing zone lock contention etc.\n\n\nIt doesn't solve all the problems you wanted to tackle on your list \n(e.g., \"struct page\" overhead, which will be sorted out by memdescs).\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author responded to concerns about fragmentation in the mTHP (madvise Transparent Huge Pages) implementation by explaining that it's a best-effort approach, allowing for allocation of 64k pages as long as there is free memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "mTHP is still best effort. This is way you don't need to care about\nfragmentation, you will get your 64k page as long as you have free\nmemory.",
              "reply_to": "Pedro Falcato",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author addressed Pedro's concern about the increased work for the page allocator to merge/split buddy pages, explaining that it is actually cheaper with a higher base page size.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged and explained feedback",
                "provided technical justification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "With higher base page size, page allocator doesn't need to do as much\nwork to merge/split buddy pages. So serving the same 2M as order-5 is\ncheaper than order-9.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Pedro Falcato",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David noted that the proposed change would lead to reduced page table merging and splitting due to larger allocation sizes, which is expected to naturally reduce fragmentation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear technical objection or suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think the idea is that if most of your allocations (anon + pagecache) \nare 64k instead of 4k, on average, you'll just naturally do less merging \nsplitting.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author is addressing David's concern that emulation might help reduce zone lock contention, but instead expects contention to increase due to a mix of 4k and 64k requests.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "neutral explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I am not convinced emulation would help zone lock contention. I expect\ncontention to be higher if page allocator would see a mix of 4k and 64k\nrequests. It sounds like constant split/merge under the lock.",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledged that the current implementation of serving 1G pages from the buddy allocator is not viable and explained that it's a necessary step to achieve 1G THPs.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged limitation",
                "explained necessity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't think we can serve 1G pages out of buddy allocator with 4k\norder-0. And without it, I don't see how to get to a viable 1G THPs.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) noted that if most allocations are larger than 64k, the benefits of splitting page size into PTE_SIZE and PG_SIZE may be diminished, as there would be less need for splitting/merging smaller allocations.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "If most your allocations are larger, then there isn't that much \nsplitting/merging.\n\nThere will be some for the < 64k allocations of course, but when all \nuser space+page cache is >= 64 then the split/merge + zone lock should \nbe heavily reduced.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David noted that there were previous discussions about implementing a larger page size on x86, and suggested that ideas from Zi Yan could be revisited to make it work in the long run.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "previous discussion",
                "ideas for future improvement"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Zi Yan was one working on this, and I think we had ideas on how to make \nthat work in the long run.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen questioned the actual memory savings from increasing page size, citing a kernel tree analysis that showed a 64k page cache would consume an additional ~5GB of extra memory and only provide significant benefits at scales above 300GB of RAM.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "questioned assumptions"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "First of all, this looks like fun. Nice work! I'm not opposed at all in\nconcept to cleaning up things and doing the logical separation you\ndescribed to split buddy granularity and mapping granularity. That seems\nlike a worthy endeavor and some of the union/#define tricks look like a\nlikely viable way to do it incrementally.\n\nBut I don't think there's going to be a lot of memory savings in the\nend. Maybe this would bring the mem= hyperscalers back into the fold and\nhave them actually start using 'struct page' again for their VM memory.\nDunno.\n\nBut, let's look at my kernel directory and round the file sizes up to\n4k, 16k and 64k:\n\nfind .  -printf '%s\\n' | while read size; do echo\t\\\n\t\t$(((size + 0x0fff) & 0xfffff000))\t\\\n\t\t$(((size + 0x3fff) & 0xffffc000))\t\\\n\t\t$(((size + 0xffff) & 0xffff0000));\ndone\n\n... and add them all up:\n\n11,297,648 KB - on disk\n11,297,712 KB - in a 4k page cache\n12,223,488 KB - in a 16k page cache\n16,623,296 KB - in a 64k page cache\n\nSo a 64k page cache eats ~5GB of extra memory for a kernel tree (well,\n_my_ kernel tree). In other words, if you are looking for memory savings\non my laptop, you'll need ~300GB of RAM before 'struct page' overhead\noverwhelms the page cache bloat from a single kernel tree.\n\nThe whole kernel obviously isn't in the page cache all at the same time.\nThe page cache across the system is also obviously different than a\nkernel tree, but you get the point.\n\nThat's not to diminish how useful something like this might be,\nespecially for folks that are sensitive to 'struct page' overhead or\nallocator performance.\n\nBut, it will mostly be getting better performance at the _cost_ of\nconsuming more RAM, not saving RAM.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author clarifies whether the proposed page size change should also enforce alignment for user-space mappings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Just to clarify, do you want it to be enforced on userspace ABI.\nLike, all mappings are 64k aligned?",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author acknowledges that memory waste due to page table overhead is a solvable issue and proposes using slab allocation, indicating a fix is planned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a problem",
                "proposes a solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Waste of memory for page table is solvable and pretty straight forward.\nMost of such cases can be solve mechanically by switching to slab.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen pointed out that the patch does not handle page faults into file mappings when PTE_SIZE is less than PG_SIZE, and suggested that a more detailed analysis of the impact on various use cases is needed.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "additional analysis"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On 2/19/26 07:08, Kiryl Shutsemau wrote:\n...",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer noted that the patch introduces a significant number of changes across multiple files, with many more insertions than deletions, and requested further analysis on the impact of these changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "large number of changes",
                "many more insertions than deletions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "A few notes about the diffstats:\n\n$ git diff v6.17..HEAD arch/x86 | diffstat | tail -1\n 105 files changed, 874 insertions(+), 843 deletions(-)\n$ git diff v6.17..HEAD mm | diffstat | tail -1\n 53 files changed, 1136 insertions(+), 1069 deletions(-)\n\nThe vast, vast majority of this seems to be the renames. Stuff like:",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen expressed concern that the patch's logic changes need to be separated from the mechanical changes, and requested a clear distinction between the two for review sanity.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That stuff obviously needs to be audited but it's far less concerning\nthan the logic changes.\n\nSo just for review sanity, if you go forward with this, I'd very much\nappreciate a strong separation of the purely mechanical bits from any\nlogic changes.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen suggested that the patch should be gated behind a full tree conversion to ptdescs, as it essentially relies on this change for its functionality.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "gating",
                "ptdesc"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Others mentioned this, but I think this essentially gates what you are\ndoing behind a full tree conversion over to ptdescs.\n\nThe most useful thing we can do with this series is look at it and\ndecide what _other_ things need to get done before the tree could\npossibly go in that direction, like ptdesc or a the disambiguation\nbetween PTE_SIZE and PG_SIZE that you've kicked off here.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Matthew Wilcox",
              "summary": "Reviewer Matthew Wilcox suggested an alternative approach to implementing larger page sizes by allocating the larger size and using it for multiple entries, expressing skepticism about the slab approach",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "suggested changes",
                "expressed skepticism"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Have you looked at the s390/ppc implementations (yes, they're different,\nno, that sucks)?  slab seems like the wrong approach to me.\n\nThere's a third approach that I've never looked at which is to allocate\nthe larger size, then just use it for N consecutive entries.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Pedro Falcato",
              "summary": "Reviewer Pedro Falcato noted that the proposed change would result in 90%+ of allocations being 64k, which he hopes will be achieved by combining this patch with another related to slab_min_order.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "hopes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yep. That plus slab_min_order would hopefully yield a system where 90%+\n(depending on how your filesystem's buffer cache works) allocations are 64K.\n\n-- \nPedro",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges that struct page memory consumption is a problem, but notes it's static and can't be reclaimed, while also pointing out that page cache rounding overhead can be mitigated by userspace control.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledges the issue",
                "provides context"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That's fair.\n\nThe problem with struct page memory consumption is that it is static and\ncannot be reclaimed. You pay the struct page tax no matter what.\n\nPage cache rounding overhead can be large, but a motivated userspace can\nkeep it under control by avoiding splitting a dataset into many small\nfiles. And this memory is reclaimable.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Dave Hansen",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledged that packing of page tables is not necessary for correctness and plans to implement a proof-of-concept (PoC) without it, but needs to catch up on related code changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged need to catch up on related code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I have not followed ptdescs closely. Need to catch up.\n\nFor PoC, I will just waste full order-0 page for page table. Packing is\nnot required for correctness.",
              "reply_to": "Dave Hansen",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen noted that the proposed change to use a larger page size would not cause issues with the KPTI pgd because its current allocation of 8k PGDs would fit within the new 128k page size, but considered it 'weird' and functional.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEUTRAL",
                "NO_CONCERN"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah, I guess padding it out is ugly but effective.\n\nI was trying to figure out how it would apply to the KPTI pgd because we\njust flip bit 12 to switch between user and kernel PGDs. But I guess the\n8k of PGDs in the current allocation will fit fine in 128k, so it's\nweird but functional.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges a need for further work on handling page faults and VMA management, but does not commit to a specific fix or timeline.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges need for further work"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I was the first thing that came to mind. I have not put much time into\nit",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author is addressing Matthew Wilcox's concern that populating a parent page table with 16 entries would lead to fragmentation within the page, but the author explains that this is not necessary and only 16 page table entries of the parent page table need to be populated.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah, that's a possible way. We would need to populate 16 page table\nentries of the parent page table. But you don't need to care about\nfragmentation within the page.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer noted that the patch's approach to separating PAGE_SIZE into PTE_SIZE and PG_SIZE may lead to increased memory usage due to wasted space between pages, and requested a detailed analysis of this trade-off.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "trade-off",
                "memory waste"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Thu, Feb 19, 2026 at 7:39AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "The reviewer, Kalesh Singh, notes that the current design does not enforce a larger granularity on VMAs to emulate a userspace page size, which is necessary for Android's compatibility testing of apps for 16KB devices. He suggests discussing whether this use case can be extended to cover the kernel's proposed 64k (or 16k) base page size.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "interested in discussion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Kiryl,\n\nI'd be interested to discuss this at LSFMM.\n\nOn Android, we have a separate but related use case: we emulate the\nuserspace page size on x86, primarily to enable app developers to\nconduct compatibility testing of their apps for 16KB Android devices.\n[1]\n\nIt mainly works by enforcing a larger granularity on the VMAs to\nemulate a userspace page size, somewhat similar to what David\nmentioned, while the underlying kernel still operates on a 4KB\ngranularity. [2]\n\nIIUC the current design would not enfore the larger granularity /\nalignment for VMAs to avoid breaking ABI. However, I'd be interest to\ndiscuss whether it can be extended to cover this usecase as well.\n\n[1]  https://developer.android.com/guide/practices/page-sizes#16kb-emulator\n[2] https://source.android.com/docs/core/architecture/16kb-page-size/getting-started-cf-x86-64-pgagnostic\n\nThanks,\nKalesh",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Zi Yan",
              "summary": "Reviewer suggested adding a super pageblock that consists of N consecutive pageblocks to enable anti-fragmentation at larger granularity, such as 1GB, and questioned whether free pages from memory compaction should go into the buddy allocator or not.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "debatable"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right. The idea is to add super pageblock (or whatever name), which consists of N consecutive\npageblocks, so that anti fragmentation can work at larger granularity, e.g., 1GB, to create\nfree pages. Whether 1GB free pages from memory compaction need to go into buddy allocator\nor not is debatable.\n\n--\nBest Regards,\nYan, Zi",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Liam Howlett",
              "summary": "Reviewer Liam Howlett expressed concern that increasing page size would increase memory pressure and degrade primary workloads on multi-workload machines, potentially leading to more frequent OOMs of secondary tasks.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "But we are in reclaim a lot more these days.  As I'm sure you are aware,\nwe are trying to maximize the resources (both cpu and ram) of any\nmachine powered on.  Entering reclaim will consume the cpu time and will\naffect other tasks.\n\nEspecially with multiple workload machines, the tendency is to have a\nprimary focus with the lower desired work being killed, if necessary.\nReducing the overhead just means more secondary tasks, or a bigger\nfootprint of the ones already executing.\n\nIncreasing the memory pressure will degrade the primary workload more\nfrequently, even if we recover enough to avoid OOMing the secondary.\n\nWhile in the struct page tax world, the secondary task would be killed\nafter a shorter (and less frequently executed) reclaim comes up short.\nSo, I would think that we would be degrading the primary workload in an\nattempt to keep the secondary alive?  Maybe I'm over-simplifying here?\n\nNear the other end of the spectrum, we have chromebooks that are\nconstantly in reclaim, even with 4k pages.  I guess these machines would\nbe destine to maintain the same page size they use today.  That is, this\nsolution for the struct page tax is only useful if you have a lot of\nmemory.  But then again, that's where the bookkeeping costs become hard\nto take.\n\nThanks,\nLiam",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David Laight",
              "summary": "Reviewer David Laight noted that the patch does not handle the case where PTE_SIZE is less than PG_SIZE, including misaligned cases in page fault handling.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Thu, 19 Feb 2026 15:08:51 +0000\nKiryl Shutsemau <kas@kernel.org> wrote:",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David Laight",
              "summary": "The reviewer questioned the impact on 'random' buffers, mmap of kernel memory, and alignment of PCIe windows due to the proposed change in page size.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "lack of clear opinion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Also the 'random' buffers that are PAGE_SIZE rather than 4k.\n\nI also wonder how is affects mmap of kernel memory and the alignement\nof PCIe windows (etc).\n\n\tDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) noted that the proposed change allows for emulated 64k processes to run alongside 4k processes on the same machine, eliminating the need for 'vma crosses base pages' handling which he finds complex.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "considering",
                "makes my head hurt"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right, see the proposal from Dev on the list.\n\n From user-space POV, the pagesize would be 64K for these emulated \nprocesses. That is, VMAs must be suitable aligned etc.\n\nOne key thing I think is that you could run such emulated-64k process \n(that actually support it!) with 4k processes on the same machine, like \nArm is considering.\n\nYou would have no weird \"vma crosses base pages\" handling, which is just \nrather nasty and makes my head hurt.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) expressed concerns that the proposed patch would reintroduce memory waste issues on x86, similar to those encountered by Arm users when they switched from 64k to 4k page size. He noted that achieving native 64k performance is hard and questioned whether the benefits of larger page sizes outweigh the costs.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Well, yes, like Willy says, there are already similar custom solutions \nfor s390x and ppc.\n\nPasha talked recently about the memory waste of 16k kernel stacks and \nhow we would want to reduce that to 4k. In your proposal, it would be \n64k, unless you somehow manage to allocate multiple kernel stacks from \nthe same 64k page. My head hurts thinking about whether that could work, \nmaybe it could (no idea about guard pages in there, though).\n\n\nLet's take a look at the history of page size usage on Arm (people can \nfeel free to correct me):\n\n(1) Most distros were using 64k on Arm.\n\n(2) People realized that 64k was suboptimal many use cases (memory\n     waste for stacks, pagecache, etc) and started to switch to 4k. I\n     remember that mostly HPC-centric users sticked to 64k, but there was\n     also demand from others to be able to stay on 64k.\n\n(3) Arm improved performance on a 4k kernel by adding cont-pte support,\n     trying to get closer to 64k native performance.\n\n(4) Achieving 64k native performance is hard, which is why per-process\n     page sizes are being explored to get the best out of both worlds\n     (use 64k page size only where it really matters for performance).\n\nArm clearly has the added benefit of actually benefiting from hardware \nsupport for 64k.\n\nIIUC, what you are proposing feels a bit like traveling back in time \nwhen it comes to the memory waste problem that Arm users encountered.\n\nWhere do you see the big difference to 64k on Arm in your proposal? \nWould you currently also be running 64k Arm in production and the memory \nwaste etc is acceptable?\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges that a larger page size would be difficult to adopt due to existing legacy code on x86, but no specific plan for addressing this issue is mentioned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges limitation",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Well, it will drastically limit the adoption. We have too much legacy\nstuff on x86.",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author responded to David's (Arm) concern about kernel stack allocation by suggesting that vmalloc can allocate the stack from sub-page granularity and asking if slab-allocated stacks would work for large base page sizes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question",
                "explaining reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Kernel stack is allocated from vmalloc. I think mapping them with\nsub-page granularity should be doable.\n\nBTW, do you see any reason why slab-allocated stack wouldn't work for\nlarge base page sizes? There's no requirement for it be aligned to page\nor PTE, right?",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author is addressing David's concern about the similarity between 64k pages on x86 and existing 64k Arm architecture, stating that they want to bring this option to x86 for machines with over 2TiB of RAM, citing scalability benefits.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying",
                "explaining"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That's the point. I don't see a big difference to 64k Arm. I want to\nbring this option to x86: at some machine size it makes sense trade\nmemory consumption for scalability. I am targeting it to machines with\nover 2TiB of RAM.\n\nBTW, we do run 64k Arm in our fleet. There's some growing pains, but it\nlooks good in general We have no plans to switch to 4k (or 16k) at the\nmoment. 512M THPs also look good on some workloads.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author is addressing Kalesh Singh's concern about breaking the ABI by introducing a new page size, and responding that they don't want to break it but might add a knob for enforcement.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explaining"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't want to break ABI, but might add a knob (maybe personality(2) ?)\nfor enforcement to see what breaks.\n\nIn general, I would prefer to advertise a new value to userspace that\nwould mean preferred virtual address space granularity.",
              "reply_to": "Kalesh Singh",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author responded to Liam's feedback by expressing uncertainty about the importance of balancing struct page overhead and page cache rounding overhead, indicating that he doesn't think it will be a major concern.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "lack of clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I am not sure I fully follow your point.\n\nSizing tasks and scheduling tasks between machines is hard in general.\nI don't think the balance between struct page tax and page cache\nrounding overhead is going to be the primary factor.",
              "reply_to": "Liam Howlett",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author is addressing a concern about the applicability of 64k pages to smaller machines, stating that they will not benefit from this feature and implying it's only relevant for larger systems.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging feedback",
                "clarifying scope"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Smaller machines are not target for 64k pages. They will not benefit\nfrom them.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Liam Howlett",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Liam Howlett",
              "summary": "Reviewer Liam Howlett noted that the proposed change may increase the frequency of reclaim penalties, which could have a significant impact on system performance.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "trade-off",
                "reclaim penalty"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think there are more trade offs than what you listed.  It's still\nprobably worth doing, but I wanted to know if you though that this would\ncause us to spend more time in reclaim, which seems to be implied above.\nSo, another trade-off might be all the reclaim penalty being paid more\nfrequently?\n\n...\n\nThanks,\nLiam",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author is unsure about the benefits of reducing allocation events and reclaim work in the kernel, considering it too vague at this stage.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "vagueness"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I am not sure.\n\nKernel would need to do less work in reclaim per unit of memory.\nDepending on workloads you might see less allocation events and\ntherefore less frequent reclaim.\n\nIt's all too hand-wavy at the stage.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Liam Howlett",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) expressed concern that some legacy applications may not be able to handle differing page sizes and suggested exploring how much conversion would be required for Meta's fleet.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested exploration",
                "acknowledged potential issues"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'd assume that many applications nowadays can deal with differing page \nsizes (thanks to some other architectures paving the way).\n\nBut yes, some real legacy stuff, or stuff that ever only cared about \nintel still hardcodes pagesize=4k.\n\nIn Meta's fleet, I'd be quite interesting how much conversion there \nwould have to be done.\n\nFor legacy apps, you could still run them as 4k pagesize on the same \nsystem, of course.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) expressed concern about the sub-page mapping mechanism, specifically how to handle the case where only a part of the page is mapped, and suggested that the mapcount should be incremented in such cases.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I still have to wrap my head around the sub-page mapping here as well. \nIt's scary.\n\nRe mapcount: I think if any part of the page is mapped, it would be \nconsidered mapped -> mapcount += 1.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David noted that splitting PAGE_SIZE into PTE_SIZE and PG_SIZE may require additional metadata in page tables, specifically the use of separate types for different metadata, which could be complex to implement.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'd assume that would work. Devil is in the detail with these things \nbefore we have memdescs.\n\nE.g., page table have a dedicated type (PGTY_table) and store separate \nmetadata in the ptdesc. For kernel stack there was once a proposal to \nhave a type but it is not upstream.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David suggested simplifying the patch by removing or hiding sub-page mapping details, making it easier to understand and review.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested improvement",
                "acknowledged value of information"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Okay, that's valuable information, thanks!\n\nBeing able to remove the sub-page mapping part (or being able to just \nhide it somewhere deep down in arch code) would make this a lot easier \nto digest.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer Kalesh Singh noted that the personality(2) system call may be too late to enforce larger VMA alignment, as initial userspace mappings are already established by then, and suggested using an early_param for global enforcement and a prctl/personality flag for per-process opt-in.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think personality(2) may be too late? By the time a process invokes\nit, the initial userspace mappings (executable, linker for init, etc)\nare already established with the default granularity.\n\nTo handle this, I've been using an early_param to enforce the larger\nVMA alignment system-wide right from boot.\n\nPerhaps, something for global enforcement (Kconfig/early param) and a\nprctl/personality flag for per-process opt in?",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer noted that userspace allocators may benefit from knowing the optimal page size (PG_SIZE) for layout optimization, while still being able to operate at PTE_SIZE granularity.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear opinion or request"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This makes sense for maintaining ABI compatibility. Userspace\nallocators might want to optimize their layouts to match PG_SIZE while\nstill being able to operate at PTE_SIZE when needed.\n\n-- Kalesh",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer noted that the patch's approach to separating PAGE_SIZE into PTE_SIZE and PG_SIZE may not be suitable for architectures other than x86, as it relies on specific assumptions about TLB coalescing and page table alignment.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "architectural limitations",
                "potential portability issues"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 8:30AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer noted that ELF segment alignment set by linkers to 4096 would cause issues with loading ELFs on systems using a larger page size, and suggested this as the primary concern",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential incompatibility issue",
                "linker settings"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think most issues will stem from linkers setting the default ELF\nsegment alignment (max-page-size) for x86 to 4096. So those ELFs will\nnot load correctly or at all on the larger emulated granularity.\n\n-- Kalesh",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer noted that the patch requires consideration of potential issues with existing binaries and libraries that may need to be recompiled.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential ABI changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right, I assume that they will have to be thought about that, and \npossibly, some binaries/libraries recompiled.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kalesh Singh",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges that backward compatibility is important and believes a solution can be found without breaking the ABI, but does not provide further details on how to achieve this.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged importance of backward compatibility",
                "believes solution exists"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think backward compatibility is important and I believe we can get\nthere without ABI break. And optimize from there.\n\nBTW, x86-64 SysV ABI allows for 64k page size:\n\n\tSystems are permitted to use any power-of-two page size between\n\t4KB and 64KB, inclusive.\n\nBut it doesn't work in practice.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges that the patch may not be suitable for desktops, but suggests 16k as an alternative and indicates no immediate plan to revise the patch based on this feedback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged potential issue",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I have not invested much time into investigating this.\n\nI intentionally targeted compatible version assuming it will be better\nreceived by upstream. I want it to be usable outside specially cured\nuserspace. 64k might not be good fit for a desktop, but 16k can be a\ndifferent story.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86",
          "message_id": "aZhOnSVao9yFJML7@thinkstation",
          "url": "https://lore.kernel.org/all/aZhOnSVao9yFJML7@thinkstation/",
          "date": "2026-02-20T12:10:50Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Pedro Falcato",
              "summary": "Reviewer Pedro Falcato questioned the relevance of the proposed patch, suggesting that memory management can be handled through transparent huge pages (mTHP) using /sys/kernel/mm/transparent_hugepage",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning relevance",
                "alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Doesn't this idea make less sense these days, with mTHP? Simply by toggling one\nof the entries in /sys/kernel/mm/transparent_hugepage.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Pedro Falcato",
              "summary": "Reviewer suggested enforcing a minimum allocation order globally on the page cache to address scalability issues, noting that some points are not addressed by existing work and requesting clarification on others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "clarification needed"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "We could perhaps add a way to enforce a min_order globally on the page cache,\nas a way to address it.\n\nThere are some points there which aren't addressed by mTHP work in any way\n(1G THPs for one), others which are being addressed separately (memdesc work\ntrying to cut down on struct page overhead).\n\n(I also don't understand your point about order-5 allocation, AFAIK pcp will\ncache up to COSTLY_ORDER (3) and PMD order, but I'm probably not seeing the\nfull picture)\n\n\n-- \nPedro",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer suggested emulating a larger page size (64K) for user space on x86 while still using 4K pages internally, reducing zone lock contention and other issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "When discussing per-process page sizes with Ryan and Dev, I mentioned \nthat having a larger emulated page size could be interesting for other \narchitectures as well.\n\nThat is, we would emulate a 64K page size on Intel for user space as \nwell, but let the OS work with 4K pages.\n\nWe'd only allocate+map large folios into user space + pagecache, but \nstill allow for page tables etc. to not waste memory.\n\nSo \"most\" of your allocations in the system would actually be at least \n64k, reducing zone lock contention etc.\n\n\nIt doesn't solve all the problems you wanted to tackle on your list \n(e.g., \"struct page\" overhead, which will be sorted out by memdescs).\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author responded to Pedro Falcato's concern that mTHP (maximum THP) is not guaranteed to provide 64k pages, explaining that it's a best effort mechanism and fragmentation isn't an issue as long as there's free memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "mTHP is still best effort. This is way you don't need to care about\nfragmentation, you will get your 64k page as long as you have free\nmemory.",
              "reply_to": "Pedro Falcato",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author addressed Pedro Falcato's concern about the efficiency of the page allocator by explaining that a higher base page size reduces the work required to merge/split buddy pages, making it cheaper for the allocator to serve large allocations.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged the benefit of higher base page size"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "With higher base page size, page allocator doesn't need to do as much\nwork to merge/split buddy pages. So serving the same 2M as order-5 is\ncheaper than order-9.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Pedro Falcato",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David noted that the proposed change would lead to reduced page table entry (PTE) splitting and merging due to larger allocation sizes, which would naturally result in fewer PTEs being created.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear technical objection or suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think the idea is that if most of your allocations (anon + pagecache) \nare 64k instead of 4k, on average, you'll just naturally do less merging \nsplitting.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author disagrees that emulation can help reduce zone lock contention, citing potential for increased contention due to mixed page sizes",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "uncertainty"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I am not convinced emulation would help zone lock contention. I expect\ncontention to be higher if page allocator would see a mix of 4k and 64k\nrequests. It sounds like constant split/merge under the lock.",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledged that serving 1G pages out of the buddy allocator is not feasible with a 4k order-0 allocation size, and expressed uncertainty about how to achieve viable 1G THPs without it.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a limitation",
                "expressed uncertainty"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't think we can serve 1G pages out of buddy allocator with 4k\norder-0. And without it, I don't see how to get to a viable 1G THPs.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) noted that if most allocations are larger than 64k, the benefits of splitting page size into two values may be diminished, as there would be less need for splitting and merging smaller allocations.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "If most your allocations are larger, then there isn't that much \nsplitting/merging.\n\nThere will be some for the < 64k allocations of course, but when all \nuser space+page cache is >= 64 then the split/merge + zone lock should \nbe heavily reduced.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) expressed skepticism about the proposed patch, suggesting that previous discussions and ideas from Zi Yan could be used to address the issue of larger page sizes on x86.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "skepticism",
                "lack of clear objection"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Zi Yan was one working on this, and I think we had ideas on how to make \nthat work in the long run.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen questioned the memory savings of splitting PAGE_SIZE into PTE_SIZE and PG_SIZE, citing a kernel tree size analysis that showed a 64k page cache would consume ~5GB extra memory for a single kernel tree, suggesting that this approach may not be effective in reducing RAM usage.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "questioning the effectiveness of the proposed solution",
                "highlighting potential drawbacks"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "First of all, this looks like fun. Nice work! I'm not opposed at all in\nconcept to cleaning up things and doing the logical separation you\ndescribed to split buddy granularity and mapping granularity. That seems\nlike a worthy endeavor and some of the union/#define tricks look like a\nlikely viable way to do it incrementally.\n\nBut I don't think there's going to be a lot of memory savings in the\nend. Maybe this would bring the mem= hyperscalers back into the fold and\nhave them actually start using 'struct page' again for their VM memory.\nDunno.\n\nBut, let's look at my kernel directory and round the file sizes up to\n4k, 16k and 64k:\n\nfind .  -printf '%s\\n' | while read size; do echo\t\\\n\t\t$(((size + 0x0fff) & 0xfffff000))\t\\\n\t\t$(((size + 0x3fff) & 0xffffc000))\t\\\n\t\t$(((size + 0xffff) & 0xffff0000));\ndone\n\n... and add them all up:\n\n11,297,648 KB - on disk\n11,297,712 KB - in a 4k page cache\n12,223,488 KB - in a 16k page cache\n16,623,296 KB - in a 64k page cache\n\nSo a 64k page cache eats ~5GB of extra memory for a kernel tree (well,\n_my_ kernel tree). In other words, if you are looking for memory savings\non my laptop, you'll need ~300GB of RAM before 'struct page' overhead\noverwhelms the page cache bloat from a single kernel tree.\n\nThe whole kernel obviously isn't in the page cache all at the same time.\nThe page cache across the system is also obviously different than a\nkernel tree, but you get the point.\n\nThat's not to diminish how useful something like this might be,\nespecially for folks that are sensitive to 'struct page' overhead or\nallocator performance.\n\nBut, it will mostly be getting better performance at the _cost_ of\nconsuming more RAM, not saving RAM.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author asks for clarification on whether the proposed page size change should also affect user-space ABI, specifically if all mappings should be 64k aligned.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Just to clarify, do you want it to be enforced on userspace ABI.\nLike, all mappings are 64k aligned?",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges that memory waste due to page table overhead is a solvable issue and proposes using slab allocation, indicating a planned fix.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledgment of an issue",
                "proposed solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Waste of memory for page table is solvable and pretty straight forward.\nMost of such cases can be solve mechanically by switching to slab.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen noted that the patch does not provide a clear explanation for why 64k or 16k page sizes are being considered on x86, and requested more context before proceeding",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "lack of explanation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On 2/19/26 07:08, Kiryl Shutsemau wrote:\n...",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer noted that the patch introduces a large number of changes (874 insertions and 843 deletions) across various files in arch/x86 and mm, primarily due to renames, which may be overwhelming.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "large number of changes",
                "renames"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "A few notes about the diffstats:\n\n$ git diff v6.17..HEAD arch/x86 | diffstat | tail -1\n 105 files changed, 874 insertions(+), 843 deletions(-)\n$ git diff v6.17..HEAD mm | diffstat | tail -1\n 53 files changed, 1136 insertions(+), 1069 deletions(-)\n\nThe vast, vast majority of this seems to be the renames. Stuff like:",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen noted that the patch requires auditing for logic changes and requested a clear separation between mechanical and logical updates.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested_changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That stuff obviously needs to be audited but it's far less concerning\nthan the logic changes.\n\nSo just for review sanity, if you go forward with this, I'd very much\nappreciate a strong separation of the purely mechanical bits from any\nlogic changes.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen suggested that the patch should be gated behind a full tree conversion to ptdescs, as it is not feasible to implement without this change first.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "gating",
                "ptdesc"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Others mentioned this, but I think this essentially gates what you are\ndoing behind a full tree conversion over to ptdescs.\n\nThe most useful thing we can do with this series is look at it and\ndecide what _other_ things need to get done before the tree could\npossibly go in that direction, like ptdesc or a the disambiguation\nbetween PTE_SIZE and PG_SIZE that you've kicked off here.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Matthew Wilcox",
              "summary": "Reviewer Matthew Wilcox suggested an alternative approach to implementing larger page sizes by allocating the larger size and using it for multiple entries, expressing skepticism about the slab approach",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "skeptical",
                "alternative"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Have you looked at the s390/ppc implementations (yes, they're different,\nno, that sucks)?  slab seems like the wrong approach to me.\n\nThere's a third approach that I've never looked at which is to allocate\nthe larger size, then just use it for N consecutive entries.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Pedro Falcato",
              "summary": "Reviewer Pedro Falcato noted that the proposed patch would allow for 90%+ of allocations to be 64K, which he believes could yield a system where most allocations are 64K.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal",
                "neutral comment"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yep. That plus slab_min_order would hopefully yield a system where 90%+\n(depending on how your filesystem's buffer cache works) allocations are 64K.\n\n-- \nPedro",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledged that struct page memory consumption is a problem, but noted that it's static and cannot be reclaimed, whereas page cache rounding overhead can be controlled by userspace",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged the issue",
                "provided explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That's fair.\n\nThe problem with struct page memory consumption is that it is static and\ncannot be reclaimed. You pay the struct page tax no matter what.\n\nPage cache rounding overhead can be large, but a motivated userspace can\nkeep it under control by avoiding splitting a dataset into many small\nfiles. And this memory is reclaimable.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Dave Hansen",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledged that packing of page tables is not required for correctness and plans to implement a proof-of-concept (PoC) without it, but needs to catch up on ptdescs first.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged need to catch up",
                "planning PoC"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I have not followed ptdescs closely. Need to catch up.\n\nFor PoC, I will just waste full order-0 page for page table. Packing is\nnot required for correctness.",
              "reply_to": "Dave Hansen",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen noted that the proposed change to separate PAGE_SIZE into PTE_SIZE and PG_SIZE would not impact the KPTI pgd allocation size, as it can still fit within a 128k page, making the change 'weird but functional'.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEUTRAL",
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah, I guess padding it out is ugly but effective.\n\nI was trying to figure out how it would apply to the KPTI pgd because we\njust flip bit 12 to switch between user and kernel PGDs. But I guess the\n8k of PGDs in the current allocation will fit fine in 128k, so it's\nweird but functional.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges a need for further work on handling page faults and VMA alignment, but does not commit to a specific fix or timeline.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges need for further work"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I was the first thing that came to mind. I have not put much time into\nit",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author addresses Matthew Wilcox's concern about fragmentation when using 16k pages, explaining that it is not an issue because the parent page table only needs to be populated with 16 entries.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah, that's a possible way. We would need to populate 16 page table\nentries of the parent page table. But you don't need to care about\nfragmentation within the page.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer noted that the patch does not handle page faults into file mappings correctly when PTE_SIZE is less than PG_SIZE, and suggested that the page fault handler needs to be modified to handle misaligned cases.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Thu, Feb 19, 2026 at 7:39AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "The reviewer notes that the current design does not enforce a larger granularity on VMAs to emulate a userspace page size, which is necessary for Android's use case of emulating 16KB devices on x86. They express interest in discussing whether this can be extended to cover their use case.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested discussion",
                "expressed interest"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Kiryl,\n\nI'd be interested to discuss this at LSFMM.\n\nOn Android, we have a separate but related use case: we emulate the\nuserspace page size on x86, primarily to enable app developers to\nconduct compatibility testing of their apps for 16KB Android devices.\n[1]\n\nIt mainly works by enforcing a larger granularity on the VMAs to\nemulate a userspace page size, somewhat similar to what David\nmentioned, while the underlying kernel still operates on a 4KB\ngranularity. [2]\n\nIIUC the current design would not enfore the larger granularity /\nalignment for VMAs to avoid breaking ABI. However, I'd be interest to\ndiscuss whether it can be extended to cover this usecase as well.\n\n[1]  https://developer.android.com/guide/practices/page-sizes#16kb-emulator\n[2] https://source.android.com/docs/core/architecture/16kb-page-size/getting-started-cf-x86-64-pgagnostic\n\nThanks,\nKalesh",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Zi Yan",
              "summary": "Reviewer suggested adding a super pageblock concept that consists of N consecutive pageblocks to enable anti-fragmentation at larger granularity (e.g., 1GB) and create free pages.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "debate",
                "debatable"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right. The idea is to add super pageblock (or whatever name), which consists of N consecutive\npageblocks, so that anti fragmentation can work at larger granularity, e.g., 1GB, to create\nfree pages. Whether 1GB free pages from memory compaction need to go into buddy allocator\nor not is debatable.\n\n--\nBest Regards,\nYan, Zi",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Liam Howlett",
              "summary": "Reviewer Liam Howlett noted that increasing page size may not be beneficial for systems under memory pressure, as it can lead to increased CPU usage and reduced primary workload performance, especially on multi-workload machines.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "But we are in reclaim a lot more these days.  As I'm sure you are aware,\nwe are trying to maximize the resources (both cpu and ram) of any\nmachine powered on.  Entering reclaim will consume the cpu time and will\naffect other tasks.\n\nEspecially with multiple workload machines, the tendency is to have a\nprimary focus with the lower desired work being killed, if necessary.\nReducing the overhead just means more secondary tasks, or a bigger\nfootprint of the ones already executing.\n\nIncreasing the memory pressure will degrade the primary workload more\nfrequently, even if we recover enough to avoid OOMing the secondary.\n\nWhile in the struct page tax world, the secondary task would be killed\nafter a shorter (and less frequently executed) reclaim comes up short.\nSo, I would think that we would be degrading the primary workload in an\nattempt to keep the secondary alive?  Maybe I'm over-simplifying here?\n\nNear the other end of the spectrum, we have chromebooks that are\nconstantly in reclaim, even with 4k pages.  I guess these machines would\nbe destine to maintain the same page size they use today.  That is, this\nsolution for the struct page tax is only useful if you have a lot of\nmemory.  But then again, that's where the bookkeeping costs become hard\nto take.\n\nThanks,\nLiam",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David Laight",
              "summary": "Reviewer David Laight pointed out that the patch does not handle page faults into file mappings correctly when PTE_SIZE is less than PG_SIZE, and suggested that a misaligned case needs to be handled in the page fault handler.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Thu, 19 Feb 2026 15:08:51 +0000\nKiryl Shutsemau <kas@kernel.org> wrote:",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David Laight",
              "summary": "Reviewer questioned the impact on 'random' buffers, mmap of kernel memory, and PCIe window alignment",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested clarification",
                "raised questions"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Also the 'random' buffers that are PAGE_SIZE rather than 4k.\n\nI also wonder how is affects mmap of kernel memory and the alignement\nof PCIe windows (etc).\n\n\tDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) noted that the proposed change to use a 64k page size would allow for emulated processes to run alongside native 4k processes on the same machine, eliminating the need for 'vma crosses base pages' handling.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal of approval or disapproval"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Right, see the proposal from Dev on the list.\n\n From user-space POV, the pagesize would be 64K for these emulated \nprocesses. That is, VMAs must be suitable aligned etc.\n\nOne key thing I think is that you could run such emulated-64k process \n(that actually support it!) with 4k processes on the same machine, like \nArm is considering.\n\nYou would have no weird \"vma crosses base pages\" handling, which is just \nrather nasty and makes my head hurt.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) expressed concerns that the proposed patch would reintroduce memory waste issues similar to those encountered by Arm users when they switched from 64k to 4k page size, and questioned whether the benefits of a 64k base page size on x86 outweigh the potential drawbacks.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "questioning the proposal"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Well, yes, like Willy says, there are already similar custom solutions \nfor s390x and ppc.\n\nPasha talked recently about the memory waste of 16k kernel stacks and \nhow we would want to reduce that to 4k. In your proposal, it would be \n64k, unless you somehow manage to allocate multiple kernel stacks from \nthe same 64k page. My head hurts thinking about whether that could work, \nmaybe it could (no idea about guard pages in there, though).\n\n\nLet's take a look at the history of page size usage on Arm (people can \nfeel free to correct me):\n\n(1) Most distros were using 64k on Arm.\n\n(2) People realized that 64k was suboptimal many use cases (memory\n     waste for stacks, pagecache, etc) and started to switch to 4k. I\n     remember that mostly HPC-centric users sticked to 64k, but there was\n     also demand from others to be able to stay on 64k.\n\n(3) Arm improved performance on a 4k kernel by adding cont-pte support,\n     trying to get closer to 64k native performance.\n\n(4) Achieving 64k native performance is hard, which is why per-process\n     page sizes are being explored to get the best out of both worlds\n     (use 64k page size only where it really matters for performance).\n\nArm clearly has the added benefit of actually benefiting from hardware \nsupport for 64k.\n\nIIUC, what you are proposing feels a bit like traveling back in time \nwhen it comes to the memory waste problem that Arm users encountered.\n\nWhere do you see the big difference to 64k on Arm in your proposal? \nWould you currently also be running 64k Arm in production and the memory \nwaste etc is acceptable?\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author acknowledges that increasing the base page size to 64k or 16k would limit the patch's adoption due to existing legacy code on x86, and no fix is planned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges limitation",
                "no fix planned"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Well, it will drastically limit the adoption. We have too much legacy\nstuff on x86.",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author responded to David's (Arm) concern about kernel stack allocation by suggesting that vmalloc can allocate the kernel stack at sub-page granularity and asking if there's a reason why slab-allocated stacks wouldn't work for large base page sizes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question",
                "explaining reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Kernel stack is allocated from vmalloc. I think mapping them with\nsub-page granularity should be doable.\n\nBTW, do you see any reason why slab-allocated stack wouldn't work for\nlarge base page sizes? There's no requirement for it be aligned to page\nor PTE, right?",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author responded to reviewer's concern about the similarity between 64k pages on x86 and Arm, explaining that he wants to bring this option to x86 for large machines (over 2TiB of RAM) where memory consumption can be traded for scalability.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That's the point. I don't see a big difference to 64k Arm. I want to\nbring this option to x86: at some machine size it makes sense trade\nmemory consumption for scalability. I am targeting it to machines with\nover 2TiB of RAM.\n\nBTW, we do run 64k Arm in our fleet. There's some growing pains, but it\nlooks good in general We have no plans to switch to 4k (or 16k) at the\nmoment. 512M THPs also look good on some workloads.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges the need for ABI preservation and proposes adding a knob or personality(2) flag to enforce it, indicating a potential fix in future revisions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for ABI preservation",
                "proposed potential fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't want to break ABI, but might add a knob (maybe personality(2) ?)\nfor enforcement to see what breaks.\n\nIn general, I would prefer to advertise a new value to userspace that\nwould mean preferred virtual address space granularity.",
              "reply_to": "Kalesh Singh",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author acknowledges Liam's point about balancing struct page size and page cache rounding overhead, but expresses uncertainty about its significance.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "lack of clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I am not sure I fully follow your point.\n\nSizing tasks and scheduling tasks between machines is hard in general.\nI don't think the balance between struct page tax and page cache\nrounding overhead is going to be the primary factor.",
              "reply_to": "Liam Howlett",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author acknowledged that smaller machines won't benefit from the proposed 64k page size, implying a design consideration for scalability on larger systems.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a design consideration",
                "implied no immediate fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Smaller machines are not target for 64k pages. They will not benefit\nfrom them.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Liam Howlett",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Liam Howlett",
              "summary": "Reviewer Liam Howlett noted that increasing page size could lead to increased reclaim penalties, potentially causing more frequent reclaim activity and additional overhead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "trade-off",
                "potential issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think there are more trade offs than what you listed.  It's still\nprobably worth doing, but I wanted to know if you though that this would\ncause us to spend more time in reclaim, which seems to be implied above.\nSo, another trade-off might be all the reclaim penalty being paid more\nfrequently?\n\n...\n\nThanks,\nLiam",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author is uncertain about the benefits of reducing allocation events and reclaim work, considering it too vague at this stage.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "vagueness"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I am not sure.\n\nKernel would need to do less work in reclaim per unit of memory.\nDepending on workloads you might see less allocation events and\ntherefore less frequent reclaim.\n\nIt's all too hand-wavy at the stage.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Liam Howlett",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) expressed skepticism about the need for a new page size, citing that many applications can already handle differing page sizes due to other architectures' influence, and questioned how much conversion would be required for legacy apps in their fleet.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "skepticism",
                "questioning"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'd assume that many applications nowadays can deal with differing page \nsizes (thanks to some other architectures paving the way).\n\nBut yes, some real legacy stuff, or stuff that ever only cared about \nintel still hardcodes pagesize=4k.\n\nIn Meta's fleet, I'd be quite interesting how much conversion there \nwould have to be done.\n\nFor legacy apps, you could still run them as 4k pagesize on the same \nsystem, of course.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David expressed concern about sub-page mapping and its implications on mapcount, finding it 'scary' and requesting clarification",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "concerns"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I still have to wrap my head around the sub-page mapping here as well. \nIt's scary.\n\nRe mapcount: I think if any part of the page is mapped, it would be \nconsidered mapped -> mapcount += 1.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David noted that splitting PAGE_SIZE into PTE_SIZE and PG_SIZE may require additional metadata in page tables, specifically a dedicated type (PGTY_table) to store separate metadata in ptdesc, similar to the kernel stack proposal.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'd assume that would work. Devil is in the detail with these things \nbefore we have memdescs.\n\nE.g., page table have a dedicated type (PGTY_table) and store separate \nmetadata in the ptdesc. For kernel stack there was once a proposal to \nhave a type but it is not upstream.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David suggested that hiding the sub-page mapping part or moving it to arch-specific code would simplify understanding of the patch, implying that the current implementation is too complex.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "complexity",
                "simplification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Okay, that's valuable information, thanks!\n\nBeing able to remove the sub-page mapping part (or being able to just \nhide it somewhere deep down in arch code) would make this a lot easier \nto digest.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer Kalesh Singh noted that the personality(2) system call may be too late to enforce larger VMA alignment, as initial userspace mappings are already established by the time it is invoked, and suggested using an early_param for global enforcement and a prctl/personality flag for per-process opt-in.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think personality(2) may be too late? By the time a process invokes\nit, the initial userspace mappings (executable, linker for init, etc)\nare already established with the default granularity.\n\nTo handle this, I've been using an early_param to enforce the larger\nVMA alignment system-wide right from boot.\n\nPerhaps, something for global enforcement (Kconfig/early param) and a\nprctl/personality flag for per-process opt in?",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer noted that userspace allocators may benefit from knowing the optimal page size (PG_SIZE) for layout optimization, while still being able to operate at PTE_SIZE granularity.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "considering benefits",
                "no clear objection"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This makes sense for maintaining ABI compatibility. Userspace\nallocators might want to optimize their layouts to match PG_SIZE while\nstill being able to operate at PTE_SIZE when needed.\n\n-- Kalesh",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer noted that the patch's proposed change to split PAGE_SIZE into PTE_SIZE and PG_SIZE would require a substantial rework of page fault and VMA handling, including changes to struct page pointers, pgprot_t, vma->vm_pgoff, and the page fault handler.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "substantial rework",
                "required changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 8:30AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer noted that ELF segment alignment set by linkers would be incompatible with the proposed 64k page size, potentially causing issues with loading of binaries built with default settings.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "incompatibility",
                "potential issues"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think most issues will stem from linkers setting the default ELF\nsegment alignment (max-page-size) for x86 to 4096. So those ELFs will\nnot load correctly or at all on the larger emulated granularity.\n\n-- Kalesh",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David noted that the patch requires consideration of potential issues with existing binaries and libraries, which may need to be recompiled.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential ABI changes",
                "recompilation requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right, I assume that they will have to be thought about that, and \npossibly, some binaries/libraries recompiled.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kalesh Singh",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges that the x86-64 SysV ABI allows for 64k page size, but notes it doesn't work in practice and emphasizes the importance of backward compatibility.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgment",
                "emphasis on backward compatibility"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think backward compatibility is important and I believe we can get\nthere without ABI break. And optimize from there.\n\nBTW, x86-64 SysV ABI allows for 64k page size:\n\n\tSystems are permitted to use any power-of-two page size between\n\t4KB and 64KB, inclusive.\n\nBut it doesn't work in practice.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledged that the patch may not be suitable for all use cases, specifically desktops, and is open to exploring alternative page sizes like 16k.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged limitations",
                "open to alternatives"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I have not invested much time into investigating this.\n\nI intentionally targeted compatible version assuming it will be better\nreceived by upstream. I want it to be usable outside specially cured\nuserspace. 64k might not be good fit for a desktop, but 16k can be a\ndifferent story.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86",
          "message_id": "aZhErt9DZcWI24_v@thinkstation",
          "url": "https://lore.kernel.org/all/aZhErt9DZcWI24_v@thinkstation/",
          "date": "2026-02-20T12:07:42Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Pedro Falcato",
              "summary": "Reviewer Pedro Falcato questioned the relevance of the patch's idea, citing that modern systems can use mTHP (memory Transparent Huge Pages) to achieve similar benefits by simply toggling a sysfs entry.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Doesn't this idea make less sense these days, with mTHP? Simply by toggling one\nof the entries in /sys/kernel/mm/transparent_hugepage.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Pedro Falcato",
              "summary": "Reviewer Pedro Falcato suggested adding a global minimum page cache order to address scalability concerns, noting that some issues are being addressed separately (e.g., memdesc work) and expressing confusion about the benefits of larger allocation sizes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "confusion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "We could perhaps add a way to enforce a min_order globally on the page cache,\nas a way to address it.\n\nThere are some points there which aren't addressed by mTHP work in any way\n(1G THPs for one), others which are being addressed separately (memdesc work\ntrying to cut down on struct page overhead).\n\n(I also don't understand your point about order-5 allocation, AFAIK pcp will\ncache up to COSTLY_ORDER (3) and PMD order, but I'm probably not seeing the\nfull picture)\n\n\n-- \nPedro",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer suggested emulating a larger page size (64k) for user space on x86 while still using 4k pages internally, reducing zone lock contention and other issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "When discussing per-process page sizes with Ryan and Dev, I mentioned \nthat having a larger emulated page size could be interesting for other \narchitectures as well.\n\nThat is, we would emulate a 64K page size on Intel for user space as \nwell, but let the OS work with 4K pages.\n\nWe'd only allocate+map large folios into user space + pagecache, but \nstill allow for page tables etc. to not waste memory.\n\nSo \"most\" of your allocations in the system would actually be at least \n64k, reducing zone lock contention etc.\n\n\nIt doesn't solve all the problems you wanted to tackle on your list \n(e.g., \"struct page\" overhead, which will be sorted out by memdescs).\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author responded to a concern that the patch does not handle fragmentation properly by explaining that mTHP is best effort and fragmentation is not a concern, implying that the patch will still work as long as there is free memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "mTHP is still best effort. This is way you don't need to care about\nfragmentation, you will get your 64k page as long as you have free\nmemory.",
              "reply_to": "Pedro Falcato",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author addressed Pedro's concern about the efficiency of the page allocator, explaining that a higher base page size reduces the work required to merge/split buddy pages.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged the benefit",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "With higher base page size, page allocator doesn't need to do as much\nwork to merge/split buddy pages. So serving the same 2M as order-5 is\ncheaper than order-9.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Pedro Falcato",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David noted that the proposed change would lead to reduced page table merging and splitting due to larger allocation sizes, which is a natural consequence of using 64k pages instead of 4k",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear technical objection or suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think the idea is that if most of your allocations (anon + pagecache) \nare 64k instead of 4k, on average, you'll just naturally do less merging \nsplitting.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author disagrees that emulation can help reduce zone lock contention, citing potential for increased contention due to mixed page sizes.",
              "sentiment": "CONTENTIOUS",
              "sentiment_signals": [
                "disagreement",
                "potential for increased contention"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I am not convinced emulation would help zone lock contention. I expect\ncontention to be higher if page allocator would see a mix of 4k and 64k\nrequests. It sounds like constant split/merge under the lock.",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author addressed David's concern about the feasibility of serving 1G pages from the buddy allocator, agreeing that it is not possible and questioning how to achieve viable 1G THPs without this capability.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging a technical limitation",
                "questioning the approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't think we can serve 1G pages out of buddy allocator with 4k\norder-0. And without it, I don't see how to get to a viable 1G THPs.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) noted that if most allocations are larger than 64k, the benefits of splitting page size into PTE_SIZE and PG_SIZE may be reduced, as there will still be some splitting/merging required for smaller allocations.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "If most your allocations are larger, then there isn't that much \nsplitting/merging.\n\nThere will be some for the < 64k allocations of course, but when all \nuser space+page cache is >= 64 then the split/merge + zone lock should \nbe heavily reduced.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David expressed skepticism about the proposed change, suggesting that previous work by Zi Yan could be leveraged to achieve the desired outcome.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "skepticism",
                "previous work"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Zi Yan was one working on this, and I think we had ideas on how to make \nthat work in the long run.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen noted that a 64k page cache would consume significantly more memory than expected, approximately 5GB extra for a kernel tree, and questioned the potential memory savings of the proposed change.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "First of all, this looks like fun. Nice work! I'm not opposed at all in\nconcept to cleaning up things and doing the logical separation you\ndescribed to split buddy granularity and mapping granularity. That seems\nlike a worthy endeavor and some of the union/#define tricks look like a\nlikely viable way to do it incrementally.\n\nBut I don't think there's going to be a lot of memory savings in the\nend. Maybe this would bring the mem= hyperscalers back into the fold and\nhave them actually start using 'struct page' again for their VM memory.\nDunno.\n\nBut, let's look at my kernel directory and round the file sizes up to\n4k, 16k and 64k:\n\nfind .  -printf '%s\\n' | while read size; do echo\t\\\n\t\t$(((size + 0x0fff) & 0xfffff000))\t\\\n\t\t$(((size + 0x3fff) & 0xffffc000))\t\\\n\t\t$(((size + 0xffff) & 0xffff0000));\ndone\n\n... and add them all up:\n\n11,297,648 KB - on disk\n11,297,712 KB - in a 4k page cache\n12,223,488 KB - in a 16k page cache\n16,623,296 KB - in a 64k page cache\n\nSo a 64k page cache eats ~5GB of extra memory for a kernel tree (well,\n_my_ kernel tree). In other words, if you are looking for memory savings\non my laptop, you'll need ~300GB of RAM before 'struct page' overhead\noverwhelms the page cache bloat from a single kernel tree.\n\nThe whole kernel obviously isn't in the page cache all at the same time.\nThe page cache across the system is also obviously different than a\nkernel tree, but you get the point.\n\nThat's not to diminish how useful something like this might be,\nespecially for folks that are sensitive to 'struct page' overhead or\nallocator performance.\n\nBut, it will mostly be getting better performance at the _cost_ of\nconsuming more RAM, not saving RAM.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author clarifies whether the proposed 64k page size should enforce alignment for user-space mappings, indicating a need to revisit the patch's impact on userspace ABI.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "need_for_revision"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Just to clarify, do you want it to be enforced on userspace ABI.\nLike, all mappings are 64k aligned?",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author acknowledges that memory waste due to page table overhead is a solvable issue, suggesting that switching to slab allocation can address it.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a problem",
                "proposed a solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Waste of memory for page table is solvable and pretty straight forward.\nMost of such cases can be solve mechanically by switching to slab.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen noted that the patch's approach to separating PAGE_SIZE into PTE_SIZE and PG_SIZE may not be suitable for architectures other than x86, as it relies on the assumption that PTE entries are aligned to a power of two, which is not guaranteed by the architecture.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "architecture",
                "assumption"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On 2/19/26 07:08, Kiryl Shutsemau wrote:\n...",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer noted that the patch introduces a large number of changes and renames across multiple files, which may make it difficult for others to review and understand the code.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "large number of changes",
                "renames"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "A few notes about the diffstats:\n\n$ git diff v6.17..HEAD arch/x86 | diffstat | tail -1\n 105 files changed, 874 insertions(+), 843 deletions(-)\n$ git diff v6.17..HEAD mm | diffstat | tail -1\n 53 files changed, 1136 insertions(+), 1069 deletions(-)\n\nThe vast, vast majority of this seems to be the renames. Stuff like:",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen expressed concern about the complexity and potential bugs introduced by the logic changes in the patch, requesting a clear separation between mechanical and logical modifications.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested_changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That stuff obviously needs to be audited but it's far less concerning\nthan the logic changes.\n\nSo just for review sanity, if you go forward with this, I'd very much\nappreciate a strong separation of the purely mechanical bits from any\nlogic changes.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer Dave Hansen noted that the proposed patch is not independent of other changes, specifically a full tree conversion to ptdescs, and suggested that the series should focus on identifying additional work needed before such a conversion can be considered.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "gates",
                "additional work"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Others mentioned this, but I think this essentially gates what you are\ndoing behind a full tree conversion over to ptdescs.\n\nThe most useful thing we can do with this series is look at it and\ndecide what _other_ things need to get done before the tree could\npossibly go in that direction, like ptdesc or a the disambiguation\nbetween PTE_SIZE and PG_SIZE that you've kicked off here.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Matthew Wilcox",
              "summary": "Reviewer suggested an alternative approach to implementing larger page sizes by allocating the larger size and using it for multiple entries, expressing skepticism about the slab approach",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "skeptical tone",
                "request for consideration of alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Have you looked at the s390/ppc implementations (yes, they're different,\nno, that sucks)?  slab seems like the wrong approach to me.\n\nThere's a third approach that I've never looked at which is to allocate\nthe larger size, then just use it for N consecutive entries.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Pedro Falcato",
              "summary": "Reviewer Pedro Falcato noted that the proposed change would result in 90%+ of allocations being 64k, which he hopes would be achieved by combining this patch with another patch modifying slab_min_order.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "hopes",
                "depending"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yep. That plus slab_min_order would hopefully yield a system where 90%+\n(depending on how your filesystem's buffer cache works) allocations are 64K.\n\n-- \nPedro",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledged that struct page memory consumption is a static cost, not reclaimed, and emphasized the importance of reclaimable page cache memory, suggesting userspace can control rounding overhead by handling data in large files.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "emphasized"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That's fair.\n\nThe problem with struct page memory consumption is that it is static and\ncannot be reclaimed. You pay the struct page tax no matter what.\n\nPage cache rounding overhead can be large, but a motivated userspace can\nkeep it under control by avoiding splitting a dataset into many small\nfiles. And this memory is reclaimable.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Dave Hansen",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledged that packing of page tables is not a requirement for correctness and plans to use the whole order-0 page for proof-of-concept",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged need to catch up on ptdescs",
                "planned PoC approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I have not followed ptdescs closely. Need to catch up.\n\nFor PoC, I will just waste full order-0 page for page table. Packing is\nnot required for correctness.",
              "reply_to": "Dave Hansen",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Hansen",
              "summary": "Reviewer noted that the proposed change to use a larger page size would not cause issues with the KPTI pgd because its current allocation of 8k PGDs would fit within the new 128k allocation, but found this outcome 'weird' and questioned whether it was functional.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning the functionality of a specific outcome"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah, I guess padding it out is ugly but effective.\n\nI was trying to figure out how it would apply to the KPTI pgd because we\njust flip bit 12 to switch between user and kernel PGDs. But I guess the\n8k of PGDs in the current allocation will fit fine in 128k, so it's\nweird but functional.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges a need for more work on handling page faults and VMA alignment, but does not commit to a specific fix or timeline.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges need for more work"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I was the first thing that came to mind. I have not put much time into\nit",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author is addressing Matthew Wilcox's concern about fragmentation when using 16k base pages on x86, explaining that the parent page table would need to be populated with 16 entries but fragmentation within the page itself is not a concern.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah, that's a possible way. We would need to populate 16 page table\nentries of the parent page table. But you don't need to care about\nfragmentation within the page.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer Kalesh Singh pointed out that the patch does not handle page faults into file mappings correctly when PTE_SIZE is less than PG_SIZE, and suggested that the page fault handler needs to be updated to handle misaligned cases.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Thu, Feb 19, 2026 at 7:39AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer Kalesh Singh noted that the current design does not enforce a larger granularity on VMAs to emulate a userspace page size, which is necessary for Android's use case of emulating 16KB devices on x86, and requested discussion on extending the design to cover this use case.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "interested in discussing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Kiryl,\n\nI'd be interested to discuss this at LSFMM.\n\nOn Android, we have a separate but related use case: we emulate the\nuserspace page size on x86, primarily to enable app developers to\nconduct compatibility testing of their apps for 16KB Android devices.\n[1]\n\nIt mainly works by enforcing a larger granularity on the VMAs to\nemulate a userspace page size, somewhat similar to what David\nmentioned, while the underlying kernel still operates on a 4KB\ngranularity. [2]\n\nIIUC the current design would not enfore the larger granularity /\nalignment for VMAs to avoid breaking ABI. However, I'd be interest to\ndiscuss whether it can be extended to cover this usecase as well.\n\n[1]  https://developer.android.com/guide/practices/page-sizes#16kb-emulator\n[2] https://source.android.com/docs/core/architecture/16kb-page-size/getting-started-cf-x86-64-pgagnostic\n\nThanks,\nKalesh",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Zi Yan",
              "summary": "The reviewer suggests adding a super pageblock that consists of N consecutive pageblocks to enable anti-fragmentation at larger granularity, specifically 1GB, and questions whether these free pages should go into the buddy allocator.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "debatable"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right. The idea is to add super pageblock (or whatever name), which consists of N consecutive\npageblocks, so that anti fragmentation can work at larger granularity, e.g., 1GB, to create\nfree pages. Whether 1GB free pages from memory compaction need to go into buddy allocator\nor not is debatable.\n\n--\nBest Regards,\nYan, Zi",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Liam Howlett",
              "summary": "The reviewer expressed concern that increasing page size would increase memory pressure and degrade primary workloads on machines with multiple concurrent tasks, potentially leading to more frequent OOMs for secondary tasks.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "But we are in reclaim a lot more these days.  As I'm sure you are aware,\nwe are trying to maximize the resources (both cpu and ram) of any\nmachine powered on.  Entering reclaim will consume the cpu time and will\naffect other tasks.\n\nEspecially with multiple workload machines, the tendency is to have a\nprimary focus with the lower desired work being killed, if necessary.\nReducing the overhead just means more secondary tasks, or a bigger\nfootprint of the ones already executing.\n\nIncreasing the memory pressure will degrade the primary workload more\nfrequently, even if we recover enough to avoid OOMing the secondary.\n\nWhile in the struct page tax world, the secondary task would be killed\nafter a shorter (and less frequently executed) reclaim comes up short.\nSo, I would think that we would be degrading the primary workload in an\nattempt to keep the secondary alive?  Maybe I'm over-simplifying here?\n\nNear the other end of the spectrum, we have chromebooks that are\nconstantly in reclaim, even with 4k pages.  I guess these machines would\nbe destine to maintain the same page size they use today.  That is, this\nsolution for the struct page tax is only useful if you have a lot of\nmemory.  But then again, that's where the bookkeeping costs become hard\nto take.\n\nThanks,\nLiam",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "David Laight",
              "summary": "Reviewer David Laight pointed out that the patch does not handle the case where PTE_SIZE is less than PG_SIZE, including misaligned cases in page fault handling.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Thu, 19 Feb 2026 15:08:51 +0000\nKiryl Shutsemau <kas@kernel.org> wrote:",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David Laight",
              "summary": "Reviewer David Laight raised concerns about potential issues with PAGE_SIZE being set to a value other than 4k, specifically mentioning random buffers and its impact on mmap of kernel memory and PCIe window alignment.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Also the 'random' buffers that are PAGE_SIZE rather than 4k.\n\nI also wonder how is affects mmap of kernel memory and the alignement\nof PCIe windows (etc).\n\n\tDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) noted that the proposed change to use a 64k page size would allow for emulated processes to run alongside native 4k processes on the same machine, eliminating the need for 'vma crosses base pages' handling.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal",
                "neutral comment"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right, see the proposal from Dev on the list.\n\n From user-space POV, the pagesize would be 64K for these emulated \nprocesses. That is, VMAs must be suitable aligned etc.\n\nOne key thing I think is that you could run such emulated-64k process \n(that actually support it!) with 4k processes on the same machine, like \nArm is considering.\n\nYou would have no weird \"vma crosses base pages\" handling, which is just \nrather nasty and makes my head hurt.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) expressed concerns that the proposed patch would reintroduce memory waste issues similar to those encountered on Arm when using 64k page size, and questioned whether the benefits of the proposal outweigh the costs.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested re-evaluation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Well, yes, like Willy says, there are already similar custom solutions \nfor s390x and ppc.\n\nPasha talked recently about the memory waste of 16k kernel stacks and \nhow we would want to reduce that to 4k. In your proposal, it would be \n64k, unless you somehow manage to allocate multiple kernel stacks from \nthe same 64k page. My head hurts thinking about whether that could work, \nmaybe it could (no idea about guard pages in there, though).\n\n\nLet's take a look at the history of page size usage on Arm (people can \nfeel free to correct me):\n\n(1) Most distros were using 64k on Arm.\n\n(2) People realized that 64k was suboptimal many use cases (memory\n     waste for stacks, pagecache, etc) and started to switch to 4k. I\n     remember that mostly HPC-centric users sticked to 64k, but there was\n     also demand from others to be able to stay on 64k.\n\n(3) Arm improved performance on a 4k kernel by adding cont-pte support,\n     trying to get closer to 64k native performance.\n\n(4) Achieving 64k native performance is hard, which is why per-process\n     page sizes are being explored to get the best out of both worlds\n     (use 64k page size only where it really matters for performance).\n\nArm clearly has the added benefit of actually benefiting from hardware \nsupport for 64k.\n\nIIUC, what you are proposing feels a bit like traveling back in time \nwhen it comes to the memory waste problem that Arm users encountered.\n\nWhere do you see the big difference to 64k on Arm in your proposal? \nWould you currently also be running 64k Arm in production and the memory \nwaste etc is acceptable?\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges that increasing page size to 64k or 16k would be a significant change and could limit adoption due to existing legacy code, but no specific plan for addressing this issue is mentioned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges limitation",
                "no clear plan"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Well, it will drastically limit the adoption. We have too much legacy\nstuff on x86.",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author responded to a concern about kernel stack allocation in the context of large base page sizes, suggesting that vmalloc can handle sub-page granularity and questioning why slab-allocated stacks couldn't work for large base page sizes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question",
                "explaining reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Kernel stack is allocated from vmalloc. I think mapping them with\nsub-page granularity should be doable.\n\nBTW, do you see any reason why slab-allocated stack wouldn't work for\nlarge base page sizes? There's no requirement for it be aligned to page\nor PTE, right?",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author is addressing David's concern about the similarity between 64k pages on x86 and existing 64k Arm hardware, explaining that they want to bring this option to x86 for large machines (over 2TiB of RAM) where memory consumption can be traded off for scalability.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That's the point. I don't see a big difference to 64k Arm. I want to\nbring this option to x86: at some machine size it makes sense trade\nmemory consumption for scalability. I am targeting it to machines with\nover 2TiB of RAM.\n\nBTW, we do run 64k Arm in our fleet. There's some growing pains, but it\nlooks good in general We have no plans to switch to 4k (or 16k) at the\nmoment. 512M THPs also look good on some workloads.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges the need for ABI preservation and proposes adding a knob or personality(2) option to enforce the new page size, but does not commit to implementing it yet.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges need for fix",
                "proposes alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't want to break ABI, but might add a knob (maybe personality(2) ?)\nfor enforcement to see what breaks.\n\nIn general, I would prefer to advertise a new value to userspace that\nwould mean preferred virtual address space granularity.",
              "reply_to": "Kalesh Singh",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author responded to Liam's feedback by expressing uncertainty about the reviewer's point, suggesting that the trade-off between struct page overhead and page cache rounding overhead may not be the primary concern.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "lack of clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I am not sure I fully follow your point.\n\nSizing tasks and scheduling tasks between machines is hard in general.\nI don't think the balance between struct page tax and page cache\nrounding overhead is going to be the primary factor.",
              "reply_to": "Liam Howlett",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author is addressing a concern about the target machines for the proposed 64k page size, stating that smaller machines will not benefit from it and implying that the feature is intended for larger systems.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Smaller machines are not target for 64k pages. They will not benefit\nfrom them.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Liam Howlett",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Liam Howlett",
              "summary": "Reviewer Liam Howlett noted that increasing page size could lead to increased reclaim penalties due to more frequent payments, which is a trade-off not previously considered.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "trade-offs",
                "reclaim penalty"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think there are more trade offs than what you listed.  It's still\nprobably worth doing, but I wanted to know if you though that this would\ncause us to spend more time in reclaim, which seems to be implied above.\nSo, another trade-off might be all the reclaim penalty being paid more\nfrequently?\n\n...\n\nThanks,\nLiam",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges that the patch requires more concrete analysis and evidence to support its claims, indicating a need for further work.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges need for more analysis",
                "patch is too hand-wavy"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I am not sure.\n\nKernel would need to do less work in reclaim per unit of memory.\nDepending on workloads you might see less allocation events and\ntherefore less frequent reclaim.\n\nIt's all too hand-wavy at the stage.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "Liam Howlett",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) expressed skepticism about the need for a new page size, suggesting that many applications can already handle differing page sizes due to other architectures' influence, and questioned the amount of conversion required for legacy apps",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "skepticism",
                "questioning"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'd assume that many applications nowadays can deal with differing page \nsizes (thanks to some other architectures paving the way).\n\nBut yes, some real legacy stuff, or stuff that ever only cared about \nintel still hardcodes pagesize=4k.\n\nIn Meta's fleet, I'd be quite interesting how much conversion there \nwould have to be done.\n\nFor legacy apps, you could still run them as 4k pagesize on the same \nsystem, of course.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David (Arm) expressed concern about the sub-page mapping approach, finding it 'scary', and questioned how the page's mapcount would be updated when only a part of the page is mapped.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested clarification",
                "expressed uncertainty"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I still have to wrap my head around the sub-page mapping here as well. \nIt's scary.\n\nRe mapcount: I think if any part of the page is mapped, it would be \nconsidered mapped -> mapcount += 1.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David noted that splitting PAGE_SIZE into PTE_SIZE and PG_SIZE may require additional metadata in the page table, specifically a dedicated type for kernel stacks, which was previously proposed but not upstream.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'd assume that would work. Devil is in the detail with these things \nbefore we have memdescs.\n\nE.g., page table have a dedicated type (PGTY_table) and store separate \nmetadata in the ptdesc. For kernel stack there was once a proposal to \nhave a type but it is not upstream.",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David suggested that hiding the sub-page mapping part or moving it to deeper architecture-specific code would simplify understanding of the patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested clarification",
                "suggested simplification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Okay, that's valuable information, thanks!\n\nBeing able to remove the sub-page mapping part (or being able to just \nhide it somewhere deep down in arch code) would make this a lot easier \nto digest.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer Kalesh Singh noted that the personality(2) system call may be too late to enforce larger VMA alignment, as initial userspace mappings are already established by the time it's invoked, and suggested using an early_param for global enforcement and a prctl/personality flag for per-process opt-in.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think personality(2) may be too late? By the time a process invokes\nit, the initial userspace mappings (executable, linker for init, etc)\nare already established with the default granularity.\n\nTo handle this, I've been using an early_param to enforce the larger\nVMA alignment system-wide right from boot.\n\nPerhaps, something for global enforcement (Kconfig/early param) and a\nprctl/personality flag for per-process opt in?",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer noted that userspace allocators may benefit from knowing the optimal page size (PG_SIZE) for layout optimization, while still being able to operate at PTE_SIZE granularity.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear opinion or request",
                "neutral tone"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This makes sense for maintaining ABI compatibility. Userspace\nallocators might want to optimize their layouts to match PG_SIZE while\nstill being able to operate at PTE_SIZE when needed.\n\n-- Kalesh",
              "reply_to": "Kiryl Shutsemau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer noted that the patch does not handle page faults into anonymous mappings correctly, as it assumes alignment to PG_SIZE in both virtual address space and physical memory, which may not always be the case.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 8:30AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Kalesh Singh",
              "summary": "Reviewer noted that ELF segment alignment is set to 4096 by default, which may cause issues when loading binaries on systems using a larger page size",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think most issues will stem from linkers setting the default ELF\nsegment alignment (max-page-size) for x86 to 4096. So those ELFs will\nnot load correctly or at all on the larger emulated granularity.\n\n-- Kalesh",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David noted that the patch requires consideration of potential issues with existing binaries and libraries, which may need to be recompiled.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential ABI changes",
                "binary compatibility"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right, I assume that they will have to be thought about that, and \npossibly, some binaries/libraries recompiled.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Kalesh Singh",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledges the importance of backward compatibility and suggests that it can be achieved without an ABI break, implying a potential fix for the issue.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledges importance of backward compatibility",
                "suggests potential fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think backward compatibility is important and I believe we can get\nthere without ABI break. And optimize from there.\n\nBTW, x86-64 SysV ABI allows for 64k page size:\n\n\tSystems are permitted to use any power-of-two page size between\n\t4KB and 64KB, inclusive.\n\nBut it doesn't work in practice.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledged that the patch may not be suitable for all use cases, specifically desktops, and is open to exploring alternative page sizes like 16k.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged potential limitations",
                "open to alternatives"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I have not invested much time into investigating this.\n\nI intentionally targeted compatible version assuming it will be better\nreceived by upstream. I want it to be usable outside specially cured\nuserspace. 64k might not be good fit for a desktop, but 16k can be a\ndifferent story.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Leo Martins",
      "primary_email": "loemra.dev@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Mark Harmstone",
      "primary_email": "mark@harmstone.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix chunk map leak in btrfs_map_block() after btrfs_translate_remap()",
          "message_id": "20260220131002.6269-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260220131002.6269-1-mark@harmstone.com/",
          "date": "2026-02-20T13:10:10Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch fixes a chunk map leak in the btrfs_map_block() function after a call to btrfs_translate_remap(). The fix involves adding a goto statement to jump out of the function when an error code is returned from btrfs_translate_remap(), preventing the chunk map from being leaked. This change addresses a bug introduced by a previous patch that redirected I/O for remapped block groups.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Identified the patch as a bug fix and provided a Reviewed-by tag. The reviewer acknowledged that it's more of a Reported-by rather than a Suggested-by, given its nature.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "Acknowledged"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 1:10PM Mark Harmstone <mark@harmstone.com> wrote:\n>\n> If the call to btrfs_translate_remap() in btrfs_map_block() returns an\n> error code, we were leaking the chunk map. Fix it by jumping to out\n> rather than returning directly.\n>\n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 18ba64992871 (\"btrfs: redirect I/O for remapped block groups\")\n> Suggested-by: Chris Mason <clm@fb.com>\n\nIt's a bug fix so this is more a Reported-by rather than a Suggested-by.\n\nOtherwise,\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\nThanks.\n\n> Link: https://lore.kernel.org/linux-btrfs/20260125125830.2352988-1-clm@meta.com/\n> ---\n>  fs/btrfs/volumes.c | 2 +-\n>  1 file changed, 1 insertion(+), 1 deletion(-)\n>\n> diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\n> index 83e2834ea273..1bd3464ccdd8 100644\n> --- a/fs/btrfs/volumes.c\n> +++ b/fs/btrfs/volumes.c\n> @@ -7082,7 +7082,7 @@ int btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n>\n>                 ret = btrfs_translate_remap(fs_info, &new_logical, length);\n>                 if (ret)\n> -                       return ret;\n> +                       goto out;\n>\n>                 if (new_logical != logical) {\n>                         btrfs_free_chunk_map(map);\n> --\n> 2.52.0\n>\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David Sterba",
              "summary": "Corrected the reviewer to use Reported-by instead of Suggested-by, given that it's a bug fix. Also requested that the patch author sort the tags according to the btrfs documentation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "Requested correction"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:09:54PM +0000, Mark Harmstone wrote:\n> If the call to btrfs_translate_remap() in btrfs_map_block() returns an\n> error code, we were leaking the chunk map. Fix it by jumping to out\n> rather than returning directly.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 18ba64992871 (\"btrfs: redirect I/O for remapped block groups\")\n> Suggested-by: Chris Mason <clm@fb.com>\n\nIf it's a but then it's Reported-by\n\n> Link: https://lore.kernel.org/linux-btrfs/20260125125830.2352988-1-clm@meta.com/\n\nPlease sort the tags according to\nhttps://btrfs.readthedocs.io/en/latest/dev/Developer-s-FAQ.html#ordering\n\nas it's been for a long time and saves me editing each patch. Thanks.\n",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone",
              "summary": "Acknowledged the corrections and edited the patches to include the tags in the correct order.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "Acknowledged correction"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On 20/02/2026 3.58 pm, David Sterba wrote:\n> On Fri, Feb 20, 2026 at 01:09:54PM +0000, Mark Harmstone wrote:\n>> If the call to btrfs_translate_remap() in btrfs_map_block() returns an\n>> error code, we were leaking the chunk map. Fix it by jumping to out\n>> rather than returning directly.\n>>\n>> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n>> Fixes: 18ba64992871 (\"btrfs: redirect I/O for remapped block groups\")\n>> Suggested-by: Chris Mason <clm@fb.com>\n> \n> If it's a but then it's Reported-by\n> \n>> Link: https://lore.kernel.org/linux-btrfs/20260125125830.2352988-1-clm@meta.com/\n> \n> Please sort the tags according to\n> https://btrfs.readthedocs.io/en/latest/dev/Developer-s-FAQ.html#ordering\n> \n> as it's been for a long time and saves me editing each patch. Thanks.\n\nOkay, I've edited the patches and pushed them with the tags in the right \norder.\n\n",
              "reply_to": "David Sterba",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix chunk map leak in btrfs_map_block() after btrfs_chunk_map_num_copies()",
          "message_id": "20260220130209.5020-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260220130209.5020-1-mark@harmstone.com/",
          "date": "2026-02-20T13:02:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch fixes a chunk map leak in the btrfs_map_block() function by properly freeing the chunk map when returning early with -EINVAL. The fix involves adding a goto label to free the chunk map before exiting the function.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Reviewed the patch and noted that a Fixes tag implies a stable kernel version is affected. Suggested removing the CC: stable tag as it's no longer necessary with the Fixes tag.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEUTRAL"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 1:02PM Mark Harmstone <mark@harmstone.com> wrote:\n>\n> Fix a chunk map leak in btrfs_map_block(): if we return early with -EINVAL,\n> we're not freeing the chunk map that we've just looked up.\n>\n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Cc: stable@vger.kernel.org\n> Fixes: 0ae653fbec2b (\"btrfs: reduce chunk_map lookups in btrfs_map_block()\")\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\nSide note: if there's a Fixes tag, we don't need a CC stable tag\nanymore nowadays.\n\n\n\n> ---\n>  fs/btrfs/volumes.c | 6 ++++--\n>  1 file changed, 4 insertions(+), 2 deletions(-)\n>\n> diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\n> index 1bd3464ccdd8..a1f0fccd552c 100644\n> --- a/fs/btrfs/volumes.c\n> +++ b/fs/btrfs/volumes.c\n> @@ -7096,8 +7096,10 @@ int btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n>         }\n>\n>         num_copies = btrfs_chunk_map_num_copies(map);\n> -       if (io_geom.mirror_num > num_copies)\n> -               return -EINVAL;\n> +       if (io_geom.mirror_num > num_copies) {\n> +               ret = -EINVAL;\n> +               goto out;\n> +       }\n>\n>         map_offset = logical - map->start;\n>         io_geom.raid56_full_stripe_start = (u64)-1;\n> --\n> 2.52.0\n>\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Greg KH",
              "summary": "Corrected Filipe's understanding of the stable kernel rules, emphasizing that a CC: stable tag is required for stable kernel versions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "CORRECTIVE"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:07:27PM +0000, Filipe Manana wrote:\n> On Fri, Feb 20, 2026 at 1:02\\u202fPM Mark Harmstone <mark@harmstone.com> wrote:\n> >\n> > Fix a chunk map leak in btrfs_map_block(): if we return early with -EINVAL,\n> > we're not freeing the chunk map that we've just looked up.\n> >\n> > Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> > Cc: stable@vger.kernel.org\n> > Fixes: 0ae653fbec2b (\"btrfs: reduce chunk_map lookups in btrfs_map_block()\")\n> \n> Reviewed-by: Filipe Manana <fdmanana@suse.com>\n> \n> Side note: if there's a Fixes tag, we don't need a CC stable tag\n> anymore nowadays.\n\nNot true at all, please read:\n\n    https://www.kernel.org/doc/html/latest/process/stable-kernel-rules.html\n\nYou HAVE to have a cc: stable if you know you want it to be added to a\nstable tree.  If you do not do that, you are at the mercy of \"when Greg\nand Sasha get bored and attempt to pick up things that maintainers\nforgot about\".\n\nthanks,\n\ngreg k-h\n\n",
              "reply_to": "Filipe Manana",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix chunk offset error message in check_dev_extent_item()",
          "message_id": "20260220113013.30254-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260220113013.30254-1-mark@harmstone.com/",
          "date": "2026-02-20T11:30:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch fixes a copy-paste bug in the error message of check_dev_extent_item() in Btrfs, which incorrectly reports an offset but prints the objectid instead.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Qu Wenruo",
              "summary": "Approved the patch with a Reviewed-by tag, noting that it fixes an error message bug in Btrfs.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "APPROVED"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\n 2026/2/20 22:00, Mark Harmstone :\n> Fix a copy-paste bug in an error message in check_dev_extent_item():\n> we're reporting an incorrect offset, but actually printing the objectid.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 008e2512dc56 (\"btrfs: tree-checker: add dev extent item checks\")\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\n\nThanks,\nQu\n\n> ---\n>   fs/btrfs/tree-checker.c | 2 +-\n>   1 file changed, 1 insertion(+), 1 deletion(-)\n> \n> diff --git a/fs/btrfs/tree-checker.c b/fs/btrfs/tree-checker.c\n> index ac4c4573ee39..133510f99fc5 100644\n> --- a/fs/btrfs/tree-checker.c\n> +++ b/fs/btrfs/tree-checker.c\n> @@ -1899,7 +1899,7 @@ static int check_dev_extent_item(const struct extent_buffer *leaf,\n>   \t\t\t\t sectorsize))) {\n>   \t\tgeneric_err(leaf, slot,\n>   \t\t\t    \"invalid dev extent chunk offset, has %llu not aligned to %u\",\n> -\t\t\t    btrfs_dev_extent_chunk_objectid(leaf, de),\n> +\t\t\t    btrfs_dev_extent_chunk_offset(leaf, de),\n>   \t\t\t    sectorsize);\n>   \t\treturn -EUCLEAN;\n>   \t}\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix unlikely in btrfs_insert_one_raid_extent()",
          "message_id": "20260218130006.9563-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260218130006.9563-1-mark@harmstone.com/",
          "date": "2026-02-18T13:00:13Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-18",
          "patch_summary": "This patch fixes an unlikely annotation in btrfs_insert_one_raid_extent() by correcting the placement of the exclamation point, which was incorrectly indicating that allocation failure is expected.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Raised concern that the Fixes tag was incorrectly used, as this patch does not fit into the categories of bug fixes or severe performance regressions. Provided a Reviewed-by tag.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "WAITING_FOR_REVIEW"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Wed, Feb 18, 2026 at 1:01PM Mark Harmstone <mark@harmstone.com> wrote:\n>\n> Fix the unlikely added to btrfs_insert_one_raid_extent() by commit\n> a929904c: the exclamation point is in the wrong place, so we are telling\n> the compiler that allocation failure is actually expected.\n>\n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: a929904cf73b (\"btrfs: add unlikely annotations to branches leading to transaction abort\")\n\nWe use the Fixes tag for things that must be backported, which are\naither bug fixes or rather severe performance regressions (i.e. things\nthat affect users), and this doesn't fit into these categories.\n\nOtherwise:\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\n\n> ---\n>  fs/btrfs/raid-stripe-tree.c | 2 +-\n>  1 file changed, 1 insertion(+), 1 deletion(-)\n>\n> diff --git a/fs/btrfs/raid-stripe-tree.c b/fs/btrfs/raid-stripe-tree.c\n> index 2987cb7c686e..638c4ad572c9 100644\n> --- a/fs/btrfs/raid-stripe-tree.c\n> +++ b/fs/btrfs/raid-stripe-tree.c\n> @@ -300,7 +300,7 @@ int btrfs_insert_one_raid_extent(struct btrfs_trans_handle *trans,\n>         int ret;\n>\n>         stripe_extent = kzalloc(item_size, GFP_NOFS);\n> -       if (!unlikely(stripe_extent)) {\n> +       if (unlikely(!stripe_extent)) {\n>                 btrfs_abort_transaction(trans, -ENOMEM);\n>                 btrfs_end_transaction(trans);\n>                 return -ENOMEM;\n> --\n> 2.52.0\n>\n>\n",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone",
              "summary": "Acknowledged the concern and agreed to remove the Fixes tag from the patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On 18/02/2026 2.08 pm, Filipe Manana wrote:\n> On Wed, Feb 18, 2026 at 1:01\\u202fPM Mark Harmstone <mark@harmstone.com> wrote:\n>>\n>> Fix the unlikely added to btrfs_insert_one_raid_extent() by commit\n>> a929904c: the exclamation point is in the wrong place, so we are telling\n>> the compiler that allocation failure is actually expected.\n>>\n>> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n>> Fixes: a929904cf73b (\"btrfs: add unlikely annotations to branches leading to transaction abort\")\n> \n> We use the Fixes tag for things that must be backported, which are\n> aither bug fixes or rather severe performance regressions (i.e. things\n> that affect users), and this doesn't fit into these categories.\n\nOkay - thanks Filipe, I'll push it with the Fixes line taken out.\n\n> Otherwise:\n> \n> Reviewed-by: Filipe Manana <fdmanana@suse.com>\n> \n> \n>> ---\n>>   fs/btrfs/raid-stripe-tree.c | 2 +-\n>>   1 file changed, 1 insertion(+), 1 deletion(-)\n>>\n>> diff --git a/fs/btrfs/raid-stripe-tree.c b/fs/btrfs/raid-stripe-tree.c\n>> index 2987cb7c686e..638c4ad572c9 100644\n>> --- a/fs/btrfs/raid-stripe-tree.c\n>> +++ b/fs/btrfs/raid-stripe-tree.c\n>> @@ -300,7 +300,7 @@ int btrfs_insert_one_raid_extent(struct btrfs_trans_handle *trans,\n>>          int ret;\n>>\n>>          stripe_extent = kzalloc(item_size, GFP_NOFS);\n>> -       if (!unlikely(stripe_extent)) {\n>> +       if (unlikely(!stripe_extent)) {\n>>                  btrfs_abort_transaction(trans, -ENOMEM);\n>>                  btrfs_end_transaction(trans);\n>>                  return -ENOMEM;\n>> --\n>> 2.52.0\n>>\n>>\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] btrfs: fix chunk map leak in btrfs_map_block() after btrfs_translate_remap()",
          "message_id": "85740194-bcd5-486f-b7a2-f31613f85c9f@harmstone.com",
          "url": "https://lore.kernel.org/all/85740194-bcd5-486f-b7a2-f31613f85c9f@harmstone.com/",
          "date": "2026-02-20T16:27:35Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "A bug fix patch for a chunk map leak in btrfs_map_block() after btrfs_translate_remap().",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Reviewed the patch and found it to be a bug fix, so marked it as Reported-by rather than Suggested-by. Provided Reviewed-by tag.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "positive review"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 1:10PM Mark Harmstone <mark@harmstone.com> wrote:\n>\n> If the call to btrfs_translate_remap() in btrfs_map_block() returns an\n> error code, we were leaking the chunk map. Fix it by jumping to out\n> rather than returning directly.\n>\n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 18ba64992871 (\"btrfs: redirect I/O for remapped block groups\")\n> Suggested-by: Chris Mason <clm@fb.com>\n\nIt's a bug fix so this is more a Reported-by rather than a Suggested-by.\n\nOtherwise,\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\nThanks.\n\n> Link: https://lore.kernel.org/linux-btrfs/20260125125830.2352988-1-clm@meta.com/\n> ---\n>  fs/btrfs/volumes.c | 2 +-\n>  1 file changed, 1 insertion(+), 1 deletion(-)\n>\n> diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\n> index 83e2834ea273..1bd3464ccdd8 100644\n> --- a/fs/btrfs/volumes.c\n> +++ b/fs/btrfs/volumes.c\n> @@ -7082,7 +7082,7 @@ int btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n>\n>                 ret = btrfs_translate_remap(fs_info, &new_logical, length);\n>                 if (ret)\n> -                       return ret;\n> +                       goto out;\n>\n>                 if (new_logical != logical) {\n>                         btrfs_free_chunk_map(map);\n> --\n> 2.52.0\n>\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "David Sterba",
              "summary": "Corrected the Reported-by tag to be in the correct order according to btrfs documentation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 01:09:54PM +0000, Mark Harmstone wrote:\n> If the call to btrfs_translate_remap() in btrfs_map_block() returns an\n> error code, we were leaking the chunk map. Fix it by jumping to out\n> rather than returning directly.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 18ba64992871 (\"btrfs: redirect I/O for remapped block groups\")\n> Suggested-by: Chris Mason <clm@fb.com>\n\nIf it's a but then it's Reported-by\n\n> Link: https://lore.kernel.org/linux-btrfs/20260125125830.2352988-1-clm@meta.com/\n\nPlease sort the tags according to\nhttps://btrfs.readthedocs.io/en/latest/dev/Developer-s-FAQ.html#ordering\n\nas it's been for a long time and saves me editing each patch. Thanks.\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone",
              "summary": "Responded to David's request by editing the patches and pushing them with the correct tag order.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "positive response"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On 20/02/2026 3.58 pm, David Sterba wrote:\n> On Fri, Feb 20, 2026 at 01:09:54PM +0000, Mark Harmstone wrote:\n>> If the call to btrfs_translate_remap() in btrfs_map_block() returns an\n>> error code, we were leaking the chunk map. Fix it by jumping to out\n>> rather than returning directly.\n>>\n>> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n>> Fixes: 18ba64992871 (\"btrfs: redirect I/O for remapped block groups\")\n>> Suggested-by: Chris Mason <clm@fb.com>\n> \n> If it's a but then it's Reported-by\n> \n>> Link: https://lore.kernel.org/linux-btrfs/20260125125830.2352988-1-clm@meta.com/\n> \n> Please sort the tags according to\n> https://btrfs.readthedocs.io/en/latest/dev/Developer-s-FAQ.html#ordering\n> \n> as it's been for a long time and saves me editing each patch. Thanks.\n\nOkay, I've edited the patches and pushed them with the tags in the right \norder.\n\n",
              "reply_to": "David Sterba",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] btrfs: fix unlikely in btrfs_insert_one_raid_extent()",
          "message_id": "6b37545b-80ee-4fef-bd55-5b6d9996716f@harmstone.com",
          "url": "https://lore.kernel.org/all/6b37545b-80ee-4fef-bd55-5b6d9996716f@harmstone.com/",
          "date": "2026-02-20T12:15:58Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "A patch to fix an unlikely annotation in btrfs_insert_one_raid_extent() by correcting the placement of an exclamation point.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Raised concerns about the use of Fixes tag and suggested removing it. Also provided a Reviewed-by tag.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Wed, Feb 18, 2026 at 1:01PM Mark Harmstone <mark@harmstone.com> wrote:\n>\n> Fix the unlikely added to btrfs_insert_one_raid_extent() by commit\n> a929904c: the exclamation point is in the wrong place, so we are telling\n> the compiler that allocation failure is actually expected.\n>\n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: a929904cf73b (\"btrfs: add unlikely annotations to branches leading to transaction abort\")\n\nWe use the Fixes tag for things that must be backported, which are\naither bug fixes or rather severe performance regressions (i.e. things\nthat affect users), and this doesn't fit into these categories.\n\nOtherwise:\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\n\n> ---\n>  fs/btrfs/raid-stripe-tree.c | 2 +-\n>  1 file changed, 1 insertion(+), 1 deletion(-)\n>\n> diff --git a/fs/btrfs/raid-stripe-tree.c b/fs/btrfs/raid-stripe-tree.c\n> index 2987cb7c686e..638c4ad572c9 100644\n> --- a/fs/btrfs/raid-stripe-tree.c\n> +++ b/fs/btrfs/raid-stripe-tree.c\n> @@ -300,7 +300,7 @@ int btrfs_insert_one_raid_extent(struct btrfs_trans_handle *trans,\n>         int ret;\n>\n>         stripe_extent = kzalloc(item_size, GFP_NOFS);\n> -       if (!unlikely(stripe_extent)) {\n> +       if (unlikely(!stripe_extent)) {\n>                 btrfs_abort_transaction(trans, -ENOMEM);\n>                 btrfs_end_transaction(trans);\n>                 return -ENOMEM;\n> --\n> 2.52.0\n>\n>\n",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Nhat Pham",
      "primary_email": "nphamcs@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] vswap: fix poor batching behavior of vswap free path",
          "message_id": "20260220210539.989603-1-nphamcs@gmail.com",
          "url": "https://lore.kernel.org/all/20260220210539.989603-1-nphamcs@gmail.com/",
          "date": "2026-02-20T22:54:20Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch addresses poor batching behavior in the vswap free path, which is a critical component of Linux's virtual memory management system. The issue arises when multiple pages are being swapped out simultaneously, causing inefficient use of system resources and leading to performance degradation. To fix this problem, the patch optimizes the vswap free path by improving how it handles page swapping, reducing contention between threads and minimizing the number of lock acquisitions. This is achieved through a combination of data structure modifications and synchronization improvements, ultimately resulting in better system responsiveness and reduced overhead.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Nhat Pham (author)",
              "summary": "Author addressed a concern about the organization of swap API functions in include/linux/swap.h, agreeing to group them into lifecycle, cache, and physical allocator categories as part of a clean-up patch with no functional changes intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "clean-up",
                "no-functional-change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "In the swap header file (include/linux/swap.h), group the swap API into\nthe following categories:\n\n1. Lifecycle swap functions (i.e the function that changes the reference\n   count of the swap entry).\n\n2. Swap cache API.\n\n3. Physical swapfile allocator and swap device API.\n\nAlso remove extern in the functions that are rearranged.\n\nThis is purely a clean up. No functional change intended.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h | 53 +++++++++++++++++++++++---------------------\n 1 file changed, 28 insertions(+), 25 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 38ca3df687160..aa29d8ac542d1 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -423,20 +423,34 @@ extern void __meminit kswapd_stop(int nid);\n \n #ifdef CONFIG_SWAP\n \n-int add_swap_extent(struct swap_info_struct *sis, unsigned long start_page,\n-\t\tunsigned long nr_pages, sector_t start_block);\n-int generic_swapfile_activate(struct swap_info_struct *, struct file *,\n-\t\tsector_t *);\n-\n+/* Lifecycle swap API (mm/swapfile.c) */\n+int folio_alloc_swap(struct folio *folio);\n+bool folio_free_swap(struct folio *folio);\n+void put_swap_folio(struct folio *folio, swp_entry_t entry);\n+void swap_shmem_alloc(swp_entry_t, int);\n+int swap_duplicate(swp_entry_t);\n+int swapcache_prepare(swp_entry_t entry, int nr);\n+void swap_free_nr(swp_entry_t entry, int nr_pages);\n+void free_swap_and_cache_nr(swp_entry_t entry, int nr);\n+int __swap_count(swp_entry_t entry);\n+bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry);\n+int swp_swapcount(swp_entry_t entry);\n+\n+/* Swap cache API (mm/swap_state.c) */\n static inline unsigned long total_swapcache_pages(void)\n {\n \treturn global_node_page_state(NR_SWAPCACHE);\n }\n-\n-void free_swap_cache(struct folio *folio);\n void free_folio_and_swap_cache(struct folio *folio);\n void free_pages_and_swap_cache(struct encoded_page **, int);\n-/* linux/mm/swapfile.c */\n+void free_swap_cache(struct folio *folio);\n+\n+/* Physical swap allocator and swap device API (mm/swapfile.c) */\n+int add_swap_extent(struct swap_info_struct *sis, unsigned long start_page,\n+\t\tunsigned long nr_pages, sector_t start_block);\n+int generic_swapfile_activate(struct swap_info_struct *, struct file *,\n+\t\tsector_t *);\n+\n extern atomic_long_t nr_swap_pages;\n extern long total_swap_pages;\n extern atomic_t nr_rotate_swap;\n@@ -452,26 +466,15 @@ static inline long get_nr_swap_pages(void)\n \treturn atomic_long_read(&nr_swap_pages);\n }\n \n-extern void si_swapinfo(struct sysinfo *);\n-int folio_alloc_swap(struct folio *folio);\n-bool folio_free_swap(struct folio *folio);\n-void put_swap_folio(struct folio *folio, swp_entry_t entry);\n-extern swp_entry_t get_swap_page_of_type(int);\n-extern int add_swap_count_continuation(swp_entry_t, gfp_t);\n-extern void swap_shmem_alloc(swp_entry_t, int);\n-extern int swap_duplicate(swp_entry_t);\n-extern int swapcache_prepare(swp_entry_t entry, int nr);\n-extern void swap_free_nr(swp_entry_t entry, int nr_pages);\n-extern void free_swap_and_cache_nr(swp_entry_t entry, int nr);\n+void si_swapinfo(struct sysinfo *);\n+swp_entry_t get_swap_page_of_type(int);\n+int add_swap_count_continuation(swp_entry_t, gfp_t);\n int swap_type_of(dev_t device, sector_t offset);\n int find_first_swap(dev_t *device);\n-extern unsigned int count_swap_pages(int, int);\n-extern sector_t swapdev_block(int, pgoff_t);\n-extern int __swap_count(swp_entry_t entry);\n-extern bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry);\n-extern int swp_swapcount(swp_entry_t entry);\n+unsigned int count_swap_pages(int, int);\n+sector_t swapdev_block(int, pgoff_t);\n struct backing_dev_info;\n-extern struct swap_info_struct *get_swap_device(swp_entry_t entry);\n+struct swap_info_struct *get_swap_device(swp_entry_t entry);\n sector_t swap_folio_sector(struct folio *folio);\n \n static inline void put_swap_device(struct swap_info_struct *si)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, and provided a generic API to abstract away the swapoff locking out behavior.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "provided"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Currently, we get a reference to the backing swap device in order to\nprevent swapoff from freeing the metadata of a swap entry. This does not\nmake sense in the new virtual swap design, especially after the swap\nbackends are decoupled - a swap entry might not have any backing swap\ndevice at all, and its backend might change at any time during its\nlifetime.\n\nIn preparation for this, abstract away the swapoff locking out behavior\ninto a generic API.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h | 17 +++++++++++++++++\n mm/memory.c          | 13 +++++++------\n mm/mincore.c         | 15 +++------------\n mm/shmem.c           | 12 ++++++------\n mm/swap_state.c      | 14 +++++++-------\n mm/userfaultfd.c     | 15 +++++++++------\n mm/zswap.c           |  5 ++---\n 7 files changed, 51 insertions(+), 40 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex aa29d8ac542d1..3da637b218baf 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -659,5 +659,22 @@ static inline bool mem_cgroup_swap_full(struct folio *folio)\n }\n #endif\n \n+static inline bool tryget_swap_entry(swp_entry_t entry,\n+\t\t\t\tstruct swap_info_struct **sip)\n+{\n+\tstruct swap_info_struct *si = get_swap_device(entry);\n+\n+\tif (sip)\n+\t\t*sip = si;\n+\n+\treturn si;\n+}\n+\n+static inline void put_swap_entry(swp_entry_t entry,\n+\t\t\t\tstruct swap_info_struct *si)\n+{\n+\tput_swap_device(si);\n+}\n+\n #endif /* __KERNEL__*/\n #endif /* _LINUX_SWAP_H */\ndiff --git a/mm/memory.c b/mm/memory.c\nindex da360a6eb8a48..90031f833f52e 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -4630,6 +4630,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \tstruct swap_info_struct *si = NULL;\n \trmap_t rmap_flags = RMAP_NONE;\n \tbool need_clear_cache = false;\n+\tbool swapoff_locked = false;\n \tbool exclusive = false;\n \tsoftleaf_t entry;\n \tpte_t pte;\n@@ -4698,8 +4699,8 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t}\n \n \t/* Prevent swapoff from happening to us. */\n-\tsi = get_swap_device(entry);\n-\tif (unlikely(!si))\n+\tswapoff_locked = tryget_swap_entry(entry, &si);\n+\tif (unlikely(!swapoff_locked))\n \t\tgoto out;\n \n \tfolio = swap_cache_get_folio(entry);\n@@ -5047,8 +5048,8 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\tif (waitqueue_active(&swapcache_wq))\n \t\t\twake_up(&swapcache_wq);\n \t}\n-\tif (si)\n-\t\tput_swap_device(si);\n+\tif (swapoff_locked)\n+\t\tput_swap_entry(entry, si);\n \treturn ret;\n out_nomap:\n \tif (vmf->pte)\n@@ -5066,8 +5067,8 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\tif (waitqueue_active(&swapcache_wq))\n \t\t\twake_up(&swapcache_wq);\n \t}\n-\tif (si)\n-\t\tput_swap_device(si);\n+\tif (swapoff_locked)\n+\t\tput_swap_entry(entry, si);\n \treturn ret;\n }\n \ndiff --git a/mm/mincore.c b/mm/mincore.c\nindex e5d13eea92347..f3eb771249d67 100644\n--- a/mm/mincore.c\n+++ b/mm/mincore.c\n@@ -77,19 +77,10 @@ static unsigned char mincore_swap(swp_entry_t entry, bool shmem)\n \tif (!softleaf_is_swap(entry))\n \t\treturn !shmem;\n \n-\t/*\n-\t * Shmem mapping lookup is lockless, so we need to grab the swap\n-\t * device. mincore page table walk locks the PTL, and the swap\n-\t * device is stable, avoid touching the si for better performance.\n-\t */\n-\tif (shmem) {\n-\t\tsi = get_swap_device(entry);\n-\t\tif (!si)\n-\t\t\treturn 0;\n-\t}\n+\tif (!tryget_swap_entry(entry, &si))\n+\t\treturn 0;\n \tfolio = swap_cache_get_folio(entry);\n-\tif (shmem)\n-\t\tput_swap_device(si);\n+\tput_swap_entry(entry, si);\n \t/* The swap cache space contains either folio, shadow or NULL */\n \tif (folio && !xa_is_value(folio)) {\n \t\tpresent = folio_test_uptodate(folio);\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 1db97ef2d14eb..b40be22fa5f09 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -2307,7 +2307,7 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \tsoftleaf_t index_entry;\n \tstruct swap_info_struct *si;\n \tstruct folio *folio = NULL;\n-\tbool skip_swapcache = false;\n+\tbool swapoff_locked, skip_swapcache = false;\n \tint error, nr_pages, order;\n \tpgoff_t offset;\n \n@@ -2319,16 +2319,16 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \tif (softleaf_is_poison_marker(index_entry))\n \t\treturn -EIO;\n \n-\tsi = get_swap_device(index_entry);\n+\tswapoff_locked = tryget_swap_entry(index_entry, &si);\n \torder = shmem_confirm_swap(mapping, index, index_entry);\n-\tif (unlikely(!si)) {\n+\tif (unlikely(!swapoff_locked)) {\n \t\tif (order < 0)\n \t\t\treturn -EEXIST;\n \t\telse\n \t\t\treturn -EINVAL;\n \t}\n \tif (unlikely(order < 0)) {\n-\t\tput_swap_device(si);\n+\t\tput_swap_entry(index_entry, si);\n \t\treturn -EEXIST;\n \t}\n \n@@ -2448,7 +2448,7 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \t}\n \tfolio_mark_dirty(folio);\n \tswap_free_nr(swap, nr_pages);\n-\tput_swap_device(si);\n+\tput_swap_entry(swap, si);\n \n \t*foliop = folio;\n \treturn 0;\n@@ -2466,7 +2466,7 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \t\tswapcache_clear(si, folio->swap, folio_nr_pages(folio));\n \tif (folio)\n \t\tfolio_put(folio);\n-\tput_swap_device(si);\n+\tput_swap_entry(swap, si);\n \n \treturn error;\n }\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 34c9d9b243a74..bece18eb540fa 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -538,8 +538,7 @@ struct folio *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \tpgoff_t ilx;\n \tstruct folio *folio;\n \n-\tsi = get_swap_device(entry);\n-\tif (!si)\n+\tif (!tryget_swap_entry(entry, &si))\n \t\treturn NULL;\n \n \tmpol = get_vma_policy(vma, addr, 0, &ilx);\n@@ -550,7 +549,7 @@ struct folio *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \tif (page_allocated)\n \t\tswap_read_folio(folio, plug);\n \n-\tput_swap_device(si);\n+\tput_swap_entry(entry, si);\n \treturn folio;\n }\n \n@@ -763,6 +762,7 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \tfor (addr = start; addr < end; ilx++, addr += PAGE_SIZE) {\n \t\tstruct swap_info_struct *si = NULL;\n \t\tsoftleaf_t entry;\n+\t\tbool swapoff_locked = false;\n \n \t\tif (!pte++) {\n \t\t\tpte = pte_offset_map(vmf->pmd, addr);\n@@ -781,14 +781,14 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \t\t * holding a reference to, try to grab a reference, or skip.\n \t\t */\n \t\tif (swp_type(entry) != swp_type(targ_entry)) {\n-\t\t\tsi = get_swap_device(entry);\n-\t\t\tif (!si)\n+\t\t\tswapoff_locked = tryget_swap_entry(entry, &si);\n+\t\t\tif (!swapoff_locked)\n \t\t\t\tcontinue;\n \t\t}\n \t\tfolio = __read_swap_cache_async(entry, gfp_mask, mpol, ilx,\n \t\t\t\t\t\t&page_allocated, false);\n-\t\tif (si)\n-\t\t\tput_swap_device(si);\n+\t\tif (swapoff_locked)\n+\t\t\tput_swap_entry(entry, si);\n \t\tif (!folio)\n \t\t\tcontinue;\n \t\tif (page_allocated) {\ndiff --git a/mm/userfaultfd.c b/mm/userfaultfd.c\nindex e6dfd5f28acd7..25f89eba0438c 100644\n--- a/mm/userfaultfd.c\n+++ b/mm/userfaultfd.c\n@@ -1262,9 +1262,11 @@ static long move_pages_ptes(struct mm_struct *mm, pmd_t *dst_pmd, pmd_t *src_pmd\n \tpte_t *dst_pte = NULL;\n \tpmd_t dummy_pmdval;\n \tpmd_t dst_pmdval;\n+\tsoftleaf_t entry;\n \tstruct folio *src_folio = NULL;\n \tstruct mmu_notifier_range range;\n \tlong ret = 0;\n+\tbool swapoff_locked = false;\n \n \tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm,\n \t\t\t\tsrc_addr, src_addr + len);\n@@ -1429,7 +1431,7 @@ static long move_pages_ptes(struct mm_struct *mm, pmd_t *dst_pmd, pmd_t *src_pmd\n \t\t\t\t\tlen);\n \t} else { /* !pte_present() */\n \t\tstruct folio *folio = NULL;\n-\t\tconst softleaf_t entry = softleaf_from_pte(orig_src_pte);\n+\t\tentry = softleaf_from_pte(orig_src_pte);\n \n \t\tif (softleaf_is_migration(entry)) {\n \t\t\tpte_unmap(src_pte);\n@@ -1449,8 +1451,8 @@ static long move_pages_ptes(struct mm_struct *mm, pmd_t *dst_pmd, pmd_t *src_pmd\n \t\t\tgoto out;\n \t\t}\n \n-\t\tsi = get_swap_device(entry);\n-\t\tif (unlikely(!si)) {\n+\t\tswapoff_locked = tryget_swap_entry(entry, &si);\n+\t\tif (unlikely(!swapoff_locked)) {\n \t\t\tret = -EAGAIN;\n \t\t\tgoto out;\n \t\t}\n@@ -1480,8 +1482,9 @@ static long move_pages_ptes(struct mm_struct *mm, pmd_t *dst_pmd, pmd_t *src_pmd\n \t\t\t\tpte_unmap(src_pte);\n \t\t\t\tpte_unmap(dst_pte);\n \t\t\t\tsrc_pte = dst_pte = NULL;\n-\t\t\t\tput_swap_device(si);\n+\t\t\t\tput_swap_entry(entry, si);\n \t\t\t\tsi = NULL;\n+\t\t\t\tswapoff_locked = false;\n \t\t\t\t/* now we can block and wait */\n \t\t\t\tfolio_lock(src_folio);\n \t\t\t\tgoto retry;\n@@ -1507,8 +1510,8 @@ static long move_pages_ptes(struct mm_struct *mm, pmd_t *dst_pmd, pmd_t *src_pmd\n \tif (dst_pte)\n \t\tpte_unmap(dst_pte);\n \tmmu_notifier_invalidate_range_end(&range);\n-\tif (si)\n-\t\tput_swap_device(si);\n+\tif (swapoff_locked)\n+\t\tput_swap_entry(entry, si);\n \n \treturn ret;\n }\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex ac9b7a60736bc..315e4d0d08311 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -1009,14 +1009,13 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \tint ret = 0;\n \n \t/* try to allocate swap cache folio */\n-\tsi = get_swap_device(swpentry);\n-\tif (!si)\n+\tif (!tryget_swap_entry(swpentry, &si))\n \t\treturn -EEXIST;\n \n \tmpol = get_task_policy(current);\n \tfolio = __read_swap_cache_async(swpentry, GFP_KERNEL, mpol,\n \t\t\tNO_INTERLEAVE_INDEX, &folio_was_allocated, true);\n-\tput_swap_device(si);\n+\tput_swap_entry(swpentry, si);\n \tif (!folio)\n \t\treturn -ENOMEM;\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that the vswap free path needs to be restructured to improve batching behavior and agreed to make changes in a future version of the patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged",
                "agreed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add new helper functions to abstract away zswap entry operations, in\norder to facilitate re-implementing these functions when swap is\nvirtualized.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n mm/zswap.c | 59 ++++++++++++++++++++++++++++++++++++------------------\n 1 file changed, 40 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex 315e4d0d08311..a5a3f068bd1a6 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -234,6 +234,38 @@ static inline struct xarray *swap_zswap_tree(swp_entry_t swp)",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the batching behavior of vswap free path, acknowledged that the current implementation is inefficient and agreed to restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged inefficiency",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "}\n \n+static inline void *zswap_entry_store(swp_entry_t swpentry,\n+\t\tstruct zswap_entry *entry)\n+{\n+\tstruct xarray *tree = swap_zswap_tree(swpentry);\n+\tpgoff_t offset = swp_offset(swpentry);\n+\n+\treturn xa_store(tree, offset, entry, GFP_KERNEL);\n+}\n+\n+static inline void *zswap_entry_load(swp_entry_t swpentry)\n+{\n+\tstruct xarray *tree = swap_zswap_tree(swpentry);\n+\tpgoff_t offset = swp_offset(swpentry);\n+\n+\treturn xa_load(tree, offset);\n+}\n+\n+static inline void *zswap_entry_erase(swp_entry_t swpentry)\n+{\n+\tstruct xarray *tree = swap_zswap_tree(swpentry);\n+\tpgoff_t offset = swp_offset(swpentry);\n+\n+\treturn xa_erase(tree, offset);\n+}\n+\n+static inline bool zswap_empty(swp_entry_t swpentry)\n+{\n+\tstruct xarray *tree = swap_zswap_tree(swpentry);\n+\n+\treturn xa_empty(tree);\n+}\n+\n #define zswap_pool_debug(msg, p)\t\t\t\\\n \tpr_debug(\"%s pool %s\\n\", msg, (p)->tfm_name)\n \n@@ -1000,8 +1032,6 @@ static bool zswap_decompress(struct zswap_entry *entry, struct folio *folio)\n static int zswap_writeback_entry(struct zswap_entry *entry,\n \t\t\t\t swp_entry_t swpentry)\n {\n-\tstruct xarray *tree;\n-\tpgoff_t offset = swp_offset(swpentry);\n \tstruct folio *folio;\n \tstruct mempolicy *mpol;\n \tbool folio_was_allocated;\n@@ -1040,8 +1070,7 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \t * old compressed data. Only when this is successful can the entry\n \t * be dereferenced.\n \t */\n-\ttree = swap_zswap_tree(swpentry);\n-\tif (entry != xa_load(tree, offset)) {\n+\tif (entry != zswap_entry_load(swpentry)) {\n \t\tret = -ENOMEM;\n \t\tgoto out;\n \t}\n@@ -1051,7 +1080,7 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \t\tgoto out;\n \t}\n \n-\txa_erase(tree, offset);\n+\tzswap_entry_erase(swpentry);\n \n \tcount_vm_event(ZSWPWB);\n \tif (entry->objcg)\n@@ -1427,9 +1456,7 @@ static bool zswap_store_page(struct page *page,\n \tif (!zswap_compress(page, entry, pool))\n \t\tgoto compress_failed;\n \n-\told = xa_store(swap_zswap_tree(page_swpentry),\n-\t\t       swp_offset(page_swpentry),\n-\t\t       entry, GFP_KERNEL);\n+\told = zswap_entry_store(page_swpentry, entry);\n \tif (xa_is_err(old)) {\n \t\tint err = xa_err(old);\n \n@@ -1563,11 +1590,9 @@ bool zswap_store(struct folio *folio)\n \t\tunsigned type = swp_type(swp);\n \t\tpgoff_t offset = swp_offset(swp);\n \t\tstruct zswap_entry *entry;\n-\t\tstruct xarray *tree;\n \n \t\tfor (index = 0; index < nr_pages; ++index) {\n-\t\t\ttree = swap_zswap_tree(swp_entry(type, offset + index));\n-\t\t\tentry = xa_erase(tree, offset + index);\n+\t\t\tentry = zswap_entry_erase(swp_entry(type, offset + index));\n \t\t\tif (entry)\n \t\t\t\tzswap_entry_free(entry);\n \t\t}\n@@ -1599,9 +1624,7 @@ bool zswap_store(struct folio *folio)\n int zswap_load(struct folio *folio)\n {\n \tswp_entry_t swp = folio->swap;\n-\tpgoff_t offset = swp_offset(swp);\n \tbool swapcache = folio_test_swapcache(folio);\n-\tstruct xarray *tree = swap_zswap_tree(swp);\n \tstruct zswap_entry *entry;\n \n \tVM_WARN_ON_ONCE(!folio_test_locked(folio));\n@@ -1619,7 +1642,7 @@ int zswap_load(struct folio *folio)\n \t\treturn -EINVAL;\n \t}\n \n-\tentry = xa_load(tree, offset);\n+\tentry = zswap_entry_load(swp);\n \tif (!entry)\n \t\treturn -ENOENT;\n \n@@ -1648,7 +1671,7 @@ int zswap_load(struct folio *folio)\n \t */\n \tif (swapcache) {\n \t\tfolio_mark_dirty(folio);\n-\t\txa_erase(tree, offset);\n+\t\tzswap_entry_erase(swp);\n \t\tzswap_entry_free(entry);\n \t}\n \n@@ -1658,14 +1681,12 @@ int zswap_load(struct folio *folio)\n \n void zswap_invalidate(swp_entry_t swp)\n {\n-\tpgoff_t offset = swp_offset(swp);\n-\tstruct xarray *tree = swap_zswap_tree(swp);\n \tstruct zswap_entry *entry;\n \n-\tif (xa_empty(tree))\n+\tif (zswap_empty(swp))\n \t\treturn;\n \n-\tentry = xa_erase(tree, offset);\n+\tentry = zswap_entry_erase(swp);\n \tif (entry)\n \t\tzswap_entry_free(entry);\n }\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author is addressing a concern about the swapoff path needing to drop the per-vswap spinlock before calling try_to_unmap(). The author agrees that this is necessary and will restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges fix needed",
                "agrees with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Userfaultfd checks whether a swap entry is in swapcache. This is\ncurrently done by directly looking at the swapfile's swap map - however,\nthe swap cached state will soon be managed at the virtual swap layer.\nAbstract away this function.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h |  6 ++++++\n mm/swapfile.c        | 15 +++++++++++++++\n mm/userfaultfd.c     |  3 +--\n 3 files changed, 22 insertions(+), 2 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 3da637b218baf..f91a442ac0e82 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -435,6 +435,7 @@ void free_swap_and_cache_nr(swp_entry_t entry, int nr);\n int __swap_count(swp_entry_t entry);\n bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry);\n int swp_swapcount(swp_entry_t entry);\n+bool is_swap_cached(swp_entry_t entry);\n \n /* Swap cache API (mm/swap_state.c) */\n static inline unsigned long total_swapcache_pages(void)\n@@ -554,6 +555,11 @@ static inline int swp_swapcount(swp_entry_t entry)\n \treturn 0;\n }\n \n+static inline bool is_swap_cached(swp_entry_t entry)\n+{\n+\treturn false;\n+}\n+\n static inline int folio_alloc_swap(struct folio *folio)\n {\n \treturn -EINVAL;\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex cacfafa9a540d..3c89dedbd5718 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -194,6 +194,21 @@ static bool swap_only_has_cache(struct swap_info_struct *si,\n \treturn true;\n }\n \n+/**\n+ * is_swap_cached - check if the swap entry is cached\n+ * @entry: swap entry to check\n+ *\n+ * Check swap_map directly to minimize overhead, READ_ONCE is sufficient.\n+ *\n+ * Returns true if the swap entry is cached, false otherwise.\n+ */\n+bool is_swap_cached(swp_entry_t entry)\n+{\n+\tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n+\n+\treturn READ_ONCE(si->swap_map[swp_offset(entry)]) & SWAP_HAS_CACHE;\n+}\n+\n static bool swap_is_last_map(struct swap_info_struct *si,\n \t\tunsigned long offset, int nr_pages, bool *has_cache)\n {\ndiff --git a/mm/userfaultfd.c b/mm/userfaultfd.c\nindex 25f89eba0438c..98be764fb3ecd 100644\n--- a/mm/userfaultfd.c\n+++ b/mm/userfaultfd.c\n@@ -1190,7 +1190,6 @@ static int move_swap_pte(struct mm_struct *mm, struct vm_area_struct *dst_vma,\n \t\t * Check if the swap entry is cached after acquiring the src_pte\n \t\t * lock. Otherwise, we might miss a newly loaded swap cache folio.\n \t\t *\n-\t\t * Check swap_map directly to minimize overhead, READ_ONCE is sufficient.\n \t\t * We are trying to catch newly added swap cache, the only possible case is\n \t\t * when a folio is swapped in and out again staying in swap cache, using the\n \t\t * same entry before the PTE check above. The PTL is acquired and released\n@@ -1200,7 +1199,7 @@ static int move_swap_pte(struct mm_struct *mm, struct vm_area_struct *dst_vma,\n \t\t * cache, or during the tiny synchronization window between swap cache and\n \t\t * swap_map, but it will be gone very quickly, worst result is retry jitters.\n \t\t */\n-\t\tif (READ_ONCE(si->swap_map[swp_offset(entry)]) & SWAP_HAS_CACHE) {\n+\t\tif (is_swap_cached(entry)) {\n \t\t\tdouble_pt_unlock(dst_ptl, src_ptl);\n \t\t\treturn -EAGAIN;\n \t\t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author addressed a concern about the swapoff path needing to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, but this patch is unrelated and only sets up virtual swap debugfs directory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no behavioral change intended",
                "unrelated patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "In prepration for the implementation of swap virtualization, add new\nscaffolds for the new code: a new mm/vswap.c source file, which\ncurrently only holds the logic to set up the (for now, empty) vswap\ndebugfs directory. Hook this up in the swap setup step in\nmm/swap_state.c, and set up vswap compilation in the Makefile.\n\nOther than the debugfs directory, no behavioral change intended.\n\nFinally, make Johannes a swap reviewer, given that he has contributed\nmajorly to the developments of virtual swap.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n MAINTAINERS          |  2 ++\n include/linux/swap.h |  2 ++\n mm/Makefile          |  2 +-\n mm/swap_state.c      |  6 ++++++\n mm/vswap.c           | 35 +++++++++++++++++++++++++++++++++++\n 5 files changed, 46 insertions(+), 1 deletion(-)\n create mode 100644 mm/vswap.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex e087673237636..b21038b160a07 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16728,6 +16728,7 @@ R:\tKemeng Shi <shikemeng@huaweicloud.com>\n R:\tNhat Pham <nphamcs@gmail.com>\n R:\tBaoquan He <bhe@redhat.com>\n R:\tBarry Song <baohua@kernel.org>\n+R:\tJohannes Weiner <hannes@cmpxchg.org>\n L:\tlinux-mm@kvack.org\n S:\tMaintained\n F:\tDocumentation/mm/swap-table.rst\n@@ -16740,6 +16741,7 @@ F:\tmm/swap.h\n F:\tmm/swap_table.h\n F:\tmm/swap_state.c\n F:\tmm/swapfile.c\n+F:\tmm/vswap.c\n \n MEMORY MANAGEMENT - THP (TRANSPARENT HUGE PAGE)\n M:\tAndrew Morton <akpm@linux-foundation.org>\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 918b47da55f44..1ff463fb3a966 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -669,6 +669,8 @@ static inline bool mem_cgroup_swap_full(struct folio *folio)\n }\n #endif\n \n+int vswap_init(void);\n+\n /**\n  * swp_entry_to_swp_slot - look up the physical swap slot corresponding to a\n  *                         virtual swap slot.\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 2d0570a16e5be..67fa4586e7e18 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -75,7 +75,7 @@ ifdef CONFIG_MMU\n \tobj-$(CONFIG_ADVISE_SYSCALLS)\t+= madvise.o\n endif\n \n-obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o\n+obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o vswap.o\n obj-$(CONFIG_ZSWAP)\t+= zswap.o\n obj-$(CONFIG_HAS_DMA)\t+= dmapool.o\n obj-$(CONFIG_HUGETLBFS)\t+= hugetlb.o hugetlb_sysfs.o hugetlb_sysctl.o\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex e2e9f55bea3bb..29ec666be4204 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -882,6 +882,12 @@ static int __init swap_init(void)\n \tint err;\n \tstruct kobject *swap_kobj;\n \n+\terr = vswap_init();\n+\tif (err) {\n+\t\tpr_err(\"failed to initialize virtual swap space\\n\");\n+\t\treturn err;\n+\t}\n+\n \tswap_kobj = kobject_create_and_add(\"swap\", mm_kobj);\n \tif (!swap_kobj) {\n \t\tpr_err(\"failed to create swap kobject\\n\");\ndiff --git a/mm/vswap.c b/mm/vswap.c\nnew file mode 100644\nindex 0000000000000..e68234f053fc9\n--- /dev/null\n+++ b/mm/vswap.c\n@@ -0,0 +1,35 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * Virtual swap space\n+ *\n+ * Copyright (C) 2024 Meta Platforms, Inc., Nhat Pham\n+ */\n+#include <linux/swap.h>\n+\n+#ifdef CONFIG_DEBUG_FS\n+#include <linux/debugfs.h>\n+\n+static struct dentry *vswap_debugfs_root;\n+\n+static int vswap_debug_fs_init(void)\n+{\n+\tif (!debugfs_initialized())\n+\t\treturn -ENODEV;\n+\n+\tvswap_debugfs_root = debugfs_create_dir(\"vswap\", NULL);\n+\treturn 0;\n+}\n+#else\n+static int vswap_debug_fs_init(void)\n+{\n+\treturn 0;\n+}\n+#endif\n+\n+int vswap_init(void)\n+{\n+\tif (vswap_debug_fs_init())\n+\t\tpr_warn(\"Failed to initialize vswap debugfs\\n\");\n+\n+\treturn 0;\n+}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the need to separate logical and physical swap slot representations by introducing new types (swp_entry_t and swp_slot_t) and renaming functions that operate at the physical level. The author confirmed that no behavioral change was made, but acknowledged that this is just the first step in preparing for swap virtualization.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "confirmed no behavioral change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "In preparation for swap virtualization, add a new type to represent the\nphysical swap slots of swapfile. This allows us to separates:\n\n1. The logical view of the swap entry (i.e what is stored in page table\n   entries and used to index into the swap cache), represented by the\n   old swp_entry_t type.\n\nfrom:\n\n2. Its physical backing state (i.e the actual backing slot on the swap\n   device), represented by the new swp_slot_t type.\n\nThe functions that operate at the physical level (i.e on the swp_slot_t\ntypes) are also renamed where appropriate (prefixed with swp_slot_* for\ne.g).\n\nNote that we have not made any behavioral change - the mapping between\nthe two types is the identity mapping. In later patches, we shall\ndynamically allocate a virtual swap slot (of type swp_entry_t) for each\nswapped out page to store in the page table entry, and associate it with\na backing store. A physical swap slot (i.e a slot on a physical swap\ndevice) is one of the backing options.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/mm_types.h |  16 +++\n include/linux/swap.h     |  47 ++++--\n include/linux/swapops.h  |  25 ++++\n kernel/power/swap.c      |   6 +-\n mm/internal.h            |  10 +-\n mm/page_io.c             |  33 +++--\n mm/shmem.c               |  19 ++-\n mm/swap.h                |  52 +++----\n mm/swap_cgroup.c         |  18 +--\n mm/swap_state.c          |  32 +++--\n mm/swapfile.c            | 300 ++++++++++++++++++++++-----------------\n 11 files changed, 352 insertions(+), 206 deletions(-)\n\ndiff --git a/include/linux/mm_types.h b/include/linux/mm_types.h\nindex 78950eb8926dc..bffde812decc5 100644\n--- a/include/linux/mm_types.h\n+++ b/include/linux/mm_types.h\n@@ -279,6 +279,13 @@ static __always_inline unsigned long encoded_nr_pages(struct encoded_page *page)\n }\n \n /*\n+ * Virtual swap slot.\n+ *\n+ * This type is used to represent a virtual swap slot, i.e an identifier of\n+ * a swap entry. This is stored in PTEs that originally refer to the swapped\n+ * out page, and is used to index into various swap architectures (swap cache,\n+ * zswap tree, swap cgroup array, etc.).\n+ *\n  * A swap entry has to fit into a \"unsigned long\", as the entry is hidden\n  * in the \"index\" field of the swapper address space.\n  */\n@@ -286,6 +293,15 @@ typedef struct {\n \tunsigned long val;\n } swp_entry_t;\n \n+/*\n+ * Physical swap slot.\n+ *\n+ * This type is used to represent a PAGE_SIZED slot on a swapfile.\n+ */\n+typedef struct {\n+\tunsigned long val;\n+} swp_slot_t;\n+\n /**\n  * typedef softleaf_t - Describes a page table software leaf entry, abstracted\n  * from its architecture-specific encoding.\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex f91a442ac0e82..918b47da55f44 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -241,7 +241,7 @@ enum {\n  * cluster to which it belongs being marked free. Therefore 0 is safe to use as\n  * a sentinel to indicate an entry is not valid.\n  */\n-#define SWAP_ENTRY_INVALID\t0\n+#define SWAP_SLOT_INVALID\t0\n \n #ifdef CONFIG_THP_SWAP\n #define SWAP_NR_ORDERS\t\t(PMD_ORDER + 1)\n@@ -442,11 +442,14 @@ static inline unsigned long total_swapcache_pages(void)\n {\n \treturn global_node_page_state(NR_SWAPCACHE);\n }\n+\n void free_folio_and_swap_cache(struct folio *folio);\n void free_pages_and_swap_cache(struct encoded_page **, int);\n void free_swap_cache(struct folio *folio);\n \n /* Physical swap allocator and swap device API (mm/swapfile.c) */\n+void swap_slot_free_nr(swp_slot_t slot, int nr_pages);\n+\n int add_swap_extent(struct swap_info_struct *sis, unsigned long start_page,\n \t\tunsigned long nr_pages, sector_t start_block);\n int generic_swapfile_activate(struct swap_info_struct *, struct file *,\n@@ -468,28 +471,28 @@ static inline long get_nr_swap_pages(void)\n }\n \n void si_swapinfo(struct sysinfo *);\n-swp_entry_t get_swap_page_of_type(int);\n+swp_slot_t swap_slot_alloc_of_type(int);\n int add_swap_count_continuation(swp_entry_t, gfp_t);\n int swap_type_of(dev_t device, sector_t offset);\n int find_first_swap(dev_t *device);\n unsigned int count_swap_pages(int, int);\n sector_t swapdev_block(int, pgoff_t);\n struct backing_dev_info;\n-struct swap_info_struct *get_swap_device(swp_entry_t entry);\n+struct swap_info_struct *swap_slot_tryget_swap_info(swp_slot_t slot);\n sector_t swap_folio_sector(struct folio *folio);\n \n-static inline void put_swap_device(struct swap_info_struct *si)\n+static inline void swap_slot_put_swap_info(struct swap_info_struct *si)\n {\n \tpercpu_ref_put(&si->users);\n }\n \n #else /* CONFIG_SWAP */\n-static inline struct swap_info_struct *get_swap_device(swp_entry_t entry)\n+static inline struct swap_info_struct *swap_slot_tryget_swap_info(swp_slot_t slot)\n {\n \treturn NULL;\n }\n \n-static inline void put_swap_device(struct swap_info_struct *si)\n+static inline void swap_slot_put_swap_info(struct swap_info_struct *si)\n {\n }\n \n@@ -536,7 +539,7 @@ static inline void swap_free_nr(swp_entry_t entry, int nr_pages)\n {\n }\n \n-static inline void put_swap_folio(struct folio *folio, swp_entry_t swp)\n+static inline void put_swap_folio(struct folio *folio, swp_entry_t entry)\n {\n }\n \n@@ -576,6 +579,7 @@ static inline int add_swap_extent(struct swap_info_struct *sis,\n {\n \treturn -EINVAL;\n }\n+\n #endif /* CONFIG_SWAP */\n \n static inline void free_swap_and_cache(swp_entry_t entry)\n@@ -665,10 +669,35 @@ static inline bool mem_cgroup_swap_full(struct folio *folio)\n }\n #endif\n \n+/**\n+ * swp_entry_to_swp_slot - look up the physical swap slot corresponding to a\n+ *                         virtual swap slot.\n+ * @entry: the virtual swap slot.\n+ *\n+ * Return: the physical swap slot corresponding to the virtual swap slot.\n+ */\n+static inline swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry)\n+{\n+\treturn (swp_slot_t) { entry.val };\n+}\n+\n+/**\n+ * swp_slot_to_swp_entry - look up the virtual swap slot corresponding to a\n+ *                         physical swap slot.\n+ * @slot: the physical swap slot.\n+ *\n+ * Return: the virtual swap slot corresponding to the physical swap slot.\n+ */\n+static inline swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot)\n+{\n+\treturn (swp_entry_t) { slot.val };\n+}\n+\n static inline bool tryget_swap_entry(swp_entry_t entry,\n \t\t\t\tstruct swap_info_struct **sip)\n {\n-\tstruct swap_info_struct *si = get_swap_device(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tstruct swap_info_struct *si = swap_slot_tryget_swap_info(slot);\n \n \tif (sip)\n \t\t*sip = si;\n@@ -679,7 +708,7 @@ static inline bool tryget_swap_entry(swp_entry_t entry,\n static inline void put_swap_entry(swp_entry_t entry,\n \t\t\t\tstruct swap_info_struct *si)\n {\n-\tput_swap_device(si);\n+\tswap_slot_put_swap_info(si);\n }\n \n #endif /* __KERNEL__*/\ndiff --git a/include/linux/swapops.h b/include/linux/swapops.h\nindex 8cfc966eae48e..9e41c35664a95 100644\n--- a/include/linux/swapops.h\n+++ b/include/linux/swapops.h\n@@ -360,5 +360,30 @@ static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)\n \n #endif  /* CONFIG_ARCH_ENABLE_THP_MIGRATION */\n \n+/* Physical swap slots operations */\n+\n+/*\n+ * Store a swap device type + offset into a swp_slot_t handle.\n+ */\n+static inline swp_slot_t swp_slot(unsigned long type, pgoff_t offset)\n+{\n+\tswp_slot_t ret;\n+\n+\tret.val = (type << SWP_TYPE_SHIFT) | (offset & SWP_OFFSET_MASK);\n+\treturn ret;\n+}\n+\n+/* Extract the `type' field from a swp_slot_t. */\n+static inline unsigned swp_slot_type(swp_slot_t slot)\n+{\n+\treturn (slot.val >> SWP_TYPE_SHIFT);\n+}\n+\n+/* Extract the `offset' field from a swp_slot_t. */\n+static inline pgoff_t swp_slot_offset(swp_slot_t slot)\n+{\n+\treturn slot.val & SWP_OFFSET_MASK;\n+}\n+\n #endif /* CONFIG_MMU */\n #endif /* _LINUX_SWAPOPS_H */\ndiff --git a/kernel/power/swap.c b/kernel/power/swap.c\nindex 8050e51828351..0129c5ffa649d 100644\n--- a/kernel/power/swap.c\n+++ b/kernel/power/swap.c\n@@ -174,10 +174,10 @@ sector_t alloc_swapdev_block(int swap)\n \t * Allocate a swap page and register that it has been allocated, so that\n \t * it can be freed in case of an error.\n \t */\n-\toffset = swp_offset(get_swap_page_of_type(swap));\n+\toffset = swp_slot_offset(swap_slot_alloc_of_type(swap));\n \tif (offset) {\n \t\tif (swsusp_extents_insert(offset))\n-\t\t\tswap_free(swp_entry(swap, offset));\n+\t\t\tswap_slot_free_nr(swp_slot(swap, offset), 1);\n \t\telse\n \t\t\treturn swapdev_block(swap, offset);\n \t}\n@@ -197,7 +197,7 @@ void free_all_swap_pages(int swap)\n \n \t\text = rb_entry(node, struct swsusp_extent, node);\n \t\trb_erase(node, &swsusp_extents);\n-\t\tswap_free_nr(swp_entry(swap, ext->start),\n+\t\tswap_slot_free_nr(swp_slot(swap, ext->start),\n \t\t\t     ext->end - ext->start + 1);\n \n \t\tkfree(ext);\ndiff --git a/mm/internal.h b/mm/internal.h\nindex f35dbcf99a86b..e739e8cac5b55 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -334,9 +334,13 @@ unsigned int folio_pte_batch(struct folio *folio, pte_t *ptep, pte_t pte,\n  */\n static inline pte_t pte_move_swp_offset(pte_t pte, long delta)\n {\n-\tconst softleaf_t entry = softleaf_from_pte(pte);\n-\tpte_t new = __swp_entry_to_pte(__swp_entry(swp_type(entry),\n-\t\t\t\t\t\t   (swp_offset(entry) + delta)));\n+\tsoftleaf_t entry = softleaf_from_pte(pte), new_entry;\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tpte_t new;\n+\n+\tnew_entry = swp_slot_to_swp_entry(swp_slot(swp_slot_type(slot),\n+\t\t\tswp_slot_offset(slot) + delta));\n+\tnew = swp_entry_to_pte(new_entry);\n \n \tif (pte_swp_soft_dirty(pte))\n \t\tnew = pte_swp_mksoft_dirty(new);\ndiff --git a/mm/page_io.c b/mm/page_io.c\nindex 3c342db77ce38..0b02bcc85e2a8 100644\n--- a/mm/page_io.c\n+++ b/mm/page_io.c\n@@ -204,14 +204,17 @@ static bool is_folio_zero_filled(struct folio *folio)\n static void swap_zeromap_folio_set(struct folio *folio)\n {\n \tstruct obj_cgroup *objcg = get_obj_cgroup_from_folio(folio);\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tstruct swap_info_struct *sis =\n+\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n \tint nr_pages = folio_nr_pages(folio);\n \tswp_entry_t entry;\n+\tswp_slot_t slot;\n \tunsigned int i;\n \n \tfor (i = 0; i < folio_nr_pages(folio); i++) {\n \t\tentry = page_swap_entry(folio_page(folio, i));\n-\t\tset_bit(swp_offset(entry), sis->zeromap);\n+\t\tslot = swp_entry_to_swp_slot(entry);\n+\t\tset_bit(swp_slot_offset(slot), sis->zeromap);\n \t}\n \n \tcount_vm_events(SWPOUT_ZERO, nr_pages);\n@@ -223,13 +226,16 @@ static void swap_zeromap_folio_set(struct folio *folio)\n \n static void swap_zeromap_folio_clear(struct folio *folio)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tstruct swap_info_struct *sis =\n+\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n \tswp_entry_t entry;\n+\tswp_slot_t slot;\n \tunsigned int i;\n \n \tfor (i = 0; i < folio_nr_pages(folio); i++) {\n \t\tentry = page_swap_entry(folio_page(folio, i));\n-\t\tclear_bit(swp_offset(entry), sis->zeromap);\n+\t\tslot = swp_entry_to_swp_slot(entry);\n+\t\tclear_bit(swp_slot_offset(slot), sis->zeromap);\n \t}\n }\n \n@@ -357,7 +363,8 @@ static void sio_write_complete(struct kiocb *iocb, long ret)\n \t\t * messages.\n \t\t */\n \t\tpr_err_ratelimited(\"Write error %ld on dio swapfile (%llu)\\n\",\n-\t\t\t\t   ret, swap_dev_pos(page_swap_entry(page)));\n+\t\t\t\t   ret,\n+\t\t\t\t   swap_slot_pos(swp_entry_to_swp_slot(page_swap_entry(page))));\n \t\tfor (p = 0; p < sio->pages; p++) {\n \t\t\tpage = sio->bvec[p].bv_page;\n \t\t\tset_page_dirty(page);\n@@ -374,9 +381,10 @@ static void sio_write_complete(struct kiocb *iocb, long ret)\n static void swap_writepage_fs(struct folio *folio, struct swap_iocb **swap_plug)\n {\n \tstruct swap_iocb *sio = swap_plug ? *swap_plug : NULL;\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(folio->swap);\n+\tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n \tstruct file *swap_file = sis->swap_file;\n-\tloff_t pos = swap_dev_pos(folio->swap);\n+\tloff_t pos = swap_slot_pos(slot);\n \n \tcount_swpout_vm_event(folio);\n \tfolio_start_writeback(folio);\n@@ -446,7 +454,8 @@ static void swap_writepage_bdev_async(struct folio *folio,\n \n void __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tstruct swap_info_struct *sis =\n+\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n \n \tVM_BUG_ON_FOLIO(!folio_test_swapcache(folio), folio);\n \t/*\n@@ -537,9 +546,10 @@ static bool swap_read_folio_zeromap(struct folio *folio)\n \n static void swap_read_folio_fs(struct folio *folio, struct swap_iocb **plug)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(folio->swap);\n+\tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n \tstruct swap_iocb *sio = NULL;\n-\tloff_t pos = swap_dev_pos(folio->swap);\n+\tloff_t pos = swap_slot_pos(slot);\n \n \tif (plug)\n \t\tsio = *plug;\n@@ -608,7 +618,8 @@ static void swap_read_folio_bdev_async(struct folio *folio,\n \n void swap_read_folio(struct folio *folio, struct swap_iocb **plug)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tstruct swap_info_struct *sis =\n+\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n \tbool synchronous = sis->flags & SWP_SYNCHRONOUS_IO;\n \tbool workingset = folio_test_workingset(folio);\n \tunsigned long pflags;\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex b40be22fa5f09..400e2fa8e77cb 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -1442,6 +1442,7 @@ static unsigned int shmem_find_swap_entries(struct address_space *mapping,\n \tXA_STATE(xas, &mapping->i_pages, start);\n \tstruct folio *folio;\n \tswp_entry_t entry;\n+\tswp_slot_t slot;\n \n \trcu_read_lock();\n \txas_for_each(&xas, folio, ULONG_MAX) {\n@@ -1452,11 +1453,13 @@ static unsigned int shmem_find_swap_entries(struct address_space *mapping,\n \t\t\tcontinue;\n \n \t\tentry = radix_to_swp_entry(folio);\n+\t\tslot = swp_entry_to_swp_slot(entry);\n+\n \t\t/*\n \t\t * swapin error entries can be found in the mapping. But they're\n \t\t * deliberately ignored here as we've done everything we can do.\n \t\t */\n-\t\tif (swp_type(entry) != type)\n+\t\tif (swp_slot_type(slot) != type)\n \t\t\tcontinue;\n \n \t\tindices[folio_batch_count(fbatch)] = xas.xa_index;\n@@ -2224,6 +2227,7 @@ static int shmem_split_large_entry(struct inode *inode, pgoff_t index,\n \tXA_STATE_ORDER(xas, &mapping->i_pages, index, 0);\n \tint split_order = 0;\n \tint i;\n+\tswp_slot_t slot = swp_entry_to_swp_slot(swap);\n \n \t/* Convert user data gfp flags to xarray node gfp flags */\n \tgfp &= GFP_RECLAIM_MASK;\n@@ -2264,13 +2268,16 @@ static int shmem_split_large_entry(struct inode *inode, pgoff_t index,\n \t\t\t */\n \t\t\tfor (i = 0; i < 1 << cur_order;\n \t\t\t     i += (1 << split_order)) {\n-\t\t\t\tswp_entry_t tmp;\n+\t\t\t\tswp_entry_t tmp_entry;\n+\t\t\t\tswp_slot_t tmp_slot;\n+\n+\t\t\t\ttmp_slot =\n+\t\t\t\t\tswp_slot(swp_slot_type(slot),\n+\t\t\t\t\t\tswp_slot_offset(slot) + swap_offset + i);\n+\t\t\t\ttmp_entry = swp_slot_to_swp_entry(tmp_slot);\n \n-\t\t\t\ttmp = swp_entry(swp_type(swap),\n-\t\t\t\t\t\tswp_offset(swap) + swap_offset +\n-\t\t\t\t\t\t\ti);\n \t\t\t\t__xa_store(&mapping->i_pages, aligned_index + i,\n-\t\t\t\t\t   swp_to_radix_entry(tmp), 0);\n+\t\t\t\t\t   swp_to_radix_entry(tmp_entry), 0);\n \t\t\t}\n \t\t\tcur_order = split_order;\n \t\t\tsplit_order = xas_try_split_min_order(split_order);\ndiff --git a/mm/swap.h b/mm/swap.h\nindex 8726b587a5b5d..bdf7aca146643 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -10,10 +10,10 @@ extern int page_cluster;\n \n #ifdef CONFIG_THP_SWAP\n #define SWAPFILE_CLUSTER\tHPAGE_PMD_NR\n-#define swap_entry_order(order)\t(order)\n+#define swap_slot_order(order)\t(order)\n #else\n #define SWAPFILE_CLUSTER\t256\n-#define swap_entry_order(order)\t0\n+#define swap_slot_order(order)\t0\n #endif\n \n extern struct swap_info_struct *swap_info[];\n@@ -57,9 +57,9 @@ enum swap_cluster_flags {\n #include <linux/swapops.h> /* for swp_offset */\n #include <linux/blk_types.h> /* for bio_end_io_t */\n \n-static inline unsigned int swp_cluster_offset(swp_entry_t entry)\n+static inline unsigned int swp_cluster_offset(swp_slot_t slot)\n {\n-\treturn swp_offset(entry) % SWAPFILE_CLUSTER;\n+\treturn swp_slot_offset(slot) % SWAPFILE_CLUSTER;\n }\n \n /*\n@@ -75,9 +75,9 @@ static inline struct swap_info_struct *__swap_type_to_info(int type)\n \treturn si;\n }\n \n-static inline struct swap_info_struct *__swap_entry_to_info(swp_entry_t entry)\n+static inline struct swap_info_struct *__swap_slot_to_info(swp_slot_t slot)\n {\n-\treturn __swap_type_to_info(swp_type(entry));\n+\treturn __swap_type_to_info(swp_slot_type(slot));\n }\n \n static inline struct swap_cluster_info *__swap_offset_to_cluster(\n@@ -88,10 +88,10 @@ static inline struct swap_cluster_info *__swap_offset_to_cluster(\n \treturn &si->cluster_info[offset / SWAPFILE_CLUSTER];\n }\n \n-static inline struct swap_cluster_info *__swap_entry_to_cluster(swp_entry_t entry)\n+static inline struct swap_cluster_info *__swap_slot_to_cluster(swp_slot_t slot)\n {\n-\treturn __swap_offset_to_cluster(__swap_entry_to_info(entry),\n-\t\t\t\t\tswp_offset(entry));\n+\treturn __swap_offset_to_cluster(__swap_slot_to_info(slot),\n+\t\t\t\t\tswp_slot_offset(slot));\n }\n \n static __always_inline struct swap_cluster_info *__swap_cluster_lock(\n@@ -120,7 +120,7 @@ static __always_inline struct swap_cluster_info *__swap_cluster_lock(\n /**\n  * swap_cluster_lock - Lock and return the swap cluster of given offset.\n  * @si: swap device the cluster belongs to.\n- * @offset: the swap entry offset, pointing to a valid slot.\n+ * @offset: the swap slot offset, pointing to a valid slot.\n  *\n  * Context: The caller must ensure the offset is in the valid range and\n  * protect the swap device with reference count or locks.\n@@ -134,10 +134,12 @@ static inline struct swap_cluster_info *swap_cluster_lock(\n static inline struct swap_cluster_info *__swap_cluster_get_and_lock(\n \t\tconst struct folio *folio, bool irq)\n {\n+\tswp_slot_t slot = swp_entry_to_swp_slot(folio->swap);\n+\n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n-\treturn __swap_cluster_lock(__swap_entry_to_info(folio->swap),\n-\t\t\t\t   swp_offset(folio->swap), irq);\n+\treturn __swap_cluster_lock(__swap_slot_to_info(slot),\n+\t\t\t\t   swp_slot_offset(slot), irq);\n }\n \n /*\n@@ -209,12 +211,10 @@ static inline struct address_space *swap_address_space(swp_entry_t entry)\n \treturn &swap_space;\n }\n \n-/*\n- * Return the swap device position of the swap entry.\n- */\n-static inline loff_t swap_dev_pos(swp_entry_t entry)\n+/* Return the swap device position of the swap slot. */\n+static inline loff_t swap_slot_pos(swp_slot_t slot)\n {\n-\treturn ((loff_t)swp_offset(entry)) << PAGE_SHIFT;\n+\treturn ((loff_t)swp_slot_offset(slot)) << PAGE_SHIFT;\n }\n \n /**\n@@ -276,7 +276,9 @@ void swap_update_readahead(struct folio *folio, struct vm_area_struct *vma,\n \n static inline unsigned int folio_swap_flags(struct folio *folio)\n {\n-\treturn __swap_entry_to_info(folio->swap)->flags;\n+\tswp_slot_t swp_slot = swp_entry_to_swp_slot(folio->swap);\n+\n+\treturn __swap_slot_to_info(swp_slot)->flags;\n }\n \n /*\n@@ -287,8 +289,9 @@ static inline unsigned int folio_swap_flags(struct folio *folio)\n static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n \t\tbool *is_zeromap)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(entry);\n-\tunsigned long start = swp_offset(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n+\tunsigned long start = swp_slot_offset(slot);\n \tunsigned long end = start + max_nr;\n \tbool first_bit;\n \n@@ -306,8 +309,9 @@ static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n \n static inline int non_swapcache_batch(swp_entry_t entry, int max_nr)\n {\n-\tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n-\tpgoff_t offset = swp_offset(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tstruct swap_info_struct *si = __swap_slot_to_info(slot);\n+\tpgoff_t offset = swp_slot_offset(slot);\n \tint i;\n \n \t/*\n@@ -326,7 +330,7 @@ static inline int non_swapcache_batch(swp_entry_t entry, int max_nr)\n #else /* CONFIG_SWAP */\n struct swap_iocb;\n static inline struct swap_cluster_info *swap_cluster_lock(\n-\tstruct swap_info_struct *si, pgoff_t offset, bool irq)\n+\tstruct swap_info_struct *si, unsigned long offset)\n {\n \treturn NULL;\n }\n@@ -351,7 +355,7 @@ static inline void swap_cluster_unlock_irq(struct swap_cluster_info *ci)\n {\n }\n \n-static inline struct swap_info_struct *__swap_entry_to_info(swp_entry_t entry)\n+static inline struct swap_info_struct *__swap_slot_to_info(swp_slot_t slot)\n {\n \treturn NULL;\n }\ndiff --git a/mm/swap_cgroup.c b/mm/swap_cgroup.c\nindex de779fed8c210..77ce1d66c318d 100644\n--- a/mm/swap_cgroup.c\n+++ b/mm/swap_cgroup.c\n@@ -65,13 +65,14 @@ void swap_cgroup_record(struct folio *folio, unsigned short id,\n \t\t\tswp_entry_t ent)\n {\n \tunsigned int nr_ents = folio_nr_pages(folio);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(ent);\n \tstruct swap_cgroup *map;\n \tpgoff_t offset, end;\n \tunsigned short old;\n \n-\toffset = swp_offset(ent);\n+\toffset = swp_slot_offset(slot);\n \tend = offset + nr_ents;\n-\tmap = swap_cgroup_ctrl[swp_type(ent)].map;\n+\tmap = swap_cgroup_ctrl[swp_slot_type(slot)].map;\n \n \tdo {\n \t\told = __swap_cgroup_id_xchg(map, offset, id);\n@@ -92,13 +93,13 @@ void swap_cgroup_record(struct folio *folio, unsigned short id,\n  */\n unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents)\n {\n-\tpgoff_t offset, end;\n+\tswp_slot_t slot = swp_entry_to_swp_slot(ent);\n+\tpgoff_t offset = swp_slot_offset(slot);\n+\tpgoff_t end = offset + nr_ents;\n \tstruct swap_cgroup *map;\n \tunsigned short old, iter = 0;\n \n-\toffset = swp_offset(ent);\n-\tend = offset + nr_ents;\n-\tmap = swap_cgroup_ctrl[swp_type(ent)].map;\n+\tmap = swap_cgroup_ctrl[swp_slot_type(slot)].map;\n \n \tdo {\n \t\told = __swap_cgroup_id_xchg(map, offset, 0);\n@@ -119,12 +120,13 @@ unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents)\n unsigned short lookup_swap_cgroup_id(swp_entry_t ent)\n {\n \tstruct swap_cgroup_ctrl *ctrl;\n+\tswp_slot_t slot = swp_entry_to_swp_slot(ent);\n \n \tif (mem_cgroup_disabled())\n \t\treturn 0;\n \n-\tctrl = &swap_cgroup_ctrl[swp_type(ent)];\n-\treturn __swap_cgroup_id_lookup(ctrl->map, swp_offset(ent));\n+\tctrl = &swap_cgroup_ctrl[swp_slot_type(slot)];\n+\treturn __swap_cgroup_id_lookup(ctrl->map, swp_slot_offset(slot));\n }\n \n int swap_cgroup_swapon(int type, unsigned long max_pages)\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex bece18eb540fa..e2e9f55bea3bb 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -421,7 +421,8 @@ struct folio *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\tstruct mempolicy *mpol, pgoff_t ilx, bool *new_page_allocated,\n \t\tbool skip_if_exists)\n {\n-\tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n+\tstruct swap_info_struct *si =\n+\t\t__swap_slot_to_info(swp_entry_to_swp_slot(entry));\n \tstruct folio *folio;\n \tstruct folio *new_folio = NULL;\n \tstruct folio *result = NULL;\n@@ -636,11 +637,12 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \t\t\t\t    struct mempolicy *mpol, pgoff_t ilx)\n {\n \tstruct folio *folio;\n-\tunsigned long entry_offset = swp_offset(entry);\n-\tunsigned long offset = entry_offset;\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tunsigned long slot_offset = swp_slot_offset(slot);\n+\tunsigned long offset = slot_offset;\n \tunsigned long start_offset, end_offset;\n \tunsigned long mask;\n-\tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n+\tstruct swap_info_struct *si = __swap_slot_to_info(slot);\n \tstruct blk_plug plug;\n \tstruct swap_iocb *splug = NULL;\n \tbool page_allocated;\n@@ -661,13 +663,13 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \tfor (offset = start_offset; offset <= end_offset ; offset++) {\n \t\t/* Ok, do the async read-ahead now */\n \t\tfolio = __read_swap_cache_async(\n-\t\t\t\tswp_entry(swp_type(entry), offset),\n+\t\t\t\tswp_slot_to_swp_entry(swp_slot(swp_slot_type(slot), offset)),\n \t\t\t\tgfp_mask, mpol, ilx, &page_allocated, false);\n \t\tif (!folio)\n \t\t\tcontinue;\n \t\tif (page_allocated) {\n \t\t\tswap_read_folio(folio, &splug);\n-\t\t\tif (offset != entry_offset) {\n+\t\t\tif (offset != slot_offset) {\n \t\t\t\tfolio_set_readahead(folio);\n \t\t\t\tcount_vm_event(SWAP_RA);\n \t\t\t}\n@@ -779,16 +781,20 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \t\t/*\n \t\t * Readahead entry may come from a device that we are not\n \t\t * holding a reference to, try to grab a reference, or skip.\n+\t\t *\n+\t\t * XXX: for now, always try to pin the swap entries in the\n+\t\t * readahead window to avoid the annoying conversion to physical\n+\t\t * swap slots. Once we move all swap metadata to virtual swap\n+\t\t * layer, we can simply compare the clusters of the target\n+\t\t * swap entry and the current swap entry, and pin the latter\n+\t\t * swap entry's cluster if it differ from the former's.\n \t\t */\n-\t\tif (swp_type(entry) != swp_type(targ_entry)) {\n-\t\t\tswapoff_locked = tryget_swap_entry(entry, &si);\n-\t\t\tif (!swapoff_locked)\n-\t\t\t\tcontinue;\n-\t\t}\n+\t\tswapoff_locked = tryget_swap_entry(entry, &si);\n+\t\tif (!swapoff_locked)\n+\t\t\tcontinue;\n \t\tfolio = __read_swap_cache_async(entry, gfp_mask, mpol, ilx,\n \t\t\t\t\t\t&page_allocated, false);\n-\t\tif (swapoff_locked)\n-\t\t\tput_swap_entry(entry, si);\n+\t\tput_swap_entry(entry, si);\n \t\tif (!folio)\n \t\t\tcontinue;\n \t\tif (page_allocated) {\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 3c89dedbd5718..4b4126d4e2769 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -53,9 +53,9 @@\n static bool swap_count_continued(struct swap_info_struct *, pgoff_t,\n \t\t\t\t unsigned char);\n static void free_swap_count_continuations(struct swap_info_struct *);\n-static void swap_entries_free(struct swap_info_struct *si,\n+static void swap_slots_free(struct swap_info_struct *si,\n \t\t\t      struct swap_cluster_info *ci,\n-\t\t\t      swp_entry_t entry, unsigned int nr_pages);\n+\t\t\t      swp_slot_t slot, unsigned int nr_pages);\n static void swap_range_alloc(struct swap_info_struct *si,\n \t\t\t     unsigned int nr_entries);\n static bool folio_swapcache_freeable(struct folio *folio);\n@@ -126,7 +126,7 @@ struct percpu_swap_cluster {\n \n static DEFINE_PER_CPU(struct percpu_swap_cluster, percpu_swap_cluster) = {\n \t.si = { NULL },\n-\t.offset = { SWAP_ENTRY_INVALID },\n+\t.offset = { SWAP_SLOT_INVALID },\n \t.lock = INIT_LOCAL_LOCK(),\n };\n \n@@ -139,9 +139,9 @@ static struct swap_info_struct *swap_type_to_info(int type)\n }\n \n /* May return NULL on invalid entry, caller must check for NULL return */\n-static struct swap_info_struct *swap_entry_to_info(swp_entry_t entry)\n+static struct swap_info_struct *swap_slot_to_info(swp_slot_t slot)\n {\n-\treturn swap_type_to_info(swp_type(entry));\n+\treturn swap_type_to_info(swp_slot_type(slot));\n }\n \n static inline unsigned char swap_count(unsigned char ent)\n@@ -204,9 +204,11 @@ static bool swap_only_has_cache(struct swap_info_struct *si,\n  */\n bool is_swap_cached(swp_entry_t entry)\n {\n-\tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tstruct swap_info_struct *si = swap_slot_to_info(slot);\n+\tunsigned long offset = swp_slot_offset(slot);\n \n-\treturn READ_ONCE(si->swap_map[swp_offset(entry)]) & SWAP_HAS_CACHE;\n+\treturn READ_ONCE(si->swap_map[offset]) & SWAP_HAS_CACHE;\n }\n \n static bool swap_is_last_map(struct swap_info_struct *si,\n@@ -236,7 +238,9 @@ static bool swap_is_last_map(struct swap_info_struct *si,\n static int __try_to_reclaim_swap(struct swap_info_struct *si,\n \t\t\t\t unsigned long offset, unsigned long flags)\n {\n-\tconst swp_entry_t entry = swp_entry(si->type, offset);\n+\tconst swp_entry_t entry =\n+\t\tswp_slot_to_swp_entry(swp_slot(si->type, offset));\n+\tswp_slot_t slot;\n \tstruct swap_cluster_info *ci;\n \tstruct folio *folio;\n \tint ret, nr_pages;\n@@ -268,7 +272,8 @@ static int __try_to_reclaim_swap(struct swap_info_struct *si,\n \t\tfolio_put(folio);\n \t\tgoto again;\n \t}\n-\toffset = swp_offset(folio->swap);\n+\tslot = swp_entry_to_swp_slot(folio->swap);\n+\toffset = swp_slot_offset(slot);\n \n \tneed_reclaim = ((flags & TTRS_ANYWAY) ||\n \t\t\t((flags & TTRS_UNMAPPED) && !folio_mapped(folio)) ||\n@@ -368,12 +373,12 @@ offset_to_swap_extent(struct swap_info_struct *sis, unsigned long offset)\n \n sector_t swap_folio_sector(struct folio *folio)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(folio->swap);\n+\tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n \tstruct swap_extent *se;\n \tsector_t sector;\n-\tpgoff_t offset;\n+\tpgoff_t offset = swp_slot_offset(slot);\n \n-\toffset = swp_offset(folio->swap);\n \tse = offset_to_swap_extent(sis, offset);\n \tsector = se->start_block + (offset - se->start_page);\n \treturn sector << (PAGE_SHIFT - 9);\n@@ -890,7 +895,7 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n \t\t\t\t\t    unsigned int order,\n \t\t\t\t\t    unsigned char usage)\n {\n-\tunsigned int next = SWAP_ENTRY_INVALID, found = SWAP_ENTRY_INVALID;\n+\tunsigned int next = SWAP_SLOT_INVALID, found = SWAP_SLOT_INVALID;\n \tunsigned long start = ALIGN_DOWN(offset, SWAPFILE_CLUSTER);\n \tunsigned long end = min(start + SWAPFILE_CLUSTER, si->max);\n \tunsigned int nr_pages = 1 << order;\n@@ -947,7 +952,7 @@ static unsigned int alloc_swap_scan_list(struct swap_info_struct *si,\n \t\t\t\t\t unsigned char usage,\n \t\t\t\t\t bool scan_all)\n {\n-\tunsigned int found = SWAP_ENTRY_INVALID;\n+\tunsigned int found = SWAP_SLOT_INVALID;\n \n \tdo {\n \t\tstruct swap_cluster_info *ci = isolate_lock_cluster(si, list);\n@@ -1017,11 +1022,11 @@ static void swap_reclaim_work(struct work_struct *work)\n  * Try to allocate swap entries with specified order and try set a new\n  * cluster for current CPU too.\n  */\n-static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si, int order,\n+static unsigned long cluster_alloc_swap_slot(struct swap_info_struct *si, int order,\n \t\t\t\t\t      unsigned char usage)\n {\n \tstruct swap_cluster_info *ci;\n-\tunsigned int offset = SWAP_ENTRY_INVALID, found = SWAP_ENTRY_INVALID;\n+\tunsigned int offset = SWAP_SLOT_INVALID, found = SWAP_SLOT_INVALID;\n \n \t/*\n \t * Swapfile is not block device so unable\n@@ -1034,7 +1039,7 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si, int o\n \t\t/* Serialize HDD SWAP allocation for each device. */\n \t\tspin_lock(&si->global_cluster_lock);\n \t\toffset = si->global_cluster->next[order];\n-\t\tif (offset == SWAP_ENTRY_INVALID)\n+\t\tif (offset == SWAP_SLOT_INVALID)\n \t\t\tgoto new_cluster;\n \n \t\tci = swap_cluster_lock(si, offset);\n@@ -1255,7 +1260,7 @@ static void swap_range_free(struct swap_info_struct *si, unsigned long offset,\n \t */\n \tfor (i = 0; i < nr_entries; i++) {\n \t\tclear_bit(offset + i, si->zeromap);\n-\t\tzswap_invalidate(swp_entry(si->type, offset + i));\n+\t\tzswap_invalidate(swp_slot_to_swp_entry(swp_slot(si->type, offset + i)));\n \t}\n \n \tif (si->flags & SWP_BLKDEV)\n@@ -1300,12 +1305,11 @@ static bool get_swap_device_info(struct swap_info_struct *si)\n  * Fast path try to get swap entries with specified order from current\n  * CPU's swap entry pool (a cluster).\n  */\n-static bool swap_alloc_fast(swp_entry_t *entry,\n-\t\t\t    int order)\n+static bool swap_alloc_fast(swp_slot_t *slot, int order)\n {\n \tstruct swap_cluster_info *ci;\n \tstruct swap_info_struct *si;\n-\tunsigned int offset, found = SWAP_ENTRY_INVALID;\n+\tunsigned int offset, found = SWAP_SLOT_INVALID;\n \n \t/*\n \t * Once allocated, swap_info_struct will never be completely freed,\n@@ -1322,18 +1326,17 @@ static bool swap_alloc_fast(swp_entry_t *entry,\n \t\t\toffset = cluster_offset(si, ci);\n \t\tfound = alloc_swap_scan_cluster(si, ci, offset, order, SWAP_HAS_CACHE);\n \t\tif (found)\n-\t\t\t*entry = swp_entry(si->type, found);\n+\t\t\t*slot = swp_slot(si->type, found);\n \t} else {\n \t\tswap_cluster_unlock(ci);\n \t}\n \n-\tput_swap_device(si);\n+\tswap_slot_put_swap_info(si);\n \treturn !!found;\n }\n \n /* Rotate the device and switch to a new cluster */\n-static void swap_alloc_slow(swp_entry_t *entry,\n-\t\t\t    int order)\n+static void swap_alloc_slow(swp_slot_t *slot, int order)\n {\n \tunsigned long offset;\n \tstruct swap_info_struct *si, *next;\n@@ -1345,10 +1348,10 @@ static void swap_alloc_slow(swp_entry_t *entry,\n \t\tplist_requeue(&si->avail_list, &swap_avail_head);\n \t\tspin_unlock(&swap_avail_lock);\n \t\tif (get_swap_device_info(si)) {\n-\t\t\toffset = cluster_alloc_swap_entry(si, order, SWAP_HAS_CACHE);\n-\t\t\tput_swap_device(si);\n+\t\t\toffset = cluster_alloc_swap_slot(si, order, SWAP_HAS_CACHE);\n+\t\t\tswap_slot_put_swap_info(si);\n \t\t\tif (offset) {\n-\t\t\t\t*entry = swp_entry(si->type, offset);\n+\t\t\t\t*slot = swp_slot(si->type, offset);\n \t\t\t\treturn;\n \t\t\t}\n \t\t\tif (order)\n@@ -1388,7 +1391,7 @@ static bool swap_sync_discard(void)\n \t\tif (get_swap_device_info(si)) {\n \t\t\tif (si->flags & SWP_PAGE_DISCARD)\n \t\t\t\tret = swap_do_scheduled_discard(si);\n-\t\t\tput_swap_device(si);\n+\t\t\tswap_slot_put_swap_info(si);\n \t\t}\n \t\tif (ret)\n \t\t\treturn true;\n@@ -1402,25 +1405,9 @@ static bool swap_sync_discard(void)\n \treturn false;\n }\n \n-/**\n- * folio_alloc_swap - allocate swap space for a folio\n- * @folio: folio we want to move to swap\n- *\n- * Allocate swap space for the folio and add the folio to the\n- * swap cache.\n- *\n- * Context: Caller needs to hold the folio lock.\n- * Return: Whether the folio was added to the swap cache.\n- */\n-int folio_alloc_swap(struct folio *folio)\n+static int swap_slot_alloc(swp_slot_t *slot, unsigned int order)\n {\n-\tunsigned int order = folio_order(folio);\n \tunsigned int size = 1 << order;\n-\tswp_entry_t entry = {};\n-\tint err;\n-\n-\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n-\tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n \n \tif (order) {\n \t\t/*\n@@ -1442,22 +1429,52 @@ int folio_alloc_swap(struct folio *folio)\n \n again:\n \tlocal_lock(&percpu_swap_cluster.lock);\n-\tif (!swap_alloc_fast(&entry, order))\n-\t\tswap_alloc_slow(&entry, order);\n+\tif (!swap_alloc_fast(slot, order))\n+\t\tswap_alloc_slow(slot, order);\n \tlocal_unlock(&percpu_swap_cluster.lock);\n \n-\tif (unlikely(!order && !entry.val)) {\n+\tif (unlikely(!order && !slot->val)) {\n \t\tif (swap_sync_discard())\n \t\t\tgoto again;\n \t}\n \n+\treturn 0;\n+}\n+\n+/**\n+ * folio_alloc_swap - allocate swap space for a folio\n+ * @folio: folio we want to move to swap\n+ *\n+ * Allocate swap space for the folio and add the folio to the\n+ * swap cache.\n+ *\n+ * Context: Caller needs to hold the folio lock.\n+ * Return: Whether the folio was added to the swap cache.\n+ */\n+int folio_alloc_swap(struct folio *folio)\n+{\n+\tunsigned int order = folio_order(folio);\n+\tswp_slot_t slot = { 0 };\n+\tswp_entry_t entry = {};\n+\tint err = 0, ret;\n+\n+\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n+\tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n+\n+\tret = swap_slot_alloc(&slot, order);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\t/* XXX: for now, physical and virtual swap slots are identical */\n+\tentry.val = slot.val;\n+\n \t/* Need to call this even if allocation failed, for MEMCG_SWAP_FAIL. */\n \tif (mem_cgroup_try_charge_swap(folio, entry)) {\n \t\terr = -ENOMEM;\n \t\tgoto out_free;\n \t}\n \n-\tif (!entry.val)\n+\tif (!slot.val)\n \t\treturn -ENOMEM;\n \n \terr = swap_cache_add_folio(folio, entry, __GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN, NULL);\n@@ -1471,46 +1488,46 @@ int folio_alloc_swap(struct folio *folio)\n \treturn err;\n }\n \n-static struct swap_info_struct *_swap_info_get(swp_entry_t entry)\n+static struct swap_info_struct *_swap_info_get(swp_slot_t slot)\n {\n \tstruct swap_info_struct *si;\n \tunsigned long offset;\n \n-\tif (!entry.val)\n+\tif (!slot.val)\n \t\tgoto out;\n-\tsi = swap_entry_to_info(entry);\n+\tsi = swap_slot_to_info(slot);\n \tif (!si)\n \t\tgoto bad_nofile;\n \tif (data_race(!(si->flags & SWP_USED)))\n \t\tgoto bad_device;\n-\toffset = swp_offset(entry);\n+\toffset = swp_slot_offset(slot);\n \tif (offset >= si->max)\n \t\tgoto bad_offset;\n-\tif (data_race(!si->swap_map[swp_offset(entry)]))\n+\tif (data_race(!si->swap_map[swp_slot_offset(slot)]))\n \t\tgoto bad_free;\n \treturn si;\n \n bad_free:\n-\tpr_err(\"%s: %s%08lx\\n\", __func__, Unused_offset, entry.val);\n+\tpr_err(\"%s: %s%08lx\\n\", __func__, Unused_offset, slot.val);\n \tgoto out;\n bad_offset:\n-\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_offset, entry.val);\n+\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_offset, slot.val);\n \tgoto out;\n bad_device:\n-\tpr_err(\"%s: %s%08lx\\n\", __func__, Unused_file, entry.val);\n+\tpr_err(\"%s: %s%08lx\\n\", __func__, Unused_file, slot.val);\n \tgoto out;\n bad_nofile:\n-\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_file, entry.val);\n+\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_file, slot.val);\n out:\n \treturn NULL;\n }\n \n-static unsigned char swap_entry_put_locked(struct swap_info_struct *si,\n+static unsigned char swap_slot_put_locked(struct swap_info_struct *si,\n \t\t\t\t\t   struct swap_cluster_info *ci,\n-\t\t\t\t\t   swp_entry_t entry,\n+\t\t\t\t\t   swp_slot_t slot,\n \t\t\t\t\t   unsigned char usage)\n {\n-\tunsigned long offset = swp_offset(entry);\n+\tunsigned long offset = swp_slot_offset(slot);\n \tunsigned char count;\n \tunsigned char has_cache;\n \n@@ -1542,7 +1559,7 @@ static unsigned char swap_entry_put_locked(struct swap_info_struct *si,\n \tif (usage)\n \t\tWRITE_ONCE(si->swap_map[offset], usage);\n \telse\n-\t\tswap_entries_free(si, ci, entry, 1);\n+\t\tswap_slots_free(si, ci, slot, 1);\n \n \treturn usage;\n }\n@@ -1552,8 +1569,9 @@ static unsigned char swap_entry_put_locked(struct swap_info_struct *si,\n  * prevent swapoff, such as the folio in swap cache is locked, RCU\n  * reader side is locked, etc., the swap entry may become invalid\n  * because of swapoff.  Then, we need to enclose all swap related\n- * functions with get_swap_device() and put_swap_device(), unless the\n- * swap functions call get/put_swap_device() by themselves.\n+ * functions with swap_slot_tryget_swap_info() and\n+ * swap_slot_put_swap_info(), unless the swap functions call\n+ * swap_slot_(tryget|put)_swap_info by themselves.\n  *\n  * RCU reader side lock (including any spinlock) is sufficient to\n  * prevent swapoff, because synchronize_rcu() is called in swapoff()\n@@ -1562,11 +1580,11 @@ static unsigned char swap_entry_put_locked(struct swap_info_struct *si,\n  * Check whether swap entry is valid in the swap device.  If so,\n  * return pointer to swap_info_struct, and keep the swap entry valid\n  * via preventing the swap device from being swapoff, until\n- * put_swap_device() is called.  Otherwise return NULL.\n+ * swap_slot_put_swap_info() is called.  Otherwise return NULL.\n  *\n  * Notice that swapoff or swapoff+swapon can still happen before the\n- * percpu_ref_tryget_live() in get_swap_device() or after the\n- * percpu_ref_put() in put_swap_device() if there isn't any other way\n+ * percpu_ref_tryget_live() in swap_slot_tryget_swap_info() or after the\n+ * percpu_ref_put() in swap_slot_put_swap_info() if there isn't any other way\n  * to prevent swapoff.  The caller must be prepared for that.  For\n  * example, the following situation is possible.\n  *\n@@ -1586,53 +1604,53 @@ static unsigned char swap_entry_put_locked(struct swap_info_struct *si,\n  * changed with the page table locked to check whether the swap device\n  * has been swapoff or swapoff+swapon.\n  */\n-struct swap_info_struct *get_swap_device(swp_entry_t entry)\n+struct swap_info_struct *swap_slot_tryget_swap_info(swp_slot_t slot)\n {\n \tstruct swap_info_struct *si;\n \tunsigned long offset;\n \n-\tif (!entry.val)\n+\tif (!slot.val)\n \t\tgoto out;\n-\tsi = swap_entry_to_info(entry);\n+\tsi = swap_slot_to_info(slot);\n \tif (!si)\n \t\tgoto bad_nofile;\n \tif (!get_swap_device_info(si))\n \t\tgoto out;\n-\toffset = swp_offset(entry);\n+\toffset = swp_slot_offset(slot);\n \tif (offset >= si->max)\n \t\tgoto put_out;\n \n \treturn si;\n bad_nofile:\n-\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_file, entry.val);\n+\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_file, slot.val);\n out:\n \treturn NULL;\n put_out:\n-\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_offset, entry.val);\n+\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_offset, slot.val);\n \tpercpu_ref_put(&si->users);\n \treturn NULL;\n }\n \n-static void swap_entries_put_cache(struct swap_info_struct *si,\n-\t\t\t\t   swp_entry_t entry, int nr)\n+static void swap_slots_put_cache(struct swap_info_struct *si,\n+\t\t\t\t   swp_slot_t slot, int nr)\n {\n-\tunsigned long offset = swp_offset(entry);\n+\tunsigned long offset = swp_slot_offset(slot);\n \tstruct swap_cluster_info *ci;\n \n \tci = swap_cluster_lock(si, offset);\n \tif (swap_only_has_cache(si, offset, nr)) {\n-\t\tswap_entries_free(si, ci, entry, nr);\n+\t\tswap_slots_free(si, ci, slot, nr);\n \t} else {\n-\t\tfor (int i = 0; i < nr; i++, entry.val++)\n-\t\t\tswap_entry_put_locked(si, ci, entry, SWAP_HAS_CACHE);\n+\t\tfor (int i = 0; i < nr; i++, slot.val++)\n+\t\t\tswap_slot_put_locked(si, ci, slot, SWAP_HAS_CACHE);\n \t}\n \tswap_cluster_unlock(ci);\n }\n \n-static bool swap_entries_put_map(struct swap_info_struct *si,\n-\t\t\t\t swp_entry_t entry, int nr)\n+static bool swap_slots_put_map(struct swap_info_struct *si,\n+\t\t\t\t swp_slot_t slot, int nr)\n {\n-\tunsigned long offset = swp_offset(entry);\n+\tunsigned long offset = swp_slot_offset(slot);\n \tstruct swap_cluster_info *ci;\n \tbool has_cache = false;\n \tunsigned char count;\n@@ -1649,7 +1667,7 @@ static bool swap_entries_put_map(struct swap_info_struct *si,\n \t\tgoto locked_fallback;\n \t}\n \tif (!has_cache)\n-\t\tswap_entries_free(si, ci, entry, nr);\n+\t\tswap_slots_free(si, ci, slot, nr);\n \telse\n \t\tfor (i = 0; i < nr; i++)\n \t\t\tWRITE_ONCE(si->swap_map[offset + i], SWAP_HAS_CACHE);\n@@ -1660,8 +1678,8 @@ static bool swap_entries_put_map(struct swap_info_struct *si,\n fallback:\n \tci = swap_cluster_lock(si, offset);\n locked_fallback:\n-\tfor (i = 0; i < nr; i++, entry.val++) {\n-\t\tcount = swap_entry_put_locked(si, ci, entry, 1);\n+\tfor (i = 0; i < nr; i++, slot.val++) {\n+\t\tcount = swap_slot_put_locked(si, ci, slot, 1);\n \t\tif (count == SWAP_HAS_CACHE)\n \t\t\thas_cache = true;\n \t}\n@@ -1674,20 +1692,20 @@ static bool swap_entries_put_map(struct swap_info_struct *si,\n  * cross multi clusters, so ensure the range is within a single cluster\n  * when freeing entries with functions without \"_nr\" suffix.\n  */\n-static bool swap_entries_put_map_nr(struct swap_info_struct *si,\n-\t\t\t\t    swp_entry_t entry, int nr)\n+static bool swap_slots_put_map_nr(struct swap_info_struct *si,\n+\t\t\t\t    swp_slot_t slot, int nr)\n {\n \tint cluster_nr, cluster_rest;\n-\tunsigned long offset = swp_offset(entry);\n+\tunsigned long offset = swp_slot_offset(slot);\n \tbool has_cache = false;\n \n \tcluster_rest = SWAPFILE_CLUSTER - offset % SWAPFILE_CLUSTER;\n \twhile (nr) {\n \t\tcluster_nr = min(nr, cluster_rest);\n-\t\thas_cache |= swap_entries_put_map(si, entry, cluster_nr);\n+\t\thas_cache |= swap_slots_put_map(si, slot, cluster_nr);\n \t\tcluster_rest = SWAPFILE_CLUSTER;\n \t\tnr -= cluster_nr;\n-\t\tentry.val += cluster_nr;\n+\t\tslot.val += cluster_nr;\n \t}\n \n \treturn has_cache;\n@@ -1707,13 +1725,14 @@ static inline bool __maybe_unused swap_is_last_ref(unsigned char count)\n  * Drop the last ref of swap entries, caller have to ensure all entries\n  * belong to the same cgroup and cluster.\n  */\n-static void swap_entries_free(struct swap_info_struct *si,\n+static void swap_slots_free(struct swap_info_struct *si,\n \t\t\t      struct swap_cluster_info *ci,\n-\t\t\t      swp_entry_t entry, unsigned int nr_pages)\n+\t\t\t      swp_slot_t slot, unsigned int nr_pages)\n {\n-\tunsigned long offset = swp_offset(entry);\n+\tunsigned long offset = swp_slot_offset(slot);\n \tunsigned char *map = si->swap_map + offset;\n \tunsigned char *map_end = map + nr_pages;\n+\tswp_entry_t entry = swp_slot_to_swp_entry(slot);\n \n \t/* It should never free entries across different clusters */\n \tVM_BUG_ON(ci != __swap_offset_to_cluster(si, offset + nr_pages - 1));\n@@ -1739,43 +1758,54 @@ static void swap_entries_free(struct swap_info_struct *si,\n  * Caller has made sure that the swap device corresponding to entry\n  * is still around or has not been recycled.\n  */\n-void swap_free_nr(swp_entry_t entry, int nr_pages)\n+void swap_slot_free_nr(swp_slot_t slot, int nr_pages)\n {\n \tint nr;\n \tstruct swap_info_struct *sis;\n-\tunsigned long offset = swp_offset(entry);\n+\tunsigned long offset = swp_slot_offset(slot);\n \n-\tsis = _swap_info_get(entry);\n+\tsis = _swap_info_get(slot);\n \tif (!sis)\n \t\treturn;\n \n \twhile (nr_pages) {\n \t\tnr = min_t(int, nr_pages, SWAPFILE_CLUSTER - offset % SWAPFILE_CLUSTER);\n-\t\tswap_entries_put_map(sis, swp_entry(sis->type, offset), nr);\n+\t\tswap_slots_put_map(sis, swp_slot(sis->type, offset), nr);\n \t\toffset += nr;\n \t\tnr_pages -= nr;\n \t}\n }\n \n+/*\n+ * Caller has made sure that the swap device corresponding to entry\n+ * is still around or has not been recycled.\n+ */\n+void swap_free_nr(swp_entry_t entry, int nr_pages)\n+{\n+\tswap_slot_free_nr(swp_entry_to_swp_slot(entry), nr_pages);\n+}\n+\n /*\n  * Called after dropping swapcache to decrease refcnt to swap entries.\n  */\n void put_swap_folio(struct folio *folio, swp_entry_t entry)\n {\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n \tstruct swap_info_struct *si;\n-\tint size = 1 << swap_entry_order(folio_order(folio));\n+\tint size = 1 << swap_slot_order(folio_order(folio));\n \n-\tsi = _swap_info_get(entry);\n+\tsi = _swap_info_get(slot);\n \tif (!si)\n \t\treturn;\n \n-\tswap_entries_put_cache(si, entry, size);\n+\tswap_slots_put_cache(si, slot, size);\n }\n \n int __swap_count(swp_entry_t entry)\n {\n-\tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n-\tpgoff_t offset = swp_offset(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tstruct swap_info_struct *si = __swap_slot_to_info(slot);\n+\tpgoff_t offset = swp_slot_offset(slot);\n \n \treturn swap_count(si->swap_map[offset]);\n }\n@@ -1787,7 +1817,8 @@ int __swap_count(swp_entry_t entry)\n  */\n bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry)\n {\n-\tpgoff_t offset = swp_offset(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tpgoff_t offset = swp_slot_offset(slot);\n \tstruct swap_cluster_info *ci;\n \tint count;\n \n@@ -1803,6 +1834,7 @@ bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry)\n  */\n int swp_swapcount(swp_entry_t entry)\n {\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n \tint count, tmp_count, n;\n \tstruct swap_info_struct *si;\n \tstruct swap_cluster_info *ci;\n@@ -1810,11 +1842,11 @@ int swp_swapcount(swp_entry_t entry)\n \tpgoff_t offset;\n \tunsigned char *map;\n \n-\tsi = _swap_info_get(entry);\n+\tsi = _swap_info_get(slot);\n \tif (!si)\n \t\treturn 0;\n \n-\toffset = swp_offset(entry);\n+\toffset = swp_slot_offset(slot);\n \n \tci = swap_cluster_lock(si, offset);\n \n@@ -1846,10 +1878,11 @@ int swp_swapcount(swp_entry_t entry)\n static bool swap_page_trans_huge_swapped(struct swap_info_struct *si,\n \t\t\t\t\t swp_entry_t entry, int order)\n {\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n \tstruct swap_cluster_info *ci;\n \tunsigned char *map = si->swap_map;\n \tunsigned int nr_pages = 1 << order;\n-\tunsigned long roffset = swp_offset(entry);\n+\tunsigned long roffset = swp_slot_offset(slot);\n \tunsigned long offset = round_down(roffset, nr_pages);\n \tint i;\n \tbool ret = false;\n@@ -1874,7 +1907,8 @@ static bool swap_page_trans_huge_swapped(struct swap_info_struct *si,\n static bool folio_swapped(struct folio *folio)\n {\n \tswp_entry_t entry = folio->swap;\n-\tstruct swap_info_struct *si = _swap_info_get(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tstruct swap_info_struct *si = _swap_info_get(slot);\n \n \tif (!si)\n \t\treturn false;\n@@ -1948,13 +1982,14 @@ bool folio_free_swap(struct folio *folio)\n  */\n void free_swap_and_cache_nr(swp_entry_t entry, int nr)\n {\n-\tconst unsigned long start_offset = swp_offset(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tconst unsigned long start_offset = swp_slot_offset(slot);\n \tconst unsigned long end_offset = start_offset + nr;\n \tstruct swap_info_struct *si;\n \tbool any_only_cache = false;\n \tunsigned long offset;\n \n-\tsi = get_swap_device(entry);\n+\tsi = swap_slot_tryget_swap_info(slot);\n \tif (!si)\n \t\treturn;\n \n@@ -1964,7 +1999,7 @@ void free_swap_and_cache_nr(swp_entry_t entry, int nr)\n \t/*\n \t * First free all entries in the range.\n \t */\n-\tany_only_cache = swap_entries_put_map_nr(si, entry, nr);\n+\tany_only_cache = swap_slots_put_map_nr(si, slot, nr);\n \n \t/*\n \t * Short-circuit the below loop if none of the entries had their\n@@ -1998,16 +2033,16 @@ void free_swap_and_cache_nr(swp_entry_t entry, int nr)\n \t}\n \n out:\n-\tput_swap_device(si);\n+\tswap_slot_put_swap_info(si);\n }\n \n #ifdef CONFIG_HIBERNATION\n \n-swp_entry_t get_swap_page_of_type(int type)\n+swp_slot_t swap_slot_alloc_of_type(int type)\n {\n \tstruct swap_info_struct *si = swap_type_to_info(type);\n \tunsigned long offset;\n-\tswp_entry_t entry = {0};\n+\tswp_slot_t slot = {0};\n \n \tif (!si)\n \t\tgoto fail;\n@@ -2020,15 +2055,15 @@ swp_entry_t get_swap_page_of_type(int type)\n \t\t\t * with swap table allocation.\n \t\t\t */\n \t\t\tlocal_lock(&percpu_swap_cluster.lock);\n-\t\t\toffset = cluster_alloc_swap_entry(si, 0, 1);\n+\t\t\toffset = cluster_alloc_swap_slot(si, 0, 1);\n \t\t\tlocal_unlock(&percpu_swap_cluster.lock);\n \t\t\tif (offset)\n-\t\t\t\tentry = swp_entry(si->type, offset);\n+\t\t\t\tslot = swp_slot(si->type, offset);\n \t\t}\n-\t\tput_swap_device(si);\n+\t\tswap_slot_put_swap_info(si);\n \t}\n fail:\n-\treturn entry;\n+\treturn slot;\n }\n \n /*\n@@ -2257,6 +2292,7 @@ static int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,\n \t\tunsigned long offset;\n \t\tunsigned char swp_count;\n \t\tsoftleaf_t entry;\n+\t\tswp_slot_t slot;\n \t\tint ret;\n \t\tpte_t ptent;\n \n@@ -2271,10 +2307,12 @@ static int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,\n \n \t\tif (!softleaf_is_swap(entry))\n \t\t\tcontinue;\n-\t\tif (swp_type(entry) != type)\n+\n+\t\tslot = swp_entry_to_swp_slot(entry);\n+\t\tif (swp_slot_type(slot) != type)\n \t\t\tcontinue;\n \n-\t\toffset = swp_offset(entry);\n+\t\toffset = swp_slot_offset(slot);\n \t\tpte_unmap(pte);\n \t\tpte = NULL;\n \n@@ -2459,6 +2497,7 @@ static int try_to_unuse(unsigned int type)\n \tstruct swap_info_struct *si = swap_info[type];\n \tstruct folio *folio;\n \tswp_entry_t entry;\n+\tswp_slot_t slot;\n \tunsigned int i;\n \n \tif (!swap_usage_in_pages(si))\n@@ -2506,7 +2545,8 @@ static int try_to_unuse(unsigned int type)\n \t       !signal_pending(current) &&\n \t       (i = find_next_to_unuse(si, i)) != 0) {\n \n-\t\tentry = swp_entry(type, i);\n+\t\tslot = swp_slot(type, i);\n+\t\tentry = swp_slot_to_swp_entry(slot);\n \t\tfolio = swap_cache_get_folio(entry);\n \t\tif (!folio)\n \t\t\tcontinue;\n@@ -2890,7 +2930,7 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \t}\n \n \t/*\n-\t * Wait for swap operations protected by get/put_swap_device()\n+\t * Wait for swap operations protected by swap_slot_(tryget|put)_swap_info()\n \t * to complete.  Because of synchronize_rcu() here, all swap\n \t * operations protected by RCU reader side lock (including any\n \t * spinlock) will be waited too.  This makes it easy to\n@@ -3331,7 +3371,7 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n \t\tif (!si->global_cluster)\n \t\t\tgoto err;\n \t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n-\t\t\tsi->global_cluster->next[i] = SWAP_ENTRY_INVALID;\n+\t\t\tsi->global_cluster->next[i] = SWAP_SLOT_INVALID;\n \t\tspin_lock_init(&si->global_cluster_lock);\n \t}\n \n@@ -3669,6 +3709,7 @@ void si_swapinfo(struct sysinfo *val)\n  */\n static int __swap_duplicate(swp_entry_t entry, unsigned char usage, int nr)\n {\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n \tstruct swap_info_struct *si;\n \tstruct swap_cluster_info *ci;\n \tunsigned long offset;\n@@ -3676,13 +3717,13 @@ static int __swap_duplicate(swp_entry_t entry, unsigned char usage, int nr)\n \tunsigned char has_cache;\n \tint err, i;\n \n-\tsi = swap_entry_to_info(entry);\n+\tsi = swap_slot_to_info(slot);\n \tif (WARN_ON_ONCE(!si)) {\n \t\tpr_err(\"%s%08lx\\n\", Bad_file, entry.val);\n \t\treturn -EINVAL;\n \t}\n \n-\toffset = swp_offset(entry);\n+\toffset = swp_slot_offset(slot);\n \tVM_WARN_ON(nr > SWAPFILE_CLUSTER - offset % SWAPFILE_CLUSTER);\n \tVM_WARN_ON(usage == 1 && nr > 1);\n \tci = swap_cluster_lock(si, offset);\n@@ -3788,7 +3829,7 @@ int swapcache_prepare(swp_entry_t entry, int nr)\n  */\n void swapcache_clear(struct swap_info_struct *si, swp_entry_t entry, int nr)\n {\n-\tswap_entries_put_cache(si, entry, nr);\n+\tswap_slots_put_cache(si, swp_entry_to_swp_slot(entry), nr);\n }\n \n /*\n@@ -3815,6 +3856,7 @@ int add_swap_count_continuation(swp_entry_t entry, gfp_t gfp_mask)\n \tstruct page *list_page;\n \tpgoff_t offset;\n \tunsigned char count;\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n \tint ret = 0;\n \n \t/*\n@@ -3823,7 +3865,7 @@ int add_swap_count_continuation(swp_entry_t entry, gfp_t gfp_mask)\n \t */\n \tpage = alloc_page(gfp_mask | __GFP_HIGHMEM);\n \n-\tsi = get_swap_device(entry);\n+\tsi = swap_slot_tryget_swap_info(slot);\n \tif (!si) {\n \t\t/*\n \t\t * An acceptable race has occurred since the failing\n@@ -3832,7 +3874,7 @@ int add_swap_count_continuation(swp_entry_t entry, gfp_t gfp_mask)\n \t\tgoto outer;\n \t}\n \n-\toffset = swp_offset(entry);\n+\toffset = swp_slot_offset(slot);\n \n \tci = swap_cluster_lock(si, offset);\n \n@@ -3895,7 +3937,7 @@ int add_swap_count_continuation(swp_entry_t entry, gfp_t gfp_mask)\n \tspin_unlock(&si->cont_lock);\n out:\n \tswap_cluster_unlock(ci);\n-\tput_swap_device(si);\n+\tswap_slot_put_swap_info(si);\n outer:\n \tif (page)\n \t\t__free_page(page);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author acknowledged that the zswap tree code's range partition logic can no longer be reused for the new virtual swap space design and decided to use a simple unified zswap tree in the new implementation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "decided"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The zswap tree code, specifically the range partition logic, can no\nlonger easily be reused for the new virtual swap space design. Use a\nsimple unified zswap tree in the new implementation for now.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/zswap.h |  7 -----\n mm/swapfile.c         |  9 +-----\n mm/zswap.c            | 69 +++++++------------------------------------\n 3 files changed, 11 insertions(+), 74 deletions(-)\n\ndiff --git a/include/linux/zswap.h b/include/linux/zswap.h\nindex 30c193a1207e1..1a04caf283dc8 100644\n--- a/include/linux/zswap.h\n+++ b/include/linux/zswap.h\n@@ -28,8 +28,6 @@ unsigned long zswap_total_pages(void);\n bool zswap_store(struct folio *folio);\n int zswap_load(struct folio *folio);\n void zswap_invalidate(swp_entry_t swp);\n-int zswap_swapon(int type, unsigned long nr_pages);\n-void zswap_swapoff(int type);\n void zswap_memcg_offline_cleanup(struct mem_cgroup *memcg);\n void zswap_lruvec_state_init(struct lruvec *lruvec);\n void zswap_folio_swapin(struct folio *folio);\n@@ -50,11 +48,6 @@ static inline int zswap_load(struct folio *folio)\n }\n \n static inline void zswap_invalidate(swp_entry_t swp) {}\n-static inline int zswap_swapon(int type, unsigned long nr_pages)\n-{\n-\treturn 0;\n-}\n-static inline void zswap_swapoff(int type) {}\n static inline void zswap_memcg_offline_cleanup(struct mem_cgroup *memcg) {}\n static inline void zswap_lruvec_state_init(struct lruvec *lruvec) {}\n static inline void zswap_folio_swapin(struct folio *folio) {}\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 4b4126d4e2769..3f70df488c1da 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -2970,7 +2970,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tspin_unlock(&p->lock);\n \tspin_unlock(&swap_lock);\n \tarch_swap_invalidate_area(p->type);\n-\tzswap_swapoff(p->type);\n \tmutex_unlock(&swapon_mutex);\n \tkfree(p->global_cluster);\n \tp->global_cluster = NULL;\n@@ -3613,10 +3612,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \t\t}\n \t}\n \n-\terror = zswap_swapon(si->type, maxpages);\n-\tif (error)\n-\t\tgoto bad_swap_unlock_inode;\n-\n \t/*\n \t * Flush any pending IO and dirty mappings before we start using this\n \t * swap device.\n@@ -3625,7 +3620,7 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \terror = inode_drain_writes(inode);\n \tif (error) {\n \t\tinode->i_flags &= ~S_SWAPFILE;\n-\t\tgoto free_swap_zswap;\n+\t\tgoto bad_swap_unlock_inode;\n \t}\n \n \tmutex_lock(&swapon_mutex);\n@@ -3648,8 +3643,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \n \terror = 0;\n \tgoto out;\n-free_swap_zswap:\n-\tzswap_swapoff(si->type);\n bad_swap_unlock_inode:\n \tinode_unlock(inode);\n bad_swap:\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex a5a3f068bd1a6..f7313261673ff 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -197,8 +197,6 @@ struct zswap_entry {\n \tstruct list_head lru;\n };\n \n-static struct xarray *zswap_trees[MAX_SWAPFILES];\n-static unsigned int nr_zswap_trees[MAX_SWAPFILES];\n \n /* RCU-protected iteration */\n static LIST_HEAD(zswap_pools);\n@@ -225,45 +223,35 @@ static bool zswap_has_pool;\n * helpers and fwd declarations\n **********************************/\n \n-/* One swap address space for each 64M swap space */\n-#define ZSWAP_ADDRESS_SPACE_SHIFT 14\n-#define ZSWAP_ADDRESS_SPACE_PAGES (1 << ZSWAP_ADDRESS_SPACE_SHIFT)\n-static inline struct xarray *swap_zswap_tree(swp_entry_t swp)\n-{\n-\treturn &zswap_trees[swp_type(swp)][swp_offset(swp)\n-\t\t>> ZSWAP_ADDRESS_SPACE_SHIFT];\n-}\n+static DEFINE_XARRAY(zswap_tree);\n+\n+#define zswap_tree_index(entry)\t(entry.val)\n \n static inline void *zswap_entry_store(swp_entry_t swpentry,\n \t\tstruct zswap_entry *entry)\n {\n-\tstruct xarray *tree = swap_zswap_tree(swpentry);\n-\tpgoff_t offset = swp_offset(swpentry);\n+\tpgoff_t offset = zswap_tree_index(swpentry);\n \n-\treturn xa_store(tree, offset, entry, GFP_KERNEL);\n+\treturn xa_store(&zswap_tree, offset, entry, GFP_KERNEL);\n }\n \n static inline void *zswap_entry_load(swp_entry_t swpentry)\n {\n-\tstruct xarray *tree = swap_zswap_tree(swpentry);\n-\tpgoff_t offset = swp_offset(swpentry);\n+\tpgoff_t offset = zswap_tree_index(swpentry);\n \n-\treturn xa_load(tree, offset);\n+\treturn xa_load(&zswap_tree, offset);\n }\n \n static inline void *zswap_entry_erase(swp_entry_t swpentry)\n {\n-\tstruct xarray *tree = swap_zswap_tree(swpentry);\n-\tpgoff_t offset = swp_offset(swpentry);\n+\tpgoff_t offset = zswap_tree_index(swpentry);\n \n-\treturn xa_erase(tree, offset);\n+\treturn xa_erase(&zswap_tree, offset);\n }\n \n static inline bool zswap_empty(swp_entry_t swpentry)\n {\n-\tstruct xarray *tree = swap_zswap_tree(swpentry);\n-\n-\treturn xa_empty(tree);\n+\treturn xa_empty(&zswap_tree);\n }\n \n #define zswap_pool_debug(msg, p)\t\t\t\\\n@@ -1691,43 +1679,6 @@ void zswap_invalidate(swp_entry_t swp)\n \t\tzswap_entry_free(entry);\n }\n \n-int zswap_swapon(int type, unsigned long nr_pages)\n-{\n-\tstruct xarray *trees, *tree;\n-\tunsigned int nr, i;\n-\n-\tnr = DIV_ROUND_UP(nr_pages, ZSWAP_ADDRESS_SPACE_PAGES);\n-\ttrees = kvcalloc(nr, sizeof(*tree), GFP_KERNEL);\n-\tif (!trees) {\n-\t\tpr_err(\"alloc failed, zswap disabled for swap type %d\\n\", type);\n-\t\treturn -ENOMEM;\n-\t}\n-\n-\tfor (i = 0; i < nr; i++)\n-\t\txa_init(trees + i);\n-\n-\tnr_zswap_trees[type] = nr;\n-\tzswap_trees[type] = trees;\n-\treturn 0;\n-}\n-\n-void zswap_swapoff(int type)\n-{\n-\tstruct xarray *trees = zswap_trees[type];\n-\tunsigned int i;\n-\n-\tif (!trees)\n-\t\treturn;\n-\n-\t/* try_to_unuse() invalidated all the entries already */\n-\tfor (i = 0; i < nr_zswap_trees[type]; i++)\n-\t\tWARN_ON_ONCE(!xa_empty(trees + i));\n-\n-\tkvfree(trees);\n-\tnr_zswap_trees[type] = 0;\n-\tzswap_trees[type] = NULL;\n-}\n-\n /*********************************\n * debugfs functions\n **********************************/\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the vswap free path's poor batching behavior, explaining that they changed the freeing ordering to clear the shadow, invalidate the zswap entry, and uncharge swap cgroup when releasing the virtual swap slot.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "For the new virtual swap space design, dynamically allocate a virtual\nslot (as well as an associated metadata structure) for each swapped out\npage, and associate it to the (physical) swap slot on the swapfile/swap\npartition. This virtual swap slot is now stored in page table entries\nand used to index into swap data structures (swap cache, zswap tree,\nswap cgroup array), in place of the old physical swap slot.\n\nFor now, there is always a physical slot in the swapfile associated for\neach virtual swap slot (except those about to be freed). The virtual\nswap slot's lifetime is still tied to the lifetime of its physical swap\nslot. We do change the freeing ordering a bit - we clear the shadow,\ninvalidate the zswap entry, and uncharge swap cgroup when we release the\nvirtual swap slot, as we now use virtual swap slot to index into these\nswap data structures.\n\nWe also repurpose the swap table infrastructure as a reverse map to look\nup the virtual swap slot from its associated physical swap slot on\nswapfile. This is used in cluster readahead, as well as several swapfile\noperations, such as the swap slot reclamation that happens when the\nswapfile is almost full.  It will also be used in a future patch that\nsimplifies swapoff.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/cpuhotplug.h |   1 +\n include/linux/swap.h       |  49 +--\n mm/internal.h              |  28 +-\n mm/page_io.c               |   6 +-\n mm/shmem.c                 |   9 +-\n mm/swap.h                  |   8 +-\n mm/swap_state.c            |   5 +-\n mm/swapfile.c              |  63 +---\n mm/vswap.c                 | 658 +++++++++++++++++++++++++++++++++++++\n 9 files changed, 710 insertions(+), 117 deletions(-)\n\ndiff --git a/include/linux/cpuhotplug.h b/include/linux/cpuhotplug.h\nindex 62cd7b35a29c9..85cb45022e796 100644\n--- a/include/linux/cpuhotplug.h\n+++ b/include/linux/cpuhotplug.h\n@@ -86,6 +86,7 @@ enum cpuhp_state {\n \tCPUHP_FS_BUFF_DEAD,\n \tCPUHP_PRINTK_DEAD,\n \tCPUHP_MM_MEMCQ_DEAD,\n+\tCPUHP_MM_VSWAP_DEAD,\n \tCPUHP_PERCPU_CNT_DEAD,\n \tCPUHP_RADIX_DEAD,\n \tCPUHP_PAGE_ALLOC,\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 1ff463fb3a966..0410a00fd353c 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -471,6 +471,7 @@ static inline long get_nr_swap_pages(void)\n }\n \n void si_swapinfo(struct sysinfo *);\n+int swap_slot_alloc(swp_slot_t *slot, unsigned int order);\n swp_slot_t swap_slot_alloc_of_type(int);\n int add_swap_count_continuation(swp_entry_t, gfp_t);\n int swap_type_of(dev_t device, sector_t offset);\n@@ -670,48 +671,12 @@ static inline bool mem_cgroup_swap_full(struct folio *folio)\n #endif\n \n int vswap_init(void);\n-\n-/**\n- * swp_entry_to_swp_slot - look up the physical swap slot corresponding to a\n- *                         virtual swap slot.\n- * @entry: the virtual swap slot.\n- *\n- * Return: the physical swap slot corresponding to the virtual swap slot.\n- */\n-static inline swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry)\n-{\n-\treturn (swp_slot_t) { entry.val };\n-}\n-\n-/**\n- * swp_slot_to_swp_entry - look up the virtual swap slot corresponding to a\n- *                         physical swap slot.\n- * @slot: the physical swap slot.\n- *\n- * Return: the virtual swap slot corresponding to the physical swap slot.\n- */\n-static inline swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot)\n-{\n-\treturn (swp_entry_t) { slot.val };\n-}\n-\n-static inline bool tryget_swap_entry(swp_entry_t entry,\n-\t\t\t\tstruct swap_info_struct **sip)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si = swap_slot_tryget_swap_info(slot);\n-\n-\tif (sip)\n-\t\t*sip = si;\n-\n-\treturn si;\n-}\n-\n-static inline void put_swap_entry(swp_entry_t entry,\n-\t\t\t\tstruct swap_info_struct *si)\n-{\n-\tswap_slot_put_swap_info(si);\n-}\n+void vswap_exit(void);\n+void vswap_free(swp_entry_t entry, struct swap_cluster_info *ci);\n+swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry);\n+swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot);\n+bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si);\n+void put_swap_entry(swp_entry_t entry, struct swap_info_struct *si);\n \n #endif /* __KERNEL__*/\n #endif /* _LINUX_SWAP_H */\ndiff --git a/mm/internal.h b/mm/internal.h\nindex e739e8cac5b55..7ced0def684ca 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -322,6 +322,25 @@ static inline unsigned int folio_pte_batch_flags(struct folio *folio,\n unsigned int folio_pte_batch(struct folio *folio, pte_t *ptep, pte_t pte,\n \t\tunsigned int max_nr);\n \n+static inline swp_entry_t swap_nth(swp_entry_t entry, long n)\n+{\n+\treturn (swp_entry_t) { entry.val + n };\n+}\n+\n+/* similar to swap_nth, but check the backing physical slots as well. */\n+static inline swp_entry_t swap_move(swp_entry_t entry, long delta)\n+{\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry), next_slot;\n+\tswp_entry_t next_entry = swap_nth(entry, delta);\n+\n+\tnext_slot = swp_entry_to_swp_slot(next_entry);\n+\tif (swp_slot_type(slot) != swp_slot_type(next_slot) ||\n+\t\t\tswp_slot_offset(slot) + delta != swp_slot_offset(next_slot))\n+\t\tnext_entry.val = 0;\n+\n+\treturn next_entry;\n+}\n+\n /**\n  * pte_move_swp_offset - Move the swap entry offset field of a swap pte\n  *\t forward or backward by delta\n@@ -334,13 +353,8 @@ unsigned int folio_pte_batch(struct folio *folio, pte_t *ptep, pte_t pte,\n  */\n static inline pte_t pte_move_swp_offset(pte_t pte, long delta)\n {\n-\tsoftleaf_t entry = softleaf_from_pte(pte), new_entry;\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tpte_t new;\n-\n-\tnew_entry = swp_slot_to_swp_entry(swp_slot(swp_slot_type(slot),\n-\t\t\tswp_slot_offset(slot) + delta));\n-\tnew = swp_entry_to_pte(new_entry);\n+\tsoftleaf_t entry = softleaf_from_pte(pte);\n+\tpte_t new = swp_entry_to_pte(swap_move(entry, delta));\n \n \tif (pte_swp_soft_dirty(pte))\n \t\tnew = pte_swp_mksoft_dirty(new);\ndiff --git a/mm/page_io.c b/mm/page_io.c\nindex 0b02bcc85e2a8..5de3705572955 100644\n--- a/mm/page_io.c\n+++ b/mm/page_io.c\n@@ -364,7 +364,7 @@ static void sio_write_complete(struct kiocb *iocb, long ret)\n \t\t */\n \t\tpr_err_ratelimited(\"Write error %ld on dio swapfile (%llu)\\n\",\n \t\t\t\t   ret,\n-\t\t\t\t   swap_slot_pos(swp_entry_to_swp_slot(page_swap_entry(page))));\n+\t\t\t\t   swap_slot_dev_pos(swp_entry_to_swp_slot(page_swap_entry(page))));\n \t\tfor (p = 0; p < sio->pages; p++) {\n \t\t\tpage = sio->bvec[p].bv_page;\n \t\t\tset_page_dirty(page);\n@@ -384,7 +384,7 @@ static void swap_writepage_fs(struct folio *folio, struct swap_iocb **swap_plug)\n \tswp_slot_t slot = swp_entry_to_swp_slot(folio->swap);\n \tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n \tstruct file *swap_file = sis->swap_file;\n-\tloff_t pos = swap_slot_pos(slot);\n+\tloff_t pos = swap_slot_dev_pos(slot);\n \n \tcount_swpout_vm_event(folio);\n \tfolio_start_writeback(folio);\n@@ -549,7 +549,7 @@ static void swap_read_folio_fs(struct folio *folio, struct swap_iocb **plug)\n \tswp_slot_t slot = swp_entry_to_swp_slot(folio->swap);\n \tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n \tstruct swap_iocb *sio = NULL;\n-\tloff_t pos = swap_slot_pos(slot);\n+\tloff_t pos = swap_slot_dev_pos(slot);\n \n \tif (plug)\n \t\tsio = *plug;\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 400e2fa8e77cb..13f7469a04c8a 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -2227,7 +2227,6 @@ static int shmem_split_large_entry(struct inode *inode, pgoff_t index,\n \tXA_STATE_ORDER(xas, &mapping->i_pages, index, 0);\n \tint split_order = 0;\n \tint i;\n-\tswp_slot_t slot = swp_entry_to_swp_slot(swap);\n \n \t/* Convert user data gfp flags to xarray node gfp flags */\n \tgfp &= GFP_RECLAIM_MASK;\n@@ -2268,13 +2267,7 @@ static int shmem_split_large_entry(struct inode *inode, pgoff_t index,\n \t\t\t */\n \t\t\tfor (i = 0; i < 1 << cur_order;\n \t\t\t     i += (1 << split_order)) {\n-\t\t\t\tswp_entry_t tmp_entry;\n-\t\t\t\tswp_slot_t tmp_slot;\n-\n-\t\t\t\ttmp_slot =\n-\t\t\t\t\tswp_slot(swp_slot_type(slot),\n-\t\t\t\t\t\tswp_slot_offset(slot) + swap_offset + i);\n-\t\t\t\ttmp_entry = swp_slot_to_swp_entry(tmp_slot);\n+\t\t\t\tswp_entry_t tmp_entry = swap_nth(swap, swap_offset + i);\n \n \t\t\t\t__xa_store(&mapping->i_pages, aligned_index + i,\n \t\t\t\t\t   swp_to_radix_entry(tmp_entry), 0);\ndiff --git a/mm/swap.h b/mm/swap.h\nindex bdf7aca146643..5eb53758bbd5d 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -36,7 +36,11 @@ struct swap_cluster_info {\n \tu16 count;\n \tu8 flags;\n \tu8 order;\n-\tatomic_long_t __rcu *table;\t/* Swap table entries, see mm/swap_table.h */\n+\t/*\n+\t * Reverse map, to look up the virtual swap slot backed by a given physical\n+\t * swap slot.\n+\t*/\n+\tatomic_long_t __rcu *table;\n \tstruct list_head list;\n };\n \n@@ -212,7 +216,7 @@ static inline struct address_space *swap_address_space(swp_entry_t entry)\n }\n \n /* Return the swap device position of the swap slot. */\n-static inline loff_t swap_slot_pos(swp_slot_t slot)\n+static inline loff_t swap_slot_dev_pos(swp_slot_t slot)\n {\n \treturn ((loff_t)swp_slot_offset(slot)) << PAGE_SHIFT;\n }\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 29ec666be4204..c5ceccd756699 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -891,7 +891,8 @@ static int __init swap_init(void)\n \tswap_kobj = kobject_create_and_add(\"swap\", mm_kobj);\n \tif (!swap_kobj) {\n \t\tpr_err(\"failed to create swap kobject\\n\");\n-\t\treturn -ENOMEM;\n+\t\terr = -ENOMEM;\n+\t\tgoto vswap_exit;\n \t}\n \terr = sysfs_create_group(swap_kobj, &swap_attr_group);\n \tif (err) {\n@@ -904,6 +905,8 @@ static int __init swap_init(void)\n \n delete_obj:\n \tkobject_put(swap_kobj);\n+vswap_exit:\n+\tvswap_exit();\n \treturn err;\n }\n subsys_initcall(swap_init);\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 3f70df488c1da..68ec5d9f05848 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1249,7 +1249,6 @@ static void swap_range_alloc(struct swap_info_struct *si,\n static void swap_range_free(struct swap_info_struct *si, unsigned long offset,\n \t\t\t    unsigned int nr_entries)\n {\n-\tunsigned long begin = offset;\n \tunsigned long end = offset + nr_entries - 1;\n \tvoid (*swap_slot_free_notify)(struct block_device *, unsigned long);\n \tunsigned int i;\n@@ -1258,10 +1257,8 @@ static void swap_range_free(struct swap_info_struct *si, unsigned long offset,\n \t * Use atomic clear_bit operations only on zeromap instead of non-atomic\n \t * bitmap_clear to prevent adjacent bits corruption due to simultaneous writes.\n \t */\n-\tfor (i = 0; i < nr_entries; i++) {\n+\tfor (i = 0; i < nr_entries; i++)\n \t\tclear_bit(offset + i, si->zeromap);\n-\t\tzswap_invalidate(swp_slot_to_swp_entry(swp_slot(si->type, offset + i)));\n-\t}\n \n \tif (si->flags & SWP_BLKDEV)\n \t\tswap_slot_free_notify =\n@@ -1274,7 +1271,6 @@ static void swap_range_free(struct swap_info_struct *si, unsigned long offset,\n \t\t\tswap_slot_free_notify(si->bdev, offset);\n \t\toffset++;\n \t}\n-\tswap_cache_clear_shadow(swp_entry(si->type, begin), nr_entries);\n \n \t/*\n \t * Make sure that try_to_unuse() observes si->inuse_pages reaching 0\n@@ -1405,7 +1401,7 @@ static bool swap_sync_discard(void)\n \treturn false;\n }\n \n-static int swap_slot_alloc(swp_slot_t *slot, unsigned int order)\n+int swap_slot_alloc(swp_slot_t *slot, unsigned int order)\n {\n \tunsigned int size = 1 << order;\n \n@@ -1441,53 +1437,6 @@ static int swap_slot_alloc(swp_slot_t *slot, unsigned int order)\n \treturn 0;\n }\n \n-/**\n- * folio_alloc_swap - allocate swap space for a folio\n- * @folio: folio we want to move to swap\n- *\n- * Allocate swap space for the folio and add the folio to the\n- * swap cache.\n- *\n- * Context: Caller needs to hold the folio lock.\n- * Return: Whether the folio was added to the swap cache.\n- */\n-int folio_alloc_swap(struct folio *folio)\n-{\n-\tunsigned int order = folio_order(folio);\n-\tswp_slot_t slot = { 0 };\n-\tswp_entry_t entry = {};\n-\tint err = 0, ret;\n-\n-\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n-\tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n-\n-\tret = swap_slot_alloc(&slot, order);\n-\tif (ret)\n-\t\treturn ret;\n-\n-\t/* XXX: for now, physical and virtual swap slots are identical */\n-\tentry.val = slot.val;\n-\n-\t/* Need to call this even if allocation failed, for MEMCG_SWAP_FAIL. */\n-\tif (mem_cgroup_try_charge_swap(folio, entry)) {\n-\t\terr = -ENOMEM;\n-\t\tgoto out_free;\n-\t}\n-\n-\tif (!slot.val)\n-\t\treturn -ENOMEM;\n-\n-\terr = swap_cache_add_folio(folio, entry, __GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN, NULL);\n-\tif (err)\n-\t\tgoto out_free;\n-\n-\treturn 0;\n-\n-out_free:\n-\tput_swap_folio(folio, entry);\n-\treturn err;\n-}\n-\n static struct swap_info_struct *_swap_info_get(swp_slot_t slot)\n {\n \tstruct swap_info_struct *si;\n@@ -1733,6 +1682,13 @@ static void swap_slots_free(struct swap_info_struct *si,\n \tunsigned char *map = si->swap_map + offset;\n \tunsigned char *map_end = map + nr_pages;\n \tswp_entry_t entry = swp_slot_to_swp_entry(slot);\n+\tint i;\n+\n+\t/* release all the associated (virtual) swap slots */\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tvswap_free(entry, ci);\n+\t\tentry.val++;\n+\t}\n \n \t/* It should never free entries across different clusters */\n \tVM_BUG_ON(ci != __swap_offset_to_cluster(si, offset + nr_pages - 1));\n@@ -1745,7 +1701,6 @@ static void swap_slots_free(struct swap_info_struct *si,\n \t\t*map = 0;\n \t} while (++map < map_end);\n \n-\tmem_cgroup_uncharge_swap(entry, nr_pages);\n \tswap_range_free(si, offset, nr_pages);\n \n \tif (!ci->count)\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex e68234f053fc9..9aa95558f320a 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -4,7 +4,147 @@\n  *\n  * Copyright (C) 2024 Meta Platforms, Inc., Nhat Pham\n  */\n+#include <linux/mm.h>\n+#include <linux/gfp.h>\n #include <linux/swap.h>\n+#include <linux/swapops.h>\n+#include <linux/swap_cgroup.h>\n+#include <linux/cpuhotplug.h>\n+#include \"swap.h\"\n+#include \"swap_table.h\"\n+\n+/*\n+ * Virtual Swap Space\n+ *\n+ * We associate with each swapped out page a virtual swap slot. This will allow\n+ * us to change the backing state of a swapped out page without having to\n+ * update every single page table entries referring to it.\n+ *\n+ * For now, there is a one-to-one correspondence between a virtual swap slot\n+ * and its associated physical swap slot.\n+ *\n+ * Virtual swap slots are organized into PMD-sized clusters, analogous to\n+ * physical swap allocator. However, unlike the physical swap allocator,\n+ * the clusters are dynamically allocated and freed on-demand. There is no\n+ * \"free list\" of virtual swap clusters - new free clusters are allocated\n+ * directly from the cluster map xarray.\n+ *\n+ * This allows us to avoid the overhead of pre-allocating a large number of\n+ * virtual swap clusters.\n+ */\n+\n+/**\n+ * Swap descriptor - metadata of a swapped out page.\n+ *\n+ * @slot: The handle to the physical swap slot backing this page.\n+ */\n+struct swp_desc {\n+\tswp_slot_t slot;\n+};\n+\n+#define VSWAP_CLUSTER_SHIFT HPAGE_PMD_ORDER\n+#define VSWAP_CLUSTER_SIZE (1UL << VSWAP_CLUSTER_SHIFT)\n+#define VSWAP_CLUSTER_MASK (VSWAP_CLUSTER_SIZE - 1)\n+\n+/*\n+ * Map from a cluster id to the number of allocated virtual swap slots in the\n+ * (PMD-sized) cluster. This allows us to quickly allocate an empty cluster\n+ * for a large folio being swapped out.\n+ *\n+ * This xarray's lock is also used as the \"global\" allocator lock (for e.g, to\n+ * synchronize global cluster lists manipulation).\n+ */\n+static DEFINE_XARRAY_FLAGS(vswap_cluster_map, XA_FLAGS_TRACK_FREE);\n+\n+#if SWP_TYPE_SHIFT > 32\n+/*\n+ * In 64 bit architecture, the maximum number of virtual swap slots is capped\n+ * by the number of clusters (as the vswap_cluster_map xarray can only allocate\n+ * up to U32 clusters).\n+ */\n+#define MAX_VSWAP\t\\\n+\t(((unsigned long)U32_MAX << VSWAP_CLUSTER_SHIFT) + (VSWAP_CLUSTER_SIZE - 1))\n+#else\n+/*\n+ * In 32 bit architecture, just make sure the range of virtual swap slots is\n+ * the same as the range of physical swap slots.\n+ */\n+#define MAX_VSWAP\t(((MAX_SWAPFILES - 1) << SWP_TYPE_SHIFT) | SWP_OFFSET_MASK)\n+#endif\n+\n+static const struct xa_limit vswap_cluster_map_limit = {\n+\t.max = MAX_VSWAP >> VSWAP_CLUSTER_SHIFT,\n+\t.min = 0,\n+};\n+\n+static struct list_head partial_clusters_lists[SWAP_NR_ORDERS];\n+\n+/**\n+ * struct vswap_cluster\n+ *\n+ * @lock: Spinlock protecting the cluster's data\n+ * @rcu: RCU head for deferred freeing when the cluster is no longer in use\n+ * @list: List entry for tracking in partial_clusters_lists when not fully allocated\n+ * @id: Unique identifier for this cluster, used to calculate swap slot values\n+ * @count: Number of allocated virtual swap slots in this cluster\n+ * @order: Order of allocation (0 for single pages, higher for contiguous ranges)\n+ * @cached: Whether this cluster is cached in a per-CPU variable for fast allocation\n+ * @full: Whether this cluster is considered full (no more allocations possible)\n+ * @refcnt: Reference count tracking usage of slots in this cluster\n+ * @bitmap: Bitmap tracking which slots in the cluster are allocated\n+ * @descriptors: Pointer to array of swap descriptors for each slot in the cluster\n+ *\n+ * A vswap_cluster manages a PMD-sized group of contiguous virtual swap slots.\n+ * It tracks which slots are allocated using a bitmap and maintains the\n+ * swap descriptors in an array. The cluster is reference-counted and freed when\n+ * all of its slots are released and the cluster is not cached. Each cluster\n+ * only allocates aligned slots of a single order, determined when the cluster is\n+ * allocated (and never change for the entire lifetime of the cluster).\n+ *\n+ * Clusters can be in the following states:\n+ * - Cached in per-CPU variables for fast allocation.\n+ * - In partial_clusters_lists when partially allocated but not cached.\n+ * - Marked as full when no more allocations are possible.\n+ */\n+struct vswap_cluster {\n+\tspinlock_t lock;\n+\tunion {\n+\t\tstruct rcu_head rcu;\n+\t\tstruct list_head list;\n+\t};\n+\tunsigned long id;\n+\tunsigned int count:VSWAP_CLUSTER_SHIFT + 1;\n+\tunsigned int order:4;\n+\tbool cached:1;\n+\tbool full:1;\n+\trefcount_t refcnt;\n+\tDECLARE_BITMAP(bitmap, VSWAP_CLUSTER_SIZE);\n+\tstruct swp_desc descriptors[VSWAP_CLUSTER_SIZE];\n+};\n+\n+#define VSWAP_VAL_CLUSTER_IDX(val) ((val) >> VSWAP_CLUSTER_SHIFT)\n+#define VSWAP_CLUSTER_IDX(entry) VSWAP_VAL_CLUSTER_IDX(entry.val)\n+#define VSWAP_IDX_WITHIN_CLUSTER_VAL(val) ((val) & VSWAP_CLUSTER_MASK)\n+#define VSWAP_IDX_WITHIN_CLUSTER(entry)\tVSWAP_IDX_WITHIN_CLUSTER_VAL(entry.val)\n+\n+struct percpu_vswap_cluster {\n+\tstruct vswap_cluster *clusters[SWAP_NR_ORDERS];\n+\tlocal_lock_t lock;\n+};\n+\n+/*\n+ * Per-CPU cache of the last allocated cluster for each order. This allows\n+ * allocation fast path to skip the global vswap_cluster_map's spinlock, if\n+ * the locally cached cluster still has free slots. Note that caching a cluster\n+ * also increments its reference count.\n+ */\n+static DEFINE_PER_CPU(struct percpu_vswap_cluster, percpu_vswap_cluster) = {\n+\t.clusters = { NULL, },\n+\t.lock = INIT_LOCAL_LOCK(),\n+};\n+\n+static atomic_t vswap_alloc_reject;\n+static atomic_t vswap_used;\n \n #ifdef CONFIG_DEBUG_FS\n #include <linux/debugfs.h>\n@@ -17,6 +157,10 @@ static int vswap_debug_fs_init(void)\n \t\treturn -ENODEV;\n \n \tvswap_debugfs_root = debugfs_create_dir(\"vswap\", NULL);\n+\tdebugfs_create_atomic_t(\"alloc_reject\", 0444,\n+\t\tvswap_debugfs_root, &vswap_alloc_reject);\n+\tdebugfs_create_atomic_t(\"used\", 0444, vswap_debugfs_root, &vswap_used);\n+\n \treturn 0;\n }\n #else\n@@ -26,10 +170,524 @@ static int vswap_debug_fs_init(void)\n }\n #endif\n \n+static struct swp_desc *vswap_iter(struct vswap_cluster **clusterp, unsigned long i)\n+{\n+\tunsigned long cluster_id = VSWAP_VAL_CLUSTER_IDX(i);\n+\tstruct vswap_cluster *cluster = *clusterp;\n+\tstruct swp_desc *desc = NULL;\n+\tunsigned long slot_index;\n+\n+\tif (!cluster || cluster_id != cluster->id) {\n+\t\tif (cluster)\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\t\tif (!cluster)\n+\t\t\tgoto done;\n+\t\tVM_WARN_ON(cluster->id != cluster_id);\n+\t\tspin_lock(&cluster->lock);\n+\t}\n+\n+\tslot_index = VSWAP_IDX_WITHIN_CLUSTER_VAL(i);\n+\tif (test_bit(slot_index, cluster->bitmap))\n+\t\tdesc = &cluster->descriptors[slot_index];\n+\n+\tif (!desc) {\n+\t\tspin_unlock(&cluster->lock);\n+\t\tcluster = NULL;\n+\t}\n+\n+done:\n+\t*clusterp = cluster;\n+\treturn desc;\n+}\n+\n+static bool cluster_is_alloc_candidate(struct vswap_cluster *cluster)\n+{\n+\treturn cluster->count + (1 << (cluster->order)) <= VSWAP_CLUSTER_SIZE;\n+}\n+\n+static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster, int start)\n+{\n+\tint i, nr = 1 << cluster->order;\n+\tstruct swp_desc *desc;\n+\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = &cluster->descriptors[start + i];\n+\t\tdesc->slot.val = 0;\n+\t}\n+\tcluster->count += nr;\n+}\n+\n+static unsigned long vswap_alloc_from_cluster(struct vswap_cluster *cluster)\n+{\n+\tint nr = 1 << cluster->order;\n+\tunsigned long i = cluster->id ? 0 : nr;\n+\n+\tVM_WARN_ON(!spin_is_locked(&cluster->lock));\n+\tif (!cluster_is_alloc_candidate(cluster))\n+\t\treturn 0;\n+\n+\t/* Find the first free range of nr contiguous aligned slots */\n+\ti = bitmap_find_next_zero_area(cluster->bitmap,\n+\t\t\tVSWAP_CLUSTER_SIZE, i, nr, nr - 1);\n+\tif (i >= VSWAP_CLUSTER_SIZE)\n+\t\treturn 0;\n+\n+\t/* Mark the range as allocated in the bitmap */\n+\tbitmap_set(cluster->bitmap, i, nr);\n+\n+\trefcount_add(nr, &cluster->refcnt);\n+\t__vswap_alloc_from_cluster(cluster, i);\n+\treturn i + (cluster->id << VSWAP_CLUSTER_SHIFT);\n+}\n+\n+/* Allocate a contiguous range of virtual swap slots */\n+static swp_entry_t vswap_alloc(int order)\n+{\n+\tstruct xa_limit limit = vswap_cluster_map_limit;\n+\tstruct vswap_cluster *local, *cluster;\n+\tint nr = 1 << order;\n+\tbool need_caching = true;\n+\tu32 cluster_id;\n+\tswp_entry_t entry;\n+\n+\tentry.val = 0;\n+\n+\t/* first, let's try the locally cached cluster */\n+\trcu_read_lock();\n+\tlocal_lock(&percpu_vswap_cluster.lock);\n+\tcluster = this_cpu_read(percpu_vswap_cluster.clusters[order]);\n+\tif (cluster) {\n+\t\tspin_lock(&cluster->lock);\n+\t\tentry.val = vswap_alloc_from_cluster(cluster);\n+\t\tneed_caching = !entry.val;\n+\n+\t\tif (!entry.val || !cluster_is_alloc_candidate(cluster)) {\n+\t\t\tthis_cpu_write(percpu_vswap_cluster.clusters[order], NULL);\n+\t\t\tcluster->cached = false;\n+\t\t\trefcount_dec(&cluster->refcnt);\n+\t\t\tcluster->full = true;\n+\t\t}\n+\t\tspin_unlock(&cluster->lock);\n+\t}\n+\tlocal_unlock(&percpu_vswap_cluster.lock);\n+\trcu_read_unlock();\n+\n+\t/*\n+\t * Local cluster does not have space. Let's try the uncached partial\n+\t * clusters before acquiring a new free cluster to reduce fragmentation,\n+\t * and avoid having to allocate a new cluster structure.\n+\t */\n+\tif (!entry.val) {\n+\t\tcluster = NULL;\n+\t\txa_lock(&vswap_cluster_map);\n+\t\tlist_for_each_entry_safe(cluster, local,\n+\t\t\t\t&partial_clusters_lists[order], list) {\n+\t\t\tif (!spin_trylock(&cluster->lock))\n+\t\t\t\tcontinue;\n+\n+\t\t\tentry.val = vswap_alloc_from_cluster(cluster);\n+\t\t\tlist_del_init(&cluster->list);\n+\t\t\tcluster->full = !entry.val || !cluster_is_alloc_candidate(cluster);\n+\t\t\tneed_caching = !cluster->full;\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t\tif (entry.val)\n+\t\t\t\tbreak;\n+\t\t}\n+\t\txa_unlock(&vswap_cluster_map);\n+\t}\n+\n+\t/* try a new free cluster */\n+\tif (!entry.val) {\n+\t\tcluster = kvzalloc(sizeof(*cluster), GFP_KERNEL);\n+\t\tif (cluster) {\n+\t\t\t/* first cluster cannot allocate a PMD-sized THP */\n+\t\t\tif (order == SWAP_NR_ORDERS - 1)\n+\t\t\t\tlimit.min = 1;\n+\n+\t\t\tif (!xa_alloc(&vswap_cluster_map, &cluster_id, cluster, limit,\n+\t\t\t\t\t\tGFP_KERNEL)) {\n+\t\t\t\tspin_lock_init(&cluster->lock);\n+\t\t\t\tcluster->id = cluster_id;\n+\t\t\t\tcluster->order = order;\n+\t\t\t\tINIT_LIST_HEAD(&cluster->list);\n+\t\t\t\t/* Initialize bitmap to all zeros (all slots free) */\n+\t\t\t\tbitmap_zero(cluster->bitmap, VSWAP_CLUSTER_SIZE);\n+\t\t\t\tentry.val = cluster->id << VSWAP_CLUSTER_SHIFT;\n+\t\t\t\trefcount_set(&cluster->refcnt, nr);\n+\t\t\t\tif (!cluster_id)\n+\t\t\t\t\tentry.val += nr;\n+\t\t\t\t__vswap_alloc_from_cluster(cluster,\n+\t\t\t\t\t(entry.val & VSWAP_CLUSTER_MASK));\n+\t\t\t\t/* Mark the allocated range in the bitmap */\n+\t\t\t\tbitmap_set(cluster->bitmap, (entry.val & VSWAP_CLUSTER_MASK), nr);\n+\t\t\t\tneed_caching = cluster_is_alloc_candidate(cluster);\n+\t\t\t} else {\n+\t\t\t\t/* Failed to insert into cluster map, free the cluster */\n+\t\t\t\tkvfree(cluster);\n+\t\t\t\tcluster = NULL;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif (need_caching && entry.val) {\n+\t\tlocal_lock(&percpu_vswap_cluster.lock);\n+\t\tlocal = this_cpu_read(percpu_vswap_cluster.clusters[order]);\n+\t\tif (local != cluster) {\n+\t\t\tif (local) {\n+\t\t\t\tspin_lock(&local->lock);\n+\t\t\t\t/* only update the local cache if cached cluster is full */\n+\t\t\t\tneed_caching = !cluster_is_alloc_candidate(local);\n+\t\t\t\tif (need_caching) {\n+\t\t\t\t\tthis_cpu_write(percpu_vswap_cluster.clusters[order], NULL);\n+\t\t\t\t\tlocal->cached = false;\n+\t\t\t\t\trefcount_dec(&local->refcnt);\n+\t\t\t\t}\n+\t\t\t\tspin_unlock(&local->lock);\n+\t\t\t}\n+\n+\t\t\tVM_WARN_ON(!cluster);\n+\t\t\tspin_lock(&cluster->lock);\n+\t\t\tif (cluster_is_alloc_candidate(cluster)) {\n+\t\t\t\tif (need_caching) {\n+\t\t\t\t\tthis_cpu_write(percpu_vswap_cluster.clusters[order], cluster);\n+\t\t\t\t\trefcount_inc(&cluster->refcnt);\n+\t\t\t\t\tcluster->cached = true;\n+\t\t\t\t} else {\n+\t\t\t\t\txa_lock(&vswap_cluster_map);\n+\t\t\t\t\tVM_WARN_ON(!list_empty(&cluster->list));\n+\t\t\t\t\tlist_add(&cluster->list, &partial_clusters_lists[order]);\n+\t\t\t\t\txa_unlock(&vswap_cluster_map);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t}\n+\t\tlocal_unlock(&percpu_vswap_cluster.lock);\n+\t}\n+\n+\tif (entry.val) {\n+\t\tVM_WARN_ON(entry.val + nr - 1 > MAX_VSWAP);\n+\t\tatomic_add(nr, &vswap_used);\n+\t} else {\n+\t\tatomic_add(nr, &vswap_alloc_reject);\n+\t}\n+\treturn entry;\n+}\n+\n+static void vswap_cluster_free(struct vswap_cluster *cluster)\n+{\n+\tVM_WARN_ON(cluster->count || cluster->cached);\n+\tVM_WARN_ON(!spin_is_locked(&cluster->lock));\n+\txa_lock(&vswap_cluster_map);\n+\tlist_del_init(&cluster->list);\n+\t__xa_erase(&vswap_cluster_map, cluster->id);\n+\txa_unlock(&vswap_cluster_map);\n+\trcu_head_init(&cluster->rcu);\n+\tkvfree_rcu(cluster, rcu);\n+}\n+\n+static inline void release_vswap_slot(struct vswap_cluster *cluster,\n+\t\tunsigned long index)\n+{\n+\tunsigned long slot_index = VSWAP_IDX_WITHIN_CLUSTER_VAL(index);\n+\n+\tVM_WARN_ON(!spin_is_locked(&cluster->lock));\n+\tcluster->count--;\n+\n+\tbitmap_clear(cluster->bitmap, slot_index, 1);\n+\n+\t/* we only free uncached empty clusters */\n+\tif (refcount_dec_and_test(&cluster->refcnt))\n+\t\tvswap_cluster_free(cluster);\n+\telse if (cluster->full && cluster_is_alloc_candidate(cluster)) {\n+\t\tcluster->full = false;\n+\t\tif (!cluster->cached) {\n+\t\t\txa_lock(&vswap_cluster_map);\n+\t\t\tVM_WARN_ON(!list_empty(&cluster->list));\n+\t\t\tlist_add_tail(&cluster->list,\n+\t\t\t\t&partial_clusters_lists[cluster->order]);\n+\t\t\txa_unlock(&vswap_cluster_map);\n+\t\t}\n+\t}\n+\n+\tatomic_dec(&vswap_used);\n+}\n+\n+/*\n+ * Update the physical-to-virtual swap slot mapping.\n+ * Caller must ensure the physical swap slot's cluster is locked.\n+ */\n+static void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n+\t\t\t   unsigned long vswap, int nr)\n+{\n+\tatomic_long_t *table;\n+\tunsigned long slot_offset = swp_slot_offset(slot);\n+\tunsigned int ci_off = slot_offset % SWAPFILE_CLUSTER;\n+\tint i;\n+\n+\ttable = rcu_dereference_protected(ci->table, lockdep_is_held(&ci->lock));\n+\tVM_WARN_ON(!table);\n+\tfor (i = 0; i < nr; i++)\n+\t\t__swap_table_set(ci, ci_off + i, vswap ? vswap + i : 0);\n+}\n+\n+/**\n+ * vswap_free - free a virtual swap slot.\n+ * @entry: the virtual swap slot to free\n+ * @ci: the physical swap slot's cluster (optional, can be NULL)\n+ *\n+ * If @ci is NULL, this function is called to clean up a virtual swap entry\n+ * when no linkage has been established between physical and virtual swap slots.\n+ * If @ci is provided, the caller must ensure it is locked.\n+ */\n+void vswap_free(swp_entry_t entry, struct swap_cluster_info *ci)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\n+\tif (!entry.val)\n+\t\treturn;\n+\n+\tswap_cache_clear_shadow(entry, 1);\n+\tzswap_invalidate(entry);\n+\tmem_cgroup_uncharge_swap(entry, 1);\n+\n+\t/* do not immediately erase the virtual slot to prevent its reuse */\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn;\n+\t}\n+\n+\tif (desc->slot.val)\n+\t\tvswap_rmap_set(ci, desc->slot, 0, 1);\n+\n+\t/* erase forward mapping and release the virtual slot for reallocation */\n+\trelease_vswap_slot(cluster, entry.val);\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+/**\n+ * folio_alloc_swap - allocate swap space for a folio.\n+ * @folio: the folio.\n+ *\n+ * Return: 0, if the allocation succeeded, -ENOMEM, if the allocation failed.\n+ */\n+int folio_alloc_swap(struct folio *folio)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swap_info_struct *si;\n+\tstruct swap_cluster_info *ci;\n+\tint i, err, nr = folio_nr_pages(folio), order = folio_order(folio);\n+\tstruct swp_desc *desc;\n+\tswp_entry_t entry;\n+\tswp_slot_t slot;\n+\n+\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n+\tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n+\n+\tentry = vswap_alloc(folio_order(folio));\n+\tif (!entry.val)\n+\t\treturn -ENOMEM;\n+\n+\t/*\n+\t * XXX: for now, we always allocate a physical swap slot for each virtual\n+\t * swap slot, and their lifetime are coupled. This will change once we\n+\t * decouple virtual swap slots from their backing states, and only allocate\n+\t * physical swap slots for them on demand (i.e on zswap writeback, or\n+\t * fallback from zswap store failure).\n+\t */\n+\tif (swap_slot_alloc(&slot, order)) {\n+\t\tfor (i = 0; i < nr; i++)\n+\t\t\tvswap_free((swp_entry_t){entry.val + i}, NULL);\n+\t\tentry.val = 0;\n+\t\treturn -ENOMEM;\n+\t}\n+\n+\t/* establish the vrtual <-> physical swap slots linkages. */\n+\tsi = __swap_slot_to_info(slot);\n+\tci = swap_cluster_lock(si, swp_slot_offset(slot));\n+\tvswap_rmap_set(ci, slot, entry.val, nr);\n+\tswap_cluster_unlock(ci);\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\n+\t\tdesc->slot.val = slot.val + i;\n+\t}\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\t/*\n+\t * XXX: for now, we charge towards the memory cgroup's swap limit on virtual\n+\t * swap slots allocation. This is acceptable because as noted above, each\n+\t * virtual swap slot corresponds to a physical swap slot. Once we have\n+\t * decoupled virtual and physical swap slots, we will only charge when we\n+\t * actually allocate a physical swap slot.\n+\t */\n+\tif (mem_cgroup_try_charge_swap(folio, entry))\n+\t\tgoto out_free;\n+\n+\terr = swap_cache_add_folio(folio, entry, __GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN, NULL);\n+\tif (err)\n+\t\tgoto out_free;\n+\n+\treturn 0;\n+\n+out_free:\n+\tput_swap_folio(folio, entry);\n+\treturn -ENOMEM;\n+}\n+\n+/**\n+ * swp_entry_to_swp_slot - look up the physical swap slot corresponding to a\n+ *                         virtual swap slot.\n+ * @entry: the virtual swap slot.\n+ *\n+ * Return: the physical swap slot corresponding to the virtual swap slot.\n+ */\n+swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tswp_slot_t slot;\n+\n+\tslot.val = 0;\n+\tif (!entry.val)\n+\t\treturn slot;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn (swp_slot_t){0};\n+\t}\n+\tslot = desc->slot;\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\treturn slot;\n+}\n+\n+/**\n+ * swp_slot_to_swp_entry - look up the virtual swap slot corresponding to a\n+ *                         physical swap slot.\n+ * @slot: the physical swap slot.\n+ *\n+ * Return: the virtual swap slot corresponding to the physical swap slot.\n+ */\n+swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot)\n+{\n+\tswp_entry_t ret;\n+\tstruct swap_cluster_info *ci;\n+\tunsigned long offset;\n+\tunsigned int ci_off;\n+\n+\tret.val = 0;\n+\tif (!slot.val)\n+\t\treturn ret;\n+\n+\toffset = swp_slot_offset(slot);\n+\tci_off = offset % SWAPFILE_CLUSTER;\n+\tci = __swap_slot_to_cluster(slot);\n+\n+\tret.val = swap_table_get(ci, ci_off);\n+\treturn ret;\n+}\n+\n+bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si)\n+{\n+\tstruct vswap_cluster *cluster;\n+\tswp_slot_t slot;\n+\n+\tslot = swp_entry_to_swp_slot(entry);\n+\t*si = swap_slot_tryget_swap_info(slot);\n+\tif (!*si)\n+\t\treturn false;\n+\n+\t/*\n+\t * Ensure the cluster and its associated data structures (swap cache etc.)\n+\t * remain valid.\n+\t */\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, VSWAP_CLUSTER_IDX(entry));\n+\tif (!cluster || !refcount_inc_not_zero(&cluster->refcnt)) {\n+\t\trcu_read_unlock();\n+\t\tswap_slot_put_swap_info(*si);\n+\t\t*si = NULL;\n+\t\treturn false;\n+\t}\n+\trcu_read_unlock();\n+\treturn true;\n+}\n+\n+void put_swap_entry(swp_entry_t entry, struct swap_info_struct *si)\n+{\n+\tstruct vswap_cluster *cluster;\n+\n+\tif (si)\n+\t\tswap_slot_put_swap_info(si);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, VSWAP_CLUSTER_IDX(entry));\n+\tspin_lock(&cluster->lock);\n+\tif (refcount_dec_and_test(&cluster->refcnt))\n+\t\tvswap_cluster_free(cluster);\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+static int vswap_cpu_dead(unsigned int cpu)\n+{\n+\tstruct percpu_vswap_cluster *percpu_cluster;\n+\tstruct vswap_cluster *cluster;\n+\tint order;\n+\n+\tpercpu_cluster = per_cpu_ptr(&percpu_vswap_cluster, cpu);\n+\n+\trcu_read_lock();\n+\tlocal_lock(&percpu_cluster->lock);\n+\tfor (order = 0; order < SWAP_NR_ORDERS; order++) {\n+\t\tcluster = percpu_cluster->clusters[order];\n+\t\tif (cluster) {\n+\t\t\tpercpu_cluster->clusters[order] = NULL;\n+\t\t\tspin_lock(&cluster->lock);\n+\t\t\tcluster->cached = false;\n+\t\t\tif (refcount_dec_and_test(&cluster->refcnt))\n+\t\t\t\tvswap_cluster_free(cluster);\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t}\n+\t}\n+\tlocal_unlock(&percpu_cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn 0;\n+}\n+\n+\n int vswap_init(void)\n {\n+\tint i;\n+\n+\tif (cpuhp_setup_state_nocalls(CPUHP_MM_VSWAP_DEAD, \"mm/vswap:dead\", NULL,\n+\t\t\t\tvswap_cpu_dead)) {\n+\t\tpr_err(\"Failed to register vswap CPU hotplug callback\\n\");\n+\t\treturn -ENOMEM;\n+\t}\n+\n \tif (vswap_debug_fs_init())\n \t\tpr_warn(\"Failed to initialize vswap debugfs\\n\");\n \n+\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n+\t\tINIT_LIST_HEAD(&partial_clusters_lists[i]);\n+\n \treturn 0;\n }\n+\n+void vswap_exit(void)\n+{\n+}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the poor batching behavior of vswap free path, explaining that they will move the swap cache and working set shadow to the virtual swap descriptor, effectively range-partitioning the swap cache by virtual swap clusters (of PMD sized), which eliminates swap cache lock contention.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed with the approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Move the swap cache (and workingset shadow for anonymous pages) to the\nvirtual swap descriptor. This effectively range-partitions the swap\ncache by virtual swap clusters (of PMD sized), eliminate swap cache lock\ncontention.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n mm/huge_memory.c |   4 +-\n mm/migrate.c     |   6 +-\n mm/shmem.c       |   4 +-\n mm/swap.h        |  16 +--\n mm/swap_state.c  | 251 +--------------------------------\n mm/vmscan.c      |   6 +-\n mm/vswap.c       | 350 ++++++++++++++++++++++++++++++++++++++++++++++-\n 7 files changed, 364 insertions(+), 273 deletions(-)\n\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 21215ac870144..dcbd3821d6178 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -3825,7 +3825,7 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \t\t\t\treturn -EINVAL;\n \t\t\t}\n \n-\t\t\tswap_cache_lock();\n+\t\t\tswap_cache_lock(folio->swap);\n \t\t}\n \n \t\t/* lock lru list/PageCompound, ref frozen by page_ref_freeze */\n@@ -3901,7 +3901,7 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \t\t\tunlock_page_lruvec(lruvec);\n \n \t\tif (folio_test_swapcache(folio))\n-\t\t\tswap_cache_unlock();\n+\t\t\tswap_cache_unlock(folio->swap);\n \t} else {\n \t\tsplit_queue_unlock(ds_queue);\n \t\treturn -EAGAIN;\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 11d9b43dff5d8..e850b05a232de 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -600,13 +600,13 @@ static int __folio_migrate_mapping(struct address_space *mapping,\n \tnewzone = folio_zone(newfolio);\n \n \tif (folio_test_swapcache(folio))\n-\t\tswap_cache_lock_irq();\n+\t\tswap_cache_lock_irq(folio->swap);\n \telse\n \t\txas_lock_irq(&xas);\n \n \tif (!folio_ref_freeze(folio, expected_count)) {\n \t\tif (folio_test_swapcache(folio))\n-\t\t\tswap_cache_unlock_irq();\n+\t\t\tswap_cache_unlock_irq(folio->swap);\n \t\telse\n \t\t\txas_unlock_irq(&xas);\n \t\treturn -EAGAIN;\n@@ -652,7 +652,7 @@ static int __folio_migrate_mapping(struct address_space *mapping,\n \n \t/* Leave irq disabled to prevent preemption while updating stats */\n \tif (folio_test_swapcache(folio))\n-\t\tswap_cache_unlock();\n+\t\tswap_cache_unlock(folio->swap);\n \telse\n \t\txas_unlock(&xas);\n \ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 13f7469a04c8a..66cf8af6779ca 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -2168,12 +2168,12 @@ static int shmem_replace_folio(struct folio **foliop, gfp_t gfp,\n \tnew->swap = entry;\n \tfolio_set_swapcache(new);\n \n-\tswap_cache_lock_irq();\n+\tswap_cache_lock_irq(entry);\n \t__swap_cache_replace_folio(old, new);\n \tmem_cgroup_replace_folio(old, new);\n \tshmem_update_stats(new, nr_pages);\n \tshmem_update_stats(old, -nr_pages);\n-\tswap_cache_unlock_irq();\n+\tswap_cache_unlock_irq(entry);\n \n \tfolio_add_lru(new);\n \t*foliop = new;\ndiff --git a/mm/swap.h b/mm/swap.h\nindex 5eb53758bbd5d..57ed24a2d6356 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -205,10 +205,12 @@ void __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug);\n \n /* linux/mm/swap_state.c */\n extern struct address_space swap_space __read_mostly;\n-void swap_cache_lock_irq(void);\n-void swap_cache_unlock_irq(void);\n-void swap_cache_lock(void);\n-void swap_cache_unlock(void);\n+\n+/* linux/mm/vswap.c */\n+void swap_cache_lock_irq(swp_entry_t entry);\n+void swap_cache_unlock_irq(swp_entry_t entry);\n+void swap_cache_lock(swp_entry_t entry);\n+void swap_cache_unlock(swp_entry_t entry);\n \n static inline struct address_space *swap_address_space(swp_entry_t entry)\n {\n@@ -256,12 +258,11 @@ static inline bool folio_matches_swap_entry(const struct folio *folio,\n  */\n struct folio *swap_cache_get_folio(swp_entry_t entry);\n void *swap_cache_get_shadow(swp_entry_t entry);\n-int swap_cache_add_folio(struct folio *folio, swp_entry_t entry, gfp_t gfp, void **shadow);\n+void swap_cache_add_folio(struct folio *folio, swp_entry_t entry, void **shadow);\n void swap_cache_del_folio(struct folio *folio);\n /* Below helpers require the caller to lock the swap cache. */\n void __swap_cache_del_folio(struct folio *folio, swp_entry_t entry, void *shadow);\n void __swap_cache_replace_folio(struct folio *old, struct folio *new);\n-void swap_cache_clear_shadow(swp_entry_t entry, int nr_ents);\n \n void show_swap_cache_info(void);\n void swapcache_clear(struct swap_info_struct *si, swp_entry_t entry, int nr);\n@@ -422,9 +423,8 @@ static inline void *swap_cache_get_shadow(swp_entry_t entry)\n \treturn NULL;\n }\n \n-static inline int swap_cache_add_folio(struct folio *folio, swp_entry_t entry, gfp_t gfp, void **shadow)\n+static inline void swap_cache_add_folio(struct folio *folio, swp_entry_t entry, void **shadow)\n {\n-\treturn 0;\n }\n \n static inline void swap_cache_del_folio(struct folio *folio)\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex c5ceccd756699..00fa3e76a5c19 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -41,28 +41,6 @@ struct address_space swap_space __read_mostly = {\n \t.a_ops = &swap_aops,\n };\n \n-static DEFINE_XARRAY(swap_cache);\n-\n-void swap_cache_lock_irq(void)\n-{\n-\txa_lock_irq(&swap_cache);\n-}\n-\n-void swap_cache_unlock_irq(void)\n-{\n-\txa_unlock_irq(&swap_cache);\n-}\n-\n-void swap_cache_lock(void)\n-{\n-\txa_lock(&swap_cache);\n-}\n-\n-void swap_cache_unlock(void)\n-{\n-\txa_unlock(&swap_cache);\n-}\n-\n static bool enable_vma_readahead __read_mostly = true;\n \n #define SWAP_RA_ORDER_CEILING\t5\n@@ -94,231 +72,6 @@ void show_swap_cache_info(void)\n \tprintk(\"Total swap = %lukB\\n\", K(total_swap_pages));\n }\n \n-/**\n- * swap_cache_get_folio - Looks up a folio in the swap cache.\n- * @entry: swap entry used for the lookup.\n- *\n- * A found folio will be returned unlocked and with its refcount increased.\n- *\n- * Context: Caller must ensure @entry is valid and protect the swap device\n- * with reference count or locks.\n- * Return: Returns the found folio on success, NULL otherwise. The caller\n- * must lock nd check if the folio still matches the swap entry before\n- * use (e.g., folio_matches_swap_entry).\n- */\n-struct folio *swap_cache_get_folio(swp_entry_t entry)\n-{\n-\tvoid *entry_val;\n-\tstruct folio *folio;\n-\n-\tfor (;;) {\n-\t\trcu_read_lock();\n-\t\tentry_val = xa_load(&swap_cache, entry.val);\n-\t\tif (!entry_val || xa_is_value(entry_val)) {\n-\t\t\trcu_read_unlock();\n-\t\t\treturn NULL;\n-\t\t}\n-\t\tfolio = entry_val;\n-\t\tif (likely(folio_try_get(folio))) {\n-\t\t\trcu_read_unlock();\n-\t\t\treturn folio;\n-\t\t}\n-\t\trcu_read_unlock();\n-\t}\n-\n-\treturn NULL;\n-}\n-\n-/**\n- * swap_cache_get_shadow - Looks up a shadow in the swap cache.\n- * @entry: swap entry used for the lookup.\n- *\n- * Context: Caller must ensure @entry is valid and protect the swap device\n- * with reference count or locks.\n- * Return: Returns either NULL or an XA_VALUE (shadow).\n- */\n-void *swap_cache_get_shadow(swp_entry_t entry)\n-{\n-\tvoid *entry_val;\n-\n-\trcu_read_lock();\n-\tentry_val = xa_load(&swap_cache, entry.val);\n-\trcu_read_unlock();\n-\n-\tif (xa_is_value(entry_val))\n-\t\treturn entry_val;\n-\treturn NULL;\n-}\n-\n-/**\n- * swap_cache_add_folio - Add a folio into the swap cache.\n- * @folio: The folio to be added.\n- * @entry: The swap entry corresponding to the folio.\n- * @gfp: gfp_mask for XArray node allocation.\n- * @shadowp: If a shadow is found, return the shadow.\n- *\n- * Context: Caller must ensure @entry is valid and protect the swap device\n- * with reference count or locks.\n- * The caller also needs to update the corresponding swap_map slots with\n- * SWAP_HAS_CACHE bit to avoid race or conflict.\n- *\n- * Return: 0 on success, negative error code on failure.\n- */\n-int swap_cache_add_folio(struct folio *folio, swp_entry_t entry, gfp_t gfp, void **shadowp)\n-{\n-\tXA_STATE_ORDER(xas, &swap_cache, entry.val, folio_order(folio));\n-\tunsigned long nr_pages = folio_nr_pages(folio);\n-\tunsigned long i;\n-\tvoid *old;\n-\n-\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n-\tVM_WARN_ON_ONCE_FOLIO(folio_test_swapcache(folio), folio);\n-\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapbacked(folio), folio);\n-\n-\tfolio_ref_add(folio, nr_pages);\n-\tfolio_set_swapcache(folio);\n-\tfolio->swap = entry;\n-\n-\tdo {\n-\t\txas_lock_irq(&xas);\n-\t\txas_create_range(&xas);\n-\t\tif (xas_error(&xas))\n-\t\t\tgoto unlock;\n-\t\tfor (i = 0; i < nr_pages; i++) {\n-\t\t\tVM_BUG_ON_FOLIO(xas.xa_index != entry.val + i, folio);\n-\t\t\told = xas_load(&xas);\n-\t\t\tif (old && !xa_is_value(old)) {\n-\t\t\t\tVM_WARN_ON_ONCE_FOLIO(1, folio);\n-\t\t\t\txas_set_err(&xas, -EEXIST);\n-\t\t\t\tgoto unlock;\n-\t\t\t}\n-\t\t\tif (shadowp && xa_is_value(old) && !*shadowp)\n-\t\t\t\t*shadowp = old;\n-\t\t\txas_store(&xas, folio);\n-\t\t\txas_next(&xas);\n-\t\t}\n-\t\tnode_stat_mod_folio(folio, NR_FILE_PAGES, nr_pages);\n-\t\tlruvec_stat_mod_folio(folio, NR_SWAPCACHE, nr_pages);\n-unlock:\n-\t\txas_unlock_irq(&xas);\n-\t} while (xas_nomem(&xas, gfp));\n-\n-\tif (!xas_error(&xas))\n-\t\treturn 0;\n-\n-\tfolio_clear_swapcache(folio);\n-\tfolio_ref_sub(folio, nr_pages);\n-\treturn xas_error(&xas);\n-}\n-\n-/**\n- * __swap_cache_del_folio - Removes a folio from the swap cache.\n- * @folio: The folio.\n- * @entry: The first swap entry that the folio corresponds to.\n- * @shadow: shadow value to be filled in the swap cache.\n- *\n- * Removes a folio from the swap cache and fills a shadow in place.\n- * This won't put the folio's refcount. The caller has to do that.\n- *\n- * Context: Caller must ensure the folio is locked and in the swap cache\n- * using the index of @entry, and lock the swap cache xarray.\n- */\n-void __swap_cache_del_folio(struct folio *folio, swp_entry_t entry, void *shadow)\n-{\n-\tlong nr_pages = folio_nr_pages(folio);\n-\tXA_STATE(xas, &swap_cache, entry.val);\n-\tint i;\n-\n-\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n-\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n-\tVM_WARN_ON_ONCE_FOLIO(folio_test_writeback(folio), folio);\n-\n-\tfor (i = 0; i < nr_pages; i++) {\n-\t\tvoid *old = xas_store(&xas, shadow);\n-\t\tVM_WARN_ON_FOLIO(old != folio, folio);\n-\t\txas_next(&xas);\n-\t}\n-\n-\tfolio->swap.val = 0;\n-\tfolio_clear_swapcache(folio);\n-\tnode_stat_mod_folio(folio, NR_FILE_PAGES, -nr_pages);\n-\tlruvec_stat_mod_folio(folio, NR_SWAPCACHE, -nr_pages);\n-}\n-\n-/**\n- * swap_cache_del_folio - Removes a folio from the swap cache.\n- * @folio: The folio.\n- *\n- * Same as __swap_cache_del_folio, but handles lock and refcount. The\n- * caller must ensure the folio is either clean or has a swap count\n- * equal to zero, or it may cause data loss.\n- *\n- * Context: Caller must ensure the folio is locked and in the swap cache.\n- */\n-void swap_cache_del_folio(struct folio *folio)\n-{\n-\tswp_entry_t entry = folio->swap;\n-\n-\txa_lock_irq(&swap_cache);\n-\t__swap_cache_del_folio(folio, entry, NULL);\n-\txa_unlock_irq(&swap_cache);\n-\n-\tput_swap_folio(folio, entry);\n-\tfolio_ref_sub(folio, folio_nr_pages(folio));\n-}\n-\n-/**\n- * __swap_cache_replace_folio - Replace a folio in the swap cache.\n- * @old: The old folio to be replaced.\n- * @new: The new folio.\n- *\n- * Replace an existing folio in the swap cache with a new folio. The\n- * caller is responsible for setting up the new folio's flag and swap\n- * entries. Replacement will take the new folio's swap entry value as\n- * the starting offset to override all slots covered by the new folio.\n- *\n- * Context: Caller must ensure both folios are locked, and lock the\n- * swap cache xarray.\n- */\n-void __swap_cache_replace_folio(struct folio *old, struct folio *new)\n-{\n-\tswp_entry_t entry = new->swap;\n-\tunsigned long nr_pages = folio_nr_pages(new);\n-\tXA_STATE(xas, &swap_cache, entry.val);\n-\tint i;\n-\n-\tVM_WARN_ON_ONCE(!folio_test_swapcache(old) || !folio_test_swapcache(new));\n-\tVM_WARN_ON_ONCE(!folio_test_locked(old) || !folio_test_locked(new));\n-\tVM_WARN_ON_ONCE(!entry.val);\n-\n-\tfor (i = 0; i < nr_pages; i++) {\n-\t\tvoid *old_entry = xas_store(&xas, new);\n-\t\tWARN_ON_ONCE(!old_entry || xa_is_value(old_entry) || old_entry != old);\n-\t\txas_next(&xas);\n-\t}\n-}\n-\n-/**\n- * swap_cache_clear_shadow - Clears a set of shadows in the swap cache.\n- * @entry: The starting index entry.\n- * @nr_ents: How many slots need to be cleared.\n- *\n- * Context: Caller must ensure the range is valid and all in one single cluster,\n- * not occupied by any folio.\n- */\n-void swap_cache_clear_shadow(swp_entry_t entry, int nr_ents)\n-{\n-\tXA_STATE(xas, &swap_cache, entry.val);\n-\tint i;\n-\n-\txas_lock(&xas);\n-\tfor (i = 0; i < nr_ents; i++) {\n-\t\txas_store(&xas, NULL);\n-\t\txas_next(&xas);\n-\t}\n-\txas_unlock(&xas);\n-}\n-\n /*\n  * If we are the only user, then try to free up the swap cache.\n  *\n@@ -497,9 +250,7 @@ struct folio *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \tif (mem_cgroup_swapin_charge_folio(new_folio, NULL, gfp_mask, entry))\n \t\tgoto fail_unlock;\n \n-\t/* May fail (-ENOMEM) if XArray node allocation failed. */\n-\tif (swap_cache_add_folio(new_folio, entry, gfp_mask & GFP_RECLAIM_MASK, &shadow))\n-\t\tgoto fail_unlock;\n+\tswap_cache_add_folio(new_folio, entry, &shadow);\n \n \tmemcg1_swapin(entry, 1);\n \ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 558ff7f413786..c9ec1a1458b4e 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -712,7 +712,7 @@ static int __remove_mapping(struct address_space *mapping, struct folio *folio,\n \tBUG_ON(mapping != folio_mapping(folio));\n \n \tif (folio_test_swapcache(folio)) {\n-\t\tswap_cache_lock_irq();\n+\t\tswap_cache_lock_irq(folio->swap);\n \t} else {\n \t\tspin_lock(&mapping->host->i_lock);\n \t\txa_lock_irq(&mapping->i_pages);\n@@ -759,7 +759,7 @@ static int __remove_mapping(struct address_space *mapping, struct folio *folio,\n \t\t\tshadow = workingset_eviction(folio, target_memcg);\n \t\t__swap_cache_del_folio(folio, swap, shadow);\n \t\tmemcg1_swapout(folio, swap);\n-\t\tswap_cache_unlock_irq();\n+\t\tswap_cache_unlock_irq(swap);\n \t\tput_swap_folio(folio, swap);\n \t} else {\n \t\tvoid (*free_folio)(struct folio *);\n@@ -798,7 +798,7 @@ static int __remove_mapping(struct address_space *mapping, struct folio *folio,\n \n cannot_free:\n \tif (folio_test_swapcache(folio)) {\n-\t\tswap_cache_unlock_irq();\n+\t\tswap_cache_unlock_irq(folio->swap);\n \t} else {\n \t\txa_unlock_irq(&mapping->i_pages);\n \t\tspin_unlock(&mapping->host->i_lock);\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex 9aa95558f320a..d44199dc059a3 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -37,9 +37,15 @@\n  * Swap descriptor - metadata of a swapped out page.\n  *\n  * @slot: The handle to the physical swap slot backing this page.\n+ * @swap_cache: The folio in swap cache.\n+ * @shadow: The shadow entry.\n  */\n struct swp_desc {\n \tswp_slot_t slot;\n+\tunion {\n+\t\tstruct folio *swap_cache;\n+\t\tvoid *shadow;\n+\t};\n };\n \n #define VSWAP_CLUSTER_SHIFT HPAGE_PMD_ORDER\n@@ -170,6 +176,24 @@ static int vswap_debug_fs_init(void)\n }\n #endif\n \n+/*\n+ * Lockless version of vswap_iter - assumes caller holds cluster lock.\n+ * Used when iterating within the same cluster with the lock already held.\n+ */\n+static struct swp_desc *__vswap_iter(struct vswap_cluster *cluster, unsigned long i)\n+{\n+\tunsigned long slot_index;\n+\n+\tlockdep_assert_held(&cluster->lock);\n+\tVM_WARN_ON(cluster->id != VSWAP_VAL_CLUSTER_IDX(i));\n+\n+\tslot_index = VSWAP_IDX_WITHIN_CLUSTER_VAL(i);\n+\tif (test_bit(slot_index, cluster->bitmap))\n+\t\treturn &cluster->descriptors[slot_index];\n+\n+\treturn NULL;\n+}\n+\n static struct swp_desc *vswap_iter(struct vswap_cluster **clusterp, unsigned long i)\n {\n \tunsigned long cluster_id = VSWAP_VAL_CLUSTER_IDX(i);\n@@ -448,7 +472,6 @@ void vswap_free(swp_entry_t entry, struct swap_cluster_info *ci)\n \tif (!entry.val)\n \t\treturn;\n \n-\tswap_cache_clear_shadow(entry, 1);\n \tzswap_invalidate(entry);\n \tmem_cgroup_uncharge_swap(entry, 1);\n \n@@ -460,6 +483,10 @@ void vswap_free(swp_entry_t entry, struct swap_cluster_info *ci)\n \t\treturn;\n \t}\n \n+\t/* Clear shadow if present */\n+\tif (xa_is_value(desc->shadow))\n+\t\tdesc->shadow = NULL;\n+\n \tif (desc->slot.val)\n \t\tvswap_rmap_set(ci, desc->slot, 0, 1);\n \n@@ -480,7 +507,7 @@ int folio_alloc_swap(struct folio *folio)\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swap_info_struct *si;\n \tstruct swap_cluster_info *ci;\n-\tint i, err, nr = folio_nr_pages(folio), order = folio_order(folio);\n+\tint i, nr = folio_nr_pages(folio), order = folio_order(folio);\n \tstruct swp_desc *desc;\n \tswp_entry_t entry;\n \tswp_slot_t slot;\n@@ -533,9 +560,7 @@ int folio_alloc_swap(struct folio *folio)\n \tif (mem_cgroup_try_charge_swap(folio, entry))\n \t\tgoto out_free;\n \n-\terr = swap_cache_add_folio(folio, entry, __GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN, NULL);\n-\tif (err)\n-\t\tgoto out_free;\n+\tswap_cache_add_folio(folio, entry, NULL);\n \n \treturn 0;\n \n@@ -668,6 +693,321 @@ static int vswap_cpu_dead(unsigned int cpu)\n \treturn 0;\n }\n \n+/**\n+ * swap_cache_lock - lock the swap cache for a swap entry\n+ * @entry: the swap entry\n+ *\n+ * Locks the vswap cluster spinlock for the given swap entry.\n+ */\n+void swap_cache_lock(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster;\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\tspin_lock(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+/**\n+ * swap_cache_unlock - unlock the swap cache for a swap entry\n+ * @entry: the swap entry\n+ *\n+ * Unlocks the vswap cluster spinlock for the given swap entry.\n+ */\n+void swap_cache_unlock(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster;\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+/**\n+ * swap_cache_lock_irq - lock the swap cache with interrupts disabled\n+ * @entry: the swap entry\n+ *\n+ * Locks the vswap cluster spinlock and disables interrupts for the given swap entry.\n+ */\n+void swap_cache_lock_irq(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster;\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\tspin_lock_irq(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+/**\n+ * swap_cache_unlock_irq - unlock the swap cache with interrupts enabled\n+ * @entry: the swap entry\n+ *\n+ * Unlocks the vswap cluster spinlock and enables interrupts for the given swap entry.\n+ */\n+void swap_cache_unlock_irq(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster;\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\tspin_unlock_irq(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+/**\n+ * swap_cache_get_folio - Looks up a folio in the swap cache.\n+ * @entry: swap entry used for the lookup.\n+ *\n+ * A found folio will be returned unlocked and with its refcount increased.\n+ *\n+ * Context: Caller must ensure @entry is valid and protect the cluster with\n+ * reference count or locks.\n+ *\n+ * Return: Returns the found folio on success, NULL otherwise. The caller\n+ * must lock and check if the folio still matches the swap entry before\n+ * use (e.g., folio_matches_swap_entry).\n+ */\n+struct folio *swap_cache_get_folio(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tstruct folio *folio;\n+\n+\tfor (;;) {\n+\t\trcu_read_lock();\n+\t\tdesc = vswap_iter(&cluster, entry.val);\n+\t\tif (!desc) {\n+\t\t\trcu_read_unlock();\n+\t\t\treturn NULL;\n+\t\t}\n+\n+\t\t/* Check if this is a shadow value (xa_is_value equivalent) */\n+\t\tif (xa_is_value(desc->shadow)) {\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t\trcu_read_unlock();\n+\t\t\treturn NULL;\n+\t\t}\n+\n+\t\tfolio = desc->swap_cache;\n+\t\tif (!folio) {\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t\trcu_read_unlock();\n+\t\t\treturn NULL;\n+\t\t}\n+\n+\t\tif (likely(folio_try_get(folio))) {\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t\trcu_read_unlock();\n+\t\t\treturn folio;\n+\t\t}\n+\t\tspin_unlock(&cluster->lock);\n+\t\trcu_read_unlock();\n+\t}\n+\n+\treturn NULL;\n+}\n+\n+/**\n+ * swap_cache_get_shadow - Looks up a shadow in the swap cache.\n+ * @entry: swap entry used for the lookup.\n+ *\n+ * Context: Caller must ensure @entry is valid and protect the cluster with\n+ * reference count or locks.\n+ *\n+ * Return: Returns either NULL or an XA_VALUE (shadow).\n+ */\n+void *swap_cache_get_shadow(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tvoid *shadow;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn NULL;\n+\t}\n+\n+\tshadow = desc->shadow;\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\tif (xa_is_value(shadow))\n+\t\treturn shadow;\n+\treturn NULL;\n+}\n+\n+/**\n+ * swap_cache_add_folio - Add a folio into the swap cache.\n+ * @folio: The folio to be added.\n+ * @entry: The swap entry corresponding to the folio.\n+ * @shadowp: If a shadow is found, return the shadow.\n+ *\n+ * Context: Caller must ensure @entry is valid and protect the cluster with\n+ * reference count or locks.\n+ *\n+ * The caller also needs to update the corresponding swap_map slots with\n+ * SWAP_HAS_CACHE bit to avoid race or conflict.\n+ */\n+void swap_cache_add_folio(struct folio *folio, swp_entry_t entry, void **shadowp)\n+{\n+\tstruct vswap_cluster *cluster;\n+\tunsigned long nr_pages = folio_nr_pages(folio);\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\tunsigned long i;\n+\tstruct swp_desc *desc;\n+\tvoid *old;\n+\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(folio_test_swapcache(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapbacked(folio), folio);\n+\n+\tfolio_ref_add(folio, nr_pages);\n+\tfolio_set_swapcache(folio);\n+\tfolio->swap = entry;\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\tspin_lock_irq(&cluster->lock);\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\t\told = desc->shadow;\n+\n+\t\t/* Warn if slot is already occupied by a folio */\n+\t\tVM_WARN_ON_FOLIO(old && !xa_is_value(old), folio);\n+\n+\t\t/* Save shadow if found and not yet saved */\n+\t\tif (shadowp && xa_is_value(old) && !*shadowp)\n+\t\t\t*shadowp = old;\n+\n+\t\tdesc->swap_cache = folio;\n+\t}\n+\n+\tspin_unlock_irq(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\tnode_stat_mod_folio(folio, NR_FILE_PAGES, nr_pages);\n+\tlruvec_stat_mod_folio(folio, NR_SWAPCACHE, nr_pages);\n+}\n+\n+/**\n+ * __swap_cache_del_folio - Removes a folio from the swap cache.\n+ * @folio: The folio.\n+ * @entry: The first swap entry that the folio corresponds to.\n+ * @shadow: shadow value to be filled in the swap cache.\n+ *\n+ * Removes a folio from the swap cache and fills a shadow in place.\n+ * This won't put the folio's refcount. The caller has to do that.\n+ *\n+ * Context: Caller must ensure the folio is locked and in the swap cache\n+ * using the index of @entry, and lock the swap cache.\n+ */\n+void __swap_cache_del_folio(struct folio *folio, swp_entry_t entry, void *shadow)\n+{\n+\tlong nr_pages = folio_nr_pages(folio);\n+\tstruct vswap_cluster *cluster;\n+\tstruct swp_desc *desc;\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\tint i;\n+\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(folio_test_writeback(folio), folio);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n+\t\tVM_WARN_ON_FOLIO(!desc || desc->swap_cache != folio, folio);\n+\t\tdesc->shadow = shadow;\n+\t}\n+\trcu_read_unlock();\n+\n+\tfolio->swap.val = 0;\n+\tfolio_clear_swapcache(folio);\n+\tnode_stat_mod_folio(folio, NR_FILE_PAGES, -nr_pages);\n+\tlruvec_stat_mod_folio(folio, NR_SWAPCACHE, -nr_pages);\n+}\n+\n+/**\n+ * swap_cache_del_folio - Removes a folio from the swap cache.\n+ * @folio: The folio.\n+ *\n+ * Same as __swap_cache_del_folio, but handles lock and refcount. The\n+ * caller must ensure the folio is either clean or has a swap count\n+ * equal to zero, or it may cause data loss.\n+ *\n+ * Context: Caller must ensure the folio is locked and in the swap cache.\n+ */\n+void swap_cache_del_folio(struct folio *folio)\n+{\n+\tswp_entry_t entry = folio->swap;\n+\n+\tswap_cache_lock_irq(entry);\n+\t__swap_cache_del_folio(folio, entry, NULL);\n+\tswap_cache_unlock_irq(entry);\n+\n+\tput_swap_folio(folio, entry);\n+\tfolio_ref_sub(folio, folio_nr_pages(folio));\n+}\n+\n+/**\n+ * __swap_cache_replace_folio - Replace a folio in the swap cache.\n+ * @old: The old folio to be replaced.\n+ * @new: The new folio.\n+ *\n+ * Replace an existing folio in the swap cache with a new folio. The\n+ * caller is responsible for setting up the new folio's flag and swap\n+ * entries. Replacement will take the new folio's swap entry value as\n+ * the starting offset to override all slots covered by the new folio.\n+ *\n+ * Context: Caller must ensure both folios are locked, and lock the\n+ * swap cache.\n+ */\n+void __swap_cache_replace_folio(struct folio *old, struct folio *new)\n+{\n+\tswp_entry_t entry = new->swap;\n+\tunsigned long nr_pages = folio_nr_pages(new);\n+\tstruct vswap_cluster *cluster;\n+\tstruct swp_desc *desc;\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\tvoid *old_entry;\n+\tint i;\n+\n+\tVM_WARN_ON_ONCE(!folio_test_swapcache(old) || !folio_test_swapcache(new));\n+\tVM_WARN_ON_ONCE(!folio_test_locked(old) || !folio_test_locked(new));\n+\tVM_WARN_ON_ONCE(!entry.val);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\t\told_entry = desc->swap_cache;\n+\t\tVM_WARN_ON(!old_entry || xa_is_value(old_entry) || old_entry != old);\n+\t\tdesc->swap_cache = new;\n+\t}\n+\trcu_read_unlock();\n+}\n \n int vswap_init(void)\n {\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the poor batching behavior of vswap free path by removing the zswap tree and managing zswap entries directly through the virtual swap descriptor, which eliminates zswap tree lock contention.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "eliminates zswap tree lock contention"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Remove the zswap tree and manage zswap entries directly\nthrough the virtual swap descriptor. This re-partitions the zswap pool\n(by virtual swap cluster), which eliminates zswap tree lock contention.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/zswap.h |   6 +++\n mm/vswap.c            | 100 ++++++++++++++++++++++++++++++++++++++++++\n mm/zswap.c            |  40 -----------------\n 3 files changed, 106 insertions(+), 40 deletions(-)\n\ndiff --git a/include/linux/zswap.h b/include/linux/zswap.h\nindex 1a04caf283dc8..7eb3ce7e124fc 100644\n--- a/include/linux/zswap.h\n+++ b/include/linux/zswap.h\n@@ -6,6 +6,7 @@\n #include <linux/mm_types.h>\n \n struct lruvec;\n+struct zswap_entry;\n \n extern atomic_long_t zswap_stored_pages;\n \n@@ -33,6 +34,11 @@ void zswap_lruvec_state_init(struct lruvec *lruvec);\n void zswap_folio_swapin(struct folio *folio);\n bool zswap_is_enabled(void);\n bool zswap_never_enabled(void);\n+void *zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry);\n+void *zswap_entry_load(swp_entry_t swpentry);\n+void *zswap_entry_erase(swp_entry_t swpentry);\n+bool zswap_empty(swp_entry_t swpentry);\n+\n #else\n \n struct zswap_lruvec_state {};\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex d44199dc059a3..9bb733f00fd21 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -10,6 +10,7 @@\n #include <linux/swapops.h>\n #include <linux/swap_cgroup.h>\n #include <linux/cpuhotplug.h>\n+#include <linux/zswap.h>\n #include \"swap.h\"\n #include \"swap_table.h\"\n \n@@ -37,11 +38,13 @@\n  * Swap descriptor - metadata of a swapped out page.\n  *\n  * @slot: The handle to the physical swap slot backing this page.\n+ * @zswap_entry: The zswap entry associated with this swap slot.\n  * @swap_cache: The folio in swap cache.\n  * @shadow: The shadow entry.\n  */\n struct swp_desc {\n \tswp_slot_t slot;\n+\tstruct zswap_entry *zswap_entry;\n \tunion {\n \t\tstruct folio *swap_cache;\n \t\tvoid *shadow;\n@@ -238,6 +241,7 @@ static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster, int start)\n \tfor (i = 0; i < nr; i++) {\n \t\tdesc = &cluster->descriptors[start + i];\n \t\tdesc->slot.val = 0;\n+\t\tdesc->zswap_entry = NULL;\n \t}\n \tcluster->count += nr;\n }\n@@ -1009,6 +1013,102 @@ void __swap_cache_replace_folio(struct folio *old, struct folio *new)\n \trcu_read_unlock();\n }\n \n+#ifdef CONFIG_ZSWAP\n+/**\n+ * zswap_entry_store - store a zswap entry for a swap entry\n+ * @swpentry: the swap entry\n+ * @entry: the zswap entry to store\n+ *\n+ * Stores a zswap entry in the swap descriptor for the given swap entry.\n+ * The cluster is locked during the store operation.\n+ *\n+ * Return: the old zswap entry if one existed, NULL otherwise\n+ */\n+void *zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tvoid *old;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, swpentry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn NULL;\n+\t}\n+\n+\told = desc->zswap_entry;\n+\tdesc->zswap_entry = entry;\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn old;\n+}\n+\n+/**\n+ * zswap_entry_load - load a zswap entry for a swap entry\n+ * @swpentry: the swap entry\n+ *\n+ * Loads the zswap entry from the swap descriptor for the given swap entry.\n+ *\n+ * Return: the zswap entry if one exists, NULL otherwise\n+ */\n+void *zswap_entry_load(swp_entry_t swpentry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tvoid *zswap_entry;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, swpentry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn NULL;\n+\t}\n+\n+\tzswap_entry = desc->zswap_entry;\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn zswap_entry;\n+}\n+\n+/**\n+ * zswap_entry_erase - erase a zswap entry for a swap entry\n+ * @swpentry: the swap entry\n+ *\n+ * Erases the zswap entry from the swap descriptor for the given swap entry.\n+ * The cluster is locked during the erase operation.\n+ *\n+ * Return: the zswap entry that was erased, NULL if none existed\n+ */\n+void *zswap_entry_erase(swp_entry_t swpentry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tvoid *old;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, swpentry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn NULL;\n+\t}\n+\n+\told = desc->zswap_entry;\n+\tdesc->zswap_entry = NULL;\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn old;\n+}\n+\n+bool zswap_empty(swp_entry_t swpentry)\n+{\n+\treturn xa_empty(&vswap_cluster_map);\n+}\n+#endif /* CONFIG_ZSWAP */\n+\n int vswap_init(void)\n {\n \tint i;\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex f7313261673ff..72441131f094e 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -223,37 +223,6 @@ static bool zswap_has_pool;\n * helpers and fwd declarations\n **********************************/\n \n-static DEFINE_XARRAY(zswap_tree);\n-\n-#define zswap_tree_index(entry)\t(entry.val)\n-\n-static inline void *zswap_entry_store(swp_entry_t swpentry,\n-\t\tstruct zswap_entry *entry)\n-{\n-\tpgoff_t offset = zswap_tree_index(swpentry);\n-\n-\treturn xa_store(&zswap_tree, offset, entry, GFP_KERNEL);\n-}\n-\n-static inline void *zswap_entry_load(swp_entry_t swpentry)\n-{\n-\tpgoff_t offset = zswap_tree_index(swpentry);\n-\n-\treturn xa_load(&zswap_tree, offset);\n-}\n-\n-static inline void *zswap_entry_erase(swp_entry_t swpentry)\n-{\n-\tpgoff_t offset = zswap_tree_index(swpentry);\n-\n-\treturn xa_erase(&zswap_tree, offset);\n-}\n-\n-static inline bool zswap_empty(swp_entry_t swpentry)\n-{\n-\treturn xa_empty(&zswap_tree);\n-}\n-\n #define zswap_pool_debug(msg, p)\t\t\t\\\n \tpr_debug(\"%s pool %s\\n\", msg, (p)->tfm_name)\n \n@@ -1445,13 +1414,6 @@ static bool zswap_store_page(struct page *page,\n \t\tgoto compress_failed;\n \n \told = zswap_entry_store(page_swpentry, entry);\n-\tif (xa_is_err(old)) {\n-\t\tint err = xa_err(old);\n-\n-\t\tWARN_ONCE(err != -ENOMEM, \"unexpected xarray error: %d\\n\", err);\n-\t\tzswap_reject_alloc_fail++;\n-\t\tgoto store_failed;\n-\t}\n \n \t/*\n \t * We may have had an existing entry that became stale when\n@@ -1498,8 +1460,6 @@ static bool zswap_store_page(struct page *page,\n \n \treturn true;\n \n-store_failed:\n-\tzs_free(pool->zs_pool, entry->handle);\n compress_failed:\n \tzswap_entry_cache_free(entry);\n \treturn false;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the memory overhead of swap cgroup information, explaining that it is now dynamically incurred when the virtual swap cluster is allocated and reducing the memory overhead in a huge but sparsely used swap space.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Once we decouple a swap entry from its backing store via the virtual\nswap, we can no longer statically allocate an array to store the swap\nentries' cgroup information. Move it to the swap descriptor.\n\nNote that the memory overhead for swap cgroup information is now on\ndemand, i.e dynamically incurred when the virtual swap cluster is\nallocated. This help reduces the memory overhead in a huge but\nsparsely used swap space.\n\nFor instance, a 2 TB swapfile consists of 2147483648 swap slots, each\nincurring 2 bytes of overhead for swap cgroup, for a total of 1 GB. If\nwe only utilize 10% of the swapfile, we will save 900 MB.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap_cgroup.h |  13 ---\n mm/Makefile                 |   3 -\n mm/swap_cgroup.c            | 174 ------------------------------------\n mm/swapfile.c               |   7 --\n mm/vswap.c                  |  95 ++++++++++++++++++++\n 5 files changed, 95 insertions(+), 197 deletions(-)\n delete mode 100644 mm/swap_cgroup.c\n\ndiff --git a/include/linux/swap_cgroup.h b/include/linux/swap_cgroup.h\nindex 91cdf12190a03..a2abb4d6fa085 100644\n--- a/include/linux/swap_cgroup.h\n+++ b/include/linux/swap_cgroup.h\n@@ -9,8 +9,6 @@\n extern void swap_cgroup_record(struct folio *folio, unsigned short id, swp_entry_t ent);\n extern unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents);\n extern unsigned short lookup_swap_cgroup_id(swp_entry_t ent);\n-extern int swap_cgroup_swapon(int type, unsigned long max_pages);\n-extern void swap_cgroup_swapoff(int type);\n \n #else\n \n@@ -31,17 +29,6 @@ unsigned short lookup_swap_cgroup_id(swp_entry_t ent)\n \treturn 0;\n }\n \n-static inline int\n-swap_cgroup_swapon(int type, unsigned long max_pages)\n-{\n-\treturn 0;\n-}\n-\n-static inline void swap_cgroup_swapoff(int type)\n-{\n-\treturn;\n-}\n-\n #endif\n \n #endif /* __LINUX_SWAP_CGROUP_H */\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 67fa4586e7e18..a7538784191bf 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -103,9 +103,6 @@ obj-$(CONFIG_PAGE_COUNTER) += page_counter.o\n obj-$(CONFIG_LIVEUPDATE) += memfd_luo.o\n obj-$(CONFIG_MEMCG_V1) += memcontrol-v1.o\n obj-$(CONFIG_MEMCG) += memcontrol.o vmpressure.o\n-ifdef CONFIG_SWAP\n-obj-$(CONFIG_MEMCG) += swap_cgroup.o\n-endif\n obj-$(CONFIG_CGROUP_HUGETLB) += hugetlb_cgroup.o\n obj-$(CONFIG_GUP_TEST) += gup_test.o\n obj-$(CONFIG_DMAPOOL_TEST) += dmapool_test.o\ndiff --git a/mm/swap_cgroup.c b/mm/swap_cgroup.c\ndeleted file mode 100644\nindex 77ce1d66c318d..0000000000000\n--- a/mm/swap_cgroup.c\n+++ /dev/null\n@@ -1,174 +0,0 @@\n-// SPDX-License-Identifier: GPL-2.0\n-#include <linux/swap_cgroup.h>\n-#include <linux/vmalloc.h>\n-#include <linux/mm.h>\n-\n-#include <linux/swapops.h> /* depends on mm.h include */\n-\n-static DEFINE_MUTEX(swap_cgroup_mutex);\n-\n-/* Pack two cgroup id (short) of two entries in one swap_cgroup (atomic_t) */\n-#define ID_PER_SC (sizeof(struct swap_cgroup) / sizeof(unsigned short))\n-#define ID_SHIFT (BITS_PER_TYPE(unsigned short))\n-#define ID_MASK (BIT(ID_SHIFT) - 1)\n-struct swap_cgroup {\n-\tatomic_t ids;\n-};\n-\n-struct swap_cgroup_ctrl {\n-\tstruct swap_cgroup *map;\n-};\n-\n-static struct swap_cgroup_ctrl swap_cgroup_ctrl[MAX_SWAPFILES];\n-\n-static unsigned short __swap_cgroup_id_lookup(struct swap_cgroup *map,\n-\t\t\t\t\t      pgoff_t offset)\n-{\n-\tunsigned int shift = (offset % ID_PER_SC) * ID_SHIFT;\n-\tunsigned int old_ids = atomic_read(&map[offset / ID_PER_SC].ids);\n-\n-\tBUILD_BUG_ON(!is_power_of_2(ID_PER_SC));\n-\tBUILD_BUG_ON(sizeof(struct swap_cgroup) != sizeof(atomic_t));\n-\n-\treturn (old_ids >> shift) & ID_MASK;\n-}\n-\n-static unsigned short __swap_cgroup_id_xchg(struct swap_cgroup *map,\n-\t\t\t\t\t    pgoff_t offset,\n-\t\t\t\t\t    unsigned short new_id)\n-{\n-\tunsigned short old_id;\n-\tstruct swap_cgroup *sc = &map[offset / ID_PER_SC];\n-\tunsigned int shift = (offset % ID_PER_SC) * ID_SHIFT;\n-\tunsigned int new_ids, old_ids = atomic_read(&sc->ids);\n-\n-\tdo {\n-\t\told_id = (old_ids >> shift) & ID_MASK;\n-\t\tnew_ids = (old_ids & ~(ID_MASK << shift));\n-\t\tnew_ids |= ((unsigned int)new_id) << shift;\n-\t} while (!atomic_try_cmpxchg(&sc->ids, &old_ids, new_ids));\n-\n-\treturn old_id;\n-}\n-\n-/**\n- * swap_cgroup_record - record mem_cgroup for a set of swap entries.\n- * These entries must belong to one single folio, and that folio\n- * must be being charged for swap space (swap out), and these\n- * entries must not have been charged\n- *\n- * @folio: the folio that the swap entry belongs to\n- * @id: mem_cgroup ID to be recorded\n- * @ent: the first swap entry to be recorded\n- */\n-void swap_cgroup_record(struct folio *folio, unsigned short id,\n-\t\t\tswp_entry_t ent)\n-{\n-\tunsigned int nr_ents = folio_nr_pages(folio);\n-\tswp_slot_t slot = swp_entry_to_swp_slot(ent);\n-\tstruct swap_cgroup *map;\n-\tpgoff_t offset, end;\n-\tunsigned short old;\n-\n-\toffset = swp_slot_offset(slot);\n-\tend = offset + nr_ents;\n-\tmap = swap_cgroup_ctrl[swp_slot_type(slot)].map;\n-\n-\tdo {\n-\t\told = __swap_cgroup_id_xchg(map, offset, id);\n-\t\tVM_BUG_ON(old);\n-\t} while (++offset != end);\n-}\n-\n-/**\n- * swap_cgroup_clear - clear mem_cgroup for a set of swap entries.\n- * These entries must be being uncharged from swap. They either\n- * belongs to one single folio in the swap cache (swap in for\n- * cgroup v1), or no longer have any users (slot freeing).\n- *\n- * @ent: the first swap entry to be recorded into\n- * @nr_ents: number of swap entries to be recorded\n- *\n- * Returns the existing old value.\n- */\n-unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(ent);\n-\tpgoff_t offset = swp_slot_offset(slot);\n-\tpgoff_t end = offset + nr_ents;\n-\tstruct swap_cgroup *map;\n-\tunsigned short old, iter = 0;\n-\n-\tmap = swap_cgroup_ctrl[swp_slot_type(slot)].map;\n-\n-\tdo {\n-\t\told = __swap_cgroup_id_xchg(map, offset, 0);\n-\t\tif (!iter)\n-\t\t\titer = old;\n-\t\tVM_BUG_ON(iter != old);\n-\t} while (++offset != end);\n-\n-\treturn old;\n-}\n-\n-/**\n- * lookup_swap_cgroup_id - lookup mem_cgroup id tied to swap entry\n- * @ent: swap entry to be looked up.\n- *\n- * Returns ID of mem_cgroup at success. 0 at failure. (0 is invalid ID)\n- */\n-unsigned short lookup_swap_cgroup_id(swp_entry_t ent)\n-{\n-\tstruct swap_cgroup_ctrl *ctrl;\n-\tswp_slot_t slot = swp_entry_to_swp_slot(ent);\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn 0;\n-\n-\tctrl = &swap_cgroup_ctrl[swp_slot_type(slot)];\n-\treturn __swap_cgroup_id_lookup(ctrl->map, swp_slot_offset(slot));\n-}\n-\n-int swap_cgroup_swapon(int type, unsigned long max_pages)\n-{\n-\tstruct swap_cgroup *map;\n-\tstruct swap_cgroup_ctrl *ctrl;\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn 0;\n-\n-\tBUILD_BUG_ON(sizeof(unsigned short) * ID_PER_SC !=\n-\t\t     sizeof(struct swap_cgroup));\n-\tmap = vzalloc(DIV_ROUND_UP(max_pages, ID_PER_SC) *\n-\t\t      sizeof(struct swap_cgroup));\n-\tif (!map)\n-\t\tgoto nomem;\n-\n-\tctrl = &swap_cgroup_ctrl[type];\n-\tmutex_lock(&swap_cgroup_mutex);\n-\tctrl->map = map;\n-\tmutex_unlock(&swap_cgroup_mutex);\n-\n-\treturn 0;\n-nomem:\n-\tpr_info(\"couldn't allocate enough memory for swap_cgroup\\n\");\n-\tpr_info(\"swap_cgroup can be disabled by swapaccount=0 boot option\\n\");\n-\treturn -ENOMEM;\n-}\n-\n-void swap_cgroup_swapoff(int type)\n-{\n-\tstruct swap_cgroup *map;\n-\tstruct swap_cgroup_ctrl *ctrl;\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn;\n-\n-\tmutex_lock(&swap_cgroup_mutex);\n-\tctrl = &swap_cgroup_ctrl[type];\n-\tmap = ctrl->map;\n-\tctrl->map = NULL;\n-\tmutex_unlock(&swap_cgroup_mutex);\n-\n-\tvfree(map);\n-}\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 68ec5d9f05848..345877786e432 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -2931,8 +2931,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tvfree(swap_map);\n \tkvfree(zeromap);\n \tfree_cluster_info(cluster_info, maxpages);\n-\t/* Destroy swap account information */\n-\tswap_cgroup_swapoff(p->type);\n \n \tinode = mapping->host;\n \n@@ -3497,10 +3495,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \t\tgoto bad_swap_unlock_inode;\n \t}\n \n-\terror = swap_cgroup_swapon(si->type, maxpages);\n-\tif (error)\n-\t\tgoto bad_swap_unlock_inode;\n-\n \terror = setup_swap_map(si, swap_header, swap_map, maxpages);\n \tif (error)\n \t\tgoto bad_swap_unlock_inode;\n@@ -3605,7 +3599,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tsi->global_cluster = NULL;\n \tinode = NULL;\n \tdestroy_swap_extents(si);\n-\tswap_cgroup_swapoff(si->type);\n \tspin_lock(&swap_lock);\n \tsi->swap_file = NULL;\n \tsi->flags = 0;\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex 9bb733f00fd21..64747493ca9f7 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -41,6 +41,7 @@\n  * @zswap_entry: The zswap entry associated with this swap slot.\n  * @swap_cache: The folio in swap cache.\n  * @shadow: The shadow entry.\n+ * @memcgid: The memcg id of the owning memcg, if any.\n  */\n struct swp_desc {\n \tswp_slot_t slot;\n@@ -49,6 +50,9 @@ struct swp_desc {\n \t\tstruct folio *swap_cache;\n \t\tvoid *shadow;\n \t};\n+#ifdef CONFIG_MEMCG\n+\tunsigned short memcgid;\n+#endif\n };\n \n #define VSWAP_CLUSTER_SHIFT HPAGE_PMD_ORDER\n@@ -242,6 +246,9 @@ static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster, int start)\n \t\tdesc = &cluster->descriptors[start + i];\n \t\tdesc->slot.val = 0;\n \t\tdesc->zswap_entry = NULL;\n+#ifdef CONFIG_MEMCG\n+\t\tdesc->memcgid = 0;\n+#endif\n \t}\n \tcluster->count += nr;\n }\n@@ -1109,6 +1116,94 @@ bool zswap_empty(swp_entry_t swpentry)\n }\n #endif /* CONFIG_ZSWAP */\n \n+#ifdef CONFIG_MEMCG\n+static unsigned short vswap_cgroup_record(swp_entry_t entry,\n+\t\t\t\tunsigned short memcgid, unsigned int nr_ents)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tunsigned short oldid, iter = 0;\n+\tint i;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr_ents; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\t\toldid = desc->memcgid;\n+\t\tdesc->memcgid = memcgid;\n+\t\tif (!iter)\n+\t\t\titer = oldid;\n+\t\tVM_WARN_ON(iter != oldid);\n+\t}\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn oldid;\n+}\n+\n+/**\n+ * swap_cgroup_record - record mem_cgroup for a set of swap entries.\n+ * These entries must belong to one single folio, and that folio\n+ * must be being charged for swap space (swap out), and these\n+ * entries must not have been charged\n+ *\n+ * @folio: the folio that the swap entry belongs to\n+ * @memcgid: mem_cgroup ID to be recorded\n+ * @entry: the first swap entry to be recorded\n+ */\n+void swap_cgroup_record(struct folio *folio, unsigned short memcgid,\n+\t\t\tswp_entry_t entry)\n+{\n+\tunsigned short oldid =\n+\t\tvswap_cgroup_record(entry, memcgid, folio_nr_pages(folio));\n+\n+\tVM_WARN_ON(oldid);\n+}\n+\n+/**\n+ * swap_cgroup_clear - clear mem_cgroup for a set of swap entries.\n+ * These entries must be being uncharged from swap. They either\n+ * belongs to one single folio in the swap cache (swap in for\n+ * cgroup v1), or no longer have any users (slot freeing).\n+ *\n+ * @entry: the first swap entry to be recorded into\n+ * @nr_ents: number of swap entries to be recorded\n+ *\n+ * Returns the existing old value.\n+ */\n+unsigned short swap_cgroup_clear(swp_entry_t entry, unsigned int nr_ents)\n+{\n+\treturn vswap_cgroup_record(entry, 0, nr_ents);\n+}\n+\n+/**\n+ * lookup_swap_cgroup_id - lookup mem_cgroup id tied to swap entry\n+ * @entry: swap entry to be looked up.\n+ *\n+ * Returns ID of mem_cgroup at success. 0 at failure. (0 is invalid ID)\n+ */\n+unsigned short lookup_swap_cgroup_id(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tunsigned short ret;\n+\n+\t/*\n+\t * Note that the virtual swap slot can be freed under us, for instance in\n+\t * the invocation of mem_cgroup_swapin_charge_folio. We need to wrap the\n+\t * entire lookup in RCU read-side critical section, and double check the\n+\t * existence of the swap descriptor.\n+\t */\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tret = desc ? desc->memcgid : 0;\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\treturn ret;\n+}\n+#endif /* CONFIG_MEMCG */\n+\n int vswap_init(void)\n {\n \tint i;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the vswap free path's poor batching behavior by explaining that they have re-implemented all swap entry lifecycle API in the virtual swap layer, but did not implement swap count continuation yet.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch moves the swap entry lifecycle management to the virtual swap\nlayer by adding to the swap descriptor two fields:\n\n1. in_swapcache, i.e whether the swap entry is in swap cache (or about\n   to be added).\n2. The swap count of the swap entry, which counts the number of page\n   table entries at which the swap entry is inserted.\n\nand re-implementing all of the swap entry lifecycle API\n(swap_duplicate(), swap_free_nr(), swapcache_prepare(), etc.) in the\nvirtual swap layer.\n\nFor now, we do not implement swap count continuation - the swap_count\nfield in the swap descriptor is big enough to hold the maximum number of\nswap counts. This vastly simplifies the logic.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h  |  29 +-\n include/linux/zswap.h |   5 +-\n mm/memory.c           |   8 +-\n mm/shmem.c            |   4 +-\n mm/swap.h             |  58 ++--\n mm/swap_state.c       |   4 +-\n mm/swapfile.c         | 786 ++----------------------------------------\n mm/vswap.c            | 452 ++++++++++++++++++++++--\n mm/zswap.c            |  14 +-\n 9 files changed, 502 insertions(+), 858 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 0410a00fd353c..aae2e502d9975 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -223,17 +223,9 @@ enum {\n #define SWAP_CLUSTER_MAX_SKIPPED (SWAP_CLUSTER_MAX << 10)\n #define COMPACT_CLUSTER_MAX SWAP_CLUSTER_MAX\n \n-/* Bit flag in swap_map */\n-#define SWAP_HAS_CACHE\t0x40\t/* Flag page is cached, in first swap_map */\n-#define COUNT_CONTINUED\t0x80\t/* Flag swap_map continuation for full count */\n-\n-/* Special value in first swap_map */\n-#define SWAP_MAP_MAX\t0x3e\t/* Max count */\n-#define SWAP_MAP_BAD\t0x3f\t/* Note page is bad */\n-#define SWAP_MAP_SHMEM\t0xbf\t/* Owned by shmem/tmpfs */\n-\n-/* Special value in each swap_map continuation */\n-#define SWAP_CONT_MAX\t0x7f\t/* Max count */\n+/* Swapfile's swap map state*/\n+#define SWAP_MAP_ALLOCATED\t0x01\t/* Page is allocated */\n+#define SWAP_MAP_BAD\t0x02\t/* Page is bad */\n \n /*\n  * The first page in the swap file is the swap header, which is always marked\n@@ -423,7 +415,7 @@ extern void __meminit kswapd_stop(int nid);\n \n #ifdef CONFIG_SWAP\n \n-/* Lifecycle swap API (mm/swapfile.c) */\n+/* Lifecycle swap API (mm/swapfile.c and mm/vswap.c) */\n int folio_alloc_swap(struct folio *folio);\n bool folio_free_swap(struct folio *folio);\n void put_swap_folio(struct folio *folio, swp_entry_t entry);\n@@ -433,7 +425,7 @@ int swapcache_prepare(swp_entry_t entry, int nr);\n void swap_free_nr(swp_entry_t entry, int nr_pages);\n void free_swap_and_cache_nr(swp_entry_t entry, int nr);\n int __swap_count(swp_entry_t entry);\n-bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry);\n+bool swap_entry_swapped(swp_entry_t entry);\n int swp_swapcount(swp_entry_t entry);\n bool is_swap_cached(swp_entry_t entry);\n \n@@ -473,7 +465,6 @@ static inline long get_nr_swap_pages(void)\n void si_swapinfo(struct sysinfo *);\n int swap_slot_alloc(swp_slot_t *slot, unsigned int order);\n swp_slot_t swap_slot_alloc_of_type(int);\n-int add_swap_count_continuation(swp_entry_t, gfp_t);\n int swap_type_of(dev_t device, sector_t offset);\n int find_first_swap(dev_t *device);\n unsigned int count_swap_pages(int, int);\n@@ -517,11 +508,6 @@ static inline void free_swap_cache(struct folio *folio)\n {\n }\n \n-static inline int add_swap_count_continuation(swp_entry_t swp, gfp_t gfp_mask)\n-{\n-\treturn 0;\n-}\n-\n static inline void swap_shmem_alloc(swp_entry_t swp, int nr)\n {\n }\n@@ -549,7 +535,7 @@ static inline int __swap_count(swp_entry_t entry)\n \treturn 0;\n }\n \n-static inline bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry)\n+static inline bool swap_entry_swapped(swp_entry_t entry)\n {\n \treturn false;\n }\n@@ -672,11 +658,12 @@ static inline bool mem_cgroup_swap_full(struct folio *folio)\n \n int vswap_init(void);\n void vswap_exit(void);\n-void vswap_free(swp_entry_t entry, struct swap_cluster_info *ci);\n swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry);\n swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot);\n bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si);\n void put_swap_entry(swp_entry_t entry, struct swap_info_struct *si);\n+bool folio_swapped(struct folio *folio);\n+bool vswap_only_has_cache(swp_entry_t entry, int nr);\n \n #endif /* __KERNEL__*/\n #endif /* _LINUX_SWAP_H */\ndiff --git a/include/linux/zswap.h b/include/linux/zswap.h\nindex 7eb3ce7e124fc..07b2936c38f29 100644\n--- a/include/linux/zswap.h\n+++ b/include/linux/zswap.h\n@@ -28,7 +28,6 @@ struct zswap_lruvec_state {\n unsigned long zswap_total_pages(void);\n bool zswap_store(struct folio *folio);\n int zswap_load(struct folio *folio);\n-void zswap_invalidate(swp_entry_t swp);\n void zswap_memcg_offline_cleanup(struct mem_cgroup *memcg);\n void zswap_lruvec_state_init(struct lruvec *lruvec);\n void zswap_folio_swapin(struct folio *folio);\n@@ -38,6 +37,7 @@ void *zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry);\n void *zswap_entry_load(swp_entry_t swpentry);\n void *zswap_entry_erase(swp_entry_t swpentry);\n bool zswap_empty(swp_entry_t swpentry);\n+void zswap_entry_free(struct zswap_entry *entry);\n \n #else\n \n@@ -53,7 +53,6 @@ static inline int zswap_load(struct folio *folio)\n \treturn -ENOENT;\n }\n \n-static inline void zswap_invalidate(swp_entry_t swp) {}\n static inline void zswap_memcg_offline_cleanup(struct mem_cgroup *memcg) {}\n static inline void zswap_lruvec_state_init(struct lruvec *lruvec) {}\n static inline void zswap_folio_swapin(struct folio *folio) {}\n@@ -68,6 +67,8 @@ static inline bool zswap_never_enabled(void)\n \treturn true;\n }\n \n+static inline void zswap_entry_free(struct zswap_entry *entry) {}\n+\n #endif\n \n #endif /* _LINUX_ZSWAP_H */\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 90031f833f52e..641e3f65edc00 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -1333,10 +1333,6 @@ copy_pte_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n \n \tif (ret == -EIO) {\n \t\tVM_WARN_ON_ONCE(!entry.val);\n-\t\tif (add_swap_count_continuation(entry, GFP_KERNEL) < 0) {\n-\t\t\tret = -ENOMEM;\n-\t\t\tgoto out;\n-\t\t}\n \t\tentry.val = 0;\n \t} else if (ret == -EBUSY || unlikely(ret == -EHWPOISON)) {\n \t\tgoto out;\n@@ -5044,7 +5040,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n out:\n \t/* Clear the swap cache pin for direct swapin after PTL unlock */\n \tif (need_clear_cache) {\n-\t\tswapcache_clear(si, entry, nr_pages);\n+\t\tswapcache_clear(entry, nr_pages);\n \t\tif (waitqueue_active(&swapcache_wq))\n \t\t\twake_up(&swapcache_wq);\n \t}\n@@ -5063,7 +5059,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\tfolio_put(swapcache);\n \t}\n \tif (need_clear_cache) {\n-\t\tswapcache_clear(si, entry, nr_pages);\n+\t\tswapcache_clear(entry, nr_pages);\n \t\tif (waitqueue_active(&swapcache_wq))\n \t\t\twake_up(&swapcache_wq);\n \t}\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 66cf8af6779ca..780571c830e5b 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -2442,7 +2442,7 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \n \tif (skip_swapcache) {\n \t\tfolio->swap.val = 0;\n-\t\tswapcache_clear(si, swap, nr_pages);\n+\t\tswapcache_clear(swap, nr_pages);\n \t} else {\n \t\tswap_cache_del_folio(folio);\n \t}\n@@ -2463,7 +2463,7 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \t\tfolio_unlock(folio);\n failed_nolock:\n \tif (skip_swapcache)\n-\t\tswapcache_clear(si, folio->swap, folio_nr_pages(folio));\n+\t\tswapcache_clear(folio->swap, folio_nr_pages(folio));\n \tif (folio)\n \t\tfolio_put(folio);\n \tput_swap_entry(swap, si);\ndiff --git a/mm/swap.h b/mm/swap.h\nindex 57ed24a2d6356..ae97cf9712c5c 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -211,6 +211,8 @@ void swap_cache_lock_irq(swp_entry_t entry);\n void swap_cache_unlock_irq(swp_entry_t entry);\n void swap_cache_lock(swp_entry_t entry);\n void swap_cache_unlock(swp_entry_t entry);\n+void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n+\t\t\t   unsigned long vswap, int nr);\n \n static inline struct address_space *swap_address_space(swp_entry_t entry)\n {\n@@ -245,6 +247,31 @@ static inline bool folio_matches_swap_entry(const struct folio *folio,\n \treturn folio_entry.val == round_down(entry.val, nr_pages);\n }\n \n+/**\n+ * folio_matches_swap_slot - Check if a folio matches both the virtual\n+ *                           swap entry and its backing physical swap slot.\n+ * @folio: The folio.\n+ * @entry: The virtual swap entry to check against.\n+ * @slot: The physical swap slot to check against.\n+ *\n+ * Context: The caller should have the folio locked to ensure it's stable\n+ * and nothing will move it in or out of the swap cache.\n+ * Return: true if both checks pass, false otherwise.\n+ */\n+static inline bool folio_matches_swap_slot(const struct folio *folio,\n+\t\t\t\t\t   swp_entry_t entry,\n+\t\t\t\t\t   swp_slot_t slot)\n+{\n+\tif (!folio_matches_swap_entry(folio, entry))\n+\t\treturn false;\n+\n+\t/*\n+\t * Confirm the virtual swap entry is still backed by the same\n+\t * physical swap slot.\n+\t */\n+\treturn slot.val == swp_entry_to_swp_slot(entry).val;\n+}\n+\n /*\n  * All swap cache helpers below require the caller to ensure the swap entries\n  * used are valid and stablize the device by any of the following ways:\n@@ -265,7 +292,7 @@ void __swap_cache_del_folio(struct folio *folio, swp_entry_t entry, void *shadow\n void __swap_cache_replace_folio(struct folio *old, struct folio *new);\n \n void show_swap_cache_info(void);\n-void swapcache_clear(struct swap_info_struct *si, swp_entry_t entry, int nr);\n+void swapcache_clear(swp_entry_t entry, int nr);\n struct folio *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\tstruct vm_area_struct *vma, unsigned long addr,\n \t\tstruct swap_iocb **plug);\n@@ -312,25 +339,7 @@ static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n \t\treturn find_next_bit(sis->zeromap, end, start) - start;\n }\n \n-static inline int non_swapcache_batch(swp_entry_t entry, int max_nr)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si = __swap_slot_to_info(slot);\n-\tpgoff_t offset = swp_slot_offset(slot);\n-\tint i;\n-\n-\t/*\n-\t * While allocating a large folio and doing mTHP swapin, we need to\n-\t * ensure all entries are not cached, otherwise, the mTHP folio will\n-\t * be in conflict with the folio in swap cache.\n-\t */\n-\tfor (i = 0; i < max_nr; i++) {\n-\t\tif ((si->swap_map[offset + i] & SWAP_HAS_CACHE))\n-\t\t\treturn i;\n-\t}\n-\n-\treturn i;\n-}\n+int non_swapcache_batch(swp_entry_t entry, int max_nr);\n \n #else /* CONFIG_SWAP */\n struct swap_iocb;\n@@ -382,6 +391,13 @@ static inline bool folio_matches_swap_entry(const struct folio *folio, swp_entry\n \treturn false;\n }\n \n+static inline bool folio_matches_swap_slot(const struct folio *folio,\n+\t\t\t\t\t   swp_entry_t entry,\n+\t\t\t\t\t   swp_slot_t slot)\n+{\n+\treturn false;\n+}\n+\n static inline void show_swap_cache_info(void)\n {\n }\n@@ -409,7 +425,7 @@ static inline int swap_writeout(struct folio *folio,\n \treturn 0;\n }\n \n-static inline void swapcache_clear(struct swap_info_struct *si, swp_entry_t entry, int nr)\n+static inline void swapcache_clear(swp_entry_t entry, int nr)\n {\n }\n \ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 00fa3e76a5c19..1827527e88d33 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -174,8 +174,6 @@ struct folio *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\tstruct mempolicy *mpol, pgoff_t ilx, bool *new_page_allocated,\n \t\tbool skip_if_exists)\n {\n-\tstruct swap_info_struct *si =\n-\t\t__swap_slot_to_info(swp_entry_to_swp_slot(entry));\n \tstruct folio *folio;\n \tstruct folio *new_folio = NULL;\n \tstruct folio *result = NULL;\n@@ -196,7 +194,7 @@ struct folio *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\t/*\n \t\t * Just skip read ahead for unused swap slot.\n \t\t */\n-\t\tif (!swap_entry_swapped(si, entry))\n+\t\tif (!swap_entry_swapped(entry))\n \t\t\tgoto put_and_return;\n \n \t\t/*\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 345877786e432..6c5e46bf40701 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -50,9 +50,6 @@\n #include \"internal.h\"\n #include \"swap.h\"\n \n-static bool swap_count_continued(struct swap_info_struct *, pgoff_t,\n-\t\t\t\t unsigned char);\n-static void free_swap_count_continuations(struct swap_info_struct *);\n static void swap_slots_free(struct swap_info_struct *si,\n \t\t\t      struct swap_cluster_info *ci,\n \t\t\t      swp_slot_t slot, unsigned int nr_pages);\n@@ -146,7 +143,7 @@ static struct swap_info_struct *swap_slot_to_info(swp_slot_t slot)\n \n static inline unsigned char swap_count(unsigned char ent)\n {\n-\treturn ent & ~SWAP_HAS_CACHE;\t/* may include COUNT_CONTINUED flag */\n+\treturn ent;\n }\n \n /*\n@@ -182,52 +179,14 @@ static long swap_usage_in_pages(struct swap_info_struct *si)\n static bool swap_only_has_cache(struct swap_info_struct *si,\n \t\t\t      unsigned long offset, int nr_pages)\n {\n-\tunsigned char *map = si->swap_map + offset;\n-\tunsigned char *map_end = map + nr_pages;\n-\n-\tdo {\n-\t\tVM_BUG_ON(!(*map & SWAP_HAS_CACHE));\n-\t\tif (*map != SWAP_HAS_CACHE)\n-\t\t\treturn false;\n-\t} while (++map < map_end);\n+\tswp_entry_t entry = swp_slot_to_swp_entry(swp_slot(si->type, offset));\n \n-\treturn true;\n+\treturn vswap_only_has_cache(entry, nr_pages);\n }\n \n-/**\n- * is_swap_cached - check if the swap entry is cached\n- * @entry: swap entry to check\n- *\n- * Check swap_map directly to minimize overhead, READ_ONCE is sufficient.\n- *\n- * Returns true if the swap entry is cached, false otherwise.\n- */\n-bool is_swap_cached(swp_entry_t entry)\n+static bool swap_cache_only(struct swap_info_struct *si, unsigned long offset)\n {\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si = swap_slot_to_info(slot);\n-\tunsigned long offset = swp_slot_offset(slot);\n-\n-\treturn READ_ONCE(si->swap_map[offset]) & SWAP_HAS_CACHE;\n-}\n-\n-static bool swap_is_last_map(struct swap_info_struct *si,\n-\t\tunsigned long offset, int nr_pages, bool *has_cache)\n-{\n-\tunsigned char *map = si->swap_map + offset;\n-\tunsigned char *map_end = map + nr_pages;\n-\tunsigned char count = *map;\n-\n-\tif (swap_count(count) != 1 && swap_count(count) != SWAP_MAP_SHMEM)\n-\t\treturn false;\n-\n-\twhile (++map < map_end) {\n-\t\tif (*map != count)\n-\t\t\treturn false;\n-\t}\n-\n-\t*has_cache = !!(count & SWAP_HAS_CACHE);\n-\treturn true;\n+\treturn swap_only_has_cache(si, offset, 1);\n }\n \n /*\n@@ -238,15 +197,15 @@ static bool swap_is_last_map(struct swap_info_struct *si,\n static int __try_to_reclaim_swap(struct swap_info_struct *si,\n \t\t\t\t unsigned long offset, unsigned long flags)\n {\n-\tconst swp_entry_t entry =\n-\t\tswp_slot_to_swp_entry(swp_slot(si->type, offset));\n-\tswp_slot_t slot;\n+\tconst swp_slot_t slot = swp_slot(si->type, offset);\n+\tswp_entry_t entry;\n \tstruct swap_cluster_info *ci;\n \tstruct folio *folio;\n \tint ret, nr_pages;\n \tbool need_reclaim;\n \n again:\n+\tentry = swp_slot_to_swp_entry(slot);\n \tfolio = swap_cache_get_folio(entry);\n \tif (!folio)\n \t\treturn 0;\n@@ -266,14 +225,15 @@ static int __try_to_reclaim_swap(struct swap_info_struct *si,\n \t/*\n \t * Offset could point to the middle of a large folio, or folio\n \t * may no longer point to the expected offset before it's locked.\n+\t * Additionally, the virtual swap entry may no longer be backed\n+\t * by the same physical swap slot.\n \t */\n-\tif (!folio_matches_swap_entry(folio, entry)) {\n+\tif (!folio_matches_swap_slot(folio, entry, slot)) {\n \t\tfolio_unlock(folio);\n \t\tfolio_put(folio);\n \t\tgoto again;\n \t}\n-\tslot = swp_entry_to_swp_slot(folio->swap);\n-\toffset = swp_slot_offset(slot);\n+\toffset = swp_slot_offset(swp_entry_to_swp_slot(folio->swap));\n \n \tneed_reclaim = ((flags & TTRS_ANYWAY) ||\n \t\t\t((flags & TTRS_UNMAPPED) && !folio_mapped(folio)) ||\n@@ -283,8 +243,7 @@ static int __try_to_reclaim_swap(struct swap_info_struct *si,\n \n \t/*\n \t * It's safe to delete the folio from swap cache only if the folio's\n-\t * swap_map is HAS_CACHE only, which means the slots have no page table\n-\t * reference or pending writeback, and can't be allocated to others.\n+\t * swap slots have no page table reference or pending writeback.\n \t */\n \tci = swap_cluster_lock(si, offset);\n \tneed_reclaim = swap_only_has_cache(si, offset, nr_pages);\n@@ -811,7 +770,7 @@ static bool cluster_reclaim_range(struct swap_info_struct *si,\n \t\tcase 0:\n \t\t\toffset++;\n \t\t\tbreak;\n-\t\tcase SWAP_HAS_CACHE:\n+\t\tcase SWAP_MAP_ALLOCATED:\n \t\t\tnr_reclaim = __try_to_reclaim_swap(si, offset, TTRS_ANYWAY);\n \t\t\tif (nr_reclaim > 0)\n \t\t\t\toffset += nr_reclaim;\n@@ -842,22 +801,23 @@ static bool cluster_scan_range(struct swap_info_struct *si,\n {\n \tunsigned long offset, end = start + nr_pages;\n \tunsigned char *map = si->swap_map;\n+\tunsigned char count;\n \n \tif (cluster_is_empty(ci))\n \t\treturn true;\n \n \tfor (offset = start; offset < end; offset++) {\n-\t\tswitch (READ_ONCE(map[offset])) {\n-\t\tcase 0:\n+\t\tcount = READ_ONCE(map[offset]);\n+\t\tif (!count)\n \t\t\tcontinue;\n-\t\tcase SWAP_HAS_CACHE:\n+\n+\t\tif (swap_cache_only(si, offset)) {\n \t\t\tif (!vm_swap_full())\n \t\t\t\treturn false;\n \t\t\t*need_reclaim = true;\n \t\t\tcontinue;\n-\t\tdefault:\n-\t\t\treturn false;\n \t\t}\n+\t\treturn false;\n \t}\n \n \treturn true;\n@@ -974,7 +934,6 @@ static void swap_reclaim_full_clusters(struct swap_info_struct *si, bool force)\n \tlong to_scan = 1;\n \tunsigned long offset, end;\n \tstruct swap_cluster_info *ci;\n-\tunsigned char *map = si->swap_map;\n \tint nr_reclaim;\n \n \tif (force)\n@@ -986,7 +945,7 @@ static void swap_reclaim_full_clusters(struct swap_info_struct *si, bool force)\n \t\tto_scan--;\n \n \t\twhile (offset < end) {\n-\t\t\tif (READ_ONCE(map[offset]) == SWAP_HAS_CACHE) {\n+\t\t\tif (swap_cache_only(si, offset)) {\n \t\t\t\tspin_unlock(&ci->lock);\n \t\t\t\tnr_reclaim = __try_to_reclaim_swap(si, offset,\n \t\t\t\t\t\t\t\t   TTRS_ANYWAY);\n@@ -1320,7 +1279,8 @@ static bool swap_alloc_fast(swp_slot_t *slot, int order)\n \tif (cluster_is_usable(ci, order)) {\n \t\tif (cluster_is_empty(ci))\n \t\t\toffset = cluster_offset(si, ci);\n-\t\tfound = alloc_swap_scan_cluster(si, ci, offset, order, SWAP_HAS_CACHE);\n+\t\tfound = alloc_swap_scan_cluster(si, ci, offset, order,\n+\t\t\tSWAP_MAP_ALLOCATED);\n \t\tif (found)\n \t\t\t*slot = swp_slot(si->type, found);\n \t} else {\n@@ -1344,7 +1304,7 @@ static void swap_alloc_slow(swp_slot_t *slot, int order)\n \t\tplist_requeue(&si->avail_list, &swap_avail_head);\n \t\tspin_unlock(&swap_avail_lock);\n \t\tif (get_swap_device_info(si)) {\n-\t\t\toffset = cluster_alloc_swap_slot(si, order, SWAP_HAS_CACHE);\n+\t\t\toffset = cluster_alloc_swap_slot(si, order, SWAP_MAP_ALLOCATED);\n \t\t\tswap_slot_put_swap_info(si);\n \t\t\tif (offset) {\n \t\t\t\t*slot = swp_slot(si->type, offset);\n@@ -1471,48 +1431,6 @@ static struct swap_info_struct *_swap_info_get(swp_slot_t slot)\n \treturn NULL;\n }\n \n-static unsigned char swap_slot_put_locked(struct swap_info_struct *si,\n-\t\t\t\t\t   struct swap_cluster_info *ci,\n-\t\t\t\t\t   swp_slot_t slot,\n-\t\t\t\t\t   unsigned char usage)\n-{\n-\tunsigned long offset = swp_slot_offset(slot);\n-\tunsigned char count;\n-\tunsigned char has_cache;\n-\n-\tcount = si->swap_map[offset];\n-\n-\thas_cache = count & SWAP_HAS_CACHE;\n-\tcount &= ~SWAP_HAS_CACHE;\n-\n-\tif (usage == SWAP_HAS_CACHE) {\n-\t\tVM_BUG_ON(!has_cache);\n-\t\thas_cache = 0;\n-\t} else if (count == SWAP_MAP_SHMEM) {\n-\t\t/*\n-\t\t * Or we could insist on shmem.c using a special\n-\t\t * swap_shmem_free() and free_shmem_swap_and_cache()...\n-\t\t */\n-\t\tcount = 0;\n-\t} else if ((count & ~COUNT_CONTINUED) <= SWAP_MAP_MAX) {\n-\t\tif (count == COUNT_CONTINUED) {\n-\t\t\tif (swap_count_continued(si, offset, count))\n-\t\t\t\tcount = SWAP_MAP_MAX | COUNT_CONTINUED;\n-\t\t\telse\n-\t\t\t\tcount = SWAP_MAP_MAX;\n-\t\t} else\n-\t\t\tcount--;\n-\t}\n-\n-\tusage = count | has_cache;\n-\tif (usage)\n-\t\tWRITE_ONCE(si->swap_map[offset], usage);\n-\telse\n-\t\tswap_slots_free(si, ci, slot, 1);\n-\n-\treturn usage;\n-}\n-\n /*\n  * When we get a swap entry, if there aren't some other ways to\n  * prevent swapoff, such as the folio in swap cache is locked, RCU\n@@ -1580,94 +1498,23 @@ struct swap_info_struct *swap_slot_tryget_swap_info(swp_slot_t slot)\n \treturn NULL;\n }\n \n-static void swap_slots_put_cache(struct swap_info_struct *si,\n-\t\t\t\t   swp_slot_t slot, int nr)\n-{\n-\tunsigned long offset = swp_slot_offset(slot);\n-\tstruct swap_cluster_info *ci;\n-\n-\tci = swap_cluster_lock(si, offset);\n-\tif (swap_only_has_cache(si, offset, nr)) {\n-\t\tswap_slots_free(si, ci, slot, nr);\n-\t} else {\n-\t\tfor (int i = 0; i < nr; i++, slot.val++)\n-\t\t\tswap_slot_put_locked(si, ci, slot, SWAP_HAS_CACHE);\n-\t}\n-\tswap_cluster_unlock(ci);\n-}\n-\n static bool swap_slots_put_map(struct swap_info_struct *si,\n \t\t\t\t swp_slot_t slot, int nr)\n {\n \tunsigned long offset = swp_slot_offset(slot);\n \tstruct swap_cluster_info *ci;\n-\tbool has_cache = false;\n-\tunsigned char count;\n-\tint i;\n-\n-\tif (nr <= 1)\n-\t\tgoto fallback;\n-\tcount = swap_count(data_race(si->swap_map[offset]));\n-\tif (count != 1 && count != SWAP_MAP_SHMEM)\n-\t\tgoto fallback;\n \n \tci = swap_cluster_lock(si, offset);\n-\tif (!swap_is_last_map(si, offset, nr, &has_cache)) {\n-\t\tgoto locked_fallback;\n-\t}\n-\tif (!has_cache)\n-\t\tswap_slots_free(si, ci, slot, nr);\n-\telse\n-\t\tfor (i = 0; i < nr; i++)\n-\t\t\tWRITE_ONCE(si->swap_map[offset + i], SWAP_HAS_CACHE);\n+\tvswap_rmap_set(ci, slot, 0, nr);\n+\tswap_slots_free(si, ci, slot, nr);\n \tswap_cluster_unlock(ci);\n \n-\treturn has_cache;\n-\n-fallback:\n-\tci = swap_cluster_lock(si, offset);\n-locked_fallback:\n-\tfor (i = 0; i < nr; i++, slot.val++) {\n-\t\tcount = swap_slot_put_locked(si, ci, slot, 1);\n-\t\tif (count == SWAP_HAS_CACHE)\n-\t\t\thas_cache = true;\n-\t}\n-\tswap_cluster_unlock(ci);\n-\treturn has_cache;\n-}\n-\n-/*\n- * Only functions with \"_nr\" suffix are able to free entries spanning\n- * cross multi clusters, so ensure the range is within a single cluster\n- * when freeing entries with functions without \"_nr\" suffix.\n- */\n-static bool swap_slots_put_map_nr(struct swap_info_struct *si,\n-\t\t\t\t    swp_slot_t slot, int nr)\n-{\n-\tint cluster_nr, cluster_rest;\n-\tunsigned long offset = swp_slot_offset(slot);\n-\tbool has_cache = false;\n-\n-\tcluster_rest = SWAPFILE_CLUSTER - offset % SWAPFILE_CLUSTER;\n-\twhile (nr) {\n-\t\tcluster_nr = min(nr, cluster_rest);\n-\t\thas_cache |= swap_slots_put_map(si, slot, cluster_nr);\n-\t\tcluster_rest = SWAPFILE_CLUSTER;\n-\t\tnr -= cluster_nr;\n-\t\tslot.val += cluster_nr;\n-\t}\n-\n-\treturn has_cache;\n+\treturn true;\n }\n \n-/*\n- * Check if it's the last ref of swap entry in the freeing path.\n- * Qualified value includes 1, SWAP_HAS_CACHE or SWAP_MAP_SHMEM.\n- */\n static inline bool __maybe_unused swap_is_last_ref(unsigned char count)\n {\n-\treturn (count == SWAP_HAS_CACHE) || (count == 1) ||\n-\t       (count == SWAP_MAP_SHMEM);\n+\treturn count == SWAP_MAP_ALLOCATED;\n }\n \n /*\n@@ -1681,14 +1528,6 @@ static void swap_slots_free(struct swap_info_struct *si,\n \tunsigned long offset = swp_slot_offset(slot);\n \tunsigned char *map = si->swap_map + offset;\n \tunsigned char *map_end = map + nr_pages;\n-\tswp_entry_t entry = swp_slot_to_swp_entry(slot);\n-\tint i;\n-\n-\t/* release all the associated (virtual) swap slots */\n-\tfor (i = 0; i < nr_pages; i++) {\n-\t\tvswap_free(entry, ci);\n-\t\tentry.val++;\n-\t}\n \n \t/* It should never free entries across different clusters */\n \tVM_BUG_ON(ci != __swap_offset_to_cluster(si, offset + nr_pages - 1));\n@@ -1731,149 +1570,6 @@ void swap_slot_free_nr(swp_slot_t slot, int nr_pages)\n \t}\n }\n \n-/*\n- * Caller has made sure that the swap device corresponding to entry\n- * is still around or has not been recycled.\n- */\n-void swap_free_nr(swp_entry_t entry, int nr_pages)\n-{\n-\tswap_slot_free_nr(swp_entry_to_swp_slot(entry), nr_pages);\n-}\n-\n-/*\n- * Called after dropping swapcache to decrease refcnt to swap entries.\n- */\n-void put_swap_folio(struct folio *folio, swp_entry_t entry)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si;\n-\tint size = 1 << swap_slot_order(folio_order(folio));\n-\n-\tsi = _swap_info_get(slot);\n-\tif (!si)\n-\t\treturn;\n-\n-\tswap_slots_put_cache(si, slot, size);\n-}\n-\n-int __swap_count(swp_entry_t entry)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si = __swap_slot_to_info(slot);\n-\tpgoff_t offset = swp_slot_offset(slot);\n-\n-\treturn swap_count(si->swap_map[offset]);\n-}\n-\n-/*\n- * How many references to @entry are currently swapped out?\n- * This does not give an exact answer when swap count is continued,\n- * but does include the high COUNT_CONTINUED flag to allow for that.\n- */\n-bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tpgoff_t offset = swp_slot_offset(slot);\n-\tstruct swap_cluster_info *ci;\n-\tint count;\n-\n-\tci = swap_cluster_lock(si, offset);\n-\tcount = swap_count(si->swap_map[offset]);\n-\tswap_cluster_unlock(ci);\n-\treturn !!count;\n-}\n-\n-/*\n- * How many references to @entry are currently swapped out?\n- * This considers COUNT_CONTINUED so it returns exact answer.\n- */\n-int swp_swapcount(swp_entry_t entry)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tint count, tmp_count, n;\n-\tstruct swap_info_struct *si;\n-\tstruct swap_cluster_info *ci;\n-\tstruct page *page;\n-\tpgoff_t offset;\n-\tunsigned char *map;\n-\n-\tsi = _swap_info_get(slot);\n-\tif (!si)\n-\t\treturn 0;\n-\n-\toffset = swp_slot_offset(slot);\n-\n-\tci = swap_cluster_lock(si, offset);\n-\n-\tcount = swap_count(si->swap_map[offset]);\n-\tif (!(count & COUNT_CONTINUED))\n-\t\tgoto out;\n-\n-\tcount &= ~COUNT_CONTINUED;\n-\tn = SWAP_MAP_MAX + 1;\n-\n-\tpage = vmalloc_to_page(si->swap_map + offset);\n-\toffset &= ~PAGE_MASK;\n-\tVM_BUG_ON(page_private(page) != SWP_CONTINUED);\n-\n-\tdo {\n-\t\tpage = list_next_entry(page, lru);\n-\t\tmap = kmap_local_page(page);\n-\t\ttmp_count = map[offset];\n-\t\tkunmap_local(map);\n-\n-\t\tcount += (tmp_count & ~COUNT_CONTINUED) * n;\n-\t\tn *= (SWAP_CONT_MAX + 1);\n-\t} while (tmp_count & COUNT_CONTINUED);\n-out:\n-\tswap_cluster_unlock(ci);\n-\treturn count;\n-}\n-\n-static bool swap_page_trans_huge_swapped(struct swap_info_struct *si,\n-\t\t\t\t\t swp_entry_t entry, int order)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_cluster_info *ci;\n-\tunsigned char *map = si->swap_map;\n-\tunsigned int nr_pages = 1 << order;\n-\tunsigned long roffset = swp_slot_offset(slot);\n-\tunsigned long offset = round_down(roffset, nr_pages);\n-\tint i;\n-\tbool ret = false;\n-\n-\tci = swap_cluster_lock(si, offset);\n-\tif (nr_pages == 1) {\n-\t\tif (swap_count(map[roffset]))\n-\t\t\tret = true;\n-\t\tgoto unlock_out;\n-\t}\n-\tfor (i = 0; i < nr_pages; i++) {\n-\t\tif (swap_count(map[offset + i])) {\n-\t\t\tret = true;\n-\t\t\tbreak;\n-\t\t}\n-\t}\n-unlock_out:\n-\tswap_cluster_unlock(ci);\n-\treturn ret;\n-}\n-\n-static bool folio_swapped(struct folio *folio)\n-{\n-\tswp_entry_t entry = folio->swap;\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si = _swap_info_get(slot);\n-\n-\tif (!si)\n-\t\treturn false;\n-\n-\tif (!IS_ENABLED(CONFIG_THP_SWAP) || likely(!folio_test_large(folio)))\n-\t\treturn swap_entry_swapped(si, entry);\n-\n-\treturn swap_page_trans_huge_swapped(si, entry, folio_order(folio));\n-}\n-\n static bool folio_swapcache_freeable(struct folio *folio)\n {\n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n@@ -1925,72 +1621,6 @@ bool folio_free_swap(struct folio *folio)\n \treturn true;\n }\n \n-/**\n- * free_swap_and_cache_nr() - Release reference on range of swap entries and\n- *                            reclaim their cache if no more references remain.\n- * @entry: First entry of range.\n- * @nr: Number of entries in range.\n- *\n- * For each swap entry in the contiguous range, release a reference. If any swap\n- * entries become free, try to reclaim their underlying folios, if present. The\n- * offset range is defined by [entry.offset, entry.offset + nr).\n- */\n-void free_swap_and_cache_nr(swp_entry_t entry, int nr)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tconst unsigned long start_offset = swp_slot_offset(slot);\n-\tconst unsigned long end_offset = start_offset + nr;\n-\tstruct swap_info_struct *si;\n-\tbool any_only_cache = false;\n-\tunsigned long offset;\n-\n-\tsi = swap_slot_tryget_swap_info(slot);\n-\tif (!si)\n-\t\treturn;\n-\n-\tif (WARN_ON(end_offset > si->max))\n-\t\tgoto out;\n-\n-\t/*\n-\t * First free all entries in the range.\n-\t */\n-\tany_only_cache = swap_slots_put_map_nr(si, slot, nr);\n-\n-\t/*\n-\t * Short-circuit the below loop if none of the entries had their\n-\t * reference drop to zero.\n-\t */\n-\tif (!any_only_cache)\n-\t\tgoto out;\n-\n-\t/*\n-\t * Now go back over the range trying to reclaim the swap cache.\n-\t */\n-\tfor (offset = start_offset; offset < end_offset; offset += nr) {\n-\t\tnr = 1;\n-\t\tif (READ_ONCE(si->swap_map[offset]) == SWAP_HAS_CACHE) {\n-\t\t\t/*\n-\t\t\t * Folios are always naturally aligned in swap so\n-\t\t\t * advance forward to the next boundary. Zero means no\n-\t\t\t * folio was found for the swap entry, so advance by 1\n-\t\t\t * in this case. Negative value means folio was found\n-\t\t\t * but could not be reclaimed. Here we can still advance\n-\t\t\t * to the next boundary.\n-\t\t\t */\n-\t\t\tnr = __try_to_reclaim_swap(si, offset,\n-\t\t\t\t\t\t   TTRS_UNMAPPED | TTRS_FULL);\n-\t\t\tif (nr == 0)\n-\t\t\t\tnr = 1;\n-\t\t\telse if (nr < 0)\n-\t\t\t\tnr = -nr;\n-\t\t\tnr = ALIGN(offset + 1, nr) - offset;\n-\t\t}\n-\t}\n-\n-out:\n-\tswap_slot_put_swap_info(si);\n-}\n-\n #ifdef CONFIG_HIBERNATION\n \n swp_slot_t swap_slot_alloc_of_type(int type)\n@@ -2901,8 +2531,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tflush_percpu_swap_cluster(p);\n \n \tdestroy_swap_extents(p);\n-\tif (p->flags & SWP_CONTINUED)\n-\t\tfree_swap_count_continuations(p);\n \n \tif (!(p->flags & SWP_SOLIDSTATE))\n \t\tatomic_dec(&nr_rotate_swap);\n@@ -3638,364 +3266,6 @@ void si_swapinfo(struct sysinfo *val)\n \tspin_unlock(&swap_lock);\n }\n \n-/*\n- * Verify that nr swap entries are valid and increment their swap map counts.\n- *\n- * Returns error code in following case.\n- * - success -> 0\n- * - swp_entry is invalid -> EINVAL\n- * - swap-cache reference is requested but there is already one. -> EEXIST\n- * - swap-cache reference is requested but the entry is not used. -> ENOENT\n- * - swap-mapped reference requested but needs continued swap count. -> ENOMEM\n- */\n-static int __swap_duplicate(swp_entry_t entry, unsigned char usage, int nr)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si;\n-\tstruct swap_cluster_info *ci;\n-\tunsigned long offset;\n-\tunsigned char count;\n-\tunsigned char has_cache;\n-\tint err, i;\n-\n-\tsi = swap_slot_to_info(slot);\n-\tif (WARN_ON_ONCE(!si)) {\n-\t\tpr_err(\"%s%08lx\\n\", Bad_file, entry.val);\n-\t\treturn -EINVAL;\n-\t}\n-\n-\toffset = swp_slot_offset(slot);\n-\tVM_WARN_ON(nr > SWAPFILE_CLUSTER - offset % SWAPFILE_CLUSTER);\n-\tVM_WARN_ON(usage == 1 && nr > 1);\n-\tci = swap_cluster_lock(si, offset);\n-\n-\terr = 0;\n-\tfor (i = 0; i < nr; i++) {\n-\t\tcount = si->swap_map[offset + i];\n-\n-\t\t/*\n-\t\t * swapin_readahead() doesn't check if a swap entry is valid, so the\n-\t\t * swap entry could be SWAP_MAP_BAD. Check here with lock held.\n-\t\t */\n-\t\tif (unlikely(swap_count(count) == SWAP_MAP_BAD)) {\n-\t\t\terr = -ENOENT;\n-\t\t\tgoto unlock_out;\n-\t\t}\n-\n-\t\thas_cache = count & SWAP_HAS_CACHE;\n-\t\tcount &= ~SWAP_HAS_CACHE;\n-\n-\t\tif (!count && !has_cache) {\n-\t\t\terr = -ENOENT;\n-\t\t} else if (usage == SWAP_HAS_CACHE) {\n-\t\t\tif (has_cache)\n-\t\t\t\terr = -EEXIST;\n-\t\t} else if ((count & ~COUNT_CONTINUED) > SWAP_MAP_MAX) {\n-\t\t\terr = -EINVAL;\n-\t\t}\n-\n-\t\tif (err)\n-\t\t\tgoto unlock_out;\n-\t}\n-\n-\tfor (i = 0; i < nr; i++) {\n-\t\tcount = si->swap_map[offset + i];\n-\t\thas_cache = count & SWAP_HAS_CACHE;\n-\t\tcount &= ~SWAP_HAS_CACHE;\n-\n-\t\tif (usage == SWAP_HAS_CACHE)\n-\t\t\thas_cache = SWAP_HAS_CACHE;\n-\t\telse if ((count & ~COUNT_CONTINUED) < SWAP_MAP_MAX)\n-\t\t\tcount += usage;\n-\t\telse if (swap_count_continued(si, offset + i, count))\n-\t\t\tcount = COUNT_CONTINUED;\n-\t\telse {\n-\t\t\t/*\n-\t\t\t * Don't need to rollback changes, because if\n-\t\t\t * usage == 1, there must be nr == 1.\n-\t\t\t */\n-\t\t\terr = -ENOMEM;\n-\t\t\tgoto unlock_out;\n-\t\t}\n-\n-\t\tWRITE_ONCE(si->swap_map[offset + i], count | has_cache);\n-\t}\n-\n-unlock_out:\n-\tswap_cluster_unlock(ci);\n-\treturn err;\n-}\n-\n-/*\n- * Help swapoff by noting that swap entry belongs to shmem/tmpfs\n- * (in which case its reference count is never incremented).\n- */\n-void swap_shmem_alloc(swp_entry_t entry, int nr)\n-{\n-\t__swap_duplicate(entry, SWAP_MAP_SHMEM, nr);\n-}\n-\n-/*\n- * Increase reference count of swap entry by 1.\n- * Returns 0 for success, or -ENOMEM if a swap_count_continuation is required\n- * but could not be atomically allocated.  Returns 0, just as if it succeeded,\n- * if __swap_duplicate() fails for another reason (-EINVAL or -ENOENT), which\n- * might occur if a page table entry has got corrupted.\n- */\n-int swap_duplicate(swp_entry_t entry)\n-{\n-\tint err = 0;\n-\n-\twhile (!err && __swap_duplicate(entry, 1, 1) == -ENOMEM)\n-\t\terr = add_swap_count_continuation(entry, GFP_ATOMIC);\n-\treturn err;\n-}\n-\n-/*\n- * @entry: first swap entry from which we allocate nr swap cache.\n- *\n- * Called when allocating swap cache for existing swap entries,\n- * This can return error codes. Returns 0 at success.\n- * -EEXIST means there is a swap cache.\n- * Note: return code is different from swap_duplicate().\n- */\n-int swapcache_prepare(swp_entry_t entry, int nr)\n-{\n-\treturn __swap_duplicate(entry, SWAP_HAS_CACHE, nr);\n-}\n-\n-/*\n- * Caller should ensure entries belong to the same folio so\n- * the entries won't span cross cluster boundary.\n- */\n-void swapcache_clear(struct swap_info_struct *si, swp_entry_t entry, int nr)\n-{\n-\tswap_slots_put_cache(si, swp_entry_to_swp_slot(entry), nr);\n-}\n-\n-/*\n- * add_swap_count_continuation - called when a swap count is duplicated\n- * beyond SWAP_MAP_MAX, it allocates a new page and links that to the entry's\n- * page of the original vmalloc'ed swap_map, to hold the continuation count\n- * (for that entry and for its neighbouring PAGE_SIZE swap entries).  Called\n- * again when count is duplicated beyond SWAP_MAP_MAX * SWAP_CONT_MAX, etc.\n- *\n- * These continuation pages are seldom referenced: the common paths all work\n- * on the original swap_map, only referring to a continuation page when the\n- * low \"digit\" of a count is incremented or decremented through SWAP_MAP_MAX.\n- *\n- * add_swap_count_continuation(, GFP_ATOMIC) can be called while holding\n- * page table locks; if it fails, add_swap_count_continuation(, GFP_KERNEL)\n- * can be called after dropping locks.\n- */\n-int add_swap_count_continuation(swp_entry_t entry, gfp_t gfp_mask)\n-{\n-\tstruct swap_info_struct *si;\n-\tstruct swap_cluster_info *ci;\n-\tstruct page *head;\n-\tstruct page *page;\n-\tstruct page *list_page;\n-\tpgoff_t offset;\n-\tunsigned char count;\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tint ret = 0;\n-\n-\t/*\n-\t * When debugging, it's easier to use __GFP_ZERO here; but it's better\n-\t * for latency not to zero a page while GFP_ATOMIC and holding locks.\n-\t */\n-\tpage = alloc_page(gfp_mask | __GFP_HIGHMEM);\n-\n-\tsi = swap_slot_tryget_swap_info(slot);\n-\tif (!si) {\n-\t\t/*\n-\t\t * An acceptable race has occurred since the failing\n-\t\t * __swap_duplicate(): the swap device may be swapoff\n-\t\t */\n-\t\tgoto outer;\n-\t}\n-\n-\toffset = swp_slot_offset(slot);\n-\n-\tci = swap_cluster_lock(si, offset);\n-\n-\tcount = swap_count(si->swap_map[offset]);\n-\n-\tif ((count & ~COUNT_CONTINUED) != SWAP_MAP_MAX) {\n-\t\t/*\n-\t\t * The higher the swap count, the more likely it is that tasks\n-\t\t * will race to add swap count continuation: we need to avoid\n-\t\t * over-provisioning.\n-\t\t */\n-\t\tgoto out;\n-\t}\n-\n-\tif (!page) {\n-\t\tret = -ENOMEM;\n-\t\tgoto out;\n-\t}\n-\n-\thead = vmalloc_to_page(si->swap_map + offset);\n-\toffset &= ~PAGE_MASK;\n-\n-\tspin_lock(&si->cont_lock);\n-\t/*\n-\t * Page allocation does not initialize the page's lru field,\n-\t * but it does always reset its private field.\n-\t */\n-\tif (!page_private(head)) {\n-\t\tBUG_ON(count & COUNT_CONTINUED);\n-\t\tINIT_LIST_HEAD(&head->lru);\n-\t\tset_page_private(head, SWP_CONTINUED);\n-\t\tsi->flags |= SWP_CONTINUED;\n-\t}\n-\n-\tlist_for_each_entry(list_page, &head->lru, lru) {\n-\t\tunsigned char *map;\n-\n-\t\t/*\n-\t\t * If the previous map said no continuation, but we've found\n-\t\t * a continuation page, free our allocation and use this one.\n-\t\t */\n-\t\tif (!(count & COUNT_CONTINUED))\n-\t\t\tgoto out_unlock_cont;\n-\n-\t\tmap = kmap_local_page(list_page) + offset;\n-\t\tcount = *map;\n-\t\tkunmap_local(map);\n-\n-\t\t/*\n-\t\t * If this continuation count now has some space in it,\n-\t\t * free our allocation and use this one.\n-\t\t */\n-\t\tif ((count & ~COUNT_CONTINUED) != SWAP_CONT_MAX)\n-\t\t\tgoto out_unlock_cont;\n-\t}\n-\n-\tlist_add_tail(&page->lru, &head->lru);\n-\tpage = NULL;\t\t\t/* now it's attached, don't free it */\n-out_unlock_cont:\n-\tspin_unlock(&si->cont_lock);\n-out:\n-\tswap_cluster_unlock(ci);\n-\tswap_slot_put_swap_info(si);\n-outer:\n-\tif (page)\n-\t\t__free_page(page);\n-\treturn ret;\n-}\n-\n-/*\n- * swap_count_continued - when the original swap_map count is incremented\n- * from SWAP_MAP_MAX, check if there is already a continuation page to carry\n- * into, carry if so, or else fail until a new continuation page is allocated;\n- * when the original swap_map count is decremented from 0 with continuation,\n- * borrow from the continuation and report whether it still holds more.\n- * Called while __swap_duplicate() or caller of swap_entry_put_locked()\n- * holds cluster lock.\n- */\n-static bool swap_count_continued(struct swap_info_struct *si,\n-\t\t\t\t pgoff_t offset, unsigned char count)\n-{\n-\tstruct page *head;\n-\tstruct page *page;\n-\tunsigned char *map;\n-\tbool ret;\n-\n-\thead = vmalloc_to_page(si->swap_map + offset);\n-\tif (page_private(head) != SWP_CONTINUED) {\n-\t\tBUG_ON(count & COUNT_CONTINUED);\n-\t\treturn false;\t\t/* need to add count continuation */\n-\t}\n-\n-\tspin_lock(&si->cont_lock);\n-\toffset &= ~PAGE_MASK;\n-\tpage = list_next_entry(head, lru);\n-\tmap = kmap_local_page(page) + offset;\n-\n-\tif (count == SWAP_MAP_MAX)\t/* initial increment from swap_map */\n-\t\tgoto init_map;\t\t/* jump over SWAP_CONT_MAX checks */\n-\n-\tif (count == (SWAP_MAP_MAX | COUNT_CONTINUED)) { /* incrementing */\n-\t\t/*\n-\t\t * Think of how you add 1 to 999\n-\t\t */\n-\t\twhile (*map == (SWAP_CONT_MAX | COUNT_CONTINUED)) {\n-\t\t\tkunmap_local(map);\n-\t\t\tpage = list_next_entry(page, lru);\n-\t\t\tBUG_ON(page == head);\n-\t\t\tmap = kmap_local_page(page) + offset;\n-\t\t}\n-\t\tif (*map == SWAP_CONT_MAX) {\n-\t\t\tkunmap_local(map);\n-\t\t\tpage = list_next_entry(page, lru);\n-\t\t\tif (page == head) {\n-\t\t\t\tret = false;\t/* add count continuation */\n-\t\t\t\tgoto out;\n-\t\t\t}\n-\t\t\tmap = kmap_local_page(page) + offset;\n-init_map:\t\t*map = 0;\t\t/* we didn't zero the page */\n-\t\t}\n-\t\t*map += 1;\n-\t\tkunmap_local(map);\n-\t\twhile ((page = list_prev_entry(page, lru)) != head) {\n-\t\t\tmap = kmap_local_page(page) + offset;\n-\t\t\t*map = COUNT_CONTINUED;\n-\t\t\tkunmap_local(map);\n-\t\t}\n-\t\tret = true;\t\t\t/* incremented */\n-\n-\t} else {\t\t\t\t/* decrementing */\n-\t\t/*\n-\t\t * Think of how you subtract 1 from 1000\n-\t\t */\n-\t\tBUG_ON(count != COUNT_CONTINUED);\n-\t\twhile (*map == COUNT_CONTINUED) {\n-\t\t\tkunmap_local(map);\n-\t\t\tpage = list_next_entry(page, lru);\n-\t\t\tBUG_ON(page == head);\n-\t\t\tmap = kmap_local_page(page) + offset;\n-\t\t}\n-\t\tBUG_ON(*map == 0);\n-\t\t*map -= 1;\n-\t\tif (*map == 0)\n-\t\t\tcount = 0;\n-\t\tkunmap_local(map);\n-\t\twhile ((page = list_prev_entry(page, lru)) != head) {\n-\t\t\tmap = kmap_local_page(page) + offset;\n-\t\t\t*map = SWAP_CONT_MAX | count;\n-\t\t\tcount = COUNT_CONTINUED;\n-\t\t\tkunmap_local(map);\n-\t\t}\n-\t\tret = count == COUNT_CONTINUED;\n-\t}\n-out:\n-\tspin_unlock(&si->cont_lock);\n-\treturn ret;\n-}\n-\n-/*\n- * free_swap_count_continuations - swapoff free all the continuation pages\n- * appended to the swap_map, after swap_map is quiesced, before vfree'ing it.\n- */\n-static void free_swap_count_continuations(struct swap_info_struct *si)\n-{\n-\tpgoff_t offset;\n-\n-\tfor (offset = 0; offset < si->max; offset += PAGE_SIZE) {\n-\t\tstruct page *head;\n-\t\thead = vmalloc_to_page(si->swap_map + offset);\n-\t\tif (page_private(head)) {\n-\t\t\tstruct page *page, *next;\n-\n-\t\t\tlist_for_each_entry_safe(page, next, &head->lru, lru) {\n-\t\t\t\tlist_del(&page->lru);\n-\t\t\t\t__free_page(page);\n-\t\t\t}\n-\t\t}\n-\t}\n-}\n-\n #if defined(CONFIG_MEMCG) && defined(CONFIG_BLK_CGROUP)\n static bool __has_usable_swap(void)\n {\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex 64747493ca9f7..318933071edc6 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -24,6 +24,8 @@\n  * For now, there is a one-to-one correspondence between a virtual swap slot\n  * and its associated physical swap slot.\n  *\n+ * I. Allocation\n+ *\n  * Virtual swap slots are organized into PMD-sized clusters, analogous to\n  * physical swap allocator. However, unlike the physical swap allocator,\n  * the clusters are dynamically allocated and freed on-demand. There is no\n@@ -32,6 +34,26 @@\n  *\n  * This allows us to avoid the overhead of pre-allocating a large number of\n  * virtual swap clusters.\n+ *\n+ * II. Swap Entry Lifecycle\n+ *\n+ * The swap entry's lifecycle is managed at the virtual swap layer. Conceptually,\n+ * each virtual swap slot has a reference count, which includes:\n+ *\n+ * 1. The number of page table entries that refer to the virtual swap slot, i.e\n+ *    its swap count.\n+ *\n+ * 2. Whether the virtual swap slot has been added to the swap cache - if so,\n+ *    its reference count is incremented by 1.\n+ *\n+ * Each virtual swap slot starts out with a reference count of 1 (since it is\n+ * about to be added to the swap cache). Its reference count is incremented or\n+ * decremented every time it is mapped to or unmapped from a PTE, as well as\n+ * when it is added to or removed from the swap cache. Finally, when its\n+ * reference count reaches 0, the virtual swap slot is freed.\n+ *\n+ * Note that we do not have a reference count field per se - it is derived from\n+ * the swap_count and the in_swapcache fields.\n  */\n \n /**\n@@ -42,6 +64,8 @@\n  * @swap_cache: The folio in swap cache.\n  * @shadow: The shadow entry.\n  * @memcgid: The memcg id of the owning memcg, if any.\n+ * @swap_count: The number of page table entries that refer to the swap entry.\n+ * @in_swapcache: Whether the swap entry is (about to be) pinned in swap cache.\n  */\n struct swp_desc {\n \tswp_slot_t slot;\n@@ -50,9 +74,14 @@ struct swp_desc {\n \t\tstruct folio *swap_cache;\n \t\tvoid *shadow;\n \t};\n+\n+\tunsigned int swap_count;\n+\n #ifdef CONFIG_MEMCG\n \tunsigned short memcgid;\n #endif\n+\n+\tbool in_swapcache;\n };\n \n #define VSWAP_CLUSTER_SHIFT HPAGE_PMD_ORDER\n@@ -249,6 +278,8 @@ static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster, int start)\n #ifdef CONFIG_MEMCG\n \t\tdesc->memcgid = 0;\n #endif\n+\t\tdesc->swap_count = 0;\n+\t\tdesc->in_swapcache = true;\n \t}\n \tcluster->count += nr;\n }\n@@ -452,7 +483,7 @@ static inline void release_vswap_slot(struct vswap_cluster *cluster,\n  * Update the physical-to-virtual swap slot mapping.\n  * Caller must ensure the physical swap slot's cluster is locked.\n  */\n-static void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n+void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\t\t   unsigned long vswap, int nr)\n {\n \tatomic_long_t *table;\n@@ -466,45 +497,50 @@ static void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\t__swap_table_set(ci, ci_off + i, vswap ? vswap + i : 0);\n }\n \n-/**\n- * vswap_free - free a virtual swap slot.\n- * @entry: the virtual swap slot to free\n- * @ci: the physical swap slot's cluster (optional, can be NULL)\n+/*\n+ * Entered with the cluster locked, but might unlock the cluster.\n+ * This is because several operations, such as releasing physical swap slots\n+ * (i.e swap_slot_free_nr()) require the cluster to be unlocked to avoid\n+ * deadlocks.\n  *\n- * If @ci is NULL, this function is called to clean up a virtual swap entry\n- * when no linkage has been established between physical and virtual swap slots.\n- * If @ci is provided, the caller must ensure it is locked.\n+ * This is safe, because:\n+ *\n+ * 1. The swap entry to be freed has refcnt (swap count and swapcache pin)\n+ *    down to 0, so no one can change its internal state\n+ *\n+ * 2. The swap entry to be freed still holds a refcnt to the cluster, keeping\n+ *    the cluster itself valid.\n+ *\n+ * We will exit the function with the cluster re-locked.\n  */\n-void vswap_free(swp_entry_t entry, struct swap_cluster_info *ci)\n+static void vswap_free(struct vswap_cluster *cluster, struct swp_desc *desc,\n+\tswp_entry_t entry)\n {\n-\tstruct vswap_cluster *cluster = NULL;\n-\tstruct swp_desc *desc;\n+\tstruct zswap_entry *zswap_entry;\n+\tswp_slot_t slot;\n \n-\tif (!entry.val)\n-\t\treturn;\n+\t/* Clear shadow if present */\n+\tif (xa_is_value(desc->shadow))\n+\t\tdesc->shadow = NULL;\n \n-\tzswap_invalidate(entry);\n-\tmem_cgroup_uncharge_swap(entry, 1);\n+\tslot = desc->slot;\n+\tdesc->slot.val = 0;\n \n-\t/* do not immediately erase the virtual slot to prevent its reuse */\n-\trcu_read_lock();\n-\tdesc = vswap_iter(&cluster, entry.val);\n-\tif (!desc) {\n-\t\trcu_read_unlock();\n-\t\treturn;\n+\tzswap_entry = desc->zswap_entry;\n+\tif (zswap_entry) {\n+\t\tdesc->zswap_entry = NULL;\n+\t\tzswap_entry_free(zswap_entry);\n \t}\n+\tspin_unlock(&cluster->lock);\n \n-\t/* Clear shadow if present */\n-\tif (xa_is_value(desc->shadow))\n-\t\tdesc->shadow = NULL;\n+\tmem_cgroup_uncharge_swap(entry, 1);\n \n-\tif (desc->slot.val)\n-\t\tvswap_rmap_set(ci, desc->slot, 0, 1);\n+\tif (slot.val)\n+\t\tswap_slot_free_nr(slot, 1);\n \n+\tspin_lock(&cluster->lock);\n \t/* erase forward mapping and release the virtual slot for reallocation */\n \trelease_vswap_slot(cluster, entry.val);\n-\tspin_unlock(&cluster->lock);\n-\trcu_read_unlock();\n }\n \n /**\n@@ -538,8 +574,12 @@ int folio_alloc_swap(struct folio *folio)\n \t * fallback from zswap store failure).\n \t */\n \tif (swap_slot_alloc(&slot, order)) {\n-\t\tfor (i = 0; i < nr; i++)\n-\t\t\tvswap_free((swp_entry_t){entry.val + i}, NULL);\n+\t\tfor (i = 0; i < nr; i++) {\n+\t\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\t\tVM_WARN_ON(!desc);\n+\t\t\tvswap_free(cluster, desc, (swp_entry_t){ entry.val + i });\n+\t\t}\n+\t\tspin_unlock(&cluster->lock);\n \t\tentry.val = 0;\n \t\treturn -ENOMEM;\n \t}\n@@ -603,9 +643,11 @@ swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry)\n \t\trcu_read_unlock();\n \t\treturn (swp_slot_t){0};\n \t}\n+\n \tslot = desc->slot;\n \tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n+\n \treturn slot;\n }\n \n@@ -635,6 +677,352 @@ swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot)\n \treturn ret;\n }\n \n+/*\n+ * Decrease the swap count of nr contiguous swap entries by 1 (when the swap\n+ * entries are removed from a range of PTEs), and check if any of the swap\n+ * entries are in swap cache only after its swap count is decreased.\n+ *\n+ * The check is racy, but it is OK because free_swap_and_cache_nr() only use\n+ * the result as a hint.\n+ */\n+static bool vswap_free_nr_any_cache_only(swp_entry_t entry, int nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tbool ret = false;\n+\tint i;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val);\n+\t\tVM_WARN_ON(!desc);\n+\t\tret |= (desc->swap_count == 1 && desc->in_swapcache);\n+\t\tdesc->swap_count--;\n+\t\tif (!desc->swap_count && !desc->in_swapcache)\n+\t\t\tvswap_free(cluster, desc, entry);\n+\t\tentry.val++;\n+\t}\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\treturn ret;\n+}\n+\n+/**\n+ * swap_free_nr - decrease the swap count of nr contiguous swap entries by 1\n+ *                (when the swap entries are removed from a range of PTEs).\n+ * @entry: the first entry in the range.\n+ * @nr: the number of entries in the range.\n+ */\n+void swap_free_nr(swp_entry_t entry, int nr)\n+{\n+\tvswap_free_nr_any_cache_only(entry, nr);\n+}\n+\n+static int swap_duplicate_nr(swp_entry_t entry, int nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tint i = 0;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tif (!desc || (!desc->swap_count && !desc->in_swapcache))\n+\t\t\tgoto done;\n+\t\tdesc->swap_count++;\n+\t}\n+done:\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\tif (i && i < nr)\n+\t\tswap_free_nr(entry, i);\n+\n+\treturn i == nr ? 0 : -ENOENT;\n+}\n+\n+/**\n+ * swap_duplicate - increase the swap count of the swap entry by 1 (i.e when\n+ *                  the swap entry is stored at a new PTE).\n+ * @entry: the swap entry.\n+ *\n+ * Return: -ENONENT, if we try to duplicate a non-existent swap entry.\n+ */\n+int swap_duplicate(swp_entry_t entry)\n+{\n+\treturn swap_duplicate_nr(entry, 1);\n+}\n+\n+\n+bool folio_swapped(struct folio *folio)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tswp_entry_t entry = folio->swap;\n+\tint i, nr = folio_nr_pages(folio);\n+\tstruct swp_desc *desc;\n+\tbool swapped = false;\n+\n+\tif (!entry.val)\n+\t\treturn false;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tif (desc && desc->swap_count) {\n+\t\t\tswapped = true;\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\treturn swapped;\n+}\n+\n+/**\n+ * swp_swapcount - return the swap count of the swap entry.\n+ * @id: the swap entry.\n+ *\n+ * Note that all the swap count functions are identical in the new design,\n+ * since we no longer need swap count continuation.\n+ *\n+ * Return: the swap count of the swap entry.\n+ */\n+int swp_swapcount(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tunsigned int ret;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tret = desc ? desc->swap_count : 0;\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn ret;\n+}\n+\n+int __swap_count(swp_entry_t entry)\n+{\n+\treturn swp_swapcount(entry);\n+}\n+\n+bool swap_entry_swapped(swp_entry_t entry)\n+{\n+\treturn !!swp_swapcount(entry);\n+}\n+\n+void swap_shmem_alloc(swp_entry_t entry, int nr)\n+{\n+\tswap_duplicate_nr(entry, nr);\n+}\n+\n+void swapcache_clear(swp_entry_t entry, int nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tint i;\n+\n+\tif (!nr)\n+\t\treturn;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val);\n+\t\tdesc->in_swapcache = false;\n+\t\tif (!desc->swap_count)\n+\t\t\tvswap_free(cluster, desc, entry);\n+\t\tentry.val++;\n+\t}\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+int swapcache_prepare(swp_entry_t entry, int nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tint i, ret = 0;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\n+\t\tif (!desc) {\n+\t\t\tret = -ENOENT;\n+\t\t\tgoto done;\n+\t\t}\n+\n+\t\tif (!desc->swap_count && !desc->in_swapcache) {\n+\t\t\tret = -ENOENT;\n+\t\t\tgoto done;\n+\t\t}\n+\n+\t\tif (desc->in_swapcache) {\n+\t\t\tret = -EEXIST;\n+\t\t\tgoto done;\n+\t\t}\n+\n+\t\tdesc->in_swapcache = true;\n+\t}\n+done:\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\tif (i && i < nr)\n+\t\tswapcache_clear(entry, i);\n+\tif (i < nr && !ret)\n+\t\tret = -ENOENT;\n+\treturn ret;\n+}\n+\n+/**\n+ * is_swap_cached - check if the swap entry is cached\n+ * @entry: swap entry to check\n+ *\n+ * Returns true if the swap entry is cached, false otherwise.\n+ */\n+bool is_swap_cached(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tbool cached;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tcached = desc ? desc->in_swapcache : false;\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn cached;\n+}\n+\n+/**\n+ * vswap_only_has_cache - check if all the slots in the range are still valid,\n+ *                        and are in swap cache only (i.e not stored in any\n+ *                        PTEs).\n+ * @entry: the first slot in the range.\n+ * @nr: the number of slots in the range.\n+ *\n+ * Return: true if all the slots in the range are still valid, and are in swap\n+ * cache only, or false otherwise.\n+ */\n+bool vswap_only_has_cache(swp_entry_t entry, int nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tint i = 0;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tif (!desc || desc->swap_count || !desc->in_swapcache)\n+\t\t\tgoto done;\n+\t}\n+done:\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\treturn i == nr;\n+}\n+\n+/**\n+ * non_swapcache_batch - count the longest range starting from a particular\n+ *                       swap slot that are stil valid, but not in swap cache.\n+ * @entry: the first slot to check.\n+ * @max_nr: the maximum number of slots to check.\n+ *\n+ * Return: the number of slots in the longest range that are still valid, but\n+ * not in swap cache.\n+ */\n+int non_swapcache_batch(swp_entry_t entry, int max_nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tint i;\n+\n+\tif (!entry.val)\n+\t\treturn 0;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < max_nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tif (!desc || desc->in_swapcache || !desc->swap_count)\n+\t\t\tgoto done;\n+\t}\n+done:\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\treturn i;\n+}\n+\n+/**\n+ * free_swap_and_cache_nr() - Release a swap count on range of swap entries and\n+ *                            reclaim their cache if no more references remain.\n+ * @entry: First entry of range.\n+ * @nr: Number of entries in range.\n+ *\n+ * For each swap entry in the contiguous range, release a swap count. If any\n+ * swap entries have their swap count decremented to zero, try to reclaim their\n+ * associated swap cache pages.\n+ */\n+void free_swap_and_cache_nr(swp_entry_t entry, int nr)\n+{\n+\tint i = 0, incr = 1;\n+\tstruct folio *folio;\n+\n+\tif (vswap_free_nr_any_cache_only(entry, nr)) {\n+\t\twhile (i < nr) {\n+\t\t\tincr = 1;\n+\t\t\tif (vswap_only_has_cache(entry, 1)) {\n+\t\t\t\tfolio = swap_cache_get_folio(entry);\n+\t\t\t\tif (!folio)\n+\t\t\t\t\tgoto next;\n+\n+\t\t\t\tif (!folio_trylock(folio)) {\n+\t\t\t\t\tfolio_put(folio);\n+\t\t\t\t\tgoto next;\n+\t\t\t\t}\n+\n+\t\t\t\tif (!folio_matches_swap_entry(folio, entry)) {\n+\t\t\t\t\tfolio_unlock(folio);\n+\t\t\t\t\tfolio_put(folio);\n+\t\t\t\t\tgoto next;\n+\t\t\t\t}\n+\n+\t\t\t\t/*\n+\t\t\t\t * Folios are always naturally aligned in swap so\n+\t\t\t\t * advance forward to the next boundary.\n+\t\t\t\t */\n+\t\t\t\tincr = ALIGN(entry.val + 1, folio_nr_pages(folio)) - entry.val;\n+\t\t\t\tfolio_free_swap(folio);\n+\t\t\t\tfolio_unlock(folio);\n+\t\t\t\tfolio_put(folio);\n+\t\t\t}\n+next:\n+\t\t\ti += incr;\n+\t\t\tentry.val += incr;\n+\t\t}\n+\t}\n+}\n+\n+/*\n+ * Called after dropping swapcache to decrease refcnt to swap entries.\n+ */\n+void put_swap_folio(struct folio *folio, swp_entry_t entry)\n+{\n+\tint nr = folio_nr_pages(folio);\n+\n+\tVM_WARN_ON(!folio_test_locked(folio));\n+\tswapcache_clear(entry, nr);\n+}\n+\n bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si)\n {\n \tstruct vswap_cluster *cluster;\n@@ -869,8 +1257,8 @@ void *swap_cache_get_shadow(swp_entry_t entry)\n  * Context: Caller must ensure @entry is valid and protect the cluster with\n  * reference count or locks.\n  *\n- * The caller also needs to update the corresponding swap_map slots with\n- * SWAP_HAS_CACHE bit to avoid race or conflict.\n+ * The caller also needs to obtain the swap entries' swap cache pins to avoid\n+ * race or conflict.\n  */\n void swap_cache_add_folio(struct folio *folio, swp_entry_t entry, void **shadowp)\n {\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex 72441131f094e..e46349f9c90bb 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -703,7 +703,7 @@ static void zswap_entry_cache_free(struct zswap_entry *entry)\n  * Carries out the common pattern of freeing an entry's zsmalloc allocation,\n  * freeing the entry itself, and decrementing the number of stored pages.\n  */\n-static void zswap_entry_free(struct zswap_entry *entry)\n+void zswap_entry_free(struct zswap_entry *entry)\n {\n \tzswap_lru_del(&zswap_list_lru, entry);\n \tzs_free(entry->pool->zs_pool, entry->handle);\n@@ -1627,18 +1627,6 @@ int zswap_load(struct folio *folio)\n \treturn 0;\n }\n \n-void zswap_invalidate(swp_entry_t swp)\n-{\n-\tstruct zswap_entry *entry;\n-\n-\tif (zswap_empty(swp))\n-\t\treturn;\n-\n-\tentry = zswap_entry_erase(swp);\n-\tif (entry)\n-\t\tzswap_entry_free(entry);\n-}\n-\n /*********************************\n * debugfs functions\n **********************************/\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author acknowledged that the vswap free path needs to be more careful about batched swap operations, specifically requiring a consistent backing state for all entries in the batch. They explained that zswap-backed entries are not supported for these batched operations and provided rules for ensuring consistency. The author did not mention any plans to address this issue in the current patch or provide a fix.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a problem",
                "explained requirements"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch presents the first real use case of the new virtual swap\ndesign. It leverages the virtualization of the swap space to decouple a\nswap entry and its backing storage. A swap entry can now be backed by\none of the following options:\n\n1. A physical swap slot (i.e on a physical swapfile/swap partition).\n2. A \"zero swap page\", i.e the swapped out page is a zero page.\n3. A compressed object in the zswap pool.\n4. An in-memory page. This can happen when a page is loaded\n   (exclusively) from the zswap pool, or if the page is rejected by\n   zswap and zswap writeback is disabled.\n\nThis allows us to use zswap and the zero swap page optimization, without\nhaving to reserved a slot on a swapfile, or a swapfile at all. This\ntranslates to tens to hundreds of GBs of disk saving on hosts and\nworkloads that have high memory usage, as well as removes this spurious\nlimit on the usage of these optimizations.\n\nOne implication of this change is that we need to be much more careful\nwith THP swapin and batched swap free operations. The central\nrequirement is the range of entries we are working with must\nhave no mixed backing states:\n\n1. For now, zswap-backed entries are not supported for these batched\n   operations.\n2. All the entries must be backed by the same type.\n3. If the swap entries in the batch are backed by in-memory folio, it\n   must be the same folio (i.e they correspond to the subpages of that\n   folio).\n4. If the swap entries in the batch are backed by slots on swapfiles, it\n   must be the same swapfile, and these physical swap slots must also be\n   contiguous.\n\nFor now, we still charge virtual swap slots towards the memcg's swap\nusage. In a following patch, we will change this behavior and only\ncharge physical (i.e on swapfile) swap slots towards the memcg's swap\nusage.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h  |  14 +\n include/linux/zswap.h |   3 +-\n mm/internal.h         |  14 +-\n mm/memcontrol.c       |  65 +++--\n mm/memory.c           |  84 ++++--\n mm/page_io.c          |  74 ++---\n mm/shmem.c            |   6 +-\n mm/swap.h             |  32 +--\n mm/swap_state.c       |  29 +-\n mm/swapfile.c         |   8 -\n mm/vmscan.c           |  19 +-\n mm/vswap.c            | 638 ++++++++++++++++++++++++++++++++++--------\n mm/zswap.c            |  45 ++-\n 13 files changed, 729 insertions(+), 302 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex aae2e502d9975..54df972608047 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -658,12 +658,26 @@ static inline bool mem_cgroup_swap_full(struct folio *folio)\n \n int vswap_init(void);\n void vswap_exit(void);\n+bool vswap_alloc_swap_slot(struct folio *folio);\n swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry);\n swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot);\n bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si);\n void put_swap_entry(swp_entry_t entry, struct swap_info_struct *si);\n bool folio_swapped(struct folio *folio);\n bool vswap_only_has_cache(swp_entry_t entry, int nr);\n+int non_swapcache_batch(swp_entry_t entry, int nr);\n+bool vswap_swapfile_backed(swp_entry_t entry, int nr);\n+bool vswap_folio_backed(swp_entry_t entry, int nr);\n+void vswap_store_folio(swp_entry_t entry, struct folio *folio);\n+void swap_zeromap_folio_set(struct folio *folio);\n+void vswap_assoc_zswap(swp_entry_t entry, struct zswap_entry *zswap_entry);\n+bool vswap_can_swapin_thp(swp_entry_t entry, int nr);\n \n+static inline struct swap_info_struct *vswap_get_device(swp_entry_t entry)\n+{\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\n+\treturn slot.val ? swap_slot_tryget_swap_info(slot) : NULL;\n+}\n #endif /* __KERNEL__*/\n #endif /* _LINUX_SWAP_H */\ndiff --git a/include/linux/zswap.h b/include/linux/zswap.h\nindex 07b2936c38f29..f33b4433a5ee8 100644\n--- a/include/linux/zswap.h\n+++ b/include/linux/zswap.h\n@@ -33,9 +33,8 @@ void zswap_lruvec_state_init(struct lruvec *lruvec);\n void zswap_folio_swapin(struct folio *folio);\n bool zswap_is_enabled(void);\n bool zswap_never_enabled(void);\n-void *zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry);\n+void zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry);\n void *zswap_entry_load(swp_entry_t swpentry);\n-void *zswap_entry_erase(swp_entry_t swpentry);\n bool zswap_empty(swp_entry_t swpentry);\n void zswap_entry_free(struct zswap_entry *entry);\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 7ced0def684ca..cfe97501e4885 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -327,19 +327,7 @@ static inline swp_entry_t swap_nth(swp_entry_t entry, long n)\n \treturn (swp_entry_t) { entry.val + n };\n }\n \n-/* similar to swap_nth, but check the backing physical slots as well. */\n-static inline swp_entry_t swap_move(swp_entry_t entry, long delta)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry), next_slot;\n-\tswp_entry_t next_entry = swap_nth(entry, delta);\n-\n-\tnext_slot = swp_entry_to_swp_slot(next_entry);\n-\tif (swp_slot_type(slot) != swp_slot_type(next_slot) ||\n-\t\t\tswp_slot_offset(slot) + delta != swp_slot_offset(next_slot))\n-\t\tnext_entry.val = 0;\n-\n-\treturn next_entry;\n-}\n+swp_entry_t swap_move(swp_entry_t entry, long delta);\n \n /**\n  * pte_move_swp_offset - Move the swap entry offset field of a swap pte\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 86f43b7e5f710..2ba5811e7edba 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -5247,10 +5247,18 @@ void __mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages)\n \trcu_read_unlock();\n }\n \n+static bool mem_cgroup_may_zswap(struct mem_cgroup *original_memcg);\n+\n long mem_cgroup_get_nr_swap_pages(struct mem_cgroup *memcg)\n {\n-\tlong nr_swap_pages = get_nr_swap_pages();\n+\tlong nr_swap_pages, nr_zswap_pages = 0;\n+\n+\tif (zswap_is_enabled() && (mem_cgroup_disabled() || do_memsw_account() ||\n+\t\t\t\tmem_cgroup_may_zswap(memcg))) {\n+\t\tnr_zswap_pages = PAGE_COUNTER_MAX;\n+\t}\n \n+\tnr_swap_pages = max_t(long, nr_zswap_pages, get_nr_swap_pages());\n \tif (mem_cgroup_disabled() || do_memsw_account())\n \t\treturn nr_swap_pages;\n \tfor (; !mem_cgroup_is_root(memcg); memcg = parent_mem_cgroup(memcg))\n@@ -5419,6 +5427,29 @@ static struct cftype swap_files[] = {\n };\n \n #ifdef CONFIG_ZSWAP\n+static bool mem_cgroup_may_zswap(struct mem_cgroup *original_memcg)\n+{\n+\tstruct mem_cgroup *memcg;\n+\n+\tfor (memcg = original_memcg; !mem_cgroup_is_root(memcg);\n+\t     memcg = parent_mem_cgroup(memcg)) {\n+\t\tunsigned long max = READ_ONCE(memcg->zswap_max);\n+\t\tunsigned long pages;\n+\n+\t\tif (max == PAGE_COUNTER_MAX)\n+\t\t\tcontinue;\n+\t\tif (max == 0)\n+\t\t\treturn false;\n+\n+\t\t/* Force flush to get accurate stats for charging */\n+\t\t__mem_cgroup_flush_stats(memcg, true);\n+\t\tpages = memcg_page_state(memcg, MEMCG_ZSWAP_B) / PAGE_SIZE;\n+\t\tif (pages >= max)\n+\t\t\treturn false;\n+\t}\n+\treturn true;\n+}\n+\n /**\n  * obj_cgroup_may_zswap - check if this cgroup can zswap\n  * @objcg: the object cgroup\n@@ -5433,34 +5464,15 @@ static struct cftype swap_files[] = {\n  */\n bool obj_cgroup_may_zswap(struct obj_cgroup *objcg)\n {\n-\tstruct mem_cgroup *memcg, *original_memcg;\n+\tstruct mem_cgroup *memcg;\n \tbool ret = true;\n \n \tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys))\n \t\treturn true;\n \n-\toriginal_memcg = get_mem_cgroup_from_objcg(objcg);\n-\tfor (memcg = original_memcg; !mem_cgroup_is_root(memcg);\n-\t     memcg = parent_mem_cgroup(memcg)) {\n-\t\tunsigned long max = READ_ONCE(memcg->zswap_max);\n-\t\tunsigned long pages;\n-\n-\t\tif (max == PAGE_COUNTER_MAX)\n-\t\t\tcontinue;\n-\t\tif (max == 0) {\n-\t\t\tret = false;\n-\t\t\tbreak;\n-\t\t}\n-\n-\t\t/* Force flush to get accurate stats for charging */\n-\t\t__mem_cgroup_flush_stats(memcg, true);\n-\t\tpages = memcg_page_state(memcg, MEMCG_ZSWAP_B) / PAGE_SIZE;\n-\t\tif (pages < max)\n-\t\t\tcontinue;\n-\t\tret = false;\n-\t\tbreak;\n-\t}\n-\tmem_cgroup_put(original_memcg);\n+\tmemcg = get_mem_cgroup_from_objcg(objcg);\n+\tret = mem_cgroup_may_zswap(memcg);\n+\tmem_cgroup_put(memcg);\n \treturn ret;\n }\n \n@@ -5604,6 +5616,11 @@ static struct cftype zswap_files[] = {\n \t},\n \t{ }\t/* terminate */\n };\n+#else\n+static inline bool mem_cgroup_may_zswap(struct mem_cgroup *original_memcg)\n+{\n+\treturn false;\n+}\n #endif /* CONFIG_ZSWAP */\n \n static int __init mem_cgroup_swap_init(void)\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 641e3f65edc00..a16bf84ebaaf9 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -4362,6 +4362,15 @@ static inline bool should_try_to_free_swap(struct folio *folio,\n \tif (mem_cgroup_swap_full(folio) || (vma->vm_flags & VM_LOCKED) ||\n \t    folio_test_mlocked(folio))\n \t\treturn true;\n+\n+\t/*\n+\t * Mixed and/or non-swapfile backends cannot be re-used for future swapouts\n+\t * anyway. Try to free swap space unless the folio is backed by contiguous\n+\t * physical swap slots.\n+\t */\n+\tif (!vswap_swapfile_backed(folio->swap, folio_nr_pages(folio)))\n+\t\treturn true;\n+\n \t/*\n \t * If we want to map a page that's in the swapcache writable, we\n \t * have to detect via the refcount if we're really the exclusive\n@@ -4623,12 +4632,12 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \tstruct folio *swapcache, *folio = NULL;\n \tDECLARE_WAITQUEUE(wait, current);\n \tstruct page *page;\n-\tstruct swap_info_struct *si = NULL;\n+\tstruct swap_info_struct *si = NULL, *stable_si;\n \trmap_t rmap_flags = RMAP_NONE;\n \tbool need_clear_cache = false;\n \tbool swapoff_locked = false;\n \tbool exclusive = false;\n-\tsoftleaf_t entry;\n+\tsoftleaf_t orig_entry, entry;\n \tpte_t pte;\n \tvm_fault_t ret = 0;\n \tvoid *shadow = NULL;\n@@ -4641,6 +4650,11 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\tgoto out;\n \n \tentry = softleaf_from_pte(vmf->orig_pte);\n+\t/*\n+\t * entry might change if we get a large folio - remember the original entry\n+\t * for unlocking swapoff etc.\n+\t */\n+\torig_entry = entry;\n \tif (unlikely(!softleaf_is_swap(entry))) {\n \t\tif (softleaf_is_migration(entry)) {\n \t\t\tmigration_entry_wait(vma->vm_mm, vmf->pmd,\n@@ -4705,7 +4719,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \tswapcache = folio;\n \n \tif (!folio) {\n-\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO) &&\n+\t\tif (si && data_race(si->flags & SWP_SYNCHRONOUS_IO) &&\n \t\t    __swap_count(entry) == 1) {\n \t\t\t/* skip swapcache */\n \t\t\tfolio = alloc_swap_folio(vmf);\n@@ -4736,6 +4750,17 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\t\t\t}\n \t\t\t\tneed_clear_cache = true;\n \n+\t\t\t\t/*\n+\t\t\t\t * Recheck to make sure the entire range is still\n+\t\t\t\t * THP-swapin-able. Note that before we call\n+\t\t\t\t * swapcache_prepare(), entries in the range can\n+\t\t\t\t * still have their backing status changed.\n+\t\t\t\t */\n+\t\t\t\tif (!vswap_can_swapin_thp(entry, nr_pages)) {\n+\t\t\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\t\t\tgoto out_page;\n+\t\t\t\t}\n+\n \t\t\t\tmemcg1_swapin(entry, nr_pages);\n \n \t\t\t\tshadow = swap_cache_get_shadow(entry);\n@@ -4916,27 +4941,40 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\t\t * swapcache -> certainly exclusive.\n \t\t\t */\n \t\t\texclusive = true;\n-\t\t} else if (exclusive && folio_test_writeback(folio) &&\n-\t\t\t  data_race(si->flags & SWP_STABLE_WRITES)) {\n+\t\t} else if (exclusive && folio_test_writeback(folio)) {\n \t\t\t/*\n-\t\t\t * This is tricky: not all swap backends support\n-\t\t\t * concurrent page modifications while under writeback.\n-\t\t\t *\n-\t\t\t * So if we stumble over such a page in the swapcache\n-\t\t\t * we must not set the page exclusive, otherwise we can\n-\t\t\t * map it writable without further checks and modify it\n-\t\t\t * while still under writeback.\n+\t\t\t * We need to look up the swap device again here, because\n+\t\t\t * the si we got from tryget_swap_entry() might have changed\n+\t\t\t * before we pin the backend.\n \t\t\t *\n-\t\t\t * For these problematic swap backends, simply drop the\n-\t\t\t * exclusive marker: this is perfectly fine as we start\n-\t\t\t * writeback only if we fully unmapped the page and\n-\t\t\t * there are no unexpected references on the page after\n-\t\t\t * unmapping succeeded. After fully unmapped, no\n-\t\t\t * further GUP references (FOLL_GET and FOLL_PIN) can\n-\t\t\t * appear, so dropping the exclusive marker and mapping\n-\t\t\t * it only R/O is fine.\n+\t\t\t * With the folio locked and loaded into the swap cache, we can\n+\t\t\t * now guarantee a stable backing state.\n \t\t\t */\n-\t\t\texclusive = false;\n+\t\t\tstable_si = vswap_get_device(entry);\n+\t\t\tif (stable_si && data_race(stable_si->flags & SWP_STABLE_WRITES)) {\n+\t\t\t\t/*\n+\t\t\t\t * This is tricky: not all swap backends support\n+\t\t\t\t * concurrent page modifications while under writeback.\n+\t\t\t\t *\n+\t\t\t\t * So if we stumble over such a page in the swapcache\n+\t\t\t\t * we must not set the page exclusive, otherwise we can\n+\t\t\t\t * map it writable without further checks and modify it\n+\t\t\t\t * while still under writeback.\n+\t\t\t\t *\n+\t\t\t\t * For these problematic swap backends, simply drop the\n+\t\t\t\t * exclusive marker: this is perfectly fine as we start\n+\t\t\t\t * writeback only if we fully unmapped the page and\n+\t\t\t\t * there are no unexpected references on the page after\n+\t\t\t\t * unmapping succeeded. After fully unmapped, no\n+\t\t\t\t * further GUP references (FOLL_GET and FOLL_PIN) can\n+\t\t\t\t * appear, so dropping the exclusive marker and mapping\n+\t\t\t\t * it only R/O is fine.\n+\t\t\t\t */\n+\t\t\t\texclusive = false;\n+\t\t\t}\n+\n+\t\t\tif (stable_si)\n+\t\t\t\tswap_slot_put_swap_info(stable_si);\n \t\t}\n \t}\n \n@@ -5045,7 +5083,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\t\twake_up(&swapcache_wq);\n \t}\n \tif (swapoff_locked)\n-\t\tput_swap_entry(entry, si);\n+\t\tput_swap_entry(orig_entry, si);\n \treturn ret;\n out_nomap:\n \tif (vmf->pte)\n@@ -5064,7 +5102,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\t\twake_up(&swapcache_wq);\n \t}\n \tif (swapoff_locked)\n-\t\tput_swap_entry(entry, si);\n+\t\tput_swap_entry(orig_entry, si);\n \treturn ret;\n }\n \ndiff --git a/mm/page_io.c b/mm/page_io.c\nindex 5de3705572955..675ec6445609b 100644\n--- a/mm/page_io.c\n+++ b/mm/page_io.c\n@@ -201,44 +201,6 @@ static bool is_folio_zero_filled(struct folio *folio)\n \treturn true;\n }\n \n-static void swap_zeromap_folio_set(struct folio *folio)\n-{\n-\tstruct obj_cgroup *objcg = get_obj_cgroup_from_folio(folio);\n-\tstruct swap_info_struct *sis =\n-\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n-\tint nr_pages = folio_nr_pages(folio);\n-\tswp_entry_t entry;\n-\tswp_slot_t slot;\n-\tunsigned int i;\n-\n-\tfor (i = 0; i < folio_nr_pages(folio); i++) {\n-\t\tentry = page_swap_entry(folio_page(folio, i));\n-\t\tslot = swp_entry_to_swp_slot(entry);\n-\t\tset_bit(swp_slot_offset(slot), sis->zeromap);\n-\t}\n-\n-\tcount_vm_events(SWPOUT_ZERO, nr_pages);\n-\tif (objcg) {\n-\t\tcount_objcg_events(objcg, SWPOUT_ZERO, nr_pages);\n-\t\tobj_cgroup_put(objcg);\n-\t}\n-}\n-\n-static void swap_zeromap_folio_clear(struct folio *folio)\n-{\n-\tstruct swap_info_struct *sis =\n-\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n-\tswp_entry_t entry;\n-\tswp_slot_t slot;\n-\tunsigned int i;\n-\n-\tfor (i = 0; i < folio_nr_pages(folio); i++) {\n-\t\tentry = page_swap_entry(folio_page(folio, i));\n-\t\tslot = swp_entry_to_swp_slot(entry);\n-\t\tclear_bit(swp_slot_offset(slot), sis->zeromap);\n-\t}\n-}\n-\n /*\n  * We may have stale swap cache pages in memory: notice\n  * them here and get rid of the unnecessary final write.\n@@ -260,23 +222,22 @@ int swap_writeout(struct folio *folio, struct swap_iocb **swap_plug)\n \t\tgoto out_unlock;\n \t}\n \n-\t/*\n-\t * Use a bitmap (zeromap) to avoid doing IO for zero-filled pages.\n-\t * The bits in zeromap are protected by the locked swapcache folio\n-\t * and atomic updates are used to protect against read-modify-write\n-\t * corruption due to other zero swap entries seeing concurrent updates.\n-\t */\n \tif (is_folio_zero_filled(folio)) {\n \t\tswap_zeromap_folio_set(folio);\n \t\tgoto out_unlock;\n \t}\n \n \t/*\n-\t * Clear bits this folio occupies in the zeromap to prevent zero data\n-\t * being read in from any previous zero writes that occupied the same\n-\t * swap entries.\n+\t * Release swap backends to make sure we do not have mixed backends\n+\t *\n+\t * The only exception is if the folio is already backed by a\n+\t * contiguous range of physical swap slots (for e.g, from a previous\n+\t * swapout attempt when zswap is disabled).\n+\t *\n+\t * Keep that backend to avoid reallocation of physical swap slots.\n \t */\n-\tswap_zeromap_folio_clear(folio);\n+\tif (!vswap_swapfile_backed(folio->swap, folio_nr_pages(folio)))\n+\t\tvswap_store_folio(folio->swap, folio);\n \n \tif (zswap_store(folio)) {\n \t\tcount_mthp_stat(folio_order(folio), MTHP_STAT_ZSWPOUT);\n@@ -287,6 +248,12 @@ int swap_writeout(struct folio *folio, struct swap_iocb **swap_plug)\n \t\treturn AOP_WRITEPAGE_ACTIVATE;\n \t}\n \n+\t/* fall back to physical swap device */\n+\tif (!vswap_alloc_swap_slot(folio)) {\n+\t\tfolio_mark_dirty(folio);\n+\t\treturn AOP_WRITEPAGE_ACTIVATE;\n+\t}\n+\n \t__swap_writepage(folio, swap_plug);\n \treturn 0;\n out_unlock:\n@@ -618,14 +585,11 @@ static void swap_read_folio_bdev_async(struct folio *folio,\n \n void swap_read_folio(struct folio *folio, struct swap_iocb **plug)\n {\n-\tstruct swap_info_struct *sis =\n-\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n-\tbool synchronous = sis->flags & SWP_SYNCHRONOUS_IO;\n-\tbool workingset = folio_test_workingset(folio);\n+\tstruct swap_info_struct *sis;\n+\tbool synchronous, workingset = folio_test_workingset(folio);\n \tunsigned long pflags;\n \tbool in_thrashing;\n \n-\tVM_BUG_ON_FOLIO(!folio_test_swapcache(folio) && !synchronous, folio);\n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n \tVM_BUG_ON_FOLIO(folio_test_uptodate(folio), folio);\n \n@@ -651,6 +615,10 @@ void swap_read_folio(struct folio *folio, struct swap_iocb **plug)\n \t/* We have to read from slower devices. Increase zswap protection. */\n \tzswap_folio_swapin(folio);\n \n+\tsis = __swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n+\tsynchronous = sis->flags & SWP_SYNCHRONOUS_IO;\n+\tVM_BUG_ON_FOLIO(!folio_test_swapcache(folio) && !synchronous, folio);\n+\n \tif (data_race(sis->flags & SWP_FS_OPS)) {\n \t\tswap_read_folio_fs(folio, plug);\n \t} else if (synchronous) {\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 780571c830e5b..3a346cca114ab 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -1459,7 +1459,7 @@ static unsigned int shmem_find_swap_entries(struct address_space *mapping,\n \t\t * swapin error entries can be found in the mapping. But they're\n \t\t * deliberately ignored here as we've done everything we can do.\n \t\t */\n-\t\tif (swp_slot_type(slot) != type)\n+\t\tif (!slot.val || swp_slot_type(slot) != type)\n \t\t\tcontinue;\n \n \t\tindices[folio_batch_count(fbatch)] = xas.xa_index;\n@@ -1604,7 +1604,7 @@ int shmem_writeout(struct folio *folio, struct swap_iocb **plug,\n \tif ((info->flags & SHMEM_F_LOCKED) || sbinfo->noswap)\n \t\tgoto redirty;\n \n-\tif (!total_swap_pages)\n+\tif (!zswap_is_enabled() && !total_swap_pages)\n \t\tgoto redirty;\n \n \t/*\n@@ -2341,7 +2341,7 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \t/* Look it up and read it in.. */\n \tfolio = swap_cache_get_folio(swap);\n \tif (!folio) {\n-\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO)) {\n+\t\tif (si && data_race(si->flags & SWP_SYNCHRONOUS_IO)) {\n \t\t\t/* Direct swapin skipping swap cache & readahead */\n \t\t\tfolio = shmem_swap_alloc_folio(inode, vma, index,\n \t\t\t\t\t\t       index_entry, order, gfp);\ndiff --git a/mm/swap.h b/mm/swap.h\nindex ae97cf9712c5c..d41e6a0e70753 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -310,35 +310,15 @@ static inline unsigned int folio_swap_flags(struct folio *folio)\n {\n \tswp_slot_t swp_slot = swp_entry_to_swp_slot(folio->swap);\n \n+\t/* The folio might not be backed by any physical swap slots\n+\t * (for e.g zswap-backed only).\n+\t */\n+\tif (!swp_slot.val)\n+\t\treturn 0;\n \treturn __swap_slot_to_info(swp_slot)->flags;\n }\n \n-/*\n- * Return the count of contiguous swap entries that share the same\n- * zeromap status as the starting entry. If is_zeromap is not NULL,\n- * it will return the zeromap status of the starting entry.\n- */\n-static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n-\t\tbool *is_zeromap)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n-\tunsigned long start = swp_slot_offset(slot);\n-\tunsigned long end = start + max_nr;\n-\tbool first_bit;\n-\n-\tfirst_bit = test_bit(start, sis->zeromap);\n-\tif (is_zeromap)\n-\t\t*is_zeromap = first_bit;\n-\n-\tif (max_nr <= 1)\n-\t\treturn max_nr;\n-\tif (first_bit)\n-\t\treturn find_next_zero_bit(sis->zeromap, end, start) - start;\n-\telse\n-\t\treturn find_next_bit(sis->zeromap, end, start) - start;\n-}\n-\n+int swap_zeromap_batch(swp_entry_t entry, int max_nr, bool *is_zeromap);\n int non_swapcache_batch(swp_entry_t entry, int max_nr);\n \n #else /* CONFIG_SWAP */\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 1827527e88d33..ad80bf098b63f 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -179,6 +179,10 @@ struct folio *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \tstruct folio *result = NULL;\n \tvoid *shadow = NULL;\n \n+\t/* we might get an unsed entry from cluster readahead - just skip */\n+\tif (!entry.val)\n+\t\treturn NULL;\n+\n \t*new_page_allocated = false;\n \tfor (;;) {\n \t\tint err;\n@@ -213,8 +217,20 @@ struct folio *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\t * Swap entry may have been freed since our caller observed it.\n \t\t */\n \t\terr = swapcache_prepare(entry, 1);\n-\t\tif (!err)\n+\t\tif (!err) {\n+\t\t\t/* This might be invoked by swap_cluster_readahead(), which can\n+\t\t\t * race with shmem_swapin_folio(). The latter might have already\n+\t\t\t * called swap_cache_del_folio(), allowing swapcache_prepare()\n+\t\t\t * to succeed here. This can lead to reading bogus data to populate\n+\t\t\t * the page. To prevent this, skip folio-backed virtual swap slots,\n+\t\t\t * and let caller retry if necessary.\n+\t\t\t */\n+\t\t\tif (vswap_folio_backed(entry, 1)) {\n+\t\t\t\tswapcache_clear(entry, 1);\n+\t\t\t\tgoto put_and_return;\n+\t\t\t}\n \t\t\tbreak;\n+\t\t}\n \t\telse if (err != -EEXIST)\n \t\t\tgoto put_and_return;\n \n@@ -391,11 +407,18 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \tunsigned long offset = slot_offset;\n \tunsigned long start_offset, end_offset;\n \tunsigned long mask;\n-\tstruct swap_info_struct *si = __swap_slot_to_info(slot);\n+\tstruct swap_info_struct *si = swap_slot_tryget_swap_info(slot);\n \tstruct blk_plug plug;\n \tstruct swap_iocb *splug = NULL;\n \tbool page_allocated;\n \n+\t/*\n+\t * The swap entry might not be backed by any physical swap slot. In that\n+\t * case, just skip readahead and bring in the target entry.\n+\t */\n+\tif (!si)\n+\t\tgoto skip;\n+\n \tmask = swapin_nr_pages(offset) - 1;\n \tif (!mask)\n \t\tgoto skip;\n@@ -429,6 +452,8 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \tswap_read_unplug(splug);\n \tlru_add_drain();\t/* Push any new pages onto the LRU now */\n skip:\n+\tif (si)\n+\t\tswap_slot_put_swap_info(si);\n \t/* The page was likely read above, so no need for plugging here */\n \tfolio = __read_swap_cache_async(entry, gfp_mask, mpol, ilx,\n \t\t\t\t\t&page_allocated, false);\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 6c5e46bf40701..1aa29dd220f9a 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1210,14 +1210,6 @@ static void swap_range_free(struct swap_info_struct *si, unsigned long offset,\n {\n \tunsigned long end = offset + nr_entries - 1;\n \tvoid (*swap_slot_free_notify)(struct block_device *, unsigned long);\n-\tunsigned int i;\n-\n-\t/*\n-\t * Use atomic clear_bit operations only on zeromap instead of non-atomic\n-\t * bitmap_clear to prevent adjacent bits corruption due to simultaneous writes.\n-\t */\n-\tfor (i = 0; i < nr_entries; i++)\n-\t\tclear_bit(offset + i, si->zeromap);\n \n \tif (si->flags & SWP_BLKDEV)\n \t\tswap_slot_free_notify =\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex c9ec1a1458b4e..6b200a6bb1160 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -365,10 +365,11 @@ static inline bool can_reclaim_anon_pages(struct mem_cgroup *memcg,\n {\n \tif (memcg == NULL) {\n \t\t/*\n-\t\t * For non-memcg reclaim, is there\n-\t\t * space in any swap device?\n+\t\t * For non-memcg reclaim:\n+\t\t *\n+\t\t * Check if zswap is enabled or if there is space in any swap device?\n \t\t */\n-\t\tif (get_nr_swap_pages() > 0)\n+\t\tif (zswap_is_enabled() || get_nr_swap_pages() > 0)\n \t\t\treturn true;\n \t} else {\n \t\t/* Is the memcg below its swap limit? */\n@@ -2640,12 +2641,12 @@ static void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,\n static bool can_age_anon_pages(struct lruvec *lruvec,\n \t\t\t       struct scan_control *sc)\n {\n-\t/* Aging the anon LRU is valuable if swap is present: */\n-\tif (total_swap_pages > 0)\n-\t\treturn true;\n-\n-\t/* Also valuable if anon pages can be demoted: */\n-\treturn can_demote(lruvec_pgdat(lruvec)->node_id, sc,\n+\t/*\n+\t * Aging the anon LRU is valuable if zswap or physical swap is available or\n+\t * anon pages can be demoted\n+\t */\n+\treturn zswap_is_enabled() || total_swap_pages > 0 ||\n+\t\t\tcan_demote(lruvec_pgdat(lruvec)->node_id, sc,\n \t\t\t  lruvec_memcg(lruvec));\n }\n \ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex 318933071edc6..fb6179ce3ace7 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -11,6 +11,7 @@\n #include <linux/swap_cgroup.h>\n #include <linux/cpuhotplug.h>\n #include <linux/zswap.h>\n+#include \"internal.h\"\n #include \"swap.h\"\n #include \"swap_table.h\"\n \n@@ -54,22 +55,48 @@\n  *\n  * Note that we do not have a reference count field per se - it is derived from\n  * the swap_count and the in_swapcache fields.\n+ *\n+ * III. Backing State\n+ *\n+ * Each virtual swap slot can be backed by:\n+ *\n+ * 1. A slot on a physical swap device (i.e a swapfile or a swap partition).\n+ * 2. A swapped out zero-filled page.\n+ * 3. A compressed object in zswap.\n+ * 4. An in-memory folio, that is not backed by neither a physical swap device\n+ *    nor zswap (i.e only in swap cache). This is used for pages that are\n+ *    rejected by zswap, but not (yet) backed by a physical swap device,\n+ *    (for e.g, due to zswap.writeback = 0), or for pages that were previously\n+ *    stored in zswap, but has since been loaded back into memory (and has its\n+ *    zswap copy invalidated).\n  */\n \n+/* The backing state options of a virtual swap slot */\n+enum swap_type {\n+\tVSWAP_SWAPFILE,\n+\tVSWAP_ZERO,\n+\tVSWAP_ZSWAP,\n+\tVSWAP_FOLIO\n+};\n+\n /**\n  * Swap descriptor - metadata of a swapped out page.\n  *\n  * @slot: The handle to the physical swap slot backing this page.\n  * @zswap_entry: The zswap entry associated with this swap slot.\n- * @swap_cache: The folio in swap cache.\n+ * @swap_cache: The folio in swap cache. If the swap entry backing type is\n+ *              VSWAP_FOLIO, the backend is also stored here.\n  * @shadow: The shadow entry.\n- * @memcgid: The memcg id of the owning memcg, if any.\n  * @swap_count: The number of page table entries that refer to the swap entry.\n+ * @memcgid: The memcg id of the owning memcg, if any.\n  * @in_swapcache: Whether the swap entry is (about to be) pinned in swap cache.\n+ * @type: The backing store type of the swap entry.\n  */\n struct swp_desc {\n-\tswp_slot_t slot;\n-\tstruct zswap_entry *zswap_entry;\n+\tunion {\n+\t\tswp_slot_t slot;\n+\t\tstruct zswap_entry *zswap_entry;\n+\t};\n \tunion {\n \t\tstruct folio *swap_cache;\n \t\tvoid *shadow;\n@@ -78,10 +105,10 @@ struct swp_desc {\n \tunsigned int swap_count;\n \n #ifdef CONFIG_MEMCG\n-\tunsigned short memcgid;\n+\tunsigned short memcgid:16;\n #endif\n-\n-\tbool in_swapcache;\n+\tbool in_swapcache:1;\n+\tenum swap_type type:2;\n };\n \n #define VSWAP_CLUSTER_SHIFT HPAGE_PMD_ORDER\n@@ -266,15 +293,16 @@ static bool cluster_is_alloc_candidate(struct vswap_cluster *cluster)\n \treturn cluster->count + (1 << (cluster->order)) <= VSWAP_CLUSTER_SIZE;\n }\n \n-static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster, int start)\n+static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster,\n+\t\tint start, struct folio *folio)\n {\n \tint i, nr = 1 << cluster->order;\n \tstruct swp_desc *desc;\n \n \tfor (i = 0; i < nr; i++) {\n \t\tdesc = &cluster->descriptors[start + i];\n-\t\tdesc->slot.val = 0;\n-\t\tdesc->zswap_entry = NULL;\n+\t\tdesc->type = VSWAP_FOLIO;\n+\t\tdesc->swap_cache = folio;\n #ifdef CONFIG_MEMCG\n \t\tdesc->memcgid = 0;\n #endif\n@@ -284,7 +312,8 @@ static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster, int start)\n \tcluster->count += nr;\n }\n \n-static unsigned long vswap_alloc_from_cluster(struct vswap_cluster *cluster)\n+static unsigned long vswap_alloc_from_cluster(struct vswap_cluster *cluster,\n+\t\tstruct folio *folio)\n {\n \tint nr = 1 << cluster->order;\n \tunsigned long i = cluster->id ? 0 : nr;\n@@ -303,16 +332,16 @@ static unsigned long vswap_alloc_from_cluster(struct vswap_cluster *cluster)\n \tbitmap_set(cluster->bitmap, i, nr);\n \n \trefcount_add(nr, &cluster->refcnt);\n-\t__vswap_alloc_from_cluster(cluster, i);\n+\t__vswap_alloc_from_cluster(cluster, i, folio);\n \treturn i + (cluster->id << VSWAP_CLUSTER_SHIFT);\n }\n \n /* Allocate a contiguous range of virtual swap slots */\n-static swp_entry_t vswap_alloc(int order)\n+static swp_entry_t vswap_alloc(struct folio *folio)\n {\n \tstruct xa_limit limit = vswap_cluster_map_limit;\n \tstruct vswap_cluster *local, *cluster;\n-\tint nr = 1 << order;\n+\tint order = folio_order(folio), nr = 1 << order;\n \tbool need_caching = true;\n \tu32 cluster_id;\n \tswp_entry_t entry;\n@@ -325,7 +354,7 @@ static swp_entry_t vswap_alloc(int order)\n \tcluster = this_cpu_read(percpu_vswap_cluster.clusters[order]);\n \tif (cluster) {\n \t\tspin_lock(&cluster->lock);\n-\t\tentry.val = vswap_alloc_from_cluster(cluster);\n+\t\tentry.val = vswap_alloc_from_cluster(cluster, folio);\n \t\tneed_caching = !entry.val;\n \n \t\tif (!entry.val || !cluster_is_alloc_candidate(cluster)) {\n@@ -352,7 +381,7 @@ static swp_entry_t vswap_alloc(int order)\n \t\t\tif (!spin_trylock(&cluster->lock))\n \t\t\t\tcontinue;\n \n-\t\t\tentry.val = vswap_alloc_from_cluster(cluster);\n+\t\t\tentry.val = vswap_alloc_from_cluster(cluster, folio);\n \t\t\tlist_del_init(&cluster->list);\n \t\t\tcluster->full = !entry.val || !cluster_is_alloc_candidate(cluster);\n \t\t\tneed_caching = !cluster->full;\n@@ -384,7 +413,7 @@ static swp_entry_t vswap_alloc(int order)\n \t\t\t\tif (!cluster_id)\n \t\t\t\t\tentry.val += nr;\n \t\t\t\t__vswap_alloc_from_cluster(cluster,\n-\t\t\t\t\t(entry.val & VSWAP_CLUSTER_MASK));\n+\t\t\t\t\t(entry.val & VSWAP_CLUSTER_MASK), folio);\n \t\t\t\t/* Mark the allocated range in the bitmap */\n \t\t\t\tbitmap_set(cluster->bitmap, (entry.val & VSWAP_CLUSTER_MASK), nr);\n \t\t\t\tneed_caching = cluster_is_alloc_candidate(cluster);\n@@ -497,6 +526,84 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\t__swap_table_set(ci, ci_off + i, vswap ? vswap + i : 0);\n }\n \n+/*\n+ * Caller needs to handle races with other operations themselves.\n+ *\n+ * Specifically, this function is safe to be called in contexts where the swap\n+ * entry has been added to the swap cache and the associated folio is locked.\n+ * We cannot race with other accessors, and the swap entry is guaranteed to be\n+ * valid the whole time (since swap cache implies one refcount).\n+ *\n+ * We cannot assume that the backends will be of the same type,\n+ * contiguous, etc. We might have a large folio coalesced from subpages with\n+ * mixed backend, which is only rectified when it is reclaimed.\n+ */\n+ static void release_backing(swp_entry_t entry, int nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tunsigned long flush_nr, phys_swap_start = 0, phys_swap_end = 0;\n+\tunsigned int phys_swap_type = 0;\n+\tbool need_flushing_phys_swap = false;\n+\tswp_slot_t flush_slot;\n+\tint i;\n+\n+\tVM_WARN_ON(!entry.val);\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\n+\t\t/*\n+\t\t * We batch contiguous physical swap slots for more efficient\n+\t\t * freeing.\n+\t\t */\n+\t\tif (phys_swap_start != phys_swap_end &&\n+\t\t\t\t(desc->type != VSWAP_SWAPFILE ||\n+\t\t\t\t\tswp_slot_type(desc->slot) != phys_swap_type ||\n+\t\t\t\t\tswp_slot_offset(desc->slot) != phys_swap_end)) {\n+\t\t\tneed_flushing_phys_swap = true;\n+\t\t\tflush_slot = swp_slot(phys_swap_type, phys_swap_start);\n+\t\t\tflush_nr = phys_swap_end - phys_swap_start;\n+\t\t\tphys_swap_start = phys_swap_end = 0;\n+\t\t}\n+\n+\t\tif (desc->type == VSWAP_ZSWAP && desc->zswap_entry) {\n+\t\t\tzswap_entry_free(desc->zswap_entry);\n+\t\t} else if (desc->type == VSWAP_SWAPFILE) {\n+\t\t\tif (!phys_swap_start) {\n+\t\t\t\t/* start a new contiguous range of phys swap */\n+\t\t\t\tphys_swap_start = swp_slot_offset(desc->slot);\n+\t\t\t\tphys_swap_end = phys_swap_start + 1;\n+\t\t\t\tphys_swap_type = swp_slot_type(desc->slot);\n+\t\t\t} else {\n+\t\t\t\t/* extend the current contiguous range of phys swap */\n+\t\t\t\tphys_swap_end++;\n+\t\t\t}\n+\t\t}\n+\n+\t\tdesc->slot.val = 0;\n+\n+\t\tif (need_flushing_phys_swap) {\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t\tcluster = NULL;\n+\t\t\tswap_slot_free_nr(flush_slot, flush_nr);\n+\t\t\tneed_flushing_phys_swap = false;\n+\t\t}\n+\t}\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\t/* Flush any remaining physical swap range */\n+\tif (phys_swap_start) {\n+\t\tflush_slot = swp_slot(phys_swap_type, phys_swap_start);\n+\t\tflush_nr = phys_swap_end - phys_swap_start;\n+\t\tswap_slot_free_nr(flush_slot, flush_nr);\n+\t}\n+ }\n+\n /*\n  * Entered with the cluster locked, but might unlock the cluster.\n  * This is because several operations, such as releasing physical swap slots\n@@ -516,35 +623,21 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n static void vswap_free(struct vswap_cluster *cluster, struct swp_desc *desc,\n \tswp_entry_t entry)\n {\n-\tstruct zswap_entry *zswap_entry;\n-\tswp_slot_t slot;\n-\n \t/* Clear shadow if present */\n \tif (xa_is_value(desc->shadow))\n \t\tdesc->shadow = NULL;\n-\n-\tslot = desc->slot;\n-\tdesc->slot.val = 0;\n-\n-\tzswap_entry = desc->zswap_entry;\n-\tif (zswap_entry) {\n-\t\tdesc->zswap_entry = NULL;\n-\t\tzswap_entry_free(zswap_entry);\n-\t}\n \tspin_unlock(&cluster->lock);\n \n+\trelease_backing(entry, 1);\n \tmem_cgroup_uncharge_swap(entry, 1);\n \n-\tif (slot.val)\n-\t\tswap_slot_free_nr(slot, 1);\n-\n-\tspin_lock(&cluster->lock);\n \t/* erase forward mapping and release the virtual slot for reallocation */\n+\tspin_lock(&cluster->lock);\n \trelease_vswap_slot(cluster, entry.val);\n }\n \n /**\n- * folio_alloc_swap - allocate swap space for a folio.\n+ * folio_alloc_swap - allocate virtual swap space for a folio.\n  * @folio: the folio.\n  *\n  * Return: 0, if the allocation succeeded, -ENOMEM, if the allocation failed.\n@@ -552,38 +645,77 @@ static void vswap_free(struct vswap_cluster *cluster, struct swp_desc *desc,\n int folio_alloc_swap(struct folio *folio)\n {\n \tstruct vswap_cluster *cluster = NULL;\n-\tstruct swap_info_struct *si;\n-\tstruct swap_cluster_info *ci;\n-\tint i, nr = folio_nr_pages(folio), order = folio_order(folio);\n+\tint i, nr = folio_nr_pages(folio);\n \tstruct swp_desc *desc;\n \tswp_entry_t entry;\n-\tswp_slot_t slot;\n \n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n \tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n \n-\tentry = vswap_alloc(folio_order(folio));\n+\tentry = vswap_alloc(folio);\n \tif (!entry.val)\n \t\treturn -ENOMEM;\n \n \t/*\n-\t * XXX: for now, we always allocate a physical swap slot for each virtual\n-\t * swap slot, and their lifetime are coupled. This will change once we\n-\t * decouple virtual swap slots from their backing states, and only allocate\n-\t * physical swap slots for them on demand (i.e on zswap writeback, or\n-\t * fallback from zswap store failure).\n+\t * XXX: for now, we charge towards the memory cgroup's swap limit on virtual\n+\t * swap slots allocation. This will be changed soon - we will only charge on\n+\t * physical swap slots allocation.\n \t */\n-\tif (swap_slot_alloc(&slot, order)) {\n+\tif (mem_cgroup_try_charge_swap(folio, entry)) {\n+\t\trcu_read_lock();\n \t\tfor (i = 0; i < nr; i++) {\n \t\t\tdesc = vswap_iter(&cluster, entry.val + i);\n \t\t\tVM_WARN_ON(!desc);\n \t\t\tvswap_free(cluster, desc, (swp_entry_t){ entry.val + i });\n \t\t}\n \t\tspin_unlock(&cluster->lock);\n+\t\trcu_read_unlock();\n+\t\tatomic_add(nr, &vswap_alloc_reject);\n \t\tentry.val = 0;\n \t\treturn -ENOMEM;\n \t}\n \n+\tswap_cache_add_folio(folio, entry, NULL);\n+\n+\treturn 0;\n+}\n+\n+/**\n+ * vswap_alloc_swap_slot - allocate physical swap space for a folio that is\n+ *                         already associated with virtual swap slots.\n+ * @folio: folio we want to allocate physical swap space for.\n+ *\n+ * Note that this does NOT release existing swap backends of the folio.\n+ * Callers need to handle this themselves.\n+\n+ * Return: true if the folio is now backed by physical swap slots, false\n+ * otherwise.\n+ */\n+bool vswap_alloc_swap_slot(struct folio *folio)\n+{\n+\tint i, nr = folio_nr_pages(folio);\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swap_info_struct *si;\n+\tstruct swap_cluster_info *ci;\n+\tswp_slot_t slot = { .val = 0 };\n+\tswp_entry_t entry = folio->swap;\n+\tstruct swp_desc *desc;\n+\tbool fallback = false;\n+\n+\t/*\n+\t * We might have already allocated a backing physical swap slot in past\n+\t * attempts (for instance, when we disable zswap). If the entire range is\n+\t * already swapfile-backed we can skip swapfile case.\n+\t */\n+\tif (vswap_swapfile_backed(entry, nr))\n+\t\treturn true;\n+\n+\tif (swap_slot_alloc(&slot, folio_order(folio)))\n+\t\treturn false;\n+\n+\tif (!slot.val)\n+\t\treturn false;\n+\n \t/* establish the vrtual <-> physical swap slots linkages. */\n \tsi = __swap_slot_to_info(slot);\n \tci = swap_cluster_lock(si, swp_slot_offset(slot));\n@@ -595,29 +727,29 @@ int folio_alloc_swap(struct folio *folio)\n \t\tdesc = vswap_iter(&cluster, entry.val + i);\n \t\tVM_WARN_ON(!desc);\n \n+\t\tif (desc->type == VSWAP_FOLIO) {\n+\t\t\t/* case 1: fallback from zswap store failure */\n+\t\t\tfallback = true;\n+\t\t\tif (!folio)\n+\t\t\t\tfolio = desc->swap_cache;\n+\t\t\telse\n+\t\t\t\tVM_WARN_ON(folio != desc->swap_cache);\n+\t\t} else {\n+\t\t\t/*\n+\t\t\t * Case 2: zswap writeback.\n+\t\t\t *\n+\t\t\t * No need to free zswap entry here - it will be freed once zswap\n+\t\t\t * writeback suceeds.\n+\t\t\t */\n+\t\t\tVM_WARN_ON(desc->type != VSWAP_ZSWAP);\n+\t\t\tVM_WARN_ON(fallback);\n+\t\t}\n+\t\tdesc->type = VSWAP_SWAPFILE;\n \t\tdesc->slot.val = slot.val + i;\n \t}\n-\tif (cluster)\n-\t\tspin_unlock(&cluster->lock);\n+\tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n-\n-\t/*\n-\t * XXX: for now, we charge towards the memory cgroup's swap limit on virtual\n-\t * swap slots allocation. This is acceptable because as noted above, each\n-\t * virtual swap slot corresponds to a physical swap slot. Once we have\n-\t * decoupled virtual and physical swap slots, we will only charge when we\n-\t * actually allocate a physical swap slot.\n-\t */\n-\tif (mem_cgroup_try_charge_swap(folio, entry))\n-\t\tgoto out_free;\n-\n-\tswap_cache_add_folio(folio, entry, NULL);\n-\n-\treturn 0;\n-\n-out_free:\n-\tput_swap_folio(folio, entry);\n-\treturn -ENOMEM;\n+\treturn true;\n }\n \n /**\n@@ -625,7 +757,9 @@ int folio_alloc_swap(struct folio *folio)\n  *                         virtual swap slot.\n  * @entry: the virtual swap slot.\n  *\n- * Return: the physical swap slot corresponding to the virtual swap slot.\n+ * Return: the physical swap slot corresponding to the virtual swap slot, if\n+ * exists, or the zero physical swap slot if the virtual swap slot is not\n+ * backed by any physical slot on a swapfile.\n  */\n swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry)\n {\n@@ -644,7 +778,10 @@ swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry)\n \t\treturn (swp_slot_t){0};\n \t}\n \n-\tslot = desc->slot;\n+\tif (desc->type != VSWAP_SWAPFILE)\n+\t\tslot.val = 0;\n+\telse\n+\t\tslot = desc->slot;\n \tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n \n@@ -962,6 +1099,293 @@ int non_swapcache_batch(swp_entry_t entry, int max_nr)\n \treturn i;\n }\n \n+/**\n+ * vswap_store_folio - set a folio as the backing of a range of virtual swap\n+ *                     slots.\n+ * @entry: the first virtual swap slot in the range.\n+ * @folio: the folio.\n+ */\n+void vswap_store_folio(swp_entry_t entry, struct folio *folio)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tint i, nr = folio_nr_pages(folio);\n+\tstruct swp_desc *desc;\n+\n+\tVM_BUG_ON(!folio_test_locked(folio));\n+\tVM_BUG_ON(folio->swap.val != entry.val);\n+\n+\trelease_backing(entry, nr);\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\t\tdesc->type = VSWAP_FOLIO;\n+\t\tdesc->swap_cache = folio;\n+\t}\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+/**\n+ * swap_zeromap_folio_set - mark a range of virtual swap slots corresponding to\n+ *                          a folio as zero-filled.\n+ * @folio: the folio\n+ */\n+void swap_zeromap_folio_set(struct folio *folio)\n+{\n+\tstruct obj_cgroup *objcg = get_obj_cgroup_from_folio(folio);\n+\tstruct vswap_cluster *cluster = NULL;\n+\tswp_entry_t entry = folio->swap;\n+\tint i, nr = folio_nr_pages(folio);\n+\tstruct swp_desc *desc;\n+\n+\tVM_BUG_ON(!folio_test_locked(folio));\n+\tVM_BUG_ON(!entry.val);\n+\n+\trelease_backing(entry, nr);\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\t\tdesc->type = VSWAP_ZERO;\n+\t}\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\tcount_vm_events(SWPOUT_ZERO, nr);\n+\tif (objcg) {\n+\t\tcount_objcg_events(objcg, SWPOUT_ZERO, nr);\n+\t\tobj_cgroup_put(objcg);\n+\t}\n+}\n+\n+/*\n+ * Iterate through the entire range of virtual swap slots, returning the\n+ * longest contiguous range of slots starting from the first slot that satisfies:\n+ *\n+ * 1. If the first slot is zero-mapped, the entire range should be\n+ *    zero-mapped.\n+ * 2. If the first slot is backed by a swapfile, the entire range should\n+ *    be backed by a range of contiguous swap slots on the same swapfile.\n+ * 3. If the first slot is zswap-backed, the entire range should be\n+ *    zswap-backed.\n+ * 4. If the first slot is backed by a folio, the entire range should\n+ *    be backed by the same folio.\n+ *\n+ * Note that this check is racy unless we can ensure that the entire range\n+ * has their backing state stable - for instance, if the caller was the one\n+ * who set the swap cache pin.\n+ */\n+static int vswap_check_backing(swp_entry_t entry, enum swap_type *type, int nr)\n+{\n+\tunsigned int swapfile_type;\n+\tstruct vswap_cluster *cluster = NULL;\n+\tenum swap_type first_type;\n+\tstruct swp_desc *desc;\n+\tpgoff_t first_offset;\n+\tstruct folio *folio;\n+\tint i = 0;\n+\n+\tif (!entry.val)\n+\t\treturn 0;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tif (!desc)\n+\t\t\tgoto done;\n+\n+\t\tif (!i) {\n+\t\t\tfirst_type = desc->type;\n+\t\t\tif (first_type == VSWAP_SWAPFILE) {\n+\t\t\t\tswapfile_type = swp_slot_type(desc->slot);\n+\t\t\t\tfirst_offset = swp_slot_offset(desc->slot);\n+\t\t\t} else if (first_type == VSWAP_FOLIO) {\n+\t\t\t\tfolio = desc->swap_cache;\n+\t\t\t}\n+\t\t} else if (desc->type != first_type) {\n+\t\t\tgoto done;\n+\t\t} else if (first_type == VSWAP_SWAPFILE &&\n+\t\t\t\t(swp_slot_type(desc->slot) != swapfile_type ||\n+\t\t\t\t\tswp_slot_offset(desc->slot) != first_offset + i)) {\n+\t\t\tgoto done;\n+\t\t} else if (first_type == VSWAP_FOLIO && desc->swap_cache != folio) {\n+\t\t\tgoto done;\n+\t\t}\n+\t}\n+done:\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\tif (type)\n+\t\t*type = first_type;\n+\treturn i;\n+}\n+\n+/**\n+ * vswap_swapfile_backed - check if the virtual swap slots are backed by physical\n+ *                         swap slots.\n+ * @entry: the first entry in the range.\n+ * @nr: the number of entries in the range.\n+ */\n+bool vswap_swapfile_backed(swp_entry_t entry, int nr)\n+{\n+\tenum swap_type type;\n+\n+\treturn vswap_check_backing(entry, &type, nr) == nr\n+\t\t\t\t&& type == VSWAP_SWAPFILE;\n+}\n+\n+/**\n+ * vswap_folio_backed - check if the virtual swap slots are backed by in-memory\n+ *                      pages.\n+ * @entry: the first virtual swap slot in the range.\n+ * @nr: the number of slots in the range.\n+ */\n+bool vswap_folio_backed(swp_entry_t entry, int nr)\n+{\n+\tenum swap_type type;\n+\n+\treturn vswap_check_backing(entry, &type, nr) == nr && type == VSWAP_FOLIO;\n+}\n+\n+/**\n+ * vswap_can_swapin_thp - check if the swap entries can be swapped in as a THP.\n+ * @entry: the first virtual swap slot in the range.\n+ * @nr: the number of slots in the range.\n+ *\n+ * For now, we can only swap in a THP if the entire range is zero-filled, or if\n+ * the entire range is backed by a contiguous range of physical swap slots on a\n+ * swapfile.\n+ */\n+bool vswap_can_swapin_thp(swp_entry_t entry, int nr)\n+{\n+\tenum swap_type type;\n+\n+\treturn vswap_check_backing(entry, &type, nr) == nr &&\n+\t\t(type == VSWAP_ZERO || type == VSWAP_SWAPFILE);\n+}\n+\n+/**\n+ * swap_move - increment the swap slot by delta, checking the backing state and\n+ *             return 0 if the backing state does not match (i.e wrong backing\n+ *             state type, or wrong offset on the backing stores).\n+ * @entry: the original virtual swap slot.\n+ * @delta: the offset to increment the original slot.\n+ *\n+ * Note that this function is racy unless we can pin the backing state of these\n+ * swap slots down with swapcache_prepare().\n+ *\n+ * Caller should only rely on this function as a best-effort hint otherwise,\n+ * and should double-check after ensuring the whole range is pinned down.\n+ *\n+ * Return: the incremented virtual swap slot if the backing state matches, or\n+ *         0 if the backing state does not match.\n+ */\n+swp_entry_t swap_move(swp_entry_t entry, long delta)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc, *next_desc;\n+\tswp_entry_t next_entry;\n+\tstruct folio *folio = NULL, *next_folio = NULL;\n+\tenum swap_type type, next_type;\n+\tswp_slot_t slot = {0}, next_slot = {0};\n+\n+\tnext_entry.val = entry.val + delta;\n+\n+\trcu_read_lock();\n+\n+\t/* Look up first descriptor and get its type and backing store */\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn (swp_entry_t){0};\n+\t}\n+\n+\ttype = desc->type;\n+\tif (type == VSWAP_ZSWAP) {\n+\t\t/* zswap not supported for move */\n+\t\tspin_unlock(&cluster->lock);\n+\t\trcu_read_unlock();\n+\t\treturn (swp_entry_t){0};\n+\t}\n+\tif (type == VSWAP_FOLIO)\n+\t\tfolio = desc->swap_cache;\n+\telse if (type == VSWAP_SWAPFILE)\n+\t\tslot = desc->slot;\n+\n+\t/* Look up second descriptor and get its type and backing store */\n+\tnext_desc = vswap_iter(&cluster, next_entry.val);\n+\tif (!next_desc) {\n+\t\trcu_read_unlock();\n+\t\treturn (swp_entry_t){0};\n+\t}\n+\n+\tnext_type = next_desc->type;\n+\tif (next_type == VSWAP_FOLIO)\n+\t\tnext_folio = next_desc->swap_cache;\n+\telse if (next_type == VSWAP_SWAPFILE)\n+\t\tnext_slot = next_desc->slot;\n+\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\n+\trcu_read_unlock();\n+\n+\t/* Check if types match */\n+\tif (next_type != type)\n+\t\treturn (swp_entry_t){0};\n+\n+\t/* Check backing state consistency */\n+\tif (type == VSWAP_SWAPFILE &&\n+\t\t\t(swp_slot_type(next_slot) != swp_slot_type(slot) ||\n+\t\t\t\tswp_slot_offset(next_slot) !=\n+\t\t\t\t\t\t\tswp_slot_offset(slot) + delta))\n+\t\treturn (swp_entry_t){0};\n+\n+\tif (type == VSWAP_FOLIO && next_folio != folio)\n+\t\treturn (swp_entry_t){0};\n+\n+\treturn next_entry;\n+}\n+\n+/*\n+ * Return the count of contiguous swap entries that share the same\n+ * VSWAP_ZERO status as the starting entry. If is_zeromap is not NULL,\n+ * it will return the VSWAP_ZERO status of the starting entry.\n+ */\n+int swap_zeromap_batch(swp_entry_t entry, int max_nr, bool *is_zeromap)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tint i = 0;\n+\tbool is_zero = false;\n+\n+\tVM_WARN_ON(!entry.val);\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < max_nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tif (!desc)\n+\t\t\tgoto done;\n+\n+\t\tif (!i)\n+\t\t\tis_zero = (desc->type == VSWAP_ZERO);\n+\t\telse if ((desc->type == VSWAP_ZERO) != is_zero)\n+\t\t\tgoto done;\n+\t}\n+done:\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\tif (i && is_zeromap)\n+\t\t*is_zeromap = is_zero;\n+\n+\treturn i;\n+}\n+\n /**\n  * free_swap_and_cache_nr() - Release a swap count on range of swap entries and\n  *                            reclaim their cache if no more references remain.\n@@ -1028,11 +1452,6 @@ bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si)\n \tstruct vswap_cluster *cluster;\n \tswp_slot_t slot;\n \n-\tslot = swp_entry_to_swp_slot(entry);\n-\t*si = swap_slot_tryget_swap_info(slot);\n-\tif (!*si)\n-\t\treturn false;\n-\n \t/*\n \t * Ensure the cluster and its associated data structures (swap cache etc.)\n \t * remain valid.\n@@ -1041,11 +1460,30 @@ bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si)\n \tcluster = xa_load(&vswap_cluster_map, VSWAP_CLUSTER_IDX(entry));\n \tif (!cluster || !refcount_inc_not_zero(&cluster->refcnt)) {\n \t\trcu_read_unlock();\n-\t\tswap_slot_put_swap_info(*si);\n \t\t*si = NULL;\n \t\treturn false;\n \t}\n \trcu_read_unlock();\n+\n+\tslot = swp_entry_to_swp_slot(entry);\n+\t/*\n+\t * Note that this function does not provide any guarantee that the virtual\n+\t * swap slot's backing state will be stable. This has several implications:\n+\t *\n+\t * 1. We have to obtain a reference to the swap device itself, because we\n+\t * need swap device's metadata in certain scenarios, for example when we\n+\t * need to inspect the swap device flag in do_swap_page().\n+\t *\n+\t * 2. The swap device we are looking up here might be outdated by the time we\n+\t * return to the caller. It is perfectly OK, if the swap_info_struct is only\n+\t * used in a best-effort manner (i.e optimization). If we need the precise\n+\t * backing state, we need to re-check after the entry is pinned in swapcache.\n+\t */\n+\tif (slot.val)\n+\t\t*si = swap_slot_tryget_swap_info(slot);\n+\telse\n+\t\t*si = NULL;\n+\n \treturn true;\n }\n \n@@ -1288,7 +1726,7 @@ void swap_cache_add_folio(struct folio *folio, swp_entry_t entry, void **shadowp\n \t\told = desc->shadow;\n \n \t\t/* Warn if slot is already occupied by a folio */\n-\t\tVM_WARN_ON_FOLIO(old && !xa_is_value(old), folio);\n+\t\tVM_WARN_ON_FOLIO(old && !xa_is_value(old) && old != folio, folio);\n \n \t\t/* Save shadow if found and not yet saved */\n \t\tif (shadowp && xa_is_value(old) && !*shadowp)\n@@ -1415,29 +1853,22 @@ void __swap_cache_replace_folio(struct folio *old, struct folio *new)\n  * @entry: the zswap entry to store\n  *\n  * Stores a zswap entry in the swap descriptor for the given swap entry.\n- * The cluster is locked during the store operation.\n- *\n- * Return: the old zswap entry if one existed, NULL otherwise\n+ * Releases the old backend if one existed.\n  */\n-void *zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry)\n+void zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry)\n {\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n-\tvoid *old;\n+\n+\trelease_backing(swpentry, 1);\n \n \trcu_read_lock();\n \tdesc = vswap_iter(&cluster, swpentry.val);\n-\tif (!desc) {\n-\t\trcu_read_unlock();\n-\t\treturn NULL;\n-\t}\n-\n-\told = desc->zswap_entry;\n+\tVM_WARN_ON(!desc);\n \tdesc->zswap_entry = entry;\n+\tdesc->type = VSWAP_ZSWAP;\n \tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n-\n-\treturn old;\n }\n \n /**\n@@ -1452,6 +1883,7 @@ void *zswap_entry_load(swp_entry_t swpentry)\n {\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n+\tenum swap_type type;\n \tvoid *zswap_entry;\n \n \trcu_read_lock();\n@@ -1461,41 +1893,15 @@ void *zswap_entry_load(swp_entry_t swpentry)\n \t\treturn NULL;\n \t}\n \n+\ttype = desc->type;\n \tzswap_entry = desc->zswap_entry;\n \tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n \n-\treturn zswap_entry;\n-}\n-\n-/**\n- * zswap_entry_erase - erase a zswap entry for a swap entry\n- * @swpentry: the swap entry\n- *\n- * Erases the zswap entry from the swap descriptor for the given swap entry.\n- * The cluster is locked during the erase operation.\n- *\n- * Return: the zswap entry that was erased, NULL if none existed\n- */\n-void *zswap_entry_erase(swp_entry_t swpentry)\n-{\n-\tstruct vswap_cluster *cluster = NULL;\n-\tstruct swp_desc *desc;\n-\tvoid *old;\n-\n-\trcu_read_lock();\n-\tdesc = vswap_iter(&cluster, swpentry.val);\n-\tif (!desc) {\n-\t\trcu_read_unlock();\n+\tif (type != VSWAP_ZSWAP)\n \t\treturn NULL;\n-\t}\n \n-\told = desc->zswap_entry;\n-\tdesc->zswap_entry = NULL;\n-\tspin_unlock(&cluster->lock);\n-\trcu_read_unlock();\n-\n-\treturn old;\n+\treturn zswap_entry;\n }\n \n bool zswap_empty(swp_entry_t swpentry)\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex e46349f9c90bb..c5e1d252cb463 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -991,8 +991,9 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n {\n \tstruct folio *folio;\n \tstruct mempolicy *mpol;\n-\tbool folio_was_allocated;\n+\tbool folio_was_allocated, phys_swap_alloced = false;\n \tstruct swap_info_struct *si;\n+\tstruct zswap_entry *new_entry = NULL;\n \tint ret = 0;\n \n \t/* try to allocate swap cache folio */\n@@ -1027,18 +1028,23 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \t * old compressed data. Only when this is successful can the entry\n \t * be dereferenced.\n \t */\n-\tif (entry != zswap_entry_load(swpentry)) {\n+\tnew_entry = zswap_entry_load(swpentry);\n+\tif (entry != new_entry) {\n \t\tret = -ENOMEM;\n \t\tgoto out;\n \t}\n \n+\tif (!vswap_alloc_swap_slot(folio)) {\n+\t\tret = -ENOMEM;\n+\t\tgoto out;\n+\t}\n+\tphys_swap_alloced = true;\n+\n \tif (!zswap_decompress(entry, folio)) {\n \t\tret = -EIO;\n \t\tgoto out;\n \t}\n \n-\tzswap_entry_erase(swpentry);\n-\n \tcount_vm_event(ZSWPWB);\n \tif (entry->objcg)\n \t\tcount_objcg_events(entry->objcg, ZSWPWB, 1);\n@@ -1056,6 +1062,8 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \n out:\n \tif (ret && ret != -EEXIST) {\n+\t\tif (phys_swap_alloced)\n+\t\t\tzswap_entry_store(swpentry, new_entry);\n \t\tswap_cache_del_folio(folio);\n \t\tfolio_unlock(folio);\n \t}\n@@ -1401,7 +1409,7 @@ static bool zswap_store_page(struct page *page,\n \t\t\t     struct zswap_pool *pool)\n {\n \tswp_entry_t page_swpentry = page_swap_entry(page);\n-\tstruct zswap_entry *entry, *old;\n+\tstruct zswap_entry *entry;\n \n \t/* allocate entry */\n \tentry = zswap_entry_cache_alloc(GFP_KERNEL, page_to_nid(page));\n@@ -1413,15 +1421,12 @@ static bool zswap_store_page(struct page *page,\n \tif (!zswap_compress(page, entry, pool))\n \t\tgoto compress_failed;\n \n-\told = zswap_entry_store(page_swpentry, entry);\n-\n \t/*\n \t * We may have had an existing entry that became stale when\n \t * the folio was redirtied and now the new version is being\n-\t * swapped out. Get rid of the old.\n+\t * swapped out. zswap_entry_store() will get rid of the old.\n \t */\n-\tif (old)\n-\t\tzswap_entry_free(old);\n+\tzswap_entry_store(page_swpentry, entry);\n \n \t/*\n \t * The entry is successfully compressed and stored in the tree, there is\n@@ -1533,18 +1538,13 @@ bool zswap_store(struct folio *folio)\n \t * the possibly stale entries which were previously stored at the\n \t * offsets corresponding to each page of the folio. Otherwise,\n \t * writeback could overwrite the new data in the swapfile.\n+\t *\n+\t * The only exception is if we still have a full contiguous\n+\t * range of physical swap slots backing the folio. Keep them for\n+\t * fallback disk swapping.\n \t */\n-\tif (!ret) {\n-\t\tunsigned type = swp_type(swp);\n-\t\tpgoff_t offset = swp_offset(swp);\n-\t\tstruct zswap_entry *entry;\n-\n-\t\tfor (index = 0; index < nr_pages; ++index) {\n-\t\t\tentry = zswap_entry_erase(swp_entry(type, offset + index));\n-\t\t\tif (entry)\n-\t\t\t\tzswap_entry_free(entry);\n-\t\t}\n-\t}\n+\tif (!ret && !vswap_swapfile_backed(swp, nr_pages))\n+\t\tvswap_store_folio(swp, folio);\n \n \treturn ret;\n }\n@@ -1619,8 +1619,7 @@ int zswap_load(struct folio *folio)\n \t */\n \tif (swapcache) {\n \t\tfolio_mark_dirty(folio);\n-\t\tzswap_entry_erase(swp);\n-\t\tzswap_entry_free(entry);\n+\t\tvswap_store_folio(swp, folio);\n \t}\n \n \tfolio_unlock(folio);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the zswap shrinker being started even when there is no physical swap available, explaining that this is because virtualized swap does not pre-allocate slots on the swap file for zswap entries.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When swap is virtualized, we no longer pre-allocate a slot on swapfile\nfor each zswap entry. Do not start the zswap shrinker if there is no\nphysical swap slots available.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n mm/zswap.c | 8 ++++++++\n 1 file changed, 8 insertions(+)\n\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex c5e1d252cb463..9d1822753d321 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -1211,6 +1211,14 @@ static unsigned long zswap_shrinker_count(struct shrinker *shrinker,\n \tif (!zswap_shrinker_enabled || !mem_cgroup_zswap_writeback_enabled(memcg))\n \t\treturn 0;\n \n+\t/*\n+\t * When swap is virtualized, we do not have any swap slots on swapfile\n+\t * preallocated for zswap objects. If there is no slot available, we\n+\t * cannot writeback and should just bail out here.\n+\t */\n+\tif (!get_nr_swap_pages())\n+\t\treturn 0;\n+\n \t/*\n \t * The shrinker resumes swap writeback, which will enter block\n \t * and may enter fs. XXX: Harmonize with vmscan.c __GFP_FS\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the swapoff path needing to drop the per-vswap spinlock before calling try_to_unmap(). They acknowledged that this is necessary and added a check in vswap_same_cluster() to skip pinning entries in the same virtual swap cluster as the target entry, which already has a reference.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "added check"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When we perform swap readahead, the target entry is already pinned by\nthe caller. No need to pin swap entries in the readahead window that\nbelongs in the same virtual swap cluster as the target swap entry.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n mm/swap.h       |  1 +\n mm/swap_state.c | 22 +++++++++-------------\n mm/vswap.c      | 10 ++++++++++\n 3 files changed, 20 insertions(+), 13 deletions(-)\n\ndiff --git a/mm/swap.h b/mm/swap.h\nindex d41e6a0e70753..08a6369a6dfad 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -213,6 +213,7 @@ void swap_cache_lock(swp_entry_t entry);\n void swap_cache_unlock(swp_entry_t entry);\n void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\t\t   unsigned long vswap, int nr);\n+bool vswap_same_cluster(swp_entry_t entry1, swp_entry_t entry2);\n \n static inline struct address_space *swap_address_space(swp_entry_t entry)\n {\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex ad80bf098b63f..e8e0905c7723f 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -553,22 +553,18 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \t\tpte_unmap(pte);\n \t\tpte = NULL;\n \t\t/*\n-\t\t * Readahead entry may come from a device that we are not\n-\t\t * holding a reference to, try to grab a reference, or skip.\n-\t\t *\n-\t\t * XXX: for now, always try to pin the swap entries in the\n-\t\t * readahead window to avoid the annoying conversion to physical\n-\t\t * swap slots. Once we move all swap metadata to virtual swap\n-\t\t * layer, we can simply compare the clusters of the target\n-\t\t * swap entry and the current swap entry, and pin the latter\n-\t\t * swap entry's cluster if it differ from the former's.\n+\t\t * The target entry is already pinned - if the readahead entry\n+\t\t * belongs to the same cluster, it's already protected.\n \t\t */\n-\t\tswapoff_locked = tryget_swap_entry(entry, &si);\n-\t\tif (!swapoff_locked)\n-\t\t\tcontinue;\n+\t\tif (!vswap_same_cluster(entry, targ_entry)) {\n+\t\t\tswapoff_locked = tryget_swap_entry(entry, &si);\n+\t\t\tif (!swapoff_locked)\n+\t\t\t\tcontinue;\n+\t\t}\n \t\tfolio = __read_swap_cache_async(entry, gfp_mask, mpol, ilx,\n \t\t\t\t\t\t&page_allocated, false);\n-\t\tput_swap_entry(entry, si);\n+\t\tif (swapoff_locked)\n+\t\t\tput_swap_entry(entry, si);\n \t\tif (!folio)\n \t\t\tcontinue;\n \t\tif (page_allocated) {\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex fb6179ce3ace7..7563107eb8eee 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -1503,6 +1503,16 @@ void put_swap_entry(swp_entry_t entry, struct swap_info_struct *si)\n \trcu_read_unlock();\n }\n \n+/*\n+ * Check if two virtual swap entries belong to the same vswap cluster.\n+ * Useful for optimizing readahead when entries in the same cluster\n+ * share protection from a pinned target entry.\n+ */\n+bool vswap_same_cluster(swp_entry_t entry1, swp_entry_t entry2)\n+{\n+\treturn VSWAP_CLUSTER_IDX(entry1) == VSWAP_CLUSTER_IDX(entry2);\n+}\n+\n static int vswap_cpu_dead(unsigned int cpu)\n {\n \tstruct percpu_vswap_cluster *percpu_cluster;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the memory overhead of the zeromap bitmap, explained that removing it does not change behavior and saves memory, and confirmed that this patch is just a cleanup.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "confirmed approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zero swap entries are now treated as a separate, decoupled backend in\nthe virtual swap layer. The zeromap bitmap of physical swapfile is no\nlonger used - remove it. This does not have any behavioral change, and\nsave 1 bit per swap page in terms of memory overhead.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h |  1 -\n mm/swapfile.c        | 30 +++++-------------------------\n 2 files changed, 5 insertions(+), 26 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 54df972608047..9cd45eab313f8 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -260,7 +260,6 @@ struct swap_info_struct {\n \tsigned char\ttype;\t\t/* strange name for an index */\n \tunsigned int\tmax;\t\t/* extent of the swap_map */\n \tunsigned char *swap_map;\t/* vmalloc'ed array of usage counts */\n-\tunsigned long *zeromap;\t\t/* kvmalloc'ed bitmap to track zero pages */\n \tstruct swap_cluster_info *cluster_info; /* cluster info. Only for SSD */\n \tstruct list_head free_clusters; /* free clusters list */\n \tstruct list_head full_clusters; /* full clusters list */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 1aa29dd220f9a..e1cb01b821ff3 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -2317,8 +2317,7 @@ static int setup_swap_extents(struct swap_info_struct *sis, sector_t *span)\n \n static void setup_swap_info(struct swap_info_struct *si, int prio,\n \t\t\t    unsigned char *swap_map,\n-\t\t\t    struct swap_cluster_info *cluster_info,\n-\t\t\t    unsigned long *zeromap)\n+\t\t\t    struct swap_cluster_info *cluster_info)\n {\n \tsi->prio = prio;\n \t/*\n@@ -2329,7 +2328,6 @@ static void setup_swap_info(struct swap_info_struct *si, int prio,\n \tsi->avail_list.prio = -si->prio;\n \tsi->swap_map = swap_map;\n \tsi->cluster_info = cluster_info;\n-\tsi->zeromap = zeromap;\n }\n \n static void _enable_swap_info(struct swap_info_struct *si)\n@@ -2347,12 +2345,11 @@ static void _enable_swap_info(struct swap_info_struct *si)\n \n static void enable_swap_info(struct swap_info_struct *si, int prio,\n \t\t\t\tunsigned char *swap_map,\n-\t\t\t\tstruct swap_cluster_info *cluster_info,\n-\t\t\t\tunsigned long *zeromap)\n+\t\t\t\tstruct swap_cluster_info *cluster_info)\n {\n \tspin_lock(&swap_lock);\n \tspin_lock(&si->lock);\n-\tsetup_swap_info(si, prio, swap_map, cluster_info, zeromap);\n+\tsetup_swap_info(si, prio, swap_map, cluster_info);\n \tspin_unlock(&si->lock);\n \tspin_unlock(&swap_lock);\n \t/*\n@@ -2370,7 +2367,7 @@ static void reinsert_swap_info(struct swap_info_struct *si)\n {\n \tspin_lock(&swap_lock);\n \tspin_lock(&si->lock);\n-\tsetup_swap_info(si, si->prio, si->swap_map, si->cluster_info, si->zeromap);\n+\tsetup_swap_info(si, si->prio, si->swap_map, si->cluster_info);\n \t_enable_swap_info(si);\n \tspin_unlock(&si->lock);\n \tspin_unlock(&swap_lock);\n@@ -2441,7 +2438,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n \tunsigned char *swap_map;\n-\tunsigned long *zeromap;\n \tstruct swap_cluster_info *cluster_info;\n \tstruct file *swap_file, *victim;\n \tstruct address_space *mapping;\n@@ -2536,8 +2532,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tp->swap_file = NULL;\n \tswap_map = p->swap_map;\n \tp->swap_map = NULL;\n-\tzeromap = p->zeromap;\n-\tp->zeromap = NULL;\n \tmaxpages = p->max;\n \tcluster_info = p->cluster_info;\n \tp->max = 0;\n@@ -2549,7 +2543,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tkfree(p->global_cluster);\n \tp->global_cluster = NULL;\n \tvfree(swap_map);\n-\tkvfree(zeromap);\n \tfree_cluster_info(cluster_info, maxpages);\n \n \tinode = mapping->host;\n@@ -3013,7 +3006,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tsector_t span;\n \tunsigned long maxpages;\n \tunsigned char *swap_map = NULL;\n-\tunsigned long *zeromap = NULL;\n \tstruct swap_cluster_info *cluster_info = NULL;\n \tstruct folio *folio = NULL;\n \tstruct inode *inode = NULL;\n@@ -3119,17 +3111,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tif (error)\n \t\tgoto bad_swap_unlock_inode;\n \n-\t/*\n-\t * Use kvmalloc_array instead of bitmap_zalloc as the allocation order might\n-\t * be above MAX_PAGE_ORDER incase of a large swap file.\n-\t */\n-\tzeromap = kvmalloc_array(BITS_TO_LONGS(maxpages), sizeof(long),\n-\t\t\t\t    GFP_KERNEL | __GFP_ZERO);\n-\tif (!zeromap) {\n-\t\terror = -ENOMEM;\n-\t\tgoto bad_swap_unlock_inode;\n-\t}\n-\n \tif (si->bdev && bdev_stable_writes(si->bdev))\n \t\tsi->flags |= SWP_STABLE_WRITES;\n \n@@ -3196,7 +3177,7 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tprio = DEF_SWAP_PRIO;\n \tif (swap_flags & SWAP_FLAG_PREFER)\n \t\tprio = swap_flags & SWAP_FLAG_PRIO_MASK;\n-\tenable_swap_info(si, prio, swap_map, cluster_info, zeromap);\n+\tenable_swap_info(si, prio, swap_map, cluster_info);\n \n \tpr_info(\"Adding %uk swap on %s.  Priority:%d extents:%d across:%lluk %s%s%s%s\\n\",\n \t\tK(si->pages), name->name, si->prio, nr_extents,\n@@ -3224,7 +3205,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tsi->flags = 0;\n \tspin_unlock(&swap_lock);\n \tvfree(swap_map);\n-\tkvfree(zeromap);\n \tif (cluster_info)\n \t\tfree_cluster_info(cluster_info, maxpages);\n \tif (inced_nr_rotate_swap)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about charging physical swap space for zswap and zero-filled swap pages, agreeing to only record the memcg id on virtual swap slot allocation and defer physical swap charging until the virtual swap slot is backed by an actual physical swap slot. The author confirmed that this change will be included in v2 of the patch.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed to address a concern",
                "confirmed changes for v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Now that zswap and the zero-filled swap page optimization no longer\ntakes up any physical swap space, we should not charge towards the swap\nusage and limits of the memcg in these case. We will only record the\nmemcg id on virtual swap slot allocation, and defer physical swap\ncharging (i.e towards memory.swap.current) until the virtual swap slot\nis backed by an actual physical swap slot (on zswap store failure\nfallback or zswap writeback).\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h | 16 +++++++++\n mm/memcontrol-v1.c   |  6 ++++\n mm/memcontrol.c      | 83 ++++++++++++++++++++++++++++++++------------\n mm/vswap.c           | 39 +++++++++------------\n 4 files changed, 98 insertions(+), 46 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 9cd45eab313f8..a30d382fb5ee1 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -613,6 +613,22 @@ static inline void folio_throttle_swaprate(struct folio *folio, gfp_t gfp)\n #endif\n \n #if defined(CONFIG_MEMCG) && defined(CONFIG_SWAP)\n+void __mem_cgroup_record_swap(struct folio *folio, swp_entry_t entry);\n+static inline void mem_cgroup_record_swap(struct folio *folio,\n+\t\tswp_entry_t entry)\n+{\n+\tif (!mem_cgroup_disabled())\n+\t\t__mem_cgroup_record_swap(folio, entry);\n+}\n+\n+void __mem_cgroup_clear_swap(swp_entry_t entry, unsigned int nr_pages);\n+static inline void mem_cgroup_clear_swap(swp_entry_t entry,\n+\t\tunsigned int nr_pages)\n+{\n+\tif (!mem_cgroup_disabled())\n+\t\t__mem_cgroup_clear_swap(entry, nr_pages);\n+}\n+\n int __mem_cgroup_try_charge_swap(struct folio *folio, swp_entry_t entry);\n static inline int mem_cgroup_try_charge_swap(struct folio *folio,\n \t\tswp_entry_t entry)\ndiff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.c\nindex 6eed14bff7426..4580a034dcf72 100644\n--- a/mm/memcontrol-v1.c\n+++ b/mm/memcontrol-v1.c\n@@ -680,6 +680,12 @@ void memcg1_swapin(swp_entry_t entry, unsigned int nr_pages)\n \t\t * memory+swap charge, drop the swap entry duplicate.\n \t\t */\n \t\tmem_cgroup_uncharge_swap(entry, nr_pages);\n+\n+\t\t/*\n+\t\t * Clear the cgroup association now to prevent double memsw\n+\t\t * uncharging when the backends are released later.\n+\t\t */\n+\t\tmem_cgroup_clear_swap(entry, nr_pages);\n \t}\n }\n \ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 2ba5811e7edba..50be8066bebec 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -5172,6 +5172,49 @@ int __init mem_cgroup_init(void)\n }\n \n #ifdef CONFIG_SWAP\n+/**\n+ * __mem_cgroup_record_swap - record the folio's cgroup for the swap entries.\n+ * @folio: folio being swapped out.\n+ * @entry: the first swap entry in the range.\n+ */\n+void __mem_cgroup_record_swap(struct folio *folio, swp_entry_t entry)\n+{\n+\tunsigned int nr_pages = folio_nr_pages(folio);\n+\tstruct mem_cgroup *memcg;\n+\n+\t/* Recording will be done by memcg1_swapout(). */\n+\tif (do_memsw_account())\n+\t\treturn;\n+\n+\tmemcg = folio_memcg(folio);\n+\n+\tVM_WARN_ON_ONCE_FOLIO(!memcg, folio);\n+\tif (!memcg)\n+\t\treturn;\n+\n+\tmemcg = mem_cgroup_id_get_online(memcg);\n+\tif (nr_pages > 1)\n+\t\tmem_cgroup_id_get_many(memcg, nr_pages - 1);\n+\tswap_cgroup_record(folio, mem_cgroup_id(memcg), entry);\n+}\n+\n+/**\n+ * __mem_cgroup_clear_swap - clear cgroup information of the swap entries.\n+ * @folio: folio being swapped out.\n+ * @entry: the first swap entry in the range.\n+ */\n+void __mem_cgroup_clear_swap(swp_entry_t entry, unsigned int nr_pages)\n+{\n+\tunsigned short id = swap_cgroup_clear(entry, nr_pages);\n+\tstruct mem_cgroup *memcg;\n+\n+\trcu_read_lock();\n+\tmemcg = mem_cgroup_from_id(id);\n+\tif (memcg)\n+\t\tmem_cgroup_id_put_many(memcg, nr_pages);\n+\trcu_read_unlock();\n+}\n+\n /**\n  * __mem_cgroup_try_charge_swap - try charging swap space for a folio\n  * @folio: folio being added to swap\n@@ -5190,34 +5233,24 @@ int __mem_cgroup_try_charge_swap(struct folio *folio, swp_entry_t entry)\n \tif (do_memsw_account())\n \t\treturn 0;\n \n-\tmemcg = folio_memcg(folio);\n-\n-\tVM_WARN_ON_ONCE_FOLIO(!memcg, folio);\n-\tif (!memcg)\n-\t\treturn 0;\n-\n-\tif (!entry.val) {\n-\t\tmemcg_memory_event(memcg, MEMCG_SWAP_FAIL);\n-\t\treturn 0;\n-\t}\n-\n-\tmemcg = mem_cgroup_id_get_online(memcg);\n+\t/*\n+\t * We already record the cgroup on virtual swap allocation.\n+\t * Note that the virtual swap slot holds a reference to memcg,\n+\t * so this lookup should be safe.\n+\t */\n+\trcu_read_lock();\n+\tmemcg = mem_cgroup_from_id(lookup_swap_cgroup_id(entry));\n+\trcu_read_unlock();\n \n \tif (!mem_cgroup_is_root(memcg) &&\n \t    !page_counter_try_charge(&memcg->swap, nr_pages, &counter)) {\n \t\tmemcg_memory_event(memcg, MEMCG_SWAP_MAX);\n \t\tmemcg_memory_event(memcg, MEMCG_SWAP_FAIL);\n-\t\tmem_cgroup_id_put(memcg);\n \t\treturn -ENOMEM;\n \t}\n \n-\t/* Get references for the tail pages, too */\n-\tif (nr_pages > 1)\n-\t\tmem_cgroup_id_get_many(memcg, nr_pages - 1);\n \tmod_memcg_state(memcg, MEMCG_SWAP, nr_pages);\n \n-\tswap_cgroup_record(folio, mem_cgroup_id(memcg), entry);\n-\n \treturn 0;\n }\n \n@@ -5231,7 +5264,8 @@ void __mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages)\n \tstruct mem_cgroup *memcg;\n \tunsigned short id;\n \n-\tid = swap_cgroup_clear(entry, nr_pages);\n+\tid = lookup_swap_cgroup_id(entry);\n+\n \trcu_read_lock();\n \tmemcg = mem_cgroup_from_id(id);\n \tif (memcg) {\n@@ -5242,7 +5276,6 @@ void __mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages)\n \t\t\t\tpage_counter_uncharge(&memcg->swap, nr_pages);\n \t\t}\n \t\tmod_memcg_state(memcg, MEMCG_SWAP, -nr_pages);\n-\t\tmem_cgroup_id_put_many(memcg, nr_pages);\n \t}\n \trcu_read_unlock();\n }\n@@ -5251,14 +5284,18 @@ static bool mem_cgroup_may_zswap(struct mem_cgroup *original_memcg);\n \n long mem_cgroup_get_nr_swap_pages(struct mem_cgroup *memcg)\n {\n-\tlong nr_swap_pages, nr_zswap_pages = 0;\n+\tlong nr_swap_pages;\n \n \tif (zswap_is_enabled() && (mem_cgroup_disabled() || do_memsw_account() ||\n \t\t\t\tmem_cgroup_may_zswap(memcg))) {\n-\t\tnr_zswap_pages = PAGE_COUNTER_MAX;\n+\t\t/*\n+\t\t * No need to check swap cgroup limits, since zswap is not charged\n+\t\t * towards swap consumption.\n+\t\t */\n+\t\treturn PAGE_COUNTER_MAX;\n \t}\n \n-\tnr_swap_pages = max_t(long, nr_zswap_pages, get_nr_swap_pages());\n+\tnr_swap_pages = get_nr_swap_pages();\n \tif (mem_cgroup_disabled() || do_memsw_account())\n \t\treturn nr_swap_pages;\n \tfor (; !mem_cgroup_is_root(memcg); memcg = parent_mem_cgroup(memcg))\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex 7563107eb8eee..2a071d5ae173c 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -543,6 +543,7 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n \tunsigned long flush_nr, phys_swap_start = 0, phys_swap_end = 0;\n+\tunsigned long phys_swap_released = 0;\n \tunsigned int phys_swap_type = 0;\n \tbool need_flushing_phys_swap = false;\n \tswp_slot_t flush_slot;\n@@ -572,6 +573,7 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\tif (desc->type == VSWAP_ZSWAP && desc->zswap_entry) {\n \t\t\tzswap_entry_free(desc->zswap_entry);\n \t\t} else if (desc->type == VSWAP_SWAPFILE) {\n+\t\t\tphys_swap_released++;\n \t\t\tif (!phys_swap_start) {\n \t\t\t\t/* start a new contiguous range of phys swap */\n \t\t\t\tphys_swap_start = swp_slot_offset(desc->slot);\n@@ -602,6 +604,9 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\tflush_nr = phys_swap_end - phys_swap_start;\n \t\tswap_slot_free_nr(flush_slot, flush_nr);\n \t}\n+\n+\tif (phys_swap_released)\n+\t\tmem_cgroup_uncharge_swap(entry, phys_swap_released);\n  }\n \n /*\n@@ -629,7 +634,7 @@ static void vswap_free(struct vswap_cluster *cluster, struct swp_desc *desc,\n \tspin_unlock(&cluster->lock);\n \n \trelease_backing(entry, 1);\n-\tmem_cgroup_uncharge_swap(entry, 1);\n+\tmem_cgroup_clear_swap(entry, 1);\n \n \t/* erase forward mapping and release the virtual slot for reallocation */\n \tspin_lock(&cluster->lock);\n@@ -644,9 +649,6 @@ static void vswap_free(struct vswap_cluster *cluster, struct swp_desc *desc,\n  */\n int folio_alloc_swap(struct folio *folio)\n {\n-\tstruct vswap_cluster *cluster = NULL;\n-\tint i, nr = folio_nr_pages(folio);\n-\tstruct swp_desc *desc;\n \tswp_entry_t entry;\n \n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n@@ -656,25 +658,7 @@ int folio_alloc_swap(struct folio *folio)\n \tif (!entry.val)\n \t\treturn -ENOMEM;\n \n-\t/*\n-\t * XXX: for now, we charge towards the memory cgroup's swap limit on virtual\n-\t * swap slots allocation. This will be changed soon - we will only charge on\n-\t * physical swap slots allocation.\n-\t */\n-\tif (mem_cgroup_try_charge_swap(folio, entry)) {\n-\t\trcu_read_lock();\n-\t\tfor (i = 0; i < nr; i++) {\n-\t\t\tdesc = vswap_iter(&cluster, entry.val + i);\n-\t\t\tVM_WARN_ON(!desc);\n-\t\t\tvswap_free(cluster, desc, (swp_entry_t){ entry.val + i });\n-\t\t}\n-\t\tspin_unlock(&cluster->lock);\n-\t\trcu_read_unlock();\n-\t\tatomic_add(nr, &vswap_alloc_reject);\n-\t\tentry.val = 0;\n-\t\treturn -ENOMEM;\n-\t}\n-\n+\tmem_cgroup_record_swap(folio, entry);\n \tswap_cache_add_folio(folio, entry, NULL);\n \n \treturn 0;\n@@ -716,6 +700,15 @@ bool vswap_alloc_swap_slot(struct folio *folio)\n \tif (!slot.val)\n \t\treturn false;\n \n+\tif (mem_cgroup_try_charge_swap(folio, entry)) {\n+\t\t/*\n+\t\t * We have not updated the backing type of the virtual swap slot.\n+\t\t * Simply free up the physical swap slots here!\n+\t\t */\n+\t\tswap_slot_free_nr(slot, nr);\n+\t\treturn false;\n+\t}\n+\n \t/* establish the vrtual <-> physical swap slots linkages. */\n \tsi = __swap_slot_to_info(slot);\n \tci = swap_cluster_lock(si, swp_slot_offset(slot));\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the swapoff path needing to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, and provided benchmark results showing improved performance.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch presents the second applications of virtual swap design -\nsimplifying and optimizing swapoff.\n\nWith virtual swap slots stored at page table entries and used as indices\nto various swap-related data structures, we no longer have to perform a\npage table walk in swapoff. Simply iterate through all the allocated\nswap slots on the swapfile, find their corresponding virtual swap slots,\nand fault them in.\n\nThis is significantly cleaner, as well as slightly more performant,\nespecially when there are a lot of unrelated VMAs (since the old swapoff\ncode would have to traverse through all of them).\n\nIn a simple benchmark, in which we swapoff a 32 GB swapfile that is 50%\nfull, and in which there is a process that maps a 128GB file into\nmemory:\n\nBaseline:\nsys: 11.48s\n\nNew Design:\nsys: 9.96s\n\nDisregarding the real time reduction (which is mostly due to more IO\nasynchrony), the new design reduces the kernel CPU time by about 13%.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/shmem_fs.h |   7 +-\n mm/shmem.c               | 184 +--------------\n mm/swapfile.c            | 474 +++++++++------------------------------\n 3 files changed, 113 insertions(+), 552 deletions(-)\n\ndiff --git a/include/linux/shmem_fs.h b/include/linux/shmem_fs.h\nindex e2069b3179c41..bac6b6cafe89c 100644\n--- a/include/linux/shmem_fs.h\n+++ b/include/linux/shmem_fs.h\n@@ -41,17 +41,13 @@ struct shmem_inode_info {\n \tunsigned long\t\tswapped;\t/* subtotal assigned to swap */\n \tunion {\n \t    struct offset_ctx\tdir_offsets;\t/* stable directory offsets */\n-\t    struct {\n-\t\tstruct list_head shrinklist;\t/* shrinkable hpage inodes */\n-\t\tstruct list_head swaplist;\t/* chain of maybes on swap */\n-\t    };\n+\t    struct list_head\tshrinklist;\t/* shrinkable hpage inodes */\n \t};\n \tstruct timespec64\ti_crtime;\t/* file creation time */\n \tstruct shared_policy\tpolicy;\t\t/* NUMA memory alloc policy */\n \tstruct simple_xattrs\txattrs;\t\t/* list of xattrs */\n \tpgoff_t\t\t\tfallocend;\t/* highest fallocate endindex */\n \tunsigned int\t\tfsflags;\t/* for FS_IOC_[SG]ETFLAGS */\n-\tatomic_t\t\tstop_eviction;\t/* hold when working on inode */\n #ifdef CONFIG_TMPFS_QUOTA\n \tstruct dquot __rcu\t*i_dquot[MAXQUOTAS];\n #endif\n@@ -127,7 +123,6 @@ struct page *shmem_read_mapping_page_gfp(struct address_space *mapping,\n int shmem_writeout(struct folio *folio, struct swap_iocb **plug,\n \t\tstruct list_head *folio_list);\n void shmem_truncate_range(struct inode *inode, loff_t start, uoff_t end);\n-int shmem_unuse(unsigned int type);\n \n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n unsigned long shmem_allowable_huge_orders(struct inode *inode,\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 3a346cca114ab..61790752bdf6d 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -290,9 +290,6 @@ bool vma_is_shmem(const struct vm_area_struct *vma)\n \treturn vma_is_anon_shmem(vma) || vma->vm_ops == &shmem_vm_ops;\n }\n \n-static LIST_HEAD(shmem_swaplist);\n-static DEFINE_SPINLOCK(shmem_swaplist_lock);\n-\n #ifdef CONFIG_TMPFS_QUOTA\n \n static int shmem_enable_quotas(struct super_block *sb,\n@@ -1413,16 +1410,6 @@ static void shmem_evict_inode(struct inode *inode)\n \t\t\t}\n \t\t\tspin_unlock(&sbinfo->shrinklist_lock);\n \t\t}\n-\t\twhile (!list_empty(&info->swaplist)) {\n-\t\t\t/* Wait while shmem_unuse() is scanning this inode... */\n-\t\t\twait_var_event(&info->stop_eviction,\n-\t\t\t\t       !atomic_read(&info->stop_eviction));\n-\t\t\tspin_lock(&shmem_swaplist_lock);\n-\t\t\t/* ...but beware of the race if we peeked too early */\n-\t\t\tif (!atomic_read(&info->stop_eviction))\n-\t\t\t\tlist_del_init(&info->swaplist);\n-\t\t\tspin_unlock(&shmem_swaplist_lock);\n-\t\t}\n \t}\n \n \tsimple_xattrs_free(&info->xattrs, sbinfo->max_inodes ? &freed : NULL);\n@@ -1435,153 +1422,6 @@ static void shmem_evict_inode(struct inode *inode)\n #endif\n }\n \n-static unsigned int shmem_find_swap_entries(struct address_space *mapping,\n-\t\t\t\tpgoff_t start, struct folio_batch *fbatch,\n-\t\t\t\tpgoff_t *indices, unsigned int type)\n-{\n-\tXA_STATE(xas, &mapping->i_pages, start);\n-\tstruct folio *folio;\n-\tswp_entry_t entry;\n-\tswp_slot_t slot;\n-\n-\trcu_read_lock();\n-\txas_for_each(&xas, folio, ULONG_MAX) {\n-\t\tif (xas_retry(&xas, folio))\n-\t\t\tcontinue;\n-\n-\t\tif (!xa_is_value(folio))\n-\t\t\tcontinue;\n-\n-\t\tentry = radix_to_swp_entry(folio);\n-\t\tslot = swp_entry_to_swp_slot(entry);\n-\n-\t\t/*\n-\t\t * swapin error entries can be found in the mapping. But they're\n-\t\t * deliberately ignored here as we've done everything we can do.\n-\t\t */\n-\t\tif (!slot.val || swp_slot_type(slot) != type)\n-\t\t\tcontinue;\n-\n-\t\tindices[folio_batch_count(fbatch)] = xas.xa_index;\n-\t\tif (!folio_batch_add(fbatch, folio))\n-\t\t\tbreak;\n-\n-\t\tif (need_resched()) {\n-\t\t\txas_pause(&xas);\n-\t\t\tcond_resched_rcu();\n-\t\t}\n-\t}\n-\trcu_read_unlock();\n-\n-\treturn folio_batch_count(fbatch);\n-}\n-\n-/*\n- * Move the swapped pages for an inode to page cache. Returns the count\n- * of pages swapped in, or the error in case of failure.\n- */\n-static int shmem_unuse_swap_entries(struct inode *inode,\n-\t\tstruct folio_batch *fbatch, pgoff_t *indices)\n-{\n-\tint i = 0;\n-\tint ret = 0;\n-\tint error = 0;\n-\tstruct address_space *mapping = inode->i_mapping;\n-\n-\tfor (i = 0; i < folio_batch_count(fbatch); i++) {\n-\t\tstruct folio *folio = fbatch->folios[i];\n-\n-\t\terror = shmem_swapin_folio(inode, indices[i], &folio, SGP_CACHE,\n-\t\t\t\t\tmapping_gfp_mask(mapping), NULL, NULL);\n-\t\tif (error == 0) {\n-\t\t\tfolio_unlock(folio);\n-\t\t\tfolio_put(folio);\n-\t\t\tret++;\n-\t\t}\n-\t\tif (error == -ENOMEM)\n-\t\t\tbreak;\n-\t\terror = 0;\n-\t}\n-\treturn error ? error : ret;\n-}\n-\n-/*\n- * If swap found in inode, free it and move page from swapcache to filecache.\n- */\n-static int shmem_unuse_inode(struct inode *inode, unsigned int type)\n-{\n-\tstruct address_space *mapping = inode->i_mapping;\n-\tpgoff_t start = 0;\n-\tstruct folio_batch fbatch;\n-\tpgoff_t indices[PAGEVEC_SIZE];\n-\tint ret = 0;\n-\n-\tdo {\n-\t\tfolio_batch_init(&fbatch);\n-\t\tif (!shmem_find_swap_entries(mapping, start, &fbatch,\n-\t\t\t\t\t     indices, type)) {\n-\t\t\tret = 0;\n-\t\t\tbreak;\n-\t\t}\n-\n-\t\tret = shmem_unuse_swap_entries(inode, &fbatch, indices);\n-\t\tif (ret < 0)\n-\t\t\tbreak;\n-\n-\t\tstart = indices[folio_batch_count(&fbatch) - 1];\n-\t} while (true);\n-\n-\treturn ret;\n-}\n-\n-/*\n- * Read all the shared memory data that resides in the swap\n- * device 'type' back into memory, so the swap device can be\n- * unused.\n- */\n-int shmem_unuse(unsigned int type)\n-{\n-\tstruct shmem_inode_info *info, *next;\n-\tint error = 0;\n-\n-\tif (list_empty(&shmem_swaplist))\n-\t\treturn 0;\n-\n-\tspin_lock(&shmem_swaplist_lock);\n-start_over:\n-\tlist_for_each_entry_safe(info, next, &shmem_swaplist, swaplist) {\n-\t\tif (!info->swapped) {\n-\t\t\tlist_del_init(&info->swaplist);\n-\t\t\tcontinue;\n-\t\t}\n-\t\t/*\n-\t\t * Drop the swaplist mutex while searching the inode for swap;\n-\t\t * but before doing so, make sure shmem_evict_inode() will not\n-\t\t * remove placeholder inode from swaplist, nor let it be freed\n-\t\t * (igrab() would protect from unlink, but not from unmount).\n-\t\t */\n-\t\tatomic_inc(&info->stop_eviction);\n-\t\tspin_unlock(&shmem_swaplist_lock);\n-\n-\t\terror = shmem_unuse_inode(&info->vfs_inode, type);\n-\t\tcond_resched();\n-\n-\t\tspin_lock(&shmem_swaplist_lock);\n-\t\tif (atomic_dec_and_test(&info->stop_eviction))\n-\t\t\twake_up_var(&info->stop_eviction);\n-\t\tif (error)\n-\t\t\tbreak;\n-\t\tif (list_empty(&info->swaplist))\n-\t\t\tgoto start_over;\n-\t\tnext = list_next_entry(info, swaplist);\n-\t\tif (!info->swapped)\n-\t\t\tlist_del_init(&info->swaplist);\n-\t}\n-\tspin_unlock(&shmem_swaplist_lock);\n-\n-\treturn error;\n-}\n-\n /**\n  * shmem_writeout - Write the folio to swap\n  * @folio: The folio to write\n@@ -1668,24 +1508,9 @@ int shmem_writeout(struct folio *folio, struct swap_iocb **plug,\n \t}\n \n \tif (!folio_alloc_swap(folio)) {\n-\t\tbool first_swapped = shmem_recalc_inode(inode, 0, nr_pages);\n \t\tint error;\n \n-\t\t/*\n-\t\t * Add inode to shmem_unuse()'s list of swapped-out inodes,\n-\t\t * if it's not already there.  Do it now before the folio is\n-\t\t * removed from page cache, when its pagelock no longer\n-\t\t * protects the inode from eviction.  And do it now, after\n-\t\t * we've incremented swapped, because shmem_unuse() will\n-\t\t * prune a !swapped inode from the swaplist.\n-\t\t */\n-\t\tif (first_swapped) {\n-\t\t\tspin_lock(&shmem_swaplist_lock);\n-\t\t\tif (list_empty(&info->swaplist))\n-\t\t\t\tlist_add(&info->swaplist, &shmem_swaplist);\n-\t\t\tspin_unlock(&shmem_swaplist_lock);\n-\t\t}\n-\n+\t\tshmem_recalc_inode(inode, 0, nr_pages);\n \t\tswap_shmem_alloc(folio->swap, nr_pages);\n \t\tshmem_delete_from_page_cache(folio, swp_to_radix_entry(folio->swap));\n \n@@ -3106,7 +2931,6 @@ static struct inode *__shmem_get_inode(struct mnt_idmap *idmap,\n \tinfo = SHMEM_I(inode);\n \tmemset(info, 0, (char *)inode - (char *)info);\n \tspin_lock_init(&info->lock);\n-\tatomic_set(&info->stop_eviction, 0);\n \tinfo->seals = F_SEAL_SEAL;\n \tinfo->flags = (flags & VM_NORESERVE) ? SHMEM_F_NORESERVE : 0;\n \tinfo->i_crtime = inode_get_mtime(inode);\n@@ -3115,7 +2939,6 @@ static struct inode *__shmem_get_inode(struct mnt_idmap *idmap,\n \tif (info->fsflags)\n \t\tshmem_set_inode_flags(inode, info->fsflags, NULL);\n \tINIT_LIST_HEAD(&info->shrinklist);\n-\tINIT_LIST_HEAD(&info->swaplist);\n \tsimple_xattrs_init(&info->xattrs);\n \tcache_no_acl(inode);\n \tif (sbinfo->noswap)\n@@ -5785,11 +5608,6 @@ void __init shmem_init(void)\n \tBUG_ON(IS_ERR(shm_mnt));\n }\n \n-int shmem_unuse(unsigned int type)\n-{\n-\treturn 0;\n-}\n-\n int shmem_lock(struct file *file, int lock, struct ucounts *ucounts)\n {\n \treturn 0;\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex e1cb01b821ff3..9478707ce3ffa 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1738,300 +1738,12 @@ unsigned int count_swap_pages(int type, int free)\n }\n #endif /* CONFIG_HIBERNATION */\n \n-static inline int pte_same_as_swp(pte_t pte, pte_t swp_pte)\n+static bool swap_slot_allocated(struct swap_info_struct *si,\n+\t\tunsigned long offset)\n {\n-\treturn pte_same(pte_swp_clear_flags(pte), swp_pte);\n-}\n-\n-/*\n- * No need to decide whether this PTE shares the swap entry with others,\n- * just let do_wp_page work it out if a write is requested later - to\n- * force COW, vm_page_prot omits write permission from any private vma.\n- */\n-static int unuse_pte(struct vm_area_struct *vma, pmd_t *pmd,\n-\t\tunsigned long addr, swp_entry_t entry, struct folio *folio)\n-{\n-\tstruct page *page;\n-\tstruct folio *swapcache;\n-\tspinlock_t *ptl;\n-\tpte_t *pte, new_pte, old_pte;\n-\tbool hwpoisoned = false;\n-\tint ret = 1;\n-\n-\t/*\n-\t * If the folio is removed from swap cache by others, continue to\n-\t * unuse other PTEs. try_to_unuse may try again if we missed this one.\n-\t */\n-\tif (!folio_matches_swap_entry(folio, entry))\n-\t\treturn 0;\n-\n-\tswapcache = folio;\n-\tfolio = ksm_might_need_to_copy(folio, vma, addr);\n-\tif (unlikely(!folio))\n-\t\treturn -ENOMEM;\n-\telse if (unlikely(folio == ERR_PTR(-EHWPOISON))) {\n-\t\thwpoisoned = true;\n-\t\tfolio = swapcache;\n-\t}\n-\n-\tpage = folio_file_page(folio, swp_offset(entry));\n-\tif (PageHWPoison(page))\n-\t\thwpoisoned = true;\n-\n-\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n-\tif (unlikely(!pte || !pte_same_as_swp(ptep_get(pte),\n-\t\t\t\t\t\tswp_entry_to_pte(entry)))) {\n-\t\tret = 0;\n-\t\tgoto out;\n-\t}\n-\n-\told_pte = ptep_get(pte);\n-\n-\tif (unlikely(hwpoisoned || !folio_test_uptodate(folio))) {\n-\t\tswp_entry_t swp_entry;\n-\n-\t\tdec_mm_counter(vma->vm_mm, MM_SWAPENTS);\n-\t\tif (hwpoisoned) {\n-\t\t\tswp_entry = make_hwpoison_entry(page);\n-\t\t} else {\n-\t\t\tswp_entry = make_poisoned_swp_entry();\n-\t\t}\n-\t\tnew_pte = swp_entry_to_pte(swp_entry);\n-\t\tret = 0;\n-\t\tgoto setpte;\n-\t}\n-\n-\t/*\n-\t * Some architectures may have to restore extra metadata to the page\n-\t * when reading from swap. This metadata may be indexed by swap entry\n-\t * so this must be called before swap_free().\n-\t */\n-\tarch_swap_restore(folio_swap(entry, folio), folio);\n-\n-\tdec_mm_counter(vma->vm_mm, MM_SWAPENTS);\n-\tinc_mm_counter(vma->vm_mm, MM_ANONPAGES);\n-\tfolio_get(folio);\n-\tif (folio == swapcache) {\n-\t\trmap_t rmap_flags = RMAP_NONE;\n-\n-\t\t/*\n-\t\t * See do_swap_page(): writeback would be problematic.\n-\t\t * However, we do a folio_wait_writeback() just before this\n-\t\t * call and have the folio locked.\n-\t\t */\n-\t\tVM_BUG_ON_FOLIO(folio_test_writeback(folio), folio);\n-\t\tif (pte_swp_exclusive(old_pte))\n-\t\t\trmap_flags |= RMAP_EXCLUSIVE;\n-\t\t/*\n-\t\t * We currently only expect small !anon folios, which are either\n-\t\t * fully exclusive or fully shared. If we ever get large folios\n-\t\t * here, we have to be careful.\n-\t\t */\n-\t\tif (!folio_test_anon(folio)) {\n-\t\t\tVM_WARN_ON_ONCE(folio_test_large(folio));\n-\t\t\tVM_WARN_ON_FOLIO(!folio_test_locked(folio), folio);\n-\t\t\tfolio_add_new_anon_rmap(folio, vma, addr, rmap_flags);\n-\t\t} else {\n-\t\t\tfolio_add_anon_rmap_pte(folio, page, vma, addr, rmap_flags);\n-\t\t}\n-\t} else { /* ksm created a completely new copy */\n-\t\tfolio_add_new_anon_rmap(folio, vma, addr, RMAP_EXCLUSIVE);\n-\t\tfolio_add_lru_vma(folio, vma);\n-\t}\n-\tnew_pte = pte_mkold(mk_pte(page, vma->vm_page_prot));\n-\tif (pte_swp_soft_dirty(old_pte))\n-\t\tnew_pte = pte_mksoft_dirty(new_pte);\n-\tif (pte_swp_uffd_wp(old_pte))\n-\t\tnew_pte = pte_mkuffd_wp(new_pte);\n-setpte:\n-\tset_pte_at(vma->vm_mm, addr, pte, new_pte);\n-\tswap_free(entry);\n-out:\n-\tif (pte)\n-\t\tpte_unmap_unlock(pte, ptl);\n-\tif (folio != swapcache) {\n-\t\tfolio_unlock(folio);\n-\t\tfolio_put(folio);\n-\t}\n-\treturn ret;\n-}\n-\n-static int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,\n-\t\t\tunsigned long addr, unsigned long end,\n-\t\t\tunsigned int type)\n-{\n-\tpte_t *pte = NULL;\n-\tstruct swap_info_struct *si;\n-\n-\tsi = swap_info[type];\n-\tdo {\n-\t\tstruct folio *folio;\n-\t\tunsigned long offset;\n-\t\tunsigned char swp_count;\n-\t\tsoftleaf_t entry;\n-\t\tswp_slot_t slot;\n-\t\tint ret;\n-\t\tpte_t ptent;\n-\n-\t\tif (!pte++) {\n-\t\t\tpte = pte_offset_map(pmd, addr);\n-\t\t\tif (!pte)\n-\t\t\t\tbreak;\n-\t\t}\n-\n-\t\tptent = ptep_get_lockless(pte);\n-\t\tentry = softleaf_from_pte(ptent);\n-\n-\t\tif (!softleaf_is_swap(entry))\n-\t\t\tcontinue;\n-\n-\t\tslot = swp_entry_to_swp_slot(entry);\n-\t\tif (swp_slot_type(slot) != type)\n-\t\t\tcontinue;\n-\n-\t\toffset = swp_slot_offset(slot);\n-\t\tpte_unmap(pte);\n-\t\tpte = NULL;\n-\n-\t\tfolio = swap_cache_get_folio(entry);\n-\t\tif (!folio) {\n-\t\t\tstruct vm_fault vmf = {\n-\t\t\t\t.vma = vma,\n-\t\t\t\t.address = addr,\n-\t\t\t\t.real_address = addr,\n-\t\t\t\t.pmd = pmd,\n-\t\t\t};\n-\n-\t\t\tfolio = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,\n-\t\t\t\t\t\t&vmf);\n-\t\t}\n-\t\tif (!folio) {\n-\t\t\tswp_count = READ_ONCE(si->swap_map[offset]);\n-\t\t\tif (swp_count == 0 || swp_count == SWAP_MAP_BAD)\n-\t\t\t\tcontinue;\n-\t\t\treturn -ENOMEM;\n-\t\t}\n-\n-\t\tfolio_lock(folio);\n-\t\tfolio_wait_writeback(folio);\n-\t\tret = unuse_pte(vma, pmd, addr, entry, folio);\n-\t\tif (ret < 0) {\n-\t\t\tfolio_unlock(folio);\n-\t\t\tfolio_put(folio);\n-\t\t\treturn ret;\n-\t\t}\n-\n-\t\tfolio_free_swap(folio);\n-\t\tfolio_unlock(folio);\n-\t\tfolio_put(folio);\n-\t} while (addr += PAGE_SIZE, addr != end);\n-\n-\tif (pte)\n-\t\tpte_unmap(pte);\n-\treturn 0;\n-}\n-\n-static inline int unuse_pmd_range(struct vm_area_struct *vma, pud_t *pud,\n-\t\t\t\tunsigned long addr, unsigned long end,\n-\t\t\t\tunsigned int type)\n-{\n-\tpmd_t *pmd;\n-\tunsigned long next;\n-\tint ret;\n-\n-\tpmd = pmd_offset(pud, addr);\n-\tdo {\n-\t\tcond_resched();\n-\t\tnext = pmd_addr_end(addr, end);\n-\t\tret = unuse_pte_range(vma, pmd, addr, next, type);\n-\t\tif (ret)\n-\t\t\treturn ret;\n-\t} while (pmd++, addr = next, addr != end);\n-\treturn 0;\n-}\n-\n-static inline int unuse_pud_range(struct vm_area_struct *vma, p4d_t *p4d,\n-\t\t\t\tunsigned long addr, unsigned long end,\n-\t\t\t\tunsigned int type)\n-{\n-\tpud_t *pud;\n-\tunsigned long next;\n-\tint ret;\n-\n-\tpud = pud_offset(p4d, addr);\n-\tdo {\n-\t\tnext = pud_addr_end(addr, end);\n-\t\tif (pud_none_or_clear_bad(pud))\n-\t\t\tcontinue;\n-\t\tret = unuse_pmd_range(vma, pud, addr, next, type);\n-\t\tif (ret)\n-\t\t\treturn ret;\n-\t} while (pud++, addr = next, addr != end);\n-\treturn 0;\n-}\n-\n-static inline int unuse_p4d_range(struct vm_area_struct *vma, pgd_t *pgd,\n-\t\t\t\tunsigned long addr, unsigned long end,\n-\t\t\t\tunsigned int type)\n-{\n-\tp4d_t *p4d;\n-\tunsigned long next;\n-\tint ret;\n-\n-\tp4d = p4d_offset(pgd, addr);\n-\tdo {\n-\t\tnext = p4d_addr_end(addr, end);\n-\t\tif (p4d_none_or_clear_bad(p4d))\n-\t\t\tcontinue;\n-\t\tret = unuse_pud_range(vma, p4d, addr, next, type);\n-\t\tif (ret)\n-\t\t\treturn ret;\n-\t} while (p4d++, addr = next, addr != end);\n-\treturn 0;\n-}\n-\n-static int unuse_vma(struct vm_area_struct *vma, unsigned int type)\n-{\n-\tpgd_t *pgd;\n-\tunsigned long addr, end, next;\n-\tint ret;\n-\n-\taddr = vma->vm_start;\n-\tend = vma->vm_end;\n-\n-\tpgd = pgd_offset(vma->vm_mm, addr);\n-\tdo {\n-\t\tnext = pgd_addr_end(addr, end);\n-\t\tif (pgd_none_or_clear_bad(pgd))\n-\t\t\tcontinue;\n-\t\tret = unuse_p4d_range(vma, pgd, addr, next, type);\n-\t\tif (ret)\n-\t\t\treturn ret;\n-\t} while (pgd++, addr = next, addr != end);\n-\treturn 0;\n-}\n+\tunsigned char count = READ_ONCE(si->swap_map[offset]);\n \n-static int unuse_mm(struct mm_struct *mm, unsigned int type)\n-{\n-\tstruct vm_area_struct *vma;\n-\tint ret = 0;\n-\tVMA_ITERATOR(vmi, mm, 0);\n-\n-\tmmap_read_lock(mm);\n-\tif (check_stable_address_space(mm))\n-\t\tgoto unlock;\n-\tfor_each_vma(vmi, vma) {\n-\t\tif (vma->anon_vma && !is_vm_hugetlb_page(vma)) {\n-\t\t\tret = unuse_vma(vma, type);\n-\t\t\tif (ret)\n-\t\t\t\tbreak;\n-\t\t}\n-\n-\t\tcond_resched();\n-\t}\n-unlock:\n-\tmmap_read_unlock(mm);\n-\treturn ret;\n+\treturn count && swap_count(count) != SWAP_MAP_BAD;\n }\n \n /*\n@@ -2043,7 +1755,6 @@ static unsigned int find_next_to_unuse(struct swap_info_struct *si,\n \t\t\t\t\tunsigned int prev)\n {\n \tunsigned int i;\n-\tunsigned char count;\n \n \t/*\n \t * No need for swap_lock here: we're just looking\n@@ -2052,8 +1763,7 @@ static unsigned int find_next_to_unuse(struct swap_info_struct *si,\n \t * allocations from this area (while holding swap_lock).\n \t */\n \tfor (i = prev + 1; i < si->max; i++) {\n-\t\tcount = READ_ONCE(si->swap_map[i]);\n-\t\tif (count && swap_count(count) != SWAP_MAP_BAD)\n+\t\tif (swap_slot_allocated(si, i))\n \t\t\tbreak;\n \t\tif ((i % LATENCY_LIMIT) == 0)\n \t\t\tcond_resched();\n@@ -2065,101 +1775,139 @@ static unsigned int find_next_to_unuse(struct swap_info_struct *si,\n \treturn i;\n }\n \n+#define\tfor_each_allocated_offset(si, offset)\t\\\n+\twhile (swap_usage_in_pages(si) && \\\n+\t\t!signal_pending(current) && \\\n+\t\t(offset = find_next_to_unuse(si, offset)) != 0)\n+\n+static struct folio *pagein(swp_entry_t entry, struct swap_iocb **splug,\n+\t\tstruct mempolicy *mpol)\n+{\n+\tbool folio_was_allocated;\n+\tstruct folio *folio = __read_swap_cache_async(entry, GFP_KERNEL, mpol,\n+\t\t\tNO_INTERLEAVE_INDEX, &folio_was_allocated, false);\n+\n+\tif (folio_was_allocated)\n+\t\tswap_read_folio(folio, splug);\n+\treturn folio;\n+}\n+\n static int try_to_unuse(unsigned int type)\n {\n-\tstruct mm_struct *prev_mm;\n-\tstruct mm_struct *mm;\n-\tstruct list_head *p;\n-\tint retval = 0;\n \tstruct swap_info_struct *si = swap_info[type];\n+\tstruct swap_iocb *splug = NULL;\n+\tstruct mempolicy *mpol;\n+\tstruct blk_plug plug;\n+\tunsigned long offset;\n \tstruct folio *folio;\n \tswp_entry_t entry;\n \tswp_slot_t slot;\n-\tunsigned int i;\n+\tint ret = 0;\n \n \tif (!swap_usage_in_pages(si))\n \t\tgoto success;\n \n-retry:\n-\tretval = shmem_unuse(type);\n-\tif (retval)\n-\t\treturn retval;\n-\n-\tprev_mm = &init_mm;\n-\tmmget(prev_mm);\n-\n-\tspin_lock(&mmlist_lock);\n-\tp = &init_mm.mmlist;\n-\twhile (swap_usage_in_pages(si) &&\n-\t       !signal_pending(current) &&\n-\t       (p = p->next) != &init_mm.mmlist) {\n+\tmpol = get_task_policy(current);\n+\tblk_start_plug(&plug);\n \n-\t\tmm = list_entry(p, struct mm_struct, mmlist);\n-\t\tif (!mmget_not_zero(mm))\n+\t/* first round - submit the reads */\n+\toffset = 0;\n+\tfor_each_allocated_offset(si, offset) {\n+\t\tslot = swp_slot(type, offset);\n+\t\tentry = swp_slot_to_swp_entry(slot);\n+\t\tif (!entry.val)\n \t\t\tcontinue;\n-\t\tspin_unlock(&mmlist_lock);\n-\t\tmmput(prev_mm);\n-\t\tprev_mm = mm;\n-\t\tretval = unuse_mm(mm, type);\n-\t\tif (retval) {\n-\t\t\tmmput(prev_mm);\n-\t\t\treturn retval;\n-\t\t}\n \n-\t\t/*\n-\t\t * Make sure that we aren't completely killing\n-\t\t * interactive performance.\n-\t\t */\n-\t\tcond_resched();\n-\t\tspin_lock(&mmlist_lock);\n+\t\tfolio = pagein(entry, &splug, mpol);\n+\t\tif (folio)\n+\t\t\tfolio_put(folio);\n \t}\n-\tspin_unlock(&mmlist_lock);\n+\tblk_finish_plug(&plug);\n+\tswap_read_unplug(splug);\n+\tsplug = NULL;\n+\tlru_add_drain();\n+\n+\t/* second round - updating the virtual swap slots' backing state */\n+\toffset = 0;\n+\tfor_each_allocated_offset(si, offset) {\n+\t\tslot = swp_slot(type, offset);\n+retry:\n+\t\tentry = swp_slot_to_swp_entry(slot);\n+\t\tif (!entry.val) {\n+\t\t\tif (!swap_slot_allocated(si, offset))\n+\t\t\t\tcontinue;\n \n-\tmmput(prev_mm);\n+\t\t\tif (signal_pending(current)) {\n+\t\t\t\tret = -EINTR;\n+\t\t\t\tgoto out;\n+\t\t\t}\n \n-\ti = 0;\n-\twhile (swap_usage_in_pages(si) &&\n-\t       !signal_pending(current) &&\n-\t       (i = find_next_to_unuse(si, i)) != 0) {\n+\t\t\t/* we might be racing with zswap writeback or disk swapout */\n+\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\tgoto retry;\n+\t\t}\n \n-\t\tslot = swp_slot(type, i);\n-\t\tentry = swp_slot_to_swp_entry(slot);\n-\t\tfolio = swap_cache_get_folio(entry);\n-\t\tif (!folio)\n-\t\t\tcontinue;\n+\t\t/* try to allocate swap cache folio */\n+\t\tfolio = pagein(entry, &splug, mpol);\n+\t\tif (!folio) {\n+\t\t\tif (!swp_slot_to_swp_entry(swp_slot(type, offset)).val)\n+\t\t\t\tcontinue;\n \n+\t\t\tret = -ENOMEM;\n+\t\t\tpr_err(\"swapoff: unable to allocate swap cache folio for %lu\\n\",\n+\t\t\t\t\t\tentry.val);\n+\t\t\tgoto out;\n+\t\t}\n+\n+\t\tfolio_lock(folio);\n \t\t/*\n-\t\t * It is conceivable that a racing task removed this folio from\n-\t\t * swap cache just before we acquired the page lock. The folio\n-\t\t * might even be back in swap cache on another swap area. But\n-\t\t * that is okay, folio_free_swap() only removes stale folios.\n+\t\t * We need to check if the folio is still in swap cache, and is still\n+\t\t * backed by the physical swap slot we are trying to release.\n+\t\t *\n+\t\t * We can, for instance, race with zswap writeback, obtaining the\n+\t\t * temporary folio it allocated for decompression and writeback, which\n+\t\t * would be promptly deleted from swap cache. By the time we lock that\n+\t\t * folio, it might have already contained stale data.\n+\t\t *\n+\t\t * Concurrent swap operations might have also come in before we\n+\t\t * reobtain the folio's lock, deleting the folio from swap cache,\n+\t\t * invalidating the virtual swap slot, then swapping out the folio\n+\t\t * again to a different swap backends.\n+\t\t *\n+\t\t * In all of these cases, we must retry the physical -> virtual lookup.\n \t\t */\n-\t\tfolio_lock(folio);\n+\t\tif (!folio_matches_swap_slot(folio, entry, slot)) {\n+\t\t\tfolio_unlock(folio);\n+\t\t\tfolio_put(folio);\n+\t\t\tif (signal_pending(current)) {\n+\t\t\t\tret = -EINTR;\n+\t\t\t\tgoto out;\n+\t\t\t}\n+\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\tgoto retry;\n+\t\t}\n+\n \t\tfolio_wait_writeback(folio);\n-\t\tfolio_free_swap(folio);\n+\t\tvswap_store_folio(entry, folio);\n+\t\tfolio_mark_dirty(folio);\n \t\tfolio_unlock(folio);\n \t\tfolio_put(folio);\n \t}\n \n-\t/*\n-\t * Lets check again to see if there are still swap entries in the map.\n-\t * If yes, we would need to do retry the unuse logic again.\n-\t * Under global memory pressure, swap entries can be reinserted back\n-\t * into process space after the mmlist loop above passes over them.\n-\t *\n-\t * Limit the number of retries? No: when mmget_not_zero()\n-\t * above fails, that mm is likely to be freeing swap from\n-\t * exit_mmap(), which proceeds at its own independent pace;\n-\t * and even shmem_writeout() could have been preempted after\n-\t * folio_alloc_swap(), temporarily hiding that swap.  It's easy\n-\t * and robust (though cpu-intensive) just to keep retrying.\n-\t */\n-\tif (swap_usage_in_pages(si)) {\n-\t\tif (!signal_pending(current))\n-\t\t\tgoto retry;\n-\t\treturn -EINTR;\n+\t/* concurrent swappers might still be releasing physical swap slots... */\n+\twhile (swap_usage_in_pages(si)) {\n+\t\tif (signal_pending(current)) {\n+\t\t\tret = -EINTR;\n+\t\t\tgoto out;\n+\t\t}\n+\t\tschedule_timeout_uninterruptible(1);\n \t}\n \n+out:\n+\tswap_read_unplug(splug);\n+\tif (ret)\n+\t\treturn ret;\n+\n success:\n \t/*\n \t * Make sure that further cleanups after try_to_unuse() returns happen\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the inefficient use of bits in the swap map by replacing it with two bitmaps, one for allocated state and one for bad state, saving 6 bits per swap entry.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Now that we have moved the swap count state to virtual swap layer, each\nswap map entry only has 3 possible states: free, allocated, and bad.\nReplace the swap map with 2 bitmaps (one for allocated state and one for\nbad state), saving 6 bits per swap entry.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h |  3 +-\n mm/swapfile.c        | 81 +++++++++++++++++++++++---------------------\n 2 files changed, 44 insertions(+), 40 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex a30d382fb5ee1..a02ce3fb2358b 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -259,7 +259,8 @@ struct swap_info_struct {\n \tstruct plist_node list;\t\t/* entry in swap_active_head */\n \tsigned char\ttype;\t\t/* strange name for an index */\n \tunsigned int\tmax;\t\t/* extent of the swap_map */\n-\tunsigned char *swap_map;\t/* vmalloc'ed array of usage counts */\n+\tunsigned long *swap_map;\t/* bitmap for allocated state */\n+\tunsigned long *bad_map;\t\t/* bitmap for bad state */\n \tstruct swap_cluster_info *cluster_info; /* cluster info. Only for SSD */\n \tstruct list_head free_clusters; /* free clusters list */\n \tstruct list_head full_clusters; /* full clusters list */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 9478707ce3ffa..b7661ffa312be 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -760,25 +760,19 @@ static bool cluster_reclaim_range(struct swap_info_struct *si,\n \t\t\t\t  struct swap_cluster_info *ci,\n \t\t\t\t  unsigned long start, unsigned long end)\n {\n-\tunsigned char *map = si->swap_map;\n \tunsigned long offset = start;\n \tint nr_reclaim;\n \n \tspin_unlock(&ci->lock);\n \tdo {\n-\t\tswitch (READ_ONCE(map[offset])) {\n-\t\tcase 0:\n+\t\tif (!test_bit(offset, si->swap_map)) {\n \t\t\toffset++;\n-\t\t\tbreak;\n-\t\tcase SWAP_MAP_ALLOCATED:\n+\t\t} else {\n \t\t\tnr_reclaim = __try_to_reclaim_swap(si, offset, TTRS_ANYWAY);\n \t\t\tif (nr_reclaim > 0)\n \t\t\t\toffset += nr_reclaim;\n \t\t\telse\n \t\t\t\tgoto out;\n-\t\t\tbreak;\n-\t\tdefault:\n-\t\t\tgoto out;\n \t\t}\n \t} while (offset < end);\n out:\n@@ -787,11 +781,7 @@ static bool cluster_reclaim_range(struct swap_info_struct *si,\n \t * Recheck the range no matter reclaim succeeded or not, the slot\n \t * could have been be freed while we are not holding the lock.\n \t */\n-\tfor (offset = start; offset < end; offset++)\n-\t\tif (READ_ONCE(map[offset]))\n-\t\t\treturn false;\n-\n-\treturn true;\n+\treturn find_next_bit(si->swap_map, end, start) >= end;\n }\n \n static bool cluster_scan_range(struct swap_info_struct *si,\n@@ -800,15 +790,16 @@ static bool cluster_scan_range(struct swap_info_struct *si,\n \t\t\t       bool *need_reclaim)\n {\n \tunsigned long offset, end = start + nr_pages;\n-\tunsigned char *map = si->swap_map;\n-\tunsigned char count;\n \n \tif (cluster_is_empty(ci))\n \t\treturn true;\n \n \tfor (offset = start; offset < end; offset++) {\n-\t\tcount = READ_ONCE(map[offset]);\n-\t\tif (!count)\n+\t\t/* Bad slots cannot be used for allocation */\n+\t\tif (test_bit(offset, si->bad_map))\n+\t\t\treturn false;\n+\n+\t\tif (!test_bit(offset, si->swap_map))\n \t\t\tcontinue;\n \n \t\tif (swap_cache_only(si, offset)) {\n@@ -841,7 +832,7 @@ static bool cluster_alloc_range(struct swap_info_struct *si, struct swap_cluster\n \tif (cluster_is_empty(ci))\n \t\tci->order = order;\n \n-\tmemset(si->swap_map + start, usage, nr_pages);\n+\tbitmap_set(si->swap_map, start, nr_pages);\n \tswap_range_alloc(si, nr_pages);\n \tci->count += nr_pages;\n \n@@ -1404,7 +1395,7 @@ static struct swap_info_struct *_swap_info_get(swp_slot_t slot)\n \toffset = swp_slot_offset(slot);\n \tif (offset >= si->max)\n \t\tgoto bad_offset;\n-\tif (data_race(!si->swap_map[swp_slot_offset(slot)]))\n+\tif (data_race(!test_bit(offset, si->swap_map)))\n \t\tgoto bad_free;\n \treturn si;\n \n@@ -1518,8 +1509,7 @@ static void swap_slots_free(struct swap_info_struct *si,\n \t\t\t      swp_slot_t slot, unsigned int nr_pages)\n {\n \tunsigned long offset = swp_slot_offset(slot);\n-\tunsigned char *map = si->swap_map + offset;\n-\tunsigned char *map_end = map + nr_pages;\n+\tunsigned long end = offset + nr_pages;\n \n \t/* It should never free entries across different clusters */\n \tVM_BUG_ON(ci != __swap_offset_to_cluster(si, offset + nr_pages - 1));\n@@ -1527,10 +1517,8 @@ static void swap_slots_free(struct swap_info_struct *si,\n \tVM_BUG_ON(ci->count < nr_pages);\n \n \tci->count -= nr_pages;\n-\tdo {\n-\t\tVM_BUG_ON(!swap_is_last_ref(*map));\n-\t\t*map = 0;\n-\t} while (++map < map_end);\n+\tVM_BUG_ON(find_next_zero_bit(si->swap_map, end, offset) < end);\n+\tbitmap_clear(si->swap_map, offset, nr_pages);\n \n \tswap_range_free(si, offset, nr_pages);\n \n@@ -1741,9 +1729,7 @@ unsigned int count_swap_pages(int type, int free)\n static bool swap_slot_allocated(struct swap_info_struct *si,\n \t\tunsigned long offset)\n {\n-\tunsigned char count = READ_ONCE(si->swap_map[offset]);\n-\n-\treturn count && swap_count(count) != SWAP_MAP_BAD;\n+\treturn test_bit(offset, si->swap_map);\n }\n \n /*\n@@ -2064,7 +2050,7 @@ static int setup_swap_extents(struct swap_info_struct *sis, sector_t *span)\n }\n \n static void setup_swap_info(struct swap_info_struct *si, int prio,\n-\t\t\t    unsigned char *swap_map,\n+\t\t\t    unsigned long *swap_map,\n \t\t\t    struct swap_cluster_info *cluster_info)\n {\n \tsi->prio = prio;\n@@ -2092,7 +2078,7 @@ static void _enable_swap_info(struct swap_info_struct *si)\n }\n \n static void enable_swap_info(struct swap_info_struct *si, int prio,\n-\t\t\t\tunsigned char *swap_map,\n+\t\t\t\tunsigned long *swap_map,\n \t\t\t\tstruct swap_cluster_info *cluster_info)\n {\n \tspin_lock(&swap_lock);\n@@ -2185,7 +2171,8 @@ static void flush_percpu_swap_cluster(struct swap_info_struct *si)\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n-\tunsigned char *swap_map;\n+\tunsigned long *swap_map;\n+\tunsigned long *bad_map;\n \tstruct swap_cluster_info *cluster_info;\n \tstruct file *swap_file, *victim;\n \tstruct address_space *mapping;\n@@ -2280,6 +2267,8 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tp->swap_file = NULL;\n \tswap_map = p->swap_map;\n \tp->swap_map = NULL;\n+\tbad_map = p->bad_map;\n+\tp->bad_map = NULL;\n \tmaxpages = p->max;\n \tcluster_info = p->cluster_info;\n \tp->max = 0;\n@@ -2290,7 +2279,8 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tmutex_unlock(&swapon_mutex);\n \tkfree(p->global_cluster);\n \tp->global_cluster = NULL;\n-\tvfree(swap_map);\n+\tkvfree(swap_map);\n+\tkvfree(bad_map);\n \tfree_cluster_info(cluster_info, maxpages);\n \n \tinode = mapping->host;\n@@ -2638,18 +2628,20 @@ static unsigned long read_swap_header(struct swap_info_struct *si,\n \n static int setup_swap_map(struct swap_info_struct *si,\n \t\t\t  union swap_header *swap_header,\n-\t\t\t  unsigned char *swap_map,\n+\t\t\t  unsigned long *swap_map,\n+\t\t\t  unsigned long *bad_map,\n \t\t\t  unsigned long maxpages)\n {\n \tunsigned long i;\n \n-\tswap_map[0] = SWAP_MAP_BAD; /* omit header page */\n+\tset_bit(0, bad_map); /* omit header page */\n+\n \tfor (i = 0; i < swap_header->info.nr_badpages; i++) {\n \t\tunsigned int page_nr = swap_header->info.badpages[i];\n \t\tif (page_nr == 0 || page_nr > swap_header->info.last_page)\n \t\t\treturn -EINVAL;\n \t\tif (page_nr < maxpages) {\n-\t\t\tswap_map[page_nr] = SWAP_MAP_BAD;\n+\t\t\tset_bit(page_nr, bad_map);\n \t\t\tsi->pages--;\n \t\t}\n \t}\n@@ -2753,7 +2745,7 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tint nr_extents;\n \tsector_t span;\n \tunsigned long maxpages;\n-\tunsigned char *swap_map = NULL;\n+\tunsigned long *swap_map = NULL, *bad_map = NULL;\n \tstruct swap_cluster_info *cluster_info = NULL;\n \tstruct folio *folio = NULL;\n \tstruct inode *inode = NULL;\n@@ -2849,16 +2841,24 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tmaxpages = si->max;\n \n \t/* OK, set up the swap map and apply the bad block list */\n-\tswap_map = vzalloc(maxpages);\n+\tswap_map = kvcalloc(BITS_TO_LONGS(maxpages), sizeof(long), GFP_KERNEL);\n \tif (!swap_map) {\n \t\terror = -ENOMEM;\n \t\tgoto bad_swap_unlock_inode;\n \t}\n \n-\terror = setup_swap_map(si, swap_header, swap_map, maxpages);\n+\tbad_map = kvcalloc(BITS_TO_LONGS(maxpages), sizeof(long), GFP_KERNEL);\n+\tif (!bad_map) {\n+\t\terror = -ENOMEM;\n+\t\tgoto bad_swap_unlock_inode;\n+\t}\n+\n+\terror = setup_swap_map(si, swap_header, swap_map, bad_map, maxpages);\n \tif (error)\n \t\tgoto bad_swap_unlock_inode;\n \n+\tsi->bad_map = bad_map;\n+\n \tif (si->bdev && bdev_stable_writes(si->bdev))\n \t\tsi->flags |= SWP_STABLE_WRITES;\n \n@@ -2952,7 +2952,10 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tsi->swap_file = NULL;\n \tsi->flags = 0;\n \tspin_unlock(&swap_lock);\n-\tvfree(swap_map);\n+\tif (swap_map)\n+\t\tkvfree(swap_map);\n+\tif (bad_map)\n+\t\tkvfree(bad_map);\n \tif (cluster_info)\n \t\tfree_cluster_info(cluster_info, maxpages);\n \tif (inced_nr_rotate_swap)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged an issue with email delivery, apologized for inconvenience, and expressed confusion about the problem.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "apology",
                "confusion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Weirdly, it seems like the cover letter (and only the cover letter) is\nnot being delivered...\n\nI'm trying to figure out what's going on :( My apologies for the\ninconvenience...",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed reviewer feedback about poor batching behavior of vswap free path, explaining that the issue is due to a missing lock drop in the swapoff path and agreeing to restructure the code in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Changelog:\n* RFC v2 -> v3:\n    * Implement a cluster-based allocation algorithm for virtual swap\n      slots, inspired by Kairui Song and Chris Li's implementation, as\n      well as Johannes Weiner's suggestions. This eliminates the lock\n\t  contention issues on the virtual swap layer.\n    * Re-use swap table for the reverse mapping.\n    * Remove CONFIG_VIRTUAL_SWAP.\n    * Reducing the size of the swap descriptor from 48 bytes to 24\n      bytes, i.e another 50% reduction in memory overhead from v2.\n    * Remove swap cache and zswap tree and use the swap descriptor\n      for this.\n    * Remove zeromap, and replace the swap_map bytemap with 2 bitmaps\n      (one for allocated slots, and one for bad slots).\n    * Rebase on top of 6.19 (7d0a66e4bb9081d75c82ec4957c50034cb0ea449)\n\t* Update cover letter to include new benchmark results and discussion\n\t  on overhead in various cases.\n* RFC v1 -> RFC v2:\n    * Use a single atomic type (swap_refs) for reference counting\n      purpose. This brings the size of the swap descriptor from 64 B\n      down to 48 B (25% reduction). Suggested by Yosry Ahmed.\n    * Zeromap bitmap is removed in the virtual swap implementation.\n      This saves one bit per phyiscal swapfile slot.\n    * Rearrange the patches and the code change to make things more\n      reviewable. Suggested by Johannes Weiner.\n    * Update the cover letter a bit.\n\nThis patch series implements the virtual swap space idea, based on Yosry's\nproposals at LSFMMBPF 2023 (see [1], [2], [3]), as well as valuable\ninputs from Johannes Weiner. The same idea (with different\nimplementation details) has been floated by Rik van Riel since at least\n2011 (see [8]).\n\nThis patch series is based on 6.19. There are a couple more\nswap-related changes in the mm-stable branch that I would need to\ncoordinate with, but I would like to send this out as an update, to show\nthat the lock contention issues that plagued earlier versions have been\nresolved and performance on the kernel build benchmark is now on-par with\nbaseline. Furthermore, memory overhead has been substantially reduced\ncompared to the last RFC version.\n\n\nI. Motivation\n\nCurrently, when an anon page is swapped out, a slot in a backing swap\ndevice is allocated and stored in the page table entries that refer to\nthe original page. This slot is also used as the \"key\" to find the\nswapped out content, as well as the index to swap data structures, such\nas the swap cache, or the swap cgroup mapping. Tying a swap entry to its\nbacking slot in this way is performant and efficient when swap is purely\njust disk space, and swapoff is rare.\n\nHowever, the advent of many swap optimizations has exposed major\ndrawbacks of this design. The first problem is that we occupy a physical\nslot in the swap space, even for pages that are NEVER expected to hit\nthe disk: pages compressed and stored in the zswap pool, zero-filled\npages, or pages rejected by both of these optimizations when zswap\nwriteback is disabled. This is the arguably central shortcoming of\nzswap:\n* In deployments when no disk space can be afforded for swap (such as\n  mobile and embedded devices), users cannot adopt zswap, and are forced\n  to use zram. This is confusing for users, and creates extra burdens\n  for developers, having to develop and maintain similar features for\n  two separate swap backends (writeback, cgroup charging, THP support,\n  etc.). For instance, see the discussion in [4].\n* Resource-wise, it is hugely wasteful in terms of disk usage. At Meta,\n  we have swapfile in the order of tens to hundreds of GBs, which are\n  mostly unused and only exist to enable zswap usage and zero-filled\n  pages swap optimizations.\n* Tying zswap (and more generally, other in-memory swap backends) to\n  the current physical swapfile infrastructure makes zswap implicitly\n  statically sized. This does not make sense, as unlike disk swap, in\n  which we consume a limited resource (disk space or swapfile space) to\n  save another resource (memory), zswap consume the same resource it is\n  saving (memory). The more we zswap, the more memory we have available,\n  not less. We are not rationing a limited resource when we limit\n  the size of he zswap pool, but rather we are capping the resource\n  (memory) saving potential of zswap. Under memory pressure, using\n  more zswap is almost always better than the alternative (disk IOs, or\n  even worse, OOMs), and dynamically sizing the zswap pool on demand\n  allows the system to flexibly respond to these precarious scenarios.\n* Operationally, static provisioning the swapfile for zswap pose\n  significant challenges, because the sysadmin has to prescribe how\n  much swap is needed a priori, for each combination of\n  (memory size x disk space x workload usage). It is even more\n  complicated when we take into account the variance of memory\n  compression, which changes the reclaim dynamics (and as a result,\n  swap space size requirement). The problem is further exarcebated for\n  users who rely on swap utilization (and exhaustion) as an OOM signal.\n\n  All of these factors make it very difficult to configure the swapfile\n  for zswap: too small of a swapfile and we risk preventable OOMs and\n  limit the memory saving potentials of zswap; too big of a swapfile\n  and we waste disk space and memory due to swap metadata overhead.\n  This dilemma becomes more drastic in high memory systems, which can\n  have up to TBs worth of memory.\n\nPast attempts to decouple disk and compressed swap backends, namely the\nghost swapfile approach (see [13]), as well as the alternative\ncompressed swap backend zram, have mainly focused on eliminating the\ndisk space usage of compressed backends. We want a solution that not\nonly tackles that same problem, but also achieve the dyamicization of\nswap space to maximize the memory saving potentials while reducing\noperational and static memory overhead.\n\nFinally, any swap redesign should support efficient backend transfer,\ni.e without having to perform the expensive page table walk to\nupdate all the PTEs that refer to the swap entry:\n* The main motivation for this requirement is zswap writeback. To quote\n  Johannes (from [14]): \"Combining compression with disk swap is\n  extremely powerful, because it dramatically reduces the worst aspects\n  of both: it reduces the memory footprint of compression by shedding\n  the coldest data to disk; it reduces the IO latencies and flash wear\n  of disk swap through the writeback cache. In practice, this reduces\n  *average event rates of the entire reclaim/paging/IO stack*.\"\n* Another motivation is to simplify swapoff, which is both complicated\n  and expensive in the current design, precisely because we are storing\n  an encoding of the backend positional information in the page table,\n  and thus requires a full page table walk to remove these references.\n\n\nII. High Level Design Overview\n\nTo fix the aforementioned issues, we need an abstraction that separates\na swap entry from its physical backing storage. IOW, we need to\n\\u201cvirtualize\\u201d the swap space: swap clients will work with a dynamically\nallocated virtual swap slot, storing it in page table entries, and\nusing it to index into various swap-related data structures. The\nbacking storage is decoupled from the virtual swap slot, and the newly\nintroduced layer will \\u201cresolve\\u201d the virtual swap slot to the actual\nstorage. This layer also manages other metadata of the swap entry, such\nas its lifetime information (swap count), via a dynamically allocated,\nper-swap-entry descriptor:\n\nstruct swp_desc {\n        union {\n                swp_slot_t         slot;                 /*     0     8 */\n                struct zswap_entry * zswap_entry;        /*     0     8 */\n        };                                               /*     0     8 */\n        union {\n                struct folio *     swap_cache;           /*     8     8 */\n                void *             shadow;               /*     8     8 */\n        };                                               /*     8     8 */\n        unsigned int               swap_count;           /*    16     4 */\n        unsigned short             memcgid:16;           /*    20: 0  2 */\n        bool                       in_swapcache:1;       /*    22: 0  1 */\n\n        /* Bitfield combined with previous fields */\n\n        enum swap_type             type:2;               /*    20:17  4 */\n\n        /* size: 24, cachelines: 1, members: 6 */\n        /* bit_padding: 13 bits */\n        /* last cacheline: 24 bytes */\n};\n\n(output from pahole).\n\nThis design allows us to:\n* Decouple zswap (and zeromapped swap entry) from backing swapfile:\n  simply associate the virtual swap slot with one of the supported\n  backends: a zswap entry, a zero-filled swap page, a slot on the\n  swapfile, or an in-memory page.\n* Simplify and optimize swapoff: we only have to fault the page in and\n  have the virtual swap slot points to the page instead of the on-disk\n  physical swap slot. No need to perform any page table walking.\n\nThe size of the virtual swap descriptor is 24 bytes. Note that this is\nnot all \"new\" overhead, as the swap descriptor will replace:\n* the swap_cgroup arrays (one per swap type) in the old design, which\n  is a massive source of static memory overhead. With the new design,\n  it is only allocated for used clusters.\n* the swap tables, which holds the swap cache and workingset shadows.\n* the zeromap bitmap, which is a bitmap of physical swap slots to\n  indicate whether the swapped out page is zero-filled or not.\n* huge chunk of the swap_map. The swap_map is now replaced by 2 bitmaps,\n  one for allocated slots, and one for bad slots, representing 3 possible\n  states of a slot on the swapfile: allocated, free, and bad.\n* the zswap tree.\n\nSo, in terms of additional memory overhead:\n* For zswap entries, the added memory overhead is rather minimal. The\n  new indirection pointer neatly replaces the existing zswap tree.\n  We really only incur less than one word of overhead for swap count\n  blow up (since we no longer use swap continuation) and the swap type.\n* For physical swap entries, the new design will impose fewer than 3 words\n  memory overhead. However, as noted above this overhead is only for\n  actively used swap entries, whereas in the current design the overhead is\n  static (including the swap cgroup array for example).\n\n  The primary victim of this overhead will be zram users. However, as\n  zswap now no longer takes up disk space, zram users can consider\n  switching to zswap (which, as a bonus, has a lot of useful features\n  out of the box, such as cgroup tracking, dynamic zswap pool sizing,\n  LRU-ordering writeback, etc.).\n\nFor a more concrete example, suppose we have a 32 GB swapfile (i.e.\n8,388,608 swap entries), and we use zswap.\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 0.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 48.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 96.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 121.00 MB\n* Vswap total overhead: 144.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 153.00 MB\n* Vswap total overhead: 193.00 MB\n\nSo even in the worst case scenario for virtual swap, i.e when we\nsomehow have an oracle to correctly size the swapfile for zswap\npool to 32 GB, the added overhead is only 40 MB, which is a mere\n0.12% of the total swapfile :)\n\nIn practice, the overhead will be closer to the 50-75% usage case, as\nsystems tend to leave swap headroom for pathological events or sudden\nspikes in memory requirements. The added overhead in these cases are\npractically neglible. And in deployments where swapfiles for zswap\nare previously sparsely used, switching over to virtual swap will\nactually reduce memory overhead.\n\nDoing the same math for the disk swap, which is the worst case for\nvirtual swap in terms of swap backends:\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 2.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 41.00 MB\n* Vswap total overhead: 66.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 130.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 73.00 MB\n* Vswap total overhead: 194.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 259.00 MB\n\nThe added overhead is 170MB, which is 0.5% of the total swapfile size,\nagain in the worst case when we have a sizing oracle.\n\nPlease see the attached patches for more implementation details.\n\n\nIII. Usage and Benchmarking\n\nThis patch series introduce no new syscalls or userspace API. Existing\nuserspace setups will work as-is, except we no longer have to create a\nswapfile or set memory.swap.max if we want to use zswap, as zswap is no\nlonger tied to physical swap. The zswap pool will be automatically and\ndynamically sized based on memory usage and reclaim dynamics.\n\nTo measure the performance of the new implementation, I have run the\nfollowing benchmarks:\n\n1. Kernel building: 52 workers (one per processor), memory.max = 3G.\n\nUsing zswap as the backend:\n\nBaseline:\nreal: mean: 185.2s, stdev: 0.93s\nsys: mean: 683.7s, stdev: 33.77s\n\nVswap:\nreal: mean: 184.88s, stdev: 0.57s\nsys: mean: 675.14s, stdev: 32.8s\n\nWe actually see a slight improvement in systime (by 1.5%) :) This is\nlikely because we no longer have to perform swap charging for zswap\nentries, and virtual swap allocator is simpler than that of physical\nswap.\n\nUsing SSD swap as the backend:\n\nBaseline:\nreal: mean: 200.3s, stdev: 2.33s\nsys: mean: 489.88s, stdev: 9.62s\n\nVswap:\nreal: mean: 201.47s, stdev: 2.98s\nsys: mean: 487.36s, stdev: 5.53s\n\nThe performance is neck-to-neck.\n\n\nIV. Future Use Cases\n\nWhile the patch series focus on two applications (decoupling swap\nbackends and swapoff optimization/simplification), this new,\nfuture-proof design also allows us to implement new swap features more\neasily and efficiently:\n\n* Multi-tier swapping (as mentioned in [5]), with transparent\n  transferring (promotion/demotion) of pages across tiers (see [8] and\n  [9]). Similar to swapoff, with the old design we would need to\n  perform the expensive page table walk.\n* Swapfile compaction to alleviate fragmentation (as proposed by Ying\n  Huang in [6]).\n* Mixed backing THP swapin (see [7]): Once you have pinned down the\n  backing store of THPs, then you can dispatch each range of subpages\n  to appropriate backend swapin handler.\n* Swapping a folio out with discontiguous physical swap slots\n  (see [10]).\n* Zswap writeback optimization: The current architecture pre-reserves\n  physical swap space for pages when they enter the zswap pool, giving\n  the kernel no flexibility at writeback time. With the virtual swap\n  implementation, the backends are decoupled, and physical swap space\n  is allocated on-demand at writeback time, at which point we can make\n  much smarter decisions: we can batch multiple zswap writeback\n  operations into a single IO request, allocating contiguous physical\n  swap slots for that request. We can even perform compressed writeback\n  (i.e writing these pages without decompressing them) (see [12]).\n\n\nV. References\n\n[1]: https://lore.kernel.org/all/CAJD7tkbCnXJ95Qow_aOjNX6NOMU5ovMSHRC+95U4wtW6cM+puw@mail.gmail.com/\n[2]: https://lwn.net/Articles/932077/\n[3]: https://www.youtube.com/watch?v=Hwqw_TBGEhg\n[4]: https://lore.kernel.org/all/Zqe_Nab-Df1CN7iW@infradead.org/\n[5]: https://lore.kernel.org/lkml/CAF8kJuN-4UE0skVHvjUzpGefavkLULMonjgkXUZSBVJrcGFXCA@mail.gmail.com/\n[6]: https://lore.kernel.org/linux-mm/87o78mzp24.fsf@yhuang6-desk2.ccr.corp.intel.com/\n[7]: https://lore.kernel.org/all/CAGsJ_4ysCN6f7qt=6gvee1x3ttbOnifGneqcRm9Hoeun=uFQ2w@mail.gmail.com/\n[8]: https://lore.kernel.org/linux-mm/4DA25039.3020700@redhat.com/\n[9]: https://lore.kernel.org/all/CA+ZsKJ7DCE8PMOSaVmsmYZL9poxK6rn0gvVXbjpqxMwxS2C9TQ@mail.gmail.com/\n[10]: https://lore.kernel.org/all/CACePvbUkMYMencuKfpDqtG1Ej7LiUS87VRAXb8sBn1yANikEmQ@mail.gmail.com/\n[11]: https://lore.kernel.org/all/CAMgjq7BvQ0ZXvyLGp2YP96+i+6COCBBJCYmjXHGBnfisCAb8VA@mail.gmail.com/\n[12]: https://lore.kernel.org/linux-mm/ZeZSDLWwDed0CgT3@casper.infradead.org/\n[13]: https://lore.kernel.org/all/20251121-ghost-v1-1-cfc0efcf3855@kernel.org/\n[14]: https://lore.kernel.org/linux-mm/20251202170222.GD430226@cmpxchg.org/\n\nNhat Pham (20):\n  mm/swap: decouple swap cache from physical swap infrastructure\n  swap: rearrange the swap header file\n  mm: swap: add an abstract API for locking out swapoff\n  zswap: add new helpers for zswap entry operations\n  mm/swap: add a new function to check if a swap entry is in swap\n    cached.\n  mm: swap: add a separate type for physical swap slots\n  mm: create scaffolds for the new virtual swap implementation\n  zswap: prepare zswap for swap virtualization\n  mm: swap: allocate a virtual swap slot for each swapped out page\n  swap: move swap cache to virtual swap descriptor\n  zswap: move zswap entry management to the virtual swap descriptor\n  swap: implement the swap_cgroup API using virtual swap\n  swap: manage swap entry lifecycle at the virtual swap layer\n  mm: swap: decouple virtual swap slot from backing store\n  zswap: do not start zswap shrinker if there is no physical swap slots\n  swap: do not unnecesarily pin readahead swap entries\n  swapfile: remove zeromap bitmap\n  memcg: swap: only charge physical swap slots\n  swap: simplify swapoff using virtual swap\n  swapfile: replace the swap map with bitmaps\n\n Documentation/mm/swap-table.rst |   69 --\n MAINTAINERS                     |    2 +\n include/linux/cpuhotplug.h      |    1 +\n include/linux/mm_types.h        |   16 +\n include/linux/shmem_fs.h        |    7 +-\n include/linux/swap.h            |  135 ++-\n include/linux/swap_cgroup.h     |   13 -\n include/linux/swapops.h         |   25 +\n include/linux/zswap.h           |   17 +-\n kernel/power/swap.c             |    6 +-\n mm/Makefile                     |    5 +-\n mm/huge_memory.c                |   11 +-\n mm/internal.h                   |   12 +-\n mm/memcontrol-v1.c              |    6 +\n mm/memcontrol.c                 |  142 ++-\n mm/memory.c                     |  101 +-\n mm/migrate.c                    |   13 +-\n mm/mincore.c                    |   15 +-\n mm/page_io.c                    |   83 +-\n mm/shmem.c                      |  215 +---\n mm/swap.h                       |  157 +--\n mm/swap_cgroup.c                |  172 ---\n mm/swap_state.c                 |  306 +----\n mm/swap_table.h                 |   78 +-\n mm/swapfile.c                   | 1518 ++++-------------------\n mm/userfaultfd.c                |   18 +-\n mm/vmscan.c                     |   28 +-\n mm/vswap.c                      | 2025 +++++++++++++++++++++++++++++++\n mm/zswap.c                      |  142 +--\n 29 files changed, 2853 insertions(+), 2485 deletions(-)\n delete mode 100644 Documentation/mm/swap-table.rst\n delete mode 100644 mm/swap_cgroup.c\n create mode 100644 mm/vswap.c\n\n\nbase-commit: 05f7e89ab9731565d8a62e3b5d1ec206485eeb0b\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the poor batching behavior of vswap free path in their patch series, explaining that they had already implemented a cluster-based allocation algorithm for virtual swap slots to eliminate lock contention issues and improve performance.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "My sincerest apologies - it seems like the cover letter (and just the\ncover letter) fails to be sent out, for some reason. I'm trying to figure\nout what happened - it works when I send the entire patch series to\nmyself...\n\nAnyway, resending this (in-reply-to patch 1 of the series):\n\nChangelog:\n* RFC v2 -> v3:\n    * Implement a cluster-based allocation algorithm for virtual swap\n      slots, inspired by Kairui Song and Chris Li's implementation, as\n      well as Johannes Weiner's suggestions. This eliminates the lock\n\t  contention issues on the virtual swap layer.\n    * Re-use swap table for the reverse mapping.\n    * Remove CONFIG_VIRTUAL_SWAP.\n    * Reducing the size of the swap descriptor from 48 bytes to 24\n      bytes, i.e another 50% reduction in memory overhead from v2.\n    * Remove swap cache and zswap tree and use the swap descriptor\n      for this.\n    * Remove zeromap, and replace the swap_map bytemap with 2 bitmaps\n      (one for allocated slots, and one for bad slots).\n    * Rebase on top of 6.19 (7d0a66e4bb9081d75c82ec4957c50034cb0ea449)\n\t* Update cover letter to include new benchmark results and discussion\n\t  on overhead in various cases.\n* RFC v1 -> RFC v2:\n    * Use a single atomic type (swap_refs) for reference counting\n      purpose. This brings the size of the swap descriptor from 64 B\n      down to 48 B (25% reduction). Suggested by Yosry Ahmed.\n    * Zeromap bitmap is removed in the virtual swap implementation.\n      This saves one bit per phyiscal swapfile slot.\n    * Rearrange the patches and the code change to make things more\n      reviewable. Suggested by Johannes Weiner.\n    * Update the cover letter a bit.\n\nThis patch series implements the virtual swap space idea, based on Yosry's\nproposals at LSFMMBPF 2023 (see [1], [2], [3]), as well as valuable\ninputs from Johannes Weiner. The same idea (with different\nimplementation details) has been floated by Rik van Riel since at least\n2011 (see [8]).\n\nThis patch series is based on 6.19. There are a couple more\nswap-related changes in the mm-stable branch that I would need to\ncoordinate with, but I would like to send this out as an update, to show\nthat the lock contention issues that plagued earlier versions have been\nresolved and performance on the kernel build benchmark is now on-par with\nbaseline. Furthermore, memory overhead has been substantially reduced\ncompared to the last RFC version.\n\n\nI. Motivation\n\nCurrently, when an anon page is swapped out, a slot in a backing swap\ndevice is allocated and stored in the page table entries that refer to\nthe original page. This slot is also used as the \"key\" to find the\nswapped out content, as well as the index to swap data structures, such\nas the swap cache, or the swap cgroup mapping. Tying a swap entry to its\nbacking slot in this way is performant and efficient when swap is purely\njust disk space, and swapoff is rare.\n\nHowever, the advent of many swap optimizations has exposed major\ndrawbacks of this design. The first problem is that we occupy a physical\nslot in the swap space, even for pages that are NEVER expected to hit\nthe disk: pages compressed and stored in the zswap pool, zero-filled\npages, or pages rejected by both of these optimizations when zswap\nwriteback is disabled. This is the arguably central shortcoming of\nzswap:\n* In deployments when no disk space can be afforded for swap (such as\n  mobile and embedded devices), users cannot adopt zswap, and are forced\n  to use zram. This is confusing for users, and creates extra burdens\n  for developers, having to develop and maintain similar features for\n  two separate swap backends (writeback, cgroup charging, THP support,\n  etc.). For instance, see the discussion in [4].\n* Resource-wise, it is hugely wasteful in terms of disk usage. At Meta,\n  we have swapfile in the order of tens to hundreds of GBs, which are\n  mostly unused and only exist to enable zswap usage and zero-filled\n  pages swap optimizations.\n* Tying zswap (and more generally, other in-memory swap backends) to\n  the current physical swapfile infrastructure makes zswap implicitly\n  statically sized. This does not make sense, as unlike disk swap, in\n  which we consume a limited resource (disk space or swapfile space) to\n  save another resource (memory), zswap consume the same resource it is\n  saving (memory). The more we zswap, the more memory we have available,\n  not less. We are not rationing a limited resource when we limit\n  the size of he zswap pool, but rather we are capping the resource\n  (memory) saving potential of zswap. Under memory pressure, using\n  more zswap is almost always better than the alternative (disk IOs, or\n  even worse, OOMs), and dynamically sizing the zswap pool on demand\n  allows the system to flexibly respond to these precarious scenarios.\n* Operationally, static provisioning the swapfile for zswap pose\n  significant challenges, because the sysadmin has to prescribe how\n  much swap is needed a priori, for each combination of\n  (memory size x disk space x workload usage). It is even more\n  complicated when we take into account the variance of memory\n  compression, which changes the reclaim dynamics (and as a result,\n  swap space size requirement). The problem is further exarcebated for\n  users who rely on swap utilization (and exhaustion) as an OOM signal.\n\n  All of these factors make it very difficult to configure the swapfile\n  for zswap: too small of a swapfile and we risk preventable OOMs and\n  limit the memory saving potentials of zswap; too big of a swapfile\n  and we waste disk space and memory due to swap metadata overhead.\n  This dilemma becomes more drastic in high memory systems, which can\n  have up to TBs worth of memory.\n\nPast attempts to decouple disk and compressed swap backends, namely the\nghost swapfile approach (see [13]), as well as the alternative\ncompressed swap backend zram, have mainly focused on eliminating the\ndisk space usage of compressed backends. We want a solution that not\nonly tackles that same problem, but also achieve the dyamicization of\nswap space to maximize the memory saving potentials while reducing\noperational and static memory overhead.\n\nFinally, any swap redesign should support efficient backend transfer,\ni.e without having to perform the expensive page table walk to\nupdate all the PTEs that refer to the swap entry:\n* The main motivation for this requirement is zswap writeback. To quote\n  Johannes (from [14]): \"Combining compression with disk swap is\n  extremely powerful, because it dramatically reduces the worst aspects\n  of both: it reduces the memory footprint of compression by shedding\n  the coldest data to disk; it reduces the IO latencies and flash wear\n  of disk swap through the writeback cache. In practice, this reduces\n  *average event rates of the entire reclaim/paging/IO stack*.\"\n* Another motivation is to simplify swapoff, which is both complicated\n  and expensive in the current design, precisely because we are storing\n  an encoding of the backend positional information in the page table,\n  and thus requires a full page table walk to remove these references.\n\n\nII. High Level Design Overview\n\nTo fix the aforementioned issues, we need an abstraction that separates\na swap entry from its physical backing storage. IOW, we need to\n\\u201cvirtualize\\u201d the swap space: swap clients will work with a dynamically\nallocated virtual swap slot, storing it in page table entries, and\nusing it to index into various swap-related data structures. The\nbacking storage is decoupled from the virtual swap slot, and the newly\nintroduced layer will \\u201cresolve\\u201d the virtual swap slot to the actual\nstorage. This layer also manages other metadata of the swap entry, such\nas its lifetime information (swap count), via a dynamically allocated,\nper-swap-entry descriptor:\n\nstruct swp_desc {\n        union {\n                swp_slot_t         slot;                 /*     0     8 */\n                struct zswap_entry * zswap_entry;        /*     0     8 */\n        };                                               /*     0     8 */\n        union {\n                struct folio *     swap_cache;           /*     8     8 */\n                void *             shadow;               /*     8     8 */\n        };                                               /*     8     8 */\n        unsigned int               swap_count;           /*    16     4 */\n        unsigned short             memcgid:16;           /*    20: 0  2 */\n        bool                       in_swapcache:1;       /*    22: 0  1 */\n\n        /* Bitfield combined with previous fields */\n\n        enum swap_type             type:2;               /*    20:17  4 */\n\n        /* size: 24, cachelines: 1, members: 6 */\n        /* bit_padding: 13 bits */\n        /* last cacheline: 24 bytes */\n};\n\n(output from pahole).\n\nThis design allows us to:\n* Decouple zswap (and zeromapped swap entry) from backing swapfile:\n  simply associate the virtual swap slot with one of the supported\n  backends: a zswap entry, a zero-filled swap page, a slot on the\n  swapfile, or an in-memory page.\n* Simplify and optimize swapoff: we only have to fault the page in and\n  have the virtual swap slot points to the page instead of the on-disk\n  physical swap slot. No need to perform any page table walking.\n\nThe size of the virtual swap descriptor is 24 bytes. Note that this is\nnot all \"new\" overhead, as the swap descriptor will replace:\n* the swap_cgroup arrays (one per swap type) in the old design, which\n  is a massive source of static memory overhead. With the new design,\n  it is only allocated for used clusters.\n* the swap tables, which holds the swap cache and workingset shadows.\n* the zeromap bitmap, which is a bitmap of physical swap slots to\n  indicate whether the swapped out page is zero-filled or not.\n* huge chunk of the swap_map. The swap_map is now replaced by 2 bitmaps,\n  one for allocated slots, and one for bad slots, representing 3 possible\n  states of a slot on the swapfile: allocated, free, and bad.\n* the zswap tree.\n\nSo, in terms of additional memory overhead:\n* For zswap entries, the added memory overhead is rather minimal. The\n  new indirection pointer neatly replaces the existing zswap tree.\n  We really only incur less than one word of overhead for swap count\n  blow up (since we no longer use swap continuation) and the swap type.\n* For physical swap entries, the new design will impose fewer than 3 words\n  memory overhead. However, as noted above this overhead is only for\n  actively used swap entries, whereas in the current design the overhead is\n  static (including the swap cgroup array for example).\n\n  The primary victim of this overhead will be zram users. However, as\n  zswap now no longer takes up disk space, zram users can consider\n  switching to zswap (which, as a bonus, has a lot of useful features\n  out of the box, such as cgroup tracking, dynamic zswap pool sizing,\n  LRU-ordering writeback, etc.).\n\nFor a more concrete example, suppose we have a 32 GB swapfile (i.e.\n8,388,608 swap entries), and we use zswap.\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 0.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 48.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 96.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 121.00 MB\n* Vswap total overhead: 144.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 153.00 MB\n* Vswap total overhead: 193.00 MB\n\nSo even in the worst case scenario for virtual swap, i.e when we\nsomehow have an oracle to correctly size the swapfile for zswap\npool to 32 GB, the added overhead is only 40 MB, which is a mere\n0.12% of the total swapfile :)\n\nIn practice, the overhead will be closer to the 50-75% usage case, as\nsystems tend to leave swap headroom for pathological events or sudden\nspikes in memory requirements. The added overhead in these cases are\npractically neglible. And in deployments where swapfiles for zswap\nare previously sparsely used, switching over to virtual swap will\nactually reduce memory overhead.\n\nDoing the same math for the disk swap, which is the worst case for\nvirtual swap in terms of swap backends:\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 2.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 41.00 MB\n* Vswap total overhead: 66.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 130.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 73.00 MB\n* Vswap total overhead: 194.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 259.00 MB\n\nThe added overhead is 170MB, which is 0.5% of the total swapfile size,\nagain in the worst case when we have a sizing oracle.\n\nPlease see the attached patches for more implementation details.\n\n\nIII. Usage and Benchmarking\n\nThis patch series introduce no new syscalls or userspace API. Existing\nuserspace setups will work as-is, except we no longer have to create a\nswapfile or set memory.swap.max if we want to use zswap, as zswap is no\nlonger tied to physical swap. The zswap pool will be automatically and\ndynamically sized based on memory usage and reclaim dynamics.\n\nTo measure the performance of the new implementation, I have run the\nfollowing benchmarks:\n\n1. Kernel building: 52 workers (one per processor), memory.max = 3G.\n\nUsing zswap as the backend:\n\nBaseline:\nreal: mean: 185.2s, stdev: 0.93s\nsys: mean: 683.7s, stdev: 33.77s\n\nVswap:\nreal: mean: 184.88s, stdev: 0.57s\nsys: mean: 675.14s, stdev: 32.8s\n\nWe actually see a slight improvement in systime (by 1.5%) :) This is\nlikely because we no longer have to perform swap charging for zswap\nentries, and virtual swap allocator is simpler than that of physical\nswap.\n\nUsing SSD swap as the backend:\n\nBaseline:\nreal: mean: 200.3s, stdev: 2.33s\nsys: mean: 489.88s, stdev: 9.62s\n\nVswap:\nreal: mean: 201.47s, stdev: 2.98s\nsys: mean: 487.36s, stdev: 5.53s\n\nThe performance is neck-to-neck.\n\n\nIV. Future Use Cases\n\nWhile the patch series focus on two applications (decoupling swap\nbackends and swapoff optimization/simplification), this new,\nfuture-proof design also allows us to implement new swap features more\neasily and efficiently:\n\n* Multi-tier swapping (as mentioned in [5]), with transparent\n  transferring (promotion/demotion) of pages across tiers (see [8] and\n  [9]). Similar to swapoff, with the old design we would need to\n  perform the expensive page table walk.\n* Swapfile compaction to alleviate fragmentation (as proposed by Ying\n  Huang in [6]).\n* Mixed backing THP swapin (see [7]): Once you have pinned down the\n  backing store of THPs, then you can dispatch each range of subpages\n  to appropriate backend swapin handler.\n* Swapping a folio out with discontiguous physical swap slots\n  (see [10]).\n* Zswap writeback optimization: The current architecture pre-reserves\n  physical swap space for pages when they enter the zswap pool, giving\n  the kernel no flexibility at writeback time. With the virtual swap\n  implementation, the backends are decoupled, and physical swap space\n  is allocated on-demand at writeback time, at which point we can make\n  much smarter decisions: we can batch multiple zswap writeback\n  operations into a single IO request, allocating contiguous physical\n  swap slots for that request. We can even perform compressed writeback\n  (i.e writing these pages without decompressing them) (see [12]).\n\n\nV. References\n\n[1]: https://lore.kernel.org/all/CAJD7tkbCnXJ95Qow_aOjNX6NOMU5ovMSHRC+95U4wtW6cM+puw@mail.gmail.com/\n[2]: https://lwn.net/Articles/932077/\n[3]: https://www.youtube.com/watch?v=Hwqw_TBGEhg\n[4]: https://lore.kernel.org/all/Zqe_Nab-Df1CN7iW@infradead.org/\n[5]: https://lore.kernel.org/lkml/CAF8kJuN-4UE0skVHvjUzpGefavkLULMonjgkXUZSBVJrcGFXCA@mail.gmail.com/\n[6]: https://lore.kernel.org/linux-mm/87o78mzp24.fsf@yhuang6-desk2.ccr.corp.intel.com/\n[7]: https://lore.kernel.org/all/CAGsJ_4ysCN6f7qt=6gvee1x3ttbOnifGneqcRm9Hoeun=uFQ2w@mail.gmail.com/\n[8]: https://lore.kernel.org/linux-mm/4DA25039.3020700@redhat.com/\n[9]: https://lore.kernel.org/all/CA+ZsKJ7DCE8PMOSaVmsmYZL9poxK6rn0gvVXbjpqxMwxS2C9TQ@mail.gmail.com/\n[10]: https://lore.kernel.org/all/CACePvbUkMYMencuKfpDqtG1Ej7LiUS87VRAXb8sBn1yANikEmQ@mail.gmail.com/\n[11]: https://lore.kernel.org/all/CAMgjq7BvQ0ZXvyLGp2YP96+i+6COCBBJCYmjXHGBnfisCAb8VA@mail.gmail.com/\n[12]: https://lore.kernel.org/linux-mm/ZeZSDLWwDed0CgT3@casper.infradead.org/\n[13]: https://lore.kernel.org/all/20251121-ghost-v1-1-cfc0efcf3855@kernel.org/\n[14]: https://lore.kernel.org/linux-mm/20251202170222.GD430226@cmpxchg.org/\n\nNhat Pham (20):\n  mm/swap: decouple swap cache from physical swap infrastructure\n  swap: rearrange the swap header file\n  mm: swap: add an abstract API for locking out swapoff\n  zswap: add new helpers for zswap entry operations\n  mm/swap: add a new function to check if a swap entry is in swap\n    cached.\n  mm: swap: add a separate type for physical swap slots\n  mm: create scaffolds for the new virtual swap implementation\n  zswap: prepare zswap for swap virtualization\n  mm: swap: allocate a virtual swap slot for each swapped out page\n  swap: move swap cache to virtual swap descriptor\n  zswap: move zswap entry management to the virtual swap descriptor\n  swap: implement the swap_cgroup API using virtual swap\n  swap: manage swap entry lifecycle at the virtual swap layer\n  mm: swap: decouple virtual swap slot from backing store\n  zswap: do not start zswap shrinker if there is no physical swap slots\n  swap: do not unnecesarily pin readahead swap entries\n  swapfile: remove zeromap bitmap\n  memcg: swap: only charge physical swap slots\n  swap: simplify swapoff using virtual swap\n  swapfile: replace the swap map with bitmaps\n\n Documentation/mm/swap-table.rst |   69 --\n MAINTAINERS                     |    2 +\n include/linux/cpuhotplug.h      |    1 +\n include/linux/mm_types.h        |   16 +\n include/linux/shmem_fs.h        |    7 +-\n include/linux/swap.h            |  135 ++-\n include/linux/swap_cgroup.h     |   13 -\n include/linux/swapops.h         |   25 +\n include/linux/zswap.h           |   17 +-\n kernel/power/swap.c             |    6 +-\n mm/Makefile                     |    5 +-\n mm/huge_memory.c                |   11 +-\n mm/internal.h                   |   12 +-\n mm/memcontrol-v1.c              |    6 +\n mm/memcontrol.c                 |  142 ++-\n mm/memory.c                     |  101 +-\n mm/migrate.c                    |   13 +-\n mm/mincore.c                    |   15 +-\n mm/page_io.c                    |   83 +-\n mm/shmem.c                      |  215 +---\n mm/swap.h                       |  157 +--\n mm/swap_cgroup.c                |  172 ---\n mm/swap_state.c                 |  306 +----\n mm/swap_table.h                 |   78 +-\n mm/swapfile.c                   | 1518 ++++-------------------\n mm/userfaultfd.c                |   18 +-\n mm/vmscan.c                     |   28 +-\n mm/vswap.c                      | 2025 +++++++++++++++++++++++++++++++\n mm/zswap.c                      |  142 +--\n 29 files changed, 2853 insertions(+), 2485 deletions(-)\n delete mode 100644 Documentation/mm/swap-table.rst\n delete mode 100644 mm/swap_cgroup.c\n create mode 100644 mm/vswap.c\n\n\nbase-commit: 05f7e89ab9731565d8a62e3b5d1ec206485eeb0b\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the batching behavior of vswap free path in their patch series, explaining that they have already implemented a cluster-based allocation algorithm for virtual swap slots to eliminate lock contention issues and reduce memory overhead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "My sincerest apologies - it seems like the cover letter (and just the\ncover letter) fails to be sent out, for some reason. I'm trying to figure\nout what happened - it works when I send the entire patch series to\nmyself...\n\nAnyway, resending this (in-reply-to patch 1 of the series):\n\nChangelog:\n* RFC v2 -> v3:\n    * Implement a cluster-based allocation algorithm for virtual swap\n      slots, inspired by Kairui Song and Chris Li's implementation, as\n      well as Johannes Weiner's suggestions. This eliminates the lock\n\t  contention issues on the virtual swap layer.\n    * Re-use swap table for the reverse mapping.\n    * Remove CONFIG_VIRTUAL_SWAP.\n    * Reducing the size of the swap descriptor from 48 bytes to 24\n      bytes, i.e another 50% reduction in memory overhead from v2.\n    * Remove swap cache and zswap tree and use the swap descriptor\n      for this.\n    * Remove zeromap, and replace the swap_map bytemap with 2 bitmaps\n      (one for allocated slots, and one for bad slots).\n    * Rebase on top of 6.19 (7d0a66e4bb9081d75c82ec4957c50034cb0ea449)\n\t* Update cover letter to include new benchmark results and discussion\n\t  on overhead in various cases.\n* RFC v1 -> RFC v2:\n    * Use a single atomic type (swap_refs) for reference counting\n      purpose. This brings the size of the swap descriptor from 64 B\n      down to 48 B (25% reduction). Suggested by Yosry Ahmed.\n    * Zeromap bitmap is removed in the virtual swap implementation.\n      This saves one bit per phyiscal swapfile slot.\n    * Rearrange the patches and the code change to make things more\n      reviewable. Suggested by Johannes Weiner.\n    * Update the cover letter a bit.\n\nThis patch series implements the virtual swap space idea, based on Yosry's\nproposals at LSFMMBPF 2023 (see [1], [2], [3]), as well as valuable\ninputs from Johannes Weiner. The same idea (with different\nimplementation details) has been floated by Rik van Riel since at least\n2011 (see [8]).\n\nThis patch series is based on 6.19. There are a couple more\nswap-related changes in the mm-stable branch that I would need to\ncoordinate with, but I would like to send this out as an update, to show\nthat the lock contention issues that plagued earlier versions have been\nresolved and performance on the kernel build benchmark is now on-par with\nbaseline. Furthermore, memory overhead has been substantially reduced\ncompared to the last RFC version.\n\n\nI. Motivation\n\nCurrently, when an anon page is swapped out, a slot in a backing swap\ndevice is allocated and stored in the page table entries that refer to\nthe original page. This slot is also used as the \"key\" to find the\nswapped out content, as well as the index to swap data structures, such\nas the swap cache, or the swap cgroup mapping. Tying a swap entry to its\nbacking slot in this way is performant and efficient when swap is purely\njust disk space, and swapoff is rare.\n\nHowever, the advent of many swap optimizations has exposed major\ndrawbacks of this design. The first problem is that we occupy a physical\nslot in the swap space, even for pages that are NEVER expected to hit\nthe disk: pages compressed and stored in the zswap pool, zero-filled\npages, or pages rejected by both of these optimizations when zswap\nwriteback is disabled. This is the arguably central shortcoming of\nzswap:\n* In deployments when no disk space can be afforded for swap (such as\n  mobile and embedded devices), users cannot adopt zswap, and are forced\n  to use zram. This is confusing for users, and creates extra burdens\n  for developers, having to develop and maintain similar features for\n  two separate swap backends (writeback, cgroup charging, THP support,\n  etc.). For instance, see the discussion in [4].\n* Resource-wise, it is hugely wasteful in terms of disk usage. At Meta,\n  we have swapfile in the order of tens to hundreds of GBs, which are\n  mostly unused and only exist to enable zswap usage and zero-filled\n  pages swap optimizations.\n* Tying zswap (and more generally, other in-memory swap backends) to\n  the current physical swapfile infrastructure makes zswap implicitly\n  statically sized. This does not make sense, as unlike disk swap, in\n  which we consume a limited resource (disk space or swapfile space) to\n  save another resource (memory), zswap consume the same resource it is\n  saving (memory). The more we zswap, the more memory we have available,\n  not less. We are not rationing a limited resource when we limit\n  the size of he zswap pool, but rather we are capping the resource\n  (memory) saving potential of zswap. Under memory pressure, using\n  more zswap is almost always better than the alternative (disk IOs, or\n  even worse, OOMs), and dynamically sizing the zswap pool on demand\n  allows the system to flexibly respond to these precarious scenarios.\n* Operationally, static provisioning the swapfile for zswap pose\n  significant challenges, because the sysadmin has to prescribe how\n  much swap is needed a priori, for each combination of\n  (memory size x disk space x workload usage). It is even more\n  complicated when we take into account the variance of memory\n  compression, which changes the reclaim dynamics (and as a result,\n  swap space size requirement). The problem is further exarcebated for\n  users who rely on swap utilization (and exhaustion) as an OOM signal.\n\n  All of these factors make it very difficult to configure the swapfile\n  for zswap: too small of a swapfile and we risk preventable OOMs and\n  limit the memory saving potentials of zswap; too big of a swapfile\n  and we waste disk space and memory due to swap metadata overhead.\n  This dilemma becomes more drastic in high memory systems, which can\n  have up to TBs worth of memory.\n\nPast attempts to decouple disk and compressed swap backends, namely the\nghost swapfile approach (see [13]), as well as the alternative\ncompressed swap backend zram, have mainly focused on eliminating the\ndisk space usage of compressed backends. We want a solution that not\nonly tackles that same problem, but also achieve the dyamicization of\nswap space to maximize the memory saving potentials while reducing\noperational and static memory overhead.\n\nFinally, any swap redesign should support efficient backend transfer,\ni.e without having to perform the expensive page table walk to\nupdate all the PTEs that refer to the swap entry:\n* The main motivation for this requirement is zswap writeback. To quote\n  Johannes (from [14]): \"Combining compression with disk swap is\n  extremely powerful, because it dramatically reduces the worst aspects\n  of both: it reduces the memory footprint of compression by shedding\n  the coldest data to disk; it reduces the IO latencies and flash wear\n  of disk swap through the writeback cache. In practice, this reduces\n  *average event rates of the entire reclaim/paging/IO stack*.\"\n* Another motivation is to simplify swapoff, which is both complicated\n  and expensive in the current design, precisely because we are storing\n  an encoding of the backend positional information in the page table,\n  and thus requires a full page table walk to remove these references.\n\n\nII. High Level Design Overview\n\nTo fix the aforementioned issues, we need an abstraction that separates\na swap entry from its physical backing storage. IOW, we need to\n\\u201cvirtualize\\u201d the swap space: swap clients will work with a dynamically\nallocated virtual swap slot, storing it in page table entries, and\nusing it to index into various swap-related data structures. The\nbacking storage is decoupled from the virtual swap slot, and the newly\nintroduced layer will \\u201cresolve\\u201d the virtual swap slot to the actual\nstorage. This layer also manages other metadata of the swap entry, such\nas its lifetime information (swap count), via a dynamically allocated,\nper-swap-entry descriptor:\n\nstruct swp_desc {\n        union {\n                swp_slot_t         slot;                 /*     0     8 */\n                struct zswap_entry * zswap_entry;        /*     0     8 */\n        };                                               /*     0     8 */\n        union {\n                struct folio *     swap_cache;           /*     8     8 */\n                void *             shadow;               /*     8     8 */\n        };                                               /*     8     8 */\n        unsigned int               swap_count;           /*    16     4 */\n        unsigned short             memcgid:16;           /*    20: 0  2 */\n        bool                       in_swapcache:1;       /*    22: 0  1 */\n\n        /* Bitfield combined with previous fields */\n\n        enum swap_type             type:2;               /*    20:17  4 */\n\n        /* size: 24, cachelines: 1, members: 6 */\n        /* bit_padding: 13 bits */\n        /* last cacheline: 24 bytes */\n};\n\n(output from pahole).\n\nThis design allows us to:\n* Decouple zswap (and zeromapped swap entry) from backing swapfile:\n  simply associate the virtual swap slot with one of the supported\n  backends: a zswap entry, a zero-filled swap page, a slot on the\n  swapfile, or an in-memory page.\n* Simplify and optimize swapoff: we only have to fault the page in and\n  have the virtual swap slot points to the page instead of the on-disk\n  physical swap slot. No need to perform any page table walking.\n\nThe size of the virtual swap descriptor is 24 bytes. Note that this is\nnot all \"new\" overhead, as the swap descriptor will replace:\n* the swap_cgroup arrays (one per swap type) in the old design, which\n  is a massive source of static memory overhead. With the new design,\n  it is only allocated for used clusters.\n* the swap tables, which holds the swap cache and workingset shadows.\n* the zeromap bitmap, which is a bitmap of physical swap slots to\n  indicate whether the swapped out page is zero-filled or not.\n* huge chunk of the swap_map. The swap_map is now replaced by 2 bitmaps,\n  one for allocated slots, and one for bad slots, representing 3 possible\n  states of a slot on the swapfile: allocated, free, and bad.\n* the zswap tree.\n\nSo, in terms of additional memory overhead:\n* For zswap entries, the added memory overhead is rather minimal. The\n  new indirection pointer neatly replaces the existing zswap tree.\n  We really only incur less than one word of overhead for swap count\n  blow up (since we no longer use swap continuation) and the swap type.\n* For physical swap entries, the new design will impose fewer than 3 words\n  memory overhead. However, as noted above this overhead is only for\n  actively used swap entries, whereas in the current design the overhead is\n  static (including the swap cgroup array for example).\n\n  The primary victim of this overhead will be zram users. However, as\n  zswap now no longer takes up disk space, zram users can consider\n  switching to zswap (which, as a bonus, has a lot of useful features\n  out of the box, such as cgroup tracking, dynamic zswap pool sizing,\n  LRU-ordering writeback, etc.).\n\nFor a more concrete example, suppose we have a 32 GB swapfile (i.e.\n8,388,608 swap entries), and we use zswap.\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 0.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 48.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 96.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 121.00 MB\n* Vswap total overhead: 144.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 153.00 MB\n* Vswap total overhead: 193.00 MB\n\nSo even in the worst case scenario for virtual swap, i.e when we\nsomehow have an oracle to correctly size the swapfile for zswap\npool to 32 GB, the added overhead is only 40 MB, which is a mere\n0.12% of the total swapfile :)\n\nIn practice, the overhead will be closer to the 50-75% usage case, as\nsystems tend to leave swap headroom for pathological events or sudden\nspikes in memory requirements. The added overhead in these cases are\npractically neglible. And in deployments where swapfiles for zswap\nare previously sparsely used, switching over to virtual swap will\nactually reduce memory overhead.\n\nDoing the same math for the disk swap, which is the worst case for\nvirtual swap in terms of swap backends:\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 2.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 41.00 MB\n* Vswap total overhead: 66.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 130.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 73.00 MB\n* Vswap total overhead: 194.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 259.00 MB\n\nThe added overhead is 170MB, which is 0.5% of the total swapfile size,\nagain in the worst case when we have a sizing oracle.\n\nPlease see the attached patches for more implementation details.\n\n\nIII. Usage and Benchmarking\n\nThis patch series introduce no new syscalls or userspace API. Existing\nuserspace setups will work as-is, except we no longer have to create a\nswapfile or set memory.swap.max if we want to use zswap, as zswap is no\nlonger tied to physical swap. The zswap pool will be automatically and\ndynamically sized based on memory usage and reclaim dynamics.\n\nTo measure the performance of the new implementation, I have run the\nfollowing benchmarks:\n\n1. Kernel building: 52 workers (one per processor), memory.max = 3G.\n\nUsing zswap as the backend:\n\nBaseline:\nreal: mean: 185.2s, stdev: 0.93s\nsys: mean: 683.7s, stdev: 33.77s\n\nVswap:\nreal: mean: 184.88s, stdev: 0.57s\nsys: mean: 675.14s, stdev: 32.8s\n\nWe actually see a slight improvement in systime (by 1.5%) :) This is\nlikely because we no longer have to perform swap charging for zswap\nentries, and virtual swap allocator is simpler than that of physical\nswap.\n\nUsing SSD swap as the backend:\n\nBaseline:\nreal: mean: 200.3s, stdev: 2.33s\nsys: mean: 489.88s, stdev: 9.62s\n\nVswap:\nreal: mean: 201.47s, stdev: 2.98s\nsys: mean: 487.36s, stdev: 5.53s\n\nThe performance is neck-to-neck.\n\n\nIV. Future Use Cases\n\nWhile the patch series focus on two applications (decoupling swap\nbackends and swapoff optimization/simplification), this new,\nfuture-proof design also allows us to implement new swap features more\neasily and efficiently:\n\n* Multi-tier swapping (as mentioned in [5]), with transparent\n  transferring (promotion/demotion) of pages across tiers (see [8] and\n  [9]). Similar to swapoff, with the old design we would need to\n  perform the expensive page table walk.\n* Swapfile compaction to alleviate fragmentation (as proposed by Ying\n  Huang in [6]).\n* Mixed backing THP swapin (see [7]): Once you have pinned down the\n  backing store of THPs, then you can dispatch each range of subpages\n  to appropriate backend swapin handler.\n* Swapping a folio out with discontiguous physical swap slots\n  (see [10]).\n* Zswap writeback optimization: The current architecture pre-reserves\n  physical swap space for pages when they enter the zswap pool, giving\n  the kernel no flexibility at writeback time. With the virtual swap\n  implementation, the backends are decoupled, and physical swap space\n  is allocated on-demand at writeback time, at which point we can make\n  much smarter decisions: we can batch multiple zswap writeback\n  operations into a single IO request, allocating contiguous physical\n  swap slots for that request. We can even perform compressed writeback\n  (i.e writing these pages without decompressing them) (see [12]).\n\n\nV. References\n\n[1]: https://lore.kernel.org/all/CAJD7tkbCnXJ95Qow_aOjNX6NOMU5ovMSHRC+95U4wtW6cM+puw@mail.gmail.com/\n[2]: https://lwn.net/Articles/932077/\n[3]: https://www.youtube.com/watch?v=Hwqw_TBGEhg\n[4]: https://lore.kernel.org/all/Zqe_Nab-Df1CN7iW@infradead.org/\n[5]: https://lore.kernel.org/lkml/CAF8kJuN-4UE0skVHvjUzpGefavkLULMonjgkXUZSBVJrcGFXCA@mail.gmail.com/\n[6]: https://lore.kernel.org/linux-mm/87o78mzp24.fsf@yhuang6-desk2.ccr.corp.intel.com/\n[7]: https://lore.kernel.org/all/CAGsJ_4ysCN6f7qt=6gvee1x3ttbOnifGneqcRm9Hoeun=uFQ2w@mail.gmail.com/\n[8]: https://lore.kernel.org/linux-mm/4DA25039.3020700@redhat.com/\n[9]: https://lore.kernel.org/all/CA+ZsKJ7DCE8PMOSaVmsmYZL9poxK6rn0gvVXbjpqxMwxS2C9TQ@mail.gmail.com/\n[10]: https://lore.kernel.org/all/CACePvbUkMYMencuKfpDqtG1Ej7LiUS87VRAXb8sBn1yANikEmQ@mail.gmail.com/\n[11]: https://lore.kernel.org/all/CAMgjq7BvQ0ZXvyLGp2YP96+i+6COCBBJCYmjXHGBnfisCAb8VA@mail.gmail.com/\n[12]: https://lore.kernel.org/linux-mm/ZeZSDLWwDed0CgT3@casper.infradead.org/\n[13]: https://lore.kernel.org/all/20251121-ghost-v1-1-cfc0efcf3855@kernel.org/\n[14]: https://lore.kernel.org/linux-mm/20251202170222.GD430226@cmpxchg.org/\n\nNhat Pham (20):\n  mm/swap: decouple swap cache from physical swap infrastructure\n  swap: rearrange the swap header file\n  mm: swap: add an abstract API for locking out swapoff\n  zswap: add new helpers for zswap entry operations\n  mm/swap: add a new function to check if a swap entry is in swap\n    cached.\n  mm: swap: add a separate type for physical swap slots\n  mm: create scaffolds for the new virtual swap implementation\n  zswap: prepare zswap for swap virtualization\n  mm: swap: allocate a virtual swap slot for each swapped out page\n  swap: move swap cache to virtual swap descriptor\n  zswap: move zswap entry management to the virtual swap descriptor\n  swap: implement the swap_cgroup API using virtual swap\n  swap: manage swap entry lifecycle at the virtual swap layer\n  mm: swap: decouple virtual swap slot from backing store\n  zswap: do not start zswap shrinker if there is no physical swap slots\n  swap: do not unnecesarily pin readahead swap entries\n  swapfile: remove zeromap bitmap\n  memcg: swap: only charge physical swap slots\n  swap: simplify swapoff using virtual swap\n  swapfile: replace the swap map with bitmaps\n\n Documentation/mm/swap-table.rst |   69 --\n MAINTAINERS                     |    2 +\n include/linux/cpuhotplug.h      |    1 +\n include/linux/mm_types.h        |   16 +\n include/linux/shmem_fs.h        |    7 +-\n include/linux/swap.h            |  135 ++-\n include/linux/swap_cgroup.h     |   13 -\n include/linux/swapops.h         |   25 +\n include/linux/zswap.h           |   17 +-\n kernel/power/swap.c             |    6 +-\n mm/Makefile                     |    5 +-\n mm/huge_memory.c                |   11 +-\n mm/internal.h                   |   12 +-\n mm/memcontrol-v1.c              |    6 +\n mm/memcontrol.c                 |  142 ++-\n mm/memory.c                     |  101 +-\n mm/migrate.c                    |   13 +-\n mm/mincore.c                    |   15 +-\n mm/page_io.c                    |   83 +-\n mm/shmem.c                      |  215 +---\n mm/swap.h                       |  157 +--\n mm/swap_cgroup.c                |  172 ---\n mm/swap_state.c                 |  306 +----\n mm/swap_table.h                 |   78 +-\n mm/swapfile.c                   | 1518 ++++-------------------\n mm/userfaultfd.c                |   18 +-\n mm/vmscan.c                     |   28 +-\n mm/vswap.c                      | 2025 +++++++++++++++++++++++++++++++\n mm/zswap.c                      |  142 +--\n 29 files changed, 2853 insertions(+), 2485 deletions(-)\n delete mode 100644 Documentation/mm/swap-table.rst\n delete mode 100644 mm/swap_cgroup.c\n create mode 100644 mm/vswap.c\n\n\nbase-commit: 05f7e89ab9731565d8a62e3b5d1ec206485eeb0b\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author apologized for a missing cover letter in their original patch series and explained that they are resending the first patch, which implements virtual swap space based on Yosry's proposals at LSFMMBPF 2023. They mentioned that lock contention issues have been resolved and performance is now on-par with baseline, but did not specifically address any feedback or concerns from reviewers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "apology",
                "clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "My sincerest apologies - it seems like the cover letter (and just the\ncover letter) fails to be sent out, for some reason. I'm trying to figure\nout what happened - it works when I send the entire patch series to\nmyself...\n\nAnyway, resending this (in-reply-to patch 1 of the series):\n\nChangelog:\n* RFC v2 -> v3:\n    * Implement a cluster-based allocation algorithm for virtual swap\n      slots, inspired by Kairui Song and Chris Li's implementation, as\n      well as Johannes Weiner's suggestions. This eliminates the lock\n\t  contention issues on the virtual swap layer.\n    * Re-use swap table for the reverse mapping.\n    * Remove CONFIG_VIRTUAL_SWAP.\n    * Reducing the size of the swap descriptor from 48 bytes to 24\n      bytes, i.e another 50% reduction in memory overhead from v2.\n    * Remove swap cache and zswap tree and use the swap descriptor\n      for this.\n    * Remove zeromap, and replace the swap_map bytemap with 2 bitmaps\n      (one for allocated slots, and one for bad slots).\n    * Rebase on top of 6.19 (7d0a66e4bb9081d75c82ec4957c50034cb0ea449)\n\t* Update cover letter to include new benchmark results and discussion\n\t  on overhead in various cases.\n* RFC v1 -> RFC v2:\n    * Use a single atomic type (swap_refs) for reference counting\n      purpose. This brings the size of the swap descriptor from 64 B\n      down to 48 B (25% reduction). Suggested by Yosry Ahmed.\n    * Zeromap bitmap is removed in the virtual swap implementation.\n      This saves one bit per phyiscal swapfile slot.\n    * Rearrange the patches and the code change to make things more\n      reviewable. Suggested by Johannes Weiner.\n    * Update the cover letter a bit.\n\nThis patch series implements the virtual swap space idea, based on Yosry's\nproposals at LSFMMBPF 2023 (see [1], [2], [3]), as well as valuable\ninputs from Johannes Weiner. The same idea (with different\nimplementation details) has been floated by Rik van Riel since at least\n2011 (see [8]).\n\nThis patch series is based on 6.19. There are a couple more\nswap-related changes in the mm-stable branch that I would need to\ncoordinate with, but I would like to send this out as an update, to show\nthat the lock contention issues that plagued earlier versions have been\nresolved and performance on the kernel build benchmark is now on-par with\nbaseline. Furthermore, memory overhead has been substantially reduced\ncompared to the last RFC version.\n\n\nI. Motivation\n\nCurrently, when an anon page is swapped out, a slot in a backing swap\ndevice is allocated and stored in the page table entries that refer to\nthe original page. This slot is also used as the \"key\" to find the\nswapped out content, as well as the index to swap data structures, such\nas the swap cache, or the swap cgroup mapping. Tying a swap entry to its\nbacking slot in this way is performant and efficient when swap is purely\njust disk space, and swapoff is rare.\n\nHowever, the advent of many swap optimizations has exposed major\ndrawbacks of this design. The first problem is that we occupy a physical\nslot in the swap space, even for pages that are NEVER expected to hit\nthe disk: pages compressed and stored in the zswap pool, zero-filled\npages, or pages rejected by both of these optimizations when zswap\nwriteback is disabled. This is the arguably central shortcoming of\nzswap:\n* In deployments when no disk space can be afforded for swap (such as\n  mobile and embedded devices), users cannot adopt zswap, and are forced\n  to use zram. This is confusing for users, and creates extra burdens\n  for developers, having to develop and maintain similar features for\n  two separate swap backends (writeback, cgroup charging, THP support,\n  etc.). For instance, see the discussion in [4].\n* Resource-wise, it is hugely wasteful in terms of disk usage. At Meta,\n  we have swapfile in the order of tens to hundreds of GBs, which are\n  mostly unused and only exist to enable zswap usage and zero-filled\n  pages swap optimizations.\n* Tying zswap (and more generally, other in-memory swap backends) to\n  the current physical swapfile infrastructure makes zswap implicitly\n  statically sized. This does not make sense, as unlike disk swap, in\n  which we consume a limited resource (disk space or swapfile space) to\n  save another resource (memory), zswap consume the same resource it is\n  saving (memory). The more we zswap, the more memory we have available,\n  not less. We are not rationing a limited resource when we limit\n  the size of he zswap pool, but rather we are capping the resource\n  (memory) saving potential of zswap. Under memory pressure, using\n  more zswap is almost always better than the alternative (disk IOs, or\n  even worse, OOMs), and dynamically sizing the zswap pool on demand\n  allows the system to flexibly respond to these precarious scenarios.\n* Operationally, static provisioning the swapfile for zswap pose\n  significant challenges, because the sysadmin has to prescribe how\n  much swap is needed a priori, for each combination of\n  (memory size x disk space x workload usage). It is even more\n  complicated when we take into account the variance of memory\n  compression, which changes the reclaim dynamics (and as a result,\n  swap space size requirement). The problem is further exarcebated for\n  users who rely on swap utilization (and exhaustion) as an OOM signal.\n\n  All of these factors make it very difficult to configure the swapfile\n  for zswap: too small of a swapfile and we risk preventable OOMs and\n  limit the memory saving potentials of zswap; too big of a swapfile\n  and we waste disk space and memory due to swap metadata overhead.\n  This dilemma becomes more drastic in high memory systems, which can\n  have up to TBs worth of memory.\n\nPast attempts to decouple disk and compressed swap backends, namely the\nghost swapfile approach (see [13]), as well as the alternative\ncompressed swap backend zram, have mainly focused on eliminating the\ndisk space usage of compressed backends. We want a solution that not\nonly tackles that same problem, but also achieve the dyamicization of\nswap space to maximize the memory saving potentials while reducing\noperational and static memory overhead.\n\nFinally, any swap redesign should support efficient backend transfer,\ni.e without having to perform the expensive page table walk to\nupdate all the PTEs that refer to the swap entry:\n* The main motivation for this requirement is zswap writeback. To quote\n  Johannes (from [14]): \"Combining compression with disk swap is\n  extremely powerful, because it dramatically reduces the worst aspects\n  of both: it reduces the memory footprint of compression by shedding\n  the coldest data to disk; it reduces the IO latencies and flash wear\n  of disk swap through the writeback cache. In practice, this reduces\n  *average event rates of the entire reclaim/paging/IO stack*.\"\n* Another motivation is to simplify swapoff, which is both complicated\n  and expensive in the current design, precisely because we are storing\n  an encoding of the backend positional information in the page table,\n  and thus requires a full page table walk to remove these references.\n\n\nII. High Level Design Overview\n\nTo fix the aforementioned issues, we need an abstraction that separates\na swap entry from its physical backing storage. IOW, we need to\n\\u201cvirtualize\\u201d the swap space: swap clients will work with a dynamically\nallocated virtual swap slot, storing it in page table entries, and\nusing it to index into various swap-related data structures. The\nbacking storage is decoupled from the virtual swap slot, and the newly\nintroduced layer will \\u201cresolve\\u201d the virtual swap slot to the actual\nstorage. This layer also manages other metadata of the swap entry, such\nas its lifetime information (swap count), via a dynamically allocated,\nper-swap-entry descriptor:\n\nstruct swp_desc {\n        union {\n                swp_slot_t         slot;                 /*     0     8 */\n                struct zswap_entry * zswap_entry;        /*     0     8 */\n        };                                               /*     0     8 */\n        union {\n                struct folio *     swap_cache;           /*     8     8 */\n                void *             shadow;               /*     8     8 */\n        };                                               /*     8     8 */\n        unsigned int               swap_count;           /*    16     4 */\n        unsigned short             memcgid:16;           /*    20: 0  2 */\n        bool                       in_swapcache:1;       /*    22: 0  1 */\n\n        /* Bitfield combined with previous fields */\n\n        enum swap_type             type:2;               /*    20:17  4 */\n\n        /* size: 24, cachelines: 1, members: 6 */\n        /* bit_padding: 13 bits */\n        /* last cacheline: 24 bytes */\n};\n\n(output from pahole).\n\nThis design allows us to:\n* Decouple zswap (and zeromapped swap entry) from backing swapfile:\n  simply associate the virtual swap slot with one of the supported\n  backends: a zswap entry, a zero-filled swap page, a slot on the\n  swapfile, or an in-memory page.\n* Simplify and optimize swapoff: we only have to fault the page in and\n  have the virtual swap slot points to the page instead of the on-disk\n  physical swap slot. No need to perform any page table walking.\n\nThe size of the virtual swap descriptor is 24 bytes. Note that this is\nnot all \"new\" overhead, as the swap descriptor will replace:\n* the swap_cgroup arrays (one per swap type) in the old design, which\n  is a massive source of static memory overhead. With the new design,\n  it is only allocated for used clusters.\n* the swap tables, which holds the swap cache and workingset shadows.\n* the zeromap bitmap, which is a bitmap of physical swap slots to\n  indicate whether the swapped out page is zero-filled or not.\n* huge chunk of the swap_map. The swap_map is now replaced by 2 bitmaps,\n  one for allocated slots, and one for bad slots, representing 3 possible\n  states of a slot on the swapfile: allocated, free, and bad.\n* the zswap tree.\n\nSo, in terms of additional memory overhead:\n* For zswap entries, the added memory overhead is rather minimal. The\n  new indirection pointer neatly replaces the existing zswap tree.\n  We really only incur less than one word of overhead for swap count\n  blow up (since we no longer use swap continuation) and the swap type.\n* For physical swap entries, the new design will impose fewer than 3 words\n  memory overhead. However, as noted above this overhead is only for\n  actively used swap entries, whereas in the current design the overhead is\n  static (including the swap cgroup array for example).\n\n  The primary victim of this overhead will be zram users. However, as\n  zswap now no longer takes up disk space, zram users can consider\n  switching to zswap (which, as a bonus, has a lot of useful features\n  out of the box, such as cgroup tracking, dynamic zswap pool sizing,\n  LRU-ordering writeback, etc.).\n\nFor a more concrete example, suppose we have a 32 GB swapfile (i.e.\n8,388,608 swap entries), and we use zswap.\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 0.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 48.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 96.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 121.00 MB\n* Vswap total overhead: 144.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 153.00 MB\n* Vswap total overhead: 193.00 MB\n\nSo even in the worst case scenario for virtual swap, i.e when we\nsomehow have an oracle to correctly size the swapfile for zswap\npool to 32 GB, the added overhead is only 40 MB, which is a mere\n0.12% of the total swapfile :)\n\nIn practice, the overhead will be closer to the 50-75% usage case, as\nsystems tend to leave swap headroom for pathological events or sudden\nspikes in memory requirements. The added overhead in these cases are\npractically neglible. And in deployments where swapfiles for zswap\nare previously sparsely used, switching over to virtual swap will\nactually reduce memory overhead.\n\nDoing the same math for the disk swap, which is the worst case for\nvirtual swap in terms of swap backends:\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 2.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 41.00 MB\n* Vswap total overhead: 66.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 130.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 73.00 MB\n* Vswap total overhead: 194.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 259.00 MB\n\nThe added overhead is 170MB, which is 0.5% of the total swapfile size,\nagain in the worst case when we have a sizing oracle.\n\nPlease see the attached patches for more implementation details.\n\n\nIII. Usage and Benchmarking\n\nThis patch series introduce no new syscalls or userspace API. Existing\nuserspace setups will work as-is, except we no longer have to create a\nswapfile or set memory.swap.max if we want to use zswap, as zswap is no\nlonger tied to physical swap. The zswap pool will be automatically and\ndynamically sized based on memory usage and reclaim dynamics.\n\nTo measure the performance of the new implementation, I have run the\nfollowing benchmarks:\n\n1. Kernel building: 52 workers (one per processor), memory.max = 3G.\n\nUsing zswap as the backend:\n\nBaseline:\nreal: mean: 185.2s, stdev: 0.93s\nsys: mean: 683.7s, stdev: 33.77s\n\nVswap:\nreal: mean: 184.88s, stdev: 0.57s\nsys: mean: 675.14s, stdev: 32.8s\n\nWe actually see a slight improvement in systime (by 1.5%) :) This is\nlikely because we no longer have to perform swap charging for zswap\nentries, and virtual swap allocator is simpler than that of physical\nswap.\n\nUsing SSD swap as the backend:\n\nBaseline:\nreal: mean: 200.3s, stdev: 2.33s\nsys: mean: 489.88s, stdev: 9.62s\n\nVswap:\nreal: mean: 201.47s, stdev: 2.98s\nsys: mean: 487.36s, stdev: 5.53s\n\nThe performance is neck-to-neck.\n\n\nIV. Future Use Cases\n\nWhile the patch series focus on two applications (decoupling swap\nbackends and swapoff optimization/simplification), this new,\nfuture-proof design also allows us to implement new swap features more\neasily and efficiently:\n\n* Multi-tier swapping (as mentioned in [5]), with transparent\n  transferring (promotion/demotion) of pages across tiers (see [8] and\n  [9]). Similar to swapoff, with the old design we would need to\n  perform the expensive page table walk.\n* Swapfile compaction to alleviate fragmentation (as proposed by Ying\n  Huang in [6]).\n* Mixed backing THP swapin (see [7]): Once you have pinned down the\n  backing store of THPs, then you can dispatch each range of subpages\n  to appropriate backend swapin handler.\n* Swapping a folio out with discontiguous physical swap slots\n  (see [10]).\n* Zswap writeback optimization: The current architecture pre-reserves\n  physical swap space for pages when they enter the zswap pool, giving\n  the kernel no flexibility at writeback time. With the virtual swap\n  implementation, the backends are decoupled, and physical swap space\n  is allocated on-demand at writeback time, at which point we can make\n  much smarter decisions: we can batch multiple zswap writeback\n  operations into a single IO request, allocating contiguous physical\n  swap slots for that request. We can even perform compressed writeback\n  (i.e writing these pages without decompressing them) (see [12]).\n\n\nV. References\n\n[1]: https://lore.kernel.org/all/CAJD7tkbCnXJ95Qow_aOjNX6NOMU5ovMSHRC+95U4wtW6cM+puw@mail.gmail.com/\n[2]: https://lwn.net/Articles/932077/\n[3]: https://www.youtube.com/watch?v=Hwqw_TBGEhg\n[4]: https://lore.kernel.org/all/Zqe_Nab-Df1CN7iW@infradead.org/\n[5]: https://lore.kernel.org/lkml/CAF8kJuN-4UE0skVHvjUzpGefavkLULMonjgkXUZSBVJrcGFXCA@mail.gmail.com/\n[6]: https://lore.kernel.org/linux-mm/87o78mzp24.fsf@yhuang6-desk2.ccr.corp.intel.com/\n[7]: https://lore.kernel.org/all/CAGsJ_4ysCN6f7qt=6gvee1x3ttbOnifGneqcRm9Hoeun=uFQ2w@mail.gmail.com/\n[8]: https://lore.kernel.org/linux-mm/4DA25039.3020700@redhat.com/\n[9]: https://lore.kernel.org/all/CA+ZsKJ7DCE8PMOSaVmsmYZL9poxK6rn0gvVXbjpqxMwxS2C9TQ@mail.gmail.com/\n[10]: https://lore.kernel.org/all/CACePvbUkMYMencuKfpDqtG1Ej7LiUS87VRAXb8sBn1yANikEmQ@mail.gmail.com/\n[11]: https://lore.kernel.org/all/CAMgjq7BvQ0ZXvyLGp2YP96+i+6COCBBJCYmjXHGBnfisCAb8VA@mail.gmail.com/\n[12]: https://lore.kernel.org/linux-mm/ZeZSDLWwDed0CgT3@casper.infradead.org/\n[13]: https://lore.kernel.org/all/20251121-ghost-v1-1-cfc0efcf3855@kernel.org/\n[14]: https://lore.kernel.org/linux-mm/20251202170222.GD430226@cmpxchg.org/\n\nNhat Pham (20):\n  mm/swap: decouple swap cache from physical swap infrastructure\n  swap: rearrange the swap header file\n  mm: swap: add an abstract API for locking out swapoff\n  zswap: add new helpers for zswap entry operations\n  mm/swap: add a new function to check if a swap entry is in swap\n    cached.\n  mm: swap: add a separate type for physical swap slots\n  mm: create scaffolds for the new virtual swap implementation\n  zswap: prepare zswap for swap virtualization\n  mm: swap: allocate a virtual swap slot for each swapped out page\n  swap: move swap cache to virtual swap descriptor\n  zswap: move zswap entry management to the virtual swap descriptor\n  swap: implement the swap_cgroup API using virtual swap\n  swap: manage swap entry lifecycle at the virtual swap layer\n  mm: swap: decouple virtual swap slot from backing store\n  zswap: do not start zswap shrinker if there is no physical swap slots\n  swap: do not unnecesarily pin readahead swap entries\n  swapfile: remove zeromap bitmap\n  memcg: swap: only charge physical swap slots\n  swap: simplify swapoff using virtual swap\n  swapfile: replace the swap map with bitmaps\n\n Documentation/mm/swap-table.rst |   69 --\n MAINTAINERS                     |    2 +\n include/linux/cpuhotplug.h      |    1 +\n include/linux/mm_types.h        |   16 +\n include/linux/shmem_fs.h        |    7 +-\n include/linux/swap.h            |  135 ++-\n include/linux/swap_cgroup.h     |   13 -\n include/linux/swapops.h         |   25 +\n include/linux/zswap.h           |   17 +-\n kernel/power/swap.c             |    6 +-\n mm/Makefile                     |    5 +-\n mm/huge_memory.c                |   11 +-\n mm/internal.h                   |   12 +-\n mm/memcontrol-v1.c              |    6 +\n mm/memcontrol.c                 |  142 ++-\n mm/memory.c                     |  101 +-\n mm/migrate.c                    |   13 +-\n mm/mincore.c                    |   15 +-\n mm/page_io.c                    |   83 +-\n mm/shmem.c                      |  215 +---\n mm/swap.h                       |  157 +--\n mm/swap_cgroup.c                |  172 ---\n mm/swap_state.c                 |  306 +----\n mm/swap_table.h                 |   78 +-\n mm/swapfile.c                   | 1518 ++++-------------------\n mm/userfaultfd.c                |   18 +-\n mm/vmscan.c                     |   28 +-\n mm/vswap.c                      | 2025 +++++++++++++++++++++++++++++++\n mm/zswap.c                      |  142 +--\n 29 files changed, 2853 insertions(+), 2485 deletions(-)\n delete mode 100644 Documentation/mm/swap-table.rst\n delete mode 100644 mm/swap_cgroup.c\n create mode 100644 mm/vswap.c\n\n\nbase-commit: 05f7e89ab9731565d8a62e3b5d1ec206485eeb0b\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that vswap_free() acquires the per-vswap spinlock while holding the folio lock, creating a potential deadlock with reclaim paths, and requested the lock be dropped before calling try_to_unmap().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential deadlock",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "For the record I did receive your original V3 cover letter from the\nlinux-mm mailing list.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li questioned the increased overhead of the per-swap slot entry in Nhat Pham's patch, pointing out that it jumps from 8 to 24 bytes, which he considers an unnecessary price compared to alternative implementations.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Is the per swap slot entry overhead 24 bytes in your implementation?\nThe current swap overhead is 3 static +8 dynamic, your 24 dynamic is a\nbig jump. You can argue that 8->24 is not a big jump . But it is an\nunnecessary price compared to the alternatives, which is 8 dynamic +\n4(optional redirect).",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li reported a compile error on Fedora 43, specifically an initialization from pointer to non-enclosed address space error in mm/vswap.c due to the use of local_lock() macro, and requested that the patch author mention this issue in their series description.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "compile_error",
                "requested_changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Ah, you need to mention that in the first line to Andrew. Spell out\nthis series is not for Andrew to consume in the MM series. It can't\nany way because it does not apply to mm-unstable nor mm-stable.\n\nBTW, I have the following compile error with this series (fedora 43).\nSame config compile fine on v6.19.\n\nIn file included from ./include/linux/local_lock.h:5,\n                 from ./include/linux/mmzone.h:24,\n                 from ./include/linux/gfp.h:7,\n                 from ./include/linux/mm.h:7,\n                 from mm/vswap.c:7:\nmm/vswap.c: In function vswap_cpu_dead:\n./include/linux/percpu-defs.h:221:45: error: initialization from\npointer to non-enclosed address space\n  221 |         const void __percpu *__vpp_verify = (typeof((ptr) +\n0))NULL;    \\\n      |                                             ^\n./include/linux/local_lock_internal.h:105:40: note: in definition of\nmacro __local_lock_acquire\n  105 |                 __l = (local_lock_t *)(lock);\n         \\\n      |                                        ^~~~\n./include/linux/local_lock.h:17:41: note: in expansion of macro\n__local_lock\n   17 | #define local_lock(lock)                __local_lock(this_cpu_ptr(lock))\n      |                                         ^~~~~~~~~~~~\n./include/linux/percpu-defs.h:245:9: note: in expansion of macro\n__verify_pcpu_ptr\n  245 |         __verify_pcpu_ptr(ptr);\n         \\\n      |         ^~~~~~~~~~~~~~~~~\n./include/linux/percpu-defs.h:256:27: note: in expansion of macro raw_cpu_ptr\n  256 | #define this_cpu_ptr(ptr) raw_cpu_ptr(ptr)\n      |                           ^~~~~~~~~~~\n./include/linux/local_lock.h:17:54: note: in expansion of macro\nthis_cpu_ptr\n   17 | #define local_lock(lock)\n__local_lock(this_cpu_ptr(lock))\n      |\n^~~~~~~~~~~~\nmm/vswap.c:1518:9: note: in expansion of macro local_lock\n 1518 |         local_lock(&percpu_cluster->lock);\n      |         ^~~~~~~~~~",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li requested additional information on user space time to provide a more comprehensive understanding of the patch's performance, specifically asking for the number of runs and standard deviation (stdev) for a specific value.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request_for_additional_info"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Can you show your user space time as well to complete the picture?\n\nHow many runs do you have for stdev 32.8s?",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li requested that the patch also include swap test data for zram, which is used by Android for swapping.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested additional test data"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Please include zram swap test data as well. Android heavily uses zram\nfor swapping.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suspects performance differences in the patch, specifically regarding swap testing and the need for others to confirm the results under stressful conditions near the OOM limit.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance",
                "OOM"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I strongly suspect there is some performance difference that hasn't\nbeen covered by your test yet. Need more conformation by others on the\nperformance measurement. The swap testing is tricky. You want to push\nto stress barely within the OOM limit. Need more data.\n\nChris",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "reviewer noted that vswap_free() acquires the per-vswap spinlock while holding the folio lock, creating a potential deadlock with reclaim paths, and requested the lock be dropped before calling try_to_unmap()",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential deadlock",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Chris,\n\nOn Mon, Feb 09, 2026 at 04:20:21AM -0800, Chris Li wrote:",
              "reply_to": "Chris Li",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Reviewer Johannes Weiner noted that the patch description's claim about reducing net overhead is incorrect, and provided a detailed breakdown of how the descriptor consolidates and eliminates other data structures.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested correction",
                "provided alternative explanation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, this is not the net overhead.\n\nThe descriptor consolidates and eliminates several other data\nstructures.\n\nHere is the more detailed breakdown:",
              "reply_to": "Chris Li",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Dan Carpenter",
              "summary": "The reviewer, Dan Carpenter, pointed out a potential bug in the vswap_alloc_swap_slot() function where the folio pointer is dereferenced before being checked for NULL. The issue arises from the fact that the function tries to access the swap entry of the folio without first checking if it's valid.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "bug",
                "potential issue"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Nhat,\n\nkernel test robot noticed the following build warnings:\n\nurl:    https://github.com/intel-lab-lkp/linux/commits/Nhat-Pham/mm-swap-decouple-swap-cache-from-physical-swap-infrastructure/20260209-120606\nbase:   05f7e89ab9731565d8a62e3b5d1ec206485eeb0b\npatch link:    https://lore.kernel.org/r/20260208215839.87595-15-nphamcs%40gmail.com\npatch subject: [PATCH v3 14/20] mm: swap: decouple virtual swap slot from backing store\nconfig: powerpc-randconfig-r073-20260209 (https://download.01.org/0day-ci/archive/20260209/202602092300.lZO4Ee4N-lkp@intel.com/config)\ncompiler: powerpc-linux-gcc (GCC) 15.2.0\nsmatch version: v0.5.0-8994-gd50c5a4c\n\nIf you fix the issue in a separate patch/commit (i.e. not just a new version of\nthe same patch/commit), kindly add following tags\n| Reported-by: kernel test robot <lkp@intel.com>\n| Reported-by: Dan Carpenter <dan.carpenter@linaro.org>\n| Closes: https://lore.kernel.org/r/202602092300.lZO4Ee4N-lkp@intel.com/\n\nsmatch warnings:\nmm/vswap.c:733 vswap_alloc_swap_slot() warn: variable dereferenced before check 'folio' (see line 701)\n\nvim +/folio +733 mm/vswap.c\n\n19a5fe94e9aae4 Nhat Pham 2026-02-08  694  bool vswap_alloc_swap_slot(struct folio *folio)\n19a5fe94e9aae4 Nhat Pham 2026-02-08  695  {\n19a5fe94e9aae4 Nhat Pham 2026-02-08  696  \tint i, nr = folio_nr_pages(folio);\n19a5fe94e9aae4 Nhat Pham 2026-02-08  697  \tstruct vswap_cluster *cluster = NULL;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  698  \tstruct swap_info_struct *si;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  699  \tstruct swap_cluster_info *ci;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  700  \tswp_slot_t slot = { .val = 0 };\n19a5fe94e9aae4 Nhat Pham 2026-02-08 @701  \tswp_entry_t entry = folio->swap;\n\nfolio dereference here\n\n19a5fe94e9aae4 Nhat Pham 2026-02-08  702  \tstruct swp_desc *desc;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  703  \tbool fallback = false;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  704  \n19a5fe94e9aae4 Nhat Pham 2026-02-08  705  \t/*\n19a5fe94e9aae4 Nhat Pham 2026-02-08  706  \t * We might have already allocated a backing physical swap slot in past\n19a5fe94e9aae4 Nhat Pham 2026-02-08  707  \t * attempts (for instance, when we disable zswap). If the entire range is\n19a5fe94e9aae4 Nhat Pham 2026-02-08  708  \t * already swapfile-backed we can skip swapfile case.\n19a5fe94e9aae4 Nhat Pham 2026-02-08  709  \t */\n19a5fe94e9aae4 Nhat Pham 2026-02-08  710  \tif (vswap_swapfile_backed(entry, nr))\n19a5fe94e9aae4 Nhat Pham 2026-02-08  711  \t\treturn true;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  712  \n19a5fe94e9aae4 Nhat Pham 2026-02-08  713  \tif (swap_slot_alloc(&slot, folio_order(folio)))\n\nand here\n\n19a5fe94e9aae4 Nhat Pham 2026-02-08  714  \t\treturn false;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  715  \n19a5fe94e9aae4 Nhat Pham 2026-02-08  716  \tif (!slot.val)\n19a5fe94e9aae4 Nhat Pham 2026-02-08  717  \t\treturn false;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  718  \n7f88e3ea20f231 Nhat Pham 2026-02-08  719  \t/* establish the vrtual <-> physical swap slots linkages. */\n7f88e3ea20f231 Nhat Pham 2026-02-08  720  \tsi = __swap_slot_to_info(slot);\n7f88e3ea20f231 Nhat Pham 2026-02-08  721  \tci = swap_cluster_lock(si, swp_slot_offset(slot));\n7f88e3ea20f231 Nhat Pham 2026-02-08  722  \tvswap_rmap_set(ci, slot, entry.val, nr);\n7f88e3ea20f231 Nhat Pham 2026-02-08  723  \tswap_cluster_unlock(ci);\n7f88e3ea20f231 Nhat Pham 2026-02-08  724  \n7f88e3ea20f231 Nhat Pham 2026-02-08  725  \trcu_read_lock();\n7f88e3ea20f231 Nhat Pham 2026-02-08  726  \tfor (i = 0; i < nr; i++) {\n7f88e3ea20f231 Nhat Pham 2026-02-08  727  \t\tdesc = vswap_iter(&cluster, entry.val + i);\n7f88e3ea20f231 Nhat Pham 2026-02-08  728  \t\tVM_WARN_ON(!desc);\n7f88e3ea20f231 Nhat Pham 2026-02-08  729  \n19a5fe94e9aae4 Nhat Pham 2026-02-08  730  \t\tif (desc->type == VSWAP_FOLIO) {\n19a5fe94e9aae4 Nhat Pham 2026-02-08  731  \t\t\t/* case 1: fallback from zswap store failure */\n19a5fe94e9aae4 Nhat Pham 2026-02-08  732  \t\t\tfallback = true;\n19a5fe94e9aae4 Nhat Pham 2026-02-08 @733  \t\t\tif (!folio)\n\nSo it can't be NULL here.\n\n19a5fe94e9aae4 Nhat Pham 2026-02-08  734  \t\t\t\tfolio = desc->swap_cache;\n\nSo we'll never do this assignment and it will never become NULL.\n\n19a5fe94e9aae4 Nhat Pham 2026-02-08  735  \t\t\telse\n19a5fe94e9aae4 Nhat Pham 2026-02-08  736  \t\t\t\tVM_WARN_ON(folio != desc->swap_cache);\n19a5fe94e9aae4 Nhat Pham 2026-02-08  737  \t\t} else {\n19a5fe94e9aae4 Nhat Pham 2026-02-08  738  \t\t\t/*\n19a5fe94e9aae4 Nhat Pham 2026-02-08  739  \t\t\t * Case 2: zswap writeback.\n19a5fe94e9aae4 Nhat Pham 2026-02-08  740  \t\t\t *\n19a5fe94e9aae4 Nhat Pham 2026-02-08  741  \t\t\t * No need to free zswap entry here - it will be freed once zswap\n19a5fe94e9aae4 Nhat Pham 2026-02-08  742  \t\t\t * writeback suceeds.\n19a5fe94e9aae4 Nhat Pham 2026-02-08  743  \t\t\t */\n19a5fe94e9aae4 Nhat Pham 2026-02-08  744  \t\t\tVM_WARN_ON(desc->type != VSWAP_ZSWAP);\n19a5fe94e9aae4 Nhat Pham 2026-02-08  745  \t\t\tVM_WARN_ON(fallback);\n19a5fe94e9aae4 Nhat Pham 2026-02-08  746  \t\t}\n19a5fe94e9aae4 Nhat Pham 2026-02-08  747  \t\tdesc->type = VSWAP_SWAPFILE;\n7f88e3ea20f231 Nhat Pham 2026-02-08  748  \t\tdesc->slot.val = slot.val + i;\n7f88e3ea20f231 Nhat Pham 2026-02-08  749  \t}\n7f88e3ea20f231 Nhat Pham 2026-02-08  750  \tspin_unlock(&cluster->lock);\n7f88e3ea20f231 Nhat Pham 2026-02-08  751  \trcu_read_unlock();\n19a5fe94e9aae4 Nhat Pham 2026-02-08  752  \treturn true;\n7f88e3ea20f231 Nhat Pham 2026-02-08  753  }\n\n-- \n0-DAY CI Kernel Test Service\nhttps://github.com/intel/lkp-tests/wiki",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "syzbot ci",
              "summary": "The reviewer detected a possible deadlock in the vswap_iter function due to the acquisition of the per-vswap spinlock while holding the folio lock, creating a lock ordering violation with reclaim paths.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "deadlock",
                "lock ordering violation"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Tested-by"
              ],
              "raw_body": "syzbot ci has tested the following series\n\n[v3] Virtual Swap Space\nhttps://lore.kernel.org/all/20260208215839.87595-1-nphamcs@gmail.com\n* [PATCH v3 01/20] mm/swap: decouple swap cache from physical swap infrastructure\n* [PATCH v3 02/20] swap: rearrange the swap header file\n* [PATCH v3 03/20] mm: swap: add an abstract API for locking out swapoff\n* [PATCH v3 04/20] zswap: add new helpers for zswap entry operations\n* [PATCH v3 05/20] mm/swap: add a new function to check if a swap entry is in swap cached.\n* [PATCH v3 06/20] mm: swap: add a separate type for physical swap slots\n* [PATCH v3 07/20] mm: create scaffolds for the new virtual swap implementation\n* [PATCH v3 08/20] zswap: prepare zswap for swap virtualization\n* [PATCH v3 09/20] mm: swap: allocate a virtual swap slot for each swapped out page\n* [PATCH v3 10/20] swap: move swap cache to virtual swap descriptor\n* [PATCH v3 11/20] zswap: move zswap entry management to the virtual swap descriptor\n* [PATCH v3 12/20] swap: implement the swap_cgroup API using virtual swap\n* [PATCH v3 13/20] swap: manage swap entry lifecycle at the virtual swap layer\n* [PATCH v3 14/20] mm: swap: decouple virtual swap slot from backing store\n* [PATCH v3 15/20] zswap: do not start zswap shrinker if there is no physical swap slots\n* [PATCH v3 16/20] swap: do not unnecesarily pin readahead swap entries\n* [PATCH v3 17/20] swapfile: remove zeromap bitmap\n* [PATCH v3 18/20] memcg: swap: only charge physical swap slots\n* [PATCH v3 19/20] swap: simplify swapoff using virtual swap\n* [PATCH v3 20/20] swapfile: replace the swap map with bitmaps\n\nand found the following issue:\npossible deadlock in vswap_iter\n\nFull report is available here:\nhttps://ci.syzbot.org/series/b9defda6-daec-4c41-bbf9-7d3b7fabd7cb\n\n***\n\npossible deadlock in vswap_iter\n\ntree:      bpf\nURL:       https://kernel.googlesource.com/pub/scm/linux/kernel/git/bpf/bpf.git\nbase:      05f7e89ab9731565d8a62e3b5d1ec206485eeb0b\narch:      amd64\ncompiler:  Debian clang version 21.1.8 (++20251221033036+2078da43e25a-1~exp1~20251221153213.50), Debian LLD 21.1.8\nconfig:    https://ci.syzbot.org/builds/f444cfbe-4ce0-4917-94aa-3a8bd96ee376/config\nC repro:   https://ci.syzbot.org/findings/7b8c50b1-47d6-42e0-bcfc-814e7b3bb596/c_repro\nsyz repro: https://ci.syzbot.org/findings/7b8c50b1-47d6-42e0-bcfc-814e7b3bb596/syz_repro\n\nloop0: detected capacity change from 0 to 764\n============================================\nWARNING: possible recursive locking detected\nsyzkaller #0 Not tainted\n--------------------------------------------\nsyz-executor625/5806 is trying to acquire lock:\nffff88811884c018 (&cluster->lock){+.+.}-{3:3}, at: spin_lock include/linux/spinlock.h:351 [inline]\nffff88811884c018 (&cluster->lock){+.+.}-{3:3}, at: vswap_iter+0xfa/0x1b0 mm/vswap.c:274\n\nbut task is already holding lock:\nffff88811884c018 (&cluster->lock){+.+.}-{3:3}, at: spin_lock_irq include/linux/spinlock.h:376 [inline]\nffff88811884c018 (&cluster->lock){+.+.}-{3:3}, at: swap_cache_lock_irq+0xe2/0x190 mm/vswap.c:1586\n\nother info that might help us debug this:\n Possible unsafe locking scenario:\n\n       CPU0\n       ----\n  lock(&cluster->lock);\n  lock(&cluster->lock);\n\n *** DEADLOCK ***\n\n May be due to missing lock nesting notation\n\n3 locks held by syz-executor625/5806:\n #0: ffff888174bc2800 (&mm->mmap_lock){++++}-{4:4}, at: mmap_read_lock include/linux/mmap_lock.h:391 [inline]\n #0: ffff888174bc2800 (&mm->mmap_lock){++++}-{4:4}, at: madvise_lock+0x152/0x2e0 mm/madvise.c:1789\n #1: ffff88811884c018 (&cluster->lock){+.+.}-{3:3}, at: spin_lock_irq include/linux/spinlock.h:376 [inline]\n #1: ffff88811884c018 (&cluster->lock){+.+.}-{3:3}, at: swap_cache_lock_irq+0xe2/0x190 mm/vswap.c:1586\n #2: ffffffff8e55a360 (rcu_read_lock){....}-{1:3}, at: rcu_lock_acquire include/linux/rcupdate.h:331 [inline]\n #2: ffffffff8e55a360 (rcu_read_lock){....}-{1:3}, at: rcu_read_lock include/linux/rcupdate.h:867 [inline]\n #2: ffffffff8e55a360 (rcu_read_lock){....}-{1:3}, at: vswap_cgroup_record+0x40/0x290 mm/vswap.c:1925\n\nstack backtrace:\n\n\n***\n\nIf these findings have caused you to resend the series or submit a\nseparate fix, please add the following tag to your commit message:\n  Tested-by: syzbot@syzkaller.appspotmail.com\n\n---\nThis report is generated by a bot. It may contain errors.\nsyzbot ci engineers can be reached at syzkaller@googlegroups.com.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song expressed concerns that the patch fundamentally changes the swap workflow and introduces many behavior changes at once, which may lead to performance or memory usage regressions for some workloads.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "performance or memory usage regression"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I really do think we better make this optional, not a replacement or\nmandatory. There are many hard to evaluate effects as this\nfundamentally changes the swap workflow with a lot of behavior changes\nat once. e.g. it seems the folio will be reactivated instead of\nsplitted if the physical swap device is fragmented; slot is allocated\nat IO and not at unmap, and maybe many others. Just like zswap is\noptional. Some common workloads would see an obvious performance or\nmemory usage regression following this design, see below.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song expressed concern that further simplification of the patch may lead to reimplementing the swap table format, suggesting a potential loss of functionality.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Honestly if you keep reducing that you might just end up\nreimplementing the swap table format :)",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that the patch's concerns about poor batching behavior in vswap free path may not be significant, as most cases will have a 1:1 virtual swap setup and static overhead will be trivial.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "reconsidering original assessment",
                "downplaying potential issue"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "So I thought about it again, this one seems not to be an issue. In\nmost cases, having a 1:1 virtual swap setup is enough, and very soon\nthe static overhead will be really trivial. There won't even be any\nfragmentation issue either, since if the physical memory size is\nidentical to swap space, then you can always find a matching part. And\nbesides, dynamic growth of swap files is actually very doable and\nuseful, that will make physical swap files adjustable at runtime, so\nusers won't need to waste a swap type id to extend physical swap\nspace.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that the new vswap free path does not perform a clean swapoff, as minor faults are still triggered afterwards and metadata is not released, which can impact performance.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance",
                "metadata"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The swapoff here is not really a clean swapoff, minor faults will\nstill be triggered afterwards, and metadata is not released. So this\nnew swapoff cannot really guarantee the same performance as the old\nswapoff. And on the other hand we can already just read everything\ninto the swap cache then ignore the page table walk with the older\ndesign too, that's just not a clean swapoff.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that a standalone bit for swap cache is similar to the problematic SWAP_HAS_CACHE, which caused several issues in the past.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "similarities to problematic code",
                "caused issues"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "A standalone bit for swapcache looks like the old SWAP_HAS_CACHE that\ncauses many issues...",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that having a struct larger than 8 bytes in the swap table limits lock design, suggesting to utilize atomic operations like CAS on swap entries once they are small and unified, similar to how atomic_long_t is used. They also mentioned an existing cluster-lockless version of swap check in the swap table p3 patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Having a struct larger than 8 bytes means you can't load it\natomically, that limits your lock design. About a year ago Chris\nshared with me an idea to use CAS on swap entries once they are small\nand unified, that's why swap table is using atomic_long_t and have\nhelpers like __swap_table_xchg, we are not making good use of them yet\nthough. Meanwhile we have already consolidated the lock scope to folio\nin many places, holding the folio lock then doing the CAS without\ntouching cluster lock at all for many swap operations might be\nfeasible soon.\n\nE.g. we already have a cluster-lockless version of swap check in swap table p3:\nhttps://lore.kernel.org/linux-mm/20260128-swap-table-p3-v2-11-fe0b67ef0215@tencent.com/\n\nThat might also greatly simplify the locking on IO and migration\nperformance between swap devices.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song expressed concern about the increased memory usage of the swap table implementation in comparison to existing solutions, citing a trade-off between features and overhead. They suggested making the new implementation optional and minimal.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hmm.. With the swap table we will have a stable 8 bytes per slot in\nall cases, in current mm-stable we use 11 bytes (8 bytes dyn and 3\nbytes static), and in the posted p3 we already get 10 bytes (8 bytes\ndyn and 2 bytes static). P4 or follow up was already demonstrated\nlast year with working code, and it makes everything dynamic\n(8 bytes fully dyn, I'll rebase and send that once p3 is merged).\n\nSo with mm-stable and follow up, for 32G swap device:\n\n0% usage, or 0/8,388,608 entries: 0.00 MB\n* mm-stable total overhead: 25.50 MB (which is swap table p2)\n* swap-table p3 overhead: 17.50 MB\n* swap-table p4 overhead: 0.50 MB\n* Vswap total overhead: 2.00 MB\n\n100% usage, or 8,388,608/8,388,608 entries:\n* mm-stable total overhead: 89.5 MB (which is swap table p2)\n* swap-table p3 overhead: 81.5 MB\n* swap-table p4 overhead: 64.5 MB\n* Vswap total overhead: 259.00 MB\n\nThat 3 - 4 times more memory usage, quite a trade off. With a\n128G device, which is not something rare, it would be 1G of memory.\nSwap table p3 / p4 is about 320M / 256M, and we do have a way to cut\nthat down close to be <1 byte or 3 byte per page with swap table\ncompaction, which was discussed in LSFMM last year, or even 1 bit\nwhich was once suggested by Baolin, that would make it much smaller\ndown to <24MB (This is just an idea for now, but the compaction is\nvery doable as we already have \"LRU\"s for swap clusters in swap\nallocator).\n\nI don't think it looks good as a mandatory overhead. We do have a huge\nuser base of swap over many different kinds of devices, it was not\nlong ago two new kernel bugzilla issue  or bug reported was sent to\nthe maillist about swap over disk, and I'm still trying to investigate\none of them which seems to be actually a page LRU issue and not swap\nproblem..  OK a little off topic, anyway, I'm not saying that we don't\nwant more features, as I mentioned above, it would be better if this\ncan be optional and minimal. See more test info below.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that the patch fixes poor batching behavior of the vswap free path, but questioned whether it's due to a smaller lock scope compared to zswap.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "comparing"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Congrats! Yeah, I guess that's because vswap has a smaller lock scope\nthan zswap with a reduced callpath?",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that the vswap patch introduces a significant regression in freeing time under global pressure, causing up to 200% slower performance and affecting common workloads such as building the kernel in a VM. They suspect that the double free or decoupling of swap/underlying slots might be the cause of this issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "regression",
                "performance impact"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Thanks for the bench, but please also test with global pressure too.\nOne mistake I made when working on the prototype of swap tables was\nonly focusing on cgroup memory pressure, which is really not how\neveryone uses Linux, and that's why I reworked it for a long time to\ntweak the RCU allocation / freeing of swap table pages so there won't\nbe any regression even for lowend and global pressure. That's kind of\ncritical for devices like Android.\n\nI did an overnight bench on this with global pressure, comparing to\nmainline 6.19 and swap table p3 (I do include such test for each swap\ntable serie, p2 / p3 is close so I just rebase and latest p3 on top of\nyour base commit just to be fair and that's easier for me too) and it\ndoesn't look that good.\n\nTest machine setup for vm-scalability:\n# lscpu | grep \"Model name\"\nModel name:          AMD EPYC 7K62 48-Core Processor\n\n# free -m\n              total        used        free      shared  buff/cache   available\nMem:          31582         909       26388           8        4284       29989\nSwap:         40959          41       40918\n\nThe swap setup follows the recommendation from Huang\n(https://lore.kernel.org/linux-mm/87ed474kvx.fsf@yhuang6-desk2.ccr.corp.intel.com/).\n\nTest (average of 18 test run):\nvm-scalability/usemem --init-time -O -y -x -n 1 56G\n\n6.19:\nThroughput: 618.49 MB/s (stdev 31.3)\nFree latency: 5754780.50us (stdev 69542.7)\n\nswap-table-p3 (3.8%, 0.5% better):\nThroughput: 642.02 MB/s (stdev 25.1)\nFree latency: 5728544.16us (stdev 48592.51)\n\nvswap (3.2%, 244% worse):\nThroughput: 598.67 MB/s (stdev 25.1)\nFree latency: 13987175.66us (stdev 125148.57)\n\nThat's a huge regression with freeing. I have a vm-scatiliby test\nmatrix, not every setup has such significant >200% regression, but on\naverage the freeing time is about at least 15 - 50% slower (for\nexample /data/vm-scalability/usemem --init-time -O -y -x -n 32 1536M\nthe regression is about 2583221.62us vs 2153735.59us). Throughput is\nall lower too.\n\nFreeing is important as it was causing many problems before, it's the\nreason why we had a swap slot freeing cache years ago (and later we\nremoved that since the freeing cache causes more problems and swap\nallocator already improved it better than having the cache). People\neven tried to optimize that:\nhttps://lore.kernel.org/linux-mm/20250909065349.574894-1-liulei.rjpt@vivo.com/\n(This seems a already fixed downstream issue, solved by swap allocator\nor swap table). Some workloads might amplify the free latency greatly\nand cause serious lags as shown above.\n\nAnother thing I personally cares about is how swap works on my daily\nlaptop :), building the kernel in a 2G test VM using NVME as swap,\nwhich is a very practical workload I do everyday, the result is also\nnot good (average of 8 test run, make -j12):\n#free -m\n               total        used        free      shared  buff/cache   available\nMem:            1465         216        1026           0         300        1248\nSwap:           4095          36        4059\n\n6.19 systime:\n109.6s\nswap-table p3:\n108.9s\nvswap systime:\n118.7s\n\nOn a build server, it's also slower (make -j48 with 4G memory VM and\nNVME swap, average of 10 testrun):\n# free -m\n               total        used        free      shared  buff/cache   available\nMem:            3877        1444        2019         737        1376        2432\nSwap:          32767        1886       30881\n\n# lscpu | grep \"Model name\"\nModel name:                              Intel(R) Xeon(R) Platinum\n8255C CPU @ 2.50GHz\n\n6.19 systime:\n435.601s\nswap-table p3:\n432.793s\nvswap systime:\n455.652s\n\nIn conclusion it's about 4.3 - 8.3% slower for common workloads under\nglobal pressure, and there is a up to 200% regression on freeing. ZRAM\nshows an even larger workload regression but I'll skip that part since\nyour series is focusing on zswap now. Redis is also ~20% slower\ncompared to mm-stable (327515.00 RPS vs 405827.81 RPS), that's mostly\ndue to swap-table-p2 in mm-stable so I didn't do further comparisons.\n\nSo if that's not a bug with this series, I think the double free or\ndecoupling of swap / underlying slots might be the problem with the\nfreeing regression shown above. That's really a serious issue, and the\nglobal pressure might be a critical issue too as the metadata is much\nlarger, and is already causing regressions for very common workloads.\nLow end users could hit the min watermark easily and could have\nserious jitters or allocation failures.\n\nThat's part of the issue I've found, so I really do think we need a\nflexible way to implementa that and not have a mandatory layer. After\nswap table P4 we should be able to figure out a way to fit all needs,\nwith a clean defined set of swap API, metadata and layers, as was\ndiscussed at LSFMM last year.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author acknowledged that the patch series did not appear on lore or reach their coworkers, attributed it to a delay in sending emails from Gmail, and promised to be more patient in the future.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgment of issue",
                "promise to improve"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I have no idea what happened to be honest. It did not show up on lore\nfor a couple of hours, and my coworkers did not receive the cover\nletter email initially. I did not receive any error message or logs\neither - git send-email returns Success to me, and when I checked on\nthe web gmail client (since I used a gmail email account), the whole\nseries is there.\n\nI tried re-sending a couple times, to no avail. Then, in a couple of\nhours, all of these attempts showed up.\n\nAnyway, this is my bad - I'll be more patient next time. If it does\nnot show up for a couple of hours then I'll do some more digging.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged a mistake in an old patch version's cover letter and promised to correct it in future versions.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a mistake",
                "promised to correct"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oh yeah I forgot to update that. That was from an old cover letter of\nan old version that never got sent out - I'll correct that in future\nversions\n\n(if you scroll down to the bottom of the cover letter you should see\nthe correct base, which should be 6.19).",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged a potential compilation issue and asked reviewer to share their configurations so they can reproduce and fix it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a potential issue",
                "asked for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ah that's strange. It compiled on all of my setups (I tested with a couple\ndifferent ones), but I must have missed some cases. Would you mind\nsharing your configs so that I can reproduce this compilation error?\n\n(although I'm sure kernel test robot will scream at me soon, which\nusually includes configs that cause the compilation issue).",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author is addressing a concern about inconsistent timing metrics in the patch description. They acknowledge that user time was previously included but have since removed it, citing industry practice of only including system time. The author promises to include real-time numbers in future versions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgment",
                "clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Will do next time! I used to include user time as well, but I noticed\nthat folks (for e.g see [1]) only include systime, not even real time,\nso I figure nobody cares about user time :)\n\n(I still include real time because some of my past work improves sys\ntime but regresses real time, so I figure that's relevant).\n\n[1]: https://lore.kernel.org/linux-mm/20260128-swap-table-p3-v2-0-fe0b67ef0215@tencent.com/\n\nBut yeah no big deal. I'll dig through my logs to see if I still have\nthe numbers, but if not I'll include it in next version.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged a concern about lock contention and performance issues, agreed to include swap activity stats in future versions, and plans to address the issue by restructuring the code.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "plans to address"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Very fair point :) I will say though - the kernel build test, with\nmemory.max limit sets, does generate a sizable amount of swapping, and\ndoes OOM if you don't set up swap. Take my words for now, but I will\ntry to include average per-run (z)swap activity stats (zswpout zswpin\netc.) in future versions if you're interested :)\n\nI've been trying to running more stress tests to trigger crashes and\nperformance regression. One of the big reasons why I haven't sent\nanything til now is to fix obvious performance issues (the\naforementioned lock contention) and bugs. It's a complicated piece of\nwork.\n\nAs always, would love to receive code/design feedback from you (and\nKairui, and other swap reviewers), and I would appreciate very much if\nother swap folks can play with the patch series on their setup as well\nfor performance testing, or let me know if there is any particular\ncase that they're interested in :)\n\nThanks for your review, Chris!",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "reviewer noted that vswap_free() acquires the per-vswap spinlock while holding the folio lock, creating a potential deadlock with reclaim paths, and requested the lock be dropped before calling try_to_unmap()",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential deadlock",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hello Kairui,\n\nOn Wed, Feb 11, 2026 at 01:59:34AM +0800, Kairui Song wrote:",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Reviewer noted that vswap_free() acquires the per-vswap spinlock while holding the folio lock, creating a potential deadlock issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential deadlock"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah, it turns out we need the same data points to describe and track\na swapped out page ;)",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "reviewer pointed out that the patch's focus on addressing swap cache issues may be missing the key problem of address space separation, and suggested re-evaluating the comparisons made in the regression reports",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The issue is address space separation. We don't want things inside the\ncompressed pool to consume disk space; nor do we want entries that\nlive on disk to take usable space away from the compressed pool.\n\nThe regression reports are fair, thanks for highlighting those. And\nwhether to make this optional is also a fair discussion.\n\nBut some of the numbers comparisons really strike me as apples to\noranges comparisons. It seems to miss the core issue this series is\ntrying to address.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Johannes Weiner questioned the necessity of try_to_unuse() scans in the vswap free path, suggesting that they are expensive and unnecessary if a fast swapoff read sequence with lazy minor faults is implemented.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That seems very academic to me. The goal is to relinquish disk space,\nand these patches make that a lot faster.\n\nLet's put it the other way round: if today we had a fast swapoff read\nsequence with lazy minor faults to resolve page tables, would we\naccept patches that implement the expensive try_to_unuse() scans and\nmake it mandatory? Considering the worst-case runtime it can cause?\n\nI don't think so. We have this scan because the page table references\nare pointing to disk slots, and this is the only way to free them.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Reviewer Johannes Weiner noted that vswap_free() cannot drop the disk slot while the swp_entry_t is still in circulation, creating a potential issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential issue",
                "cannot drop"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "How can you relinquish the disk slot as long as the swp_entry_t is in\ncirculation?",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledges that maintaining two swap implementations would make the patch series unreadable, unreviewable, and unmaintainable, but does not commit to a single implementation.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges maintenance challenges",
                "does not commit to a single implementation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ideally, if we can close the performance gap and have only one\nversion, then that would be the best :)\n\nProblem with making it optional, or maintaining effectively two swap\nimplementations, is that it will make the patch series unreadable and\nunreviewable, and the code base unmaintanable :) You'll have x2 the\namount of code to reason about and test, much more merge conflicts at\nrebase and cherry-pick time. And any improvement to one version takes\nextra work to graft onto the other version.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged Kairui Song's feedback on swap table format, confirmed they like it, and clarified their patch series' goal is not to remove the design but to separate physical and virtual swaps for new use cases.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged",
                "confirmed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "There's nothing wrong with that ;)\n\nI like the swap table format (and your cluster-based swap allocator) a\nlot. This patch series does not aim to remove that design - I just\nwant to separate the address space of physical and virtual swaps to\nenable new use cases...",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author acknowledged that poor batching behavior in the vswap free path is a real production issue, citing varying server and service characteristics that make static sizing of swap files operationally impossible.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a problem",
                "agreed to address it"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I mean, it is a real production issue :) We have a variety of server\nmachines and services. Each of the former has its own memory and drive\nsize. Each of the latter has its own access characteristics,\ncompressibility, latency tolerance (and hence would prefer a different\nswapping solutions - zswap, disk swap, zswap x disk swap). Coupled\nwith the fact that now multiple services can cooccur on one host, and\none services can be deployed on different kinds of hosts, statically\nsizing the swapfile becomes operationally impossible and leaves a lot\nof wins on the table. So swap space has to be dynamic.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that dynamic growth of swap files is a missing feature and expressed skepticism about the current design, but did not commit to revising it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "skepticism",
                "acknowledgment"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"dynamic growth of swap files\", do you mean dynamically adjusting\nthe size of the swapfile? then that capacity does not exist right now,\nand I don't see a good design laid out for it... At the very least,\nthe swap allocator needs to be dynamic in nature. I assume it's going\nto look something very similar to vswap's current attempt, which\nrelies on a tree structure (radix tree i.e xarray). Sounds familiar?\n;)\n\nI feel like each of the problem I mention in this cover letter can be\nsolved partially with some amount of hacks, but none of them will\nsolve it all. And once you slaps all the hacks together, you just get\nvirtual swap, potentially shoved within specific backend codebase\n(zswap or zram). That's not... ideal.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author responded to Kairui Song's feedback that the patch still locks the swap device in place, explaining that page table entries refer to slots on the physical swap device and cannot be freed without locking.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "author provided explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't understand your point regarding the \"reading everything into\nswap cache\". Yes, you can do that, but you would still lock the swap\ndevice in place, because the page table entries still refer to slots\non the physical swap device - you cannot free the swap device, nor\nspace on disk, not even the swapfile's metadata (especially since the\nswap cache is now intertwined with the physical swap layer).",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that their patch was based on an older kernel version and suggested incorporating recent swap cache changes into their work, but did not commit to revising the patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged prior version",
                "suggested incorporation of new features"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah this was based on 6.19, which did not have your swap cache change yet :)\n\nI have taken a look at your latest swap table work in mm-stable, and I\nthink most of that can conceptually incorporated in to this line of\nwork as well.\n\nChiefly, the new swap cache synchronization scheme (i.e whoever puts\nthe folio in swap cache first gets exclusive rights) still works in\nvirtual swap world (and hence, the removal of swap cache pin, which is\none bit in the virtual swap descriptor).\n\nSimilarly, do you think we cannot hold the folio lock in place of the\ncluster lock in the virtual swap world? Same for a lot of the memory\noverhead reduction tricks (such as using shadow for cgroup id instead\nof a separate swap_cgroup unsigned short field). I think comparing the\ntwo this way is a bit apples-to-oranges (especially given the new\nfeatures enabled by vswap).\n\n[...]",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledges a need for further review and debugging of the vswap free path, but does not explicitly state that they will rework or revise the patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges need for further review",
                "willing to help debug"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Side note - I might have missed this. If it's still ongoing, would\nlove to help debug this :)",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that the vswap free path can merge several swap operations, eliminating the need for release-then-reacquire of swap locks, but noted this as a side point to their main goal of enabling new features.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical detail",
                "emphasized primary goals"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ah yeah that too. I neglected to mention this, but with vswap you can\nmerge several swap operations in zswap code path and no longer have to\nrelease-then-reacquire the swap locks, since zswap entries live in the\nsame lock scope as swap cache entries.\n\nIt's more of a side note either way, because my main goal with this\npatch series is to enable new features. Getting a performance win is\nalways nice of course :)",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author responded to reviewer feedback about poor batching behavior in vswap free path, stating that they ran kernel build with disk swapping and saw performance on par with baseline, but questioned whether concurrency difference was the cause of discrepancy.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "lack of clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hmm this one I don't think I can reproduce without your laptop ;)\n\nJokes aside, I did try to run the kernel build with disk swapping, and\nthe performance is on par with baseline. Swap performance with NVME\nswap tends to be dominated by IO work in my experiments. Do you think\nI missed something here? Maybe it's the concurrency difference (since\nI always run with -j$(nproc), i.e the number of workers == the number\nof processors).",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged a need for further testing and reproduction of reported issues, specifically starting with the 'usemem' case.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for further testing",
                "will start with usemem"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'll see if I can reproduce the issues! I'll start with usemem one\nfirst, as that seems easier to reproduce...",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledges the potential issue as a 'non-crashing bug' and agrees to review the provided test case before deciding on a fix.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges potential issue",
                "agrees to study test case"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It could be a non-crashing bug that subtly regresses certain swap\noperations, but yeah let me study your test case first!",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that IO wait times are a concern in production systems and emphasized their importance when using disk swapping.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "emphasized"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ah I just noticed that your numbers include only systime. Ignore my IO\ncomments then.\n\n(I still think in real production system, with disk swapping enabled,\nthen IO wait time is going to be really important. If you're going to\nuse disk swap, then this affects real time just as much if not more\nthan kernel CPU overhead).",
              "reply_to": "",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the patch does not address the batching behavior of vswap free path, which can lead to poor performance and requested additional work to fix this issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested additional work",
                "poor performance"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Johannes,\n\nOn Mon, Feb 9, 2026 at 6:36PM Johannes Weiner <hannes@cmpxchg.org> wrote:",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the patch adds new members to existing structures and increases their size, specifically mentioning the change from a 1-byte swap_map to a 4-byte count.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "patch size increase",
                "structural changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Adding members previously not there and making some members bigger\nalong the way. For example, the swap_map from 1 byte to a 4 byte\ncount.",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the patch increased the total per-swap entry overhead to 24 bytes, and requested confirmation on whether this is the correct number for virtual swap (VS) if he disagrees.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "disagreement",
                "requested confirmation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It seems you did not finish your sentence before sending your reply.\n\nAnyway, I saw the total per swap entry overhead bump to 24 bytes\ndynamic. Let me know what is the correct number for VS if you\ndisagree.\n\nChris",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the vswap free path has poor batching behavior and requested a fix to improve performance.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance concern",
                "requested fix"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Kairui,\n\nThank you so much for the performance test.\n\nI will only comment on the performance number in this sub email thread.\n\nOn Tue, Feb 10, 2026 at 10:00AM Kairui Song <ryncsn@gmail.com> wrote:",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the vswap series has poor memory and CPU performance compared to the original swap table, which he attributes to the per-swap-entry metadata overhead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance",
                "metadata_overhead"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Agree. That has been my main complaint about VS is the per swap entry\nmetadata overhead. This VS series reverted the swap table, but memory\nand CPU performance is worse than swap table.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li expressed strong disagreement with the patch, citing poor performance compared to baseline and swap table P3 as a 'deal breaker' for them.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance",
                "deal breaker"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Now that is a deal breaker for me. Not the similar performance with\nbaseline or swap table P3.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the patch introduces a 4-8% performance regression due to its preference for swap table implementations, considering this a statically significant issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance_regression",
                "statically_significant"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "At 4-8% I would consider it a statically significant performance\nregression to favor swap table implementations.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li agrees with the patch's approach, suggesting that getting the fundamental infrastructure for swap right first is crucial before implementing more complex features.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "agreement",
                "endorsement"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Agree. That matches my view, get the fundamental infrastructure for\nswap right first (swap table), then do those fancier feature\nenhancement like online growing the size of swapfile.\n\nChris",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Reviewer noted that vswap_free() acquires the per-vswap spinlock while holding the folio lock, creating a potential deadlock with reclaim paths, and requested the lock be dropped before calling try_to_unmap().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential deadlock",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Chris,\n\nOn Tue, Feb 10, 2026 at 01:24:03PM -0800, Chris Li wrote:",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "reviewer noted that vswap_free() acquires the per-vswap spinlock while holding the folio lock, creating a potential deadlock with reclaim paths",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I did. I trimmed the quote of Nhat's cover letter to the parts\naddressing your questions. If you use gmail, click the three dots:",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li provided additional debugging information, but did not raise any specific technical concerns or objections about the patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no specific concern raised"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "No problem. Just want to provide more data points if that helps you\ndebug your email issue.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested simplifying the explanation of virtual swap (VS) memory overhead by referencing a '24B dynamic' value, which is sufficient for most use cases without requiring detailed tables.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested improvement"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think the \"24B dynamic\" sums up the VS memory overhead pretty well\nwithout going into the detail tables. You can drive from case\ndiscussion from that.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li reported an issue with their ability to stress test the patch series due to a compilation error, possibly related to a newer GCC version.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "compiler issue",
                "testing"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "See attached config.gz. It is also possible the newer gcc version\ncontributes to that error. Anyway, that is preventing me from stress\ntesting your series on my setup.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that measuring stddev for 33 seconds is insufficient to achieve a 1.5% resolution, as it falls within the range of noise and suggested taking more samples.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested additional data",
                "questioned measurement validity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The stddev is 33 seconds. Measure 5 times then average result is not\nenough sample to get your to 1.5% resolution (8 seconds), which fall\ninto the range of noise.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested including user space time in the swap pressure calculation to better determine the level of swap pressure, but did not request any changes to the patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggestion",
                "request"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Including the user space time will help determine the level of swap\npressure as well. I don't need the absolutely zswapout count just yet.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that he understands there are performance regressions, but requested additional work to fix a compiling error before he can perform stress testing.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "regressions",
                "compiling error"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I understand Kairui has some measurements that show regressions.\n\nIf you can fix the compiling error I can do some stress testing myself\nto provide more data points.\n\nThanks\n\nChris",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested making the vswap batching behavior optional at runtime, specifically for types of swap that do not benefit from it, such as zram.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I second that this should be run time optional for other types of\nswap. It should not be mandatory for other swap that does not benefit\nfrom it. e.g. zram.\n\nChris",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Thu, Feb 12, 2026 at 4:23AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author is addressing a concern about the large number of people CC'd on the patch, explaining they manually add individuals to the list and plan to use a script in the future.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explaining"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I hope so... did I miss someone? If so, my apologies - I manually add\nthem one at a time to be completely honest. The list is huge...\n\nI'll probably use a script to convert that huge output next time into \"--cc\".\n\n(Or are you suggesting I should not send it out to everyone? I can try\nto trim the list, but tbh it touches areas that I'm not familiar with,\nso I figure I should just cc everyone).",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author acknowledges that they should have done a proper CC list from the start, promises to be more careful in the future, and mentions scripting the process.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledgment",
                "commitment"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ok let's try... this :) Probably should have done it from the start,\nbut better late than never...\n\nNot sure who was missing from the first run - my apologies if I did\nthat.... I'll be more careful with huge cc list next time and just\nscriptify it.",
              "reply_to": "",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David noted that the vswap free path has poor batching behavior, which can lead to unnecessary wakeups of the swap device's worker thread.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I stumbled over this patch set while scrolling through the mailing list \nafter a while (now that my inbox is \"mostly\" cleaned up) and wondered \nwhy no revision ended in my inbox :)",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David requested that the patch author include a CC list in the cover letter, which can be used to specify additional recipients for the email sent by git send-email.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "off-topic",
                "administrative"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I usually add them as\n\nCc:\n\nto the cover letter and then use something like \"--cc-cover \" with git \nsend-email.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that the vswap free path needs to be restructured to improve batching behavior, agreed to make changes in a future version of the patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for restructuring",
                "agreed to make changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Thu, Feb 12, 2026 at 9:41AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that their large patch series caused issues with CC list and email delivery, but did not indicate a plan to revise the patch itself.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgment of issue",
                "lack of revision plan"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oh TIL. Thanks, David!\n\nYeah this is the biggest patch series I've ever sent out. Most of my\npast patches are contained in one or two files, so usually only the\nmaintainers and contributors are pulled in, and the cc list never\nexceeds 15-20 cc's. So I've been getting away with just manually\npreparing a send command, do a quick eyeball check, then send things\nout.\n\nThat system breaks down hard this case (the email debacle aside, which\nI still haven't figured out - still looking at gmail as the prime\nsuspect...).",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David noted that the patch description should be more nuanced in its CC: behavior, suggesting it's not always as simple as copying output to the cover letter and proposing a distinction between CC'ing maintainers and reviewers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested clarification",
                "proposed improvement"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It's usually not as easy as copying the output to the cover letter via Cc:.\n\nSometimes you want to CC all maintainers+reviewers of some subsystem, \nsometimes only the maintainers (heads-up, mostly simplistic unrelated \nchanges that don't need any real subsystem-specific review).\n\nFine line between flooding people with patches or annoying people with \npatches :)",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author addressed concerns about poor batching behavior of vswap free path by identifying an issue with PTE zapping and unnecessary xarray lookups. They plan to fix this in a future version, but no specific changes are mentioned yet.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a problem",
                "planned to address it"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Kairui - a quick update.\n\nTook me awhile to get a host that matches your memory spec:\n\nfree -m\n               total        used        free      shared  buff/cache   available\nMem:           31609        5778        7634          20       18664       25831\nSwap:          65535           1       65534\n\nI think I managed to reproduce your observations (average over 5 runs):\n\nBaseline (6.19)\n\nreal: mean: 191.19s, stdev: 4.53s\nuser: mean: 46.98s, stdev: 0.15s\nsys: mean: 127.97s, stdev: 3.95s\naverage throughput: 382057 KB/s\naverage free time: 8179978 usecs\n\nVswap:\n\nreal: mean: 199.85s, stdev: 6.09s\nuser: mean: 46.51s, stdev: 0.25s\nsys: mean: 137.24s, stdev: 6.46s\naverage throughput: 367437 KB/s\naverage free time: 9887107.6 usecs\n\n(command is time ./usemem --init-time -w -O -s 10 -n 1 56g)\n\nI think I figured out where the bulk of the regression lay - it's in\nthe PTE zapping path. In a nutshell, we're not batching in the case\nwhere these PTEs are backed by virtual swap entries with zswap\nbackends (even though there is not a good reason not to batch), and\nunnecessarily performing unnecesary xarray lookups to resolve the\nbackend for some superfluous checks (2 xarray lookups for every PTE,\nwhich is wasted work because as noted earlier, we ended up not\nbatching anyway).\n\nJust by simply fixing this issue, the gap is much closer\n\nreal: mean: 192.24s, stdev: 4.82s\nuser: mean: 46.42s, stdev: 0.27s\nsys: mean: 129.84s, stdev: 4.59s\naverage throughput: 380670 KB/s\naverage free time: 8583381.4 usecs\n\nI also discovered a couple more inefficiencies in vswap free path.\nHopefully once we fix those, the gap will be non-existent.",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author Nhat Pham is addressing Kairui Song's feedback on the vswap patch, specifically a concern about poor batching behavior in the vswap free path. The author asks Kairui to apply the patch on top of the vswap series and run it on their test suite, providing data from their end. The author mentions that they have already fixed the issue on their system but wants to confirm if there are any discrepancies.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "asking for clarification",
                "requesting additional testing"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Kairui, could you apply this patch on top of the vswap series and run it\non your test suite? It runs fairly well on my system (I actually rerun\nthe benchmark on a different host to double check as well), but I'd love\nto get some data from your ends as well.\n\nIf there are serious discrepancies, could you also include your build\nconfig etc.? There might be differences in our setups, but since I\nmanaged to reproduce the free time regression on my first try I figured\nI should just fix it first :)\n\n---------------\n\nFix two issues that make the swap free path inefficient:\n\n1. At the PTE zapping step, we are unnecessarily resolving the backends,\n   and fall back to batch size of 1, even though virtual swap\n   infrastructure now already supports freeing of mixed backend ranges\n   (as long the PTEs contain virtually contiguous swap slots).\n2. Optimize vswap_free() by batching consecutive free operations, and\n   avoid releasing locks unnecessarily (most notably, when we release\n   non-disk-swap backends).\n\nPer a report from Kairui Song ([1]), I have run the following benchmark:\n\nfree -m\n               total        used        free      shared  buff/cache   available\nMem:           31596        5094       11667          19       15302       26502\nSwap:          65535          33       65502\n\nRunning the usemem benchmark with n = 1, 56G for 5 times, and average\nout the result:\n\nBaseline (6.19):\n\nreal: mean: 190.93s, stdev: 5.09s\nuser: mean: 46.62s, stdev: 0.27s\nsys: mean: 128.51s, stdev: 5.17s\nthroughput: mean: 382093 KB/s, stdev: 11173.6 KB/s\nfree time: mean: 7916690.2 usecs, stdev: 88923.0 usecs\n\nVSS without this patch:\nreal: mean: 194.59s, stdev: 7.61s\nuser: mean: 46.71s, stdev: 0.46s\nsys: mean: 131.97s, stdev: 7.93s\nthroughput: mean: 379236.4 KB/s, stdev: 15912.26 KB/s\nfree time: mean: 10115572.2 usecs, stdev: 108318.35 usecs\n\nVSS with this patch:\nreal: mean: 187.66s, stdev: 5.67s\nuser: mean: 46.5s, stdev: 0.16s\nsys: mean: 125.3s, stdev: 5.58s\nthroughput: mean: 387506.4 KB/s, stdev: 12556.56 KB/s\nfree time: mean: 7029733.8 usecs, stdev: 124661.34 usecs\n\n[1]: https://lore.kernel.org/linux-mm/CAMgjq7AQNGK-a=AOgvn4-V+zGO21QMbMTVbrYSW_R2oDSLoC+A@mail.gmail.com/\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/memcontrol.h |   6 +\n mm/internal.h              |  18 ++-\n mm/madvise.c               |   2 +-\n mm/memcontrol.c            |   2 +-\n mm/memory.c                |   8 +-\n mm/vswap.c                 | 294 ++++++++++++++++++-------------------\n 6 files changed, 165 insertions(+), 165 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 0651865a4564f..0f7f5489e1675 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -827,6 +827,7 @@ static inline unsigned short mem_cgroup_id(struct mem_cgroup *memcg)\n \treturn memcg->id.id;\n }\n struct mem_cgroup *mem_cgroup_from_id(unsigned short id);\n+void mem_cgroup_id_put_many(struct mem_cgroup *memcg, unsigned int n);\n \n #ifdef CONFIG_SHRINKER_DEBUG\n static inline unsigned long mem_cgroup_ino(struct mem_cgroup *memcg)\n@@ -1289,6 +1290,11 @@ static inline struct mem_cgroup *mem_cgroup_from_id(unsigned short id)\n \treturn NULL;\n }\n \n+static inline void mem_cgroup_id_put_many(struct mem_cgroup *memcg,\n+\t\t\t\t\t  unsigned int n)\n+{\n+}\n+\n #ifdef CONFIG_SHRINKER_DEBUG\n static inline unsigned long mem_cgroup_ino(struct mem_cgroup *memcg)\n {\ndiff --git a/mm/internal.h b/mm/internal.h\nindex cfe97501e4885..df991f601702c 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -327,8 +327,6 @@ static inline swp_entry_t swap_nth(swp_entry_t entry, long n)\n \treturn (swp_entry_t) { entry.val + n };\n }\n \n-swp_entry_t swap_move(swp_entry_t entry, long delta);\n-\n /**\n  * pte_move_swp_offset - Move the swap entry offset field of a swap pte\n  *\t forward or backward by delta\n@@ -342,7 +340,7 @@ swp_entry_t swap_move(swp_entry_t entry, long delta);\n static inline pte_t pte_move_swp_offset(pte_t pte, long delta)\n {\n \tsoftleaf_t entry = softleaf_from_pte(pte);\n-\tpte_t new = swp_entry_to_pte(swap_move(entry, delta));\n+\tpte_t new = swp_entry_to_pte(swap_nth(entry, delta));\n \n \tif (pte_swp_soft_dirty(pte))\n \t\tnew = pte_swp_mksoft_dirty(new);\n@@ -372,6 +370,7 @@ static inline pte_t pte_next_swp_offset(pte_t pte)\n  * @start_ptep: Page table pointer for the first entry.\n  * @max_nr: The maximum number of table entries to consider.\n  * @pte: Page table entry for the first entry.\n+ * @free_batch: Whether the batch will be passed to free_swap_and_cache_nr().\n  *\n  * Detect a batch of contiguous swap entries: consecutive (non-present) PTEs\n  * containing swap entries all with consecutive offsets and targeting the same\n@@ -382,13 +381,15 @@ static inline pte_t pte_next_swp_offset(pte_t pte)\n  *\n  * Return: the number of table entries in the batch.\n  */\n-static inline int swap_pte_batch(pte_t *start_ptep, int max_nr, pte_t pte)\n+static inline int swap_pte_batch(pte_t *start_ptep, int max_nr, pte_t pte,\n+\t\t\t\t bool free_batch)\n {\n \tpte_t expected_pte = pte_next_swp_offset(pte);\n \tconst pte_t *end_ptep = start_ptep + max_nr;\n \tconst softleaf_t entry = softleaf_from_pte(pte);\n \tpte_t *ptep = start_ptep + 1;\n \tunsigned short cgroup_id;\n+\tint nr;\n \n \tVM_WARN_ON(max_nr < 1);\n \tVM_WARN_ON(!softleaf_is_swap(entry));\n@@ -408,7 +409,14 @@ static inline int swap_pte_batch(pte_t *start_ptep, int max_nr, pte_t pte)\n \t\tptep++;\n \t}\n \n-\treturn ptep - start_ptep;\n+\tnr = ptep - start_ptep;\n+\t/*\n+\t * free_swap_and_cache_nr can handle mixed backends, as long as virtual\n+\t * swap entries backing these PTEs are contiguous.\n+\t */\n+\tif (!free_batch && !vswap_can_swapin_thp(entry, nr))\n+\t\treturn 1;\n+\treturn nr;\n }\n #endif /* CONFIG_MMU */\n \ndiff --git a/mm/madvise.c b/mm/madvise.c\nindex b617b1be0f535..441da03c5d2b9 100644\n--- a/mm/madvise.c\n+++ b/mm/madvise.c\n@@ -692,7 +692,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,\n \n \t\t\tif (softleaf_is_swap(entry)) {\n \t\t\t\tmax_nr = (end - addr) / PAGE_SIZE;\n-\t\t\t\tnr = swap_pte_batch(pte, max_nr, ptent);\n+\t\t\t\tnr = swap_pte_batch(pte, max_nr, ptent, true);\n \t\t\t\tnr_swap -= nr;\n \t\t\t\tfree_swap_and_cache_nr(entry, nr);\n \t\t\t\tclear_not_present_full_ptes(mm, addr, pte, nr, tlb->fullmm);\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 50be8066bebec..bfa25eaffa12a 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -3597,7 +3597,7 @@ void __maybe_unused mem_cgroup_id_get_many(struct mem_cgroup *memcg,\n \trefcount_add(n, &memcg->id.ref);\n }\n \n-static void mem_cgroup_id_put_many(struct mem_cgroup *memcg, unsigned int n)\n+void mem_cgroup_id_put_many(struct mem_cgroup *memcg, unsigned int n)\n {\n \tif (refcount_sub_and_test(n, &memcg->id.ref)) {\n \t\tmem_cgroup_id_remove(memcg);\ndiff --git a/mm/memory.c b/mm/memory.c\nindex a16bf84ebaaf9..59645ad238e22 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -1742,7 +1742,7 @@ static inline int zap_nonpresent_ptes(struct mmu_gather *tlb,\n \t\tif (!should_zap_cows(details))\n \t\t\treturn 1;\n \n-\t\tnr = swap_pte_batch(pte, max_nr, ptent);\n+\t\tnr = swap_pte_batch(pte, max_nr, ptent, true);\n \t\trss[MM_SWAPENTS] -= nr;\n \t\tfree_swap_and_cache_nr(entry, nr);\n \t} else if (softleaf_is_migration(entry)) {\n@@ -4491,7 +4491,7 @@ static bool can_swapin_thp(struct vm_fault *vmf, pte_t *ptep, int nr_pages)\n \tif (!pte_same(pte, pte_move_swp_offset(vmf->orig_pte, -idx)))\n \t\treturn false;\n \tentry = softleaf_from_pte(pte);\n-\tif (swap_pte_batch(ptep, nr_pages, pte) != nr_pages)\n+\tif (swap_pte_batch(ptep, nr_pages, pte, false) != nr_pages)\n \t\treturn false;\n \n \t/*\n@@ -4877,7 +4877,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\tpte_t folio_pte = ptep_get(folio_ptep);\n \n \t\tif (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||\n-\t\t    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)\n+\t\t    swap_pte_batch(folio_ptep, nr, folio_pte, false) != nr)\n \t\t\tgoto out_nomap;\n \n \t\tpage_idx = idx;\n@@ -4906,7 +4906,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\tfolio_ptep = vmf->pte - idx;\n \t\tfolio_pte = ptep_get(folio_ptep);\n \t\tif (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||\n-\t\t    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)\n+\t\t    swap_pte_batch(folio_ptep, nr, folio_pte, false) != nr)\n \t\t\tgoto check_folio;\n \n \t\tpage_idx = idx;\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex 2a071d5ae173c..047c6476ef23c 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -481,18 +481,18 @@ static void vswap_cluster_free(struct vswap_cluster *cluster)\n \tkvfree_rcu(cluster, rcu);\n }\n \n-static inline void release_vswap_slot(struct vswap_cluster *cluster,\n-\t\tunsigned long index)\n+static inline void release_vswap_slot_nr(struct vswap_cluster *cluster,\n+\t\tunsigned long index, int nr)\n {\n \tunsigned long slot_index = VSWAP_IDX_WITHIN_CLUSTER_VAL(index);\n \n \tVM_WARN_ON(!spin_is_locked(&cluster->lock));\n-\tcluster->count--;\n+\tcluster->count -= nr;\n \n-\tbitmap_clear(cluster->bitmap, slot_index, 1);\n+\tbitmap_clear(cluster->bitmap, slot_index, nr);\n \n \t/* we only free uncached empty clusters */\n-\tif (refcount_dec_and_test(&cluster->refcnt))\n+\tif (refcount_sub_and_test(nr, &cluster->refcnt))\n \t\tvswap_cluster_free(cluster);\n \telse if (cluster->full && cluster_is_alloc_candidate(cluster)) {\n \t\tcluster->full = false;\n@@ -505,7 +505,7 @@ static inline void release_vswap_slot(struct vswap_cluster *cluster,\n \t\t}\n \t}\n \n-\tatomic_dec(&vswap_used);\n+\tatomic_sub(nr, &vswap_used);\n }\n \n /*\n@@ -527,23 +527,29 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n }\n \n /*\n- * Caller needs to handle races with other operations themselves.\n+ * release_backing - release the backend storage for a given range of virtual\n+ * swap slots.\n+ *\n+ * Entered with the cluster locked, but might drop the lock in between.\n+ * This is because several operations, such as releasing physical swap slots\n+ * (i.e swap_slot_free_nr()) require the cluster to be unlocked to avoid\n+ * deadlocks.\n  *\n- * Specifically, this function is safe to be called in contexts where the swap\n- * entry has been added to the swap cache and the associated folio is locked.\n- * We cannot race with other accessors, and the swap entry is guaranteed to be\n- * valid the whole time (since swap cache implies one refcount).\n+ * This is safe, because:\n+ *\n+ * 1. The swap entry to be freed has refcnt (swap count and swapcache pin)\n+ *    down to 0, so no one can change its internal state\n  *\n- * We cannot assume that the backends will be of the same type,\n- * contiguous, etc. We might have a large folio coalesced from subpages with\n- * mixed backend, which is only rectified when it is reclaimed.\n+ * 2. The swap entry to be freed still holds a refcnt to the cluster, keeping\n+ *    the cluster itself valid.\n+ *\n+ * We will exit the function with the cluster re-locked.\n  */\n- static void release_backing(swp_entry_t entry, int nr)\n+static void release_backing(struct vswap_cluster *cluster, swp_entry_t entry,\n+\t\tint nr)\n {\n-\tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n \tunsigned long flush_nr, phys_swap_start = 0, phys_swap_end = 0;\n-\tunsigned long phys_swap_released = 0;\n \tunsigned int phys_swap_type = 0;\n \tbool need_flushing_phys_swap = false;\n \tswp_slot_t flush_slot;\n@@ -551,9 +557,8 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \n \tVM_WARN_ON(!entry.val);\n \n-\trcu_read_lock();\n \tfor (i = 0; i < nr; i++) {\n-\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n \t\tVM_WARN_ON(!desc);\n \n \t\t/*\n@@ -573,7 +578,6 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\tif (desc->type == VSWAP_ZSWAP && desc->zswap_entry) {\n \t\t\tzswap_entry_free(desc->zswap_entry);\n \t\t} else if (desc->type == VSWAP_SWAPFILE) {\n-\t\t\tphys_swap_released++;\n \t\t\tif (!phys_swap_start) {\n \t\t\t\t/* start a new contiguous range of phys swap */\n \t\t\t\tphys_swap_start = swp_slot_offset(desc->slot);\n@@ -589,56 +593,49 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \n \t\tif (need_flushing_phys_swap) {\n \t\t\tspin_unlock(&cluster->lock);\n-\t\t\tcluster = NULL;\n \t\t\tswap_slot_free_nr(flush_slot, flush_nr);\n+\t\t\tmem_cgroup_uncharge_swap(entry, flush_nr);\n+\t\t\tspin_lock(&cluster->lock);\n \t\t\tneed_flushing_phys_swap = false;\n \t\t}\n \t}\n-\tif (cluster)\n-\t\tspin_unlock(&cluster->lock);\n-\trcu_read_unlock();\n \n \t/* Flush any remaining physical swap range */\n \tif (phys_swap_start) {\n \t\tflush_slot = swp_slot(phys_swap_type, phys_swap_start);\n \t\tflush_nr = phys_swap_end - phys_swap_start;\n+\t\tspin_unlock(&cluster->lock);\n \t\tswap_slot_free_nr(flush_slot, flush_nr);\n+\t\tmem_cgroup_uncharge_swap(entry, flush_nr);\n+\t\tspin_lock(&cluster->lock);\n \t}\n+}\n \n-\tif (phys_swap_released)\n-\t\tmem_cgroup_uncharge_swap(entry, phys_swap_released);\n- }\n+static void __vswap_swap_cgroup_clear(struct vswap_cluster *cluster,\n+\t\tswp_entry_t entry, unsigned int nr_ents);\n \n /*\n- * Entered with the cluster locked, but might unlock the cluster.\n- * This is because several operations, such as releasing physical swap slots\n- * (i.e swap_slot_free_nr()) require the cluster to be unlocked to avoid\n- * deadlocks.\n- *\n- * This is safe, because:\n- *\n- * 1. The swap entry to be freed has refcnt (swap count and swapcache pin)\n- *    down to 0, so no one can change its internal state\n- *\n- * 2. The swap entry to be freed still holds a refcnt to the cluster, keeping\n- *    the cluster itself valid.\n- *\n- * We will exit the function with the cluster re-locked.\n+ * Entered with the cluster locked. We will exit the function with the cluster\n+ * still locked.\n  */\n-static void vswap_free(struct vswap_cluster *cluster, struct swp_desc *desc,\n-\tswp_entry_t entry)\n+static void vswap_free_nr(struct vswap_cluster *cluster, swp_entry_t entry,\n+\t\tint nr)\n {\n-\t/* Clear shadow if present */\n-\tif (xa_is_value(desc->shadow))\n-\t\tdesc->shadow = NULL;\n-\tspin_unlock(&cluster->lock);\n+\tstruct swp_desc *desc;\n+\tint i;\n \n-\trelease_backing(entry, 1);\n-\tmem_cgroup_clear_swap(entry, 1);\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n+\t\t/* Clear shadow if present */\n+\t\tif (xa_is_value(desc->shadow))\n+\t\t\tdesc->shadow = NULL;\n+\t}\n \n-\t/* erase forward mapping and release the virtual slot for reallocation */\n-\tspin_lock(&cluster->lock);\n-\trelease_vswap_slot(cluster, entry.val);\n+\trelease_backing(cluster, entry, nr);\n+\t__vswap_swap_cgroup_clear(cluster, entry, nr);\n+\n+\t/* erase forward mapping and release the virtual slots for reallocation */\n+\trelease_vswap_slot_nr(cluster, entry.val, nr);\n }\n \n /**\n@@ -820,18 +817,32 @@ static bool vswap_free_nr_any_cache_only(swp_entry_t entry, int nr)\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n \tbool ret = false;\n-\tint i;\n+\tswp_entry_t free_start;\n+\tint i, free_nr = 0;\n \n+\tfree_start.val = 0;\n \trcu_read_lock();\n \tfor (i = 0; i < nr; i++) {\n+\t\t/* flush pending free batch at cluster boundary */\n+\t\tif (free_nr && !VSWAP_IDX_WITHIN_CLUSTER_VAL(entry.val)) {\n+\t\t\tvswap_free_nr(cluster, free_start, free_nr);\n+\t\t\tfree_nr = 0;\n+\t\t}\n \t\tdesc = vswap_iter(&cluster, entry.val);\n \t\tVM_WARN_ON(!desc);\n \t\tret |= (desc->swap_count == 1 && desc->in_swapcache);\n \t\tdesc->swap_count--;\n-\t\tif (!desc->swap_count && !desc->in_swapcache)\n-\t\t\tvswap_free(cluster, desc, entry);\n+\t\tif (!desc->swap_count && !desc->in_swapcache) {\n+\t\t\tif (!free_nr++)\n+\t\t\t\tfree_start = entry;\n+\t\t} else if (free_nr) {\n+\t\t\tvswap_free_nr(cluster, free_start, free_nr);\n+\t\t\tfree_nr = 0;\n+\t\t}\n \t\tentry.val++;\n \t}\n+\tif (free_nr)\n+\t\tvswap_free_nr(cluster, free_start, free_nr);\n \tif (cluster)\n \t\tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n@@ -954,19 +965,33 @@ void swapcache_clear(swp_entry_t entry, int nr)\n {\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n-\tint i;\n+\tswp_entry_t free_start;\n+\tint i, free_nr = 0;\n \n \tif (!nr)\n \t\treturn;\n \n+\tfree_start.val = 0;\n \trcu_read_lock();\n \tfor (i = 0; i < nr; i++) {\n+\t\t/* flush pending free batch at cluster boundary */\n+\t\tif (free_nr && !VSWAP_IDX_WITHIN_CLUSTER_VAL(entry.val)) {\n+\t\t\tvswap_free_nr(cluster, free_start, free_nr);\n+\t\t\tfree_nr = 0;\n+\t\t}\n \t\tdesc = vswap_iter(&cluster, entry.val);\n \t\tdesc->in_swapcache = false;\n-\t\tif (!desc->swap_count)\n-\t\t\tvswap_free(cluster, desc, entry);\n+\t\tif (!desc->swap_count) {\n+\t\t\tif (!free_nr++)\n+\t\t\t\tfree_start = entry;\n+\t\t} else if (free_nr) {\n+\t\t\tvswap_free_nr(cluster, free_start, free_nr);\n+\t\t\tfree_nr = 0;\n+\t\t}\n \t\tentry.val++;\n \t}\n+\tif (free_nr)\n+\t\tvswap_free_nr(cluster, free_start, free_nr);\n \tif (cluster)\n \t\tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n@@ -1107,11 +1132,13 @@ void vswap_store_folio(swp_entry_t entry, struct folio *folio)\n \tVM_BUG_ON(!folio_test_locked(folio));\n \tVM_BUG_ON(folio->swap.val != entry.val);\n \n-\trelease_backing(entry, nr);\n-\n \trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tVM_WARN_ON(!desc);\n+\trelease_backing(cluster, entry, nr);\n+\n \tfor (i = 0; i < nr; i++) {\n-\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n \t\tVM_WARN_ON(!desc);\n \t\tdesc->type = VSWAP_FOLIO;\n \t\tdesc->swap_cache = folio;\n@@ -1136,11 +1163,13 @@ void swap_zeromap_folio_set(struct folio *folio)\n \tVM_BUG_ON(!folio_test_locked(folio));\n \tVM_BUG_ON(!entry.val);\n \n-\trelease_backing(entry, nr);\n-\n \trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tVM_WARN_ON(!desc);\n+\trelease_backing(cluster, entry, nr);\n+\n \tfor (i = 0; i < nr; i++) {\n-\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n \t\tVM_WARN_ON(!desc);\n \t\tdesc->type = VSWAP_ZERO;\n \t}\n@@ -1261,89 +1290,6 @@ bool vswap_can_swapin_thp(swp_entry_t entry, int nr)\n \t\t(type == VSWAP_ZERO || type == VSWAP_SWAPFILE);\n }\n \n-/**\n- * swap_move - increment the swap slot by delta, checking the backing state and\n- *             return 0 if the backing state does not match (i.e wrong backing\n- *             state type, or wrong offset on the backing stores).\n- * @entry: the original virtual swap slot.\n- * @delta: the offset to increment the original slot.\n- *\n- * Note that this function is racy unless we can pin the backing state of these\n- * swap slots down with swapcache_prepare().\n- *\n- * Caller should only rely on this function as a best-effort hint otherwise,\n- * and should double-check after ensuring the whole range is pinned down.\n- *\n- * Return: the incremented virtual swap slot if the backing state matches, or\n- *         0 if the backing state does not match.\n- */\n-swp_entry_t swap_move(swp_entry_t entry, long delta)\n-{\n-\tstruct vswap_cluster *cluster = NULL;\n-\tstruct swp_desc *desc, *next_desc;\n-\tswp_entry_t next_entry;\n-\tstruct folio *folio = NULL, *next_folio = NULL;\n-\tenum swap_type type, next_type;\n-\tswp_slot_t slot = {0}, next_slot = {0};\n-\n-\tnext_entry.val = entry.val + delta;\n-\n-\trcu_read_lock();\n-\n-\t/* Look up first descriptor and get its type and backing store */\n-\tdesc = vswap_iter(&cluster, entry.val);\n-\tif (!desc) {\n-\t\trcu_read_unlock();\n-\t\treturn (swp_entry_t){0};\n-\t}\n-\n-\ttype = desc->type;\n-\tif (type == VSWAP_ZSWAP) {\n-\t\t/* zswap not supported for move */\n-\t\tspin_unlock(&cluster->lock);\n-\t\trcu_read_unlock();\n-\t\treturn (swp_entry_t){0};\n-\t}\n-\tif (type == VSWAP_FOLIO)\n-\t\tfolio = desc->swap_cache;\n-\telse if (type == VSWAP_SWAPFILE)\n-\t\tslot = desc->slot;\n-\n-\t/* Look up second descriptor and get its type and backing store */\n-\tnext_desc = vswap_iter(&cluster, next_entry.val);\n-\tif (!next_desc) {\n-\t\trcu_read_unlock();\n-\t\treturn (swp_entry_t){0};\n-\t}\n-\n-\tnext_type = next_desc->type;\n-\tif (next_type == VSWAP_FOLIO)\n-\t\tnext_folio = next_desc->swap_cache;\n-\telse if (next_type == VSWAP_SWAPFILE)\n-\t\tnext_slot = next_desc->slot;\n-\n-\tif (cluster)\n-\t\tspin_unlock(&cluster->lock);\n-\n-\trcu_read_unlock();\n-\n-\t/* Check if types match */\n-\tif (next_type != type)\n-\t\treturn (swp_entry_t){0};\n-\n-\t/* Check backing state consistency */\n-\tif (type == VSWAP_SWAPFILE &&\n-\t\t\t(swp_slot_type(next_slot) != swp_slot_type(slot) ||\n-\t\t\t\tswp_slot_offset(next_slot) !=\n-\t\t\t\t\t\t\tswp_slot_offset(slot) + delta))\n-\t\treturn (swp_entry_t){0};\n-\n-\tif (type == VSWAP_FOLIO && next_folio != folio)\n-\t\treturn (swp_entry_t){0};\n-\n-\treturn next_entry;\n-}\n-\n /*\n  * Return the count of contiguous swap entries that share the same\n  * VSWAP_ZERO status as the starting entry. If is_zeromap is not NULL,\n@@ -1863,11 +1809,10 @@ void zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry)\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n \n-\trelease_backing(swpentry, 1);\n-\n \trcu_read_lock();\n \tdesc = vswap_iter(&cluster, swpentry.val);\n \tVM_WARN_ON(!desc);\n+\trelease_backing(cluster, swpentry, 1);\n \tdesc->zswap_entry = entry;\n \tdesc->type = VSWAP_ZSWAP;\n \tspin_unlock(&cluster->lock);\n@@ -1914,17 +1859,22 @@ bool zswap_empty(swp_entry_t swpentry)\n #endif /* CONFIG_ZSWAP */\n \n #ifdef CONFIG_MEMCG\n-static unsigned short vswap_cgroup_record(swp_entry_t entry,\n-\t\t\t\tunsigned short memcgid, unsigned int nr_ents)\n+/*\n+ * __vswap_cgroup_record - record mem_cgroup for a set of swap entries\n+ *\n+ * Entered with the cluster locked. We will exit the function with the cluster\n+ * still locked.\n+ */\n+static unsigned short __vswap_cgroup_record(struct vswap_cluster *cluster,\n+\t\t\t\tswp_entry_t entry, unsigned short memcgid,\n+\t\t\t\tunsigned int nr_ents)\n {\n-\tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n \tunsigned short oldid, iter = 0;\n \tint i;\n \n-\trcu_read_lock();\n \tfor (i = 0; i < nr_ents; i++) {\n-\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n \t\tVM_WARN_ON(!desc);\n \t\toldid = desc->memcgid;\n \t\tdesc->memcgid = memcgid;\n@@ -1932,6 +1882,37 @@ static unsigned short vswap_cgroup_record(swp_entry_t entry,\n \t\t\titer = oldid;\n \t\tVM_WARN_ON(iter != oldid);\n \t}\n+\n+\treturn oldid;\n+}\n+\n+/*\n+ * Clear swap cgroup for a range of swap entries.\n+ * Entered with the cluster locked. Caller must be under rcu_read_lock().\n+ */\n+static void __vswap_swap_cgroup_clear(struct vswap_cluster *cluster,\n+\t\t\t\t      swp_entry_t entry, unsigned int nr_ents)\n+{\n+\tunsigned short id;\n+\tstruct mem_cgroup *memcg;\n+\n+\tid = __vswap_cgroup_record(cluster, entry, 0, nr_ents);\n+\tmemcg = mem_cgroup_from_id(id);\n+\tif (memcg)\n+\t\tmem_cgroup_id_put_many(memcg, nr_ents);\n+}\n+\n+static unsigned short vswap_cgroup_record(swp_entry_t entry,\n+\t\t\t\tunsigned short memcgid, unsigned int nr_ents)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tunsigned short oldid;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tVM_WARN_ON(!desc);\n+\toldid = __vswap_cgroup_record(cluster, entry, memcgid, nr_ents);\n \tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n \n@@ -1999,6 +1980,11 @@ unsigned short lookup_swap_cgroup_id(swp_entry_t entry)\n \trcu_read_unlock();\n \treturn ret;\n }\n+#else /* !CONFIG_MEMCG */\n+static void __vswap_swap_cgroup_clear(struct vswap_cluster *cluster,\n+\t\t\t\t      swp_entry_t entry, unsigned int nr_ents)\n+{\n+}\n #endif /* CONFIG_MEMCG */\n \n int vswap_init(void)\n-- \n2.47.3",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 00/20] Virtual Swap Space",
          "message_id": "20260208223900.428408-1-nphamcs@gmail.com",
          "url": "https://lore.kernel.org/all/20260208223900.428408-1-nphamcs@gmail.com/",
          "date": "2026-02-09T00:32:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-09",
          "patch_summary": "This patch decouples the swap cache from physical swap infrastructure to prepare for virtualizing the swap space. It removes swap cache related helpers of the swap table and introduces a single global lock for synchronizing swap cache accesses, which will be replaced by virtual swap clusters in the future.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Nhat Pham (author)",
              "summary": "Author addressed a concern about the organization of the swap API in include/linux/swap.h, agreeing to group functions into lifecycle, cache, and physical allocator categories for clarity, with no functional changes intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a clean-up",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "In the swap header file (include/linux/swap.h), group the swap API into\nthe following categories:\n\n1. Lifecycle swap functions (i.e the function that changes the reference\n   count of the swap entry).\n\n2. Swap cache API.\n\n3. Physical swapfile allocator and swap device API.\n\nAlso remove extern in the functions that are rearranged.\n\nThis is purely a clean up. No functional change intended.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h | 53 +++++++++++++++++++++++---------------------\n 1 file changed, 28 insertions(+), 25 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 38ca3df687160..aa29d8ac542d1 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -423,20 +423,34 @@ extern void __meminit kswapd_stop(int nid);\n \n #ifdef CONFIG_SWAP\n \n-int add_swap_extent(struct swap_info_struct *sis, unsigned long start_page,\n-\t\tunsigned long nr_pages, sector_t start_block);\n-int generic_swapfile_activate(struct swap_info_struct *, struct file *,\n-\t\tsector_t *);\n-\n+/* Lifecycle swap API (mm/swapfile.c) */\n+int folio_alloc_swap(struct folio *folio);\n+bool folio_free_swap(struct folio *folio);\n+void put_swap_folio(struct folio *folio, swp_entry_t entry);\n+void swap_shmem_alloc(swp_entry_t, int);\n+int swap_duplicate(swp_entry_t);\n+int swapcache_prepare(swp_entry_t entry, int nr);\n+void swap_free_nr(swp_entry_t entry, int nr_pages);\n+void free_swap_and_cache_nr(swp_entry_t entry, int nr);\n+int __swap_count(swp_entry_t entry);\n+bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry);\n+int swp_swapcount(swp_entry_t entry);\n+\n+/* Swap cache API (mm/swap_state.c) */\n static inline unsigned long total_swapcache_pages(void)\n {\n \treturn global_node_page_state(NR_SWAPCACHE);\n }\n-\n-void free_swap_cache(struct folio *folio);\n void free_folio_and_swap_cache(struct folio *folio);\n void free_pages_and_swap_cache(struct encoded_page **, int);\n-/* linux/mm/swapfile.c */\n+void free_swap_cache(struct folio *folio);\n+\n+/* Physical swap allocator and swap device API (mm/swapfile.c) */\n+int add_swap_extent(struct swap_info_struct *sis, unsigned long start_page,\n+\t\tunsigned long nr_pages, sector_t start_block);\n+int generic_swapfile_activate(struct swap_info_struct *, struct file *,\n+\t\tsector_t *);\n+\n extern atomic_long_t nr_swap_pages;\n extern long total_swap_pages;\n extern atomic_t nr_rotate_swap;\n@@ -452,26 +466,15 @@ static inline long get_nr_swap_pages(void)\n \treturn atomic_long_read(&nr_swap_pages);\n }\n \n-extern void si_swapinfo(struct sysinfo *);\n-int folio_alloc_swap(struct folio *folio);\n-bool folio_free_swap(struct folio *folio);\n-void put_swap_folio(struct folio *folio, swp_entry_t entry);\n-extern swp_entry_t get_swap_page_of_type(int);\n-extern int add_swap_count_continuation(swp_entry_t, gfp_t);\n-extern void swap_shmem_alloc(swp_entry_t, int);\n-extern int swap_duplicate(swp_entry_t);\n-extern int swapcache_prepare(swp_entry_t entry, int nr);\n-extern void swap_free_nr(swp_entry_t entry, int nr_pages);\n-extern void free_swap_and_cache_nr(swp_entry_t entry, int nr);\n+void si_swapinfo(struct sysinfo *);\n+swp_entry_t get_swap_page_of_type(int);\n+int add_swap_count_continuation(swp_entry_t, gfp_t);\n int swap_type_of(dev_t device, sector_t offset);\n int find_first_swap(dev_t *device);\n-extern unsigned int count_swap_pages(int, int);\n-extern sector_t swapdev_block(int, pgoff_t);\n-extern int __swap_count(swp_entry_t entry);\n-extern bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry);\n-extern int swp_swapcount(swp_entry_t entry);\n+unsigned int count_swap_pages(int, int);\n+sector_t swapdev_block(int, pgoff_t);\n struct backing_dev_info;\n-extern struct swap_info_struct *get_swap_device(swp_entry_t entry);\n+struct swap_info_struct *get_swap_device(swp_entry_t entry);\n sector_t swap_folio_sector(struct folio *folio);\n \n static inline void put_swap_device(struct swap_info_struct *si)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, and provided a generic API to abstract away the swapoff locking out behavior.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "agreed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Currently, we get a reference to the backing swap device in order to\nprevent swapoff from freeing the metadata of a swap entry. This does not\nmake sense in the new virtual swap design, especially after the swap\nbackends are decoupled - a swap entry might not have any backing swap\ndevice at all, and its backend might change at any time during its\nlifetime.\n\nIn preparation for this, abstract away the swapoff locking out behavior\ninto a generic API.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h | 17 +++++++++++++++++\n mm/memory.c          | 13 +++++++------\n mm/mincore.c         | 15 +++------------\n mm/shmem.c           | 12 ++++++------\n mm/swap_state.c      | 14 +++++++-------\n mm/userfaultfd.c     | 15 +++++++++------\n mm/zswap.c           |  5 ++---\n 7 files changed, 51 insertions(+), 40 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex aa29d8ac542d1..3da637b218baf 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -659,5 +659,22 @@ static inline bool mem_cgroup_swap_full(struct folio *folio)\n }\n #endif\n \n+static inline bool tryget_swap_entry(swp_entry_t entry,\n+\t\t\t\tstruct swap_info_struct **sip)\n+{\n+\tstruct swap_info_struct *si = get_swap_device(entry);\n+\n+\tif (sip)\n+\t\t*sip = si;\n+\n+\treturn si;\n+}\n+\n+static inline void put_swap_entry(swp_entry_t entry,\n+\t\t\t\tstruct swap_info_struct *si)\n+{\n+\tput_swap_device(si);\n+}\n+\n #endif /* __KERNEL__*/\n #endif /* _LINUX_SWAP_H */\ndiff --git a/mm/memory.c b/mm/memory.c\nindex da360a6eb8a48..90031f833f52e 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -4630,6 +4630,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \tstruct swap_info_struct *si = NULL;\n \trmap_t rmap_flags = RMAP_NONE;\n \tbool need_clear_cache = false;\n+\tbool swapoff_locked = false;\n \tbool exclusive = false;\n \tsoftleaf_t entry;\n \tpte_t pte;\n@@ -4698,8 +4699,8 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t}\n \n \t/* Prevent swapoff from happening to us. */\n-\tsi = get_swap_device(entry);\n-\tif (unlikely(!si))\n+\tswapoff_locked = tryget_swap_entry(entry, &si);\n+\tif (unlikely(!swapoff_locked))\n \t\tgoto out;\n \n \tfolio = swap_cache_get_folio(entry);\n@@ -5047,8 +5048,8 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\tif (waitqueue_active(&swapcache_wq))\n \t\t\twake_up(&swapcache_wq);\n \t}\n-\tif (si)\n-\t\tput_swap_device(si);\n+\tif (swapoff_locked)\n+\t\tput_swap_entry(entry, si);\n \treturn ret;\n out_nomap:\n \tif (vmf->pte)\n@@ -5066,8 +5067,8 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\tif (waitqueue_active(&swapcache_wq))\n \t\t\twake_up(&swapcache_wq);\n \t}\n-\tif (si)\n-\t\tput_swap_device(si);\n+\tif (swapoff_locked)\n+\t\tput_swap_entry(entry, si);\n \treturn ret;\n }\n \ndiff --git a/mm/mincore.c b/mm/mincore.c\nindex e5d13eea92347..f3eb771249d67 100644\n--- a/mm/mincore.c\n+++ b/mm/mincore.c\n@@ -77,19 +77,10 @@ static unsigned char mincore_swap(swp_entry_t entry, bool shmem)\n \tif (!softleaf_is_swap(entry))\n \t\treturn !shmem;\n \n-\t/*\n-\t * Shmem mapping lookup is lockless, so we need to grab the swap\n-\t * device. mincore page table walk locks the PTL, and the swap\n-\t * device is stable, avoid touching the si for better performance.\n-\t */\n-\tif (shmem) {\n-\t\tsi = get_swap_device(entry);\n-\t\tif (!si)\n-\t\t\treturn 0;\n-\t}\n+\tif (!tryget_swap_entry(entry, &si))\n+\t\treturn 0;\n \tfolio = swap_cache_get_folio(entry);\n-\tif (shmem)\n-\t\tput_swap_device(si);\n+\tput_swap_entry(entry, si);\n \t/* The swap cache space contains either folio, shadow or NULL */\n \tif (folio && !xa_is_value(folio)) {\n \t\tpresent = folio_test_uptodate(folio);\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 1db97ef2d14eb..b40be22fa5f09 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -2307,7 +2307,7 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \tsoftleaf_t index_entry;\n \tstruct swap_info_struct *si;\n \tstruct folio *folio = NULL;\n-\tbool skip_swapcache = false;\n+\tbool swapoff_locked, skip_swapcache = false;\n \tint error, nr_pages, order;\n \tpgoff_t offset;\n \n@@ -2319,16 +2319,16 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \tif (softleaf_is_poison_marker(index_entry))\n \t\treturn -EIO;\n \n-\tsi = get_swap_device(index_entry);\n+\tswapoff_locked = tryget_swap_entry(index_entry, &si);\n \torder = shmem_confirm_swap(mapping, index, index_entry);\n-\tif (unlikely(!si)) {\n+\tif (unlikely(!swapoff_locked)) {\n \t\tif (order < 0)\n \t\t\treturn -EEXIST;\n \t\telse\n \t\t\treturn -EINVAL;\n \t}\n \tif (unlikely(order < 0)) {\n-\t\tput_swap_device(si);\n+\t\tput_swap_entry(index_entry, si);\n \t\treturn -EEXIST;\n \t}\n \n@@ -2448,7 +2448,7 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \t}\n \tfolio_mark_dirty(folio);\n \tswap_free_nr(swap, nr_pages);\n-\tput_swap_device(si);\n+\tput_swap_entry(swap, si);\n \n \t*foliop = folio;\n \treturn 0;\n@@ -2466,7 +2466,7 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \t\tswapcache_clear(si, folio->swap, folio_nr_pages(folio));\n \tif (folio)\n \t\tfolio_put(folio);\n-\tput_swap_device(si);\n+\tput_swap_entry(swap, si);\n \n \treturn error;\n }\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 34c9d9b243a74..bece18eb540fa 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -538,8 +538,7 @@ struct folio *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \tpgoff_t ilx;\n \tstruct folio *folio;\n \n-\tsi = get_swap_device(entry);\n-\tif (!si)\n+\tif (!tryget_swap_entry(entry, &si))\n \t\treturn NULL;\n \n \tmpol = get_vma_policy(vma, addr, 0, &ilx);\n@@ -550,7 +549,7 @@ struct folio *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \tif (page_allocated)\n \t\tswap_read_folio(folio, plug);\n \n-\tput_swap_device(si);\n+\tput_swap_entry(entry, si);\n \treturn folio;\n }\n \n@@ -763,6 +762,7 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \tfor (addr = start; addr < end; ilx++, addr += PAGE_SIZE) {\n \t\tstruct swap_info_struct *si = NULL;\n \t\tsoftleaf_t entry;\n+\t\tbool swapoff_locked = false;\n \n \t\tif (!pte++) {\n \t\t\tpte = pte_offset_map(vmf->pmd, addr);\n@@ -781,14 +781,14 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \t\t * holding a reference to, try to grab a reference, or skip.\n \t\t */\n \t\tif (swp_type(entry) != swp_type(targ_entry)) {\n-\t\t\tsi = get_swap_device(entry);\n-\t\t\tif (!si)\n+\t\t\tswapoff_locked = tryget_swap_entry(entry, &si);\n+\t\t\tif (!swapoff_locked)\n \t\t\t\tcontinue;\n \t\t}\n \t\tfolio = __read_swap_cache_async(entry, gfp_mask, mpol, ilx,\n \t\t\t\t\t\t&page_allocated, false);\n-\t\tif (si)\n-\t\t\tput_swap_device(si);\n+\t\tif (swapoff_locked)\n+\t\t\tput_swap_entry(entry, si);\n \t\tif (!folio)\n \t\t\tcontinue;\n \t\tif (page_allocated) {\ndiff --git a/mm/userfaultfd.c b/mm/userfaultfd.c\nindex e6dfd5f28acd7..25f89eba0438c 100644\n--- a/mm/userfaultfd.c\n+++ b/mm/userfaultfd.c\n@@ -1262,9 +1262,11 @@ static long move_pages_ptes(struct mm_struct *mm, pmd_t *dst_pmd, pmd_t *src_pmd\n \tpte_t *dst_pte = NULL;\n \tpmd_t dummy_pmdval;\n \tpmd_t dst_pmdval;\n+\tsoftleaf_t entry;\n \tstruct folio *src_folio = NULL;\n \tstruct mmu_notifier_range range;\n \tlong ret = 0;\n+\tbool swapoff_locked = false;\n \n \tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm,\n \t\t\t\tsrc_addr, src_addr + len);\n@@ -1429,7 +1431,7 @@ static long move_pages_ptes(struct mm_struct *mm, pmd_t *dst_pmd, pmd_t *src_pmd\n \t\t\t\t\tlen);\n \t} else { /* !pte_present() */\n \t\tstruct folio *folio = NULL;\n-\t\tconst softleaf_t entry = softleaf_from_pte(orig_src_pte);\n+\t\tentry = softleaf_from_pte(orig_src_pte);\n \n \t\tif (softleaf_is_migration(entry)) {\n \t\t\tpte_unmap(src_pte);\n@@ -1449,8 +1451,8 @@ static long move_pages_ptes(struct mm_struct *mm, pmd_t *dst_pmd, pmd_t *src_pmd\n \t\t\tgoto out;\n \t\t}\n \n-\t\tsi = get_swap_device(entry);\n-\t\tif (unlikely(!si)) {\n+\t\tswapoff_locked = tryget_swap_entry(entry, &si);\n+\t\tif (unlikely(!swapoff_locked)) {\n \t\t\tret = -EAGAIN;\n \t\t\tgoto out;\n \t\t}\n@@ -1480,8 +1482,9 @@ static long move_pages_ptes(struct mm_struct *mm, pmd_t *dst_pmd, pmd_t *src_pmd\n \t\t\t\tpte_unmap(src_pte);\n \t\t\t\tpte_unmap(dst_pte);\n \t\t\t\tsrc_pte = dst_pte = NULL;\n-\t\t\t\tput_swap_device(si);\n+\t\t\t\tput_swap_entry(entry, si);\n \t\t\t\tsi = NULL;\n+\t\t\t\tswapoff_locked = false;\n \t\t\t\t/* now we can block and wait */\n \t\t\t\tfolio_lock(src_folio);\n \t\t\t\tgoto retry;\n@@ -1507,8 +1510,8 @@ static long move_pages_ptes(struct mm_struct *mm, pmd_t *dst_pmd, pmd_t *src_pmd\n \tif (dst_pte)\n \t\tpte_unmap(dst_pte);\n \tmmu_notifier_invalidate_range_end(&range);\n-\tif (si)\n-\t\tput_swap_device(si);\n+\tif (swapoff_locked)\n+\t\tput_swap_entry(entry, si);\n \n \treturn ret;\n }\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex ac9b7a60736bc..315e4d0d08311 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -1009,14 +1009,13 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \tint ret = 0;\n \n \t/* try to allocate swap cache folio */\n-\tsi = get_swap_device(swpentry);\n-\tif (!si)\n+\tif (!tryget_swap_entry(swpentry, &si))\n \t\treturn -EEXIST;\n \n \tmpol = get_task_policy(current);\n \tfolio = __read_swap_cache_async(swpentry, GFP_KERNEL, mpol,\n \t\t\tNO_INTERLEAVE_INDEX, &folio_was_allocated, true);\n-\tput_swap_device(si);\n+\tput_swap_entry(swpentry, si);\n \tif (!folio)\n \t\treturn -ENOMEM;\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the lack of abstraction in zswap entry operations by adding new helper functions to facilitate re-implementation when swap is virtualized.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged feedback",
                "added new functionality"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add new helper functions to abstract away zswap entry operations, in\norder to facilitate re-implementing these functions when swap is\nvirtualized.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n mm/zswap.c | 59 ++++++++++++++++++++++++++++++++++++------------------\n 1 file changed, 40 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex 315e4d0d08311..a5a3f068bd1a6 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -234,6 +234,38 @@ static inline struct xarray *swap_zswap_tree(swp_entry_t swp)",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author addressed a concern about the swap_zswap_tree() function being called multiple times, acknowledged that it's inefficient and agreed to replace it with zswap_entry_load(), zswap_entry_erase(), and zswap_empty() functions in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged inefficiency",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "}\n \n+static inline void *zswap_entry_store(swp_entry_t swpentry,\n+\t\tstruct zswap_entry *entry)\n+{\n+\tstruct xarray *tree = swap_zswap_tree(swpentry);\n+\tpgoff_t offset = swp_offset(swpentry);\n+\n+\treturn xa_store(tree, offset, entry, GFP_KERNEL);\n+}\n+\n+static inline void *zswap_entry_load(swp_entry_t swpentry)\n+{\n+\tstruct xarray *tree = swap_zswap_tree(swpentry);\n+\tpgoff_t offset = swp_offset(swpentry);\n+\n+\treturn xa_load(tree, offset);\n+}\n+\n+static inline void *zswap_entry_erase(swp_entry_t swpentry)\n+{\n+\tstruct xarray *tree = swap_zswap_tree(swpentry);\n+\tpgoff_t offset = swp_offset(swpentry);\n+\n+\treturn xa_erase(tree, offset);\n+}\n+\n+static inline bool zswap_empty(swp_entry_t swpentry)\n+{\n+\tstruct xarray *tree = swap_zswap_tree(swpentry);\n+\n+\treturn xa_empty(tree);\n+}\n+\n #define zswap_pool_debug(msg, p)\t\t\t\\\n \tpr_debug(\"%s pool %s\\n\", msg, (p)->tfm_name)\n \n@@ -1000,8 +1032,6 @@ static bool zswap_decompress(struct zswap_entry *entry, struct folio *folio)\n static int zswap_writeback_entry(struct zswap_entry *entry,\n \t\t\t\t swp_entry_t swpentry)\n {\n-\tstruct xarray *tree;\n-\tpgoff_t offset = swp_offset(swpentry);\n \tstruct folio *folio;\n \tstruct mempolicy *mpol;\n \tbool folio_was_allocated;\n@@ -1040,8 +1070,7 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \t * old compressed data. Only when this is successful can the entry\n \t * be dereferenced.\n \t */\n-\ttree = swap_zswap_tree(swpentry);\n-\tif (entry != xa_load(tree, offset)) {\n+\tif (entry != zswap_entry_load(swpentry)) {\n \t\tret = -ENOMEM;\n \t\tgoto out;\n \t}\n@@ -1051,7 +1080,7 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \t\tgoto out;\n \t}\n \n-\txa_erase(tree, offset);\n+\tzswap_entry_erase(swpentry);\n \n \tcount_vm_event(ZSWPWB);\n \tif (entry->objcg)\n@@ -1427,9 +1456,7 @@ static bool zswap_store_page(struct page *page,\n \tif (!zswap_compress(page, entry, pool))\n \t\tgoto compress_failed;\n \n-\told = xa_store(swap_zswap_tree(page_swpentry),\n-\t\t       swp_offset(page_swpentry),\n-\t\t       entry, GFP_KERNEL);\n+\told = zswap_entry_store(page_swpentry, entry);\n \tif (xa_is_err(old)) {\n \t\tint err = xa_err(old);\n \n@@ -1563,11 +1590,9 @@ bool zswap_store(struct folio *folio)\n \t\tunsigned type = swp_type(swp);\n \t\tpgoff_t offset = swp_offset(swp);\n \t\tstruct zswap_entry *entry;\n-\t\tstruct xarray *tree;\n \n \t\tfor (index = 0; index < nr_pages; ++index) {\n-\t\t\ttree = swap_zswap_tree(swp_entry(type, offset + index));\n-\t\t\tentry = xa_erase(tree, offset + index);\n+\t\t\tentry = zswap_entry_erase(swp_entry(type, offset + index));\n \t\t\tif (entry)\n \t\t\t\tzswap_entry_free(entry);\n \t\t}\n@@ -1599,9 +1624,7 @@ bool zswap_store(struct folio *folio)\n int zswap_load(struct folio *folio)\n {\n \tswp_entry_t swp = folio->swap;\n-\tpgoff_t offset = swp_offset(swp);\n \tbool swapcache = folio_test_swapcache(folio);\n-\tstruct xarray *tree = swap_zswap_tree(swp);\n \tstruct zswap_entry *entry;\n \n \tVM_WARN_ON_ONCE(!folio_test_locked(folio));\n@@ -1619,7 +1642,7 @@ int zswap_load(struct folio *folio)\n \t\treturn -EINVAL;\n \t}\n \n-\tentry = xa_load(tree, offset);\n+\tentry = zswap_entry_load(swp);\n \tif (!entry)\n \t\treturn -ENOENT;\n \n@@ -1648,7 +1671,7 @@ int zswap_load(struct folio *folio)\n \t */\n \tif (swapcache) {\n \t\tfolio_mark_dirty(folio);\n-\t\txa_erase(tree, offset);\n+\t\tzswap_entry_erase(swp);\n \t\tzswap_entry_free(entry);\n \t}\n \n@@ -1658,14 +1681,12 @@ int zswap_load(struct folio *folio)\n \n void zswap_invalidate(swp_entry_t swp)\n {\n-\tpgoff_t offset = swp_offset(swp);\n-\tstruct xarray *tree = swap_zswap_tree(swp);\n \tstruct zswap_entry *entry;\n \n-\tif (xa_empty(tree))\n+\tif (zswap_empty(swp))\n \t\treturn;\n \n-\tentry = xa_erase(tree, offset);\n+\tentry = zswap_entry_erase(swp);\n \tif (entry)\n \t\tzswap_entry_free(entry);\n }\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author addressed a concern about the swap cached state being managed at the virtual swap layer and promised to abstract away the function in userfaultfd.c that checks whether a swap entry is in swapcache by directly looking at the swapfile's swap map.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Userfaultfd checks whether a swap entry is in swapcache. This is\ncurrently done by directly looking at the swapfile's swap map - however,\nthe swap cached state will soon be managed at the virtual swap layer.\nAbstract away this function.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h |  6 ++++++\n mm/swapfile.c        | 15 +++++++++++++++\n mm/userfaultfd.c     |  3 +--\n 3 files changed, 22 insertions(+), 2 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 3da637b218baf..f91a442ac0e82 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -435,6 +435,7 @@ void free_swap_and_cache_nr(swp_entry_t entry, int nr);\n int __swap_count(swp_entry_t entry);\n bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry);\n int swp_swapcount(swp_entry_t entry);\n+bool is_swap_cached(swp_entry_t entry);\n \n /* Swap cache API (mm/swap_state.c) */\n static inline unsigned long total_swapcache_pages(void)\n@@ -554,6 +555,11 @@ static inline int swp_swapcount(swp_entry_t entry)\n \treturn 0;\n }\n \n+static inline bool is_swap_cached(swp_entry_t entry)\n+{\n+\treturn false;\n+}\n+\n static inline int folio_alloc_swap(struct folio *folio)\n {\n \treturn -EINVAL;\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex cacfafa9a540d..3c89dedbd5718 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -194,6 +194,21 @@ static bool swap_only_has_cache(struct swap_info_struct *si,\n \treturn true;\n }\n \n+/**\n+ * is_swap_cached - check if the swap entry is cached\n+ * @entry: swap entry to check\n+ *\n+ * Check swap_map directly to minimize overhead, READ_ONCE is sufficient.\n+ *\n+ * Returns true if the swap entry is cached, false otherwise.\n+ */\n+bool is_swap_cached(swp_entry_t entry)\n+{\n+\tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n+\n+\treturn READ_ONCE(si->swap_map[swp_offset(entry)]) & SWAP_HAS_CACHE;\n+}\n+\n static bool swap_is_last_map(struct swap_info_struct *si,\n \t\tunsigned long offset, int nr_pages, bool *has_cache)\n {\ndiff --git a/mm/userfaultfd.c b/mm/userfaultfd.c\nindex 25f89eba0438c..98be764fb3ecd 100644\n--- a/mm/userfaultfd.c\n+++ b/mm/userfaultfd.c\n@@ -1190,7 +1190,6 @@ static int move_swap_pte(struct mm_struct *mm, struct vm_area_struct *dst_vma,\n \t\t * Check if the swap entry is cached after acquiring the src_pte\n \t\t * lock. Otherwise, we might miss a newly loaded swap cache folio.\n \t\t *\n-\t\t * Check swap_map directly to minimize overhead, READ_ONCE is sufficient.\n \t\t * We are trying to catch newly added swap cache, the only possible case is\n \t\t * when a folio is swapped in and out again staying in swap cache, using the\n \t\t * same entry before the PTE check above. The PTL is acquired and released\n@@ -1200,7 +1199,7 @@ static int move_swap_pte(struct mm_struct *mm, struct vm_area_struct *dst_vma,\n \t\t * cache, or during the tiny synchronization window between swap cache and\n \t\t * swap_map, but it will be gone very quickly, worst result is retry jitters.\n \t\t */\n-\t\tif (READ_ONCE(si->swap_map[swp_offset(entry)]) & SWAP_HAS_CACHE) {\n+\t\tif (is_swap_cached(entry)) {\n \t\t\tdouble_pt_unlock(dst_ptl, src_ptl);\n \t\t\treturn -EAGAIN;\n \t\t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the swap virtualization implementation, specifically the creation of a new mm/vswap.c source file to hold the logic for setting up the vswap debugfs directory. The author confirmed that no behavioral changes are intended and added Johannes Weiner as a reviewer.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged concern",
                "no behavioral change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "In prepration for the implementation of swap virtualization, add new\nscaffolds for the new code: a new mm/vswap.c source file, which\ncurrently only holds the logic to set up the (for now, empty) vswap\ndebugfs directory. Hook this up in the swap setup step in\nmm/swap_state.c, and set up vswap compilation in the Makefile.\n\nOther than the debugfs directory, no behavioral change intended.\n\nFinally, make Johannes a swap reviewer, given that he has contributed\nmajorly to the developments of virtual swap.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n MAINTAINERS          |  2 ++\n include/linux/swap.h |  2 ++\n mm/Makefile          |  2 +-\n mm/swap_state.c      |  6 ++++++\n mm/vswap.c           | 35 +++++++++++++++++++++++++++++++++++\n 5 files changed, 46 insertions(+), 1 deletion(-)\n create mode 100644 mm/vswap.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex e087673237636..b21038b160a07 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16728,6 +16728,7 @@ R:\tKemeng Shi <shikemeng@huaweicloud.com>\n R:\tNhat Pham <nphamcs@gmail.com>\n R:\tBaoquan He <bhe@redhat.com>\n R:\tBarry Song <baohua@kernel.org>\n+R:\tJohannes Weiner <hannes@cmpxchg.org>\n L:\tlinux-mm@kvack.org\n S:\tMaintained\n F:\tDocumentation/mm/swap-table.rst\n@@ -16740,6 +16741,7 @@ F:\tmm/swap.h\n F:\tmm/swap_table.h\n F:\tmm/swap_state.c\n F:\tmm/swapfile.c\n+F:\tmm/vswap.c\n \n MEMORY MANAGEMENT - THP (TRANSPARENT HUGE PAGE)\n M:\tAndrew Morton <akpm@linux-foundation.org>\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 918b47da55f44..1ff463fb3a966 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -669,6 +669,8 @@ static inline bool mem_cgroup_swap_full(struct folio *folio)\n }\n #endif\n \n+int vswap_init(void);\n+\n /**\n  * swp_entry_to_swp_slot - look up the physical swap slot corresponding to a\n  *                         virtual swap slot.\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 2d0570a16e5be..67fa4586e7e18 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -75,7 +75,7 @@ ifdef CONFIG_MMU\n \tobj-$(CONFIG_ADVISE_SYSCALLS)\t+= madvise.o\n endif\n \n-obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o\n+obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o vswap.o\n obj-$(CONFIG_ZSWAP)\t+= zswap.o\n obj-$(CONFIG_HAS_DMA)\t+= dmapool.o\n obj-$(CONFIG_HUGETLBFS)\t+= hugetlb.o hugetlb_sysfs.o hugetlb_sysctl.o\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex e2e9f55bea3bb..29ec666be4204 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -882,6 +882,12 @@ static int __init swap_init(void)\n \tint err;\n \tstruct kobject *swap_kobj;\n \n+\terr = vswap_init();\n+\tif (err) {\n+\t\tpr_err(\"failed to initialize virtual swap space\\n\");\n+\t\treturn err;\n+\t}\n+\n \tswap_kobj = kobject_create_and_add(\"swap\", mm_kobj);\n \tif (!swap_kobj) {\n \t\tpr_err(\"failed to create swap kobject\\n\");\ndiff --git a/mm/vswap.c b/mm/vswap.c\nnew file mode 100644\nindex 0000000000000..e68234f053fc9\n--- /dev/null\n+++ b/mm/vswap.c\n@@ -0,0 +1,35 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * Virtual swap space\n+ *\n+ * Copyright (C) 2024 Meta Platforms, Inc., Nhat Pham\n+ */\n+#include <linux/swap.h>\n+\n+#ifdef CONFIG_DEBUG_FS\n+#include <linux/debugfs.h>\n+\n+static struct dentry *vswap_debugfs_root;\n+\n+static int vswap_debug_fs_init(void)\n+{\n+\tif (!debugfs_initialized())\n+\t\treturn -ENODEV;\n+\n+\tvswap_debugfs_root = debugfs_create_dir(\"vswap\", NULL);\n+\treturn 0;\n+}\n+#else\n+static int vswap_debug_fs_init(void)\n+{\n+\treturn 0;\n+}\n+#endif\n+\n+int vswap_init(void)\n+{\n+\tif (vswap_debug_fs_init())\n+\t\tpr_warn(\"Failed to initialize vswap debugfs\\n\");\n+\n+\treturn 0;\n+}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the need to decouple swap cache from physical swap infrastructure by introducing new types (swp_slot_t and swp_entry_t) to represent physical and virtual swap slots respectively, allowing for separation of logical and physical views. The author confirmed that no behavioral change has been made yet, with dynamic allocation of virtual swap slots planned in later patches.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "confirmed the approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "In preparation for swap virtualization, add a new type to represent the\nphysical swap slots of swapfile. This allows us to separates:\n\n1. The logical view of the swap entry (i.e what is stored in page table\n   entries and used to index into the swap cache), represented by the\n   old swp_entry_t type.\n\nfrom:\n\n2. Its physical backing state (i.e the actual backing slot on the swap\n   device), represented by the new swp_slot_t type.\n\nThe functions that operate at the physical level (i.e on the swp_slot_t\ntypes) are also renamed where appropriate (prefixed with swp_slot_* for\ne.g).\n\nNote that we have not made any behavioral change - the mapping between\nthe two types is the identity mapping. In later patches, we shall\ndynamically allocate a virtual swap slot (of type swp_entry_t) for each\nswapped out page to store in the page table entry, and associate it with\na backing store. A physical swap slot (i.e a slot on a physical swap\ndevice) is one of the backing options.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/mm_types.h |  16 +++\n include/linux/swap.h     |  47 ++++--\n include/linux/swapops.h  |  25 ++++\n kernel/power/swap.c      |   6 +-\n mm/internal.h            |  10 +-\n mm/page_io.c             |  33 +++--\n mm/shmem.c               |  19 ++-\n mm/swap.h                |  52 +++----\n mm/swap_cgroup.c         |  18 +--\n mm/swap_state.c          |  32 +++--\n mm/swapfile.c            | 300 ++++++++++++++++++++++-----------------\n 11 files changed, 352 insertions(+), 206 deletions(-)\n\ndiff --git a/include/linux/mm_types.h b/include/linux/mm_types.h\nindex 78950eb8926dc..bffde812decc5 100644\n--- a/include/linux/mm_types.h\n+++ b/include/linux/mm_types.h\n@@ -279,6 +279,13 @@ static __always_inline unsigned long encoded_nr_pages(struct encoded_page *page)\n }\n \n /*\n+ * Virtual swap slot.\n+ *\n+ * This type is used to represent a virtual swap slot, i.e an identifier of\n+ * a swap entry. This is stored in PTEs that originally refer to the swapped\n+ * out page, and is used to index into various swap architectures (swap cache,\n+ * zswap tree, swap cgroup array, etc.).\n+ *\n  * A swap entry has to fit into a \"unsigned long\", as the entry is hidden\n  * in the \"index\" field of the swapper address space.\n  */\n@@ -286,6 +293,15 @@ typedef struct {\n \tunsigned long val;\n } swp_entry_t;\n \n+/*\n+ * Physical swap slot.\n+ *\n+ * This type is used to represent a PAGE_SIZED slot on a swapfile.\n+ */\n+typedef struct {\n+\tunsigned long val;\n+} swp_slot_t;\n+\n /**\n  * typedef softleaf_t - Describes a page table software leaf entry, abstracted\n  * from its architecture-specific encoding.\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex f91a442ac0e82..918b47da55f44 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -241,7 +241,7 @@ enum {\n  * cluster to which it belongs being marked free. Therefore 0 is safe to use as\n  * a sentinel to indicate an entry is not valid.\n  */\n-#define SWAP_ENTRY_INVALID\t0\n+#define SWAP_SLOT_INVALID\t0\n \n #ifdef CONFIG_THP_SWAP\n #define SWAP_NR_ORDERS\t\t(PMD_ORDER + 1)\n@@ -442,11 +442,14 @@ static inline unsigned long total_swapcache_pages(void)\n {\n \treturn global_node_page_state(NR_SWAPCACHE);\n }\n+\n void free_folio_and_swap_cache(struct folio *folio);\n void free_pages_and_swap_cache(struct encoded_page **, int);\n void free_swap_cache(struct folio *folio);\n \n /* Physical swap allocator and swap device API (mm/swapfile.c) */\n+void swap_slot_free_nr(swp_slot_t slot, int nr_pages);\n+\n int add_swap_extent(struct swap_info_struct *sis, unsigned long start_page,\n \t\tunsigned long nr_pages, sector_t start_block);\n int generic_swapfile_activate(struct swap_info_struct *, struct file *,\n@@ -468,28 +471,28 @@ static inline long get_nr_swap_pages(void)\n }\n \n void si_swapinfo(struct sysinfo *);\n-swp_entry_t get_swap_page_of_type(int);\n+swp_slot_t swap_slot_alloc_of_type(int);\n int add_swap_count_continuation(swp_entry_t, gfp_t);\n int swap_type_of(dev_t device, sector_t offset);\n int find_first_swap(dev_t *device);\n unsigned int count_swap_pages(int, int);\n sector_t swapdev_block(int, pgoff_t);\n struct backing_dev_info;\n-struct swap_info_struct *get_swap_device(swp_entry_t entry);\n+struct swap_info_struct *swap_slot_tryget_swap_info(swp_slot_t slot);\n sector_t swap_folio_sector(struct folio *folio);\n \n-static inline void put_swap_device(struct swap_info_struct *si)\n+static inline void swap_slot_put_swap_info(struct swap_info_struct *si)\n {\n \tpercpu_ref_put(&si->users);\n }\n \n #else /* CONFIG_SWAP */\n-static inline struct swap_info_struct *get_swap_device(swp_entry_t entry)\n+static inline struct swap_info_struct *swap_slot_tryget_swap_info(swp_slot_t slot)\n {\n \treturn NULL;\n }\n \n-static inline void put_swap_device(struct swap_info_struct *si)\n+static inline void swap_slot_put_swap_info(struct swap_info_struct *si)\n {\n }\n \n@@ -536,7 +539,7 @@ static inline void swap_free_nr(swp_entry_t entry, int nr_pages)\n {\n }\n \n-static inline void put_swap_folio(struct folio *folio, swp_entry_t swp)\n+static inline void put_swap_folio(struct folio *folio, swp_entry_t entry)\n {\n }\n \n@@ -576,6 +579,7 @@ static inline int add_swap_extent(struct swap_info_struct *sis,\n {\n \treturn -EINVAL;\n }\n+\n #endif /* CONFIG_SWAP */\n \n static inline void free_swap_and_cache(swp_entry_t entry)\n@@ -665,10 +669,35 @@ static inline bool mem_cgroup_swap_full(struct folio *folio)\n }\n #endif\n \n+/**\n+ * swp_entry_to_swp_slot - look up the physical swap slot corresponding to a\n+ *                         virtual swap slot.\n+ * @entry: the virtual swap slot.\n+ *\n+ * Return: the physical swap slot corresponding to the virtual swap slot.\n+ */\n+static inline swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry)\n+{\n+\treturn (swp_slot_t) { entry.val };\n+}\n+\n+/**\n+ * swp_slot_to_swp_entry - look up the virtual swap slot corresponding to a\n+ *                         physical swap slot.\n+ * @slot: the physical swap slot.\n+ *\n+ * Return: the virtual swap slot corresponding to the physical swap slot.\n+ */\n+static inline swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot)\n+{\n+\treturn (swp_entry_t) { slot.val };\n+}\n+\n static inline bool tryget_swap_entry(swp_entry_t entry,\n \t\t\t\tstruct swap_info_struct **sip)\n {\n-\tstruct swap_info_struct *si = get_swap_device(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tstruct swap_info_struct *si = swap_slot_tryget_swap_info(slot);\n \n \tif (sip)\n \t\t*sip = si;\n@@ -679,7 +708,7 @@ static inline bool tryget_swap_entry(swp_entry_t entry,\n static inline void put_swap_entry(swp_entry_t entry,\n \t\t\t\tstruct swap_info_struct *si)\n {\n-\tput_swap_device(si);\n+\tswap_slot_put_swap_info(si);\n }\n \n #endif /* __KERNEL__*/\ndiff --git a/include/linux/swapops.h b/include/linux/swapops.h\nindex 8cfc966eae48e..9e41c35664a95 100644\n--- a/include/linux/swapops.h\n+++ b/include/linux/swapops.h\n@@ -360,5 +360,30 @@ static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)\n \n #endif  /* CONFIG_ARCH_ENABLE_THP_MIGRATION */\n \n+/* Physical swap slots operations */\n+\n+/*\n+ * Store a swap device type + offset into a swp_slot_t handle.\n+ */\n+static inline swp_slot_t swp_slot(unsigned long type, pgoff_t offset)\n+{\n+\tswp_slot_t ret;\n+\n+\tret.val = (type << SWP_TYPE_SHIFT) | (offset & SWP_OFFSET_MASK);\n+\treturn ret;\n+}\n+\n+/* Extract the `type' field from a swp_slot_t. */\n+static inline unsigned swp_slot_type(swp_slot_t slot)\n+{\n+\treturn (slot.val >> SWP_TYPE_SHIFT);\n+}\n+\n+/* Extract the `offset' field from a swp_slot_t. */\n+static inline pgoff_t swp_slot_offset(swp_slot_t slot)\n+{\n+\treturn slot.val & SWP_OFFSET_MASK;\n+}\n+\n #endif /* CONFIG_MMU */\n #endif /* _LINUX_SWAPOPS_H */\ndiff --git a/kernel/power/swap.c b/kernel/power/swap.c\nindex 8050e51828351..0129c5ffa649d 100644\n--- a/kernel/power/swap.c\n+++ b/kernel/power/swap.c\n@@ -174,10 +174,10 @@ sector_t alloc_swapdev_block(int swap)\n \t * Allocate a swap page and register that it has been allocated, so that\n \t * it can be freed in case of an error.\n \t */\n-\toffset = swp_offset(get_swap_page_of_type(swap));\n+\toffset = swp_slot_offset(swap_slot_alloc_of_type(swap));\n \tif (offset) {\n \t\tif (swsusp_extents_insert(offset))\n-\t\t\tswap_free(swp_entry(swap, offset));\n+\t\t\tswap_slot_free_nr(swp_slot(swap, offset), 1);\n \t\telse\n \t\t\treturn swapdev_block(swap, offset);\n \t}\n@@ -197,7 +197,7 @@ void free_all_swap_pages(int swap)\n \n \t\text = rb_entry(node, struct swsusp_extent, node);\n \t\trb_erase(node, &swsusp_extents);\n-\t\tswap_free_nr(swp_entry(swap, ext->start),\n+\t\tswap_slot_free_nr(swp_slot(swap, ext->start),\n \t\t\t     ext->end - ext->start + 1);\n \n \t\tkfree(ext);\ndiff --git a/mm/internal.h b/mm/internal.h\nindex f35dbcf99a86b..e739e8cac5b55 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -334,9 +334,13 @@ unsigned int folio_pte_batch(struct folio *folio, pte_t *ptep, pte_t pte,\n  */\n static inline pte_t pte_move_swp_offset(pte_t pte, long delta)\n {\n-\tconst softleaf_t entry = softleaf_from_pte(pte);\n-\tpte_t new = __swp_entry_to_pte(__swp_entry(swp_type(entry),\n-\t\t\t\t\t\t   (swp_offset(entry) + delta)));\n+\tsoftleaf_t entry = softleaf_from_pte(pte), new_entry;\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tpte_t new;\n+\n+\tnew_entry = swp_slot_to_swp_entry(swp_slot(swp_slot_type(slot),\n+\t\t\tswp_slot_offset(slot) + delta));\n+\tnew = swp_entry_to_pte(new_entry);\n \n \tif (pte_swp_soft_dirty(pte))\n \t\tnew = pte_swp_mksoft_dirty(new);\ndiff --git a/mm/page_io.c b/mm/page_io.c\nindex 3c342db77ce38..0b02bcc85e2a8 100644\n--- a/mm/page_io.c\n+++ b/mm/page_io.c\n@@ -204,14 +204,17 @@ static bool is_folio_zero_filled(struct folio *folio)\n static void swap_zeromap_folio_set(struct folio *folio)\n {\n \tstruct obj_cgroup *objcg = get_obj_cgroup_from_folio(folio);\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tstruct swap_info_struct *sis =\n+\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n \tint nr_pages = folio_nr_pages(folio);\n \tswp_entry_t entry;\n+\tswp_slot_t slot;\n \tunsigned int i;\n \n \tfor (i = 0; i < folio_nr_pages(folio); i++) {\n \t\tentry = page_swap_entry(folio_page(folio, i));\n-\t\tset_bit(swp_offset(entry), sis->zeromap);\n+\t\tslot = swp_entry_to_swp_slot(entry);\n+\t\tset_bit(swp_slot_offset(slot), sis->zeromap);\n \t}\n \n \tcount_vm_events(SWPOUT_ZERO, nr_pages);\n@@ -223,13 +226,16 @@ static void swap_zeromap_folio_set(struct folio *folio)\n \n static void swap_zeromap_folio_clear(struct folio *folio)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tstruct swap_info_struct *sis =\n+\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n \tswp_entry_t entry;\n+\tswp_slot_t slot;\n \tunsigned int i;\n \n \tfor (i = 0; i < folio_nr_pages(folio); i++) {\n \t\tentry = page_swap_entry(folio_page(folio, i));\n-\t\tclear_bit(swp_offset(entry), sis->zeromap);\n+\t\tslot = swp_entry_to_swp_slot(entry);\n+\t\tclear_bit(swp_slot_offset(slot), sis->zeromap);\n \t}\n }\n \n@@ -357,7 +363,8 @@ static void sio_write_complete(struct kiocb *iocb, long ret)\n \t\t * messages.\n \t\t */\n \t\tpr_err_ratelimited(\"Write error %ld on dio swapfile (%llu)\\n\",\n-\t\t\t\t   ret, swap_dev_pos(page_swap_entry(page)));\n+\t\t\t\t   ret,\n+\t\t\t\t   swap_slot_pos(swp_entry_to_swp_slot(page_swap_entry(page))));\n \t\tfor (p = 0; p < sio->pages; p++) {\n \t\t\tpage = sio->bvec[p].bv_page;\n \t\t\tset_page_dirty(page);\n@@ -374,9 +381,10 @@ static void sio_write_complete(struct kiocb *iocb, long ret)\n static void swap_writepage_fs(struct folio *folio, struct swap_iocb **swap_plug)\n {\n \tstruct swap_iocb *sio = swap_plug ? *swap_plug : NULL;\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(folio->swap);\n+\tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n \tstruct file *swap_file = sis->swap_file;\n-\tloff_t pos = swap_dev_pos(folio->swap);\n+\tloff_t pos = swap_slot_pos(slot);\n \n \tcount_swpout_vm_event(folio);\n \tfolio_start_writeback(folio);\n@@ -446,7 +454,8 @@ static void swap_writepage_bdev_async(struct folio *folio,\n \n void __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tstruct swap_info_struct *sis =\n+\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n \n \tVM_BUG_ON_FOLIO(!folio_test_swapcache(folio), folio);\n \t/*\n@@ -537,9 +546,10 @@ static bool swap_read_folio_zeromap(struct folio *folio)\n \n static void swap_read_folio_fs(struct folio *folio, struct swap_iocb **plug)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(folio->swap);\n+\tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n \tstruct swap_iocb *sio = NULL;\n-\tloff_t pos = swap_dev_pos(folio->swap);\n+\tloff_t pos = swap_slot_pos(slot);\n \n \tif (plug)\n \t\tsio = *plug;\n@@ -608,7 +618,8 @@ static void swap_read_folio_bdev_async(struct folio *folio,\n \n void swap_read_folio(struct folio *folio, struct swap_iocb **plug)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tstruct swap_info_struct *sis =\n+\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n \tbool synchronous = sis->flags & SWP_SYNCHRONOUS_IO;\n \tbool workingset = folio_test_workingset(folio);\n \tunsigned long pflags;\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex b40be22fa5f09..400e2fa8e77cb 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -1442,6 +1442,7 @@ static unsigned int shmem_find_swap_entries(struct address_space *mapping,\n \tXA_STATE(xas, &mapping->i_pages, start);\n \tstruct folio *folio;\n \tswp_entry_t entry;\n+\tswp_slot_t slot;\n \n \trcu_read_lock();\n \txas_for_each(&xas, folio, ULONG_MAX) {\n@@ -1452,11 +1453,13 @@ static unsigned int shmem_find_swap_entries(struct address_space *mapping,\n \t\t\tcontinue;\n \n \t\tentry = radix_to_swp_entry(folio);\n+\t\tslot = swp_entry_to_swp_slot(entry);\n+\n \t\t/*\n \t\t * swapin error entries can be found in the mapping. But they're\n \t\t * deliberately ignored here as we've done everything we can do.\n \t\t */\n-\t\tif (swp_type(entry) != type)\n+\t\tif (swp_slot_type(slot) != type)\n \t\t\tcontinue;\n \n \t\tindices[folio_batch_count(fbatch)] = xas.xa_index;\n@@ -2224,6 +2227,7 @@ static int shmem_split_large_entry(struct inode *inode, pgoff_t index,\n \tXA_STATE_ORDER(xas, &mapping->i_pages, index, 0);\n \tint split_order = 0;\n \tint i;\n+\tswp_slot_t slot = swp_entry_to_swp_slot(swap);\n \n \t/* Convert user data gfp flags to xarray node gfp flags */\n \tgfp &= GFP_RECLAIM_MASK;\n@@ -2264,13 +2268,16 @@ static int shmem_split_large_entry(struct inode *inode, pgoff_t index,\n \t\t\t */\n \t\t\tfor (i = 0; i < 1 << cur_order;\n \t\t\t     i += (1 << split_order)) {\n-\t\t\t\tswp_entry_t tmp;\n+\t\t\t\tswp_entry_t tmp_entry;\n+\t\t\t\tswp_slot_t tmp_slot;\n+\n+\t\t\t\ttmp_slot =\n+\t\t\t\t\tswp_slot(swp_slot_type(slot),\n+\t\t\t\t\t\tswp_slot_offset(slot) + swap_offset + i);\n+\t\t\t\ttmp_entry = swp_slot_to_swp_entry(tmp_slot);\n \n-\t\t\t\ttmp = swp_entry(swp_type(swap),\n-\t\t\t\t\t\tswp_offset(swap) + swap_offset +\n-\t\t\t\t\t\t\ti);\n \t\t\t\t__xa_store(&mapping->i_pages, aligned_index + i,\n-\t\t\t\t\t   swp_to_radix_entry(tmp), 0);\n+\t\t\t\t\t   swp_to_radix_entry(tmp_entry), 0);\n \t\t\t}\n \t\t\tcur_order = split_order;\n \t\t\tsplit_order = xas_try_split_min_order(split_order);\ndiff --git a/mm/swap.h b/mm/swap.h\nindex 8726b587a5b5d..bdf7aca146643 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -10,10 +10,10 @@ extern int page_cluster;\n \n #ifdef CONFIG_THP_SWAP\n #define SWAPFILE_CLUSTER\tHPAGE_PMD_NR\n-#define swap_entry_order(order)\t(order)\n+#define swap_slot_order(order)\t(order)\n #else\n #define SWAPFILE_CLUSTER\t256\n-#define swap_entry_order(order)\t0\n+#define swap_slot_order(order)\t0\n #endif\n \n extern struct swap_info_struct *swap_info[];\n@@ -57,9 +57,9 @@ enum swap_cluster_flags {\n #include <linux/swapops.h> /* for swp_offset */\n #include <linux/blk_types.h> /* for bio_end_io_t */\n \n-static inline unsigned int swp_cluster_offset(swp_entry_t entry)\n+static inline unsigned int swp_cluster_offset(swp_slot_t slot)\n {\n-\treturn swp_offset(entry) % SWAPFILE_CLUSTER;\n+\treturn swp_slot_offset(slot) % SWAPFILE_CLUSTER;\n }\n \n /*\n@@ -75,9 +75,9 @@ static inline struct swap_info_struct *__swap_type_to_info(int type)\n \treturn si;\n }\n \n-static inline struct swap_info_struct *__swap_entry_to_info(swp_entry_t entry)\n+static inline struct swap_info_struct *__swap_slot_to_info(swp_slot_t slot)\n {\n-\treturn __swap_type_to_info(swp_type(entry));\n+\treturn __swap_type_to_info(swp_slot_type(slot));\n }\n \n static inline struct swap_cluster_info *__swap_offset_to_cluster(\n@@ -88,10 +88,10 @@ static inline struct swap_cluster_info *__swap_offset_to_cluster(\n \treturn &si->cluster_info[offset / SWAPFILE_CLUSTER];\n }\n \n-static inline struct swap_cluster_info *__swap_entry_to_cluster(swp_entry_t entry)\n+static inline struct swap_cluster_info *__swap_slot_to_cluster(swp_slot_t slot)\n {\n-\treturn __swap_offset_to_cluster(__swap_entry_to_info(entry),\n-\t\t\t\t\tswp_offset(entry));\n+\treturn __swap_offset_to_cluster(__swap_slot_to_info(slot),\n+\t\t\t\t\tswp_slot_offset(slot));\n }\n \n static __always_inline struct swap_cluster_info *__swap_cluster_lock(\n@@ -120,7 +120,7 @@ static __always_inline struct swap_cluster_info *__swap_cluster_lock(\n /**\n  * swap_cluster_lock - Lock and return the swap cluster of given offset.\n  * @si: swap device the cluster belongs to.\n- * @offset: the swap entry offset, pointing to a valid slot.\n+ * @offset: the swap slot offset, pointing to a valid slot.\n  *\n  * Context: The caller must ensure the offset is in the valid range and\n  * protect the swap device with reference count or locks.\n@@ -134,10 +134,12 @@ static inline struct swap_cluster_info *swap_cluster_lock(\n static inline struct swap_cluster_info *__swap_cluster_get_and_lock(\n \t\tconst struct folio *folio, bool irq)\n {\n+\tswp_slot_t slot = swp_entry_to_swp_slot(folio->swap);\n+\n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n-\treturn __swap_cluster_lock(__swap_entry_to_info(folio->swap),\n-\t\t\t\t   swp_offset(folio->swap), irq);\n+\treturn __swap_cluster_lock(__swap_slot_to_info(slot),\n+\t\t\t\t   swp_slot_offset(slot), irq);\n }\n \n /*\n@@ -209,12 +211,10 @@ static inline struct address_space *swap_address_space(swp_entry_t entry)\n \treturn &swap_space;\n }\n \n-/*\n- * Return the swap device position of the swap entry.\n- */\n-static inline loff_t swap_dev_pos(swp_entry_t entry)\n+/* Return the swap device position of the swap slot. */\n+static inline loff_t swap_slot_pos(swp_slot_t slot)\n {\n-\treturn ((loff_t)swp_offset(entry)) << PAGE_SHIFT;\n+\treturn ((loff_t)swp_slot_offset(slot)) << PAGE_SHIFT;\n }\n \n /**\n@@ -276,7 +276,9 @@ void swap_update_readahead(struct folio *folio, struct vm_area_struct *vma,\n \n static inline unsigned int folio_swap_flags(struct folio *folio)\n {\n-\treturn __swap_entry_to_info(folio->swap)->flags;\n+\tswp_slot_t swp_slot = swp_entry_to_swp_slot(folio->swap);\n+\n+\treturn __swap_slot_to_info(swp_slot)->flags;\n }\n \n /*\n@@ -287,8 +289,9 @@ static inline unsigned int folio_swap_flags(struct folio *folio)\n static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n \t\tbool *is_zeromap)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(entry);\n-\tunsigned long start = swp_offset(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n+\tunsigned long start = swp_slot_offset(slot);\n \tunsigned long end = start + max_nr;\n \tbool first_bit;\n \n@@ -306,8 +309,9 @@ static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n \n static inline int non_swapcache_batch(swp_entry_t entry, int max_nr)\n {\n-\tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n-\tpgoff_t offset = swp_offset(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tstruct swap_info_struct *si = __swap_slot_to_info(slot);\n+\tpgoff_t offset = swp_slot_offset(slot);\n \tint i;\n \n \t/*\n@@ -326,7 +330,7 @@ static inline int non_swapcache_batch(swp_entry_t entry, int max_nr)\n #else /* CONFIG_SWAP */\n struct swap_iocb;\n static inline struct swap_cluster_info *swap_cluster_lock(\n-\tstruct swap_info_struct *si, pgoff_t offset, bool irq)\n+\tstruct swap_info_struct *si, unsigned long offset)\n {\n \treturn NULL;\n }\n@@ -351,7 +355,7 @@ static inline void swap_cluster_unlock_irq(struct swap_cluster_info *ci)\n {\n }\n \n-static inline struct swap_info_struct *__swap_entry_to_info(swp_entry_t entry)\n+static inline struct swap_info_struct *__swap_slot_to_info(swp_slot_t slot)\n {\n \treturn NULL;\n }\ndiff --git a/mm/swap_cgroup.c b/mm/swap_cgroup.c\nindex de779fed8c210..77ce1d66c318d 100644\n--- a/mm/swap_cgroup.c\n+++ b/mm/swap_cgroup.c\n@@ -65,13 +65,14 @@ void swap_cgroup_record(struct folio *folio, unsigned short id,\n \t\t\tswp_entry_t ent)\n {\n \tunsigned int nr_ents = folio_nr_pages(folio);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(ent);\n \tstruct swap_cgroup *map;\n \tpgoff_t offset, end;\n \tunsigned short old;\n \n-\toffset = swp_offset(ent);\n+\toffset = swp_slot_offset(slot);\n \tend = offset + nr_ents;\n-\tmap = swap_cgroup_ctrl[swp_type(ent)].map;\n+\tmap = swap_cgroup_ctrl[swp_slot_type(slot)].map;\n \n \tdo {\n \t\told = __swap_cgroup_id_xchg(map, offset, id);\n@@ -92,13 +93,13 @@ void swap_cgroup_record(struct folio *folio, unsigned short id,\n  */\n unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents)\n {\n-\tpgoff_t offset, end;\n+\tswp_slot_t slot = swp_entry_to_swp_slot(ent);\n+\tpgoff_t offset = swp_slot_offset(slot);\n+\tpgoff_t end = offset + nr_ents;\n \tstruct swap_cgroup *map;\n \tunsigned short old, iter = 0;\n \n-\toffset = swp_offset(ent);\n-\tend = offset + nr_ents;\n-\tmap = swap_cgroup_ctrl[swp_type(ent)].map;\n+\tmap = swap_cgroup_ctrl[swp_slot_type(slot)].map;\n \n \tdo {\n \t\told = __swap_cgroup_id_xchg(map, offset, 0);\n@@ -119,12 +120,13 @@ unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents)\n unsigned short lookup_swap_cgroup_id(swp_entry_t ent)\n {\n \tstruct swap_cgroup_ctrl *ctrl;\n+\tswp_slot_t slot = swp_entry_to_swp_slot(ent);\n \n \tif (mem_cgroup_disabled())\n \t\treturn 0;\n \n-\tctrl = &swap_cgroup_ctrl[swp_type(ent)];\n-\treturn __swap_cgroup_id_lookup(ctrl->map, swp_offset(ent));\n+\tctrl = &swap_cgroup_ctrl[swp_slot_type(slot)];\n+\treturn __swap_cgroup_id_lookup(ctrl->map, swp_slot_offset(slot));\n }\n \n int swap_cgroup_swapon(int type, unsigned long max_pages)\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex bece18eb540fa..e2e9f55bea3bb 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -421,7 +421,8 @@ struct folio *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\tstruct mempolicy *mpol, pgoff_t ilx, bool *new_page_allocated,\n \t\tbool skip_if_exists)\n {\n-\tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n+\tstruct swap_info_struct *si =\n+\t\t__swap_slot_to_info(swp_entry_to_swp_slot(entry));\n \tstruct folio *folio;\n \tstruct folio *new_folio = NULL;\n \tstruct folio *result = NULL;\n@@ -636,11 +637,12 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \t\t\t\t    struct mempolicy *mpol, pgoff_t ilx)\n {\n \tstruct folio *folio;\n-\tunsigned long entry_offset = swp_offset(entry);\n-\tunsigned long offset = entry_offset;\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tunsigned long slot_offset = swp_slot_offset(slot);\n+\tunsigned long offset = slot_offset;\n \tunsigned long start_offset, end_offset;\n \tunsigned long mask;\n-\tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n+\tstruct swap_info_struct *si = __swap_slot_to_info(slot);\n \tstruct blk_plug plug;\n \tstruct swap_iocb *splug = NULL;\n \tbool page_allocated;\n@@ -661,13 +663,13 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \tfor (offset = start_offset; offset <= end_offset ; offset++) {\n \t\t/* Ok, do the async read-ahead now */\n \t\tfolio = __read_swap_cache_async(\n-\t\t\t\tswp_entry(swp_type(entry), offset),\n+\t\t\t\tswp_slot_to_swp_entry(swp_slot(swp_slot_type(slot), offset)),\n \t\t\t\tgfp_mask, mpol, ilx, &page_allocated, false);\n \t\tif (!folio)\n \t\t\tcontinue;\n \t\tif (page_allocated) {\n \t\t\tswap_read_folio(folio, &splug);\n-\t\t\tif (offset != entry_offset) {\n+\t\t\tif (offset != slot_offset) {\n \t\t\t\tfolio_set_readahead(folio);\n \t\t\t\tcount_vm_event(SWAP_RA);\n \t\t\t}\n@@ -779,16 +781,20 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \t\t/*\n \t\t * Readahead entry may come from a device that we are not\n \t\t * holding a reference to, try to grab a reference, or skip.\n+\t\t *\n+\t\t * XXX: for now, always try to pin the swap entries in the\n+\t\t * readahead window to avoid the annoying conversion to physical\n+\t\t * swap slots. Once we move all swap metadata to virtual swap\n+\t\t * layer, we can simply compare the clusters of the target\n+\t\t * swap entry and the current swap entry, and pin the latter\n+\t\t * swap entry's cluster if it differ from the former's.\n \t\t */\n-\t\tif (swp_type(entry) != swp_type(targ_entry)) {\n-\t\t\tswapoff_locked = tryget_swap_entry(entry, &si);\n-\t\t\tif (!swapoff_locked)\n-\t\t\t\tcontinue;\n-\t\t}\n+\t\tswapoff_locked = tryget_swap_entry(entry, &si);\n+\t\tif (!swapoff_locked)\n+\t\t\tcontinue;\n \t\tfolio = __read_swap_cache_async(entry, gfp_mask, mpol, ilx,\n \t\t\t\t\t\t&page_allocated, false);\n-\t\tif (swapoff_locked)\n-\t\t\tput_swap_entry(entry, si);\n+\t\tput_swap_entry(entry, si);\n \t\tif (!folio)\n \t\t\tcontinue;\n \t\tif (page_allocated) {\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 3c89dedbd5718..4b4126d4e2769 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -53,9 +53,9 @@\n static bool swap_count_continued(struct swap_info_struct *, pgoff_t,\n \t\t\t\t unsigned char);\n static void free_swap_count_continuations(struct swap_info_struct *);\n-static void swap_entries_free(struct swap_info_struct *si,\n+static void swap_slots_free(struct swap_info_struct *si,\n \t\t\t      struct swap_cluster_info *ci,\n-\t\t\t      swp_entry_t entry, unsigned int nr_pages);\n+\t\t\t      swp_slot_t slot, unsigned int nr_pages);\n static void swap_range_alloc(struct swap_info_struct *si,\n \t\t\t     unsigned int nr_entries);\n static bool folio_swapcache_freeable(struct folio *folio);\n@@ -126,7 +126,7 @@ struct percpu_swap_cluster {\n \n static DEFINE_PER_CPU(struct percpu_swap_cluster, percpu_swap_cluster) = {\n \t.si = { NULL },\n-\t.offset = { SWAP_ENTRY_INVALID },\n+\t.offset = { SWAP_SLOT_INVALID },\n \t.lock = INIT_LOCAL_LOCK(),\n };\n \n@@ -139,9 +139,9 @@ static struct swap_info_struct *swap_type_to_info(int type)\n }\n \n /* May return NULL on invalid entry, caller must check for NULL return */\n-static struct swap_info_struct *swap_entry_to_info(swp_entry_t entry)\n+static struct swap_info_struct *swap_slot_to_info(swp_slot_t slot)\n {\n-\treturn swap_type_to_info(swp_type(entry));\n+\treturn swap_type_to_info(swp_slot_type(slot));\n }\n \n static inline unsigned char swap_count(unsigned char ent)\n@@ -204,9 +204,11 @@ static bool swap_only_has_cache(struct swap_info_struct *si,\n  */\n bool is_swap_cached(swp_entry_t entry)\n {\n-\tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tstruct swap_info_struct *si = swap_slot_to_info(slot);\n+\tunsigned long offset = swp_slot_offset(slot);\n \n-\treturn READ_ONCE(si->swap_map[swp_offset(entry)]) & SWAP_HAS_CACHE;\n+\treturn READ_ONCE(si->swap_map[offset]) & SWAP_HAS_CACHE;\n }\n \n static bool swap_is_last_map(struct swap_info_struct *si,\n@@ -236,7 +238,9 @@ static bool swap_is_last_map(struct swap_info_struct *si,\n static int __try_to_reclaim_swap(struct swap_info_struct *si,\n \t\t\t\t unsigned long offset, unsigned long flags)\n {\n-\tconst swp_entry_t entry = swp_entry(si->type, offset);\n+\tconst swp_entry_t entry =\n+\t\tswp_slot_to_swp_entry(swp_slot(si->type, offset));\n+\tswp_slot_t slot;\n \tstruct swap_cluster_info *ci;\n \tstruct folio *folio;\n \tint ret, nr_pages;\n@@ -268,7 +272,8 @@ static int __try_to_reclaim_swap(struct swap_info_struct *si,\n \t\tfolio_put(folio);\n \t\tgoto again;\n \t}\n-\toffset = swp_offset(folio->swap);\n+\tslot = swp_entry_to_swp_slot(folio->swap);\n+\toffset = swp_slot_offset(slot);\n \n \tneed_reclaim = ((flags & TTRS_ANYWAY) ||\n \t\t\t((flags & TTRS_UNMAPPED) && !folio_mapped(folio)) ||\n@@ -368,12 +373,12 @@ offset_to_swap_extent(struct swap_info_struct *sis, unsigned long offset)\n \n sector_t swap_folio_sector(struct folio *folio)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(folio->swap);\n+\tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n \tstruct swap_extent *se;\n \tsector_t sector;\n-\tpgoff_t offset;\n+\tpgoff_t offset = swp_slot_offset(slot);\n \n-\toffset = swp_offset(folio->swap);\n \tse = offset_to_swap_extent(sis, offset);\n \tsector = se->start_block + (offset - se->start_page);\n \treturn sector << (PAGE_SHIFT - 9);\n@@ -890,7 +895,7 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n \t\t\t\t\t    unsigned int order,\n \t\t\t\t\t    unsigned char usage)\n {\n-\tunsigned int next = SWAP_ENTRY_INVALID, found = SWAP_ENTRY_INVALID;\n+\tunsigned int next = SWAP_SLOT_INVALID, found = SWAP_SLOT_INVALID;\n \tunsigned long start = ALIGN_DOWN(offset, SWAPFILE_CLUSTER);\n \tunsigned long end = min(start + SWAPFILE_CLUSTER, si->max);\n \tunsigned int nr_pages = 1 << order;\n@@ -947,7 +952,7 @@ static unsigned int alloc_swap_scan_list(struct swap_info_struct *si,\n \t\t\t\t\t unsigned char usage,\n \t\t\t\t\t bool scan_all)\n {\n-\tunsigned int found = SWAP_ENTRY_INVALID;\n+\tunsigned int found = SWAP_SLOT_INVALID;\n \n \tdo {\n \t\tstruct swap_cluster_info *ci = isolate_lock_cluster(si, list);\n@@ -1017,11 +1022,11 @@ static void swap_reclaim_work(struct work_struct *work)\n  * Try to allocate swap entries with specified order and try set a new\n  * cluster for current CPU too.\n  */\n-static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si, int order,\n+static unsigned long cluster_alloc_swap_slot(struct swap_info_struct *si, int order,\n \t\t\t\t\t      unsigned char usage)\n {\n \tstruct swap_cluster_info *ci;\n-\tunsigned int offset = SWAP_ENTRY_INVALID, found = SWAP_ENTRY_INVALID;\n+\tunsigned int offset = SWAP_SLOT_INVALID, found = SWAP_SLOT_INVALID;\n \n \t/*\n \t * Swapfile is not block device so unable\n@@ -1034,7 +1039,7 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si, int o\n \t\t/* Serialize HDD SWAP allocation for each device. */\n \t\tspin_lock(&si->global_cluster_lock);\n \t\toffset = si->global_cluster->next[order];\n-\t\tif (offset == SWAP_ENTRY_INVALID)\n+\t\tif (offset == SWAP_SLOT_INVALID)\n \t\t\tgoto new_cluster;\n \n \t\tci = swap_cluster_lock(si, offset);\n@@ -1255,7 +1260,7 @@ static void swap_range_free(struct swap_info_struct *si, unsigned long offset,\n \t */\n \tfor (i = 0; i < nr_entries; i++) {\n \t\tclear_bit(offset + i, si->zeromap);\n-\t\tzswap_invalidate(swp_entry(si->type, offset + i));\n+\t\tzswap_invalidate(swp_slot_to_swp_entry(swp_slot(si->type, offset + i)));\n \t}\n \n \tif (si->flags & SWP_BLKDEV)\n@@ -1300,12 +1305,11 @@ static bool get_swap_device_info(struct swap_info_struct *si)\n  * Fast path try to get swap entries with specified order from current\n  * CPU's swap entry pool (a cluster).\n  */\n-static bool swap_alloc_fast(swp_entry_t *entry,\n-\t\t\t    int order)\n+static bool swap_alloc_fast(swp_slot_t *slot, int order)\n {\n \tstruct swap_cluster_info *ci;\n \tstruct swap_info_struct *si;\n-\tunsigned int offset, found = SWAP_ENTRY_INVALID;\n+\tunsigned int offset, found = SWAP_SLOT_INVALID;\n \n \t/*\n \t * Once allocated, swap_info_struct will never be completely freed,\n@@ -1322,18 +1326,17 @@ static bool swap_alloc_fast(swp_entry_t *entry,\n \t\t\toffset = cluster_offset(si, ci);\n \t\tfound = alloc_swap_scan_cluster(si, ci, offset, order, SWAP_HAS_CACHE);\n \t\tif (found)\n-\t\t\t*entry = swp_entry(si->type, found);\n+\t\t\t*slot = swp_slot(si->type, found);\n \t} else {\n \t\tswap_cluster_unlock(ci);\n \t}\n \n-\tput_swap_device(si);\n+\tswap_slot_put_swap_info(si);\n \treturn !!found;\n }\n \n /* Rotate the device and switch to a new cluster */\n-static void swap_alloc_slow(swp_entry_t *entry,\n-\t\t\t    int order)\n+static void swap_alloc_slow(swp_slot_t *slot, int order)\n {\n \tunsigned long offset;\n \tstruct swap_info_struct *si, *next;\n@@ -1345,10 +1348,10 @@ static void swap_alloc_slow(swp_entry_t *entry,\n \t\tplist_requeue(&si->avail_list, &swap_avail_head);\n \t\tspin_unlock(&swap_avail_lock);\n \t\tif (get_swap_device_info(si)) {\n-\t\t\toffset = cluster_alloc_swap_entry(si, order, SWAP_HAS_CACHE);\n-\t\t\tput_swap_device(si);\n+\t\t\toffset = cluster_alloc_swap_slot(si, order, SWAP_HAS_CACHE);\n+\t\t\tswap_slot_put_swap_info(si);\n \t\t\tif (offset) {\n-\t\t\t\t*entry = swp_entry(si->type, offset);\n+\t\t\t\t*slot = swp_slot(si->type, offset);\n \t\t\t\treturn;\n \t\t\t}\n \t\t\tif (order)\n@@ -1388,7 +1391,7 @@ static bool swap_sync_discard(void)\n \t\tif (get_swap_device_info(si)) {\n \t\t\tif (si->flags & SWP_PAGE_DISCARD)\n \t\t\t\tret = swap_do_scheduled_discard(si);\n-\t\t\tput_swap_device(si);\n+\t\t\tswap_slot_put_swap_info(si);\n \t\t}\n \t\tif (ret)\n \t\t\treturn true;\n@@ -1402,25 +1405,9 @@ static bool swap_sync_discard(void)\n \treturn false;\n }\n \n-/**\n- * folio_alloc_swap - allocate swap space for a folio\n- * @folio: folio we want to move to swap\n- *\n- * Allocate swap space for the folio and add the folio to the\n- * swap cache.\n- *\n- * Context: Caller needs to hold the folio lock.\n- * Return: Whether the folio was added to the swap cache.\n- */\n-int folio_alloc_swap(struct folio *folio)\n+static int swap_slot_alloc(swp_slot_t *slot, unsigned int order)\n {\n-\tunsigned int order = folio_order(folio);\n \tunsigned int size = 1 << order;\n-\tswp_entry_t entry = {};\n-\tint err;\n-\n-\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n-\tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n \n \tif (order) {\n \t\t/*\n@@ -1442,22 +1429,52 @@ int folio_alloc_swap(struct folio *folio)\n \n again:\n \tlocal_lock(&percpu_swap_cluster.lock);\n-\tif (!swap_alloc_fast(&entry, order))\n-\t\tswap_alloc_slow(&entry, order);\n+\tif (!swap_alloc_fast(slot, order))\n+\t\tswap_alloc_slow(slot, order);\n \tlocal_unlock(&percpu_swap_cluster.lock);\n \n-\tif (unlikely(!order && !entry.val)) {\n+\tif (unlikely(!order && !slot->val)) {\n \t\tif (swap_sync_discard())\n \t\t\tgoto again;\n \t}\n \n+\treturn 0;\n+}\n+\n+/**\n+ * folio_alloc_swap - allocate swap space for a folio\n+ * @folio: folio we want to move to swap\n+ *\n+ * Allocate swap space for the folio and add the folio to the\n+ * swap cache.\n+ *\n+ * Context: Caller needs to hold the folio lock.\n+ * Return: Whether the folio was added to the swap cache.\n+ */\n+int folio_alloc_swap(struct folio *folio)\n+{\n+\tunsigned int order = folio_order(folio);\n+\tswp_slot_t slot = { 0 };\n+\tswp_entry_t entry = {};\n+\tint err = 0, ret;\n+\n+\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n+\tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n+\n+\tret = swap_slot_alloc(&slot, order);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\t/* XXX: for now, physical and virtual swap slots are identical */\n+\tentry.val = slot.val;\n+\n \t/* Need to call this even if allocation failed, for MEMCG_SWAP_FAIL. */\n \tif (mem_cgroup_try_charge_swap(folio, entry)) {\n \t\terr = -ENOMEM;\n \t\tgoto out_free;\n \t}\n \n-\tif (!entry.val)\n+\tif (!slot.val)\n \t\treturn -ENOMEM;\n \n \terr = swap_cache_add_folio(folio, entry, __GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN, NULL);\n@@ -1471,46 +1488,46 @@ int folio_alloc_swap(struct folio *folio)\n \treturn err;\n }\n \n-static struct swap_info_struct *_swap_info_get(swp_entry_t entry)\n+static struct swap_info_struct *_swap_info_get(swp_slot_t slot)\n {\n \tstruct swap_info_struct *si;\n \tunsigned long offset;\n \n-\tif (!entry.val)\n+\tif (!slot.val)\n \t\tgoto out;\n-\tsi = swap_entry_to_info(entry);\n+\tsi = swap_slot_to_info(slot);\n \tif (!si)\n \t\tgoto bad_nofile;\n \tif (data_race(!(si->flags & SWP_USED)))\n \t\tgoto bad_device;\n-\toffset = swp_offset(entry);\n+\toffset = swp_slot_offset(slot);\n \tif (offset >= si->max)\n \t\tgoto bad_offset;\n-\tif (data_race(!si->swap_map[swp_offset(entry)]))\n+\tif (data_race(!si->swap_map[swp_slot_offset(slot)]))\n \t\tgoto bad_free;\n \treturn si;\n \n bad_free:\n-\tpr_err(\"%s: %s%08lx\\n\", __func__, Unused_offset, entry.val);\n+\tpr_err(\"%s: %s%08lx\\n\", __func__, Unused_offset, slot.val);\n \tgoto out;\n bad_offset:\n-\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_offset, entry.val);\n+\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_offset, slot.val);\n \tgoto out;\n bad_device:\n-\tpr_err(\"%s: %s%08lx\\n\", __func__, Unused_file, entry.val);\n+\tpr_err(\"%s: %s%08lx\\n\", __func__, Unused_file, slot.val);\n \tgoto out;\n bad_nofile:\n-\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_file, entry.val);\n+\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_file, slot.val);\n out:\n \treturn NULL;\n }\n \n-static unsigned char swap_entry_put_locked(struct swap_info_struct *si,\n+static unsigned char swap_slot_put_locked(struct swap_info_struct *si,\n \t\t\t\t\t   struct swap_cluster_info *ci,\n-\t\t\t\t\t   swp_entry_t entry,\n+\t\t\t\t\t   swp_slot_t slot,\n \t\t\t\t\t   unsigned char usage)\n {\n-\tunsigned long offset = swp_offset(entry);\n+\tunsigned long offset = swp_slot_offset(slot);\n \tunsigned char count;\n \tunsigned char has_cache;\n \n@@ -1542,7 +1559,7 @@ static unsigned char swap_entry_put_locked(struct swap_info_struct *si,\n \tif (usage)\n \t\tWRITE_ONCE(si->swap_map[offset], usage);\n \telse\n-\t\tswap_entries_free(si, ci, entry, 1);\n+\t\tswap_slots_free(si, ci, slot, 1);\n \n \treturn usage;\n }\n@@ -1552,8 +1569,9 @@ static unsigned char swap_entry_put_locked(struct swap_info_struct *si,\n  * prevent swapoff, such as the folio in swap cache is locked, RCU\n  * reader side is locked, etc., the swap entry may become invalid\n  * because of swapoff.  Then, we need to enclose all swap related\n- * functions with get_swap_device() and put_swap_device(), unless the\n- * swap functions call get/put_swap_device() by themselves.\n+ * functions with swap_slot_tryget_swap_info() and\n+ * swap_slot_put_swap_info(), unless the swap functions call\n+ * swap_slot_(tryget|put)_swap_info by themselves.\n  *\n  * RCU reader side lock (including any spinlock) is sufficient to\n  * prevent swapoff, because synchronize_rcu() is called in swapoff()\n@@ -1562,11 +1580,11 @@ static unsigned char swap_entry_put_locked(struct swap_info_struct *si,\n  * Check whether swap entry is valid in the swap device.  If so,\n  * return pointer to swap_info_struct, and keep the swap entry valid\n  * via preventing the swap device from being swapoff, until\n- * put_swap_device() is called.  Otherwise return NULL.\n+ * swap_slot_put_swap_info() is called.  Otherwise return NULL.\n  *\n  * Notice that swapoff or swapoff+swapon can still happen before the\n- * percpu_ref_tryget_live() in get_swap_device() or after the\n- * percpu_ref_put() in put_swap_device() if there isn't any other way\n+ * percpu_ref_tryget_live() in swap_slot_tryget_swap_info() or after the\n+ * percpu_ref_put() in swap_slot_put_swap_info() if there isn't any other way\n  * to prevent swapoff.  The caller must be prepared for that.  For\n  * example, the following situation is possible.\n  *\n@@ -1586,53 +1604,53 @@ static unsigned char swap_entry_put_locked(struct swap_info_struct *si,\n  * changed with the page table locked to check whether the swap device\n  * has been swapoff or swapoff+swapon.\n  */\n-struct swap_info_struct *get_swap_device(swp_entry_t entry)\n+struct swap_info_struct *swap_slot_tryget_swap_info(swp_slot_t slot)\n {\n \tstruct swap_info_struct *si;\n \tunsigned long offset;\n \n-\tif (!entry.val)\n+\tif (!slot.val)\n \t\tgoto out;\n-\tsi = swap_entry_to_info(entry);\n+\tsi = swap_slot_to_info(slot);\n \tif (!si)\n \t\tgoto bad_nofile;\n \tif (!get_swap_device_info(si))\n \t\tgoto out;\n-\toffset = swp_offset(entry);\n+\toffset = swp_slot_offset(slot);\n \tif (offset >= si->max)\n \t\tgoto put_out;\n \n \treturn si;\n bad_nofile:\n-\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_file, entry.val);\n+\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_file, slot.val);\n out:\n \treturn NULL;\n put_out:\n-\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_offset, entry.val);\n+\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_offset, slot.val);\n \tpercpu_ref_put(&si->users);\n \treturn NULL;\n }\n \n-static void swap_entries_put_cache(struct swap_info_struct *si,\n-\t\t\t\t   swp_entry_t entry, int nr)\n+static void swap_slots_put_cache(struct swap_info_struct *si,\n+\t\t\t\t   swp_slot_t slot, int nr)\n {\n-\tunsigned long offset = swp_offset(entry);\n+\tunsigned long offset = swp_slot_offset(slot);\n \tstruct swap_cluster_info *ci;\n \n \tci = swap_cluster_lock(si, offset);\n \tif (swap_only_has_cache(si, offset, nr)) {\n-\t\tswap_entries_free(si, ci, entry, nr);\n+\t\tswap_slots_free(si, ci, slot, nr);\n \t} else {\n-\t\tfor (int i = 0; i < nr; i++, entry.val++)\n-\t\t\tswap_entry_put_locked(si, ci, entry, SWAP_HAS_CACHE);\n+\t\tfor (int i = 0; i < nr; i++, slot.val++)\n+\t\t\tswap_slot_put_locked(si, ci, slot, SWAP_HAS_CACHE);\n \t}\n \tswap_cluster_unlock(ci);\n }\n \n-static bool swap_entries_put_map(struct swap_info_struct *si,\n-\t\t\t\t swp_entry_t entry, int nr)\n+static bool swap_slots_put_map(struct swap_info_struct *si,\n+\t\t\t\t swp_slot_t slot, int nr)\n {\n-\tunsigned long offset = swp_offset(entry);\n+\tunsigned long offset = swp_slot_offset(slot);\n \tstruct swap_cluster_info *ci;\n \tbool has_cache = false;\n \tunsigned char count;\n@@ -1649,7 +1667,7 @@ static bool swap_entries_put_map(struct swap_info_struct *si,\n \t\tgoto locked_fallback;\n \t}\n \tif (!has_cache)\n-\t\tswap_entries_free(si, ci, entry, nr);\n+\t\tswap_slots_free(si, ci, slot, nr);\n \telse\n \t\tfor (i = 0; i < nr; i++)\n \t\t\tWRITE_ONCE(si->swap_map[offset + i], SWAP_HAS_CACHE);\n@@ -1660,8 +1678,8 @@ static bool swap_entries_put_map(struct swap_info_struct *si,\n fallback:\n \tci = swap_cluster_lock(si, offset);\n locked_fallback:\n-\tfor (i = 0; i < nr; i++, entry.val++) {\n-\t\tcount = swap_entry_put_locked(si, ci, entry, 1);\n+\tfor (i = 0; i < nr; i++, slot.val++) {\n+\t\tcount = swap_slot_put_locked(si, ci, slot, 1);\n \t\tif (count == SWAP_HAS_CACHE)\n \t\t\thas_cache = true;\n \t}\n@@ -1674,20 +1692,20 @@ static bool swap_entries_put_map(struct swap_info_struct *si,\n  * cross multi clusters, so ensure the range is within a single cluster\n  * when freeing entries with functions without \"_nr\" suffix.\n  */\n-static bool swap_entries_put_map_nr(struct swap_info_struct *si,\n-\t\t\t\t    swp_entry_t entry, int nr)\n+static bool swap_slots_put_map_nr(struct swap_info_struct *si,\n+\t\t\t\t    swp_slot_t slot, int nr)\n {\n \tint cluster_nr, cluster_rest;\n-\tunsigned long offset = swp_offset(entry);\n+\tunsigned long offset = swp_slot_offset(slot);\n \tbool has_cache = false;\n \n \tcluster_rest = SWAPFILE_CLUSTER - offset % SWAPFILE_CLUSTER;\n \twhile (nr) {\n \t\tcluster_nr = min(nr, cluster_rest);\n-\t\thas_cache |= swap_entries_put_map(si, entry, cluster_nr);\n+\t\thas_cache |= swap_slots_put_map(si, slot, cluster_nr);\n \t\tcluster_rest = SWAPFILE_CLUSTER;\n \t\tnr -= cluster_nr;\n-\t\tentry.val += cluster_nr;\n+\t\tslot.val += cluster_nr;\n \t}\n \n \treturn has_cache;\n@@ -1707,13 +1725,14 @@ static inline bool __maybe_unused swap_is_last_ref(unsigned char count)\n  * Drop the last ref of swap entries, caller have to ensure all entries\n  * belong to the same cgroup and cluster.\n  */\n-static void swap_entries_free(struct swap_info_struct *si,\n+static void swap_slots_free(struct swap_info_struct *si,\n \t\t\t      struct swap_cluster_info *ci,\n-\t\t\t      swp_entry_t entry, unsigned int nr_pages)\n+\t\t\t      swp_slot_t slot, unsigned int nr_pages)\n {\n-\tunsigned long offset = swp_offset(entry);\n+\tunsigned long offset = swp_slot_offset(slot);\n \tunsigned char *map = si->swap_map + offset;\n \tunsigned char *map_end = map + nr_pages;\n+\tswp_entry_t entry = swp_slot_to_swp_entry(slot);\n \n \t/* It should never free entries across different clusters */\n \tVM_BUG_ON(ci != __swap_offset_to_cluster(si, offset + nr_pages - 1));\n@@ -1739,43 +1758,54 @@ static void swap_entries_free(struct swap_info_struct *si,\n  * Caller has made sure that the swap device corresponding to entry\n  * is still around or has not been recycled.\n  */\n-void swap_free_nr(swp_entry_t entry, int nr_pages)\n+void swap_slot_free_nr(swp_slot_t slot, int nr_pages)\n {\n \tint nr;\n \tstruct swap_info_struct *sis;\n-\tunsigned long offset = swp_offset(entry);\n+\tunsigned long offset = swp_slot_offset(slot);\n \n-\tsis = _swap_info_get(entry);\n+\tsis = _swap_info_get(slot);\n \tif (!sis)\n \t\treturn;\n \n \twhile (nr_pages) {\n \t\tnr = min_t(int, nr_pages, SWAPFILE_CLUSTER - offset % SWAPFILE_CLUSTER);\n-\t\tswap_entries_put_map(sis, swp_entry(sis->type, offset), nr);\n+\t\tswap_slots_put_map(sis, swp_slot(sis->type, offset), nr);\n \t\toffset += nr;\n \t\tnr_pages -= nr;\n \t}\n }\n \n+/*\n+ * Caller has made sure that the swap device corresponding to entry\n+ * is still around or has not been recycled.\n+ */\n+void swap_free_nr(swp_entry_t entry, int nr_pages)\n+{\n+\tswap_slot_free_nr(swp_entry_to_swp_slot(entry), nr_pages);\n+}\n+\n /*\n  * Called after dropping swapcache to decrease refcnt to swap entries.\n  */\n void put_swap_folio(struct folio *folio, swp_entry_t entry)\n {\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n \tstruct swap_info_struct *si;\n-\tint size = 1 << swap_entry_order(folio_order(folio));\n+\tint size = 1 << swap_slot_order(folio_order(folio));\n \n-\tsi = _swap_info_get(entry);\n+\tsi = _swap_info_get(slot);\n \tif (!si)\n \t\treturn;\n \n-\tswap_entries_put_cache(si, entry, size);\n+\tswap_slots_put_cache(si, slot, size);\n }\n \n int __swap_count(swp_entry_t entry)\n {\n-\tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n-\tpgoff_t offset = swp_offset(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tstruct swap_info_struct *si = __swap_slot_to_info(slot);\n+\tpgoff_t offset = swp_slot_offset(slot);\n \n \treturn swap_count(si->swap_map[offset]);\n }\n@@ -1787,7 +1817,8 @@ int __swap_count(swp_entry_t entry)\n  */\n bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry)\n {\n-\tpgoff_t offset = swp_offset(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tpgoff_t offset = swp_slot_offset(slot);\n \tstruct swap_cluster_info *ci;\n \tint count;\n \n@@ -1803,6 +1834,7 @@ bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry)\n  */\n int swp_swapcount(swp_entry_t entry)\n {\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n \tint count, tmp_count, n;\n \tstruct swap_info_struct *si;\n \tstruct swap_cluster_info *ci;\n@@ -1810,11 +1842,11 @@ int swp_swapcount(swp_entry_t entry)\n \tpgoff_t offset;\n \tunsigned char *map;\n \n-\tsi = _swap_info_get(entry);\n+\tsi = _swap_info_get(slot);\n \tif (!si)\n \t\treturn 0;\n \n-\toffset = swp_offset(entry);\n+\toffset = swp_slot_offset(slot);\n \n \tci = swap_cluster_lock(si, offset);\n \n@@ -1846,10 +1878,11 @@ int swp_swapcount(swp_entry_t entry)\n static bool swap_page_trans_huge_swapped(struct swap_info_struct *si,\n \t\t\t\t\t swp_entry_t entry, int order)\n {\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n \tstruct swap_cluster_info *ci;\n \tunsigned char *map = si->swap_map;\n \tunsigned int nr_pages = 1 << order;\n-\tunsigned long roffset = swp_offset(entry);\n+\tunsigned long roffset = swp_slot_offset(slot);\n \tunsigned long offset = round_down(roffset, nr_pages);\n \tint i;\n \tbool ret = false;\n@@ -1874,7 +1907,8 @@ static bool swap_page_trans_huge_swapped(struct swap_info_struct *si,\n static bool folio_swapped(struct folio *folio)\n {\n \tswp_entry_t entry = folio->swap;\n-\tstruct swap_info_struct *si = _swap_info_get(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tstruct swap_info_struct *si = _swap_info_get(slot);\n \n \tif (!si)\n \t\treturn false;\n@@ -1948,13 +1982,14 @@ bool folio_free_swap(struct folio *folio)\n  */\n void free_swap_and_cache_nr(swp_entry_t entry, int nr)\n {\n-\tconst unsigned long start_offset = swp_offset(entry);\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\tconst unsigned long start_offset = swp_slot_offset(slot);\n \tconst unsigned long end_offset = start_offset + nr;\n \tstruct swap_info_struct *si;\n \tbool any_only_cache = false;\n \tunsigned long offset;\n \n-\tsi = get_swap_device(entry);\n+\tsi = swap_slot_tryget_swap_info(slot);\n \tif (!si)\n \t\treturn;\n \n@@ -1964,7 +1999,7 @@ void free_swap_and_cache_nr(swp_entry_t entry, int nr)\n \t/*\n \t * First free all entries in the range.\n \t */\n-\tany_only_cache = swap_entries_put_map_nr(si, entry, nr);\n+\tany_only_cache = swap_slots_put_map_nr(si, slot, nr);\n \n \t/*\n \t * Short-circuit the below loop if none of the entries had their\n@@ -1998,16 +2033,16 @@ void free_swap_and_cache_nr(swp_entry_t entry, int nr)\n \t}\n \n out:\n-\tput_swap_device(si);\n+\tswap_slot_put_swap_info(si);\n }\n \n #ifdef CONFIG_HIBERNATION\n \n-swp_entry_t get_swap_page_of_type(int type)\n+swp_slot_t swap_slot_alloc_of_type(int type)\n {\n \tstruct swap_info_struct *si = swap_type_to_info(type);\n \tunsigned long offset;\n-\tswp_entry_t entry = {0};\n+\tswp_slot_t slot = {0};\n \n \tif (!si)\n \t\tgoto fail;\n@@ -2020,15 +2055,15 @@ swp_entry_t get_swap_page_of_type(int type)\n \t\t\t * with swap table allocation.\n \t\t\t */\n \t\t\tlocal_lock(&percpu_swap_cluster.lock);\n-\t\t\toffset = cluster_alloc_swap_entry(si, 0, 1);\n+\t\t\toffset = cluster_alloc_swap_slot(si, 0, 1);\n \t\t\tlocal_unlock(&percpu_swap_cluster.lock);\n \t\t\tif (offset)\n-\t\t\t\tentry = swp_entry(si->type, offset);\n+\t\t\t\tslot = swp_slot(si->type, offset);\n \t\t}\n-\t\tput_swap_device(si);\n+\t\tswap_slot_put_swap_info(si);\n \t}\n fail:\n-\treturn entry;\n+\treturn slot;\n }\n \n /*\n@@ -2257,6 +2292,7 @@ static int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,\n \t\tunsigned long offset;\n \t\tunsigned char swp_count;\n \t\tsoftleaf_t entry;\n+\t\tswp_slot_t slot;\n \t\tint ret;\n \t\tpte_t ptent;\n \n@@ -2271,10 +2307,12 @@ static int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,\n \n \t\tif (!softleaf_is_swap(entry))\n \t\t\tcontinue;\n-\t\tif (swp_type(entry) != type)\n+\n+\t\tslot = swp_entry_to_swp_slot(entry);\n+\t\tif (swp_slot_type(slot) != type)\n \t\t\tcontinue;\n \n-\t\toffset = swp_offset(entry);\n+\t\toffset = swp_slot_offset(slot);\n \t\tpte_unmap(pte);\n \t\tpte = NULL;\n \n@@ -2459,6 +2497,7 @@ static int try_to_unuse(unsigned int type)\n \tstruct swap_info_struct *si = swap_info[type];\n \tstruct folio *folio;\n \tswp_entry_t entry;\n+\tswp_slot_t slot;\n \tunsigned int i;\n \n \tif (!swap_usage_in_pages(si))\n@@ -2506,7 +2545,8 @@ static int try_to_unuse(unsigned int type)\n \t       !signal_pending(current) &&\n \t       (i = find_next_to_unuse(si, i)) != 0) {\n \n-\t\tentry = swp_entry(type, i);\n+\t\tslot = swp_slot(type, i);\n+\t\tentry = swp_slot_to_swp_entry(slot);\n \t\tfolio = swap_cache_get_folio(entry);\n \t\tif (!folio)\n \t\t\tcontinue;\n@@ -2890,7 +2930,7 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \t}\n \n \t/*\n-\t * Wait for swap operations protected by get/put_swap_device()\n+\t * Wait for swap operations protected by swap_slot_(tryget|put)_swap_info()\n \t * to complete.  Because of synchronize_rcu() here, all swap\n \t * operations protected by RCU reader side lock (including any\n \t * spinlock) will be waited too.  This makes it easy to\n@@ -3331,7 +3371,7 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n \t\tif (!si->global_cluster)\n \t\t\tgoto err;\n \t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n-\t\t\tsi->global_cluster->next[i] = SWAP_ENTRY_INVALID;\n+\t\t\tsi->global_cluster->next[i] = SWAP_SLOT_INVALID;\n \t\tspin_lock_init(&si->global_cluster_lock);\n \t}\n \n@@ -3669,6 +3709,7 @@ void si_swapinfo(struct sysinfo *val)\n  */\n static int __swap_duplicate(swp_entry_t entry, unsigned char usage, int nr)\n {\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n \tstruct swap_info_struct *si;\n \tstruct swap_cluster_info *ci;\n \tunsigned long offset;\n@@ -3676,13 +3717,13 @@ static int __swap_duplicate(swp_entry_t entry, unsigned char usage, int nr)\n \tunsigned char has_cache;\n \tint err, i;\n \n-\tsi = swap_entry_to_info(entry);\n+\tsi = swap_slot_to_info(slot);\n \tif (WARN_ON_ONCE(!si)) {\n \t\tpr_err(\"%s%08lx\\n\", Bad_file, entry.val);\n \t\treturn -EINVAL;\n \t}\n \n-\toffset = swp_offset(entry);\n+\toffset = swp_slot_offset(slot);\n \tVM_WARN_ON(nr > SWAPFILE_CLUSTER - offset % SWAPFILE_CLUSTER);\n \tVM_WARN_ON(usage == 1 && nr > 1);\n \tci = swap_cluster_lock(si, offset);\n@@ -3788,7 +3829,7 @@ int swapcache_prepare(swp_entry_t entry, int nr)\n  */\n void swapcache_clear(struct swap_info_struct *si, swp_entry_t entry, int nr)\n {\n-\tswap_entries_put_cache(si, entry, nr);\n+\tswap_slots_put_cache(si, swp_entry_to_swp_slot(entry), nr);\n }\n \n /*\n@@ -3815,6 +3856,7 @@ int add_swap_count_continuation(swp_entry_t entry, gfp_t gfp_mask)\n \tstruct page *list_page;\n \tpgoff_t offset;\n \tunsigned char count;\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n \tint ret = 0;\n \n \t/*\n@@ -3823,7 +3865,7 @@ int add_swap_count_continuation(swp_entry_t entry, gfp_t gfp_mask)\n \t */\n \tpage = alloc_page(gfp_mask | __GFP_HIGHMEM);\n \n-\tsi = get_swap_device(entry);\n+\tsi = swap_slot_tryget_swap_info(slot);\n \tif (!si) {\n \t\t/*\n \t\t * An acceptable race has occurred since the failing\n@@ -3832,7 +3874,7 @@ int add_swap_count_continuation(swp_entry_t entry, gfp_t gfp_mask)\n \t\tgoto outer;\n \t}\n \n-\toffset = swp_offset(entry);\n+\toffset = swp_slot_offset(slot);\n \n \tci = swap_cluster_lock(si, offset);\n \n@@ -3895,7 +3937,7 @@ int add_swap_count_continuation(swp_entry_t entry, gfp_t gfp_mask)\n \tspin_unlock(&si->cont_lock);\n out:\n \tswap_cluster_unlock(ci);\n-\tput_swap_device(si);\n+\tswap_slot_put_swap_info(si);\n outer:\n \tif (page)\n \t\t__free_page(page);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author acknowledged that the zswap tree code's range partition logic cannot be reused for the new virtual swap space design and decided to use a simple unified zswap tree in the new implementation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged limitation",
                "new approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The zswap tree code, specifically the range partition logic, can no\nlonger easily be reused for the new virtual swap space design. Use a\nsimple unified zswap tree in the new implementation for now.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/zswap.h |  7 -----\n mm/swapfile.c         |  9 +-----\n mm/zswap.c            | 69 +++++++------------------------------------\n 3 files changed, 11 insertions(+), 74 deletions(-)\n\ndiff --git a/include/linux/zswap.h b/include/linux/zswap.h\nindex 30c193a1207e1..1a04caf283dc8 100644\n--- a/include/linux/zswap.h\n+++ b/include/linux/zswap.h\n@@ -28,8 +28,6 @@ unsigned long zswap_total_pages(void);\n bool zswap_store(struct folio *folio);\n int zswap_load(struct folio *folio);\n void zswap_invalidate(swp_entry_t swp);\n-int zswap_swapon(int type, unsigned long nr_pages);\n-void zswap_swapoff(int type);\n void zswap_memcg_offline_cleanup(struct mem_cgroup *memcg);\n void zswap_lruvec_state_init(struct lruvec *lruvec);\n void zswap_folio_swapin(struct folio *folio);\n@@ -50,11 +48,6 @@ static inline int zswap_load(struct folio *folio)\n }\n \n static inline void zswap_invalidate(swp_entry_t swp) {}\n-static inline int zswap_swapon(int type, unsigned long nr_pages)\n-{\n-\treturn 0;\n-}\n-static inline void zswap_swapoff(int type) {}\n static inline void zswap_memcg_offline_cleanup(struct mem_cgroup *memcg) {}\n static inline void zswap_lruvec_state_init(struct lruvec *lruvec) {}\n static inline void zswap_folio_swapin(struct folio *folio) {}\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 4b4126d4e2769..3f70df488c1da 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -2970,7 +2970,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tspin_unlock(&p->lock);\n \tspin_unlock(&swap_lock);\n \tarch_swap_invalidate_area(p->type);\n-\tzswap_swapoff(p->type);\n \tmutex_unlock(&swapon_mutex);\n \tkfree(p->global_cluster);\n \tp->global_cluster = NULL;\n@@ -3613,10 +3612,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \t\t}\n \t}\n \n-\terror = zswap_swapon(si->type, maxpages);\n-\tif (error)\n-\t\tgoto bad_swap_unlock_inode;\n-\n \t/*\n \t * Flush any pending IO and dirty mappings before we start using this\n \t * swap device.\n@@ -3625,7 +3620,7 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \terror = inode_drain_writes(inode);\n \tif (error) {\n \t\tinode->i_flags &= ~S_SWAPFILE;\n-\t\tgoto free_swap_zswap;\n+\t\tgoto bad_swap_unlock_inode;\n \t}\n \n \tmutex_lock(&swapon_mutex);\n@@ -3648,8 +3643,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \n \terror = 0;\n \tgoto out;\n-free_swap_zswap:\n-\tzswap_swapoff(si->type);\n bad_swap_unlock_inode:\n \tinode_unlock(inode);\n bad_swap:\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex a5a3f068bd1a6..f7313261673ff 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -197,8 +197,6 @@ struct zswap_entry {\n \tstruct list_head lru;\n };\n \n-static struct xarray *zswap_trees[MAX_SWAPFILES];\n-static unsigned int nr_zswap_trees[MAX_SWAPFILES];\n \n /* RCU-protected iteration */\n static LIST_HEAD(zswap_pools);\n@@ -225,45 +223,35 @@ static bool zswap_has_pool;\n * helpers and fwd declarations\n **********************************/\n \n-/* One swap address space for each 64M swap space */\n-#define ZSWAP_ADDRESS_SPACE_SHIFT 14\n-#define ZSWAP_ADDRESS_SPACE_PAGES (1 << ZSWAP_ADDRESS_SPACE_SHIFT)\n-static inline struct xarray *swap_zswap_tree(swp_entry_t swp)\n-{\n-\treturn &zswap_trees[swp_type(swp)][swp_offset(swp)\n-\t\t>> ZSWAP_ADDRESS_SPACE_SHIFT];\n-}\n+static DEFINE_XARRAY(zswap_tree);\n+\n+#define zswap_tree_index(entry)\t(entry.val)\n \n static inline void *zswap_entry_store(swp_entry_t swpentry,\n \t\tstruct zswap_entry *entry)\n {\n-\tstruct xarray *tree = swap_zswap_tree(swpentry);\n-\tpgoff_t offset = swp_offset(swpentry);\n+\tpgoff_t offset = zswap_tree_index(swpentry);\n \n-\treturn xa_store(tree, offset, entry, GFP_KERNEL);\n+\treturn xa_store(&zswap_tree, offset, entry, GFP_KERNEL);\n }\n \n static inline void *zswap_entry_load(swp_entry_t swpentry)\n {\n-\tstruct xarray *tree = swap_zswap_tree(swpentry);\n-\tpgoff_t offset = swp_offset(swpentry);\n+\tpgoff_t offset = zswap_tree_index(swpentry);\n \n-\treturn xa_load(tree, offset);\n+\treturn xa_load(&zswap_tree, offset);\n }\n \n static inline void *zswap_entry_erase(swp_entry_t swpentry)\n {\n-\tstruct xarray *tree = swap_zswap_tree(swpentry);\n-\tpgoff_t offset = swp_offset(swpentry);\n+\tpgoff_t offset = zswap_tree_index(swpentry);\n \n-\treturn xa_erase(tree, offset);\n+\treturn xa_erase(&zswap_tree, offset);\n }\n \n static inline bool zswap_empty(swp_entry_t swpentry)\n {\n-\tstruct xarray *tree = swap_zswap_tree(swpentry);\n-\n-\treturn xa_empty(tree);\n+\treturn xa_empty(&zswap_tree);\n }\n \n #define zswap_pool_debug(msg, p)\t\t\t\\\n@@ -1691,43 +1679,6 @@ void zswap_invalidate(swp_entry_t swp)\n \t\tzswap_entry_free(entry);\n }\n \n-int zswap_swapon(int type, unsigned long nr_pages)\n-{\n-\tstruct xarray *trees, *tree;\n-\tunsigned int nr, i;\n-\n-\tnr = DIV_ROUND_UP(nr_pages, ZSWAP_ADDRESS_SPACE_PAGES);\n-\ttrees = kvcalloc(nr, sizeof(*tree), GFP_KERNEL);\n-\tif (!trees) {\n-\t\tpr_err(\"alloc failed, zswap disabled for swap type %d\\n\", type);\n-\t\treturn -ENOMEM;\n-\t}\n-\n-\tfor (i = 0; i < nr; i++)\n-\t\txa_init(trees + i);\n-\n-\tnr_zswap_trees[type] = nr;\n-\tzswap_trees[type] = trees;\n-\treturn 0;\n-}\n-\n-void zswap_swapoff(int type)\n-{\n-\tstruct xarray *trees = zswap_trees[type];\n-\tunsigned int i;\n-\n-\tif (!trees)\n-\t\treturn;\n-\n-\t/* try_to_unuse() invalidated all the entries already */\n-\tfor (i = 0; i < nr_zswap_trees[type]; i++)\n-\t\tWARN_ON_ONCE(!xa_empty(trees + i));\n-\n-\tkvfree(trees);\n-\tnr_zswap_trees[type] = 0;\n-\tzswap_trees[type] = NULL;\n-}\n-\n /*********************************\n * debugfs functions\n **********************************/\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the freeing ordering of virtual swap slots, explaining that they are now cleared and invalidated in a specific order to index into swap data structures. They confirmed that this change is part of the new virtual swap space design.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "confirmed approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "For the new virtual swap space design, dynamically allocate a virtual\nslot (as well as an associated metadata structure) for each swapped out\npage, and associate it to the (physical) swap slot on the swapfile/swap\npartition. This virtual swap slot is now stored in page table entries\nand used to index into swap data structures (swap cache, zswap tree,\nswap cgroup array), in place of the old physical swap slot.\n\nFor now, there is always a physical slot in the swapfile associated for\neach virtual swap slot (except those about to be freed). The virtual\nswap slot's lifetime is still tied to the lifetime of its physical swap\nslot. We do change the freeing ordering a bit - we clear the shadow,\ninvalidate the zswap entry, and uncharge swap cgroup when we release the\nvirtual swap slot, as we now use virtual swap slot to index into these\nswap data structures.\n\nWe also repurpose the swap table infrastructure as a reverse map to look\nup the virtual swap slot from its associated physical swap slot on\nswapfile. This is used in cluster readahead, as well as several swapfile\noperations, such as the swap slot reclamation that happens when the\nswapfile is almost full.  It will also be used in a future patch that\nsimplifies swapoff.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/cpuhotplug.h |   1 +\n include/linux/swap.h       |  49 +--\n mm/internal.h              |  28 +-\n mm/page_io.c               |   6 +-\n mm/shmem.c                 |   9 +-\n mm/swap.h                  |   8 +-\n mm/swap_state.c            |   5 +-\n mm/swapfile.c              |  63 +---\n mm/vswap.c                 | 658 +++++++++++++++++++++++++++++++++++++\n 9 files changed, 710 insertions(+), 117 deletions(-)\n\ndiff --git a/include/linux/cpuhotplug.h b/include/linux/cpuhotplug.h\nindex 62cd7b35a29c9..85cb45022e796 100644\n--- a/include/linux/cpuhotplug.h\n+++ b/include/linux/cpuhotplug.h\n@@ -86,6 +86,7 @@ enum cpuhp_state {\n \tCPUHP_FS_BUFF_DEAD,\n \tCPUHP_PRINTK_DEAD,\n \tCPUHP_MM_MEMCQ_DEAD,\n+\tCPUHP_MM_VSWAP_DEAD,\n \tCPUHP_PERCPU_CNT_DEAD,\n \tCPUHP_RADIX_DEAD,\n \tCPUHP_PAGE_ALLOC,\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 1ff463fb3a966..0410a00fd353c 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -471,6 +471,7 @@ static inline long get_nr_swap_pages(void)\n }\n \n void si_swapinfo(struct sysinfo *);\n+int swap_slot_alloc(swp_slot_t *slot, unsigned int order);\n swp_slot_t swap_slot_alloc_of_type(int);\n int add_swap_count_continuation(swp_entry_t, gfp_t);\n int swap_type_of(dev_t device, sector_t offset);\n@@ -670,48 +671,12 @@ static inline bool mem_cgroup_swap_full(struct folio *folio)\n #endif\n \n int vswap_init(void);\n-\n-/**\n- * swp_entry_to_swp_slot - look up the physical swap slot corresponding to a\n- *                         virtual swap slot.\n- * @entry: the virtual swap slot.\n- *\n- * Return: the physical swap slot corresponding to the virtual swap slot.\n- */\n-static inline swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry)\n-{\n-\treturn (swp_slot_t) { entry.val };\n-}\n-\n-/**\n- * swp_slot_to_swp_entry - look up the virtual swap slot corresponding to a\n- *                         physical swap slot.\n- * @slot: the physical swap slot.\n- *\n- * Return: the virtual swap slot corresponding to the physical swap slot.\n- */\n-static inline swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot)\n-{\n-\treturn (swp_entry_t) { slot.val };\n-}\n-\n-static inline bool tryget_swap_entry(swp_entry_t entry,\n-\t\t\t\tstruct swap_info_struct **sip)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si = swap_slot_tryget_swap_info(slot);\n-\n-\tif (sip)\n-\t\t*sip = si;\n-\n-\treturn si;\n-}\n-\n-static inline void put_swap_entry(swp_entry_t entry,\n-\t\t\t\tstruct swap_info_struct *si)\n-{\n-\tswap_slot_put_swap_info(si);\n-}\n+void vswap_exit(void);\n+void vswap_free(swp_entry_t entry, struct swap_cluster_info *ci);\n+swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry);\n+swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot);\n+bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si);\n+void put_swap_entry(swp_entry_t entry, struct swap_info_struct *si);\n \n #endif /* __KERNEL__*/\n #endif /* _LINUX_SWAP_H */\ndiff --git a/mm/internal.h b/mm/internal.h\nindex e739e8cac5b55..7ced0def684ca 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -322,6 +322,25 @@ static inline unsigned int folio_pte_batch_flags(struct folio *folio,\n unsigned int folio_pte_batch(struct folio *folio, pte_t *ptep, pte_t pte,\n \t\tunsigned int max_nr);\n \n+static inline swp_entry_t swap_nth(swp_entry_t entry, long n)\n+{\n+\treturn (swp_entry_t) { entry.val + n };\n+}\n+\n+/* similar to swap_nth, but check the backing physical slots as well. */\n+static inline swp_entry_t swap_move(swp_entry_t entry, long delta)\n+{\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry), next_slot;\n+\tswp_entry_t next_entry = swap_nth(entry, delta);\n+\n+\tnext_slot = swp_entry_to_swp_slot(next_entry);\n+\tif (swp_slot_type(slot) != swp_slot_type(next_slot) ||\n+\t\t\tswp_slot_offset(slot) + delta != swp_slot_offset(next_slot))\n+\t\tnext_entry.val = 0;\n+\n+\treturn next_entry;\n+}\n+\n /**\n  * pte_move_swp_offset - Move the swap entry offset field of a swap pte\n  *\t forward or backward by delta\n@@ -334,13 +353,8 @@ unsigned int folio_pte_batch(struct folio *folio, pte_t *ptep, pte_t pte,\n  */\n static inline pte_t pte_move_swp_offset(pte_t pte, long delta)\n {\n-\tsoftleaf_t entry = softleaf_from_pte(pte), new_entry;\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tpte_t new;\n-\n-\tnew_entry = swp_slot_to_swp_entry(swp_slot(swp_slot_type(slot),\n-\t\t\tswp_slot_offset(slot) + delta));\n-\tnew = swp_entry_to_pte(new_entry);\n+\tsoftleaf_t entry = softleaf_from_pte(pte);\n+\tpte_t new = swp_entry_to_pte(swap_move(entry, delta));\n \n \tif (pte_swp_soft_dirty(pte))\n \t\tnew = pte_swp_mksoft_dirty(new);\ndiff --git a/mm/page_io.c b/mm/page_io.c\nindex 0b02bcc85e2a8..5de3705572955 100644\n--- a/mm/page_io.c\n+++ b/mm/page_io.c\n@@ -364,7 +364,7 @@ static void sio_write_complete(struct kiocb *iocb, long ret)\n \t\t */\n \t\tpr_err_ratelimited(\"Write error %ld on dio swapfile (%llu)\\n\",\n \t\t\t\t   ret,\n-\t\t\t\t   swap_slot_pos(swp_entry_to_swp_slot(page_swap_entry(page))));\n+\t\t\t\t   swap_slot_dev_pos(swp_entry_to_swp_slot(page_swap_entry(page))));\n \t\tfor (p = 0; p < sio->pages; p++) {\n \t\t\tpage = sio->bvec[p].bv_page;\n \t\t\tset_page_dirty(page);\n@@ -384,7 +384,7 @@ static void swap_writepage_fs(struct folio *folio, struct swap_iocb **swap_plug)\n \tswp_slot_t slot = swp_entry_to_swp_slot(folio->swap);\n \tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n \tstruct file *swap_file = sis->swap_file;\n-\tloff_t pos = swap_slot_pos(slot);\n+\tloff_t pos = swap_slot_dev_pos(slot);\n \n \tcount_swpout_vm_event(folio);\n \tfolio_start_writeback(folio);\n@@ -549,7 +549,7 @@ static void swap_read_folio_fs(struct folio *folio, struct swap_iocb **plug)\n \tswp_slot_t slot = swp_entry_to_swp_slot(folio->swap);\n \tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n \tstruct swap_iocb *sio = NULL;\n-\tloff_t pos = swap_slot_pos(slot);\n+\tloff_t pos = swap_slot_dev_pos(slot);\n \n \tif (plug)\n \t\tsio = *plug;\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 400e2fa8e77cb..13f7469a04c8a 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -2227,7 +2227,6 @@ static int shmem_split_large_entry(struct inode *inode, pgoff_t index,\n \tXA_STATE_ORDER(xas, &mapping->i_pages, index, 0);\n \tint split_order = 0;\n \tint i;\n-\tswp_slot_t slot = swp_entry_to_swp_slot(swap);\n \n \t/* Convert user data gfp flags to xarray node gfp flags */\n \tgfp &= GFP_RECLAIM_MASK;\n@@ -2268,13 +2267,7 @@ static int shmem_split_large_entry(struct inode *inode, pgoff_t index,\n \t\t\t */\n \t\t\tfor (i = 0; i < 1 << cur_order;\n \t\t\t     i += (1 << split_order)) {\n-\t\t\t\tswp_entry_t tmp_entry;\n-\t\t\t\tswp_slot_t tmp_slot;\n-\n-\t\t\t\ttmp_slot =\n-\t\t\t\t\tswp_slot(swp_slot_type(slot),\n-\t\t\t\t\t\tswp_slot_offset(slot) + swap_offset + i);\n-\t\t\t\ttmp_entry = swp_slot_to_swp_entry(tmp_slot);\n+\t\t\t\tswp_entry_t tmp_entry = swap_nth(swap, swap_offset + i);\n \n \t\t\t\t__xa_store(&mapping->i_pages, aligned_index + i,\n \t\t\t\t\t   swp_to_radix_entry(tmp_entry), 0);\ndiff --git a/mm/swap.h b/mm/swap.h\nindex bdf7aca146643..5eb53758bbd5d 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -36,7 +36,11 @@ struct swap_cluster_info {\n \tu16 count;\n \tu8 flags;\n \tu8 order;\n-\tatomic_long_t __rcu *table;\t/* Swap table entries, see mm/swap_table.h */\n+\t/*\n+\t * Reverse map, to look up the virtual swap slot backed by a given physical\n+\t * swap slot.\n+\t*/\n+\tatomic_long_t __rcu *table;\n \tstruct list_head list;\n };\n \n@@ -212,7 +216,7 @@ static inline struct address_space *swap_address_space(swp_entry_t entry)\n }\n \n /* Return the swap device position of the swap slot. */\n-static inline loff_t swap_slot_pos(swp_slot_t slot)\n+static inline loff_t swap_slot_dev_pos(swp_slot_t slot)\n {\n \treturn ((loff_t)swp_slot_offset(slot)) << PAGE_SHIFT;\n }\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 29ec666be4204..c5ceccd756699 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -891,7 +891,8 @@ static int __init swap_init(void)\n \tswap_kobj = kobject_create_and_add(\"swap\", mm_kobj);\n \tif (!swap_kobj) {\n \t\tpr_err(\"failed to create swap kobject\\n\");\n-\t\treturn -ENOMEM;\n+\t\terr = -ENOMEM;\n+\t\tgoto vswap_exit;\n \t}\n \terr = sysfs_create_group(swap_kobj, &swap_attr_group);\n \tif (err) {\n@@ -904,6 +905,8 @@ static int __init swap_init(void)\n \n delete_obj:\n \tkobject_put(swap_kobj);\n+vswap_exit:\n+\tvswap_exit();\n \treturn err;\n }\n subsys_initcall(swap_init);\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 3f70df488c1da..68ec5d9f05848 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1249,7 +1249,6 @@ static void swap_range_alloc(struct swap_info_struct *si,\n static void swap_range_free(struct swap_info_struct *si, unsigned long offset,\n \t\t\t    unsigned int nr_entries)\n {\n-\tunsigned long begin = offset;\n \tunsigned long end = offset + nr_entries - 1;\n \tvoid (*swap_slot_free_notify)(struct block_device *, unsigned long);\n \tunsigned int i;\n@@ -1258,10 +1257,8 @@ static void swap_range_free(struct swap_info_struct *si, unsigned long offset,\n \t * Use atomic clear_bit operations only on zeromap instead of non-atomic\n \t * bitmap_clear to prevent adjacent bits corruption due to simultaneous writes.\n \t */\n-\tfor (i = 0; i < nr_entries; i++) {\n+\tfor (i = 0; i < nr_entries; i++)\n \t\tclear_bit(offset + i, si->zeromap);\n-\t\tzswap_invalidate(swp_slot_to_swp_entry(swp_slot(si->type, offset + i)));\n-\t}\n \n \tif (si->flags & SWP_BLKDEV)\n \t\tswap_slot_free_notify =\n@@ -1274,7 +1271,6 @@ static void swap_range_free(struct swap_info_struct *si, unsigned long offset,\n \t\t\tswap_slot_free_notify(si->bdev, offset);\n \t\toffset++;\n \t}\n-\tswap_cache_clear_shadow(swp_entry(si->type, begin), nr_entries);\n \n \t/*\n \t * Make sure that try_to_unuse() observes si->inuse_pages reaching 0\n@@ -1405,7 +1401,7 @@ static bool swap_sync_discard(void)\n \treturn false;\n }\n \n-static int swap_slot_alloc(swp_slot_t *slot, unsigned int order)\n+int swap_slot_alloc(swp_slot_t *slot, unsigned int order)\n {\n \tunsigned int size = 1 << order;\n \n@@ -1441,53 +1437,6 @@ static int swap_slot_alloc(swp_slot_t *slot, unsigned int order)\n \treturn 0;\n }\n \n-/**\n- * folio_alloc_swap - allocate swap space for a folio\n- * @folio: folio we want to move to swap\n- *\n- * Allocate swap space for the folio and add the folio to the\n- * swap cache.\n- *\n- * Context: Caller needs to hold the folio lock.\n- * Return: Whether the folio was added to the swap cache.\n- */\n-int folio_alloc_swap(struct folio *folio)\n-{\n-\tunsigned int order = folio_order(folio);\n-\tswp_slot_t slot = { 0 };\n-\tswp_entry_t entry = {};\n-\tint err = 0, ret;\n-\n-\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n-\tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n-\n-\tret = swap_slot_alloc(&slot, order);\n-\tif (ret)\n-\t\treturn ret;\n-\n-\t/* XXX: for now, physical and virtual swap slots are identical */\n-\tentry.val = slot.val;\n-\n-\t/* Need to call this even if allocation failed, for MEMCG_SWAP_FAIL. */\n-\tif (mem_cgroup_try_charge_swap(folio, entry)) {\n-\t\terr = -ENOMEM;\n-\t\tgoto out_free;\n-\t}\n-\n-\tif (!slot.val)\n-\t\treturn -ENOMEM;\n-\n-\terr = swap_cache_add_folio(folio, entry, __GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN, NULL);\n-\tif (err)\n-\t\tgoto out_free;\n-\n-\treturn 0;\n-\n-out_free:\n-\tput_swap_folio(folio, entry);\n-\treturn err;\n-}\n-\n static struct swap_info_struct *_swap_info_get(swp_slot_t slot)\n {\n \tstruct swap_info_struct *si;\n@@ -1733,6 +1682,13 @@ static void swap_slots_free(struct swap_info_struct *si,\n \tunsigned char *map = si->swap_map + offset;\n \tunsigned char *map_end = map + nr_pages;\n \tswp_entry_t entry = swp_slot_to_swp_entry(slot);\n+\tint i;\n+\n+\t/* release all the associated (virtual) swap slots */\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tvswap_free(entry, ci);\n+\t\tentry.val++;\n+\t}\n \n \t/* It should never free entries across different clusters */\n \tVM_BUG_ON(ci != __swap_offset_to_cluster(si, offset + nr_pages - 1));\n@@ -1745,7 +1701,6 @@ static void swap_slots_free(struct swap_info_struct *si,\n \t\t*map = 0;\n \t} while (++map < map_end);\n \n-\tmem_cgroup_uncharge_swap(entry, nr_pages);\n \tswap_range_free(si, offset, nr_pages);\n \n \tif (!ci->count)\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex e68234f053fc9..9aa95558f320a 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -4,7 +4,147 @@\n  *\n  * Copyright (C) 2024 Meta Platforms, Inc., Nhat Pham\n  */\n+#include <linux/mm.h>\n+#include <linux/gfp.h>\n #include <linux/swap.h>\n+#include <linux/swapops.h>\n+#include <linux/swap_cgroup.h>\n+#include <linux/cpuhotplug.h>\n+#include \"swap.h\"\n+#include \"swap_table.h\"\n+\n+/*\n+ * Virtual Swap Space\n+ *\n+ * We associate with each swapped out page a virtual swap slot. This will allow\n+ * us to change the backing state of a swapped out page without having to\n+ * update every single page table entries referring to it.\n+ *\n+ * For now, there is a one-to-one correspondence between a virtual swap slot\n+ * and its associated physical swap slot.\n+ *\n+ * Virtual swap slots are organized into PMD-sized clusters, analogous to\n+ * physical swap allocator. However, unlike the physical swap allocator,\n+ * the clusters are dynamically allocated and freed on-demand. There is no\n+ * \"free list\" of virtual swap clusters - new free clusters are allocated\n+ * directly from the cluster map xarray.\n+ *\n+ * This allows us to avoid the overhead of pre-allocating a large number of\n+ * virtual swap clusters.\n+ */\n+\n+/**\n+ * Swap descriptor - metadata of a swapped out page.\n+ *\n+ * @slot: The handle to the physical swap slot backing this page.\n+ */\n+struct swp_desc {\n+\tswp_slot_t slot;\n+};\n+\n+#define VSWAP_CLUSTER_SHIFT HPAGE_PMD_ORDER\n+#define VSWAP_CLUSTER_SIZE (1UL << VSWAP_CLUSTER_SHIFT)\n+#define VSWAP_CLUSTER_MASK (VSWAP_CLUSTER_SIZE - 1)\n+\n+/*\n+ * Map from a cluster id to the number of allocated virtual swap slots in the\n+ * (PMD-sized) cluster. This allows us to quickly allocate an empty cluster\n+ * for a large folio being swapped out.\n+ *\n+ * This xarray's lock is also used as the \"global\" allocator lock (for e.g, to\n+ * synchronize global cluster lists manipulation).\n+ */\n+static DEFINE_XARRAY_FLAGS(vswap_cluster_map, XA_FLAGS_TRACK_FREE);\n+\n+#if SWP_TYPE_SHIFT > 32\n+/*\n+ * In 64 bit architecture, the maximum number of virtual swap slots is capped\n+ * by the number of clusters (as the vswap_cluster_map xarray can only allocate\n+ * up to U32 clusters).\n+ */\n+#define MAX_VSWAP\t\\\n+\t(((unsigned long)U32_MAX << VSWAP_CLUSTER_SHIFT) + (VSWAP_CLUSTER_SIZE - 1))\n+#else\n+/*\n+ * In 32 bit architecture, just make sure the range of virtual swap slots is\n+ * the same as the range of physical swap slots.\n+ */\n+#define MAX_VSWAP\t(((MAX_SWAPFILES - 1) << SWP_TYPE_SHIFT) | SWP_OFFSET_MASK)\n+#endif\n+\n+static const struct xa_limit vswap_cluster_map_limit = {\n+\t.max = MAX_VSWAP >> VSWAP_CLUSTER_SHIFT,\n+\t.min = 0,\n+};\n+\n+static struct list_head partial_clusters_lists[SWAP_NR_ORDERS];\n+\n+/**\n+ * struct vswap_cluster\n+ *\n+ * @lock: Spinlock protecting the cluster's data\n+ * @rcu: RCU head for deferred freeing when the cluster is no longer in use\n+ * @list: List entry for tracking in partial_clusters_lists when not fully allocated\n+ * @id: Unique identifier for this cluster, used to calculate swap slot values\n+ * @count: Number of allocated virtual swap slots in this cluster\n+ * @order: Order of allocation (0 for single pages, higher for contiguous ranges)\n+ * @cached: Whether this cluster is cached in a per-CPU variable for fast allocation\n+ * @full: Whether this cluster is considered full (no more allocations possible)\n+ * @refcnt: Reference count tracking usage of slots in this cluster\n+ * @bitmap: Bitmap tracking which slots in the cluster are allocated\n+ * @descriptors: Pointer to array of swap descriptors for each slot in the cluster\n+ *\n+ * A vswap_cluster manages a PMD-sized group of contiguous virtual swap slots.\n+ * It tracks which slots are allocated using a bitmap and maintains the\n+ * swap descriptors in an array. The cluster is reference-counted and freed when\n+ * all of its slots are released and the cluster is not cached. Each cluster\n+ * only allocates aligned slots of a single order, determined when the cluster is\n+ * allocated (and never change for the entire lifetime of the cluster).\n+ *\n+ * Clusters can be in the following states:\n+ * - Cached in per-CPU variables for fast allocation.\n+ * - In partial_clusters_lists when partially allocated but not cached.\n+ * - Marked as full when no more allocations are possible.\n+ */\n+struct vswap_cluster {\n+\tspinlock_t lock;\n+\tunion {\n+\t\tstruct rcu_head rcu;\n+\t\tstruct list_head list;\n+\t};\n+\tunsigned long id;\n+\tunsigned int count:VSWAP_CLUSTER_SHIFT + 1;\n+\tunsigned int order:4;\n+\tbool cached:1;\n+\tbool full:1;\n+\trefcount_t refcnt;\n+\tDECLARE_BITMAP(bitmap, VSWAP_CLUSTER_SIZE);\n+\tstruct swp_desc descriptors[VSWAP_CLUSTER_SIZE];\n+};\n+\n+#define VSWAP_VAL_CLUSTER_IDX(val) ((val) >> VSWAP_CLUSTER_SHIFT)\n+#define VSWAP_CLUSTER_IDX(entry) VSWAP_VAL_CLUSTER_IDX(entry.val)\n+#define VSWAP_IDX_WITHIN_CLUSTER_VAL(val) ((val) & VSWAP_CLUSTER_MASK)\n+#define VSWAP_IDX_WITHIN_CLUSTER(entry)\tVSWAP_IDX_WITHIN_CLUSTER_VAL(entry.val)\n+\n+struct percpu_vswap_cluster {\n+\tstruct vswap_cluster *clusters[SWAP_NR_ORDERS];\n+\tlocal_lock_t lock;\n+};\n+\n+/*\n+ * Per-CPU cache of the last allocated cluster for each order. This allows\n+ * allocation fast path to skip the global vswap_cluster_map's spinlock, if\n+ * the locally cached cluster still has free slots. Note that caching a cluster\n+ * also increments its reference count.\n+ */\n+static DEFINE_PER_CPU(struct percpu_vswap_cluster, percpu_vswap_cluster) = {\n+\t.clusters = { NULL, },\n+\t.lock = INIT_LOCAL_LOCK(),\n+};\n+\n+static atomic_t vswap_alloc_reject;\n+static atomic_t vswap_used;\n \n #ifdef CONFIG_DEBUG_FS\n #include <linux/debugfs.h>\n@@ -17,6 +157,10 @@ static int vswap_debug_fs_init(void)\n \t\treturn -ENODEV;\n \n \tvswap_debugfs_root = debugfs_create_dir(\"vswap\", NULL);\n+\tdebugfs_create_atomic_t(\"alloc_reject\", 0444,\n+\t\tvswap_debugfs_root, &vswap_alloc_reject);\n+\tdebugfs_create_atomic_t(\"used\", 0444, vswap_debugfs_root, &vswap_used);\n+\n \treturn 0;\n }\n #else\n@@ -26,10 +170,524 @@ static int vswap_debug_fs_init(void)\n }\n #endif\n \n+static struct swp_desc *vswap_iter(struct vswap_cluster **clusterp, unsigned long i)\n+{\n+\tunsigned long cluster_id = VSWAP_VAL_CLUSTER_IDX(i);\n+\tstruct vswap_cluster *cluster = *clusterp;\n+\tstruct swp_desc *desc = NULL;\n+\tunsigned long slot_index;\n+\n+\tif (!cluster || cluster_id != cluster->id) {\n+\t\tif (cluster)\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\t\tif (!cluster)\n+\t\t\tgoto done;\n+\t\tVM_WARN_ON(cluster->id != cluster_id);\n+\t\tspin_lock(&cluster->lock);\n+\t}\n+\n+\tslot_index = VSWAP_IDX_WITHIN_CLUSTER_VAL(i);\n+\tif (test_bit(slot_index, cluster->bitmap))\n+\t\tdesc = &cluster->descriptors[slot_index];\n+\n+\tif (!desc) {\n+\t\tspin_unlock(&cluster->lock);\n+\t\tcluster = NULL;\n+\t}\n+\n+done:\n+\t*clusterp = cluster;\n+\treturn desc;\n+}\n+\n+static bool cluster_is_alloc_candidate(struct vswap_cluster *cluster)\n+{\n+\treturn cluster->count + (1 << (cluster->order)) <= VSWAP_CLUSTER_SIZE;\n+}\n+\n+static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster, int start)\n+{\n+\tint i, nr = 1 << cluster->order;\n+\tstruct swp_desc *desc;\n+\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = &cluster->descriptors[start + i];\n+\t\tdesc->slot.val = 0;\n+\t}\n+\tcluster->count += nr;\n+}\n+\n+static unsigned long vswap_alloc_from_cluster(struct vswap_cluster *cluster)\n+{\n+\tint nr = 1 << cluster->order;\n+\tunsigned long i = cluster->id ? 0 : nr;\n+\n+\tVM_WARN_ON(!spin_is_locked(&cluster->lock));\n+\tif (!cluster_is_alloc_candidate(cluster))\n+\t\treturn 0;\n+\n+\t/* Find the first free range of nr contiguous aligned slots */\n+\ti = bitmap_find_next_zero_area(cluster->bitmap,\n+\t\t\tVSWAP_CLUSTER_SIZE, i, nr, nr - 1);\n+\tif (i >= VSWAP_CLUSTER_SIZE)\n+\t\treturn 0;\n+\n+\t/* Mark the range as allocated in the bitmap */\n+\tbitmap_set(cluster->bitmap, i, nr);\n+\n+\trefcount_add(nr, &cluster->refcnt);\n+\t__vswap_alloc_from_cluster(cluster, i);\n+\treturn i + (cluster->id << VSWAP_CLUSTER_SHIFT);\n+}\n+\n+/* Allocate a contiguous range of virtual swap slots */\n+static swp_entry_t vswap_alloc(int order)\n+{\n+\tstruct xa_limit limit = vswap_cluster_map_limit;\n+\tstruct vswap_cluster *local, *cluster;\n+\tint nr = 1 << order;\n+\tbool need_caching = true;\n+\tu32 cluster_id;\n+\tswp_entry_t entry;\n+\n+\tentry.val = 0;\n+\n+\t/* first, let's try the locally cached cluster */\n+\trcu_read_lock();\n+\tlocal_lock(&percpu_vswap_cluster.lock);\n+\tcluster = this_cpu_read(percpu_vswap_cluster.clusters[order]);\n+\tif (cluster) {\n+\t\tspin_lock(&cluster->lock);\n+\t\tentry.val = vswap_alloc_from_cluster(cluster);\n+\t\tneed_caching = !entry.val;\n+\n+\t\tif (!entry.val || !cluster_is_alloc_candidate(cluster)) {\n+\t\t\tthis_cpu_write(percpu_vswap_cluster.clusters[order], NULL);\n+\t\t\tcluster->cached = false;\n+\t\t\trefcount_dec(&cluster->refcnt);\n+\t\t\tcluster->full = true;\n+\t\t}\n+\t\tspin_unlock(&cluster->lock);\n+\t}\n+\tlocal_unlock(&percpu_vswap_cluster.lock);\n+\trcu_read_unlock();\n+\n+\t/*\n+\t * Local cluster does not have space. Let's try the uncached partial\n+\t * clusters before acquiring a new free cluster to reduce fragmentation,\n+\t * and avoid having to allocate a new cluster structure.\n+\t */\n+\tif (!entry.val) {\n+\t\tcluster = NULL;\n+\t\txa_lock(&vswap_cluster_map);\n+\t\tlist_for_each_entry_safe(cluster, local,\n+\t\t\t\t&partial_clusters_lists[order], list) {\n+\t\t\tif (!spin_trylock(&cluster->lock))\n+\t\t\t\tcontinue;\n+\n+\t\t\tentry.val = vswap_alloc_from_cluster(cluster);\n+\t\t\tlist_del_init(&cluster->list);\n+\t\t\tcluster->full = !entry.val || !cluster_is_alloc_candidate(cluster);\n+\t\t\tneed_caching = !cluster->full;\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t\tif (entry.val)\n+\t\t\t\tbreak;\n+\t\t}\n+\t\txa_unlock(&vswap_cluster_map);\n+\t}\n+\n+\t/* try a new free cluster */\n+\tif (!entry.val) {\n+\t\tcluster = kvzalloc(sizeof(*cluster), GFP_KERNEL);\n+\t\tif (cluster) {\n+\t\t\t/* first cluster cannot allocate a PMD-sized THP */\n+\t\t\tif (order == SWAP_NR_ORDERS - 1)\n+\t\t\t\tlimit.min = 1;\n+\n+\t\t\tif (!xa_alloc(&vswap_cluster_map, &cluster_id, cluster, limit,\n+\t\t\t\t\t\tGFP_KERNEL)) {\n+\t\t\t\tspin_lock_init(&cluster->lock);\n+\t\t\t\tcluster->id = cluster_id;\n+\t\t\t\tcluster->order = order;\n+\t\t\t\tINIT_LIST_HEAD(&cluster->list);\n+\t\t\t\t/* Initialize bitmap to all zeros (all slots free) */\n+\t\t\t\tbitmap_zero(cluster->bitmap, VSWAP_CLUSTER_SIZE);\n+\t\t\t\tentry.val = cluster->id << VSWAP_CLUSTER_SHIFT;\n+\t\t\t\trefcount_set(&cluster->refcnt, nr);\n+\t\t\t\tif (!cluster_id)\n+\t\t\t\t\tentry.val += nr;\n+\t\t\t\t__vswap_alloc_from_cluster(cluster,\n+\t\t\t\t\t(entry.val & VSWAP_CLUSTER_MASK));\n+\t\t\t\t/* Mark the allocated range in the bitmap */\n+\t\t\t\tbitmap_set(cluster->bitmap, (entry.val & VSWAP_CLUSTER_MASK), nr);\n+\t\t\t\tneed_caching = cluster_is_alloc_candidate(cluster);\n+\t\t\t} else {\n+\t\t\t\t/* Failed to insert into cluster map, free the cluster */\n+\t\t\t\tkvfree(cluster);\n+\t\t\t\tcluster = NULL;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif (need_caching && entry.val) {\n+\t\tlocal_lock(&percpu_vswap_cluster.lock);\n+\t\tlocal = this_cpu_read(percpu_vswap_cluster.clusters[order]);\n+\t\tif (local != cluster) {\n+\t\t\tif (local) {\n+\t\t\t\tspin_lock(&local->lock);\n+\t\t\t\t/* only update the local cache if cached cluster is full */\n+\t\t\t\tneed_caching = !cluster_is_alloc_candidate(local);\n+\t\t\t\tif (need_caching) {\n+\t\t\t\t\tthis_cpu_write(percpu_vswap_cluster.clusters[order], NULL);\n+\t\t\t\t\tlocal->cached = false;\n+\t\t\t\t\trefcount_dec(&local->refcnt);\n+\t\t\t\t}\n+\t\t\t\tspin_unlock(&local->lock);\n+\t\t\t}\n+\n+\t\t\tVM_WARN_ON(!cluster);\n+\t\t\tspin_lock(&cluster->lock);\n+\t\t\tif (cluster_is_alloc_candidate(cluster)) {\n+\t\t\t\tif (need_caching) {\n+\t\t\t\t\tthis_cpu_write(percpu_vswap_cluster.clusters[order], cluster);\n+\t\t\t\t\trefcount_inc(&cluster->refcnt);\n+\t\t\t\t\tcluster->cached = true;\n+\t\t\t\t} else {\n+\t\t\t\t\txa_lock(&vswap_cluster_map);\n+\t\t\t\t\tVM_WARN_ON(!list_empty(&cluster->list));\n+\t\t\t\t\tlist_add(&cluster->list, &partial_clusters_lists[order]);\n+\t\t\t\t\txa_unlock(&vswap_cluster_map);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t}\n+\t\tlocal_unlock(&percpu_vswap_cluster.lock);\n+\t}\n+\n+\tif (entry.val) {\n+\t\tVM_WARN_ON(entry.val + nr - 1 > MAX_VSWAP);\n+\t\tatomic_add(nr, &vswap_used);\n+\t} else {\n+\t\tatomic_add(nr, &vswap_alloc_reject);\n+\t}\n+\treturn entry;\n+}\n+\n+static void vswap_cluster_free(struct vswap_cluster *cluster)\n+{\n+\tVM_WARN_ON(cluster->count || cluster->cached);\n+\tVM_WARN_ON(!spin_is_locked(&cluster->lock));\n+\txa_lock(&vswap_cluster_map);\n+\tlist_del_init(&cluster->list);\n+\t__xa_erase(&vswap_cluster_map, cluster->id);\n+\txa_unlock(&vswap_cluster_map);\n+\trcu_head_init(&cluster->rcu);\n+\tkvfree_rcu(cluster, rcu);\n+}\n+\n+static inline void release_vswap_slot(struct vswap_cluster *cluster,\n+\t\tunsigned long index)\n+{\n+\tunsigned long slot_index = VSWAP_IDX_WITHIN_CLUSTER_VAL(index);\n+\n+\tVM_WARN_ON(!spin_is_locked(&cluster->lock));\n+\tcluster->count--;\n+\n+\tbitmap_clear(cluster->bitmap, slot_index, 1);\n+\n+\t/* we only free uncached empty clusters */\n+\tif (refcount_dec_and_test(&cluster->refcnt))\n+\t\tvswap_cluster_free(cluster);\n+\telse if (cluster->full && cluster_is_alloc_candidate(cluster)) {\n+\t\tcluster->full = false;\n+\t\tif (!cluster->cached) {\n+\t\t\txa_lock(&vswap_cluster_map);\n+\t\t\tVM_WARN_ON(!list_empty(&cluster->list));\n+\t\t\tlist_add_tail(&cluster->list,\n+\t\t\t\t&partial_clusters_lists[cluster->order]);\n+\t\t\txa_unlock(&vswap_cluster_map);\n+\t\t}\n+\t}\n+\n+\tatomic_dec(&vswap_used);\n+}\n+\n+/*\n+ * Update the physical-to-virtual swap slot mapping.\n+ * Caller must ensure the physical swap slot's cluster is locked.\n+ */\n+static void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n+\t\t\t   unsigned long vswap, int nr)\n+{\n+\tatomic_long_t *table;\n+\tunsigned long slot_offset = swp_slot_offset(slot);\n+\tunsigned int ci_off = slot_offset % SWAPFILE_CLUSTER;\n+\tint i;\n+\n+\ttable = rcu_dereference_protected(ci->table, lockdep_is_held(&ci->lock));\n+\tVM_WARN_ON(!table);\n+\tfor (i = 0; i < nr; i++)\n+\t\t__swap_table_set(ci, ci_off + i, vswap ? vswap + i : 0);\n+}\n+\n+/**\n+ * vswap_free - free a virtual swap slot.\n+ * @entry: the virtual swap slot to free\n+ * @ci: the physical swap slot's cluster (optional, can be NULL)\n+ *\n+ * If @ci is NULL, this function is called to clean up a virtual swap entry\n+ * when no linkage has been established between physical and virtual swap slots.\n+ * If @ci is provided, the caller must ensure it is locked.\n+ */\n+void vswap_free(swp_entry_t entry, struct swap_cluster_info *ci)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\n+\tif (!entry.val)\n+\t\treturn;\n+\n+\tswap_cache_clear_shadow(entry, 1);\n+\tzswap_invalidate(entry);\n+\tmem_cgroup_uncharge_swap(entry, 1);\n+\n+\t/* do not immediately erase the virtual slot to prevent its reuse */\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn;\n+\t}\n+\n+\tif (desc->slot.val)\n+\t\tvswap_rmap_set(ci, desc->slot, 0, 1);\n+\n+\t/* erase forward mapping and release the virtual slot for reallocation */\n+\trelease_vswap_slot(cluster, entry.val);\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+/**\n+ * folio_alloc_swap - allocate swap space for a folio.\n+ * @folio: the folio.\n+ *\n+ * Return: 0, if the allocation succeeded, -ENOMEM, if the allocation failed.\n+ */\n+int folio_alloc_swap(struct folio *folio)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swap_info_struct *si;\n+\tstruct swap_cluster_info *ci;\n+\tint i, err, nr = folio_nr_pages(folio), order = folio_order(folio);\n+\tstruct swp_desc *desc;\n+\tswp_entry_t entry;\n+\tswp_slot_t slot;\n+\n+\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n+\tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n+\n+\tentry = vswap_alloc(folio_order(folio));\n+\tif (!entry.val)\n+\t\treturn -ENOMEM;\n+\n+\t/*\n+\t * XXX: for now, we always allocate a physical swap slot for each virtual\n+\t * swap slot, and their lifetime are coupled. This will change once we\n+\t * decouple virtual swap slots from their backing states, and only allocate\n+\t * physical swap slots for them on demand (i.e on zswap writeback, or\n+\t * fallback from zswap store failure).\n+\t */\n+\tif (swap_slot_alloc(&slot, order)) {\n+\t\tfor (i = 0; i < nr; i++)\n+\t\t\tvswap_free((swp_entry_t){entry.val + i}, NULL);\n+\t\tentry.val = 0;\n+\t\treturn -ENOMEM;\n+\t}\n+\n+\t/* establish the vrtual <-> physical swap slots linkages. */\n+\tsi = __swap_slot_to_info(slot);\n+\tci = swap_cluster_lock(si, swp_slot_offset(slot));\n+\tvswap_rmap_set(ci, slot, entry.val, nr);\n+\tswap_cluster_unlock(ci);\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\n+\t\tdesc->slot.val = slot.val + i;\n+\t}\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\t/*\n+\t * XXX: for now, we charge towards the memory cgroup's swap limit on virtual\n+\t * swap slots allocation. This is acceptable because as noted above, each\n+\t * virtual swap slot corresponds to a physical swap slot. Once we have\n+\t * decoupled virtual and physical swap slots, we will only charge when we\n+\t * actually allocate a physical swap slot.\n+\t */\n+\tif (mem_cgroup_try_charge_swap(folio, entry))\n+\t\tgoto out_free;\n+\n+\terr = swap_cache_add_folio(folio, entry, __GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN, NULL);\n+\tif (err)\n+\t\tgoto out_free;\n+\n+\treturn 0;\n+\n+out_free:\n+\tput_swap_folio(folio, entry);\n+\treturn -ENOMEM;\n+}\n+\n+/**\n+ * swp_entry_to_swp_slot - look up the physical swap slot corresponding to a\n+ *                         virtual swap slot.\n+ * @entry: the virtual swap slot.\n+ *\n+ * Return: the physical swap slot corresponding to the virtual swap slot.\n+ */\n+swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tswp_slot_t slot;\n+\n+\tslot.val = 0;\n+\tif (!entry.val)\n+\t\treturn slot;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn (swp_slot_t){0};\n+\t}\n+\tslot = desc->slot;\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\treturn slot;\n+}\n+\n+/**\n+ * swp_slot_to_swp_entry - look up the virtual swap slot corresponding to a\n+ *                         physical swap slot.\n+ * @slot: the physical swap slot.\n+ *\n+ * Return: the virtual swap slot corresponding to the physical swap slot.\n+ */\n+swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot)\n+{\n+\tswp_entry_t ret;\n+\tstruct swap_cluster_info *ci;\n+\tunsigned long offset;\n+\tunsigned int ci_off;\n+\n+\tret.val = 0;\n+\tif (!slot.val)\n+\t\treturn ret;\n+\n+\toffset = swp_slot_offset(slot);\n+\tci_off = offset % SWAPFILE_CLUSTER;\n+\tci = __swap_slot_to_cluster(slot);\n+\n+\tret.val = swap_table_get(ci, ci_off);\n+\treturn ret;\n+}\n+\n+bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si)\n+{\n+\tstruct vswap_cluster *cluster;\n+\tswp_slot_t slot;\n+\n+\tslot = swp_entry_to_swp_slot(entry);\n+\t*si = swap_slot_tryget_swap_info(slot);\n+\tif (!*si)\n+\t\treturn false;\n+\n+\t/*\n+\t * Ensure the cluster and its associated data structures (swap cache etc.)\n+\t * remain valid.\n+\t */\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, VSWAP_CLUSTER_IDX(entry));\n+\tif (!cluster || !refcount_inc_not_zero(&cluster->refcnt)) {\n+\t\trcu_read_unlock();\n+\t\tswap_slot_put_swap_info(*si);\n+\t\t*si = NULL;\n+\t\treturn false;\n+\t}\n+\trcu_read_unlock();\n+\treturn true;\n+}\n+\n+void put_swap_entry(swp_entry_t entry, struct swap_info_struct *si)\n+{\n+\tstruct vswap_cluster *cluster;\n+\n+\tif (si)\n+\t\tswap_slot_put_swap_info(si);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, VSWAP_CLUSTER_IDX(entry));\n+\tspin_lock(&cluster->lock);\n+\tif (refcount_dec_and_test(&cluster->refcnt))\n+\t\tvswap_cluster_free(cluster);\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+static int vswap_cpu_dead(unsigned int cpu)\n+{\n+\tstruct percpu_vswap_cluster *percpu_cluster;\n+\tstruct vswap_cluster *cluster;\n+\tint order;\n+\n+\tpercpu_cluster = per_cpu_ptr(&percpu_vswap_cluster, cpu);\n+\n+\trcu_read_lock();\n+\tlocal_lock(&percpu_cluster->lock);\n+\tfor (order = 0; order < SWAP_NR_ORDERS; order++) {\n+\t\tcluster = percpu_cluster->clusters[order];\n+\t\tif (cluster) {\n+\t\t\tpercpu_cluster->clusters[order] = NULL;\n+\t\t\tspin_lock(&cluster->lock);\n+\t\t\tcluster->cached = false;\n+\t\t\tif (refcount_dec_and_test(&cluster->refcnt))\n+\t\t\t\tvswap_cluster_free(cluster);\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t}\n+\t}\n+\tlocal_unlock(&percpu_cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn 0;\n+}\n+\n+\n int vswap_init(void)\n {\n+\tint i;\n+\n+\tif (cpuhp_setup_state_nocalls(CPUHP_MM_VSWAP_DEAD, \"mm/vswap:dead\", NULL,\n+\t\t\t\tvswap_cpu_dead)) {\n+\t\tpr_err(\"Failed to register vswap CPU hotplug callback\\n\");\n+\t\treturn -ENOMEM;\n+\t}\n+\n \tif (vswap_debug_fs_init())\n \t\tpr_warn(\"Failed to initialize vswap debugfs\\n\");\n \n+\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n+\t\tINIT_LIST_HEAD(&partial_clusters_lists[i]);\n+\n \treturn 0;\n }\n+\n+void vswap_exit(void)\n+{\n+}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the swap cache lock contention by moving the swap cache and working set shadow to the virtual swap descriptor, effectively range-partitioning the swap cache by virtual swap clusters of PMD size.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Move the swap cache (and workingset shadow for anonymous pages) to the\nvirtual swap descriptor. This effectively range-partitions the swap\ncache by virtual swap clusters (of PMD sized), eliminate swap cache lock\ncontention.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n mm/huge_memory.c |   4 +-\n mm/migrate.c     |   6 +-\n mm/shmem.c       |   4 +-\n mm/swap.h        |  16 +--\n mm/swap_state.c  | 251 +--------------------------------\n mm/vmscan.c      |   6 +-\n mm/vswap.c       | 350 ++++++++++++++++++++++++++++++++++++++++++++++-\n 7 files changed, 364 insertions(+), 273 deletions(-)\n\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 21215ac870144..dcbd3821d6178 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -3825,7 +3825,7 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \t\t\t\treturn -EINVAL;\n \t\t\t}\n \n-\t\t\tswap_cache_lock();\n+\t\t\tswap_cache_lock(folio->swap);\n \t\t}\n \n \t\t/* lock lru list/PageCompound, ref frozen by page_ref_freeze */\n@@ -3901,7 +3901,7 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \t\t\tunlock_page_lruvec(lruvec);\n \n \t\tif (folio_test_swapcache(folio))\n-\t\t\tswap_cache_unlock();\n+\t\t\tswap_cache_unlock(folio->swap);\n \t} else {\n \t\tsplit_queue_unlock(ds_queue);\n \t\treturn -EAGAIN;\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 11d9b43dff5d8..e850b05a232de 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -600,13 +600,13 @@ static int __folio_migrate_mapping(struct address_space *mapping,\n \tnewzone = folio_zone(newfolio);\n \n \tif (folio_test_swapcache(folio))\n-\t\tswap_cache_lock_irq();\n+\t\tswap_cache_lock_irq(folio->swap);\n \telse\n \t\txas_lock_irq(&xas);\n \n \tif (!folio_ref_freeze(folio, expected_count)) {\n \t\tif (folio_test_swapcache(folio))\n-\t\t\tswap_cache_unlock_irq();\n+\t\t\tswap_cache_unlock_irq(folio->swap);\n \t\telse\n \t\t\txas_unlock_irq(&xas);\n \t\treturn -EAGAIN;\n@@ -652,7 +652,7 @@ static int __folio_migrate_mapping(struct address_space *mapping,\n \n \t/* Leave irq disabled to prevent preemption while updating stats */\n \tif (folio_test_swapcache(folio))\n-\t\tswap_cache_unlock();\n+\t\tswap_cache_unlock(folio->swap);\n \telse\n \t\txas_unlock(&xas);\n \ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 13f7469a04c8a..66cf8af6779ca 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -2168,12 +2168,12 @@ static int shmem_replace_folio(struct folio **foliop, gfp_t gfp,\n \tnew->swap = entry;\n \tfolio_set_swapcache(new);\n \n-\tswap_cache_lock_irq();\n+\tswap_cache_lock_irq(entry);\n \t__swap_cache_replace_folio(old, new);\n \tmem_cgroup_replace_folio(old, new);\n \tshmem_update_stats(new, nr_pages);\n \tshmem_update_stats(old, -nr_pages);\n-\tswap_cache_unlock_irq();\n+\tswap_cache_unlock_irq(entry);\n \n \tfolio_add_lru(new);\n \t*foliop = new;\ndiff --git a/mm/swap.h b/mm/swap.h\nindex 5eb53758bbd5d..57ed24a2d6356 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -205,10 +205,12 @@ void __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug);\n \n /* linux/mm/swap_state.c */\n extern struct address_space swap_space __read_mostly;\n-void swap_cache_lock_irq(void);\n-void swap_cache_unlock_irq(void);\n-void swap_cache_lock(void);\n-void swap_cache_unlock(void);\n+\n+/* linux/mm/vswap.c */\n+void swap_cache_lock_irq(swp_entry_t entry);\n+void swap_cache_unlock_irq(swp_entry_t entry);\n+void swap_cache_lock(swp_entry_t entry);\n+void swap_cache_unlock(swp_entry_t entry);\n \n static inline struct address_space *swap_address_space(swp_entry_t entry)\n {\n@@ -256,12 +258,11 @@ static inline bool folio_matches_swap_entry(const struct folio *folio,\n  */\n struct folio *swap_cache_get_folio(swp_entry_t entry);\n void *swap_cache_get_shadow(swp_entry_t entry);\n-int swap_cache_add_folio(struct folio *folio, swp_entry_t entry, gfp_t gfp, void **shadow);\n+void swap_cache_add_folio(struct folio *folio, swp_entry_t entry, void **shadow);\n void swap_cache_del_folio(struct folio *folio);\n /* Below helpers require the caller to lock the swap cache. */\n void __swap_cache_del_folio(struct folio *folio, swp_entry_t entry, void *shadow);\n void __swap_cache_replace_folio(struct folio *old, struct folio *new);\n-void swap_cache_clear_shadow(swp_entry_t entry, int nr_ents);\n \n void show_swap_cache_info(void);\n void swapcache_clear(struct swap_info_struct *si, swp_entry_t entry, int nr);\n@@ -422,9 +423,8 @@ static inline void *swap_cache_get_shadow(swp_entry_t entry)\n \treturn NULL;\n }\n \n-static inline int swap_cache_add_folio(struct folio *folio, swp_entry_t entry, gfp_t gfp, void **shadow)\n+static inline void swap_cache_add_folio(struct folio *folio, swp_entry_t entry, void **shadow)\n {\n-\treturn 0;\n }\n \n static inline void swap_cache_del_folio(struct folio *folio)\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex c5ceccd756699..00fa3e76a5c19 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -41,28 +41,6 @@ struct address_space swap_space __read_mostly = {\n \t.a_ops = &swap_aops,\n };\n \n-static DEFINE_XARRAY(swap_cache);\n-\n-void swap_cache_lock_irq(void)\n-{\n-\txa_lock_irq(&swap_cache);\n-}\n-\n-void swap_cache_unlock_irq(void)\n-{\n-\txa_unlock_irq(&swap_cache);\n-}\n-\n-void swap_cache_lock(void)\n-{\n-\txa_lock(&swap_cache);\n-}\n-\n-void swap_cache_unlock(void)\n-{\n-\txa_unlock(&swap_cache);\n-}\n-\n static bool enable_vma_readahead __read_mostly = true;\n \n #define SWAP_RA_ORDER_CEILING\t5\n@@ -94,231 +72,6 @@ void show_swap_cache_info(void)\n \tprintk(\"Total swap = %lukB\\n\", K(total_swap_pages));\n }\n \n-/**\n- * swap_cache_get_folio - Looks up a folio in the swap cache.\n- * @entry: swap entry used for the lookup.\n- *\n- * A found folio will be returned unlocked and with its refcount increased.\n- *\n- * Context: Caller must ensure @entry is valid and protect the swap device\n- * with reference count or locks.\n- * Return: Returns the found folio on success, NULL otherwise. The caller\n- * must lock nd check if the folio still matches the swap entry before\n- * use (e.g., folio_matches_swap_entry).\n- */\n-struct folio *swap_cache_get_folio(swp_entry_t entry)\n-{\n-\tvoid *entry_val;\n-\tstruct folio *folio;\n-\n-\tfor (;;) {\n-\t\trcu_read_lock();\n-\t\tentry_val = xa_load(&swap_cache, entry.val);\n-\t\tif (!entry_val || xa_is_value(entry_val)) {\n-\t\t\trcu_read_unlock();\n-\t\t\treturn NULL;\n-\t\t}\n-\t\tfolio = entry_val;\n-\t\tif (likely(folio_try_get(folio))) {\n-\t\t\trcu_read_unlock();\n-\t\t\treturn folio;\n-\t\t}\n-\t\trcu_read_unlock();\n-\t}\n-\n-\treturn NULL;\n-}\n-\n-/**\n- * swap_cache_get_shadow - Looks up a shadow in the swap cache.\n- * @entry: swap entry used for the lookup.\n- *\n- * Context: Caller must ensure @entry is valid and protect the swap device\n- * with reference count or locks.\n- * Return: Returns either NULL or an XA_VALUE (shadow).\n- */\n-void *swap_cache_get_shadow(swp_entry_t entry)\n-{\n-\tvoid *entry_val;\n-\n-\trcu_read_lock();\n-\tentry_val = xa_load(&swap_cache, entry.val);\n-\trcu_read_unlock();\n-\n-\tif (xa_is_value(entry_val))\n-\t\treturn entry_val;\n-\treturn NULL;\n-}\n-\n-/**\n- * swap_cache_add_folio - Add a folio into the swap cache.\n- * @folio: The folio to be added.\n- * @entry: The swap entry corresponding to the folio.\n- * @gfp: gfp_mask for XArray node allocation.\n- * @shadowp: If a shadow is found, return the shadow.\n- *\n- * Context: Caller must ensure @entry is valid and protect the swap device\n- * with reference count or locks.\n- * The caller also needs to update the corresponding swap_map slots with\n- * SWAP_HAS_CACHE bit to avoid race or conflict.\n- *\n- * Return: 0 on success, negative error code on failure.\n- */\n-int swap_cache_add_folio(struct folio *folio, swp_entry_t entry, gfp_t gfp, void **shadowp)\n-{\n-\tXA_STATE_ORDER(xas, &swap_cache, entry.val, folio_order(folio));\n-\tunsigned long nr_pages = folio_nr_pages(folio);\n-\tunsigned long i;\n-\tvoid *old;\n-\n-\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n-\tVM_WARN_ON_ONCE_FOLIO(folio_test_swapcache(folio), folio);\n-\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapbacked(folio), folio);\n-\n-\tfolio_ref_add(folio, nr_pages);\n-\tfolio_set_swapcache(folio);\n-\tfolio->swap = entry;\n-\n-\tdo {\n-\t\txas_lock_irq(&xas);\n-\t\txas_create_range(&xas);\n-\t\tif (xas_error(&xas))\n-\t\t\tgoto unlock;\n-\t\tfor (i = 0; i < nr_pages; i++) {\n-\t\t\tVM_BUG_ON_FOLIO(xas.xa_index != entry.val + i, folio);\n-\t\t\told = xas_load(&xas);\n-\t\t\tif (old && !xa_is_value(old)) {\n-\t\t\t\tVM_WARN_ON_ONCE_FOLIO(1, folio);\n-\t\t\t\txas_set_err(&xas, -EEXIST);\n-\t\t\t\tgoto unlock;\n-\t\t\t}\n-\t\t\tif (shadowp && xa_is_value(old) && !*shadowp)\n-\t\t\t\t*shadowp = old;\n-\t\t\txas_store(&xas, folio);\n-\t\t\txas_next(&xas);\n-\t\t}\n-\t\tnode_stat_mod_folio(folio, NR_FILE_PAGES, nr_pages);\n-\t\tlruvec_stat_mod_folio(folio, NR_SWAPCACHE, nr_pages);\n-unlock:\n-\t\txas_unlock_irq(&xas);\n-\t} while (xas_nomem(&xas, gfp));\n-\n-\tif (!xas_error(&xas))\n-\t\treturn 0;\n-\n-\tfolio_clear_swapcache(folio);\n-\tfolio_ref_sub(folio, nr_pages);\n-\treturn xas_error(&xas);\n-}\n-\n-/**\n- * __swap_cache_del_folio - Removes a folio from the swap cache.\n- * @folio: The folio.\n- * @entry: The first swap entry that the folio corresponds to.\n- * @shadow: shadow value to be filled in the swap cache.\n- *\n- * Removes a folio from the swap cache and fills a shadow in place.\n- * This won't put the folio's refcount. The caller has to do that.\n- *\n- * Context: Caller must ensure the folio is locked and in the swap cache\n- * using the index of @entry, and lock the swap cache xarray.\n- */\n-void __swap_cache_del_folio(struct folio *folio, swp_entry_t entry, void *shadow)\n-{\n-\tlong nr_pages = folio_nr_pages(folio);\n-\tXA_STATE(xas, &swap_cache, entry.val);\n-\tint i;\n-\n-\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n-\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n-\tVM_WARN_ON_ONCE_FOLIO(folio_test_writeback(folio), folio);\n-\n-\tfor (i = 0; i < nr_pages; i++) {\n-\t\tvoid *old = xas_store(&xas, shadow);\n-\t\tVM_WARN_ON_FOLIO(old != folio, folio);\n-\t\txas_next(&xas);\n-\t}\n-\n-\tfolio->swap.val = 0;\n-\tfolio_clear_swapcache(folio);\n-\tnode_stat_mod_folio(folio, NR_FILE_PAGES, -nr_pages);\n-\tlruvec_stat_mod_folio(folio, NR_SWAPCACHE, -nr_pages);\n-}\n-\n-/**\n- * swap_cache_del_folio - Removes a folio from the swap cache.\n- * @folio: The folio.\n- *\n- * Same as __swap_cache_del_folio, but handles lock and refcount. The\n- * caller must ensure the folio is either clean or has a swap count\n- * equal to zero, or it may cause data loss.\n- *\n- * Context: Caller must ensure the folio is locked and in the swap cache.\n- */\n-void swap_cache_del_folio(struct folio *folio)\n-{\n-\tswp_entry_t entry = folio->swap;\n-\n-\txa_lock_irq(&swap_cache);\n-\t__swap_cache_del_folio(folio, entry, NULL);\n-\txa_unlock_irq(&swap_cache);\n-\n-\tput_swap_folio(folio, entry);\n-\tfolio_ref_sub(folio, folio_nr_pages(folio));\n-}\n-\n-/**\n- * __swap_cache_replace_folio - Replace a folio in the swap cache.\n- * @old: The old folio to be replaced.\n- * @new: The new folio.\n- *\n- * Replace an existing folio in the swap cache with a new folio. The\n- * caller is responsible for setting up the new folio's flag and swap\n- * entries. Replacement will take the new folio's swap entry value as\n- * the starting offset to override all slots covered by the new folio.\n- *\n- * Context: Caller must ensure both folios are locked, and lock the\n- * swap cache xarray.\n- */\n-void __swap_cache_replace_folio(struct folio *old, struct folio *new)\n-{\n-\tswp_entry_t entry = new->swap;\n-\tunsigned long nr_pages = folio_nr_pages(new);\n-\tXA_STATE(xas, &swap_cache, entry.val);\n-\tint i;\n-\n-\tVM_WARN_ON_ONCE(!folio_test_swapcache(old) || !folio_test_swapcache(new));\n-\tVM_WARN_ON_ONCE(!folio_test_locked(old) || !folio_test_locked(new));\n-\tVM_WARN_ON_ONCE(!entry.val);\n-\n-\tfor (i = 0; i < nr_pages; i++) {\n-\t\tvoid *old_entry = xas_store(&xas, new);\n-\t\tWARN_ON_ONCE(!old_entry || xa_is_value(old_entry) || old_entry != old);\n-\t\txas_next(&xas);\n-\t}\n-}\n-\n-/**\n- * swap_cache_clear_shadow - Clears a set of shadows in the swap cache.\n- * @entry: The starting index entry.\n- * @nr_ents: How many slots need to be cleared.\n- *\n- * Context: Caller must ensure the range is valid and all in one single cluster,\n- * not occupied by any folio.\n- */\n-void swap_cache_clear_shadow(swp_entry_t entry, int nr_ents)\n-{\n-\tXA_STATE(xas, &swap_cache, entry.val);\n-\tint i;\n-\n-\txas_lock(&xas);\n-\tfor (i = 0; i < nr_ents; i++) {\n-\t\txas_store(&xas, NULL);\n-\t\txas_next(&xas);\n-\t}\n-\txas_unlock(&xas);\n-}\n-\n /*\n  * If we are the only user, then try to free up the swap cache.\n  *\n@@ -497,9 +250,7 @@ struct folio *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \tif (mem_cgroup_swapin_charge_folio(new_folio, NULL, gfp_mask, entry))\n \t\tgoto fail_unlock;\n \n-\t/* May fail (-ENOMEM) if XArray node allocation failed. */\n-\tif (swap_cache_add_folio(new_folio, entry, gfp_mask & GFP_RECLAIM_MASK, &shadow))\n-\t\tgoto fail_unlock;\n+\tswap_cache_add_folio(new_folio, entry, &shadow);\n \n \tmemcg1_swapin(entry, 1);\n \ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 558ff7f413786..c9ec1a1458b4e 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -712,7 +712,7 @@ static int __remove_mapping(struct address_space *mapping, struct folio *folio,\n \tBUG_ON(mapping != folio_mapping(folio));\n \n \tif (folio_test_swapcache(folio)) {\n-\t\tswap_cache_lock_irq();\n+\t\tswap_cache_lock_irq(folio->swap);\n \t} else {\n \t\tspin_lock(&mapping->host->i_lock);\n \t\txa_lock_irq(&mapping->i_pages);\n@@ -759,7 +759,7 @@ static int __remove_mapping(struct address_space *mapping, struct folio *folio,\n \t\t\tshadow = workingset_eviction(folio, target_memcg);\n \t\t__swap_cache_del_folio(folio, swap, shadow);\n \t\tmemcg1_swapout(folio, swap);\n-\t\tswap_cache_unlock_irq();\n+\t\tswap_cache_unlock_irq(swap);\n \t\tput_swap_folio(folio, swap);\n \t} else {\n \t\tvoid (*free_folio)(struct folio *);\n@@ -798,7 +798,7 @@ static int __remove_mapping(struct address_space *mapping, struct folio *folio,\n \n cannot_free:\n \tif (folio_test_swapcache(folio)) {\n-\t\tswap_cache_unlock_irq();\n+\t\tswap_cache_unlock_irq(folio->swap);\n \t} else {\n \t\txa_unlock_irq(&mapping->i_pages);\n \t\tspin_unlock(&mapping->host->i_lock);\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex 9aa95558f320a..d44199dc059a3 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -37,9 +37,15 @@\n  * Swap descriptor - metadata of a swapped out page.\n  *\n  * @slot: The handle to the physical swap slot backing this page.\n+ * @swap_cache: The folio in swap cache.\n+ * @shadow: The shadow entry.\n  */\n struct swp_desc {\n \tswp_slot_t slot;\n+\tunion {\n+\t\tstruct folio *swap_cache;\n+\t\tvoid *shadow;\n+\t};\n };\n \n #define VSWAP_CLUSTER_SHIFT HPAGE_PMD_ORDER\n@@ -170,6 +176,24 @@ static int vswap_debug_fs_init(void)\n }\n #endif\n \n+/*\n+ * Lockless version of vswap_iter - assumes caller holds cluster lock.\n+ * Used when iterating within the same cluster with the lock already held.\n+ */\n+static struct swp_desc *__vswap_iter(struct vswap_cluster *cluster, unsigned long i)\n+{\n+\tunsigned long slot_index;\n+\n+\tlockdep_assert_held(&cluster->lock);\n+\tVM_WARN_ON(cluster->id != VSWAP_VAL_CLUSTER_IDX(i));\n+\n+\tslot_index = VSWAP_IDX_WITHIN_CLUSTER_VAL(i);\n+\tif (test_bit(slot_index, cluster->bitmap))\n+\t\treturn &cluster->descriptors[slot_index];\n+\n+\treturn NULL;\n+}\n+\n static struct swp_desc *vswap_iter(struct vswap_cluster **clusterp, unsigned long i)\n {\n \tunsigned long cluster_id = VSWAP_VAL_CLUSTER_IDX(i);\n@@ -448,7 +472,6 @@ void vswap_free(swp_entry_t entry, struct swap_cluster_info *ci)\n \tif (!entry.val)\n \t\treturn;\n \n-\tswap_cache_clear_shadow(entry, 1);\n \tzswap_invalidate(entry);\n \tmem_cgroup_uncharge_swap(entry, 1);\n \n@@ -460,6 +483,10 @@ void vswap_free(swp_entry_t entry, struct swap_cluster_info *ci)\n \t\treturn;\n \t}\n \n+\t/* Clear shadow if present */\n+\tif (xa_is_value(desc->shadow))\n+\t\tdesc->shadow = NULL;\n+\n \tif (desc->slot.val)\n \t\tvswap_rmap_set(ci, desc->slot, 0, 1);\n \n@@ -480,7 +507,7 @@ int folio_alloc_swap(struct folio *folio)\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swap_info_struct *si;\n \tstruct swap_cluster_info *ci;\n-\tint i, err, nr = folio_nr_pages(folio), order = folio_order(folio);\n+\tint i, nr = folio_nr_pages(folio), order = folio_order(folio);\n \tstruct swp_desc *desc;\n \tswp_entry_t entry;\n \tswp_slot_t slot;\n@@ -533,9 +560,7 @@ int folio_alloc_swap(struct folio *folio)\n \tif (mem_cgroup_try_charge_swap(folio, entry))\n \t\tgoto out_free;\n \n-\terr = swap_cache_add_folio(folio, entry, __GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN, NULL);\n-\tif (err)\n-\t\tgoto out_free;\n+\tswap_cache_add_folio(folio, entry, NULL);\n \n \treturn 0;\n \n@@ -668,6 +693,321 @@ static int vswap_cpu_dead(unsigned int cpu)\n \treturn 0;\n }\n \n+/**\n+ * swap_cache_lock - lock the swap cache for a swap entry\n+ * @entry: the swap entry\n+ *\n+ * Locks the vswap cluster spinlock for the given swap entry.\n+ */\n+void swap_cache_lock(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster;\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\tspin_lock(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+/**\n+ * swap_cache_unlock - unlock the swap cache for a swap entry\n+ * @entry: the swap entry\n+ *\n+ * Unlocks the vswap cluster spinlock for the given swap entry.\n+ */\n+void swap_cache_unlock(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster;\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+/**\n+ * swap_cache_lock_irq - lock the swap cache with interrupts disabled\n+ * @entry: the swap entry\n+ *\n+ * Locks the vswap cluster spinlock and disables interrupts for the given swap entry.\n+ */\n+void swap_cache_lock_irq(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster;\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\tspin_lock_irq(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+/**\n+ * swap_cache_unlock_irq - unlock the swap cache with interrupts enabled\n+ * @entry: the swap entry\n+ *\n+ * Unlocks the vswap cluster spinlock and enables interrupts for the given swap entry.\n+ */\n+void swap_cache_unlock_irq(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster;\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\tspin_unlock_irq(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+/**\n+ * swap_cache_get_folio - Looks up a folio in the swap cache.\n+ * @entry: swap entry used for the lookup.\n+ *\n+ * A found folio will be returned unlocked and with its refcount increased.\n+ *\n+ * Context: Caller must ensure @entry is valid and protect the cluster with\n+ * reference count or locks.\n+ *\n+ * Return: Returns the found folio on success, NULL otherwise. The caller\n+ * must lock and check if the folio still matches the swap entry before\n+ * use (e.g., folio_matches_swap_entry).\n+ */\n+struct folio *swap_cache_get_folio(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tstruct folio *folio;\n+\n+\tfor (;;) {\n+\t\trcu_read_lock();\n+\t\tdesc = vswap_iter(&cluster, entry.val);\n+\t\tif (!desc) {\n+\t\t\trcu_read_unlock();\n+\t\t\treturn NULL;\n+\t\t}\n+\n+\t\t/* Check if this is a shadow value (xa_is_value equivalent) */\n+\t\tif (xa_is_value(desc->shadow)) {\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t\trcu_read_unlock();\n+\t\t\treturn NULL;\n+\t\t}\n+\n+\t\tfolio = desc->swap_cache;\n+\t\tif (!folio) {\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t\trcu_read_unlock();\n+\t\t\treturn NULL;\n+\t\t}\n+\n+\t\tif (likely(folio_try_get(folio))) {\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t\trcu_read_unlock();\n+\t\t\treturn folio;\n+\t\t}\n+\t\tspin_unlock(&cluster->lock);\n+\t\trcu_read_unlock();\n+\t}\n+\n+\treturn NULL;\n+}\n+\n+/**\n+ * swap_cache_get_shadow - Looks up a shadow in the swap cache.\n+ * @entry: swap entry used for the lookup.\n+ *\n+ * Context: Caller must ensure @entry is valid and protect the cluster with\n+ * reference count or locks.\n+ *\n+ * Return: Returns either NULL or an XA_VALUE (shadow).\n+ */\n+void *swap_cache_get_shadow(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tvoid *shadow;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn NULL;\n+\t}\n+\n+\tshadow = desc->shadow;\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\tif (xa_is_value(shadow))\n+\t\treturn shadow;\n+\treturn NULL;\n+}\n+\n+/**\n+ * swap_cache_add_folio - Add a folio into the swap cache.\n+ * @folio: The folio to be added.\n+ * @entry: The swap entry corresponding to the folio.\n+ * @shadowp: If a shadow is found, return the shadow.\n+ *\n+ * Context: Caller must ensure @entry is valid and protect the cluster with\n+ * reference count or locks.\n+ *\n+ * The caller also needs to update the corresponding swap_map slots with\n+ * SWAP_HAS_CACHE bit to avoid race or conflict.\n+ */\n+void swap_cache_add_folio(struct folio *folio, swp_entry_t entry, void **shadowp)\n+{\n+\tstruct vswap_cluster *cluster;\n+\tunsigned long nr_pages = folio_nr_pages(folio);\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\tunsigned long i;\n+\tstruct swp_desc *desc;\n+\tvoid *old;\n+\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(folio_test_swapcache(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapbacked(folio), folio);\n+\n+\tfolio_ref_add(folio, nr_pages);\n+\tfolio_set_swapcache(folio);\n+\tfolio->swap = entry;\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\tspin_lock_irq(&cluster->lock);\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\t\told = desc->shadow;\n+\n+\t\t/* Warn if slot is already occupied by a folio */\n+\t\tVM_WARN_ON_FOLIO(old && !xa_is_value(old), folio);\n+\n+\t\t/* Save shadow if found and not yet saved */\n+\t\tif (shadowp && xa_is_value(old) && !*shadowp)\n+\t\t\t*shadowp = old;\n+\n+\t\tdesc->swap_cache = folio;\n+\t}\n+\n+\tspin_unlock_irq(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\tnode_stat_mod_folio(folio, NR_FILE_PAGES, nr_pages);\n+\tlruvec_stat_mod_folio(folio, NR_SWAPCACHE, nr_pages);\n+}\n+\n+/**\n+ * __swap_cache_del_folio - Removes a folio from the swap cache.\n+ * @folio: The folio.\n+ * @entry: The first swap entry that the folio corresponds to.\n+ * @shadow: shadow value to be filled in the swap cache.\n+ *\n+ * Removes a folio from the swap cache and fills a shadow in place.\n+ * This won't put the folio's refcount. The caller has to do that.\n+ *\n+ * Context: Caller must ensure the folio is locked and in the swap cache\n+ * using the index of @entry, and lock the swap cache.\n+ */\n+void __swap_cache_del_folio(struct folio *folio, swp_entry_t entry, void *shadow)\n+{\n+\tlong nr_pages = folio_nr_pages(folio);\n+\tstruct vswap_cluster *cluster;\n+\tstruct swp_desc *desc;\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\tint i;\n+\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(folio_test_writeback(folio), folio);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n+\t\tVM_WARN_ON_FOLIO(!desc || desc->swap_cache != folio, folio);\n+\t\tdesc->shadow = shadow;\n+\t}\n+\trcu_read_unlock();\n+\n+\tfolio->swap.val = 0;\n+\tfolio_clear_swapcache(folio);\n+\tnode_stat_mod_folio(folio, NR_FILE_PAGES, -nr_pages);\n+\tlruvec_stat_mod_folio(folio, NR_SWAPCACHE, -nr_pages);\n+}\n+\n+/**\n+ * swap_cache_del_folio - Removes a folio from the swap cache.\n+ * @folio: The folio.\n+ *\n+ * Same as __swap_cache_del_folio, but handles lock and refcount. The\n+ * caller must ensure the folio is either clean or has a swap count\n+ * equal to zero, or it may cause data loss.\n+ *\n+ * Context: Caller must ensure the folio is locked and in the swap cache.\n+ */\n+void swap_cache_del_folio(struct folio *folio)\n+{\n+\tswp_entry_t entry = folio->swap;\n+\n+\tswap_cache_lock_irq(entry);\n+\t__swap_cache_del_folio(folio, entry, NULL);\n+\tswap_cache_unlock_irq(entry);\n+\n+\tput_swap_folio(folio, entry);\n+\tfolio_ref_sub(folio, folio_nr_pages(folio));\n+}\n+\n+/**\n+ * __swap_cache_replace_folio - Replace a folio in the swap cache.\n+ * @old: The old folio to be replaced.\n+ * @new: The new folio.\n+ *\n+ * Replace an existing folio in the swap cache with a new folio. The\n+ * caller is responsible for setting up the new folio's flag and swap\n+ * entries. Replacement will take the new folio's swap entry value as\n+ * the starting offset to override all slots covered by the new folio.\n+ *\n+ * Context: Caller must ensure both folios are locked, and lock the\n+ * swap cache.\n+ */\n+void __swap_cache_replace_folio(struct folio *old, struct folio *new)\n+{\n+\tswp_entry_t entry = new->swap;\n+\tunsigned long nr_pages = folio_nr_pages(new);\n+\tstruct vswap_cluster *cluster;\n+\tstruct swp_desc *desc;\n+\tunsigned long cluster_id = VSWAP_CLUSTER_IDX(entry);\n+\tvoid *old_entry;\n+\tint i;\n+\n+\tVM_WARN_ON_ONCE(!folio_test_swapcache(old) || !folio_test_swapcache(new));\n+\tVM_WARN_ON_ONCE(!folio_test_locked(old) || !folio_test_locked(new));\n+\tVM_WARN_ON_ONCE(!entry.val);\n+\n+\trcu_read_lock();\n+\tcluster = xa_load(&vswap_cluster_map, cluster_id);\n+\tVM_WARN_ON(!cluster);\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\t\told_entry = desc->swap_cache;\n+\t\tVM_WARN_ON(!old_entry || xa_is_value(old_entry) || old_entry != old);\n+\t\tdesc->swap_cache = new;\n+\t}\n+\trcu_read_unlock();\n+}\n \n int vswap_init(void)\n {\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author addressed a concern about zswap tree lock contention by re-partitioning the zswap pool into virtual swap clusters, eliminating the need for the zswap tree lock.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Remove the zswap tree and manage zswap entries directly\nthrough the virtual swap descriptor. This re-partitions the zswap pool\n(by virtual swap cluster), which eliminates zswap tree lock contention.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/zswap.h |   6 +++\n mm/vswap.c            | 100 ++++++++++++++++++++++++++++++++++++++++++\n mm/zswap.c            |  40 -----------------\n 3 files changed, 106 insertions(+), 40 deletions(-)\n\ndiff --git a/include/linux/zswap.h b/include/linux/zswap.h\nindex 1a04caf283dc8..7eb3ce7e124fc 100644\n--- a/include/linux/zswap.h\n+++ b/include/linux/zswap.h\n@@ -6,6 +6,7 @@\n #include <linux/mm_types.h>\n \n struct lruvec;\n+struct zswap_entry;\n \n extern atomic_long_t zswap_stored_pages;\n \n@@ -33,6 +34,11 @@ void zswap_lruvec_state_init(struct lruvec *lruvec);\n void zswap_folio_swapin(struct folio *folio);\n bool zswap_is_enabled(void);\n bool zswap_never_enabled(void);\n+void *zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry);\n+void *zswap_entry_load(swp_entry_t swpentry);\n+void *zswap_entry_erase(swp_entry_t swpentry);\n+bool zswap_empty(swp_entry_t swpentry);\n+\n #else\n \n struct zswap_lruvec_state {};\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex d44199dc059a3..9bb733f00fd21 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -10,6 +10,7 @@\n #include <linux/swapops.h>\n #include <linux/swap_cgroup.h>\n #include <linux/cpuhotplug.h>\n+#include <linux/zswap.h>\n #include \"swap.h\"\n #include \"swap_table.h\"\n \n@@ -37,11 +38,13 @@\n  * Swap descriptor - metadata of a swapped out page.\n  *\n  * @slot: The handle to the physical swap slot backing this page.\n+ * @zswap_entry: The zswap entry associated with this swap slot.\n  * @swap_cache: The folio in swap cache.\n  * @shadow: The shadow entry.\n  */\n struct swp_desc {\n \tswp_slot_t slot;\n+\tstruct zswap_entry *zswap_entry;\n \tunion {\n \t\tstruct folio *swap_cache;\n \t\tvoid *shadow;\n@@ -238,6 +241,7 @@ static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster, int start)\n \tfor (i = 0; i < nr; i++) {\n \t\tdesc = &cluster->descriptors[start + i];\n \t\tdesc->slot.val = 0;\n+\t\tdesc->zswap_entry = NULL;\n \t}\n \tcluster->count += nr;\n }\n@@ -1009,6 +1013,102 @@ void __swap_cache_replace_folio(struct folio *old, struct folio *new)\n \trcu_read_unlock();\n }\n \n+#ifdef CONFIG_ZSWAP\n+/**\n+ * zswap_entry_store - store a zswap entry for a swap entry\n+ * @swpentry: the swap entry\n+ * @entry: the zswap entry to store\n+ *\n+ * Stores a zswap entry in the swap descriptor for the given swap entry.\n+ * The cluster is locked during the store operation.\n+ *\n+ * Return: the old zswap entry if one existed, NULL otherwise\n+ */\n+void *zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tvoid *old;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, swpentry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn NULL;\n+\t}\n+\n+\told = desc->zswap_entry;\n+\tdesc->zswap_entry = entry;\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn old;\n+}\n+\n+/**\n+ * zswap_entry_load - load a zswap entry for a swap entry\n+ * @swpentry: the swap entry\n+ *\n+ * Loads the zswap entry from the swap descriptor for the given swap entry.\n+ *\n+ * Return: the zswap entry if one exists, NULL otherwise\n+ */\n+void *zswap_entry_load(swp_entry_t swpentry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tvoid *zswap_entry;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, swpentry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn NULL;\n+\t}\n+\n+\tzswap_entry = desc->zswap_entry;\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn zswap_entry;\n+}\n+\n+/**\n+ * zswap_entry_erase - erase a zswap entry for a swap entry\n+ * @swpentry: the swap entry\n+ *\n+ * Erases the zswap entry from the swap descriptor for the given swap entry.\n+ * The cluster is locked during the erase operation.\n+ *\n+ * Return: the zswap entry that was erased, NULL if none existed\n+ */\n+void *zswap_entry_erase(swp_entry_t swpentry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tvoid *old;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, swpentry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn NULL;\n+\t}\n+\n+\told = desc->zswap_entry;\n+\tdesc->zswap_entry = NULL;\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn old;\n+}\n+\n+bool zswap_empty(swp_entry_t swpentry)\n+{\n+\treturn xa_empty(&vswap_cluster_map);\n+}\n+#endif /* CONFIG_ZSWAP */\n+\n int vswap_init(void)\n {\n \tint i;\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex f7313261673ff..72441131f094e 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -223,37 +223,6 @@ static bool zswap_has_pool;\n * helpers and fwd declarations\n **********************************/\n \n-static DEFINE_XARRAY(zswap_tree);\n-\n-#define zswap_tree_index(entry)\t(entry.val)\n-\n-static inline void *zswap_entry_store(swp_entry_t swpentry,\n-\t\tstruct zswap_entry *entry)\n-{\n-\tpgoff_t offset = zswap_tree_index(swpentry);\n-\n-\treturn xa_store(&zswap_tree, offset, entry, GFP_KERNEL);\n-}\n-\n-static inline void *zswap_entry_load(swp_entry_t swpentry)\n-{\n-\tpgoff_t offset = zswap_tree_index(swpentry);\n-\n-\treturn xa_load(&zswap_tree, offset);\n-}\n-\n-static inline void *zswap_entry_erase(swp_entry_t swpentry)\n-{\n-\tpgoff_t offset = zswap_tree_index(swpentry);\n-\n-\treturn xa_erase(&zswap_tree, offset);\n-}\n-\n-static inline bool zswap_empty(swp_entry_t swpentry)\n-{\n-\treturn xa_empty(&zswap_tree);\n-}\n-\n #define zswap_pool_debug(msg, p)\t\t\t\\\n \tpr_debug(\"%s pool %s\\n\", msg, (p)->tfm_name)\n \n@@ -1445,13 +1414,6 @@ static bool zswap_store_page(struct page *page,\n \t\tgoto compress_failed;\n \n \told = zswap_entry_store(page_swpentry, entry);\n-\tif (xa_is_err(old)) {\n-\t\tint err = xa_err(old);\n-\n-\t\tWARN_ONCE(err != -ENOMEM, \"unexpected xarray error: %d\\n\", err);\n-\t\tzswap_reject_alloc_fail++;\n-\t\tgoto store_failed;\n-\t}\n \n \t/*\n \t * We may have had an existing entry that became stale when\n@@ -1498,8 +1460,6 @@ static bool zswap_store_page(struct page *page,\n \n \treturn true;\n \n-store_failed:\n-\tzs_free(pool->zs_pool, entry->handle);\n compress_failed:\n \tzswap_entry_cache_free(entry);\n \treturn false;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the memory overhead of swap cgroup information, explaining that it is now dynamically incurred when the virtual swap cluster is allocated and reducing the memory overhead in a huge but sparsely used swap space.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "reduces memory overhead",
                "on demand"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Once we decouple a swap entry from its backing store via the virtual\nswap, we can no longer statically allocate an array to store the swap\nentries' cgroup information. Move it to the swap descriptor.\n\nNote that the memory overhead for swap cgroup information is now on\ndemand, i.e dynamically incurred when the virtual swap cluster is\nallocated. This help reduces the memory overhead in a huge but\nsparsely used swap space.\n\nFor instance, a 2 TB swapfile consists of 2147483648 swap slots, each\nincurring 2 bytes of overhead for swap cgroup, for a total of 1 GB. If\nwe only utilize 10% of the swapfile, we will save 900 MB.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap_cgroup.h |  13 ---\n mm/Makefile                 |   3 -\n mm/swap_cgroup.c            | 174 ------------------------------------\n mm/swapfile.c               |   7 --\n mm/vswap.c                  |  95 ++++++++++++++++++++\n 5 files changed, 95 insertions(+), 197 deletions(-)\n delete mode 100644 mm/swap_cgroup.c\n\ndiff --git a/include/linux/swap_cgroup.h b/include/linux/swap_cgroup.h\nindex 91cdf12190a03..a2abb4d6fa085 100644\n--- a/include/linux/swap_cgroup.h\n+++ b/include/linux/swap_cgroup.h\n@@ -9,8 +9,6 @@\n extern void swap_cgroup_record(struct folio *folio, unsigned short id, swp_entry_t ent);\n extern unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents);\n extern unsigned short lookup_swap_cgroup_id(swp_entry_t ent);\n-extern int swap_cgroup_swapon(int type, unsigned long max_pages);\n-extern void swap_cgroup_swapoff(int type);\n \n #else\n \n@@ -31,17 +29,6 @@ unsigned short lookup_swap_cgroup_id(swp_entry_t ent)\n \treturn 0;\n }\n \n-static inline int\n-swap_cgroup_swapon(int type, unsigned long max_pages)\n-{\n-\treturn 0;\n-}\n-\n-static inline void swap_cgroup_swapoff(int type)\n-{\n-\treturn;\n-}\n-\n #endif\n \n #endif /* __LINUX_SWAP_CGROUP_H */\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 67fa4586e7e18..a7538784191bf 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -103,9 +103,6 @@ obj-$(CONFIG_PAGE_COUNTER) += page_counter.o\n obj-$(CONFIG_LIVEUPDATE) += memfd_luo.o\n obj-$(CONFIG_MEMCG_V1) += memcontrol-v1.o\n obj-$(CONFIG_MEMCG) += memcontrol.o vmpressure.o\n-ifdef CONFIG_SWAP\n-obj-$(CONFIG_MEMCG) += swap_cgroup.o\n-endif\n obj-$(CONFIG_CGROUP_HUGETLB) += hugetlb_cgroup.o\n obj-$(CONFIG_GUP_TEST) += gup_test.o\n obj-$(CONFIG_DMAPOOL_TEST) += dmapool_test.o\ndiff --git a/mm/swap_cgroup.c b/mm/swap_cgroup.c\ndeleted file mode 100644\nindex 77ce1d66c318d..0000000000000\n--- a/mm/swap_cgroup.c\n+++ /dev/null\n@@ -1,174 +0,0 @@\n-// SPDX-License-Identifier: GPL-2.0\n-#include <linux/swap_cgroup.h>\n-#include <linux/vmalloc.h>\n-#include <linux/mm.h>\n-\n-#include <linux/swapops.h> /* depends on mm.h include */\n-\n-static DEFINE_MUTEX(swap_cgroup_mutex);\n-\n-/* Pack two cgroup id (short) of two entries in one swap_cgroup (atomic_t) */\n-#define ID_PER_SC (sizeof(struct swap_cgroup) / sizeof(unsigned short))\n-#define ID_SHIFT (BITS_PER_TYPE(unsigned short))\n-#define ID_MASK (BIT(ID_SHIFT) - 1)\n-struct swap_cgroup {\n-\tatomic_t ids;\n-};\n-\n-struct swap_cgroup_ctrl {\n-\tstruct swap_cgroup *map;\n-};\n-\n-static struct swap_cgroup_ctrl swap_cgroup_ctrl[MAX_SWAPFILES];\n-\n-static unsigned short __swap_cgroup_id_lookup(struct swap_cgroup *map,\n-\t\t\t\t\t      pgoff_t offset)\n-{\n-\tunsigned int shift = (offset % ID_PER_SC) * ID_SHIFT;\n-\tunsigned int old_ids = atomic_read(&map[offset / ID_PER_SC].ids);\n-\n-\tBUILD_BUG_ON(!is_power_of_2(ID_PER_SC));\n-\tBUILD_BUG_ON(sizeof(struct swap_cgroup) != sizeof(atomic_t));\n-\n-\treturn (old_ids >> shift) & ID_MASK;\n-}\n-\n-static unsigned short __swap_cgroup_id_xchg(struct swap_cgroup *map,\n-\t\t\t\t\t    pgoff_t offset,\n-\t\t\t\t\t    unsigned short new_id)\n-{\n-\tunsigned short old_id;\n-\tstruct swap_cgroup *sc = &map[offset / ID_PER_SC];\n-\tunsigned int shift = (offset % ID_PER_SC) * ID_SHIFT;\n-\tunsigned int new_ids, old_ids = atomic_read(&sc->ids);\n-\n-\tdo {\n-\t\told_id = (old_ids >> shift) & ID_MASK;\n-\t\tnew_ids = (old_ids & ~(ID_MASK << shift));\n-\t\tnew_ids |= ((unsigned int)new_id) << shift;\n-\t} while (!atomic_try_cmpxchg(&sc->ids, &old_ids, new_ids));\n-\n-\treturn old_id;\n-}\n-\n-/**\n- * swap_cgroup_record - record mem_cgroup for a set of swap entries.\n- * These entries must belong to one single folio, and that folio\n- * must be being charged for swap space (swap out), and these\n- * entries must not have been charged\n- *\n- * @folio: the folio that the swap entry belongs to\n- * @id: mem_cgroup ID to be recorded\n- * @ent: the first swap entry to be recorded\n- */\n-void swap_cgroup_record(struct folio *folio, unsigned short id,\n-\t\t\tswp_entry_t ent)\n-{\n-\tunsigned int nr_ents = folio_nr_pages(folio);\n-\tswp_slot_t slot = swp_entry_to_swp_slot(ent);\n-\tstruct swap_cgroup *map;\n-\tpgoff_t offset, end;\n-\tunsigned short old;\n-\n-\toffset = swp_slot_offset(slot);\n-\tend = offset + nr_ents;\n-\tmap = swap_cgroup_ctrl[swp_slot_type(slot)].map;\n-\n-\tdo {\n-\t\told = __swap_cgroup_id_xchg(map, offset, id);\n-\t\tVM_BUG_ON(old);\n-\t} while (++offset != end);\n-}\n-\n-/**\n- * swap_cgroup_clear - clear mem_cgroup for a set of swap entries.\n- * These entries must be being uncharged from swap. They either\n- * belongs to one single folio in the swap cache (swap in for\n- * cgroup v1), or no longer have any users (slot freeing).\n- *\n- * @ent: the first swap entry to be recorded into\n- * @nr_ents: number of swap entries to be recorded\n- *\n- * Returns the existing old value.\n- */\n-unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(ent);\n-\tpgoff_t offset = swp_slot_offset(slot);\n-\tpgoff_t end = offset + nr_ents;\n-\tstruct swap_cgroup *map;\n-\tunsigned short old, iter = 0;\n-\n-\tmap = swap_cgroup_ctrl[swp_slot_type(slot)].map;\n-\n-\tdo {\n-\t\told = __swap_cgroup_id_xchg(map, offset, 0);\n-\t\tif (!iter)\n-\t\t\titer = old;\n-\t\tVM_BUG_ON(iter != old);\n-\t} while (++offset != end);\n-\n-\treturn old;\n-}\n-\n-/**\n- * lookup_swap_cgroup_id - lookup mem_cgroup id tied to swap entry\n- * @ent: swap entry to be looked up.\n- *\n- * Returns ID of mem_cgroup at success. 0 at failure. (0 is invalid ID)\n- */\n-unsigned short lookup_swap_cgroup_id(swp_entry_t ent)\n-{\n-\tstruct swap_cgroup_ctrl *ctrl;\n-\tswp_slot_t slot = swp_entry_to_swp_slot(ent);\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn 0;\n-\n-\tctrl = &swap_cgroup_ctrl[swp_slot_type(slot)];\n-\treturn __swap_cgroup_id_lookup(ctrl->map, swp_slot_offset(slot));\n-}\n-\n-int swap_cgroup_swapon(int type, unsigned long max_pages)\n-{\n-\tstruct swap_cgroup *map;\n-\tstruct swap_cgroup_ctrl *ctrl;\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn 0;\n-\n-\tBUILD_BUG_ON(sizeof(unsigned short) * ID_PER_SC !=\n-\t\t     sizeof(struct swap_cgroup));\n-\tmap = vzalloc(DIV_ROUND_UP(max_pages, ID_PER_SC) *\n-\t\t      sizeof(struct swap_cgroup));\n-\tif (!map)\n-\t\tgoto nomem;\n-\n-\tctrl = &swap_cgroup_ctrl[type];\n-\tmutex_lock(&swap_cgroup_mutex);\n-\tctrl->map = map;\n-\tmutex_unlock(&swap_cgroup_mutex);\n-\n-\treturn 0;\n-nomem:\n-\tpr_info(\"couldn't allocate enough memory for swap_cgroup\\n\");\n-\tpr_info(\"swap_cgroup can be disabled by swapaccount=0 boot option\\n\");\n-\treturn -ENOMEM;\n-}\n-\n-void swap_cgroup_swapoff(int type)\n-{\n-\tstruct swap_cgroup *map;\n-\tstruct swap_cgroup_ctrl *ctrl;\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn;\n-\n-\tmutex_lock(&swap_cgroup_mutex);\n-\tctrl = &swap_cgroup_ctrl[type];\n-\tmap = ctrl->map;\n-\tctrl->map = NULL;\n-\tmutex_unlock(&swap_cgroup_mutex);\n-\n-\tvfree(map);\n-}\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 68ec5d9f05848..345877786e432 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -2931,8 +2931,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tvfree(swap_map);\n \tkvfree(zeromap);\n \tfree_cluster_info(cluster_info, maxpages);\n-\t/* Destroy swap account information */\n-\tswap_cgroup_swapoff(p->type);\n \n \tinode = mapping->host;\n \n@@ -3497,10 +3495,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \t\tgoto bad_swap_unlock_inode;\n \t}\n \n-\terror = swap_cgroup_swapon(si->type, maxpages);\n-\tif (error)\n-\t\tgoto bad_swap_unlock_inode;\n-\n \terror = setup_swap_map(si, swap_header, swap_map, maxpages);\n \tif (error)\n \t\tgoto bad_swap_unlock_inode;\n@@ -3605,7 +3599,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tsi->global_cluster = NULL;\n \tinode = NULL;\n \tdestroy_swap_extents(si);\n-\tswap_cgroup_swapoff(si->type);\n \tspin_lock(&swap_lock);\n \tsi->swap_file = NULL;\n \tsi->flags = 0;\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex 9bb733f00fd21..64747493ca9f7 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -41,6 +41,7 @@\n  * @zswap_entry: The zswap entry associated with this swap slot.\n  * @swap_cache: The folio in swap cache.\n  * @shadow: The shadow entry.\n+ * @memcgid: The memcg id of the owning memcg, if any.\n  */\n struct swp_desc {\n \tswp_slot_t slot;\n@@ -49,6 +50,9 @@ struct swp_desc {\n \t\tstruct folio *swap_cache;\n \t\tvoid *shadow;\n \t};\n+#ifdef CONFIG_MEMCG\n+\tunsigned short memcgid;\n+#endif\n };\n \n #define VSWAP_CLUSTER_SHIFT HPAGE_PMD_ORDER\n@@ -242,6 +246,9 @@ static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster, int start)\n \t\tdesc = &cluster->descriptors[start + i];\n \t\tdesc->slot.val = 0;\n \t\tdesc->zswap_entry = NULL;\n+#ifdef CONFIG_MEMCG\n+\t\tdesc->memcgid = 0;\n+#endif\n \t}\n \tcluster->count += nr;\n }\n@@ -1109,6 +1116,94 @@ bool zswap_empty(swp_entry_t swpentry)\n }\n #endif /* CONFIG_ZSWAP */\n \n+#ifdef CONFIG_MEMCG\n+static unsigned short vswap_cgroup_record(swp_entry_t entry,\n+\t\t\t\tunsigned short memcgid, unsigned int nr_ents)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tunsigned short oldid, iter = 0;\n+\tint i;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr_ents; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\t\toldid = desc->memcgid;\n+\t\tdesc->memcgid = memcgid;\n+\t\tif (!iter)\n+\t\t\titer = oldid;\n+\t\tVM_WARN_ON(iter != oldid);\n+\t}\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn oldid;\n+}\n+\n+/**\n+ * swap_cgroup_record - record mem_cgroup for a set of swap entries.\n+ * These entries must belong to one single folio, and that folio\n+ * must be being charged for swap space (swap out), and these\n+ * entries must not have been charged\n+ *\n+ * @folio: the folio that the swap entry belongs to\n+ * @memcgid: mem_cgroup ID to be recorded\n+ * @entry: the first swap entry to be recorded\n+ */\n+void swap_cgroup_record(struct folio *folio, unsigned short memcgid,\n+\t\t\tswp_entry_t entry)\n+{\n+\tunsigned short oldid =\n+\t\tvswap_cgroup_record(entry, memcgid, folio_nr_pages(folio));\n+\n+\tVM_WARN_ON(oldid);\n+}\n+\n+/**\n+ * swap_cgroup_clear - clear mem_cgroup for a set of swap entries.\n+ * These entries must be being uncharged from swap. They either\n+ * belongs to one single folio in the swap cache (swap in for\n+ * cgroup v1), or no longer have any users (slot freeing).\n+ *\n+ * @entry: the first swap entry to be recorded into\n+ * @nr_ents: number of swap entries to be recorded\n+ *\n+ * Returns the existing old value.\n+ */\n+unsigned short swap_cgroup_clear(swp_entry_t entry, unsigned int nr_ents)\n+{\n+\treturn vswap_cgroup_record(entry, 0, nr_ents);\n+}\n+\n+/**\n+ * lookup_swap_cgroup_id - lookup mem_cgroup id tied to swap entry\n+ * @entry: swap entry to be looked up.\n+ *\n+ * Returns ID of mem_cgroup at success. 0 at failure. (0 is invalid ID)\n+ */\n+unsigned short lookup_swap_cgroup_id(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tunsigned short ret;\n+\n+\t/*\n+\t * Note that the virtual swap slot can be freed under us, for instance in\n+\t * the invocation of mem_cgroup_swapin_charge_folio. We need to wrap the\n+\t * entire lookup in RCU read-side critical section, and double check the\n+\t * existence of the swap descriptor.\n+\t */\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tret = desc ? desc->memcgid : 0;\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\treturn ret;\n+}\n+#endif /* CONFIG_MEMCG */\n+\n int vswap_init(void)\n {\n \tint i;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the swap entry lifecycle management being split between mm/swapfile.c and mm/vswap.c, explaining that they have re-implemented all of the swap entry lifecycle API in the virtual swap layer.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch moves the swap entry lifecycle management to the virtual swap\nlayer by adding to the swap descriptor two fields:\n\n1. in_swapcache, i.e whether the swap entry is in swap cache (or about\n   to be added).\n2. The swap count of the swap entry, which counts the number of page\n   table entries at which the swap entry is inserted.\n\nand re-implementing all of the swap entry lifecycle API\n(swap_duplicate(), swap_free_nr(), swapcache_prepare(), etc.) in the\nvirtual swap layer.\n\nFor now, we do not implement swap count continuation - the swap_count\nfield in the swap descriptor is big enough to hold the maximum number of\nswap counts. This vastly simplifies the logic.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h  |  29 +-\n include/linux/zswap.h |   5 +-\n mm/memory.c           |   8 +-\n mm/shmem.c            |   4 +-\n mm/swap.h             |  58 ++--\n mm/swap_state.c       |   4 +-\n mm/swapfile.c         | 786 ++----------------------------------------\n mm/vswap.c            | 452 ++++++++++++++++++++++--\n mm/zswap.c            |  14 +-\n 9 files changed, 502 insertions(+), 858 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 0410a00fd353c..aae2e502d9975 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -223,17 +223,9 @@ enum {\n #define SWAP_CLUSTER_MAX_SKIPPED (SWAP_CLUSTER_MAX << 10)\n #define COMPACT_CLUSTER_MAX SWAP_CLUSTER_MAX\n \n-/* Bit flag in swap_map */\n-#define SWAP_HAS_CACHE\t0x40\t/* Flag page is cached, in first swap_map */\n-#define COUNT_CONTINUED\t0x80\t/* Flag swap_map continuation for full count */\n-\n-/* Special value in first swap_map */\n-#define SWAP_MAP_MAX\t0x3e\t/* Max count */\n-#define SWAP_MAP_BAD\t0x3f\t/* Note page is bad */\n-#define SWAP_MAP_SHMEM\t0xbf\t/* Owned by shmem/tmpfs */\n-\n-/* Special value in each swap_map continuation */\n-#define SWAP_CONT_MAX\t0x7f\t/* Max count */\n+/* Swapfile's swap map state*/\n+#define SWAP_MAP_ALLOCATED\t0x01\t/* Page is allocated */\n+#define SWAP_MAP_BAD\t0x02\t/* Page is bad */\n \n /*\n  * The first page in the swap file is the swap header, which is always marked\n@@ -423,7 +415,7 @@ extern void __meminit kswapd_stop(int nid);\n \n #ifdef CONFIG_SWAP\n \n-/* Lifecycle swap API (mm/swapfile.c) */\n+/* Lifecycle swap API (mm/swapfile.c and mm/vswap.c) */\n int folio_alloc_swap(struct folio *folio);\n bool folio_free_swap(struct folio *folio);\n void put_swap_folio(struct folio *folio, swp_entry_t entry);\n@@ -433,7 +425,7 @@ int swapcache_prepare(swp_entry_t entry, int nr);\n void swap_free_nr(swp_entry_t entry, int nr_pages);\n void free_swap_and_cache_nr(swp_entry_t entry, int nr);\n int __swap_count(swp_entry_t entry);\n-bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry);\n+bool swap_entry_swapped(swp_entry_t entry);\n int swp_swapcount(swp_entry_t entry);\n bool is_swap_cached(swp_entry_t entry);\n \n@@ -473,7 +465,6 @@ static inline long get_nr_swap_pages(void)\n void si_swapinfo(struct sysinfo *);\n int swap_slot_alloc(swp_slot_t *slot, unsigned int order);\n swp_slot_t swap_slot_alloc_of_type(int);\n-int add_swap_count_continuation(swp_entry_t, gfp_t);\n int swap_type_of(dev_t device, sector_t offset);\n int find_first_swap(dev_t *device);\n unsigned int count_swap_pages(int, int);\n@@ -517,11 +508,6 @@ static inline void free_swap_cache(struct folio *folio)\n {\n }\n \n-static inline int add_swap_count_continuation(swp_entry_t swp, gfp_t gfp_mask)\n-{\n-\treturn 0;\n-}\n-\n static inline void swap_shmem_alloc(swp_entry_t swp, int nr)\n {\n }\n@@ -549,7 +535,7 @@ static inline int __swap_count(swp_entry_t entry)\n \treturn 0;\n }\n \n-static inline bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry)\n+static inline bool swap_entry_swapped(swp_entry_t entry)\n {\n \treturn false;\n }\n@@ -672,11 +658,12 @@ static inline bool mem_cgroup_swap_full(struct folio *folio)\n \n int vswap_init(void);\n void vswap_exit(void);\n-void vswap_free(swp_entry_t entry, struct swap_cluster_info *ci);\n swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry);\n swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot);\n bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si);\n void put_swap_entry(swp_entry_t entry, struct swap_info_struct *si);\n+bool folio_swapped(struct folio *folio);\n+bool vswap_only_has_cache(swp_entry_t entry, int nr);\n \n #endif /* __KERNEL__*/\n #endif /* _LINUX_SWAP_H */\ndiff --git a/include/linux/zswap.h b/include/linux/zswap.h\nindex 7eb3ce7e124fc..07b2936c38f29 100644\n--- a/include/linux/zswap.h\n+++ b/include/linux/zswap.h\n@@ -28,7 +28,6 @@ struct zswap_lruvec_state {\n unsigned long zswap_total_pages(void);\n bool zswap_store(struct folio *folio);\n int zswap_load(struct folio *folio);\n-void zswap_invalidate(swp_entry_t swp);\n void zswap_memcg_offline_cleanup(struct mem_cgroup *memcg);\n void zswap_lruvec_state_init(struct lruvec *lruvec);\n void zswap_folio_swapin(struct folio *folio);\n@@ -38,6 +37,7 @@ void *zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry);\n void *zswap_entry_load(swp_entry_t swpentry);\n void *zswap_entry_erase(swp_entry_t swpentry);\n bool zswap_empty(swp_entry_t swpentry);\n+void zswap_entry_free(struct zswap_entry *entry);\n \n #else\n \n@@ -53,7 +53,6 @@ static inline int zswap_load(struct folio *folio)\n \treturn -ENOENT;\n }\n \n-static inline void zswap_invalidate(swp_entry_t swp) {}\n static inline void zswap_memcg_offline_cleanup(struct mem_cgroup *memcg) {}\n static inline void zswap_lruvec_state_init(struct lruvec *lruvec) {}\n static inline void zswap_folio_swapin(struct folio *folio) {}\n@@ -68,6 +67,8 @@ static inline bool zswap_never_enabled(void)\n \treturn true;\n }\n \n+static inline void zswap_entry_free(struct zswap_entry *entry) {}\n+\n #endif\n \n #endif /* _LINUX_ZSWAP_H */\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 90031f833f52e..641e3f65edc00 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -1333,10 +1333,6 @@ copy_pte_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n \n \tif (ret == -EIO) {\n \t\tVM_WARN_ON_ONCE(!entry.val);\n-\t\tif (add_swap_count_continuation(entry, GFP_KERNEL) < 0) {\n-\t\t\tret = -ENOMEM;\n-\t\t\tgoto out;\n-\t\t}\n \t\tentry.val = 0;\n \t} else if (ret == -EBUSY || unlikely(ret == -EHWPOISON)) {\n \t\tgoto out;\n@@ -5044,7 +5040,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n out:\n \t/* Clear the swap cache pin for direct swapin after PTL unlock */\n \tif (need_clear_cache) {\n-\t\tswapcache_clear(si, entry, nr_pages);\n+\t\tswapcache_clear(entry, nr_pages);\n \t\tif (waitqueue_active(&swapcache_wq))\n \t\t\twake_up(&swapcache_wq);\n \t}\n@@ -5063,7 +5059,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\tfolio_put(swapcache);\n \t}\n \tif (need_clear_cache) {\n-\t\tswapcache_clear(si, entry, nr_pages);\n+\t\tswapcache_clear(entry, nr_pages);\n \t\tif (waitqueue_active(&swapcache_wq))\n \t\t\twake_up(&swapcache_wq);\n \t}\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 66cf8af6779ca..780571c830e5b 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -2442,7 +2442,7 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \n \tif (skip_swapcache) {\n \t\tfolio->swap.val = 0;\n-\t\tswapcache_clear(si, swap, nr_pages);\n+\t\tswapcache_clear(swap, nr_pages);\n \t} else {\n \t\tswap_cache_del_folio(folio);\n \t}\n@@ -2463,7 +2463,7 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \t\tfolio_unlock(folio);\n failed_nolock:\n \tif (skip_swapcache)\n-\t\tswapcache_clear(si, folio->swap, folio_nr_pages(folio));\n+\t\tswapcache_clear(folio->swap, folio_nr_pages(folio));\n \tif (folio)\n \t\tfolio_put(folio);\n \tput_swap_entry(swap, si);\ndiff --git a/mm/swap.h b/mm/swap.h\nindex 57ed24a2d6356..ae97cf9712c5c 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -211,6 +211,8 @@ void swap_cache_lock_irq(swp_entry_t entry);\n void swap_cache_unlock_irq(swp_entry_t entry);\n void swap_cache_lock(swp_entry_t entry);\n void swap_cache_unlock(swp_entry_t entry);\n+void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n+\t\t\t   unsigned long vswap, int nr);\n \n static inline struct address_space *swap_address_space(swp_entry_t entry)\n {\n@@ -245,6 +247,31 @@ static inline bool folio_matches_swap_entry(const struct folio *folio,\n \treturn folio_entry.val == round_down(entry.val, nr_pages);\n }\n \n+/**\n+ * folio_matches_swap_slot - Check if a folio matches both the virtual\n+ *                           swap entry and its backing physical swap slot.\n+ * @folio: The folio.\n+ * @entry: The virtual swap entry to check against.\n+ * @slot: The physical swap slot to check against.\n+ *\n+ * Context: The caller should have the folio locked to ensure it's stable\n+ * and nothing will move it in or out of the swap cache.\n+ * Return: true if both checks pass, false otherwise.\n+ */\n+static inline bool folio_matches_swap_slot(const struct folio *folio,\n+\t\t\t\t\t   swp_entry_t entry,\n+\t\t\t\t\t   swp_slot_t slot)\n+{\n+\tif (!folio_matches_swap_entry(folio, entry))\n+\t\treturn false;\n+\n+\t/*\n+\t * Confirm the virtual swap entry is still backed by the same\n+\t * physical swap slot.\n+\t */\n+\treturn slot.val == swp_entry_to_swp_slot(entry).val;\n+}\n+\n /*\n  * All swap cache helpers below require the caller to ensure the swap entries\n  * used are valid and stablize the device by any of the following ways:\n@@ -265,7 +292,7 @@ void __swap_cache_del_folio(struct folio *folio, swp_entry_t entry, void *shadow\n void __swap_cache_replace_folio(struct folio *old, struct folio *new);\n \n void show_swap_cache_info(void);\n-void swapcache_clear(struct swap_info_struct *si, swp_entry_t entry, int nr);\n+void swapcache_clear(swp_entry_t entry, int nr);\n struct folio *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\tstruct vm_area_struct *vma, unsigned long addr,\n \t\tstruct swap_iocb **plug);\n@@ -312,25 +339,7 @@ static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n \t\treturn find_next_bit(sis->zeromap, end, start) - start;\n }\n \n-static inline int non_swapcache_batch(swp_entry_t entry, int max_nr)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si = __swap_slot_to_info(slot);\n-\tpgoff_t offset = swp_slot_offset(slot);\n-\tint i;\n-\n-\t/*\n-\t * While allocating a large folio and doing mTHP swapin, we need to\n-\t * ensure all entries are not cached, otherwise, the mTHP folio will\n-\t * be in conflict with the folio in swap cache.\n-\t */\n-\tfor (i = 0; i < max_nr; i++) {\n-\t\tif ((si->swap_map[offset + i] & SWAP_HAS_CACHE))\n-\t\t\treturn i;\n-\t}\n-\n-\treturn i;\n-}\n+int non_swapcache_batch(swp_entry_t entry, int max_nr);\n \n #else /* CONFIG_SWAP */\n struct swap_iocb;\n@@ -382,6 +391,13 @@ static inline bool folio_matches_swap_entry(const struct folio *folio, swp_entry\n \treturn false;\n }\n \n+static inline bool folio_matches_swap_slot(const struct folio *folio,\n+\t\t\t\t\t   swp_entry_t entry,\n+\t\t\t\t\t   swp_slot_t slot)\n+{\n+\treturn false;\n+}\n+\n static inline void show_swap_cache_info(void)\n {\n }\n@@ -409,7 +425,7 @@ static inline int swap_writeout(struct folio *folio,\n \treturn 0;\n }\n \n-static inline void swapcache_clear(struct swap_info_struct *si, swp_entry_t entry, int nr)\n+static inline void swapcache_clear(swp_entry_t entry, int nr)\n {\n }\n \ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 00fa3e76a5c19..1827527e88d33 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -174,8 +174,6 @@ struct folio *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\tstruct mempolicy *mpol, pgoff_t ilx, bool *new_page_allocated,\n \t\tbool skip_if_exists)\n {\n-\tstruct swap_info_struct *si =\n-\t\t__swap_slot_to_info(swp_entry_to_swp_slot(entry));\n \tstruct folio *folio;\n \tstruct folio *new_folio = NULL;\n \tstruct folio *result = NULL;\n@@ -196,7 +194,7 @@ struct folio *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\t/*\n \t\t * Just skip read ahead for unused swap slot.\n \t\t */\n-\t\tif (!swap_entry_swapped(si, entry))\n+\t\tif (!swap_entry_swapped(entry))\n \t\t\tgoto put_and_return;\n \n \t\t/*\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 345877786e432..6c5e46bf40701 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -50,9 +50,6 @@\n #include \"internal.h\"\n #include \"swap.h\"\n \n-static bool swap_count_continued(struct swap_info_struct *, pgoff_t,\n-\t\t\t\t unsigned char);\n-static void free_swap_count_continuations(struct swap_info_struct *);\n static void swap_slots_free(struct swap_info_struct *si,\n \t\t\t      struct swap_cluster_info *ci,\n \t\t\t      swp_slot_t slot, unsigned int nr_pages);\n@@ -146,7 +143,7 @@ static struct swap_info_struct *swap_slot_to_info(swp_slot_t slot)\n \n static inline unsigned char swap_count(unsigned char ent)\n {\n-\treturn ent & ~SWAP_HAS_CACHE;\t/* may include COUNT_CONTINUED flag */\n+\treturn ent;\n }\n \n /*\n@@ -182,52 +179,14 @@ static long swap_usage_in_pages(struct swap_info_struct *si)\n static bool swap_only_has_cache(struct swap_info_struct *si,\n \t\t\t      unsigned long offset, int nr_pages)\n {\n-\tunsigned char *map = si->swap_map + offset;\n-\tunsigned char *map_end = map + nr_pages;\n-\n-\tdo {\n-\t\tVM_BUG_ON(!(*map & SWAP_HAS_CACHE));\n-\t\tif (*map != SWAP_HAS_CACHE)\n-\t\t\treturn false;\n-\t} while (++map < map_end);\n+\tswp_entry_t entry = swp_slot_to_swp_entry(swp_slot(si->type, offset));\n \n-\treturn true;\n+\treturn vswap_only_has_cache(entry, nr_pages);\n }\n \n-/**\n- * is_swap_cached - check if the swap entry is cached\n- * @entry: swap entry to check\n- *\n- * Check swap_map directly to minimize overhead, READ_ONCE is sufficient.\n- *\n- * Returns true if the swap entry is cached, false otherwise.\n- */\n-bool is_swap_cached(swp_entry_t entry)\n+static bool swap_cache_only(struct swap_info_struct *si, unsigned long offset)\n {\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si = swap_slot_to_info(slot);\n-\tunsigned long offset = swp_slot_offset(slot);\n-\n-\treturn READ_ONCE(si->swap_map[offset]) & SWAP_HAS_CACHE;\n-}\n-\n-static bool swap_is_last_map(struct swap_info_struct *si,\n-\t\tunsigned long offset, int nr_pages, bool *has_cache)\n-{\n-\tunsigned char *map = si->swap_map + offset;\n-\tunsigned char *map_end = map + nr_pages;\n-\tunsigned char count = *map;\n-\n-\tif (swap_count(count) != 1 && swap_count(count) != SWAP_MAP_SHMEM)\n-\t\treturn false;\n-\n-\twhile (++map < map_end) {\n-\t\tif (*map != count)\n-\t\t\treturn false;\n-\t}\n-\n-\t*has_cache = !!(count & SWAP_HAS_CACHE);\n-\treturn true;\n+\treturn swap_only_has_cache(si, offset, 1);\n }\n \n /*\n@@ -238,15 +197,15 @@ static bool swap_is_last_map(struct swap_info_struct *si,\n static int __try_to_reclaim_swap(struct swap_info_struct *si,\n \t\t\t\t unsigned long offset, unsigned long flags)\n {\n-\tconst swp_entry_t entry =\n-\t\tswp_slot_to_swp_entry(swp_slot(si->type, offset));\n-\tswp_slot_t slot;\n+\tconst swp_slot_t slot = swp_slot(si->type, offset);\n+\tswp_entry_t entry;\n \tstruct swap_cluster_info *ci;\n \tstruct folio *folio;\n \tint ret, nr_pages;\n \tbool need_reclaim;\n \n again:\n+\tentry = swp_slot_to_swp_entry(slot);\n \tfolio = swap_cache_get_folio(entry);\n \tif (!folio)\n \t\treturn 0;\n@@ -266,14 +225,15 @@ static int __try_to_reclaim_swap(struct swap_info_struct *si,\n \t/*\n \t * Offset could point to the middle of a large folio, or folio\n \t * may no longer point to the expected offset before it's locked.\n+\t * Additionally, the virtual swap entry may no longer be backed\n+\t * by the same physical swap slot.\n \t */\n-\tif (!folio_matches_swap_entry(folio, entry)) {\n+\tif (!folio_matches_swap_slot(folio, entry, slot)) {\n \t\tfolio_unlock(folio);\n \t\tfolio_put(folio);\n \t\tgoto again;\n \t}\n-\tslot = swp_entry_to_swp_slot(folio->swap);\n-\toffset = swp_slot_offset(slot);\n+\toffset = swp_slot_offset(swp_entry_to_swp_slot(folio->swap));\n \n \tneed_reclaim = ((flags & TTRS_ANYWAY) ||\n \t\t\t((flags & TTRS_UNMAPPED) && !folio_mapped(folio)) ||\n@@ -283,8 +243,7 @@ static int __try_to_reclaim_swap(struct swap_info_struct *si,\n \n \t/*\n \t * It's safe to delete the folio from swap cache only if the folio's\n-\t * swap_map is HAS_CACHE only, which means the slots have no page table\n-\t * reference or pending writeback, and can't be allocated to others.\n+\t * swap slots have no page table reference or pending writeback.\n \t */\n \tci = swap_cluster_lock(si, offset);\n \tneed_reclaim = swap_only_has_cache(si, offset, nr_pages);\n@@ -811,7 +770,7 @@ static bool cluster_reclaim_range(struct swap_info_struct *si,\n \t\tcase 0:\n \t\t\toffset++;\n \t\t\tbreak;\n-\t\tcase SWAP_HAS_CACHE:\n+\t\tcase SWAP_MAP_ALLOCATED:\n \t\t\tnr_reclaim = __try_to_reclaim_swap(si, offset, TTRS_ANYWAY);\n \t\t\tif (nr_reclaim > 0)\n \t\t\t\toffset += nr_reclaim;\n@@ -842,22 +801,23 @@ static bool cluster_scan_range(struct swap_info_struct *si,\n {\n \tunsigned long offset, end = start + nr_pages;\n \tunsigned char *map = si->swap_map;\n+\tunsigned char count;\n \n \tif (cluster_is_empty(ci))\n \t\treturn true;\n \n \tfor (offset = start; offset < end; offset++) {\n-\t\tswitch (READ_ONCE(map[offset])) {\n-\t\tcase 0:\n+\t\tcount = READ_ONCE(map[offset]);\n+\t\tif (!count)\n \t\t\tcontinue;\n-\t\tcase SWAP_HAS_CACHE:\n+\n+\t\tif (swap_cache_only(si, offset)) {\n \t\t\tif (!vm_swap_full())\n \t\t\t\treturn false;\n \t\t\t*need_reclaim = true;\n \t\t\tcontinue;\n-\t\tdefault:\n-\t\t\treturn false;\n \t\t}\n+\t\treturn false;\n \t}\n \n \treturn true;\n@@ -974,7 +934,6 @@ static void swap_reclaim_full_clusters(struct swap_info_struct *si, bool force)\n \tlong to_scan = 1;\n \tunsigned long offset, end;\n \tstruct swap_cluster_info *ci;\n-\tunsigned char *map = si->swap_map;\n \tint nr_reclaim;\n \n \tif (force)\n@@ -986,7 +945,7 @@ static void swap_reclaim_full_clusters(struct swap_info_struct *si, bool force)\n \t\tto_scan--;\n \n \t\twhile (offset < end) {\n-\t\t\tif (READ_ONCE(map[offset]) == SWAP_HAS_CACHE) {\n+\t\t\tif (swap_cache_only(si, offset)) {\n \t\t\t\tspin_unlock(&ci->lock);\n \t\t\t\tnr_reclaim = __try_to_reclaim_swap(si, offset,\n \t\t\t\t\t\t\t\t   TTRS_ANYWAY);\n@@ -1320,7 +1279,8 @@ static bool swap_alloc_fast(swp_slot_t *slot, int order)\n \tif (cluster_is_usable(ci, order)) {\n \t\tif (cluster_is_empty(ci))\n \t\t\toffset = cluster_offset(si, ci);\n-\t\tfound = alloc_swap_scan_cluster(si, ci, offset, order, SWAP_HAS_CACHE);\n+\t\tfound = alloc_swap_scan_cluster(si, ci, offset, order,\n+\t\t\tSWAP_MAP_ALLOCATED);\n \t\tif (found)\n \t\t\t*slot = swp_slot(si->type, found);\n \t} else {\n@@ -1344,7 +1304,7 @@ static void swap_alloc_slow(swp_slot_t *slot, int order)\n \t\tplist_requeue(&si->avail_list, &swap_avail_head);\n \t\tspin_unlock(&swap_avail_lock);\n \t\tif (get_swap_device_info(si)) {\n-\t\t\toffset = cluster_alloc_swap_slot(si, order, SWAP_HAS_CACHE);\n+\t\t\toffset = cluster_alloc_swap_slot(si, order, SWAP_MAP_ALLOCATED);\n \t\t\tswap_slot_put_swap_info(si);\n \t\t\tif (offset) {\n \t\t\t\t*slot = swp_slot(si->type, offset);\n@@ -1471,48 +1431,6 @@ static struct swap_info_struct *_swap_info_get(swp_slot_t slot)\n \treturn NULL;\n }\n \n-static unsigned char swap_slot_put_locked(struct swap_info_struct *si,\n-\t\t\t\t\t   struct swap_cluster_info *ci,\n-\t\t\t\t\t   swp_slot_t slot,\n-\t\t\t\t\t   unsigned char usage)\n-{\n-\tunsigned long offset = swp_slot_offset(slot);\n-\tunsigned char count;\n-\tunsigned char has_cache;\n-\n-\tcount = si->swap_map[offset];\n-\n-\thas_cache = count & SWAP_HAS_CACHE;\n-\tcount &= ~SWAP_HAS_CACHE;\n-\n-\tif (usage == SWAP_HAS_CACHE) {\n-\t\tVM_BUG_ON(!has_cache);\n-\t\thas_cache = 0;\n-\t} else if (count == SWAP_MAP_SHMEM) {\n-\t\t/*\n-\t\t * Or we could insist on shmem.c using a special\n-\t\t * swap_shmem_free() and free_shmem_swap_and_cache()...\n-\t\t */\n-\t\tcount = 0;\n-\t} else if ((count & ~COUNT_CONTINUED) <= SWAP_MAP_MAX) {\n-\t\tif (count == COUNT_CONTINUED) {\n-\t\t\tif (swap_count_continued(si, offset, count))\n-\t\t\t\tcount = SWAP_MAP_MAX | COUNT_CONTINUED;\n-\t\t\telse\n-\t\t\t\tcount = SWAP_MAP_MAX;\n-\t\t} else\n-\t\t\tcount--;\n-\t}\n-\n-\tusage = count | has_cache;\n-\tif (usage)\n-\t\tWRITE_ONCE(si->swap_map[offset], usage);\n-\telse\n-\t\tswap_slots_free(si, ci, slot, 1);\n-\n-\treturn usage;\n-}\n-\n /*\n  * When we get a swap entry, if there aren't some other ways to\n  * prevent swapoff, such as the folio in swap cache is locked, RCU\n@@ -1580,94 +1498,23 @@ struct swap_info_struct *swap_slot_tryget_swap_info(swp_slot_t slot)\n \treturn NULL;\n }\n \n-static void swap_slots_put_cache(struct swap_info_struct *si,\n-\t\t\t\t   swp_slot_t slot, int nr)\n-{\n-\tunsigned long offset = swp_slot_offset(slot);\n-\tstruct swap_cluster_info *ci;\n-\n-\tci = swap_cluster_lock(si, offset);\n-\tif (swap_only_has_cache(si, offset, nr)) {\n-\t\tswap_slots_free(si, ci, slot, nr);\n-\t} else {\n-\t\tfor (int i = 0; i < nr; i++, slot.val++)\n-\t\t\tswap_slot_put_locked(si, ci, slot, SWAP_HAS_CACHE);\n-\t}\n-\tswap_cluster_unlock(ci);\n-}\n-\n static bool swap_slots_put_map(struct swap_info_struct *si,\n \t\t\t\t swp_slot_t slot, int nr)\n {\n \tunsigned long offset = swp_slot_offset(slot);\n \tstruct swap_cluster_info *ci;\n-\tbool has_cache = false;\n-\tunsigned char count;\n-\tint i;\n-\n-\tif (nr <= 1)\n-\t\tgoto fallback;\n-\tcount = swap_count(data_race(si->swap_map[offset]));\n-\tif (count != 1 && count != SWAP_MAP_SHMEM)\n-\t\tgoto fallback;\n \n \tci = swap_cluster_lock(si, offset);\n-\tif (!swap_is_last_map(si, offset, nr, &has_cache)) {\n-\t\tgoto locked_fallback;\n-\t}\n-\tif (!has_cache)\n-\t\tswap_slots_free(si, ci, slot, nr);\n-\telse\n-\t\tfor (i = 0; i < nr; i++)\n-\t\t\tWRITE_ONCE(si->swap_map[offset + i], SWAP_HAS_CACHE);\n+\tvswap_rmap_set(ci, slot, 0, nr);\n+\tswap_slots_free(si, ci, slot, nr);\n \tswap_cluster_unlock(ci);\n \n-\treturn has_cache;\n-\n-fallback:\n-\tci = swap_cluster_lock(si, offset);\n-locked_fallback:\n-\tfor (i = 0; i < nr; i++, slot.val++) {\n-\t\tcount = swap_slot_put_locked(si, ci, slot, 1);\n-\t\tif (count == SWAP_HAS_CACHE)\n-\t\t\thas_cache = true;\n-\t}\n-\tswap_cluster_unlock(ci);\n-\treturn has_cache;\n-}\n-\n-/*\n- * Only functions with \"_nr\" suffix are able to free entries spanning\n- * cross multi clusters, so ensure the range is within a single cluster\n- * when freeing entries with functions without \"_nr\" suffix.\n- */\n-static bool swap_slots_put_map_nr(struct swap_info_struct *si,\n-\t\t\t\t    swp_slot_t slot, int nr)\n-{\n-\tint cluster_nr, cluster_rest;\n-\tunsigned long offset = swp_slot_offset(slot);\n-\tbool has_cache = false;\n-\n-\tcluster_rest = SWAPFILE_CLUSTER - offset % SWAPFILE_CLUSTER;\n-\twhile (nr) {\n-\t\tcluster_nr = min(nr, cluster_rest);\n-\t\thas_cache |= swap_slots_put_map(si, slot, cluster_nr);\n-\t\tcluster_rest = SWAPFILE_CLUSTER;\n-\t\tnr -= cluster_nr;\n-\t\tslot.val += cluster_nr;\n-\t}\n-\n-\treturn has_cache;\n+\treturn true;\n }\n \n-/*\n- * Check if it's the last ref of swap entry in the freeing path.\n- * Qualified value includes 1, SWAP_HAS_CACHE or SWAP_MAP_SHMEM.\n- */\n static inline bool __maybe_unused swap_is_last_ref(unsigned char count)\n {\n-\treturn (count == SWAP_HAS_CACHE) || (count == 1) ||\n-\t       (count == SWAP_MAP_SHMEM);\n+\treturn count == SWAP_MAP_ALLOCATED;\n }\n \n /*\n@@ -1681,14 +1528,6 @@ static void swap_slots_free(struct swap_info_struct *si,\n \tunsigned long offset = swp_slot_offset(slot);\n \tunsigned char *map = si->swap_map + offset;\n \tunsigned char *map_end = map + nr_pages;\n-\tswp_entry_t entry = swp_slot_to_swp_entry(slot);\n-\tint i;\n-\n-\t/* release all the associated (virtual) swap slots */\n-\tfor (i = 0; i < nr_pages; i++) {\n-\t\tvswap_free(entry, ci);\n-\t\tentry.val++;\n-\t}\n \n \t/* It should never free entries across different clusters */\n \tVM_BUG_ON(ci != __swap_offset_to_cluster(si, offset + nr_pages - 1));\n@@ -1731,149 +1570,6 @@ void swap_slot_free_nr(swp_slot_t slot, int nr_pages)\n \t}\n }\n \n-/*\n- * Caller has made sure that the swap device corresponding to entry\n- * is still around or has not been recycled.\n- */\n-void swap_free_nr(swp_entry_t entry, int nr_pages)\n-{\n-\tswap_slot_free_nr(swp_entry_to_swp_slot(entry), nr_pages);\n-}\n-\n-/*\n- * Called after dropping swapcache to decrease refcnt to swap entries.\n- */\n-void put_swap_folio(struct folio *folio, swp_entry_t entry)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si;\n-\tint size = 1 << swap_slot_order(folio_order(folio));\n-\n-\tsi = _swap_info_get(slot);\n-\tif (!si)\n-\t\treturn;\n-\n-\tswap_slots_put_cache(si, slot, size);\n-}\n-\n-int __swap_count(swp_entry_t entry)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si = __swap_slot_to_info(slot);\n-\tpgoff_t offset = swp_slot_offset(slot);\n-\n-\treturn swap_count(si->swap_map[offset]);\n-}\n-\n-/*\n- * How many references to @entry are currently swapped out?\n- * This does not give an exact answer when swap count is continued,\n- * but does include the high COUNT_CONTINUED flag to allow for that.\n- */\n-bool swap_entry_swapped(struct swap_info_struct *si, swp_entry_t entry)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tpgoff_t offset = swp_slot_offset(slot);\n-\tstruct swap_cluster_info *ci;\n-\tint count;\n-\n-\tci = swap_cluster_lock(si, offset);\n-\tcount = swap_count(si->swap_map[offset]);\n-\tswap_cluster_unlock(ci);\n-\treturn !!count;\n-}\n-\n-/*\n- * How many references to @entry are currently swapped out?\n- * This considers COUNT_CONTINUED so it returns exact answer.\n- */\n-int swp_swapcount(swp_entry_t entry)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tint count, tmp_count, n;\n-\tstruct swap_info_struct *si;\n-\tstruct swap_cluster_info *ci;\n-\tstruct page *page;\n-\tpgoff_t offset;\n-\tunsigned char *map;\n-\n-\tsi = _swap_info_get(slot);\n-\tif (!si)\n-\t\treturn 0;\n-\n-\toffset = swp_slot_offset(slot);\n-\n-\tci = swap_cluster_lock(si, offset);\n-\n-\tcount = swap_count(si->swap_map[offset]);\n-\tif (!(count & COUNT_CONTINUED))\n-\t\tgoto out;\n-\n-\tcount &= ~COUNT_CONTINUED;\n-\tn = SWAP_MAP_MAX + 1;\n-\n-\tpage = vmalloc_to_page(si->swap_map + offset);\n-\toffset &= ~PAGE_MASK;\n-\tVM_BUG_ON(page_private(page) != SWP_CONTINUED);\n-\n-\tdo {\n-\t\tpage = list_next_entry(page, lru);\n-\t\tmap = kmap_local_page(page);\n-\t\ttmp_count = map[offset];\n-\t\tkunmap_local(map);\n-\n-\t\tcount += (tmp_count & ~COUNT_CONTINUED) * n;\n-\t\tn *= (SWAP_CONT_MAX + 1);\n-\t} while (tmp_count & COUNT_CONTINUED);\n-out:\n-\tswap_cluster_unlock(ci);\n-\treturn count;\n-}\n-\n-static bool swap_page_trans_huge_swapped(struct swap_info_struct *si,\n-\t\t\t\t\t swp_entry_t entry, int order)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_cluster_info *ci;\n-\tunsigned char *map = si->swap_map;\n-\tunsigned int nr_pages = 1 << order;\n-\tunsigned long roffset = swp_slot_offset(slot);\n-\tunsigned long offset = round_down(roffset, nr_pages);\n-\tint i;\n-\tbool ret = false;\n-\n-\tci = swap_cluster_lock(si, offset);\n-\tif (nr_pages == 1) {\n-\t\tif (swap_count(map[roffset]))\n-\t\t\tret = true;\n-\t\tgoto unlock_out;\n-\t}\n-\tfor (i = 0; i < nr_pages; i++) {\n-\t\tif (swap_count(map[offset + i])) {\n-\t\t\tret = true;\n-\t\t\tbreak;\n-\t\t}\n-\t}\n-unlock_out:\n-\tswap_cluster_unlock(ci);\n-\treturn ret;\n-}\n-\n-static bool folio_swapped(struct folio *folio)\n-{\n-\tswp_entry_t entry = folio->swap;\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si = _swap_info_get(slot);\n-\n-\tif (!si)\n-\t\treturn false;\n-\n-\tif (!IS_ENABLED(CONFIG_THP_SWAP) || likely(!folio_test_large(folio)))\n-\t\treturn swap_entry_swapped(si, entry);\n-\n-\treturn swap_page_trans_huge_swapped(si, entry, folio_order(folio));\n-}\n-\n static bool folio_swapcache_freeable(struct folio *folio)\n {\n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n@@ -1925,72 +1621,6 @@ bool folio_free_swap(struct folio *folio)\n \treturn true;\n }\n \n-/**\n- * free_swap_and_cache_nr() - Release reference on range of swap entries and\n- *                            reclaim their cache if no more references remain.\n- * @entry: First entry of range.\n- * @nr: Number of entries in range.\n- *\n- * For each swap entry in the contiguous range, release a reference. If any swap\n- * entries become free, try to reclaim their underlying folios, if present. The\n- * offset range is defined by [entry.offset, entry.offset + nr).\n- */\n-void free_swap_and_cache_nr(swp_entry_t entry, int nr)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tconst unsigned long start_offset = swp_slot_offset(slot);\n-\tconst unsigned long end_offset = start_offset + nr;\n-\tstruct swap_info_struct *si;\n-\tbool any_only_cache = false;\n-\tunsigned long offset;\n-\n-\tsi = swap_slot_tryget_swap_info(slot);\n-\tif (!si)\n-\t\treturn;\n-\n-\tif (WARN_ON(end_offset > si->max))\n-\t\tgoto out;\n-\n-\t/*\n-\t * First free all entries in the range.\n-\t */\n-\tany_only_cache = swap_slots_put_map_nr(si, slot, nr);\n-\n-\t/*\n-\t * Short-circuit the below loop if none of the entries had their\n-\t * reference drop to zero.\n-\t */\n-\tif (!any_only_cache)\n-\t\tgoto out;\n-\n-\t/*\n-\t * Now go back over the range trying to reclaim the swap cache.\n-\t */\n-\tfor (offset = start_offset; offset < end_offset; offset += nr) {\n-\t\tnr = 1;\n-\t\tif (READ_ONCE(si->swap_map[offset]) == SWAP_HAS_CACHE) {\n-\t\t\t/*\n-\t\t\t * Folios are always naturally aligned in swap so\n-\t\t\t * advance forward to the next boundary. Zero means no\n-\t\t\t * folio was found for the swap entry, so advance by 1\n-\t\t\t * in this case. Negative value means folio was found\n-\t\t\t * but could not be reclaimed. Here we can still advance\n-\t\t\t * to the next boundary.\n-\t\t\t */\n-\t\t\tnr = __try_to_reclaim_swap(si, offset,\n-\t\t\t\t\t\t   TTRS_UNMAPPED | TTRS_FULL);\n-\t\t\tif (nr == 0)\n-\t\t\t\tnr = 1;\n-\t\t\telse if (nr < 0)\n-\t\t\t\tnr = -nr;\n-\t\t\tnr = ALIGN(offset + 1, nr) - offset;\n-\t\t}\n-\t}\n-\n-out:\n-\tswap_slot_put_swap_info(si);\n-}\n-\n #ifdef CONFIG_HIBERNATION\n \n swp_slot_t swap_slot_alloc_of_type(int type)\n@@ -2901,8 +2531,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tflush_percpu_swap_cluster(p);\n \n \tdestroy_swap_extents(p);\n-\tif (p->flags & SWP_CONTINUED)\n-\t\tfree_swap_count_continuations(p);\n \n \tif (!(p->flags & SWP_SOLIDSTATE))\n \t\tatomic_dec(&nr_rotate_swap);\n@@ -3638,364 +3266,6 @@ void si_swapinfo(struct sysinfo *val)\n \tspin_unlock(&swap_lock);\n }\n \n-/*\n- * Verify that nr swap entries are valid and increment their swap map counts.\n- *\n- * Returns error code in following case.\n- * - success -> 0\n- * - swp_entry is invalid -> EINVAL\n- * - swap-cache reference is requested but there is already one. -> EEXIST\n- * - swap-cache reference is requested but the entry is not used. -> ENOENT\n- * - swap-mapped reference requested but needs continued swap count. -> ENOMEM\n- */\n-static int __swap_duplicate(swp_entry_t entry, unsigned char usage, int nr)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *si;\n-\tstruct swap_cluster_info *ci;\n-\tunsigned long offset;\n-\tunsigned char count;\n-\tunsigned char has_cache;\n-\tint err, i;\n-\n-\tsi = swap_slot_to_info(slot);\n-\tif (WARN_ON_ONCE(!si)) {\n-\t\tpr_err(\"%s%08lx\\n\", Bad_file, entry.val);\n-\t\treturn -EINVAL;\n-\t}\n-\n-\toffset = swp_slot_offset(slot);\n-\tVM_WARN_ON(nr > SWAPFILE_CLUSTER - offset % SWAPFILE_CLUSTER);\n-\tVM_WARN_ON(usage == 1 && nr > 1);\n-\tci = swap_cluster_lock(si, offset);\n-\n-\terr = 0;\n-\tfor (i = 0; i < nr; i++) {\n-\t\tcount = si->swap_map[offset + i];\n-\n-\t\t/*\n-\t\t * swapin_readahead() doesn't check if a swap entry is valid, so the\n-\t\t * swap entry could be SWAP_MAP_BAD. Check here with lock held.\n-\t\t */\n-\t\tif (unlikely(swap_count(count) == SWAP_MAP_BAD)) {\n-\t\t\terr = -ENOENT;\n-\t\t\tgoto unlock_out;\n-\t\t}\n-\n-\t\thas_cache = count & SWAP_HAS_CACHE;\n-\t\tcount &= ~SWAP_HAS_CACHE;\n-\n-\t\tif (!count && !has_cache) {\n-\t\t\terr = -ENOENT;\n-\t\t} else if (usage == SWAP_HAS_CACHE) {\n-\t\t\tif (has_cache)\n-\t\t\t\terr = -EEXIST;\n-\t\t} else if ((count & ~COUNT_CONTINUED) > SWAP_MAP_MAX) {\n-\t\t\terr = -EINVAL;\n-\t\t}\n-\n-\t\tif (err)\n-\t\t\tgoto unlock_out;\n-\t}\n-\n-\tfor (i = 0; i < nr; i++) {\n-\t\tcount = si->swap_map[offset + i];\n-\t\thas_cache = count & SWAP_HAS_CACHE;\n-\t\tcount &= ~SWAP_HAS_CACHE;\n-\n-\t\tif (usage == SWAP_HAS_CACHE)\n-\t\t\thas_cache = SWAP_HAS_CACHE;\n-\t\telse if ((count & ~COUNT_CONTINUED) < SWAP_MAP_MAX)\n-\t\t\tcount += usage;\n-\t\telse if (swap_count_continued(si, offset + i, count))\n-\t\t\tcount = COUNT_CONTINUED;\n-\t\telse {\n-\t\t\t/*\n-\t\t\t * Don't need to rollback changes, because if\n-\t\t\t * usage == 1, there must be nr == 1.\n-\t\t\t */\n-\t\t\terr = -ENOMEM;\n-\t\t\tgoto unlock_out;\n-\t\t}\n-\n-\t\tWRITE_ONCE(si->swap_map[offset + i], count | has_cache);\n-\t}\n-\n-unlock_out:\n-\tswap_cluster_unlock(ci);\n-\treturn err;\n-}\n-\n-/*\n- * Help swapoff by noting that swap entry belongs to shmem/tmpfs\n- * (in which case its reference count is never incremented).\n- */\n-void swap_shmem_alloc(swp_entry_t entry, int nr)\n-{\n-\t__swap_duplicate(entry, SWAP_MAP_SHMEM, nr);\n-}\n-\n-/*\n- * Increase reference count of swap entry by 1.\n- * Returns 0 for success, or -ENOMEM if a swap_count_continuation is required\n- * but could not be atomically allocated.  Returns 0, just as if it succeeded,\n- * if __swap_duplicate() fails for another reason (-EINVAL or -ENOENT), which\n- * might occur if a page table entry has got corrupted.\n- */\n-int swap_duplicate(swp_entry_t entry)\n-{\n-\tint err = 0;\n-\n-\twhile (!err && __swap_duplicate(entry, 1, 1) == -ENOMEM)\n-\t\terr = add_swap_count_continuation(entry, GFP_ATOMIC);\n-\treturn err;\n-}\n-\n-/*\n- * @entry: first swap entry from which we allocate nr swap cache.\n- *\n- * Called when allocating swap cache for existing swap entries,\n- * This can return error codes. Returns 0 at success.\n- * -EEXIST means there is a swap cache.\n- * Note: return code is different from swap_duplicate().\n- */\n-int swapcache_prepare(swp_entry_t entry, int nr)\n-{\n-\treturn __swap_duplicate(entry, SWAP_HAS_CACHE, nr);\n-}\n-\n-/*\n- * Caller should ensure entries belong to the same folio so\n- * the entries won't span cross cluster boundary.\n- */\n-void swapcache_clear(struct swap_info_struct *si, swp_entry_t entry, int nr)\n-{\n-\tswap_slots_put_cache(si, swp_entry_to_swp_slot(entry), nr);\n-}\n-\n-/*\n- * add_swap_count_continuation - called when a swap count is duplicated\n- * beyond SWAP_MAP_MAX, it allocates a new page and links that to the entry's\n- * page of the original vmalloc'ed swap_map, to hold the continuation count\n- * (for that entry and for its neighbouring PAGE_SIZE swap entries).  Called\n- * again when count is duplicated beyond SWAP_MAP_MAX * SWAP_CONT_MAX, etc.\n- *\n- * These continuation pages are seldom referenced: the common paths all work\n- * on the original swap_map, only referring to a continuation page when the\n- * low \"digit\" of a count is incremented or decremented through SWAP_MAP_MAX.\n- *\n- * add_swap_count_continuation(, GFP_ATOMIC) can be called while holding\n- * page table locks; if it fails, add_swap_count_continuation(, GFP_KERNEL)\n- * can be called after dropping locks.\n- */\n-int add_swap_count_continuation(swp_entry_t entry, gfp_t gfp_mask)\n-{\n-\tstruct swap_info_struct *si;\n-\tstruct swap_cluster_info *ci;\n-\tstruct page *head;\n-\tstruct page *page;\n-\tstruct page *list_page;\n-\tpgoff_t offset;\n-\tunsigned char count;\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tint ret = 0;\n-\n-\t/*\n-\t * When debugging, it's easier to use __GFP_ZERO here; but it's better\n-\t * for latency not to zero a page while GFP_ATOMIC and holding locks.\n-\t */\n-\tpage = alloc_page(gfp_mask | __GFP_HIGHMEM);\n-\n-\tsi = swap_slot_tryget_swap_info(slot);\n-\tif (!si) {\n-\t\t/*\n-\t\t * An acceptable race has occurred since the failing\n-\t\t * __swap_duplicate(): the swap device may be swapoff\n-\t\t */\n-\t\tgoto outer;\n-\t}\n-\n-\toffset = swp_slot_offset(slot);\n-\n-\tci = swap_cluster_lock(si, offset);\n-\n-\tcount = swap_count(si->swap_map[offset]);\n-\n-\tif ((count & ~COUNT_CONTINUED) != SWAP_MAP_MAX) {\n-\t\t/*\n-\t\t * The higher the swap count, the more likely it is that tasks\n-\t\t * will race to add swap count continuation: we need to avoid\n-\t\t * over-provisioning.\n-\t\t */\n-\t\tgoto out;\n-\t}\n-\n-\tif (!page) {\n-\t\tret = -ENOMEM;\n-\t\tgoto out;\n-\t}\n-\n-\thead = vmalloc_to_page(si->swap_map + offset);\n-\toffset &= ~PAGE_MASK;\n-\n-\tspin_lock(&si->cont_lock);\n-\t/*\n-\t * Page allocation does not initialize the page's lru field,\n-\t * but it does always reset its private field.\n-\t */\n-\tif (!page_private(head)) {\n-\t\tBUG_ON(count & COUNT_CONTINUED);\n-\t\tINIT_LIST_HEAD(&head->lru);\n-\t\tset_page_private(head, SWP_CONTINUED);\n-\t\tsi->flags |= SWP_CONTINUED;\n-\t}\n-\n-\tlist_for_each_entry(list_page, &head->lru, lru) {\n-\t\tunsigned char *map;\n-\n-\t\t/*\n-\t\t * If the previous map said no continuation, but we've found\n-\t\t * a continuation page, free our allocation and use this one.\n-\t\t */\n-\t\tif (!(count & COUNT_CONTINUED))\n-\t\t\tgoto out_unlock_cont;\n-\n-\t\tmap = kmap_local_page(list_page) + offset;\n-\t\tcount = *map;\n-\t\tkunmap_local(map);\n-\n-\t\t/*\n-\t\t * If this continuation count now has some space in it,\n-\t\t * free our allocation and use this one.\n-\t\t */\n-\t\tif ((count & ~COUNT_CONTINUED) != SWAP_CONT_MAX)\n-\t\t\tgoto out_unlock_cont;\n-\t}\n-\n-\tlist_add_tail(&page->lru, &head->lru);\n-\tpage = NULL;\t\t\t/* now it's attached, don't free it */\n-out_unlock_cont:\n-\tspin_unlock(&si->cont_lock);\n-out:\n-\tswap_cluster_unlock(ci);\n-\tswap_slot_put_swap_info(si);\n-outer:\n-\tif (page)\n-\t\t__free_page(page);\n-\treturn ret;\n-}\n-\n-/*\n- * swap_count_continued - when the original swap_map count is incremented\n- * from SWAP_MAP_MAX, check if there is already a continuation page to carry\n- * into, carry if so, or else fail until a new continuation page is allocated;\n- * when the original swap_map count is decremented from 0 with continuation,\n- * borrow from the continuation and report whether it still holds more.\n- * Called while __swap_duplicate() or caller of swap_entry_put_locked()\n- * holds cluster lock.\n- */\n-static bool swap_count_continued(struct swap_info_struct *si,\n-\t\t\t\t pgoff_t offset, unsigned char count)\n-{\n-\tstruct page *head;\n-\tstruct page *page;\n-\tunsigned char *map;\n-\tbool ret;\n-\n-\thead = vmalloc_to_page(si->swap_map + offset);\n-\tif (page_private(head) != SWP_CONTINUED) {\n-\t\tBUG_ON(count & COUNT_CONTINUED);\n-\t\treturn false;\t\t/* need to add count continuation */\n-\t}\n-\n-\tspin_lock(&si->cont_lock);\n-\toffset &= ~PAGE_MASK;\n-\tpage = list_next_entry(head, lru);\n-\tmap = kmap_local_page(page) + offset;\n-\n-\tif (count == SWAP_MAP_MAX)\t/* initial increment from swap_map */\n-\t\tgoto init_map;\t\t/* jump over SWAP_CONT_MAX checks */\n-\n-\tif (count == (SWAP_MAP_MAX | COUNT_CONTINUED)) { /* incrementing */\n-\t\t/*\n-\t\t * Think of how you add 1 to 999\n-\t\t */\n-\t\twhile (*map == (SWAP_CONT_MAX | COUNT_CONTINUED)) {\n-\t\t\tkunmap_local(map);\n-\t\t\tpage = list_next_entry(page, lru);\n-\t\t\tBUG_ON(page == head);\n-\t\t\tmap = kmap_local_page(page) + offset;\n-\t\t}\n-\t\tif (*map == SWAP_CONT_MAX) {\n-\t\t\tkunmap_local(map);\n-\t\t\tpage = list_next_entry(page, lru);\n-\t\t\tif (page == head) {\n-\t\t\t\tret = false;\t/* add count continuation */\n-\t\t\t\tgoto out;\n-\t\t\t}\n-\t\t\tmap = kmap_local_page(page) + offset;\n-init_map:\t\t*map = 0;\t\t/* we didn't zero the page */\n-\t\t}\n-\t\t*map += 1;\n-\t\tkunmap_local(map);\n-\t\twhile ((page = list_prev_entry(page, lru)) != head) {\n-\t\t\tmap = kmap_local_page(page) + offset;\n-\t\t\t*map = COUNT_CONTINUED;\n-\t\t\tkunmap_local(map);\n-\t\t}\n-\t\tret = true;\t\t\t/* incremented */\n-\n-\t} else {\t\t\t\t/* decrementing */\n-\t\t/*\n-\t\t * Think of how you subtract 1 from 1000\n-\t\t */\n-\t\tBUG_ON(count != COUNT_CONTINUED);\n-\t\twhile (*map == COUNT_CONTINUED) {\n-\t\t\tkunmap_local(map);\n-\t\t\tpage = list_next_entry(page, lru);\n-\t\t\tBUG_ON(page == head);\n-\t\t\tmap = kmap_local_page(page) + offset;\n-\t\t}\n-\t\tBUG_ON(*map == 0);\n-\t\t*map -= 1;\n-\t\tif (*map == 0)\n-\t\t\tcount = 0;\n-\t\tkunmap_local(map);\n-\t\twhile ((page = list_prev_entry(page, lru)) != head) {\n-\t\t\tmap = kmap_local_page(page) + offset;\n-\t\t\t*map = SWAP_CONT_MAX | count;\n-\t\t\tcount = COUNT_CONTINUED;\n-\t\t\tkunmap_local(map);\n-\t\t}\n-\t\tret = count == COUNT_CONTINUED;\n-\t}\n-out:\n-\tspin_unlock(&si->cont_lock);\n-\treturn ret;\n-}\n-\n-/*\n- * free_swap_count_continuations - swapoff free all the continuation pages\n- * appended to the swap_map, after swap_map is quiesced, before vfree'ing it.\n- */\n-static void free_swap_count_continuations(struct swap_info_struct *si)\n-{\n-\tpgoff_t offset;\n-\n-\tfor (offset = 0; offset < si->max; offset += PAGE_SIZE) {\n-\t\tstruct page *head;\n-\t\thead = vmalloc_to_page(si->swap_map + offset);\n-\t\tif (page_private(head)) {\n-\t\t\tstruct page *page, *next;\n-\n-\t\t\tlist_for_each_entry_safe(page, next, &head->lru, lru) {\n-\t\t\t\tlist_del(&page->lru);\n-\t\t\t\t__free_page(page);\n-\t\t\t}\n-\t\t}\n-\t}\n-}\n-\n #if defined(CONFIG_MEMCG) && defined(CONFIG_BLK_CGROUP)\n static bool __has_usable_swap(void)\n {\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex 64747493ca9f7..318933071edc6 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -24,6 +24,8 @@\n  * For now, there is a one-to-one correspondence between a virtual swap slot\n  * and its associated physical swap slot.\n  *\n+ * I. Allocation\n+ *\n  * Virtual swap slots are organized into PMD-sized clusters, analogous to\n  * physical swap allocator. However, unlike the physical swap allocator,\n  * the clusters are dynamically allocated and freed on-demand. There is no\n@@ -32,6 +34,26 @@\n  *\n  * This allows us to avoid the overhead of pre-allocating a large number of\n  * virtual swap clusters.\n+ *\n+ * II. Swap Entry Lifecycle\n+ *\n+ * The swap entry's lifecycle is managed at the virtual swap layer. Conceptually,\n+ * each virtual swap slot has a reference count, which includes:\n+ *\n+ * 1. The number of page table entries that refer to the virtual swap slot, i.e\n+ *    its swap count.\n+ *\n+ * 2. Whether the virtual swap slot has been added to the swap cache - if so,\n+ *    its reference count is incremented by 1.\n+ *\n+ * Each virtual swap slot starts out with a reference count of 1 (since it is\n+ * about to be added to the swap cache). Its reference count is incremented or\n+ * decremented every time it is mapped to or unmapped from a PTE, as well as\n+ * when it is added to or removed from the swap cache. Finally, when its\n+ * reference count reaches 0, the virtual swap slot is freed.\n+ *\n+ * Note that we do not have a reference count field per se - it is derived from\n+ * the swap_count and the in_swapcache fields.\n  */\n \n /**\n@@ -42,6 +64,8 @@\n  * @swap_cache: The folio in swap cache.\n  * @shadow: The shadow entry.\n  * @memcgid: The memcg id of the owning memcg, if any.\n+ * @swap_count: The number of page table entries that refer to the swap entry.\n+ * @in_swapcache: Whether the swap entry is (about to be) pinned in swap cache.\n  */\n struct swp_desc {\n \tswp_slot_t slot;\n@@ -50,9 +74,14 @@ struct swp_desc {\n \t\tstruct folio *swap_cache;\n \t\tvoid *shadow;\n \t};\n+\n+\tunsigned int swap_count;\n+\n #ifdef CONFIG_MEMCG\n \tunsigned short memcgid;\n #endif\n+\n+\tbool in_swapcache;\n };\n \n #define VSWAP_CLUSTER_SHIFT HPAGE_PMD_ORDER\n@@ -249,6 +278,8 @@ static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster, int start)\n #ifdef CONFIG_MEMCG\n \t\tdesc->memcgid = 0;\n #endif\n+\t\tdesc->swap_count = 0;\n+\t\tdesc->in_swapcache = true;\n \t}\n \tcluster->count += nr;\n }\n@@ -452,7 +483,7 @@ static inline void release_vswap_slot(struct vswap_cluster *cluster,\n  * Update the physical-to-virtual swap slot mapping.\n  * Caller must ensure the physical swap slot's cluster is locked.\n  */\n-static void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n+void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\t\t   unsigned long vswap, int nr)\n {\n \tatomic_long_t *table;\n@@ -466,45 +497,50 @@ static void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\t__swap_table_set(ci, ci_off + i, vswap ? vswap + i : 0);\n }\n \n-/**\n- * vswap_free - free a virtual swap slot.\n- * @entry: the virtual swap slot to free\n- * @ci: the physical swap slot's cluster (optional, can be NULL)\n+/*\n+ * Entered with the cluster locked, but might unlock the cluster.\n+ * This is because several operations, such as releasing physical swap slots\n+ * (i.e swap_slot_free_nr()) require the cluster to be unlocked to avoid\n+ * deadlocks.\n  *\n- * If @ci is NULL, this function is called to clean up a virtual swap entry\n- * when no linkage has been established between physical and virtual swap slots.\n- * If @ci is provided, the caller must ensure it is locked.\n+ * This is safe, because:\n+ *\n+ * 1. The swap entry to be freed has refcnt (swap count and swapcache pin)\n+ *    down to 0, so no one can change its internal state\n+ *\n+ * 2. The swap entry to be freed still holds a refcnt to the cluster, keeping\n+ *    the cluster itself valid.\n+ *\n+ * We will exit the function with the cluster re-locked.\n  */\n-void vswap_free(swp_entry_t entry, struct swap_cluster_info *ci)\n+static void vswap_free(struct vswap_cluster *cluster, struct swp_desc *desc,\n+\tswp_entry_t entry)\n {\n-\tstruct vswap_cluster *cluster = NULL;\n-\tstruct swp_desc *desc;\n+\tstruct zswap_entry *zswap_entry;\n+\tswp_slot_t slot;\n \n-\tif (!entry.val)\n-\t\treturn;\n+\t/* Clear shadow if present */\n+\tif (xa_is_value(desc->shadow))\n+\t\tdesc->shadow = NULL;\n \n-\tzswap_invalidate(entry);\n-\tmem_cgroup_uncharge_swap(entry, 1);\n+\tslot = desc->slot;\n+\tdesc->slot.val = 0;\n \n-\t/* do not immediately erase the virtual slot to prevent its reuse */\n-\trcu_read_lock();\n-\tdesc = vswap_iter(&cluster, entry.val);\n-\tif (!desc) {\n-\t\trcu_read_unlock();\n-\t\treturn;\n+\tzswap_entry = desc->zswap_entry;\n+\tif (zswap_entry) {\n+\t\tdesc->zswap_entry = NULL;\n+\t\tzswap_entry_free(zswap_entry);\n \t}\n+\tspin_unlock(&cluster->lock);\n \n-\t/* Clear shadow if present */\n-\tif (xa_is_value(desc->shadow))\n-\t\tdesc->shadow = NULL;\n+\tmem_cgroup_uncharge_swap(entry, 1);\n \n-\tif (desc->slot.val)\n-\t\tvswap_rmap_set(ci, desc->slot, 0, 1);\n+\tif (slot.val)\n+\t\tswap_slot_free_nr(slot, 1);\n \n+\tspin_lock(&cluster->lock);\n \t/* erase forward mapping and release the virtual slot for reallocation */\n \trelease_vswap_slot(cluster, entry.val);\n-\tspin_unlock(&cluster->lock);\n-\trcu_read_unlock();\n }\n \n /**\n@@ -538,8 +574,12 @@ int folio_alloc_swap(struct folio *folio)\n \t * fallback from zswap store failure).\n \t */\n \tif (swap_slot_alloc(&slot, order)) {\n-\t\tfor (i = 0; i < nr; i++)\n-\t\t\tvswap_free((swp_entry_t){entry.val + i}, NULL);\n+\t\tfor (i = 0; i < nr; i++) {\n+\t\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\t\tVM_WARN_ON(!desc);\n+\t\t\tvswap_free(cluster, desc, (swp_entry_t){ entry.val + i });\n+\t\t}\n+\t\tspin_unlock(&cluster->lock);\n \t\tentry.val = 0;\n \t\treturn -ENOMEM;\n \t}\n@@ -603,9 +643,11 @@ swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry)\n \t\trcu_read_unlock();\n \t\treturn (swp_slot_t){0};\n \t}\n+\n \tslot = desc->slot;\n \tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n+\n \treturn slot;\n }\n \n@@ -635,6 +677,352 @@ swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot)\n \treturn ret;\n }\n \n+/*\n+ * Decrease the swap count of nr contiguous swap entries by 1 (when the swap\n+ * entries are removed from a range of PTEs), and check if any of the swap\n+ * entries are in swap cache only after its swap count is decreased.\n+ *\n+ * The check is racy, but it is OK because free_swap_and_cache_nr() only use\n+ * the result as a hint.\n+ */\n+static bool vswap_free_nr_any_cache_only(swp_entry_t entry, int nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tbool ret = false;\n+\tint i;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val);\n+\t\tVM_WARN_ON(!desc);\n+\t\tret |= (desc->swap_count == 1 && desc->in_swapcache);\n+\t\tdesc->swap_count--;\n+\t\tif (!desc->swap_count && !desc->in_swapcache)\n+\t\t\tvswap_free(cluster, desc, entry);\n+\t\tentry.val++;\n+\t}\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\treturn ret;\n+}\n+\n+/**\n+ * swap_free_nr - decrease the swap count of nr contiguous swap entries by 1\n+ *                (when the swap entries are removed from a range of PTEs).\n+ * @entry: the first entry in the range.\n+ * @nr: the number of entries in the range.\n+ */\n+void swap_free_nr(swp_entry_t entry, int nr)\n+{\n+\tvswap_free_nr_any_cache_only(entry, nr);\n+}\n+\n+static int swap_duplicate_nr(swp_entry_t entry, int nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tint i = 0;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tif (!desc || (!desc->swap_count && !desc->in_swapcache))\n+\t\t\tgoto done;\n+\t\tdesc->swap_count++;\n+\t}\n+done:\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\tif (i && i < nr)\n+\t\tswap_free_nr(entry, i);\n+\n+\treturn i == nr ? 0 : -ENOENT;\n+}\n+\n+/**\n+ * swap_duplicate - increase the swap count of the swap entry by 1 (i.e when\n+ *                  the swap entry is stored at a new PTE).\n+ * @entry: the swap entry.\n+ *\n+ * Return: -ENONENT, if we try to duplicate a non-existent swap entry.\n+ */\n+int swap_duplicate(swp_entry_t entry)\n+{\n+\treturn swap_duplicate_nr(entry, 1);\n+}\n+\n+\n+bool folio_swapped(struct folio *folio)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tswp_entry_t entry = folio->swap;\n+\tint i, nr = folio_nr_pages(folio);\n+\tstruct swp_desc *desc;\n+\tbool swapped = false;\n+\n+\tif (!entry.val)\n+\t\treturn false;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tif (desc && desc->swap_count) {\n+\t\t\tswapped = true;\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\treturn swapped;\n+}\n+\n+/**\n+ * swp_swapcount - return the swap count of the swap entry.\n+ * @id: the swap entry.\n+ *\n+ * Note that all the swap count functions are identical in the new design,\n+ * since we no longer need swap count continuation.\n+ *\n+ * Return: the swap count of the swap entry.\n+ */\n+int swp_swapcount(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tunsigned int ret;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tret = desc ? desc->swap_count : 0;\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn ret;\n+}\n+\n+int __swap_count(swp_entry_t entry)\n+{\n+\treturn swp_swapcount(entry);\n+}\n+\n+bool swap_entry_swapped(swp_entry_t entry)\n+{\n+\treturn !!swp_swapcount(entry);\n+}\n+\n+void swap_shmem_alloc(swp_entry_t entry, int nr)\n+{\n+\tswap_duplicate_nr(entry, nr);\n+}\n+\n+void swapcache_clear(swp_entry_t entry, int nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tint i;\n+\n+\tif (!nr)\n+\t\treturn;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val);\n+\t\tdesc->in_swapcache = false;\n+\t\tif (!desc->swap_count)\n+\t\t\tvswap_free(cluster, desc, entry);\n+\t\tentry.val++;\n+\t}\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+int swapcache_prepare(swp_entry_t entry, int nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tint i, ret = 0;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\n+\t\tif (!desc) {\n+\t\t\tret = -ENOENT;\n+\t\t\tgoto done;\n+\t\t}\n+\n+\t\tif (!desc->swap_count && !desc->in_swapcache) {\n+\t\t\tret = -ENOENT;\n+\t\t\tgoto done;\n+\t\t}\n+\n+\t\tif (desc->in_swapcache) {\n+\t\t\tret = -EEXIST;\n+\t\t\tgoto done;\n+\t\t}\n+\n+\t\tdesc->in_swapcache = true;\n+\t}\n+done:\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\tif (i && i < nr)\n+\t\tswapcache_clear(entry, i);\n+\tif (i < nr && !ret)\n+\t\tret = -ENOENT;\n+\treturn ret;\n+}\n+\n+/**\n+ * is_swap_cached - check if the swap entry is cached\n+ * @entry: swap entry to check\n+ *\n+ * Returns true if the swap entry is cached, false otherwise.\n+ */\n+bool is_swap_cached(swp_entry_t entry)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tbool cached;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tcached = desc ? desc->in_swapcache : false;\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\treturn cached;\n+}\n+\n+/**\n+ * vswap_only_has_cache - check if all the slots in the range are still valid,\n+ *                        and are in swap cache only (i.e not stored in any\n+ *                        PTEs).\n+ * @entry: the first slot in the range.\n+ * @nr: the number of slots in the range.\n+ *\n+ * Return: true if all the slots in the range are still valid, and are in swap\n+ * cache only, or false otherwise.\n+ */\n+bool vswap_only_has_cache(swp_entry_t entry, int nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tint i = 0;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tif (!desc || desc->swap_count || !desc->in_swapcache)\n+\t\t\tgoto done;\n+\t}\n+done:\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\treturn i == nr;\n+}\n+\n+/**\n+ * non_swapcache_batch - count the longest range starting from a particular\n+ *                       swap slot that are stil valid, but not in swap cache.\n+ * @entry: the first slot to check.\n+ * @max_nr: the maximum number of slots to check.\n+ *\n+ * Return: the number of slots in the longest range that are still valid, but\n+ * not in swap cache.\n+ */\n+int non_swapcache_batch(swp_entry_t entry, int max_nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tint i;\n+\n+\tif (!entry.val)\n+\t\treturn 0;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < max_nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tif (!desc || desc->in_swapcache || !desc->swap_count)\n+\t\t\tgoto done;\n+\t}\n+done:\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\treturn i;\n+}\n+\n+/**\n+ * free_swap_and_cache_nr() - Release a swap count on range of swap entries and\n+ *                            reclaim their cache if no more references remain.\n+ * @entry: First entry of range.\n+ * @nr: Number of entries in range.\n+ *\n+ * For each swap entry in the contiguous range, release a swap count. If any\n+ * swap entries have their swap count decremented to zero, try to reclaim their\n+ * associated swap cache pages.\n+ */\n+void free_swap_and_cache_nr(swp_entry_t entry, int nr)\n+{\n+\tint i = 0, incr = 1;\n+\tstruct folio *folio;\n+\n+\tif (vswap_free_nr_any_cache_only(entry, nr)) {\n+\t\twhile (i < nr) {\n+\t\t\tincr = 1;\n+\t\t\tif (vswap_only_has_cache(entry, 1)) {\n+\t\t\t\tfolio = swap_cache_get_folio(entry);\n+\t\t\t\tif (!folio)\n+\t\t\t\t\tgoto next;\n+\n+\t\t\t\tif (!folio_trylock(folio)) {\n+\t\t\t\t\tfolio_put(folio);\n+\t\t\t\t\tgoto next;\n+\t\t\t\t}\n+\n+\t\t\t\tif (!folio_matches_swap_entry(folio, entry)) {\n+\t\t\t\t\tfolio_unlock(folio);\n+\t\t\t\t\tfolio_put(folio);\n+\t\t\t\t\tgoto next;\n+\t\t\t\t}\n+\n+\t\t\t\t/*\n+\t\t\t\t * Folios are always naturally aligned in swap so\n+\t\t\t\t * advance forward to the next boundary.\n+\t\t\t\t */\n+\t\t\t\tincr = ALIGN(entry.val + 1, folio_nr_pages(folio)) - entry.val;\n+\t\t\t\tfolio_free_swap(folio);\n+\t\t\t\tfolio_unlock(folio);\n+\t\t\t\tfolio_put(folio);\n+\t\t\t}\n+next:\n+\t\t\ti += incr;\n+\t\t\tentry.val += incr;\n+\t\t}\n+\t}\n+}\n+\n+/*\n+ * Called after dropping swapcache to decrease refcnt to swap entries.\n+ */\n+void put_swap_folio(struct folio *folio, swp_entry_t entry)\n+{\n+\tint nr = folio_nr_pages(folio);\n+\n+\tVM_WARN_ON(!folio_test_locked(folio));\n+\tswapcache_clear(entry, nr);\n+}\n+\n bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si)\n {\n \tstruct vswap_cluster *cluster;\n@@ -869,8 +1257,8 @@ void *swap_cache_get_shadow(swp_entry_t entry)\n  * Context: Caller must ensure @entry is valid and protect the cluster with\n  * reference count or locks.\n  *\n- * The caller also needs to update the corresponding swap_map slots with\n- * SWAP_HAS_CACHE bit to avoid race or conflict.\n+ * The caller also needs to obtain the swap entries' swap cache pins to avoid\n+ * race or conflict.\n  */\n void swap_cache_add_folio(struct folio *folio, swp_entry_t entry, void **shadowp)\n {\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex 72441131f094e..e46349f9c90bb 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -703,7 +703,7 @@ static void zswap_entry_cache_free(struct zswap_entry *entry)\n  * Carries out the common pattern of freeing an entry's zsmalloc allocation,\n  * freeing the entry itself, and decrementing the number of stored pages.\n  */\n-static void zswap_entry_free(struct zswap_entry *entry)\n+void zswap_entry_free(struct zswap_entry *entry)\n {\n \tzswap_lru_del(&zswap_list_lru, entry);\n \tzs_free(entry->pool->zs_pool, entry->handle);\n@@ -1627,18 +1627,6 @@ int zswap_load(struct folio *folio)\n \treturn 0;\n }\n \n-void zswap_invalidate(swp_entry_t swp)\n-{\n-\tstruct zswap_entry *entry;\n-\n-\tif (zswap_empty(swp))\n-\t\treturn;\n-\n-\tentry = zswap_entry_erase(swp);\n-\tif (entry)\n-\t\tzswap_entry_free(entry);\n-}\n-\n /*********************************\n * debugfs functions\n **********************************/\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in a future version (v2) and explained that this change is necessary due to the new virtual swap design.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch presents the first real use case of the new virtual swap\ndesign. It leverages the virtualization of the swap space to decouple a\nswap entry and its backing storage. A swap entry can now be backed by\none of the following options:\n\n1. A physical swap slot (i.e on a physical swapfile/swap partition).\n2. A \"zero swap page\", i.e the swapped out page is a zero page.\n3. A compressed object in the zswap pool.\n4. An in-memory page. This can happen when a page is loaded\n   (exclusively) from the zswap pool, or if the page is rejected by\n   zswap and zswap writeback is disabled.\n\nThis allows us to use zswap and the zero swap page optimization, without\nhaving to reserved a slot on a swapfile, or a swapfile at all. This\ntranslates to tens to hundreds of GBs of disk saving on hosts and\nworkloads that have high memory usage, as well as removes this spurious\nlimit on the usage of these optimizations.\n\nOne implication of this change is that we need to be much more careful\nwith THP swapin and batched swap free operations. The central\nrequirement is the range of entries we are working with must\nhave no mixed backing states:\n\n1. For now, zswap-backed entries are not supported for these batched\n   operations.\n2. All the entries must be backed by the same type.\n3. If the swap entries in the batch are backed by in-memory folio, it\n   must be the same folio (i.e they correspond to the subpages of that\n   folio).\n4. If the swap entries in the batch are backed by slots on swapfiles, it\n   must be the same swapfile, and these physical swap slots must also be\n   contiguous.\n\nFor now, we still charge virtual swap slots towards the memcg's swap\nusage. In a following patch, we will change this behavior and only\ncharge physical (i.e on swapfile) swap slots towards the memcg's swap\nusage.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h  |  14 +\n include/linux/zswap.h |   3 +-\n mm/internal.h         |  14 +-\n mm/memcontrol.c       |  65 +++--\n mm/memory.c           |  84 ++++--\n mm/page_io.c          |  74 ++---\n mm/shmem.c            |   6 +-\n mm/swap.h             |  32 +--\n mm/swap_state.c       |  29 +-\n mm/swapfile.c         |   8 -\n mm/vmscan.c           |  19 +-\n mm/vswap.c            | 638 ++++++++++++++++++++++++++++++++++--------\n mm/zswap.c            |  45 ++-\n 13 files changed, 729 insertions(+), 302 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex aae2e502d9975..54df972608047 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -658,12 +658,26 @@ static inline bool mem_cgroup_swap_full(struct folio *folio)\n \n int vswap_init(void);\n void vswap_exit(void);\n+bool vswap_alloc_swap_slot(struct folio *folio);\n swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry);\n swp_entry_t swp_slot_to_swp_entry(swp_slot_t slot);\n bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si);\n void put_swap_entry(swp_entry_t entry, struct swap_info_struct *si);\n bool folio_swapped(struct folio *folio);\n bool vswap_only_has_cache(swp_entry_t entry, int nr);\n+int non_swapcache_batch(swp_entry_t entry, int nr);\n+bool vswap_swapfile_backed(swp_entry_t entry, int nr);\n+bool vswap_folio_backed(swp_entry_t entry, int nr);\n+void vswap_store_folio(swp_entry_t entry, struct folio *folio);\n+void swap_zeromap_folio_set(struct folio *folio);\n+void vswap_assoc_zswap(swp_entry_t entry, struct zswap_entry *zswap_entry);\n+bool vswap_can_swapin_thp(swp_entry_t entry, int nr);\n \n+static inline struct swap_info_struct *vswap_get_device(swp_entry_t entry)\n+{\n+\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n+\n+\treturn slot.val ? swap_slot_tryget_swap_info(slot) : NULL;\n+}\n #endif /* __KERNEL__*/\n #endif /* _LINUX_SWAP_H */\ndiff --git a/include/linux/zswap.h b/include/linux/zswap.h\nindex 07b2936c38f29..f33b4433a5ee8 100644\n--- a/include/linux/zswap.h\n+++ b/include/linux/zswap.h\n@@ -33,9 +33,8 @@ void zswap_lruvec_state_init(struct lruvec *lruvec);\n void zswap_folio_swapin(struct folio *folio);\n bool zswap_is_enabled(void);\n bool zswap_never_enabled(void);\n-void *zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry);\n+void zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry);\n void *zswap_entry_load(swp_entry_t swpentry);\n-void *zswap_entry_erase(swp_entry_t swpentry);\n bool zswap_empty(swp_entry_t swpentry);\n void zswap_entry_free(struct zswap_entry *entry);\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 7ced0def684ca..cfe97501e4885 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -327,19 +327,7 @@ static inline swp_entry_t swap_nth(swp_entry_t entry, long n)\n \treturn (swp_entry_t) { entry.val + n };\n }\n \n-/* similar to swap_nth, but check the backing physical slots as well. */\n-static inline swp_entry_t swap_move(swp_entry_t entry, long delta)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry), next_slot;\n-\tswp_entry_t next_entry = swap_nth(entry, delta);\n-\n-\tnext_slot = swp_entry_to_swp_slot(next_entry);\n-\tif (swp_slot_type(slot) != swp_slot_type(next_slot) ||\n-\t\t\tswp_slot_offset(slot) + delta != swp_slot_offset(next_slot))\n-\t\tnext_entry.val = 0;\n-\n-\treturn next_entry;\n-}\n+swp_entry_t swap_move(swp_entry_t entry, long delta);\n \n /**\n  * pte_move_swp_offset - Move the swap entry offset field of a swap pte\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 86f43b7e5f710..2ba5811e7edba 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -5247,10 +5247,18 @@ void __mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages)\n \trcu_read_unlock();\n }\n \n+static bool mem_cgroup_may_zswap(struct mem_cgroup *original_memcg);\n+\n long mem_cgroup_get_nr_swap_pages(struct mem_cgroup *memcg)\n {\n-\tlong nr_swap_pages = get_nr_swap_pages();\n+\tlong nr_swap_pages, nr_zswap_pages = 0;\n+\n+\tif (zswap_is_enabled() && (mem_cgroup_disabled() || do_memsw_account() ||\n+\t\t\t\tmem_cgroup_may_zswap(memcg))) {\n+\t\tnr_zswap_pages = PAGE_COUNTER_MAX;\n+\t}\n \n+\tnr_swap_pages = max_t(long, nr_zswap_pages, get_nr_swap_pages());\n \tif (mem_cgroup_disabled() || do_memsw_account())\n \t\treturn nr_swap_pages;\n \tfor (; !mem_cgroup_is_root(memcg); memcg = parent_mem_cgroup(memcg))\n@@ -5419,6 +5427,29 @@ static struct cftype swap_files[] = {\n };\n \n #ifdef CONFIG_ZSWAP\n+static bool mem_cgroup_may_zswap(struct mem_cgroup *original_memcg)\n+{\n+\tstruct mem_cgroup *memcg;\n+\n+\tfor (memcg = original_memcg; !mem_cgroup_is_root(memcg);\n+\t     memcg = parent_mem_cgroup(memcg)) {\n+\t\tunsigned long max = READ_ONCE(memcg->zswap_max);\n+\t\tunsigned long pages;\n+\n+\t\tif (max == PAGE_COUNTER_MAX)\n+\t\t\tcontinue;\n+\t\tif (max == 0)\n+\t\t\treturn false;\n+\n+\t\t/* Force flush to get accurate stats for charging */\n+\t\t__mem_cgroup_flush_stats(memcg, true);\n+\t\tpages = memcg_page_state(memcg, MEMCG_ZSWAP_B) / PAGE_SIZE;\n+\t\tif (pages >= max)\n+\t\t\treturn false;\n+\t}\n+\treturn true;\n+}\n+\n /**\n  * obj_cgroup_may_zswap - check if this cgroup can zswap\n  * @objcg: the object cgroup\n@@ -5433,34 +5464,15 @@ static struct cftype swap_files[] = {\n  */\n bool obj_cgroup_may_zswap(struct obj_cgroup *objcg)\n {\n-\tstruct mem_cgroup *memcg, *original_memcg;\n+\tstruct mem_cgroup *memcg;\n \tbool ret = true;\n \n \tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys))\n \t\treturn true;\n \n-\toriginal_memcg = get_mem_cgroup_from_objcg(objcg);\n-\tfor (memcg = original_memcg; !mem_cgroup_is_root(memcg);\n-\t     memcg = parent_mem_cgroup(memcg)) {\n-\t\tunsigned long max = READ_ONCE(memcg->zswap_max);\n-\t\tunsigned long pages;\n-\n-\t\tif (max == PAGE_COUNTER_MAX)\n-\t\t\tcontinue;\n-\t\tif (max == 0) {\n-\t\t\tret = false;\n-\t\t\tbreak;\n-\t\t}\n-\n-\t\t/* Force flush to get accurate stats for charging */\n-\t\t__mem_cgroup_flush_stats(memcg, true);\n-\t\tpages = memcg_page_state(memcg, MEMCG_ZSWAP_B) / PAGE_SIZE;\n-\t\tif (pages < max)\n-\t\t\tcontinue;\n-\t\tret = false;\n-\t\tbreak;\n-\t}\n-\tmem_cgroup_put(original_memcg);\n+\tmemcg = get_mem_cgroup_from_objcg(objcg);\n+\tret = mem_cgroup_may_zswap(memcg);\n+\tmem_cgroup_put(memcg);\n \treturn ret;\n }\n \n@@ -5604,6 +5616,11 @@ static struct cftype zswap_files[] = {\n \t},\n \t{ }\t/* terminate */\n };\n+#else\n+static inline bool mem_cgroup_may_zswap(struct mem_cgroup *original_memcg)\n+{\n+\treturn false;\n+}\n #endif /* CONFIG_ZSWAP */\n \n static int __init mem_cgroup_swap_init(void)\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 641e3f65edc00..a16bf84ebaaf9 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -4362,6 +4362,15 @@ static inline bool should_try_to_free_swap(struct folio *folio,\n \tif (mem_cgroup_swap_full(folio) || (vma->vm_flags & VM_LOCKED) ||\n \t    folio_test_mlocked(folio))\n \t\treturn true;\n+\n+\t/*\n+\t * Mixed and/or non-swapfile backends cannot be re-used for future swapouts\n+\t * anyway. Try to free swap space unless the folio is backed by contiguous\n+\t * physical swap slots.\n+\t */\n+\tif (!vswap_swapfile_backed(folio->swap, folio_nr_pages(folio)))\n+\t\treturn true;\n+\n \t/*\n \t * If we want to map a page that's in the swapcache writable, we\n \t * have to detect via the refcount if we're really the exclusive\n@@ -4623,12 +4632,12 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \tstruct folio *swapcache, *folio = NULL;\n \tDECLARE_WAITQUEUE(wait, current);\n \tstruct page *page;\n-\tstruct swap_info_struct *si = NULL;\n+\tstruct swap_info_struct *si = NULL, *stable_si;\n \trmap_t rmap_flags = RMAP_NONE;\n \tbool need_clear_cache = false;\n \tbool swapoff_locked = false;\n \tbool exclusive = false;\n-\tsoftleaf_t entry;\n+\tsoftleaf_t orig_entry, entry;\n \tpte_t pte;\n \tvm_fault_t ret = 0;\n \tvoid *shadow = NULL;\n@@ -4641,6 +4650,11 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\tgoto out;\n \n \tentry = softleaf_from_pte(vmf->orig_pte);\n+\t/*\n+\t * entry might change if we get a large folio - remember the original entry\n+\t * for unlocking swapoff etc.\n+\t */\n+\torig_entry = entry;\n \tif (unlikely(!softleaf_is_swap(entry))) {\n \t\tif (softleaf_is_migration(entry)) {\n \t\t\tmigration_entry_wait(vma->vm_mm, vmf->pmd,\n@@ -4705,7 +4719,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \tswapcache = folio;\n \n \tif (!folio) {\n-\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO) &&\n+\t\tif (si && data_race(si->flags & SWP_SYNCHRONOUS_IO) &&\n \t\t    __swap_count(entry) == 1) {\n \t\t\t/* skip swapcache */\n \t\t\tfolio = alloc_swap_folio(vmf);\n@@ -4736,6 +4750,17 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\t\t\t}\n \t\t\t\tneed_clear_cache = true;\n \n+\t\t\t\t/*\n+\t\t\t\t * Recheck to make sure the entire range is still\n+\t\t\t\t * THP-swapin-able. Note that before we call\n+\t\t\t\t * swapcache_prepare(), entries in the range can\n+\t\t\t\t * still have their backing status changed.\n+\t\t\t\t */\n+\t\t\t\tif (!vswap_can_swapin_thp(entry, nr_pages)) {\n+\t\t\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\t\t\tgoto out_page;\n+\t\t\t\t}\n+\n \t\t\t\tmemcg1_swapin(entry, nr_pages);\n \n \t\t\t\tshadow = swap_cache_get_shadow(entry);\n@@ -4916,27 +4941,40 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\t\t * swapcache -> certainly exclusive.\n \t\t\t */\n \t\t\texclusive = true;\n-\t\t} else if (exclusive && folio_test_writeback(folio) &&\n-\t\t\t  data_race(si->flags & SWP_STABLE_WRITES)) {\n+\t\t} else if (exclusive && folio_test_writeback(folio)) {\n \t\t\t/*\n-\t\t\t * This is tricky: not all swap backends support\n-\t\t\t * concurrent page modifications while under writeback.\n-\t\t\t *\n-\t\t\t * So if we stumble over such a page in the swapcache\n-\t\t\t * we must not set the page exclusive, otherwise we can\n-\t\t\t * map it writable without further checks and modify it\n-\t\t\t * while still under writeback.\n+\t\t\t * We need to look up the swap device again here, because\n+\t\t\t * the si we got from tryget_swap_entry() might have changed\n+\t\t\t * before we pin the backend.\n \t\t\t *\n-\t\t\t * For these problematic swap backends, simply drop the\n-\t\t\t * exclusive marker: this is perfectly fine as we start\n-\t\t\t * writeback only if we fully unmapped the page and\n-\t\t\t * there are no unexpected references on the page after\n-\t\t\t * unmapping succeeded. After fully unmapped, no\n-\t\t\t * further GUP references (FOLL_GET and FOLL_PIN) can\n-\t\t\t * appear, so dropping the exclusive marker and mapping\n-\t\t\t * it only R/O is fine.\n+\t\t\t * With the folio locked and loaded into the swap cache, we can\n+\t\t\t * now guarantee a stable backing state.\n \t\t\t */\n-\t\t\texclusive = false;\n+\t\t\tstable_si = vswap_get_device(entry);\n+\t\t\tif (stable_si && data_race(stable_si->flags & SWP_STABLE_WRITES)) {\n+\t\t\t\t/*\n+\t\t\t\t * This is tricky: not all swap backends support\n+\t\t\t\t * concurrent page modifications while under writeback.\n+\t\t\t\t *\n+\t\t\t\t * So if we stumble over such a page in the swapcache\n+\t\t\t\t * we must not set the page exclusive, otherwise we can\n+\t\t\t\t * map it writable without further checks and modify it\n+\t\t\t\t * while still under writeback.\n+\t\t\t\t *\n+\t\t\t\t * For these problematic swap backends, simply drop the\n+\t\t\t\t * exclusive marker: this is perfectly fine as we start\n+\t\t\t\t * writeback only if we fully unmapped the page and\n+\t\t\t\t * there are no unexpected references on the page after\n+\t\t\t\t * unmapping succeeded. After fully unmapped, no\n+\t\t\t\t * further GUP references (FOLL_GET and FOLL_PIN) can\n+\t\t\t\t * appear, so dropping the exclusive marker and mapping\n+\t\t\t\t * it only R/O is fine.\n+\t\t\t\t */\n+\t\t\t\texclusive = false;\n+\t\t\t}\n+\n+\t\t\tif (stable_si)\n+\t\t\t\tswap_slot_put_swap_info(stable_si);\n \t\t}\n \t}\n \n@@ -5045,7 +5083,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\t\twake_up(&swapcache_wq);\n \t}\n \tif (swapoff_locked)\n-\t\tput_swap_entry(entry, si);\n+\t\tput_swap_entry(orig_entry, si);\n \treturn ret;\n out_nomap:\n \tif (vmf->pte)\n@@ -5064,7 +5102,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\t\twake_up(&swapcache_wq);\n \t}\n \tif (swapoff_locked)\n-\t\tput_swap_entry(entry, si);\n+\t\tput_swap_entry(orig_entry, si);\n \treturn ret;\n }\n \ndiff --git a/mm/page_io.c b/mm/page_io.c\nindex 5de3705572955..675ec6445609b 100644\n--- a/mm/page_io.c\n+++ b/mm/page_io.c\n@@ -201,44 +201,6 @@ static bool is_folio_zero_filled(struct folio *folio)\n \treturn true;\n }\n \n-static void swap_zeromap_folio_set(struct folio *folio)\n-{\n-\tstruct obj_cgroup *objcg = get_obj_cgroup_from_folio(folio);\n-\tstruct swap_info_struct *sis =\n-\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n-\tint nr_pages = folio_nr_pages(folio);\n-\tswp_entry_t entry;\n-\tswp_slot_t slot;\n-\tunsigned int i;\n-\n-\tfor (i = 0; i < folio_nr_pages(folio); i++) {\n-\t\tentry = page_swap_entry(folio_page(folio, i));\n-\t\tslot = swp_entry_to_swp_slot(entry);\n-\t\tset_bit(swp_slot_offset(slot), sis->zeromap);\n-\t}\n-\n-\tcount_vm_events(SWPOUT_ZERO, nr_pages);\n-\tif (objcg) {\n-\t\tcount_objcg_events(objcg, SWPOUT_ZERO, nr_pages);\n-\t\tobj_cgroup_put(objcg);\n-\t}\n-}\n-\n-static void swap_zeromap_folio_clear(struct folio *folio)\n-{\n-\tstruct swap_info_struct *sis =\n-\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n-\tswp_entry_t entry;\n-\tswp_slot_t slot;\n-\tunsigned int i;\n-\n-\tfor (i = 0; i < folio_nr_pages(folio); i++) {\n-\t\tentry = page_swap_entry(folio_page(folio, i));\n-\t\tslot = swp_entry_to_swp_slot(entry);\n-\t\tclear_bit(swp_slot_offset(slot), sis->zeromap);\n-\t}\n-}\n-\n /*\n  * We may have stale swap cache pages in memory: notice\n  * them here and get rid of the unnecessary final write.\n@@ -260,23 +222,22 @@ int swap_writeout(struct folio *folio, struct swap_iocb **swap_plug)\n \t\tgoto out_unlock;\n \t}\n \n-\t/*\n-\t * Use a bitmap (zeromap) to avoid doing IO for zero-filled pages.\n-\t * The bits in zeromap are protected by the locked swapcache folio\n-\t * and atomic updates are used to protect against read-modify-write\n-\t * corruption due to other zero swap entries seeing concurrent updates.\n-\t */\n \tif (is_folio_zero_filled(folio)) {\n \t\tswap_zeromap_folio_set(folio);\n \t\tgoto out_unlock;\n \t}\n \n \t/*\n-\t * Clear bits this folio occupies in the zeromap to prevent zero data\n-\t * being read in from any previous zero writes that occupied the same\n-\t * swap entries.\n+\t * Release swap backends to make sure we do not have mixed backends\n+\t *\n+\t * The only exception is if the folio is already backed by a\n+\t * contiguous range of physical swap slots (for e.g, from a previous\n+\t * swapout attempt when zswap is disabled).\n+\t *\n+\t * Keep that backend to avoid reallocation of physical swap slots.\n \t */\n-\tswap_zeromap_folio_clear(folio);\n+\tif (!vswap_swapfile_backed(folio->swap, folio_nr_pages(folio)))\n+\t\tvswap_store_folio(folio->swap, folio);\n \n \tif (zswap_store(folio)) {\n \t\tcount_mthp_stat(folio_order(folio), MTHP_STAT_ZSWPOUT);\n@@ -287,6 +248,12 @@ int swap_writeout(struct folio *folio, struct swap_iocb **swap_plug)\n \t\treturn AOP_WRITEPAGE_ACTIVATE;\n \t}\n \n+\t/* fall back to physical swap device */\n+\tif (!vswap_alloc_swap_slot(folio)) {\n+\t\tfolio_mark_dirty(folio);\n+\t\treturn AOP_WRITEPAGE_ACTIVATE;\n+\t}\n+\n \t__swap_writepage(folio, swap_plug);\n \treturn 0;\n out_unlock:\n@@ -618,14 +585,11 @@ static void swap_read_folio_bdev_async(struct folio *folio,\n \n void swap_read_folio(struct folio *folio, struct swap_iocb **plug)\n {\n-\tstruct swap_info_struct *sis =\n-\t\t__swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n-\tbool synchronous = sis->flags & SWP_SYNCHRONOUS_IO;\n-\tbool workingset = folio_test_workingset(folio);\n+\tstruct swap_info_struct *sis;\n+\tbool synchronous, workingset = folio_test_workingset(folio);\n \tunsigned long pflags;\n \tbool in_thrashing;\n \n-\tVM_BUG_ON_FOLIO(!folio_test_swapcache(folio) && !synchronous, folio);\n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n \tVM_BUG_ON_FOLIO(folio_test_uptodate(folio), folio);\n \n@@ -651,6 +615,10 @@ void swap_read_folio(struct folio *folio, struct swap_iocb **plug)\n \t/* We have to read from slower devices. Increase zswap protection. */\n \tzswap_folio_swapin(folio);\n \n+\tsis = __swap_slot_to_info(swp_entry_to_swp_slot(folio->swap));\n+\tsynchronous = sis->flags & SWP_SYNCHRONOUS_IO;\n+\tVM_BUG_ON_FOLIO(!folio_test_swapcache(folio) && !synchronous, folio);\n+\n \tif (data_race(sis->flags & SWP_FS_OPS)) {\n \t\tswap_read_folio_fs(folio, plug);\n \t} else if (synchronous) {\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 780571c830e5b..3a346cca114ab 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -1459,7 +1459,7 @@ static unsigned int shmem_find_swap_entries(struct address_space *mapping,\n \t\t * swapin error entries can be found in the mapping. But they're\n \t\t * deliberately ignored here as we've done everything we can do.\n \t\t */\n-\t\tif (swp_slot_type(slot) != type)\n+\t\tif (!slot.val || swp_slot_type(slot) != type)\n \t\t\tcontinue;\n \n \t\tindices[folio_batch_count(fbatch)] = xas.xa_index;\n@@ -1604,7 +1604,7 @@ int shmem_writeout(struct folio *folio, struct swap_iocb **plug,\n \tif ((info->flags & SHMEM_F_LOCKED) || sbinfo->noswap)\n \t\tgoto redirty;\n \n-\tif (!total_swap_pages)\n+\tif (!zswap_is_enabled() && !total_swap_pages)\n \t\tgoto redirty;\n \n \t/*\n@@ -2341,7 +2341,7 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \t/* Look it up and read it in.. */\n \tfolio = swap_cache_get_folio(swap);\n \tif (!folio) {\n-\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO)) {\n+\t\tif (si && data_race(si->flags & SWP_SYNCHRONOUS_IO)) {\n \t\t\t/* Direct swapin skipping swap cache & readahead */\n \t\t\tfolio = shmem_swap_alloc_folio(inode, vma, index,\n \t\t\t\t\t\t       index_entry, order, gfp);\ndiff --git a/mm/swap.h b/mm/swap.h\nindex ae97cf9712c5c..d41e6a0e70753 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -310,35 +310,15 @@ static inline unsigned int folio_swap_flags(struct folio *folio)\n {\n \tswp_slot_t swp_slot = swp_entry_to_swp_slot(folio->swap);\n \n+\t/* The folio might not be backed by any physical swap slots\n+\t * (for e.g zswap-backed only).\n+\t */\n+\tif (!swp_slot.val)\n+\t\treturn 0;\n \treturn __swap_slot_to_info(swp_slot)->flags;\n }\n \n-/*\n- * Return the count of contiguous swap entries that share the same\n- * zeromap status as the starting entry. If is_zeromap is not NULL,\n- * it will return the zeromap status of the starting entry.\n- */\n-static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n-\t\tbool *is_zeromap)\n-{\n-\tswp_slot_t slot = swp_entry_to_swp_slot(entry);\n-\tstruct swap_info_struct *sis = __swap_slot_to_info(slot);\n-\tunsigned long start = swp_slot_offset(slot);\n-\tunsigned long end = start + max_nr;\n-\tbool first_bit;\n-\n-\tfirst_bit = test_bit(start, sis->zeromap);\n-\tif (is_zeromap)\n-\t\t*is_zeromap = first_bit;\n-\n-\tif (max_nr <= 1)\n-\t\treturn max_nr;\n-\tif (first_bit)\n-\t\treturn find_next_zero_bit(sis->zeromap, end, start) - start;\n-\telse\n-\t\treturn find_next_bit(sis->zeromap, end, start) - start;\n-}\n-\n+int swap_zeromap_batch(swp_entry_t entry, int max_nr, bool *is_zeromap);\n int non_swapcache_batch(swp_entry_t entry, int max_nr);\n \n #else /* CONFIG_SWAP */\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 1827527e88d33..ad80bf098b63f 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -179,6 +179,10 @@ struct folio *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \tstruct folio *result = NULL;\n \tvoid *shadow = NULL;\n \n+\t/* we might get an unsed entry from cluster readahead - just skip */\n+\tif (!entry.val)\n+\t\treturn NULL;\n+\n \t*new_page_allocated = false;\n \tfor (;;) {\n \t\tint err;\n@@ -213,8 +217,20 @@ struct folio *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\t * Swap entry may have been freed since our caller observed it.\n \t\t */\n \t\terr = swapcache_prepare(entry, 1);\n-\t\tif (!err)\n+\t\tif (!err) {\n+\t\t\t/* This might be invoked by swap_cluster_readahead(), which can\n+\t\t\t * race with shmem_swapin_folio(). The latter might have already\n+\t\t\t * called swap_cache_del_folio(), allowing swapcache_prepare()\n+\t\t\t * to succeed here. This can lead to reading bogus data to populate\n+\t\t\t * the page. To prevent this, skip folio-backed virtual swap slots,\n+\t\t\t * and let caller retry if necessary.\n+\t\t\t */\n+\t\t\tif (vswap_folio_backed(entry, 1)) {\n+\t\t\t\tswapcache_clear(entry, 1);\n+\t\t\t\tgoto put_and_return;\n+\t\t\t}\n \t\t\tbreak;\n+\t\t}\n \t\telse if (err != -EEXIST)\n \t\t\tgoto put_and_return;\n \n@@ -391,11 +407,18 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \tunsigned long offset = slot_offset;\n \tunsigned long start_offset, end_offset;\n \tunsigned long mask;\n-\tstruct swap_info_struct *si = __swap_slot_to_info(slot);\n+\tstruct swap_info_struct *si = swap_slot_tryget_swap_info(slot);\n \tstruct blk_plug plug;\n \tstruct swap_iocb *splug = NULL;\n \tbool page_allocated;\n \n+\t/*\n+\t * The swap entry might not be backed by any physical swap slot. In that\n+\t * case, just skip readahead and bring in the target entry.\n+\t */\n+\tif (!si)\n+\t\tgoto skip;\n+\n \tmask = swapin_nr_pages(offset) - 1;\n \tif (!mask)\n \t\tgoto skip;\n@@ -429,6 +452,8 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \tswap_read_unplug(splug);\n \tlru_add_drain();\t/* Push any new pages onto the LRU now */\n skip:\n+\tif (si)\n+\t\tswap_slot_put_swap_info(si);\n \t/* The page was likely read above, so no need for plugging here */\n \tfolio = __read_swap_cache_async(entry, gfp_mask, mpol, ilx,\n \t\t\t\t\t&page_allocated, false);\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 6c5e46bf40701..1aa29dd220f9a 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1210,14 +1210,6 @@ static void swap_range_free(struct swap_info_struct *si, unsigned long offset,\n {\n \tunsigned long end = offset + nr_entries - 1;\n \tvoid (*swap_slot_free_notify)(struct block_device *, unsigned long);\n-\tunsigned int i;\n-\n-\t/*\n-\t * Use atomic clear_bit operations only on zeromap instead of non-atomic\n-\t * bitmap_clear to prevent adjacent bits corruption due to simultaneous writes.\n-\t */\n-\tfor (i = 0; i < nr_entries; i++)\n-\t\tclear_bit(offset + i, si->zeromap);\n \n \tif (si->flags & SWP_BLKDEV)\n \t\tswap_slot_free_notify =\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex c9ec1a1458b4e..6b200a6bb1160 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -365,10 +365,11 @@ static inline bool can_reclaim_anon_pages(struct mem_cgroup *memcg,\n {\n \tif (memcg == NULL) {\n \t\t/*\n-\t\t * For non-memcg reclaim, is there\n-\t\t * space in any swap device?\n+\t\t * For non-memcg reclaim:\n+\t\t *\n+\t\t * Check if zswap is enabled or if there is space in any swap device?\n \t\t */\n-\t\tif (get_nr_swap_pages() > 0)\n+\t\tif (zswap_is_enabled() || get_nr_swap_pages() > 0)\n \t\t\treturn true;\n \t} else {\n \t\t/* Is the memcg below its swap limit? */\n@@ -2640,12 +2641,12 @@ static void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,\n static bool can_age_anon_pages(struct lruvec *lruvec,\n \t\t\t       struct scan_control *sc)\n {\n-\t/* Aging the anon LRU is valuable if swap is present: */\n-\tif (total_swap_pages > 0)\n-\t\treturn true;\n-\n-\t/* Also valuable if anon pages can be demoted: */\n-\treturn can_demote(lruvec_pgdat(lruvec)->node_id, sc,\n+\t/*\n+\t * Aging the anon LRU is valuable if zswap or physical swap is available or\n+\t * anon pages can be demoted\n+\t */\n+\treturn zswap_is_enabled() || total_swap_pages > 0 ||\n+\t\t\tcan_demote(lruvec_pgdat(lruvec)->node_id, sc,\n \t\t\t  lruvec_memcg(lruvec));\n }\n \ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex 318933071edc6..fb6179ce3ace7 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -11,6 +11,7 @@\n #include <linux/swap_cgroup.h>\n #include <linux/cpuhotplug.h>\n #include <linux/zswap.h>\n+#include \"internal.h\"\n #include \"swap.h\"\n #include \"swap_table.h\"\n \n@@ -54,22 +55,48 @@\n  *\n  * Note that we do not have a reference count field per se - it is derived from\n  * the swap_count and the in_swapcache fields.\n+ *\n+ * III. Backing State\n+ *\n+ * Each virtual swap slot can be backed by:\n+ *\n+ * 1. A slot on a physical swap device (i.e a swapfile or a swap partition).\n+ * 2. A swapped out zero-filled page.\n+ * 3. A compressed object in zswap.\n+ * 4. An in-memory folio, that is not backed by neither a physical swap device\n+ *    nor zswap (i.e only in swap cache). This is used for pages that are\n+ *    rejected by zswap, but not (yet) backed by a physical swap device,\n+ *    (for e.g, due to zswap.writeback = 0), or for pages that were previously\n+ *    stored in zswap, but has since been loaded back into memory (and has its\n+ *    zswap copy invalidated).\n  */\n \n+/* The backing state options of a virtual swap slot */\n+enum swap_type {\n+\tVSWAP_SWAPFILE,\n+\tVSWAP_ZERO,\n+\tVSWAP_ZSWAP,\n+\tVSWAP_FOLIO\n+};\n+\n /**\n  * Swap descriptor - metadata of a swapped out page.\n  *\n  * @slot: The handle to the physical swap slot backing this page.\n  * @zswap_entry: The zswap entry associated with this swap slot.\n- * @swap_cache: The folio in swap cache.\n+ * @swap_cache: The folio in swap cache. If the swap entry backing type is\n+ *              VSWAP_FOLIO, the backend is also stored here.\n  * @shadow: The shadow entry.\n- * @memcgid: The memcg id of the owning memcg, if any.\n  * @swap_count: The number of page table entries that refer to the swap entry.\n+ * @memcgid: The memcg id of the owning memcg, if any.\n  * @in_swapcache: Whether the swap entry is (about to be) pinned in swap cache.\n+ * @type: The backing store type of the swap entry.\n  */\n struct swp_desc {\n-\tswp_slot_t slot;\n-\tstruct zswap_entry *zswap_entry;\n+\tunion {\n+\t\tswp_slot_t slot;\n+\t\tstruct zswap_entry *zswap_entry;\n+\t};\n \tunion {\n \t\tstruct folio *swap_cache;\n \t\tvoid *shadow;\n@@ -78,10 +105,10 @@ struct swp_desc {\n \tunsigned int swap_count;\n \n #ifdef CONFIG_MEMCG\n-\tunsigned short memcgid;\n+\tunsigned short memcgid:16;\n #endif\n-\n-\tbool in_swapcache;\n+\tbool in_swapcache:1;\n+\tenum swap_type type:2;\n };\n \n #define VSWAP_CLUSTER_SHIFT HPAGE_PMD_ORDER\n@@ -266,15 +293,16 @@ static bool cluster_is_alloc_candidate(struct vswap_cluster *cluster)\n \treturn cluster->count + (1 << (cluster->order)) <= VSWAP_CLUSTER_SIZE;\n }\n \n-static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster, int start)\n+static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster,\n+\t\tint start, struct folio *folio)\n {\n \tint i, nr = 1 << cluster->order;\n \tstruct swp_desc *desc;\n \n \tfor (i = 0; i < nr; i++) {\n \t\tdesc = &cluster->descriptors[start + i];\n-\t\tdesc->slot.val = 0;\n-\t\tdesc->zswap_entry = NULL;\n+\t\tdesc->type = VSWAP_FOLIO;\n+\t\tdesc->swap_cache = folio;\n #ifdef CONFIG_MEMCG\n \t\tdesc->memcgid = 0;\n #endif\n@@ -284,7 +312,8 @@ static void __vswap_alloc_from_cluster(struct vswap_cluster *cluster, int start)\n \tcluster->count += nr;\n }\n \n-static unsigned long vswap_alloc_from_cluster(struct vswap_cluster *cluster)\n+static unsigned long vswap_alloc_from_cluster(struct vswap_cluster *cluster,\n+\t\tstruct folio *folio)\n {\n \tint nr = 1 << cluster->order;\n \tunsigned long i = cluster->id ? 0 : nr;\n@@ -303,16 +332,16 @@ static unsigned long vswap_alloc_from_cluster(struct vswap_cluster *cluster)\n \tbitmap_set(cluster->bitmap, i, nr);\n \n \trefcount_add(nr, &cluster->refcnt);\n-\t__vswap_alloc_from_cluster(cluster, i);\n+\t__vswap_alloc_from_cluster(cluster, i, folio);\n \treturn i + (cluster->id << VSWAP_CLUSTER_SHIFT);\n }\n \n /* Allocate a contiguous range of virtual swap slots */\n-static swp_entry_t vswap_alloc(int order)\n+static swp_entry_t vswap_alloc(struct folio *folio)\n {\n \tstruct xa_limit limit = vswap_cluster_map_limit;\n \tstruct vswap_cluster *local, *cluster;\n-\tint nr = 1 << order;\n+\tint order = folio_order(folio), nr = 1 << order;\n \tbool need_caching = true;\n \tu32 cluster_id;\n \tswp_entry_t entry;\n@@ -325,7 +354,7 @@ static swp_entry_t vswap_alloc(int order)\n \tcluster = this_cpu_read(percpu_vswap_cluster.clusters[order]);\n \tif (cluster) {\n \t\tspin_lock(&cluster->lock);\n-\t\tentry.val = vswap_alloc_from_cluster(cluster);\n+\t\tentry.val = vswap_alloc_from_cluster(cluster, folio);\n \t\tneed_caching = !entry.val;\n \n \t\tif (!entry.val || !cluster_is_alloc_candidate(cluster)) {\n@@ -352,7 +381,7 @@ static swp_entry_t vswap_alloc(int order)\n \t\t\tif (!spin_trylock(&cluster->lock))\n \t\t\t\tcontinue;\n \n-\t\t\tentry.val = vswap_alloc_from_cluster(cluster);\n+\t\t\tentry.val = vswap_alloc_from_cluster(cluster, folio);\n \t\t\tlist_del_init(&cluster->list);\n \t\t\tcluster->full = !entry.val || !cluster_is_alloc_candidate(cluster);\n \t\t\tneed_caching = !cluster->full;\n@@ -384,7 +413,7 @@ static swp_entry_t vswap_alloc(int order)\n \t\t\t\tif (!cluster_id)\n \t\t\t\t\tentry.val += nr;\n \t\t\t\t__vswap_alloc_from_cluster(cluster,\n-\t\t\t\t\t(entry.val & VSWAP_CLUSTER_MASK));\n+\t\t\t\t\t(entry.val & VSWAP_CLUSTER_MASK), folio);\n \t\t\t\t/* Mark the allocated range in the bitmap */\n \t\t\t\tbitmap_set(cluster->bitmap, (entry.val & VSWAP_CLUSTER_MASK), nr);\n \t\t\t\tneed_caching = cluster_is_alloc_candidate(cluster);\n@@ -497,6 +526,84 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\t__swap_table_set(ci, ci_off + i, vswap ? vswap + i : 0);\n }\n \n+/*\n+ * Caller needs to handle races with other operations themselves.\n+ *\n+ * Specifically, this function is safe to be called in contexts where the swap\n+ * entry has been added to the swap cache and the associated folio is locked.\n+ * We cannot race with other accessors, and the swap entry is guaranteed to be\n+ * valid the whole time (since swap cache implies one refcount).\n+ *\n+ * We cannot assume that the backends will be of the same type,\n+ * contiguous, etc. We might have a large folio coalesced from subpages with\n+ * mixed backend, which is only rectified when it is reclaimed.\n+ */\n+ static void release_backing(swp_entry_t entry, int nr)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tunsigned long flush_nr, phys_swap_start = 0, phys_swap_end = 0;\n+\tunsigned int phys_swap_type = 0;\n+\tbool need_flushing_phys_swap = false;\n+\tswp_slot_t flush_slot;\n+\tint i;\n+\n+\tVM_WARN_ON(!entry.val);\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\n+\t\t/*\n+\t\t * We batch contiguous physical swap slots for more efficient\n+\t\t * freeing.\n+\t\t */\n+\t\tif (phys_swap_start != phys_swap_end &&\n+\t\t\t\t(desc->type != VSWAP_SWAPFILE ||\n+\t\t\t\t\tswp_slot_type(desc->slot) != phys_swap_type ||\n+\t\t\t\t\tswp_slot_offset(desc->slot) != phys_swap_end)) {\n+\t\t\tneed_flushing_phys_swap = true;\n+\t\t\tflush_slot = swp_slot(phys_swap_type, phys_swap_start);\n+\t\t\tflush_nr = phys_swap_end - phys_swap_start;\n+\t\t\tphys_swap_start = phys_swap_end = 0;\n+\t\t}\n+\n+\t\tif (desc->type == VSWAP_ZSWAP && desc->zswap_entry) {\n+\t\t\tzswap_entry_free(desc->zswap_entry);\n+\t\t} else if (desc->type == VSWAP_SWAPFILE) {\n+\t\t\tif (!phys_swap_start) {\n+\t\t\t\t/* start a new contiguous range of phys swap */\n+\t\t\t\tphys_swap_start = swp_slot_offset(desc->slot);\n+\t\t\t\tphys_swap_end = phys_swap_start + 1;\n+\t\t\t\tphys_swap_type = swp_slot_type(desc->slot);\n+\t\t\t} else {\n+\t\t\t\t/* extend the current contiguous range of phys swap */\n+\t\t\t\tphys_swap_end++;\n+\t\t\t}\n+\t\t}\n+\n+\t\tdesc->slot.val = 0;\n+\n+\t\tif (need_flushing_phys_swap) {\n+\t\t\tspin_unlock(&cluster->lock);\n+\t\t\tcluster = NULL;\n+\t\t\tswap_slot_free_nr(flush_slot, flush_nr);\n+\t\t\tneed_flushing_phys_swap = false;\n+\t\t}\n+\t}\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\t/* Flush any remaining physical swap range */\n+\tif (phys_swap_start) {\n+\t\tflush_slot = swp_slot(phys_swap_type, phys_swap_start);\n+\t\tflush_nr = phys_swap_end - phys_swap_start;\n+\t\tswap_slot_free_nr(flush_slot, flush_nr);\n+\t}\n+ }\n+\n /*\n  * Entered with the cluster locked, but might unlock the cluster.\n  * This is because several operations, such as releasing physical swap slots\n@@ -516,35 +623,21 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n static void vswap_free(struct vswap_cluster *cluster, struct swp_desc *desc,\n \tswp_entry_t entry)\n {\n-\tstruct zswap_entry *zswap_entry;\n-\tswp_slot_t slot;\n-\n \t/* Clear shadow if present */\n \tif (xa_is_value(desc->shadow))\n \t\tdesc->shadow = NULL;\n-\n-\tslot = desc->slot;\n-\tdesc->slot.val = 0;\n-\n-\tzswap_entry = desc->zswap_entry;\n-\tif (zswap_entry) {\n-\t\tdesc->zswap_entry = NULL;\n-\t\tzswap_entry_free(zswap_entry);\n-\t}\n \tspin_unlock(&cluster->lock);\n \n+\trelease_backing(entry, 1);\n \tmem_cgroup_uncharge_swap(entry, 1);\n \n-\tif (slot.val)\n-\t\tswap_slot_free_nr(slot, 1);\n-\n-\tspin_lock(&cluster->lock);\n \t/* erase forward mapping and release the virtual slot for reallocation */\n+\tspin_lock(&cluster->lock);\n \trelease_vswap_slot(cluster, entry.val);\n }\n \n /**\n- * folio_alloc_swap - allocate swap space for a folio.\n+ * folio_alloc_swap - allocate virtual swap space for a folio.\n  * @folio: the folio.\n  *\n  * Return: 0, if the allocation succeeded, -ENOMEM, if the allocation failed.\n@@ -552,38 +645,77 @@ static void vswap_free(struct vswap_cluster *cluster, struct swp_desc *desc,\n int folio_alloc_swap(struct folio *folio)\n {\n \tstruct vswap_cluster *cluster = NULL;\n-\tstruct swap_info_struct *si;\n-\tstruct swap_cluster_info *ci;\n-\tint i, nr = folio_nr_pages(folio), order = folio_order(folio);\n+\tint i, nr = folio_nr_pages(folio);\n \tstruct swp_desc *desc;\n \tswp_entry_t entry;\n-\tswp_slot_t slot;\n \n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n \tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n \n-\tentry = vswap_alloc(folio_order(folio));\n+\tentry = vswap_alloc(folio);\n \tif (!entry.val)\n \t\treturn -ENOMEM;\n \n \t/*\n-\t * XXX: for now, we always allocate a physical swap slot for each virtual\n-\t * swap slot, and their lifetime are coupled. This will change once we\n-\t * decouple virtual swap slots from their backing states, and only allocate\n-\t * physical swap slots for them on demand (i.e on zswap writeback, or\n-\t * fallback from zswap store failure).\n+\t * XXX: for now, we charge towards the memory cgroup's swap limit on virtual\n+\t * swap slots allocation. This will be changed soon - we will only charge on\n+\t * physical swap slots allocation.\n \t */\n-\tif (swap_slot_alloc(&slot, order)) {\n+\tif (mem_cgroup_try_charge_swap(folio, entry)) {\n+\t\trcu_read_lock();\n \t\tfor (i = 0; i < nr; i++) {\n \t\t\tdesc = vswap_iter(&cluster, entry.val + i);\n \t\t\tVM_WARN_ON(!desc);\n \t\t\tvswap_free(cluster, desc, (swp_entry_t){ entry.val + i });\n \t\t}\n \t\tspin_unlock(&cluster->lock);\n+\t\trcu_read_unlock();\n+\t\tatomic_add(nr, &vswap_alloc_reject);\n \t\tentry.val = 0;\n \t\treturn -ENOMEM;\n \t}\n \n+\tswap_cache_add_folio(folio, entry, NULL);\n+\n+\treturn 0;\n+}\n+\n+/**\n+ * vswap_alloc_swap_slot - allocate physical swap space for a folio that is\n+ *                         already associated with virtual swap slots.\n+ * @folio: folio we want to allocate physical swap space for.\n+ *\n+ * Note that this does NOT release existing swap backends of the folio.\n+ * Callers need to handle this themselves.\n+\n+ * Return: true if the folio is now backed by physical swap slots, false\n+ * otherwise.\n+ */\n+bool vswap_alloc_swap_slot(struct folio *folio)\n+{\n+\tint i, nr = folio_nr_pages(folio);\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swap_info_struct *si;\n+\tstruct swap_cluster_info *ci;\n+\tswp_slot_t slot = { .val = 0 };\n+\tswp_entry_t entry = folio->swap;\n+\tstruct swp_desc *desc;\n+\tbool fallback = false;\n+\n+\t/*\n+\t * We might have already allocated a backing physical swap slot in past\n+\t * attempts (for instance, when we disable zswap). If the entire range is\n+\t * already swapfile-backed we can skip swapfile case.\n+\t */\n+\tif (vswap_swapfile_backed(entry, nr))\n+\t\treturn true;\n+\n+\tif (swap_slot_alloc(&slot, folio_order(folio)))\n+\t\treturn false;\n+\n+\tif (!slot.val)\n+\t\treturn false;\n+\n \t/* establish the vrtual <-> physical swap slots linkages. */\n \tsi = __swap_slot_to_info(slot);\n \tci = swap_cluster_lock(si, swp_slot_offset(slot));\n@@ -595,29 +727,29 @@ int folio_alloc_swap(struct folio *folio)\n \t\tdesc = vswap_iter(&cluster, entry.val + i);\n \t\tVM_WARN_ON(!desc);\n \n+\t\tif (desc->type == VSWAP_FOLIO) {\n+\t\t\t/* case 1: fallback from zswap store failure */\n+\t\t\tfallback = true;\n+\t\t\tif (!folio)\n+\t\t\t\tfolio = desc->swap_cache;\n+\t\t\telse\n+\t\t\t\tVM_WARN_ON(folio != desc->swap_cache);\n+\t\t} else {\n+\t\t\t/*\n+\t\t\t * Case 2: zswap writeback.\n+\t\t\t *\n+\t\t\t * No need to free zswap entry here - it will be freed once zswap\n+\t\t\t * writeback suceeds.\n+\t\t\t */\n+\t\t\tVM_WARN_ON(desc->type != VSWAP_ZSWAP);\n+\t\t\tVM_WARN_ON(fallback);\n+\t\t}\n+\t\tdesc->type = VSWAP_SWAPFILE;\n \t\tdesc->slot.val = slot.val + i;\n \t}\n-\tif (cluster)\n-\t\tspin_unlock(&cluster->lock);\n+\tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n-\n-\t/*\n-\t * XXX: for now, we charge towards the memory cgroup's swap limit on virtual\n-\t * swap slots allocation. This is acceptable because as noted above, each\n-\t * virtual swap slot corresponds to a physical swap slot. Once we have\n-\t * decoupled virtual and physical swap slots, we will only charge when we\n-\t * actually allocate a physical swap slot.\n-\t */\n-\tif (mem_cgroup_try_charge_swap(folio, entry))\n-\t\tgoto out_free;\n-\n-\tswap_cache_add_folio(folio, entry, NULL);\n-\n-\treturn 0;\n-\n-out_free:\n-\tput_swap_folio(folio, entry);\n-\treturn -ENOMEM;\n+\treturn true;\n }\n \n /**\n@@ -625,7 +757,9 @@ int folio_alloc_swap(struct folio *folio)\n  *                         virtual swap slot.\n  * @entry: the virtual swap slot.\n  *\n- * Return: the physical swap slot corresponding to the virtual swap slot.\n+ * Return: the physical swap slot corresponding to the virtual swap slot, if\n+ * exists, or the zero physical swap slot if the virtual swap slot is not\n+ * backed by any physical slot on a swapfile.\n  */\n swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry)\n {\n@@ -644,7 +778,10 @@ swp_slot_t swp_entry_to_swp_slot(swp_entry_t entry)\n \t\treturn (swp_slot_t){0};\n \t}\n \n-\tslot = desc->slot;\n+\tif (desc->type != VSWAP_SWAPFILE)\n+\t\tslot.val = 0;\n+\telse\n+\t\tslot = desc->slot;\n \tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n \n@@ -962,6 +1099,293 @@ int non_swapcache_batch(swp_entry_t entry, int max_nr)\n \treturn i;\n }\n \n+/**\n+ * vswap_store_folio - set a folio as the backing of a range of virtual swap\n+ *                     slots.\n+ * @entry: the first virtual swap slot in the range.\n+ * @folio: the folio.\n+ */\n+void vswap_store_folio(swp_entry_t entry, struct folio *folio)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tint i, nr = folio_nr_pages(folio);\n+\tstruct swp_desc *desc;\n+\n+\tVM_BUG_ON(!folio_test_locked(folio));\n+\tVM_BUG_ON(folio->swap.val != entry.val);\n+\n+\trelease_backing(entry, nr);\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\t\tdesc->type = VSWAP_FOLIO;\n+\t\tdesc->swap_cache = folio;\n+\t}\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+}\n+\n+/**\n+ * swap_zeromap_folio_set - mark a range of virtual swap slots corresponding to\n+ *                          a folio as zero-filled.\n+ * @folio: the folio\n+ */\n+void swap_zeromap_folio_set(struct folio *folio)\n+{\n+\tstruct obj_cgroup *objcg = get_obj_cgroup_from_folio(folio);\n+\tstruct vswap_cluster *cluster = NULL;\n+\tswp_entry_t entry = folio->swap;\n+\tint i, nr = folio_nr_pages(folio);\n+\tstruct swp_desc *desc;\n+\n+\tVM_BUG_ON(!folio_test_locked(folio));\n+\tVM_BUG_ON(!entry.val);\n+\n+\trelease_backing(entry, nr);\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tVM_WARN_ON(!desc);\n+\t\tdesc->type = VSWAP_ZERO;\n+\t}\n+\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\n+\tcount_vm_events(SWPOUT_ZERO, nr);\n+\tif (objcg) {\n+\t\tcount_objcg_events(objcg, SWPOUT_ZERO, nr);\n+\t\tobj_cgroup_put(objcg);\n+\t}\n+}\n+\n+/*\n+ * Iterate through the entire range of virtual swap slots, returning the\n+ * longest contiguous range of slots starting from the first slot that satisfies:\n+ *\n+ * 1. If the first slot is zero-mapped, the entire range should be\n+ *    zero-mapped.\n+ * 2. If the first slot is backed by a swapfile, the entire range should\n+ *    be backed by a range of contiguous swap slots on the same swapfile.\n+ * 3. If the first slot is zswap-backed, the entire range should be\n+ *    zswap-backed.\n+ * 4. If the first slot is backed by a folio, the entire range should\n+ *    be backed by the same folio.\n+ *\n+ * Note that this check is racy unless we can ensure that the entire range\n+ * has their backing state stable - for instance, if the caller was the one\n+ * who set the swap cache pin.\n+ */\n+static int vswap_check_backing(swp_entry_t entry, enum swap_type *type, int nr)\n+{\n+\tunsigned int swapfile_type;\n+\tstruct vswap_cluster *cluster = NULL;\n+\tenum swap_type first_type;\n+\tstruct swp_desc *desc;\n+\tpgoff_t first_offset;\n+\tstruct folio *folio;\n+\tint i = 0;\n+\n+\tif (!entry.val)\n+\t\treturn 0;\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tif (!desc)\n+\t\t\tgoto done;\n+\n+\t\tif (!i) {\n+\t\t\tfirst_type = desc->type;\n+\t\t\tif (first_type == VSWAP_SWAPFILE) {\n+\t\t\t\tswapfile_type = swp_slot_type(desc->slot);\n+\t\t\t\tfirst_offset = swp_slot_offset(desc->slot);\n+\t\t\t} else if (first_type == VSWAP_FOLIO) {\n+\t\t\t\tfolio = desc->swap_cache;\n+\t\t\t}\n+\t\t} else if (desc->type != first_type) {\n+\t\t\tgoto done;\n+\t\t} else if (first_type == VSWAP_SWAPFILE &&\n+\t\t\t\t(swp_slot_type(desc->slot) != swapfile_type ||\n+\t\t\t\t\tswp_slot_offset(desc->slot) != first_offset + i)) {\n+\t\t\tgoto done;\n+\t\t} else if (first_type == VSWAP_FOLIO && desc->swap_cache != folio) {\n+\t\t\tgoto done;\n+\t\t}\n+\t}\n+done:\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\tif (type)\n+\t\t*type = first_type;\n+\treturn i;\n+}\n+\n+/**\n+ * vswap_swapfile_backed - check if the virtual swap slots are backed by physical\n+ *                         swap slots.\n+ * @entry: the first entry in the range.\n+ * @nr: the number of entries in the range.\n+ */\n+bool vswap_swapfile_backed(swp_entry_t entry, int nr)\n+{\n+\tenum swap_type type;\n+\n+\treturn vswap_check_backing(entry, &type, nr) == nr\n+\t\t\t\t&& type == VSWAP_SWAPFILE;\n+}\n+\n+/**\n+ * vswap_folio_backed - check if the virtual swap slots are backed by in-memory\n+ *                      pages.\n+ * @entry: the first virtual swap slot in the range.\n+ * @nr: the number of slots in the range.\n+ */\n+bool vswap_folio_backed(swp_entry_t entry, int nr)\n+{\n+\tenum swap_type type;\n+\n+\treturn vswap_check_backing(entry, &type, nr) == nr && type == VSWAP_FOLIO;\n+}\n+\n+/**\n+ * vswap_can_swapin_thp - check if the swap entries can be swapped in as a THP.\n+ * @entry: the first virtual swap slot in the range.\n+ * @nr: the number of slots in the range.\n+ *\n+ * For now, we can only swap in a THP if the entire range is zero-filled, or if\n+ * the entire range is backed by a contiguous range of physical swap slots on a\n+ * swapfile.\n+ */\n+bool vswap_can_swapin_thp(swp_entry_t entry, int nr)\n+{\n+\tenum swap_type type;\n+\n+\treturn vswap_check_backing(entry, &type, nr) == nr &&\n+\t\t(type == VSWAP_ZERO || type == VSWAP_SWAPFILE);\n+}\n+\n+/**\n+ * swap_move - increment the swap slot by delta, checking the backing state and\n+ *             return 0 if the backing state does not match (i.e wrong backing\n+ *             state type, or wrong offset on the backing stores).\n+ * @entry: the original virtual swap slot.\n+ * @delta: the offset to increment the original slot.\n+ *\n+ * Note that this function is racy unless we can pin the backing state of these\n+ * swap slots down with swapcache_prepare().\n+ *\n+ * Caller should only rely on this function as a best-effort hint otherwise,\n+ * and should double-check after ensuring the whole range is pinned down.\n+ *\n+ * Return: the incremented virtual swap slot if the backing state matches, or\n+ *         0 if the backing state does not match.\n+ */\n+swp_entry_t swap_move(swp_entry_t entry, long delta)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc, *next_desc;\n+\tswp_entry_t next_entry;\n+\tstruct folio *folio = NULL, *next_folio = NULL;\n+\tenum swap_type type, next_type;\n+\tswp_slot_t slot = {0}, next_slot = {0};\n+\n+\tnext_entry.val = entry.val + delta;\n+\n+\trcu_read_lock();\n+\n+\t/* Look up first descriptor and get its type and backing store */\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tif (!desc) {\n+\t\trcu_read_unlock();\n+\t\treturn (swp_entry_t){0};\n+\t}\n+\n+\ttype = desc->type;\n+\tif (type == VSWAP_ZSWAP) {\n+\t\t/* zswap not supported for move */\n+\t\tspin_unlock(&cluster->lock);\n+\t\trcu_read_unlock();\n+\t\treturn (swp_entry_t){0};\n+\t}\n+\tif (type == VSWAP_FOLIO)\n+\t\tfolio = desc->swap_cache;\n+\telse if (type == VSWAP_SWAPFILE)\n+\t\tslot = desc->slot;\n+\n+\t/* Look up second descriptor and get its type and backing store */\n+\tnext_desc = vswap_iter(&cluster, next_entry.val);\n+\tif (!next_desc) {\n+\t\trcu_read_unlock();\n+\t\treturn (swp_entry_t){0};\n+\t}\n+\n+\tnext_type = next_desc->type;\n+\tif (next_type == VSWAP_FOLIO)\n+\t\tnext_folio = next_desc->swap_cache;\n+\telse if (next_type == VSWAP_SWAPFILE)\n+\t\tnext_slot = next_desc->slot;\n+\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\n+\trcu_read_unlock();\n+\n+\t/* Check if types match */\n+\tif (next_type != type)\n+\t\treturn (swp_entry_t){0};\n+\n+\t/* Check backing state consistency */\n+\tif (type == VSWAP_SWAPFILE &&\n+\t\t\t(swp_slot_type(next_slot) != swp_slot_type(slot) ||\n+\t\t\t\tswp_slot_offset(next_slot) !=\n+\t\t\t\t\t\t\tswp_slot_offset(slot) + delta))\n+\t\treturn (swp_entry_t){0};\n+\n+\tif (type == VSWAP_FOLIO && next_folio != folio)\n+\t\treturn (swp_entry_t){0};\n+\n+\treturn next_entry;\n+}\n+\n+/*\n+ * Return the count of contiguous swap entries that share the same\n+ * VSWAP_ZERO status as the starting entry. If is_zeromap is not NULL,\n+ * it will return the VSWAP_ZERO status of the starting entry.\n+ */\n+int swap_zeromap_batch(swp_entry_t entry, int max_nr, bool *is_zeromap)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tint i = 0;\n+\tbool is_zero = false;\n+\n+\tVM_WARN_ON(!entry.val);\n+\n+\trcu_read_lock();\n+\tfor (i = 0; i < max_nr; i++) {\n+\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tif (!desc)\n+\t\t\tgoto done;\n+\n+\t\tif (!i)\n+\t\t\tis_zero = (desc->type == VSWAP_ZERO);\n+\t\telse if ((desc->type == VSWAP_ZERO) != is_zero)\n+\t\t\tgoto done;\n+\t}\n+done:\n+\tif (cluster)\n+\t\tspin_unlock(&cluster->lock);\n+\trcu_read_unlock();\n+\tif (i && is_zeromap)\n+\t\t*is_zeromap = is_zero;\n+\n+\treturn i;\n+}\n+\n /**\n  * free_swap_and_cache_nr() - Release a swap count on range of swap entries and\n  *                            reclaim their cache if no more references remain.\n@@ -1028,11 +1452,6 @@ bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si)\n \tstruct vswap_cluster *cluster;\n \tswp_slot_t slot;\n \n-\tslot = swp_entry_to_swp_slot(entry);\n-\t*si = swap_slot_tryget_swap_info(slot);\n-\tif (!*si)\n-\t\treturn false;\n-\n \t/*\n \t * Ensure the cluster and its associated data structures (swap cache etc.)\n \t * remain valid.\n@@ -1041,11 +1460,30 @@ bool tryget_swap_entry(swp_entry_t entry, struct swap_info_struct **si)\n \tcluster = xa_load(&vswap_cluster_map, VSWAP_CLUSTER_IDX(entry));\n \tif (!cluster || !refcount_inc_not_zero(&cluster->refcnt)) {\n \t\trcu_read_unlock();\n-\t\tswap_slot_put_swap_info(*si);\n \t\t*si = NULL;\n \t\treturn false;\n \t}\n \trcu_read_unlock();\n+\n+\tslot = swp_entry_to_swp_slot(entry);\n+\t/*\n+\t * Note that this function does not provide any guarantee that the virtual\n+\t * swap slot's backing state will be stable. This has several implications:\n+\t *\n+\t * 1. We have to obtain a reference to the swap device itself, because we\n+\t * need swap device's metadata in certain scenarios, for example when we\n+\t * need to inspect the swap device flag in do_swap_page().\n+\t *\n+\t * 2. The swap device we are looking up here might be outdated by the time we\n+\t * return to the caller. It is perfectly OK, if the swap_info_struct is only\n+\t * used in a best-effort manner (i.e optimization). If we need the precise\n+\t * backing state, we need to re-check after the entry is pinned in swapcache.\n+\t */\n+\tif (slot.val)\n+\t\t*si = swap_slot_tryget_swap_info(slot);\n+\telse\n+\t\t*si = NULL;\n+\n \treturn true;\n }\n \n@@ -1288,7 +1726,7 @@ void swap_cache_add_folio(struct folio *folio, swp_entry_t entry, void **shadowp\n \t\told = desc->shadow;\n \n \t\t/* Warn if slot is already occupied by a folio */\n-\t\tVM_WARN_ON_FOLIO(old && !xa_is_value(old), folio);\n+\t\tVM_WARN_ON_FOLIO(old && !xa_is_value(old) && old != folio, folio);\n \n \t\t/* Save shadow if found and not yet saved */\n \t\tif (shadowp && xa_is_value(old) && !*shadowp)\n@@ -1415,29 +1853,22 @@ void __swap_cache_replace_folio(struct folio *old, struct folio *new)\n  * @entry: the zswap entry to store\n  *\n  * Stores a zswap entry in the swap descriptor for the given swap entry.\n- * The cluster is locked during the store operation.\n- *\n- * Return: the old zswap entry if one existed, NULL otherwise\n+ * Releases the old backend if one existed.\n  */\n-void *zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry)\n+void zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry)\n {\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n-\tvoid *old;\n+\n+\trelease_backing(swpentry, 1);\n \n \trcu_read_lock();\n \tdesc = vswap_iter(&cluster, swpentry.val);\n-\tif (!desc) {\n-\t\trcu_read_unlock();\n-\t\treturn NULL;\n-\t}\n-\n-\told = desc->zswap_entry;\n+\tVM_WARN_ON(!desc);\n \tdesc->zswap_entry = entry;\n+\tdesc->type = VSWAP_ZSWAP;\n \tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n-\n-\treturn old;\n }\n \n /**\n@@ -1452,6 +1883,7 @@ void *zswap_entry_load(swp_entry_t swpentry)\n {\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n+\tenum swap_type type;\n \tvoid *zswap_entry;\n \n \trcu_read_lock();\n@@ -1461,41 +1893,15 @@ void *zswap_entry_load(swp_entry_t swpentry)\n \t\treturn NULL;\n \t}\n \n+\ttype = desc->type;\n \tzswap_entry = desc->zswap_entry;\n \tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n \n-\treturn zswap_entry;\n-}\n-\n-/**\n- * zswap_entry_erase - erase a zswap entry for a swap entry\n- * @swpentry: the swap entry\n- *\n- * Erases the zswap entry from the swap descriptor for the given swap entry.\n- * The cluster is locked during the erase operation.\n- *\n- * Return: the zswap entry that was erased, NULL if none existed\n- */\n-void *zswap_entry_erase(swp_entry_t swpentry)\n-{\n-\tstruct vswap_cluster *cluster = NULL;\n-\tstruct swp_desc *desc;\n-\tvoid *old;\n-\n-\trcu_read_lock();\n-\tdesc = vswap_iter(&cluster, swpentry.val);\n-\tif (!desc) {\n-\t\trcu_read_unlock();\n+\tif (type != VSWAP_ZSWAP)\n \t\treturn NULL;\n-\t}\n \n-\told = desc->zswap_entry;\n-\tdesc->zswap_entry = NULL;\n-\tspin_unlock(&cluster->lock);\n-\trcu_read_unlock();\n-\n-\treturn old;\n+\treturn zswap_entry;\n }\n \n bool zswap_empty(swp_entry_t swpentry)\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex e46349f9c90bb..c5e1d252cb463 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -991,8 +991,9 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n {\n \tstruct folio *folio;\n \tstruct mempolicy *mpol;\n-\tbool folio_was_allocated;\n+\tbool folio_was_allocated, phys_swap_alloced = false;\n \tstruct swap_info_struct *si;\n+\tstruct zswap_entry *new_entry = NULL;\n \tint ret = 0;\n \n \t/* try to allocate swap cache folio */\n@@ -1027,18 +1028,23 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \t * old compressed data. Only when this is successful can the entry\n \t * be dereferenced.\n \t */\n-\tif (entry != zswap_entry_load(swpentry)) {\n+\tnew_entry = zswap_entry_load(swpentry);\n+\tif (entry != new_entry) {\n \t\tret = -ENOMEM;\n \t\tgoto out;\n \t}\n \n+\tif (!vswap_alloc_swap_slot(folio)) {\n+\t\tret = -ENOMEM;\n+\t\tgoto out;\n+\t}\n+\tphys_swap_alloced = true;\n+\n \tif (!zswap_decompress(entry, folio)) {\n \t\tret = -EIO;\n \t\tgoto out;\n \t}\n \n-\tzswap_entry_erase(swpentry);\n-\n \tcount_vm_event(ZSWPWB);\n \tif (entry->objcg)\n \t\tcount_objcg_events(entry->objcg, ZSWPWB, 1);\n@@ -1056,6 +1062,8 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \n out:\n \tif (ret && ret != -EEXIST) {\n+\t\tif (phys_swap_alloced)\n+\t\t\tzswap_entry_store(swpentry, new_entry);\n \t\tswap_cache_del_folio(folio);\n \t\tfolio_unlock(folio);\n \t}\n@@ -1401,7 +1409,7 @@ static bool zswap_store_page(struct page *page,\n \t\t\t     struct zswap_pool *pool)\n {\n \tswp_entry_t page_swpentry = page_swap_entry(page);\n-\tstruct zswap_entry *entry, *old;\n+\tstruct zswap_entry *entry;\n \n \t/* allocate entry */\n \tentry = zswap_entry_cache_alloc(GFP_KERNEL, page_to_nid(page));\n@@ -1413,15 +1421,12 @@ static bool zswap_store_page(struct page *page,\n \tif (!zswap_compress(page, entry, pool))\n \t\tgoto compress_failed;\n \n-\told = zswap_entry_store(page_swpentry, entry);\n-\n \t/*\n \t * We may have had an existing entry that became stale when\n \t * the folio was redirtied and now the new version is being\n-\t * swapped out. Get rid of the old.\n+\t * swapped out. zswap_entry_store() will get rid of the old.\n \t */\n-\tif (old)\n-\t\tzswap_entry_free(old);\n+\tzswap_entry_store(page_swpentry, entry);\n \n \t/*\n \t * The entry is successfully compressed and stored in the tree, there is\n@@ -1533,18 +1538,13 @@ bool zswap_store(struct folio *folio)\n \t * the possibly stale entries which were previously stored at the\n \t * offsets corresponding to each page of the folio. Otherwise,\n \t * writeback could overwrite the new data in the swapfile.\n+\t *\n+\t * The only exception is if we still have a full contiguous\n+\t * range of physical swap slots backing the folio. Keep them for\n+\t * fallback disk swapping.\n \t */\n-\tif (!ret) {\n-\t\tunsigned type = swp_type(swp);\n-\t\tpgoff_t offset = swp_offset(swp);\n-\t\tstruct zswap_entry *entry;\n-\n-\t\tfor (index = 0; index < nr_pages; ++index) {\n-\t\t\tentry = zswap_entry_erase(swp_entry(type, offset + index));\n-\t\t\tif (entry)\n-\t\t\t\tzswap_entry_free(entry);\n-\t\t}\n-\t}\n+\tif (!ret && !vswap_swapfile_backed(swp, nr_pages))\n+\t\tvswap_store_folio(swp, folio);\n \n \treturn ret;\n }\n@@ -1619,8 +1619,7 @@ int zswap_load(struct folio *folio)\n \t */\n \tif (swapcache) {\n \t\tfolio_mark_dirty(folio);\n-\t\tzswap_entry_erase(swp);\n-\t\tzswap_entry_free(entry);\n+\t\tvswap_store_folio(swp, folio);\n \t}\n \n \tfolio_unlock(folio);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that the zswap shrinker should not start if there is no physical swap slots available, and added a check to get_nr_swap_pages() in vmscan.c",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When swap is virtualized, we no longer pre-allocate a slot on swapfile\nfor each zswap entry. Do not start the zswap shrinker if there is no\nphysical swap slots available.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n mm/zswap.c | 8 ++++++++\n 1 file changed, 8 insertions(+)\n\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex c5e1d252cb463..9d1822753d321 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -1211,6 +1211,14 @@ static unsigned long zswap_shrinker_count(struct shrinker *shrinker,\n \tif (!zswap_shrinker_enabled || !mem_cgroup_zswap_writeback_enabled(memcg))\n \t\treturn 0;\n \n+\t/*\n+\t * When swap is virtualized, we do not have any swap slots on swapfile\n+\t * preallocated for zswap objects. If there is no slot available, we\n+\t * cannot writeback and should just bail out here.\n+\t */\n+\tif (!get_nr_swap_pages())\n+\t\treturn 0;\n+\n \t/*\n \t * The shrinker resumes swap writeback, which will enter block\n \t * and may enter fs. XXX: Harmonize with vmscan.c __GFP_FS\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about unnecessary pinning of swap entries during readahead, explained that the target entry is already pinned by the caller, and added a function to check if two virtual swap entries belong to the same vswap cluster.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "added new functionality"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When we perform swap readahead, the target entry is already pinned by\nthe caller. No need to pin swap entries in the readahead window that\nbelongs in the same virtual swap cluster as the target swap entry.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n mm/swap.h       |  1 +\n mm/swap_state.c | 22 +++++++++-------------\n mm/vswap.c      | 10 ++++++++++\n 3 files changed, 20 insertions(+), 13 deletions(-)\n\ndiff --git a/mm/swap.h b/mm/swap.h\nindex d41e6a0e70753..08a6369a6dfad 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -213,6 +213,7 @@ void swap_cache_lock(swp_entry_t entry);\n void swap_cache_unlock(swp_entry_t entry);\n void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\t\t   unsigned long vswap, int nr);\n+bool vswap_same_cluster(swp_entry_t entry1, swp_entry_t entry2);\n \n static inline struct address_space *swap_address_space(swp_entry_t entry)\n {\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex ad80bf098b63f..e8e0905c7723f 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -553,22 +553,18 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \t\tpte_unmap(pte);\n \t\tpte = NULL;\n \t\t/*\n-\t\t * Readahead entry may come from a device that we are not\n-\t\t * holding a reference to, try to grab a reference, or skip.\n-\t\t *\n-\t\t * XXX: for now, always try to pin the swap entries in the\n-\t\t * readahead window to avoid the annoying conversion to physical\n-\t\t * swap slots. Once we move all swap metadata to virtual swap\n-\t\t * layer, we can simply compare the clusters of the target\n-\t\t * swap entry and the current swap entry, and pin the latter\n-\t\t * swap entry's cluster if it differ from the former's.\n+\t\t * The target entry is already pinned - if the readahead entry\n+\t\t * belongs to the same cluster, it's already protected.\n \t\t */\n-\t\tswapoff_locked = tryget_swap_entry(entry, &si);\n-\t\tif (!swapoff_locked)\n-\t\t\tcontinue;\n+\t\tif (!vswap_same_cluster(entry, targ_entry)) {\n+\t\t\tswapoff_locked = tryget_swap_entry(entry, &si);\n+\t\t\tif (!swapoff_locked)\n+\t\t\t\tcontinue;\n+\t\t}\n \t\tfolio = __read_swap_cache_async(entry, gfp_mask, mpol, ilx,\n \t\t\t\t\t\t&page_allocated, false);\n-\t\tput_swap_entry(entry, si);\n+\t\tif (swapoff_locked)\n+\t\t\tput_swap_entry(entry, si);\n \t\tif (!folio)\n \t\t\tcontinue;\n \t\tif (page_allocated) {\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex fb6179ce3ace7..7563107eb8eee 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -1503,6 +1503,16 @@ void put_swap_entry(swp_entry_t entry, struct swap_info_struct *si)\n \trcu_read_unlock();\n }\n \n+/*\n+ * Check if two virtual swap entries belong to the same vswap cluster.\n+ * Useful for optimizing readahead when entries in the same cluster\n+ * share protection from a pinned target entry.\n+ */\n+bool vswap_same_cluster(swp_entry_t entry1, swp_entry_t entry2)\n+{\n+\treturn VSWAP_CLUSTER_IDX(entry1) == VSWAP_CLUSTER_IDX(entry2);\n+}\n+\n static int vswap_cpu_dead(unsigned int cpu)\n {\n \tstruct percpu_vswap_cluster *percpu_cluster;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about memory overhead by removing the zeromap bitmap from physical swapfile, which will save 1 bit per swap page in terms of memory usage.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "behavioral change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zero swap entries are now treated as a separate, decoupled backend in\nthe virtual swap layer. The zeromap bitmap of physical swapfile is no\nlonger used - remove it. This does not have any behavioral change, and\nsave 1 bit per swap page in terms of memory overhead.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h |  1 -\n mm/swapfile.c        | 30 +++++-------------------------\n 2 files changed, 5 insertions(+), 26 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 54df972608047..9cd45eab313f8 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -260,7 +260,6 @@ struct swap_info_struct {\n \tsigned char\ttype;\t\t/* strange name for an index */\n \tunsigned int\tmax;\t\t/* extent of the swap_map */\n \tunsigned char *swap_map;\t/* vmalloc'ed array of usage counts */\n-\tunsigned long *zeromap;\t\t/* kvmalloc'ed bitmap to track zero pages */\n \tstruct swap_cluster_info *cluster_info; /* cluster info. Only for SSD */\n \tstruct list_head free_clusters; /* free clusters list */\n \tstruct list_head full_clusters; /* full clusters list */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 1aa29dd220f9a..e1cb01b821ff3 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -2317,8 +2317,7 @@ static int setup_swap_extents(struct swap_info_struct *sis, sector_t *span)\n \n static void setup_swap_info(struct swap_info_struct *si, int prio,\n \t\t\t    unsigned char *swap_map,\n-\t\t\t    struct swap_cluster_info *cluster_info,\n-\t\t\t    unsigned long *zeromap)\n+\t\t\t    struct swap_cluster_info *cluster_info)\n {\n \tsi->prio = prio;\n \t/*\n@@ -2329,7 +2328,6 @@ static void setup_swap_info(struct swap_info_struct *si, int prio,\n \tsi->avail_list.prio = -si->prio;\n \tsi->swap_map = swap_map;\n \tsi->cluster_info = cluster_info;\n-\tsi->zeromap = zeromap;\n }\n \n static void _enable_swap_info(struct swap_info_struct *si)\n@@ -2347,12 +2345,11 @@ static void _enable_swap_info(struct swap_info_struct *si)\n \n static void enable_swap_info(struct swap_info_struct *si, int prio,\n \t\t\t\tunsigned char *swap_map,\n-\t\t\t\tstruct swap_cluster_info *cluster_info,\n-\t\t\t\tunsigned long *zeromap)\n+\t\t\t\tstruct swap_cluster_info *cluster_info)\n {\n \tspin_lock(&swap_lock);\n \tspin_lock(&si->lock);\n-\tsetup_swap_info(si, prio, swap_map, cluster_info, zeromap);\n+\tsetup_swap_info(si, prio, swap_map, cluster_info);\n \tspin_unlock(&si->lock);\n \tspin_unlock(&swap_lock);\n \t/*\n@@ -2370,7 +2367,7 @@ static void reinsert_swap_info(struct swap_info_struct *si)\n {\n \tspin_lock(&swap_lock);\n \tspin_lock(&si->lock);\n-\tsetup_swap_info(si, si->prio, si->swap_map, si->cluster_info, si->zeromap);\n+\tsetup_swap_info(si, si->prio, si->swap_map, si->cluster_info);\n \t_enable_swap_info(si);\n \tspin_unlock(&si->lock);\n \tspin_unlock(&swap_lock);\n@@ -2441,7 +2438,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n \tunsigned char *swap_map;\n-\tunsigned long *zeromap;\n \tstruct swap_cluster_info *cluster_info;\n \tstruct file *swap_file, *victim;\n \tstruct address_space *mapping;\n@@ -2536,8 +2532,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tp->swap_file = NULL;\n \tswap_map = p->swap_map;\n \tp->swap_map = NULL;\n-\tzeromap = p->zeromap;\n-\tp->zeromap = NULL;\n \tmaxpages = p->max;\n \tcluster_info = p->cluster_info;\n \tp->max = 0;\n@@ -2549,7 +2543,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tkfree(p->global_cluster);\n \tp->global_cluster = NULL;\n \tvfree(swap_map);\n-\tkvfree(zeromap);\n \tfree_cluster_info(cluster_info, maxpages);\n \n \tinode = mapping->host;\n@@ -3013,7 +3006,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tsector_t span;\n \tunsigned long maxpages;\n \tunsigned char *swap_map = NULL;\n-\tunsigned long *zeromap = NULL;\n \tstruct swap_cluster_info *cluster_info = NULL;\n \tstruct folio *folio = NULL;\n \tstruct inode *inode = NULL;\n@@ -3119,17 +3111,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tif (error)\n \t\tgoto bad_swap_unlock_inode;\n \n-\t/*\n-\t * Use kvmalloc_array instead of bitmap_zalloc as the allocation order might\n-\t * be above MAX_PAGE_ORDER incase of a large swap file.\n-\t */\n-\tzeromap = kvmalloc_array(BITS_TO_LONGS(maxpages), sizeof(long),\n-\t\t\t\t    GFP_KERNEL | __GFP_ZERO);\n-\tif (!zeromap) {\n-\t\terror = -ENOMEM;\n-\t\tgoto bad_swap_unlock_inode;\n-\t}\n-\n \tif (si->bdev && bdev_stable_writes(si->bdev))\n \t\tsi->flags |= SWP_STABLE_WRITES;\n \n@@ -3196,7 +3177,7 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tprio = DEF_SWAP_PRIO;\n \tif (swap_flags & SWAP_FLAG_PREFER)\n \t\tprio = swap_flags & SWAP_FLAG_PRIO_MASK;\n-\tenable_swap_info(si, prio, swap_map, cluster_info, zeromap);\n+\tenable_swap_info(si, prio, swap_map, cluster_info);\n \n \tpr_info(\"Adding %uk swap on %s.  Priority:%d extents:%d across:%lluk %s%s%s%s\\n\",\n \t\tK(si->pages), name->name, si->prio, nr_extents,\n@@ -3224,7 +3205,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tsi->flags = 0;\n \tspin_unlock(&swap_lock);\n \tvfree(swap_map);\n-\tkvfree(zeromap);\n \tif (cluster_info)\n \t\tfree_cluster_info(cluster_info, maxpages);\n \tif (inced_nr_rotate_swap)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, and explained that they will only record the memcg id on virtual swap slot allocation, deferring physical swap charging until the virtual swap slot is backed by an actual physical swap slot.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Now that zswap and the zero-filled swap page optimization no longer\ntakes up any physical swap space, we should not charge towards the swap\nusage and limits of the memcg in these case. We will only record the\nmemcg id on virtual swap slot allocation, and defer physical swap\ncharging (i.e towards memory.swap.current) until the virtual swap slot\nis backed by an actual physical swap slot (on zswap store failure\nfallback or zswap writeback).\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h | 16 +++++++++\n mm/memcontrol-v1.c   |  6 ++++\n mm/memcontrol.c      | 83 ++++++++++++++++++++++++++++++++------------\n mm/vswap.c           | 39 +++++++++------------\n 4 files changed, 98 insertions(+), 46 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 9cd45eab313f8..a30d382fb5ee1 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -613,6 +613,22 @@ static inline void folio_throttle_swaprate(struct folio *folio, gfp_t gfp)\n #endif\n \n #if defined(CONFIG_MEMCG) && defined(CONFIG_SWAP)\n+void __mem_cgroup_record_swap(struct folio *folio, swp_entry_t entry);\n+static inline void mem_cgroup_record_swap(struct folio *folio,\n+\t\tswp_entry_t entry)\n+{\n+\tif (!mem_cgroup_disabled())\n+\t\t__mem_cgroup_record_swap(folio, entry);\n+}\n+\n+void __mem_cgroup_clear_swap(swp_entry_t entry, unsigned int nr_pages);\n+static inline void mem_cgroup_clear_swap(swp_entry_t entry,\n+\t\tunsigned int nr_pages)\n+{\n+\tif (!mem_cgroup_disabled())\n+\t\t__mem_cgroup_clear_swap(entry, nr_pages);\n+}\n+\n int __mem_cgroup_try_charge_swap(struct folio *folio, swp_entry_t entry);\n static inline int mem_cgroup_try_charge_swap(struct folio *folio,\n \t\tswp_entry_t entry)\ndiff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.c\nindex 6eed14bff7426..4580a034dcf72 100644\n--- a/mm/memcontrol-v1.c\n+++ b/mm/memcontrol-v1.c\n@@ -680,6 +680,12 @@ void memcg1_swapin(swp_entry_t entry, unsigned int nr_pages)\n \t\t * memory+swap charge, drop the swap entry duplicate.\n \t\t */\n \t\tmem_cgroup_uncharge_swap(entry, nr_pages);\n+\n+\t\t/*\n+\t\t * Clear the cgroup association now to prevent double memsw\n+\t\t * uncharging when the backends are released later.\n+\t\t */\n+\t\tmem_cgroup_clear_swap(entry, nr_pages);\n \t}\n }\n \ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 2ba5811e7edba..50be8066bebec 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -5172,6 +5172,49 @@ int __init mem_cgroup_init(void)\n }\n \n #ifdef CONFIG_SWAP\n+/**\n+ * __mem_cgroup_record_swap - record the folio's cgroup for the swap entries.\n+ * @folio: folio being swapped out.\n+ * @entry: the first swap entry in the range.\n+ */\n+void __mem_cgroup_record_swap(struct folio *folio, swp_entry_t entry)\n+{\n+\tunsigned int nr_pages = folio_nr_pages(folio);\n+\tstruct mem_cgroup *memcg;\n+\n+\t/* Recording will be done by memcg1_swapout(). */\n+\tif (do_memsw_account())\n+\t\treturn;\n+\n+\tmemcg = folio_memcg(folio);\n+\n+\tVM_WARN_ON_ONCE_FOLIO(!memcg, folio);\n+\tif (!memcg)\n+\t\treturn;\n+\n+\tmemcg = mem_cgroup_id_get_online(memcg);\n+\tif (nr_pages > 1)\n+\t\tmem_cgroup_id_get_many(memcg, nr_pages - 1);\n+\tswap_cgroup_record(folio, mem_cgroup_id(memcg), entry);\n+}\n+\n+/**\n+ * __mem_cgroup_clear_swap - clear cgroup information of the swap entries.\n+ * @folio: folio being swapped out.\n+ * @entry: the first swap entry in the range.\n+ */\n+void __mem_cgroup_clear_swap(swp_entry_t entry, unsigned int nr_pages)\n+{\n+\tunsigned short id = swap_cgroup_clear(entry, nr_pages);\n+\tstruct mem_cgroup *memcg;\n+\n+\trcu_read_lock();\n+\tmemcg = mem_cgroup_from_id(id);\n+\tif (memcg)\n+\t\tmem_cgroup_id_put_many(memcg, nr_pages);\n+\trcu_read_unlock();\n+}\n+\n /**\n  * __mem_cgroup_try_charge_swap - try charging swap space for a folio\n  * @folio: folio being added to swap\n@@ -5190,34 +5233,24 @@ int __mem_cgroup_try_charge_swap(struct folio *folio, swp_entry_t entry)\n \tif (do_memsw_account())\n \t\treturn 0;\n \n-\tmemcg = folio_memcg(folio);\n-\n-\tVM_WARN_ON_ONCE_FOLIO(!memcg, folio);\n-\tif (!memcg)\n-\t\treturn 0;\n-\n-\tif (!entry.val) {\n-\t\tmemcg_memory_event(memcg, MEMCG_SWAP_FAIL);\n-\t\treturn 0;\n-\t}\n-\n-\tmemcg = mem_cgroup_id_get_online(memcg);\n+\t/*\n+\t * We already record the cgroup on virtual swap allocation.\n+\t * Note that the virtual swap slot holds a reference to memcg,\n+\t * so this lookup should be safe.\n+\t */\n+\trcu_read_lock();\n+\tmemcg = mem_cgroup_from_id(lookup_swap_cgroup_id(entry));\n+\trcu_read_unlock();\n \n \tif (!mem_cgroup_is_root(memcg) &&\n \t    !page_counter_try_charge(&memcg->swap, nr_pages, &counter)) {\n \t\tmemcg_memory_event(memcg, MEMCG_SWAP_MAX);\n \t\tmemcg_memory_event(memcg, MEMCG_SWAP_FAIL);\n-\t\tmem_cgroup_id_put(memcg);\n \t\treturn -ENOMEM;\n \t}\n \n-\t/* Get references for the tail pages, too */\n-\tif (nr_pages > 1)\n-\t\tmem_cgroup_id_get_many(memcg, nr_pages - 1);\n \tmod_memcg_state(memcg, MEMCG_SWAP, nr_pages);\n \n-\tswap_cgroup_record(folio, mem_cgroup_id(memcg), entry);\n-\n \treturn 0;\n }\n \n@@ -5231,7 +5264,8 @@ void __mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages)\n \tstruct mem_cgroup *memcg;\n \tunsigned short id;\n \n-\tid = swap_cgroup_clear(entry, nr_pages);\n+\tid = lookup_swap_cgroup_id(entry);\n+\n \trcu_read_lock();\n \tmemcg = mem_cgroup_from_id(id);\n \tif (memcg) {\n@@ -5242,7 +5276,6 @@ void __mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages)\n \t\t\t\tpage_counter_uncharge(&memcg->swap, nr_pages);\n \t\t}\n \t\tmod_memcg_state(memcg, MEMCG_SWAP, -nr_pages);\n-\t\tmem_cgroup_id_put_many(memcg, nr_pages);\n \t}\n \trcu_read_unlock();\n }\n@@ -5251,14 +5284,18 @@ static bool mem_cgroup_may_zswap(struct mem_cgroup *original_memcg);\n \n long mem_cgroup_get_nr_swap_pages(struct mem_cgroup *memcg)\n {\n-\tlong nr_swap_pages, nr_zswap_pages = 0;\n+\tlong nr_swap_pages;\n \n \tif (zswap_is_enabled() && (mem_cgroup_disabled() || do_memsw_account() ||\n \t\t\t\tmem_cgroup_may_zswap(memcg))) {\n-\t\tnr_zswap_pages = PAGE_COUNTER_MAX;\n+\t\t/*\n+\t\t * No need to check swap cgroup limits, since zswap is not charged\n+\t\t * towards swap consumption.\n+\t\t */\n+\t\treturn PAGE_COUNTER_MAX;\n \t}\n \n-\tnr_swap_pages = max_t(long, nr_zswap_pages, get_nr_swap_pages());\n+\tnr_swap_pages = get_nr_swap_pages();\n \tif (mem_cgroup_disabled() || do_memsw_account())\n \t\treturn nr_swap_pages;\n \tfor (; !mem_cgroup_is_root(memcg); memcg = parent_mem_cgroup(memcg))\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex 7563107eb8eee..2a071d5ae173c 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -543,6 +543,7 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n \tunsigned long flush_nr, phys_swap_start = 0, phys_swap_end = 0;\n+\tunsigned long phys_swap_released = 0;\n \tunsigned int phys_swap_type = 0;\n \tbool need_flushing_phys_swap = false;\n \tswp_slot_t flush_slot;\n@@ -572,6 +573,7 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\tif (desc->type == VSWAP_ZSWAP && desc->zswap_entry) {\n \t\t\tzswap_entry_free(desc->zswap_entry);\n \t\t} else if (desc->type == VSWAP_SWAPFILE) {\n+\t\t\tphys_swap_released++;\n \t\t\tif (!phys_swap_start) {\n \t\t\t\t/* start a new contiguous range of phys swap */\n \t\t\t\tphys_swap_start = swp_slot_offset(desc->slot);\n@@ -602,6 +604,9 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\tflush_nr = phys_swap_end - phys_swap_start;\n \t\tswap_slot_free_nr(flush_slot, flush_nr);\n \t}\n+\n+\tif (phys_swap_released)\n+\t\tmem_cgroup_uncharge_swap(entry, phys_swap_released);\n  }\n \n /*\n@@ -629,7 +634,7 @@ static void vswap_free(struct vswap_cluster *cluster, struct swp_desc *desc,\n \tspin_unlock(&cluster->lock);\n \n \trelease_backing(entry, 1);\n-\tmem_cgroup_uncharge_swap(entry, 1);\n+\tmem_cgroup_clear_swap(entry, 1);\n \n \t/* erase forward mapping and release the virtual slot for reallocation */\n \tspin_lock(&cluster->lock);\n@@ -644,9 +649,6 @@ static void vswap_free(struct vswap_cluster *cluster, struct swp_desc *desc,\n  */\n int folio_alloc_swap(struct folio *folio)\n {\n-\tstruct vswap_cluster *cluster = NULL;\n-\tint i, nr = folio_nr_pages(folio);\n-\tstruct swp_desc *desc;\n \tswp_entry_t entry;\n \n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n@@ -656,25 +658,7 @@ int folio_alloc_swap(struct folio *folio)\n \tif (!entry.val)\n \t\treturn -ENOMEM;\n \n-\t/*\n-\t * XXX: for now, we charge towards the memory cgroup's swap limit on virtual\n-\t * swap slots allocation. This will be changed soon - we will only charge on\n-\t * physical swap slots allocation.\n-\t */\n-\tif (mem_cgroup_try_charge_swap(folio, entry)) {\n-\t\trcu_read_lock();\n-\t\tfor (i = 0; i < nr; i++) {\n-\t\t\tdesc = vswap_iter(&cluster, entry.val + i);\n-\t\t\tVM_WARN_ON(!desc);\n-\t\t\tvswap_free(cluster, desc, (swp_entry_t){ entry.val + i });\n-\t\t}\n-\t\tspin_unlock(&cluster->lock);\n-\t\trcu_read_unlock();\n-\t\tatomic_add(nr, &vswap_alloc_reject);\n-\t\tentry.val = 0;\n-\t\treturn -ENOMEM;\n-\t}\n-\n+\tmem_cgroup_record_swap(folio, entry);\n \tswap_cache_add_folio(folio, entry, NULL);\n \n \treturn 0;\n@@ -716,6 +700,15 @@ bool vswap_alloc_swap_slot(struct folio *folio)\n \tif (!slot.val)\n \t\treturn false;\n \n+\tif (mem_cgroup_try_charge_swap(folio, entry)) {\n+\t\t/*\n+\t\t * We have not updated the backing type of the virtual swap slot.\n+\t\t * Simply free up the physical swap slots here!\n+\t\t */\n+\t\tswap_slot_free_nr(slot, nr);\n+\t\treturn false;\n+\t}\n+\n \t/* establish the vrtual <-> physical swap slots linkages. */\n \tsi = __swap_slot_to_info(slot);\n \tci = swap_cluster_lock(si, swp_slot_offset(slot));\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the swapoff path needing to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, and provided performance metrics showing the new design reduces kernel CPU time by about 13%.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "performance_metrics",
                "agreed_to_restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch presents the second applications of virtual swap design -\nsimplifying and optimizing swapoff.\n\nWith virtual swap slots stored at page table entries and used as indices\nto various swap-related data structures, we no longer have to perform a\npage table walk in swapoff. Simply iterate through all the allocated\nswap slots on the swapfile, find their corresponding virtual swap slots,\nand fault them in.\n\nThis is significantly cleaner, as well as slightly more performant,\nespecially when there are a lot of unrelated VMAs (since the old swapoff\ncode would have to traverse through all of them).\n\nIn a simple benchmark, in which we swapoff a 32 GB swapfile that is 50%\nfull, and in which there is a process that maps a 128GB file into\nmemory:\n\nBaseline:\nsys: 11.48s\n\nNew Design:\nsys: 9.96s\n\nDisregarding the real time reduction (which is mostly due to more IO\nasynchrony), the new design reduces the kernel CPU time by about 13%.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/shmem_fs.h |   7 +-\n mm/shmem.c               | 184 +--------------\n mm/swapfile.c            | 474 +++++++++------------------------------\n 3 files changed, 113 insertions(+), 552 deletions(-)\n\ndiff --git a/include/linux/shmem_fs.h b/include/linux/shmem_fs.h\nindex e2069b3179c41..bac6b6cafe89c 100644\n--- a/include/linux/shmem_fs.h\n+++ b/include/linux/shmem_fs.h\n@@ -41,17 +41,13 @@ struct shmem_inode_info {\n \tunsigned long\t\tswapped;\t/* subtotal assigned to swap */\n \tunion {\n \t    struct offset_ctx\tdir_offsets;\t/* stable directory offsets */\n-\t    struct {\n-\t\tstruct list_head shrinklist;\t/* shrinkable hpage inodes */\n-\t\tstruct list_head swaplist;\t/* chain of maybes on swap */\n-\t    };\n+\t    struct list_head\tshrinklist;\t/* shrinkable hpage inodes */\n \t};\n \tstruct timespec64\ti_crtime;\t/* file creation time */\n \tstruct shared_policy\tpolicy;\t\t/* NUMA memory alloc policy */\n \tstruct simple_xattrs\txattrs;\t\t/* list of xattrs */\n \tpgoff_t\t\t\tfallocend;\t/* highest fallocate endindex */\n \tunsigned int\t\tfsflags;\t/* for FS_IOC_[SG]ETFLAGS */\n-\tatomic_t\t\tstop_eviction;\t/* hold when working on inode */\n #ifdef CONFIG_TMPFS_QUOTA\n \tstruct dquot __rcu\t*i_dquot[MAXQUOTAS];\n #endif\n@@ -127,7 +123,6 @@ struct page *shmem_read_mapping_page_gfp(struct address_space *mapping,\n int shmem_writeout(struct folio *folio, struct swap_iocb **plug,\n \t\tstruct list_head *folio_list);\n void shmem_truncate_range(struct inode *inode, loff_t start, uoff_t end);\n-int shmem_unuse(unsigned int type);\n \n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n unsigned long shmem_allowable_huge_orders(struct inode *inode,\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 3a346cca114ab..61790752bdf6d 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -290,9 +290,6 @@ bool vma_is_shmem(const struct vm_area_struct *vma)\n \treturn vma_is_anon_shmem(vma) || vma->vm_ops == &shmem_vm_ops;\n }\n \n-static LIST_HEAD(shmem_swaplist);\n-static DEFINE_SPINLOCK(shmem_swaplist_lock);\n-\n #ifdef CONFIG_TMPFS_QUOTA\n \n static int shmem_enable_quotas(struct super_block *sb,\n@@ -1413,16 +1410,6 @@ static void shmem_evict_inode(struct inode *inode)\n \t\t\t}\n \t\t\tspin_unlock(&sbinfo->shrinklist_lock);\n \t\t}\n-\t\twhile (!list_empty(&info->swaplist)) {\n-\t\t\t/* Wait while shmem_unuse() is scanning this inode... */\n-\t\t\twait_var_event(&info->stop_eviction,\n-\t\t\t\t       !atomic_read(&info->stop_eviction));\n-\t\t\tspin_lock(&shmem_swaplist_lock);\n-\t\t\t/* ...but beware of the race if we peeked too early */\n-\t\t\tif (!atomic_read(&info->stop_eviction))\n-\t\t\t\tlist_del_init(&info->swaplist);\n-\t\t\tspin_unlock(&shmem_swaplist_lock);\n-\t\t}\n \t}\n \n \tsimple_xattrs_free(&info->xattrs, sbinfo->max_inodes ? &freed : NULL);\n@@ -1435,153 +1422,6 @@ static void shmem_evict_inode(struct inode *inode)\n #endif\n }\n \n-static unsigned int shmem_find_swap_entries(struct address_space *mapping,\n-\t\t\t\tpgoff_t start, struct folio_batch *fbatch,\n-\t\t\t\tpgoff_t *indices, unsigned int type)\n-{\n-\tXA_STATE(xas, &mapping->i_pages, start);\n-\tstruct folio *folio;\n-\tswp_entry_t entry;\n-\tswp_slot_t slot;\n-\n-\trcu_read_lock();\n-\txas_for_each(&xas, folio, ULONG_MAX) {\n-\t\tif (xas_retry(&xas, folio))\n-\t\t\tcontinue;\n-\n-\t\tif (!xa_is_value(folio))\n-\t\t\tcontinue;\n-\n-\t\tentry = radix_to_swp_entry(folio);\n-\t\tslot = swp_entry_to_swp_slot(entry);\n-\n-\t\t/*\n-\t\t * swapin error entries can be found in the mapping. But they're\n-\t\t * deliberately ignored here as we've done everything we can do.\n-\t\t */\n-\t\tif (!slot.val || swp_slot_type(slot) != type)\n-\t\t\tcontinue;\n-\n-\t\tindices[folio_batch_count(fbatch)] = xas.xa_index;\n-\t\tif (!folio_batch_add(fbatch, folio))\n-\t\t\tbreak;\n-\n-\t\tif (need_resched()) {\n-\t\t\txas_pause(&xas);\n-\t\t\tcond_resched_rcu();\n-\t\t}\n-\t}\n-\trcu_read_unlock();\n-\n-\treturn folio_batch_count(fbatch);\n-}\n-\n-/*\n- * Move the swapped pages for an inode to page cache. Returns the count\n- * of pages swapped in, or the error in case of failure.\n- */\n-static int shmem_unuse_swap_entries(struct inode *inode,\n-\t\tstruct folio_batch *fbatch, pgoff_t *indices)\n-{\n-\tint i = 0;\n-\tint ret = 0;\n-\tint error = 0;\n-\tstruct address_space *mapping = inode->i_mapping;\n-\n-\tfor (i = 0; i < folio_batch_count(fbatch); i++) {\n-\t\tstruct folio *folio = fbatch->folios[i];\n-\n-\t\terror = shmem_swapin_folio(inode, indices[i], &folio, SGP_CACHE,\n-\t\t\t\t\tmapping_gfp_mask(mapping), NULL, NULL);\n-\t\tif (error == 0) {\n-\t\t\tfolio_unlock(folio);\n-\t\t\tfolio_put(folio);\n-\t\t\tret++;\n-\t\t}\n-\t\tif (error == -ENOMEM)\n-\t\t\tbreak;\n-\t\terror = 0;\n-\t}\n-\treturn error ? error : ret;\n-}\n-\n-/*\n- * If swap found in inode, free it and move page from swapcache to filecache.\n- */\n-static int shmem_unuse_inode(struct inode *inode, unsigned int type)\n-{\n-\tstruct address_space *mapping = inode->i_mapping;\n-\tpgoff_t start = 0;\n-\tstruct folio_batch fbatch;\n-\tpgoff_t indices[PAGEVEC_SIZE];\n-\tint ret = 0;\n-\n-\tdo {\n-\t\tfolio_batch_init(&fbatch);\n-\t\tif (!shmem_find_swap_entries(mapping, start, &fbatch,\n-\t\t\t\t\t     indices, type)) {\n-\t\t\tret = 0;\n-\t\t\tbreak;\n-\t\t}\n-\n-\t\tret = shmem_unuse_swap_entries(inode, &fbatch, indices);\n-\t\tif (ret < 0)\n-\t\t\tbreak;\n-\n-\t\tstart = indices[folio_batch_count(&fbatch) - 1];\n-\t} while (true);\n-\n-\treturn ret;\n-}\n-\n-/*\n- * Read all the shared memory data that resides in the swap\n- * device 'type' back into memory, so the swap device can be\n- * unused.\n- */\n-int shmem_unuse(unsigned int type)\n-{\n-\tstruct shmem_inode_info *info, *next;\n-\tint error = 0;\n-\n-\tif (list_empty(&shmem_swaplist))\n-\t\treturn 0;\n-\n-\tspin_lock(&shmem_swaplist_lock);\n-start_over:\n-\tlist_for_each_entry_safe(info, next, &shmem_swaplist, swaplist) {\n-\t\tif (!info->swapped) {\n-\t\t\tlist_del_init(&info->swaplist);\n-\t\t\tcontinue;\n-\t\t}\n-\t\t/*\n-\t\t * Drop the swaplist mutex while searching the inode for swap;\n-\t\t * but before doing so, make sure shmem_evict_inode() will not\n-\t\t * remove placeholder inode from swaplist, nor let it be freed\n-\t\t * (igrab() would protect from unlink, but not from unmount).\n-\t\t */\n-\t\tatomic_inc(&info->stop_eviction);\n-\t\tspin_unlock(&shmem_swaplist_lock);\n-\n-\t\terror = shmem_unuse_inode(&info->vfs_inode, type);\n-\t\tcond_resched();\n-\n-\t\tspin_lock(&shmem_swaplist_lock);\n-\t\tif (atomic_dec_and_test(&info->stop_eviction))\n-\t\t\twake_up_var(&info->stop_eviction);\n-\t\tif (error)\n-\t\t\tbreak;\n-\t\tif (list_empty(&info->swaplist))\n-\t\t\tgoto start_over;\n-\t\tnext = list_next_entry(info, swaplist);\n-\t\tif (!info->swapped)\n-\t\t\tlist_del_init(&info->swaplist);\n-\t}\n-\tspin_unlock(&shmem_swaplist_lock);\n-\n-\treturn error;\n-}\n-\n /**\n  * shmem_writeout - Write the folio to swap\n  * @folio: The folio to write\n@@ -1668,24 +1508,9 @@ int shmem_writeout(struct folio *folio, struct swap_iocb **plug,\n \t}\n \n \tif (!folio_alloc_swap(folio)) {\n-\t\tbool first_swapped = shmem_recalc_inode(inode, 0, nr_pages);\n \t\tint error;\n \n-\t\t/*\n-\t\t * Add inode to shmem_unuse()'s list of swapped-out inodes,\n-\t\t * if it's not already there.  Do it now before the folio is\n-\t\t * removed from page cache, when its pagelock no longer\n-\t\t * protects the inode from eviction.  And do it now, after\n-\t\t * we've incremented swapped, because shmem_unuse() will\n-\t\t * prune a !swapped inode from the swaplist.\n-\t\t */\n-\t\tif (first_swapped) {\n-\t\t\tspin_lock(&shmem_swaplist_lock);\n-\t\t\tif (list_empty(&info->swaplist))\n-\t\t\t\tlist_add(&info->swaplist, &shmem_swaplist);\n-\t\t\tspin_unlock(&shmem_swaplist_lock);\n-\t\t}\n-\n+\t\tshmem_recalc_inode(inode, 0, nr_pages);\n \t\tswap_shmem_alloc(folio->swap, nr_pages);\n \t\tshmem_delete_from_page_cache(folio, swp_to_radix_entry(folio->swap));\n \n@@ -3106,7 +2931,6 @@ static struct inode *__shmem_get_inode(struct mnt_idmap *idmap,\n \tinfo = SHMEM_I(inode);\n \tmemset(info, 0, (char *)inode - (char *)info);\n \tspin_lock_init(&info->lock);\n-\tatomic_set(&info->stop_eviction, 0);\n \tinfo->seals = F_SEAL_SEAL;\n \tinfo->flags = (flags & VM_NORESERVE) ? SHMEM_F_NORESERVE : 0;\n \tinfo->i_crtime = inode_get_mtime(inode);\n@@ -3115,7 +2939,6 @@ static struct inode *__shmem_get_inode(struct mnt_idmap *idmap,\n \tif (info->fsflags)\n \t\tshmem_set_inode_flags(inode, info->fsflags, NULL);\n \tINIT_LIST_HEAD(&info->shrinklist);\n-\tINIT_LIST_HEAD(&info->swaplist);\n \tsimple_xattrs_init(&info->xattrs);\n \tcache_no_acl(inode);\n \tif (sbinfo->noswap)\n@@ -5785,11 +5608,6 @@ void __init shmem_init(void)\n \tBUG_ON(IS_ERR(shm_mnt));\n }\n \n-int shmem_unuse(unsigned int type)\n-{\n-\treturn 0;\n-}\n-\n int shmem_lock(struct file *file, int lock, struct ucounts *ucounts)\n {\n \treturn 0;\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex e1cb01b821ff3..9478707ce3ffa 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1738,300 +1738,12 @@ unsigned int count_swap_pages(int type, int free)\n }\n #endif /* CONFIG_HIBERNATION */\n \n-static inline int pte_same_as_swp(pte_t pte, pte_t swp_pte)\n+static bool swap_slot_allocated(struct swap_info_struct *si,\n+\t\tunsigned long offset)\n {\n-\treturn pte_same(pte_swp_clear_flags(pte), swp_pte);\n-}\n-\n-/*\n- * No need to decide whether this PTE shares the swap entry with others,\n- * just let do_wp_page work it out if a write is requested later - to\n- * force COW, vm_page_prot omits write permission from any private vma.\n- */\n-static int unuse_pte(struct vm_area_struct *vma, pmd_t *pmd,\n-\t\tunsigned long addr, swp_entry_t entry, struct folio *folio)\n-{\n-\tstruct page *page;\n-\tstruct folio *swapcache;\n-\tspinlock_t *ptl;\n-\tpte_t *pte, new_pte, old_pte;\n-\tbool hwpoisoned = false;\n-\tint ret = 1;\n-\n-\t/*\n-\t * If the folio is removed from swap cache by others, continue to\n-\t * unuse other PTEs. try_to_unuse may try again if we missed this one.\n-\t */\n-\tif (!folio_matches_swap_entry(folio, entry))\n-\t\treturn 0;\n-\n-\tswapcache = folio;\n-\tfolio = ksm_might_need_to_copy(folio, vma, addr);\n-\tif (unlikely(!folio))\n-\t\treturn -ENOMEM;\n-\telse if (unlikely(folio == ERR_PTR(-EHWPOISON))) {\n-\t\thwpoisoned = true;\n-\t\tfolio = swapcache;\n-\t}\n-\n-\tpage = folio_file_page(folio, swp_offset(entry));\n-\tif (PageHWPoison(page))\n-\t\thwpoisoned = true;\n-\n-\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n-\tif (unlikely(!pte || !pte_same_as_swp(ptep_get(pte),\n-\t\t\t\t\t\tswp_entry_to_pte(entry)))) {\n-\t\tret = 0;\n-\t\tgoto out;\n-\t}\n-\n-\told_pte = ptep_get(pte);\n-\n-\tif (unlikely(hwpoisoned || !folio_test_uptodate(folio))) {\n-\t\tswp_entry_t swp_entry;\n-\n-\t\tdec_mm_counter(vma->vm_mm, MM_SWAPENTS);\n-\t\tif (hwpoisoned) {\n-\t\t\tswp_entry = make_hwpoison_entry(page);\n-\t\t} else {\n-\t\t\tswp_entry = make_poisoned_swp_entry();\n-\t\t}\n-\t\tnew_pte = swp_entry_to_pte(swp_entry);\n-\t\tret = 0;\n-\t\tgoto setpte;\n-\t}\n-\n-\t/*\n-\t * Some architectures may have to restore extra metadata to the page\n-\t * when reading from swap. This metadata may be indexed by swap entry\n-\t * so this must be called before swap_free().\n-\t */\n-\tarch_swap_restore(folio_swap(entry, folio), folio);\n-\n-\tdec_mm_counter(vma->vm_mm, MM_SWAPENTS);\n-\tinc_mm_counter(vma->vm_mm, MM_ANONPAGES);\n-\tfolio_get(folio);\n-\tif (folio == swapcache) {\n-\t\trmap_t rmap_flags = RMAP_NONE;\n-\n-\t\t/*\n-\t\t * See do_swap_page(): writeback would be problematic.\n-\t\t * However, we do a folio_wait_writeback() just before this\n-\t\t * call and have the folio locked.\n-\t\t */\n-\t\tVM_BUG_ON_FOLIO(folio_test_writeback(folio), folio);\n-\t\tif (pte_swp_exclusive(old_pte))\n-\t\t\trmap_flags |= RMAP_EXCLUSIVE;\n-\t\t/*\n-\t\t * We currently only expect small !anon folios, which are either\n-\t\t * fully exclusive or fully shared. If we ever get large folios\n-\t\t * here, we have to be careful.\n-\t\t */\n-\t\tif (!folio_test_anon(folio)) {\n-\t\t\tVM_WARN_ON_ONCE(folio_test_large(folio));\n-\t\t\tVM_WARN_ON_FOLIO(!folio_test_locked(folio), folio);\n-\t\t\tfolio_add_new_anon_rmap(folio, vma, addr, rmap_flags);\n-\t\t} else {\n-\t\t\tfolio_add_anon_rmap_pte(folio, page, vma, addr, rmap_flags);\n-\t\t}\n-\t} else { /* ksm created a completely new copy */\n-\t\tfolio_add_new_anon_rmap(folio, vma, addr, RMAP_EXCLUSIVE);\n-\t\tfolio_add_lru_vma(folio, vma);\n-\t}\n-\tnew_pte = pte_mkold(mk_pte(page, vma->vm_page_prot));\n-\tif (pte_swp_soft_dirty(old_pte))\n-\t\tnew_pte = pte_mksoft_dirty(new_pte);\n-\tif (pte_swp_uffd_wp(old_pte))\n-\t\tnew_pte = pte_mkuffd_wp(new_pte);\n-setpte:\n-\tset_pte_at(vma->vm_mm, addr, pte, new_pte);\n-\tswap_free(entry);\n-out:\n-\tif (pte)\n-\t\tpte_unmap_unlock(pte, ptl);\n-\tif (folio != swapcache) {\n-\t\tfolio_unlock(folio);\n-\t\tfolio_put(folio);\n-\t}\n-\treturn ret;\n-}\n-\n-static int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,\n-\t\t\tunsigned long addr, unsigned long end,\n-\t\t\tunsigned int type)\n-{\n-\tpte_t *pte = NULL;\n-\tstruct swap_info_struct *si;\n-\n-\tsi = swap_info[type];\n-\tdo {\n-\t\tstruct folio *folio;\n-\t\tunsigned long offset;\n-\t\tunsigned char swp_count;\n-\t\tsoftleaf_t entry;\n-\t\tswp_slot_t slot;\n-\t\tint ret;\n-\t\tpte_t ptent;\n-\n-\t\tif (!pte++) {\n-\t\t\tpte = pte_offset_map(pmd, addr);\n-\t\t\tif (!pte)\n-\t\t\t\tbreak;\n-\t\t}\n-\n-\t\tptent = ptep_get_lockless(pte);\n-\t\tentry = softleaf_from_pte(ptent);\n-\n-\t\tif (!softleaf_is_swap(entry))\n-\t\t\tcontinue;\n-\n-\t\tslot = swp_entry_to_swp_slot(entry);\n-\t\tif (swp_slot_type(slot) != type)\n-\t\t\tcontinue;\n-\n-\t\toffset = swp_slot_offset(slot);\n-\t\tpte_unmap(pte);\n-\t\tpte = NULL;\n-\n-\t\tfolio = swap_cache_get_folio(entry);\n-\t\tif (!folio) {\n-\t\t\tstruct vm_fault vmf = {\n-\t\t\t\t.vma = vma,\n-\t\t\t\t.address = addr,\n-\t\t\t\t.real_address = addr,\n-\t\t\t\t.pmd = pmd,\n-\t\t\t};\n-\n-\t\t\tfolio = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,\n-\t\t\t\t\t\t&vmf);\n-\t\t}\n-\t\tif (!folio) {\n-\t\t\tswp_count = READ_ONCE(si->swap_map[offset]);\n-\t\t\tif (swp_count == 0 || swp_count == SWAP_MAP_BAD)\n-\t\t\t\tcontinue;\n-\t\t\treturn -ENOMEM;\n-\t\t}\n-\n-\t\tfolio_lock(folio);\n-\t\tfolio_wait_writeback(folio);\n-\t\tret = unuse_pte(vma, pmd, addr, entry, folio);\n-\t\tif (ret < 0) {\n-\t\t\tfolio_unlock(folio);\n-\t\t\tfolio_put(folio);\n-\t\t\treturn ret;\n-\t\t}\n-\n-\t\tfolio_free_swap(folio);\n-\t\tfolio_unlock(folio);\n-\t\tfolio_put(folio);\n-\t} while (addr += PAGE_SIZE, addr != end);\n-\n-\tif (pte)\n-\t\tpte_unmap(pte);\n-\treturn 0;\n-}\n-\n-static inline int unuse_pmd_range(struct vm_area_struct *vma, pud_t *pud,\n-\t\t\t\tunsigned long addr, unsigned long end,\n-\t\t\t\tunsigned int type)\n-{\n-\tpmd_t *pmd;\n-\tunsigned long next;\n-\tint ret;\n-\n-\tpmd = pmd_offset(pud, addr);\n-\tdo {\n-\t\tcond_resched();\n-\t\tnext = pmd_addr_end(addr, end);\n-\t\tret = unuse_pte_range(vma, pmd, addr, next, type);\n-\t\tif (ret)\n-\t\t\treturn ret;\n-\t} while (pmd++, addr = next, addr != end);\n-\treturn 0;\n-}\n-\n-static inline int unuse_pud_range(struct vm_area_struct *vma, p4d_t *p4d,\n-\t\t\t\tunsigned long addr, unsigned long end,\n-\t\t\t\tunsigned int type)\n-{\n-\tpud_t *pud;\n-\tunsigned long next;\n-\tint ret;\n-\n-\tpud = pud_offset(p4d, addr);\n-\tdo {\n-\t\tnext = pud_addr_end(addr, end);\n-\t\tif (pud_none_or_clear_bad(pud))\n-\t\t\tcontinue;\n-\t\tret = unuse_pmd_range(vma, pud, addr, next, type);\n-\t\tif (ret)\n-\t\t\treturn ret;\n-\t} while (pud++, addr = next, addr != end);\n-\treturn 0;\n-}\n-\n-static inline int unuse_p4d_range(struct vm_area_struct *vma, pgd_t *pgd,\n-\t\t\t\tunsigned long addr, unsigned long end,\n-\t\t\t\tunsigned int type)\n-{\n-\tp4d_t *p4d;\n-\tunsigned long next;\n-\tint ret;\n-\n-\tp4d = p4d_offset(pgd, addr);\n-\tdo {\n-\t\tnext = p4d_addr_end(addr, end);\n-\t\tif (p4d_none_or_clear_bad(p4d))\n-\t\t\tcontinue;\n-\t\tret = unuse_pud_range(vma, p4d, addr, next, type);\n-\t\tif (ret)\n-\t\t\treturn ret;\n-\t} while (p4d++, addr = next, addr != end);\n-\treturn 0;\n-}\n-\n-static int unuse_vma(struct vm_area_struct *vma, unsigned int type)\n-{\n-\tpgd_t *pgd;\n-\tunsigned long addr, end, next;\n-\tint ret;\n-\n-\taddr = vma->vm_start;\n-\tend = vma->vm_end;\n-\n-\tpgd = pgd_offset(vma->vm_mm, addr);\n-\tdo {\n-\t\tnext = pgd_addr_end(addr, end);\n-\t\tif (pgd_none_or_clear_bad(pgd))\n-\t\t\tcontinue;\n-\t\tret = unuse_p4d_range(vma, pgd, addr, next, type);\n-\t\tif (ret)\n-\t\t\treturn ret;\n-\t} while (pgd++, addr = next, addr != end);\n-\treturn 0;\n-}\n+\tunsigned char count = READ_ONCE(si->swap_map[offset]);\n \n-static int unuse_mm(struct mm_struct *mm, unsigned int type)\n-{\n-\tstruct vm_area_struct *vma;\n-\tint ret = 0;\n-\tVMA_ITERATOR(vmi, mm, 0);\n-\n-\tmmap_read_lock(mm);\n-\tif (check_stable_address_space(mm))\n-\t\tgoto unlock;\n-\tfor_each_vma(vmi, vma) {\n-\t\tif (vma->anon_vma && !is_vm_hugetlb_page(vma)) {\n-\t\t\tret = unuse_vma(vma, type);\n-\t\t\tif (ret)\n-\t\t\t\tbreak;\n-\t\t}\n-\n-\t\tcond_resched();\n-\t}\n-unlock:\n-\tmmap_read_unlock(mm);\n-\treturn ret;\n+\treturn count && swap_count(count) != SWAP_MAP_BAD;\n }\n \n /*\n@@ -2043,7 +1755,6 @@ static unsigned int find_next_to_unuse(struct swap_info_struct *si,\n \t\t\t\t\tunsigned int prev)\n {\n \tunsigned int i;\n-\tunsigned char count;\n \n \t/*\n \t * No need for swap_lock here: we're just looking\n@@ -2052,8 +1763,7 @@ static unsigned int find_next_to_unuse(struct swap_info_struct *si,\n \t * allocations from this area (while holding swap_lock).\n \t */\n \tfor (i = prev + 1; i < si->max; i++) {\n-\t\tcount = READ_ONCE(si->swap_map[i]);\n-\t\tif (count && swap_count(count) != SWAP_MAP_BAD)\n+\t\tif (swap_slot_allocated(si, i))\n \t\t\tbreak;\n \t\tif ((i % LATENCY_LIMIT) == 0)\n \t\t\tcond_resched();\n@@ -2065,101 +1775,139 @@ static unsigned int find_next_to_unuse(struct swap_info_struct *si,\n \treturn i;\n }\n \n+#define\tfor_each_allocated_offset(si, offset)\t\\\n+\twhile (swap_usage_in_pages(si) && \\\n+\t\t!signal_pending(current) && \\\n+\t\t(offset = find_next_to_unuse(si, offset)) != 0)\n+\n+static struct folio *pagein(swp_entry_t entry, struct swap_iocb **splug,\n+\t\tstruct mempolicy *mpol)\n+{\n+\tbool folio_was_allocated;\n+\tstruct folio *folio = __read_swap_cache_async(entry, GFP_KERNEL, mpol,\n+\t\t\tNO_INTERLEAVE_INDEX, &folio_was_allocated, false);\n+\n+\tif (folio_was_allocated)\n+\t\tswap_read_folio(folio, splug);\n+\treturn folio;\n+}\n+\n static int try_to_unuse(unsigned int type)\n {\n-\tstruct mm_struct *prev_mm;\n-\tstruct mm_struct *mm;\n-\tstruct list_head *p;\n-\tint retval = 0;\n \tstruct swap_info_struct *si = swap_info[type];\n+\tstruct swap_iocb *splug = NULL;\n+\tstruct mempolicy *mpol;\n+\tstruct blk_plug plug;\n+\tunsigned long offset;\n \tstruct folio *folio;\n \tswp_entry_t entry;\n \tswp_slot_t slot;\n-\tunsigned int i;\n+\tint ret = 0;\n \n \tif (!swap_usage_in_pages(si))\n \t\tgoto success;\n \n-retry:\n-\tretval = shmem_unuse(type);\n-\tif (retval)\n-\t\treturn retval;\n-\n-\tprev_mm = &init_mm;\n-\tmmget(prev_mm);\n-\n-\tspin_lock(&mmlist_lock);\n-\tp = &init_mm.mmlist;\n-\twhile (swap_usage_in_pages(si) &&\n-\t       !signal_pending(current) &&\n-\t       (p = p->next) != &init_mm.mmlist) {\n+\tmpol = get_task_policy(current);\n+\tblk_start_plug(&plug);\n \n-\t\tmm = list_entry(p, struct mm_struct, mmlist);\n-\t\tif (!mmget_not_zero(mm))\n+\t/* first round - submit the reads */\n+\toffset = 0;\n+\tfor_each_allocated_offset(si, offset) {\n+\t\tslot = swp_slot(type, offset);\n+\t\tentry = swp_slot_to_swp_entry(slot);\n+\t\tif (!entry.val)\n \t\t\tcontinue;\n-\t\tspin_unlock(&mmlist_lock);\n-\t\tmmput(prev_mm);\n-\t\tprev_mm = mm;\n-\t\tretval = unuse_mm(mm, type);\n-\t\tif (retval) {\n-\t\t\tmmput(prev_mm);\n-\t\t\treturn retval;\n-\t\t}\n \n-\t\t/*\n-\t\t * Make sure that we aren't completely killing\n-\t\t * interactive performance.\n-\t\t */\n-\t\tcond_resched();\n-\t\tspin_lock(&mmlist_lock);\n+\t\tfolio = pagein(entry, &splug, mpol);\n+\t\tif (folio)\n+\t\t\tfolio_put(folio);\n \t}\n-\tspin_unlock(&mmlist_lock);\n+\tblk_finish_plug(&plug);\n+\tswap_read_unplug(splug);\n+\tsplug = NULL;\n+\tlru_add_drain();\n+\n+\t/* second round - updating the virtual swap slots' backing state */\n+\toffset = 0;\n+\tfor_each_allocated_offset(si, offset) {\n+\t\tslot = swp_slot(type, offset);\n+retry:\n+\t\tentry = swp_slot_to_swp_entry(slot);\n+\t\tif (!entry.val) {\n+\t\t\tif (!swap_slot_allocated(si, offset))\n+\t\t\t\tcontinue;\n \n-\tmmput(prev_mm);\n+\t\t\tif (signal_pending(current)) {\n+\t\t\t\tret = -EINTR;\n+\t\t\t\tgoto out;\n+\t\t\t}\n \n-\ti = 0;\n-\twhile (swap_usage_in_pages(si) &&\n-\t       !signal_pending(current) &&\n-\t       (i = find_next_to_unuse(si, i)) != 0) {\n+\t\t\t/* we might be racing with zswap writeback or disk swapout */\n+\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\tgoto retry;\n+\t\t}\n \n-\t\tslot = swp_slot(type, i);\n-\t\tentry = swp_slot_to_swp_entry(slot);\n-\t\tfolio = swap_cache_get_folio(entry);\n-\t\tif (!folio)\n-\t\t\tcontinue;\n+\t\t/* try to allocate swap cache folio */\n+\t\tfolio = pagein(entry, &splug, mpol);\n+\t\tif (!folio) {\n+\t\t\tif (!swp_slot_to_swp_entry(swp_slot(type, offset)).val)\n+\t\t\t\tcontinue;\n \n+\t\t\tret = -ENOMEM;\n+\t\t\tpr_err(\"swapoff: unable to allocate swap cache folio for %lu\\n\",\n+\t\t\t\t\t\tentry.val);\n+\t\t\tgoto out;\n+\t\t}\n+\n+\t\tfolio_lock(folio);\n \t\t/*\n-\t\t * It is conceivable that a racing task removed this folio from\n-\t\t * swap cache just before we acquired the page lock. The folio\n-\t\t * might even be back in swap cache on another swap area. But\n-\t\t * that is okay, folio_free_swap() only removes stale folios.\n+\t\t * We need to check if the folio is still in swap cache, and is still\n+\t\t * backed by the physical swap slot we are trying to release.\n+\t\t *\n+\t\t * We can, for instance, race with zswap writeback, obtaining the\n+\t\t * temporary folio it allocated for decompression and writeback, which\n+\t\t * would be promptly deleted from swap cache. By the time we lock that\n+\t\t * folio, it might have already contained stale data.\n+\t\t *\n+\t\t * Concurrent swap operations might have also come in before we\n+\t\t * reobtain the folio's lock, deleting the folio from swap cache,\n+\t\t * invalidating the virtual swap slot, then swapping out the folio\n+\t\t * again to a different swap backends.\n+\t\t *\n+\t\t * In all of these cases, we must retry the physical -> virtual lookup.\n \t\t */\n-\t\tfolio_lock(folio);\n+\t\tif (!folio_matches_swap_slot(folio, entry, slot)) {\n+\t\t\tfolio_unlock(folio);\n+\t\t\tfolio_put(folio);\n+\t\t\tif (signal_pending(current)) {\n+\t\t\t\tret = -EINTR;\n+\t\t\t\tgoto out;\n+\t\t\t}\n+\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\tgoto retry;\n+\t\t}\n+\n \t\tfolio_wait_writeback(folio);\n-\t\tfolio_free_swap(folio);\n+\t\tvswap_store_folio(entry, folio);\n+\t\tfolio_mark_dirty(folio);\n \t\tfolio_unlock(folio);\n \t\tfolio_put(folio);\n \t}\n \n-\t/*\n-\t * Lets check again to see if there are still swap entries in the map.\n-\t * If yes, we would need to do retry the unuse logic again.\n-\t * Under global memory pressure, swap entries can be reinserted back\n-\t * into process space after the mmlist loop above passes over them.\n-\t *\n-\t * Limit the number of retries? No: when mmget_not_zero()\n-\t * above fails, that mm is likely to be freeing swap from\n-\t * exit_mmap(), which proceeds at its own independent pace;\n-\t * and even shmem_writeout() could have been preempted after\n-\t * folio_alloc_swap(), temporarily hiding that swap.  It's easy\n-\t * and robust (though cpu-intensive) just to keep retrying.\n-\t */\n-\tif (swap_usage_in_pages(si)) {\n-\t\tif (!signal_pending(current))\n-\t\t\tgoto retry;\n-\t\treturn -EINTR;\n+\t/* concurrent swappers might still be releasing physical swap slots... */\n+\twhile (swap_usage_in_pages(si)) {\n+\t\tif (signal_pending(current)) {\n+\t\t\tret = -EINTR;\n+\t\t\tgoto out;\n+\t\t}\n+\t\tschedule_timeout_uninterruptible(1);\n \t}\n \n+out:\n+\tswap_read_unplug(splug);\n+\tif (ret)\n+\t\treturn ret;\n+\n success:\n \t/*\n \t * Make sure that further cleanups after try_to_unuse() returns happen\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed a concern about the memory usage of the swap map by replacing it with two bitmaps, one for allocated state and one for bad state, which will save 6 bits per swap entry.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Now that we have moved the swap count state to virtual swap layer, each\nswap map entry only has 3 possible states: free, allocated, and bad.\nReplace the swap map with 2 bitmaps (one for allocated state and one for\nbad state), saving 6 bits per swap entry.\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/swap.h |  3 +-\n mm/swapfile.c        | 81 +++++++++++++++++++++++---------------------\n 2 files changed, 44 insertions(+), 40 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex a30d382fb5ee1..a02ce3fb2358b 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -259,7 +259,8 @@ struct swap_info_struct {\n \tstruct plist_node list;\t\t/* entry in swap_active_head */\n \tsigned char\ttype;\t\t/* strange name for an index */\n \tunsigned int\tmax;\t\t/* extent of the swap_map */\n-\tunsigned char *swap_map;\t/* vmalloc'ed array of usage counts */\n+\tunsigned long *swap_map;\t/* bitmap for allocated state */\n+\tunsigned long *bad_map;\t\t/* bitmap for bad state */\n \tstruct swap_cluster_info *cluster_info; /* cluster info. Only for SSD */\n \tstruct list_head free_clusters; /* free clusters list */\n \tstruct list_head full_clusters; /* full clusters list */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 9478707ce3ffa..b7661ffa312be 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -760,25 +760,19 @@ static bool cluster_reclaim_range(struct swap_info_struct *si,\n \t\t\t\t  struct swap_cluster_info *ci,\n \t\t\t\t  unsigned long start, unsigned long end)\n {\n-\tunsigned char *map = si->swap_map;\n \tunsigned long offset = start;\n \tint nr_reclaim;\n \n \tspin_unlock(&ci->lock);\n \tdo {\n-\t\tswitch (READ_ONCE(map[offset])) {\n-\t\tcase 0:\n+\t\tif (!test_bit(offset, si->swap_map)) {\n \t\t\toffset++;\n-\t\t\tbreak;\n-\t\tcase SWAP_MAP_ALLOCATED:\n+\t\t} else {\n \t\t\tnr_reclaim = __try_to_reclaim_swap(si, offset, TTRS_ANYWAY);\n \t\t\tif (nr_reclaim > 0)\n \t\t\t\toffset += nr_reclaim;\n \t\t\telse\n \t\t\t\tgoto out;\n-\t\t\tbreak;\n-\t\tdefault:\n-\t\t\tgoto out;\n \t\t}\n \t} while (offset < end);\n out:\n@@ -787,11 +781,7 @@ static bool cluster_reclaim_range(struct swap_info_struct *si,\n \t * Recheck the range no matter reclaim succeeded or not, the slot\n \t * could have been be freed while we are not holding the lock.\n \t */\n-\tfor (offset = start; offset < end; offset++)\n-\t\tif (READ_ONCE(map[offset]))\n-\t\t\treturn false;\n-\n-\treturn true;\n+\treturn find_next_bit(si->swap_map, end, start) >= end;\n }\n \n static bool cluster_scan_range(struct swap_info_struct *si,\n@@ -800,15 +790,16 @@ static bool cluster_scan_range(struct swap_info_struct *si,\n \t\t\t       bool *need_reclaim)\n {\n \tunsigned long offset, end = start + nr_pages;\n-\tunsigned char *map = si->swap_map;\n-\tunsigned char count;\n \n \tif (cluster_is_empty(ci))\n \t\treturn true;\n \n \tfor (offset = start; offset < end; offset++) {\n-\t\tcount = READ_ONCE(map[offset]);\n-\t\tif (!count)\n+\t\t/* Bad slots cannot be used for allocation */\n+\t\tif (test_bit(offset, si->bad_map))\n+\t\t\treturn false;\n+\n+\t\tif (!test_bit(offset, si->swap_map))\n \t\t\tcontinue;\n \n \t\tif (swap_cache_only(si, offset)) {\n@@ -841,7 +832,7 @@ static bool cluster_alloc_range(struct swap_info_struct *si, struct swap_cluster\n \tif (cluster_is_empty(ci))\n \t\tci->order = order;\n \n-\tmemset(si->swap_map + start, usage, nr_pages);\n+\tbitmap_set(si->swap_map, start, nr_pages);\n \tswap_range_alloc(si, nr_pages);\n \tci->count += nr_pages;\n \n@@ -1404,7 +1395,7 @@ static struct swap_info_struct *_swap_info_get(swp_slot_t slot)\n \toffset = swp_slot_offset(slot);\n \tif (offset >= si->max)\n \t\tgoto bad_offset;\n-\tif (data_race(!si->swap_map[swp_slot_offset(slot)]))\n+\tif (data_race(!test_bit(offset, si->swap_map)))\n \t\tgoto bad_free;\n \treturn si;\n \n@@ -1518,8 +1509,7 @@ static void swap_slots_free(struct swap_info_struct *si,\n \t\t\t      swp_slot_t slot, unsigned int nr_pages)\n {\n \tunsigned long offset = swp_slot_offset(slot);\n-\tunsigned char *map = si->swap_map + offset;\n-\tunsigned char *map_end = map + nr_pages;\n+\tunsigned long end = offset + nr_pages;\n \n \t/* It should never free entries across different clusters */\n \tVM_BUG_ON(ci != __swap_offset_to_cluster(si, offset + nr_pages - 1));\n@@ -1527,10 +1517,8 @@ static void swap_slots_free(struct swap_info_struct *si,\n \tVM_BUG_ON(ci->count < nr_pages);\n \n \tci->count -= nr_pages;\n-\tdo {\n-\t\tVM_BUG_ON(!swap_is_last_ref(*map));\n-\t\t*map = 0;\n-\t} while (++map < map_end);\n+\tVM_BUG_ON(find_next_zero_bit(si->swap_map, end, offset) < end);\n+\tbitmap_clear(si->swap_map, offset, nr_pages);\n \n \tswap_range_free(si, offset, nr_pages);\n \n@@ -1741,9 +1729,7 @@ unsigned int count_swap_pages(int type, int free)\n static bool swap_slot_allocated(struct swap_info_struct *si,\n \t\tunsigned long offset)\n {\n-\tunsigned char count = READ_ONCE(si->swap_map[offset]);\n-\n-\treturn count && swap_count(count) != SWAP_MAP_BAD;\n+\treturn test_bit(offset, si->swap_map);\n }\n \n /*\n@@ -2064,7 +2050,7 @@ static int setup_swap_extents(struct swap_info_struct *sis, sector_t *span)\n }\n \n static void setup_swap_info(struct swap_info_struct *si, int prio,\n-\t\t\t    unsigned char *swap_map,\n+\t\t\t    unsigned long *swap_map,\n \t\t\t    struct swap_cluster_info *cluster_info)\n {\n \tsi->prio = prio;\n@@ -2092,7 +2078,7 @@ static void _enable_swap_info(struct swap_info_struct *si)\n }\n \n static void enable_swap_info(struct swap_info_struct *si, int prio,\n-\t\t\t\tunsigned char *swap_map,\n+\t\t\t\tunsigned long *swap_map,\n \t\t\t\tstruct swap_cluster_info *cluster_info)\n {\n \tspin_lock(&swap_lock);\n@@ -2185,7 +2171,8 @@ static void flush_percpu_swap_cluster(struct swap_info_struct *si)\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n-\tunsigned char *swap_map;\n+\tunsigned long *swap_map;\n+\tunsigned long *bad_map;\n \tstruct swap_cluster_info *cluster_info;\n \tstruct file *swap_file, *victim;\n \tstruct address_space *mapping;\n@@ -2280,6 +2267,8 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tp->swap_file = NULL;\n \tswap_map = p->swap_map;\n \tp->swap_map = NULL;\n+\tbad_map = p->bad_map;\n+\tp->bad_map = NULL;\n \tmaxpages = p->max;\n \tcluster_info = p->cluster_info;\n \tp->max = 0;\n@@ -2290,7 +2279,8 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tmutex_unlock(&swapon_mutex);\n \tkfree(p->global_cluster);\n \tp->global_cluster = NULL;\n-\tvfree(swap_map);\n+\tkvfree(swap_map);\n+\tkvfree(bad_map);\n \tfree_cluster_info(cluster_info, maxpages);\n \n \tinode = mapping->host;\n@@ -2638,18 +2628,20 @@ static unsigned long read_swap_header(struct swap_info_struct *si,\n \n static int setup_swap_map(struct swap_info_struct *si,\n \t\t\t  union swap_header *swap_header,\n-\t\t\t  unsigned char *swap_map,\n+\t\t\t  unsigned long *swap_map,\n+\t\t\t  unsigned long *bad_map,\n \t\t\t  unsigned long maxpages)\n {\n \tunsigned long i;\n \n-\tswap_map[0] = SWAP_MAP_BAD; /* omit header page */\n+\tset_bit(0, bad_map); /* omit header page */\n+\n \tfor (i = 0; i < swap_header->info.nr_badpages; i++) {\n \t\tunsigned int page_nr = swap_header->info.badpages[i];\n \t\tif (page_nr == 0 || page_nr > swap_header->info.last_page)\n \t\t\treturn -EINVAL;\n \t\tif (page_nr < maxpages) {\n-\t\t\tswap_map[page_nr] = SWAP_MAP_BAD;\n+\t\t\tset_bit(page_nr, bad_map);\n \t\t\tsi->pages--;\n \t\t}\n \t}\n@@ -2753,7 +2745,7 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tint nr_extents;\n \tsector_t span;\n \tunsigned long maxpages;\n-\tunsigned char *swap_map = NULL;\n+\tunsigned long *swap_map = NULL, *bad_map = NULL;\n \tstruct swap_cluster_info *cluster_info = NULL;\n \tstruct folio *folio = NULL;\n \tstruct inode *inode = NULL;\n@@ -2849,16 +2841,24 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tmaxpages = si->max;\n \n \t/* OK, set up the swap map and apply the bad block list */\n-\tswap_map = vzalloc(maxpages);\n+\tswap_map = kvcalloc(BITS_TO_LONGS(maxpages), sizeof(long), GFP_KERNEL);\n \tif (!swap_map) {\n \t\terror = -ENOMEM;\n \t\tgoto bad_swap_unlock_inode;\n \t}\n \n-\terror = setup_swap_map(si, swap_header, swap_map, maxpages);\n+\tbad_map = kvcalloc(BITS_TO_LONGS(maxpages), sizeof(long), GFP_KERNEL);\n+\tif (!bad_map) {\n+\t\terror = -ENOMEM;\n+\t\tgoto bad_swap_unlock_inode;\n+\t}\n+\n+\terror = setup_swap_map(si, swap_header, swap_map, bad_map, maxpages);\n \tif (error)\n \t\tgoto bad_swap_unlock_inode;\n \n+\tsi->bad_map = bad_map;\n+\n \tif (si->bdev && bdev_stable_writes(si->bdev))\n \t\tsi->flags |= SWP_STABLE_WRITES;\n \n@@ -2952,7 +2952,10 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tsi->swap_file = NULL;\n \tsi->flags = 0;\n \tspin_unlock(&swap_lock);\n-\tvfree(swap_map);\n+\tif (swap_map)\n+\t\tkvfree(swap_map);\n+\tif (bad_map)\n+\t\tkvfree(bad_map);\n \tif (cluster_info)\n \t\tfree_cluster_info(cluster_info, maxpages);\n \tif (inced_nr_rotate_swap)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged an issue with email delivery, apologized for inconvenience, and expressed confusion about the problem.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "apology",
                "confusion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Weirdly, it seems like the cover letter (and only the cover letter) is\nnot being delivered...\n\nI'm trying to figure out what's going on :( My apologies for the\ninconvenience...",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed concerns about lock contention issues in the virtual swap layer by implementing a cluster-based allocation algorithm, inspired by previous work, which eliminates lock contention issues.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "resolved",
                "performance on-par with baseline"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Changelog:\n* RFC v2 -> v3:\n    * Implement a cluster-based allocation algorithm for virtual swap\n      slots, inspired by Kairui Song and Chris Li's implementation, as\n      well as Johannes Weiner's suggestions. This eliminates the lock\n\t  contention issues on the virtual swap layer.\n    * Re-use swap table for the reverse mapping.\n    * Remove CONFIG_VIRTUAL_SWAP.\n    * Reducing the size of the swap descriptor from 48 bytes to 24\n      bytes, i.e another 50% reduction in memory overhead from v2.\n    * Remove swap cache and zswap tree and use the swap descriptor\n      for this.\n    * Remove zeromap, and replace the swap_map bytemap with 2 bitmaps\n      (one for allocated slots, and one for bad slots).\n    * Rebase on top of 6.19 (7d0a66e4bb9081d75c82ec4957c50034cb0ea449)\n\t* Update cover letter to include new benchmark results and discussion\n\t  on overhead in various cases.\n* RFC v1 -> RFC v2:\n    * Use a single atomic type (swap_refs) for reference counting\n      purpose. This brings the size of the swap descriptor from 64 B\n      down to 48 B (25% reduction). Suggested by Yosry Ahmed.\n    * Zeromap bitmap is removed in the virtual swap implementation.\n      This saves one bit per phyiscal swapfile slot.\n    * Rearrange the patches and the code change to make things more\n      reviewable. Suggested by Johannes Weiner.\n    * Update the cover letter a bit.\n\nThis patch series implements the virtual swap space idea, based on Yosry's\nproposals at LSFMMBPF 2023 (see [1], [2], [3]), as well as valuable\ninputs from Johannes Weiner. The same idea (with different\nimplementation details) has been floated by Rik van Riel since at least\n2011 (see [8]).\n\nThis patch series is based on 6.19. There are a couple more\nswap-related changes in the mm-stable branch that I would need to\ncoordinate with, but I would like to send this out as an update, to show\nthat the lock contention issues that plagued earlier versions have been\nresolved and performance on the kernel build benchmark is now on-par with\nbaseline. Furthermore, memory overhead has been substantially reduced\ncompared to the last RFC version.\n\n\nI. Motivation\n\nCurrently, when an anon page is swapped out, a slot in a backing swap\ndevice is allocated and stored in the page table entries that refer to\nthe original page. This slot is also used as the \"key\" to find the\nswapped out content, as well as the index to swap data structures, such\nas the swap cache, or the swap cgroup mapping. Tying a swap entry to its\nbacking slot in this way is performant and efficient when swap is purely\njust disk space, and swapoff is rare.\n\nHowever, the advent of many swap optimizations has exposed major\ndrawbacks of this design. The first problem is that we occupy a physical\nslot in the swap space, even for pages that are NEVER expected to hit\nthe disk: pages compressed and stored in the zswap pool, zero-filled\npages, or pages rejected by both of these optimizations when zswap\nwriteback is disabled. This is the arguably central shortcoming of\nzswap:\n* In deployments when no disk space can be afforded for swap (such as\n  mobile and embedded devices), users cannot adopt zswap, and are forced\n  to use zram. This is confusing for users, and creates extra burdens\n  for developers, having to develop and maintain similar features for\n  two separate swap backends (writeback, cgroup charging, THP support,\n  etc.). For instance, see the discussion in [4].\n* Resource-wise, it is hugely wasteful in terms of disk usage. At Meta,\n  we have swapfile in the order of tens to hundreds of GBs, which are\n  mostly unused and only exist to enable zswap usage and zero-filled\n  pages swap optimizations.\n* Tying zswap (and more generally, other in-memory swap backends) to\n  the current physical swapfile infrastructure makes zswap implicitly\n  statically sized. This does not make sense, as unlike disk swap, in\n  which we consume a limited resource (disk space or swapfile space) to\n  save another resource (memory), zswap consume the same resource it is\n  saving (memory). The more we zswap, the more memory we have available,\n  not less. We are not rationing a limited resource when we limit\n  the size of he zswap pool, but rather we are capping the resource\n  (memory) saving potential of zswap. Under memory pressure, using\n  more zswap is almost always better than the alternative (disk IOs, or\n  even worse, OOMs), and dynamically sizing the zswap pool on demand\n  allows the system to flexibly respond to these precarious scenarios.\n* Operationally, static provisioning the swapfile for zswap pose\n  significant challenges, because the sysadmin has to prescribe how\n  much swap is needed a priori, for each combination of\n  (memory size x disk space x workload usage). It is even more\n  complicated when we take into account the variance of memory\n  compression, which changes the reclaim dynamics (and as a result,\n  swap space size requirement). The problem is further exarcebated for\n  users who rely on swap utilization (and exhaustion) as an OOM signal.\n\n  All of these factors make it very difficult to configure the swapfile\n  for zswap: too small of a swapfile and we risk preventable OOMs and\n  limit the memory saving potentials of zswap; too big of a swapfile\n  and we waste disk space and memory due to swap metadata overhead.\n  This dilemma becomes more drastic in high memory systems, which can\n  have up to TBs worth of memory.\n\nPast attempts to decouple disk and compressed swap backends, namely the\nghost swapfile approach (see [13]), as well as the alternative\ncompressed swap backend zram, have mainly focused on eliminating the\ndisk space usage of compressed backends. We want a solution that not\nonly tackles that same problem, but also achieve the dyamicization of\nswap space to maximize the memory saving potentials while reducing\noperational and static memory overhead.\n\nFinally, any swap redesign should support efficient backend transfer,\ni.e without having to perform the expensive page table walk to\nupdate all the PTEs that refer to the swap entry:\n* The main motivation for this requirement is zswap writeback. To quote\n  Johannes (from [14]): \"Combining compression with disk swap is\n  extremely powerful, because it dramatically reduces the worst aspects\n  of both: it reduces the memory footprint of compression by shedding\n  the coldest data to disk; it reduces the IO latencies and flash wear\n  of disk swap through the writeback cache. In practice, this reduces\n  *average event rates of the entire reclaim/paging/IO stack*.\"\n* Another motivation is to simplify swapoff, which is both complicated\n  and expensive in the current design, precisely because we are storing\n  an encoding of the backend positional information in the page table,\n  and thus requires a full page table walk to remove these references.\n\n\nII. High Level Design Overview\n\nTo fix the aforementioned issues, we need an abstraction that separates\na swap entry from its physical backing storage. IOW, we need to\n\\u201cvirtualize\\u201d the swap space: swap clients will work with a dynamically\nallocated virtual swap slot, storing it in page table entries, and\nusing it to index into various swap-related data structures. The\nbacking storage is decoupled from the virtual swap slot, and the newly\nintroduced layer will \\u201cresolve\\u201d the virtual swap slot to the actual\nstorage. This layer also manages other metadata of the swap entry, such\nas its lifetime information (swap count), via a dynamically allocated,\nper-swap-entry descriptor:\n\nstruct swp_desc {\n        union {\n                swp_slot_t         slot;                 /*     0     8 */\n                struct zswap_entry * zswap_entry;        /*     0     8 */\n        };                                               /*     0     8 */\n        union {\n                struct folio *     swap_cache;           /*     8     8 */\n                void *             shadow;               /*     8     8 */\n        };                                               /*     8     8 */\n        unsigned int               swap_count;           /*    16     4 */\n        unsigned short             memcgid:16;           /*    20: 0  2 */\n        bool                       in_swapcache:1;       /*    22: 0  1 */\n\n        /* Bitfield combined with previous fields */\n\n        enum swap_type             type:2;               /*    20:17  4 */\n\n        /* size: 24, cachelines: 1, members: 6 */\n        /* bit_padding: 13 bits */\n        /* last cacheline: 24 bytes */\n};\n\n(output from pahole).\n\nThis design allows us to:\n* Decouple zswap (and zeromapped swap entry) from backing swapfile:\n  simply associate the virtual swap slot with one of the supported\n  backends: a zswap entry, a zero-filled swap page, a slot on the\n  swapfile, or an in-memory page.\n* Simplify and optimize swapoff: we only have to fault the page in and\n  have the virtual swap slot points to the page instead of the on-disk\n  physical swap slot. No need to perform any page table walking.\n\nThe size of the virtual swap descriptor is 24 bytes. Note that this is\nnot all \"new\" overhead, as the swap descriptor will replace:\n* the swap_cgroup arrays (one per swap type) in the old design, which\n  is a massive source of static memory overhead. With the new design,\n  it is only allocated for used clusters.\n* the swap tables, which holds the swap cache and workingset shadows.\n* the zeromap bitmap, which is a bitmap of physical swap slots to\n  indicate whether the swapped out page is zero-filled or not.\n* huge chunk of the swap_map. The swap_map is now replaced by 2 bitmaps,\n  one for allocated slots, and one for bad slots, representing 3 possible\n  states of a slot on the swapfile: allocated, free, and bad.\n* the zswap tree.\n\nSo, in terms of additional memory overhead:\n* For zswap entries, the added memory overhead is rather minimal. The\n  new indirection pointer neatly replaces the existing zswap tree.\n  We really only incur less than one word of overhead for swap count\n  blow up (since we no longer use swap continuation) and the swap type.\n* For physical swap entries, the new design will impose fewer than 3 words\n  memory overhead. However, as noted above this overhead is only for\n  actively used swap entries, whereas in the current design the overhead is\n  static (including the swap cgroup array for example).\n\n  The primary victim of this overhead will be zram users. However, as\n  zswap now no longer takes up disk space, zram users can consider\n  switching to zswap (which, as a bonus, has a lot of useful features\n  out of the box, such as cgroup tracking, dynamic zswap pool sizing,\n  LRU-ordering writeback, etc.).\n\nFor a more concrete example, suppose we have a 32 GB swapfile (i.e.\n8,388,608 swap entries), and we use zswap.\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 0.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 48.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 96.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 121.00 MB\n* Vswap total overhead: 144.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 153.00 MB\n* Vswap total overhead: 193.00 MB\n\nSo even in the worst case scenario for virtual swap, i.e when we\nsomehow have an oracle to correctly size the swapfile for zswap\npool to 32 GB, the added overhead is only 40 MB, which is a mere\n0.12% of the total swapfile :)\n\nIn practice, the overhead will be closer to the 50-75% usage case, as\nsystems tend to leave swap headroom for pathological events or sudden\nspikes in memory requirements. The added overhead in these cases are\npractically neglible. And in deployments where swapfiles for zswap\nare previously sparsely used, switching over to virtual swap will\nactually reduce memory overhead.\n\nDoing the same math for the disk swap, which is the worst case for\nvirtual swap in terms of swap backends:\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 2.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 41.00 MB\n* Vswap total overhead: 66.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 130.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 73.00 MB\n* Vswap total overhead: 194.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 259.00 MB\n\nThe added overhead is 170MB, which is 0.5% of the total swapfile size,\nagain in the worst case when we have a sizing oracle.\n\nPlease see the attached patches for more implementation details.\n\n\nIII. Usage and Benchmarking\n\nThis patch series introduce no new syscalls or userspace API. Existing\nuserspace setups will work as-is, except we no longer have to create a\nswapfile or set memory.swap.max if we want to use zswap, as zswap is no\nlonger tied to physical swap. The zswap pool will be automatically and\ndynamically sized based on memory usage and reclaim dynamics.\n\nTo measure the performance of the new implementation, I have run the\nfollowing benchmarks:\n\n1. Kernel building: 52 workers (one per processor), memory.max = 3G.\n\nUsing zswap as the backend:\n\nBaseline:\nreal: mean: 185.2s, stdev: 0.93s\nsys: mean: 683.7s, stdev: 33.77s\n\nVswap:\nreal: mean: 184.88s, stdev: 0.57s\nsys: mean: 675.14s, stdev: 32.8s\n\nWe actually see a slight improvement in systime (by 1.5%) :) This is\nlikely because we no longer have to perform swap charging for zswap\nentries, and virtual swap allocator is simpler than that of physical\nswap.\n\nUsing SSD swap as the backend:\n\nBaseline:\nreal: mean: 200.3s, stdev: 2.33s\nsys: mean: 489.88s, stdev: 9.62s\n\nVswap:\nreal: mean: 201.47s, stdev: 2.98s\nsys: mean: 487.36s, stdev: 5.53s\n\nThe performance is neck-to-neck.\n\n\nIV. Future Use Cases\n\nWhile the patch series focus on two applications (decoupling swap\nbackends and swapoff optimization/simplification), this new,\nfuture-proof design also allows us to implement new swap features more\neasily and efficiently:\n\n* Multi-tier swapping (as mentioned in [5]), with transparent\n  transferring (promotion/demotion) of pages across tiers (see [8] and\n  [9]). Similar to swapoff, with the old design we would need to\n  perform the expensive page table walk.\n* Swapfile compaction to alleviate fragmentation (as proposed by Ying\n  Huang in [6]).\n* Mixed backing THP swapin (see [7]): Once you have pinned down the\n  backing store of THPs, then you can dispatch each range of subpages\n  to appropriate backend swapin handler.\n* Swapping a folio out with discontiguous physical swap slots\n  (see [10]).\n* Zswap writeback optimization: The current architecture pre-reserves\n  physical swap space for pages when they enter the zswap pool, giving\n  the kernel no flexibility at writeback time. With the virtual swap\n  implementation, the backends are decoupled, and physical swap space\n  is allocated on-demand at writeback time, at which point we can make\n  much smarter decisions: we can batch multiple zswap writeback\n  operations into a single IO request, allocating contiguous physical\n  swap slots for that request. We can even perform compressed writeback\n  (i.e writing these pages without decompressing them) (see [12]).\n\n\nV. References\n\n[1]: https://lore.kernel.org/all/CAJD7tkbCnXJ95Qow_aOjNX6NOMU5ovMSHRC+95U4wtW6cM+puw@mail.gmail.com/\n[2]: https://lwn.net/Articles/932077/\n[3]: https://www.youtube.com/watch?v=Hwqw_TBGEhg\n[4]: https://lore.kernel.org/all/Zqe_Nab-Df1CN7iW@infradead.org/\n[5]: https://lore.kernel.org/lkml/CAF8kJuN-4UE0skVHvjUzpGefavkLULMonjgkXUZSBVJrcGFXCA@mail.gmail.com/\n[6]: https://lore.kernel.org/linux-mm/87o78mzp24.fsf@yhuang6-desk2.ccr.corp.intel.com/\n[7]: https://lore.kernel.org/all/CAGsJ_4ysCN6f7qt=6gvee1x3ttbOnifGneqcRm9Hoeun=uFQ2w@mail.gmail.com/\n[8]: https://lore.kernel.org/linux-mm/4DA25039.3020700@redhat.com/\n[9]: https://lore.kernel.org/all/CA+ZsKJ7DCE8PMOSaVmsmYZL9poxK6rn0gvVXbjpqxMwxS2C9TQ@mail.gmail.com/\n[10]: https://lore.kernel.org/all/CACePvbUkMYMencuKfpDqtG1Ej7LiUS87VRAXb8sBn1yANikEmQ@mail.gmail.com/\n[11]: https://lore.kernel.org/all/CAMgjq7BvQ0ZXvyLGp2YP96+i+6COCBBJCYmjXHGBnfisCAb8VA@mail.gmail.com/\n[12]: https://lore.kernel.org/linux-mm/ZeZSDLWwDed0CgT3@casper.infradead.org/\n[13]: https://lore.kernel.org/all/20251121-ghost-v1-1-cfc0efcf3855@kernel.org/\n[14]: https://lore.kernel.org/linux-mm/20251202170222.GD430226@cmpxchg.org/\n\nNhat Pham (20):\n  mm/swap: decouple swap cache from physical swap infrastructure\n  swap: rearrange the swap header file\n  mm: swap: add an abstract API for locking out swapoff\n  zswap: add new helpers for zswap entry operations\n  mm/swap: add a new function to check if a swap entry is in swap\n    cached.\n  mm: swap: add a separate type for physical swap slots\n  mm: create scaffolds for the new virtual swap implementation\n  zswap: prepare zswap for swap virtualization\n  mm: swap: allocate a virtual swap slot for each swapped out page\n  swap: move swap cache to virtual swap descriptor\n  zswap: move zswap entry management to the virtual swap descriptor\n  swap: implement the swap_cgroup API using virtual swap\n  swap: manage swap entry lifecycle at the virtual swap layer\n  mm: swap: decouple virtual swap slot from backing store\n  zswap: do not start zswap shrinker if there is no physical swap slots\n  swap: do not unnecesarily pin readahead swap entries\n  swapfile: remove zeromap bitmap\n  memcg: swap: only charge physical swap slots\n  swap: simplify swapoff using virtual swap\n  swapfile: replace the swap map with bitmaps\n\n Documentation/mm/swap-table.rst |   69 --\n MAINTAINERS                     |    2 +\n include/linux/cpuhotplug.h      |    1 +\n include/linux/mm_types.h        |   16 +\n include/linux/shmem_fs.h        |    7 +-\n include/linux/swap.h            |  135 ++-\n include/linux/swap_cgroup.h     |   13 -\n include/linux/swapops.h         |   25 +\n include/linux/zswap.h           |   17 +-\n kernel/power/swap.c             |    6 +-\n mm/Makefile                     |    5 +-\n mm/huge_memory.c                |   11 +-\n mm/internal.h                   |   12 +-\n mm/memcontrol-v1.c              |    6 +\n mm/memcontrol.c                 |  142 ++-\n mm/memory.c                     |  101 +-\n mm/migrate.c                    |   13 +-\n mm/mincore.c                    |   15 +-\n mm/page_io.c                    |   83 +-\n mm/shmem.c                      |  215 +---\n mm/swap.h                       |  157 +--\n mm/swap_cgroup.c                |  172 ---\n mm/swap_state.c                 |  306 +----\n mm/swap_table.h                 |   78 +-\n mm/swapfile.c                   | 1518 ++++-------------------\n mm/userfaultfd.c                |   18 +-\n mm/vmscan.c                     |   28 +-\n mm/vswap.c                      | 2025 +++++++++++++++++++++++++++++++\n mm/zswap.c                      |  142 +--\n 29 files changed, 2853 insertions(+), 2485 deletions(-)\n delete mode 100644 Documentation/mm/swap-table.rst\n delete mode 100644 mm/swap_cgroup.c\n create mode 100644 mm/vswap.c\n\n\nbase-commit: 05f7e89ab9731565d8a62e3b5d1ec206485eeb0b\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author apologized for a missing cover letter in the original patch series and explained that they are resending it, along with an updated changelog that addresses previous feedback on lock contention issues, memory overhead, and implementation details.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "apology",
                "clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "My sincerest apologies - it seems like the cover letter (and just the\ncover letter) fails to be sent out, for some reason. I'm trying to figure\nout what happened - it works when I send the entire patch series to\nmyself...\n\nAnyway, resending this (in-reply-to patch 1 of the series):\n\nChangelog:\n* RFC v2 -> v3:\n    * Implement a cluster-based allocation algorithm for virtual swap\n      slots, inspired by Kairui Song and Chris Li's implementation, as\n      well as Johannes Weiner's suggestions. This eliminates the lock\n\t  contention issues on the virtual swap layer.\n    * Re-use swap table for the reverse mapping.\n    * Remove CONFIG_VIRTUAL_SWAP.\n    * Reducing the size of the swap descriptor from 48 bytes to 24\n      bytes, i.e another 50% reduction in memory overhead from v2.\n    * Remove swap cache and zswap tree and use the swap descriptor\n      for this.\n    * Remove zeromap, and replace the swap_map bytemap with 2 bitmaps\n      (one for allocated slots, and one for bad slots).\n    * Rebase on top of 6.19 (7d0a66e4bb9081d75c82ec4957c50034cb0ea449)\n\t* Update cover letter to include new benchmark results and discussion\n\t  on overhead in various cases.\n* RFC v1 -> RFC v2:\n    * Use a single atomic type (swap_refs) for reference counting\n      purpose. This brings the size of the swap descriptor from 64 B\n      down to 48 B (25% reduction). Suggested by Yosry Ahmed.\n    * Zeromap bitmap is removed in the virtual swap implementation.\n      This saves one bit per phyiscal swapfile slot.\n    * Rearrange the patches and the code change to make things more\n      reviewable. Suggested by Johannes Weiner.\n    * Update the cover letter a bit.\n\nThis patch series implements the virtual swap space idea, based on Yosry's\nproposals at LSFMMBPF 2023 (see [1], [2], [3]), as well as valuable\ninputs from Johannes Weiner. The same idea (with different\nimplementation details) has been floated by Rik van Riel since at least\n2011 (see [8]).\n\nThis patch series is based on 6.19. There are a couple more\nswap-related changes in the mm-stable branch that I would need to\ncoordinate with, but I would like to send this out as an update, to show\nthat the lock contention issues that plagued earlier versions have been\nresolved and performance on the kernel build benchmark is now on-par with\nbaseline. Furthermore, memory overhead has been substantially reduced\ncompared to the last RFC version.\n\n\nI. Motivation\n\nCurrently, when an anon page is swapped out, a slot in a backing swap\ndevice is allocated and stored in the page table entries that refer to\nthe original page. This slot is also used as the \"key\" to find the\nswapped out content, as well as the index to swap data structures, such\nas the swap cache, or the swap cgroup mapping. Tying a swap entry to its\nbacking slot in this way is performant and efficient when swap is purely\njust disk space, and swapoff is rare.\n\nHowever, the advent of many swap optimizations has exposed major\ndrawbacks of this design. The first problem is that we occupy a physical\nslot in the swap space, even for pages that are NEVER expected to hit\nthe disk: pages compressed and stored in the zswap pool, zero-filled\npages, or pages rejected by both of these optimizations when zswap\nwriteback is disabled. This is the arguably central shortcoming of\nzswap:\n* In deployments when no disk space can be afforded for swap (such as\n  mobile and embedded devices), users cannot adopt zswap, and are forced\n  to use zram. This is confusing for users, and creates extra burdens\n  for developers, having to develop and maintain similar features for\n  two separate swap backends (writeback, cgroup charging, THP support,\n  etc.). For instance, see the discussion in [4].\n* Resource-wise, it is hugely wasteful in terms of disk usage. At Meta,\n  we have swapfile in the order of tens to hundreds of GBs, which are\n  mostly unused and only exist to enable zswap usage and zero-filled\n  pages swap optimizations.\n* Tying zswap (and more generally, other in-memory swap backends) to\n  the current physical swapfile infrastructure makes zswap implicitly\n  statically sized. This does not make sense, as unlike disk swap, in\n  which we consume a limited resource (disk space or swapfile space) to\n  save another resource (memory), zswap consume the same resource it is\n  saving (memory). The more we zswap, the more memory we have available,\n  not less. We are not rationing a limited resource when we limit\n  the size of he zswap pool, but rather we are capping the resource\n  (memory) saving potential of zswap. Under memory pressure, using\n  more zswap is almost always better than the alternative (disk IOs, or\n  even worse, OOMs), and dynamically sizing the zswap pool on demand\n  allows the system to flexibly respond to these precarious scenarios.\n* Operationally, static provisioning the swapfile for zswap pose\n  significant challenges, because the sysadmin has to prescribe how\n  much swap is needed a priori, for each combination of\n  (memory size x disk space x workload usage). It is even more\n  complicated when we take into account the variance of memory\n  compression, which changes the reclaim dynamics (and as a result,\n  swap space size requirement). The problem is further exarcebated for\n  users who rely on swap utilization (and exhaustion) as an OOM signal.\n\n  All of these factors make it very difficult to configure the swapfile\n  for zswap: too small of a swapfile and we risk preventable OOMs and\n  limit the memory saving potentials of zswap; too big of a swapfile\n  and we waste disk space and memory due to swap metadata overhead.\n  This dilemma becomes more drastic in high memory systems, which can\n  have up to TBs worth of memory.\n\nPast attempts to decouple disk and compressed swap backends, namely the\nghost swapfile approach (see [13]), as well as the alternative\ncompressed swap backend zram, have mainly focused on eliminating the\ndisk space usage of compressed backends. We want a solution that not\nonly tackles that same problem, but also achieve the dyamicization of\nswap space to maximize the memory saving potentials while reducing\noperational and static memory overhead.\n\nFinally, any swap redesign should support efficient backend transfer,\ni.e without having to perform the expensive page table walk to\nupdate all the PTEs that refer to the swap entry:\n* The main motivation for this requirement is zswap writeback. To quote\n  Johannes (from [14]): \"Combining compression with disk swap is\n  extremely powerful, because it dramatically reduces the worst aspects\n  of both: it reduces the memory footprint of compression by shedding\n  the coldest data to disk; it reduces the IO latencies and flash wear\n  of disk swap through the writeback cache. In practice, this reduces\n  *average event rates of the entire reclaim/paging/IO stack*.\"\n* Another motivation is to simplify swapoff, which is both complicated\n  and expensive in the current design, precisely because we are storing\n  an encoding of the backend positional information in the page table,\n  and thus requires a full page table walk to remove these references.\n\n\nII. High Level Design Overview\n\nTo fix the aforementioned issues, we need an abstraction that separates\na swap entry from its physical backing storage. IOW, we need to\n\\u201cvirtualize\\u201d the swap space: swap clients will work with a dynamically\nallocated virtual swap slot, storing it in page table entries, and\nusing it to index into various swap-related data structures. The\nbacking storage is decoupled from the virtual swap slot, and the newly\nintroduced layer will \\u201cresolve\\u201d the virtual swap slot to the actual\nstorage. This layer also manages other metadata of the swap entry, such\nas its lifetime information (swap count), via a dynamically allocated,\nper-swap-entry descriptor:\n\nstruct swp_desc {\n        union {\n                swp_slot_t         slot;                 /*     0     8 */\n                struct zswap_entry * zswap_entry;        /*     0     8 */\n        };                                               /*     0     8 */\n        union {\n                struct folio *     swap_cache;           /*     8     8 */\n                void *             shadow;               /*     8     8 */\n        };                                               /*     8     8 */\n        unsigned int               swap_count;           /*    16     4 */\n        unsigned short             memcgid:16;           /*    20: 0  2 */\n        bool                       in_swapcache:1;       /*    22: 0  1 */\n\n        /* Bitfield combined with previous fields */\n\n        enum swap_type             type:2;               /*    20:17  4 */\n\n        /* size: 24, cachelines: 1, members: 6 */\n        /* bit_padding: 13 bits */\n        /* last cacheline: 24 bytes */\n};\n\n(output from pahole).\n\nThis design allows us to:\n* Decouple zswap (and zeromapped swap entry) from backing swapfile:\n  simply associate the virtual swap slot with one of the supported\n  backends: a zswap entry, a zero-filled swap page, a slot on the\n  swapfile, or an in-memory page.\n* Simplify and optimize swapoff: we only have to fault the page in and\n  have the virtual swap slot points to the page instead of the on-disk\n  physical swap slot. No need to perform any page table walking.\n\nThe size of the virtual swap descriptor is 24 bytes. Note that this is\nnot all \"new\" overhead, as the swap descriptor will replace:\n* the swap_cgroup arrays (one per swap type) in the old design, which\n  is a massive source of static memory overhead. With the new design,\n  it is only allocated for used clusters.\n* the swap tables, which holds the swap cache and workingset shadows.\n* the zeromap bitmap, which is a bitmap of physical swap slots to\n  indicate whether the swapped out page is zero-filled or not.\n* huge chunk of the swap_map. The swap_map is now replaced by 2 bitmaps,\n  one for allocated slots, and one for bad slots, representing 3 possible\n  states of a slot on the swapfile: allocated, free, and bad.\n* the zswap tree.\n\nSo, in terms of additional memory overhead:\n* For zswap entries, the added memory overhead is rather minimal. The\n  new indirection pointer neatly replaces the existing zswap tree.\n  We really only incur less than one word of overhead for swap count\n  blow up (since we no longer use swap continuation) and the swap type.\n* For physical swap entries, the new design will impose fewer than 3 words\n  memory overhead. However, as noted above this overhead is only for\n  actively used swap entries, whereas in the current design the overhead is\n  static (including the swap cgroup array for example).\n\n  The primary victim of this overhead will be zram users. However, as\n  zswap now no longer takes up disk space, zram users can consider\n  switching to zswap (which, as a bonus, has a lot of useful features\n  out of the box, such as cgroup tracking, dynamic zswap pool sizing,\n  LRU-ordering writeback, etc.).\n\nFor a more concrete example, suppose we have a 32 GB swapfile (i.e.\n8,388,608 swap entries), and we use zswap.\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 0.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 48.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 96.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 121.00 MB\n* Vswap total overhead: 144.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 153.00 MB\n* Vswap total overhead: 193.00 MB\n\nSo even in the worst case scenario for virtual swap, i.e when we\nsomehow have an oracle to correctly size the swapfile for zswap\npool to 32 GB, the added overhead is only 40 MB, which is a mere\n0.12% of the total swapfile :)\n\nIn practice, the overhead will be closer to the 50-75% usage case, as\nsystems tend to leave swap headroom for pathological events or sudden\nspikes in memory requirements. The added overhead in these cases are\npractically neglible. And in deployments where swapfiles for zswap\nare previously sparsely used, switching over to virtual swap will\nactually reduce memory overhead.\n\nDoing the same math for the disk swap, which is the worst case for\nvirtual swap in terms of swap backends:\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 2.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 41.00 MB\n* Vswap total overhead: 66.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 130.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 73.00 MB\n* Vswap total overhead: 194.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 259.00 MB\n\nThe added overhead is 170MB, which is 0.5% of the total swapfile size,\nagain in the worst case when we have a sizing oracle.\n\nPlease see the attached patches for more implementation details.\n\n\nIII. Usage and Benchmarking\n\nThis patch series introduce no new syscalls or userspace API. Existing\nuserspace setups will work as-is, except we no longer have to create a\nswapfile or set memory.swap.max if we want to use zswap, as zswap is no\nlonger tied to physical swap. The zswap pool will be automatically and\ndynamically sized based on memory usage and reclaim dynamics.\n\nTo measure the performance of the new implementation, I have run the\nfollowing benchmarks:\n\n1. Kernel building: 52 workers (one per processor), memory.max = 3G.\n\nUsing zswap as the backend:\n\nBaseline:\nreal: mean: 185.2s, stdev: 0.93s\nsys: mean: 683.7s, stdev: 33.77s\n\nVswap:\nreal: mean: 184.88s, stdev: 0.57s\nsys: mean: 675.14s, stdev: 32.8s\n\nWe actually see a slight improvement in systime (by 1.5%) :) This is\nlikely because we no longer have to perform swap charging for zswap\nentries, and virtual swap allocator is simpler than that of physical\nswap.\n\nUsing SSD swap as the backend:\n\nBaseline:\nreal: mean: 200.3s, stdev: 2.33s\nsys: mean: 489.88s, stdev: 9.62s\n\nVswap:\nreal: mean: 201.47s, stdev: 2.98s\nsys: mean: 487.36s, stdev: 5.53s\n\nThe performance is neck-to-neck.\n\n\nIV. Future Use Cases\n\nWhile the patch series focus on two applications (decoupling swap\nbackends and swapoff optimization/simplification), this new,\nfuture-proof design also allows us to implement new swap features more\neasily and efficiently:\n\n* Multi-tier swapping (as mentioned in [5]), with transparent\n  transferring (promotion/demotion) of pages across tiers (see [8] and\n  [9]). Similar to swapoff, with the old design we would need to\n  perform the expensive page table walk.\n* Swapfile compaction to alleviate fragmentation (as proposed by Ying\n  Huang in [6]).\n* Mixed backing THP swapin (see [7]): Once you have pinned down the\n  backing store of THPs, then you can dispatch each range of subpages\n  to appropriate backend swapin handler.\n* Swapping a folio out with discontiguous physical swap slots\n  (see [10]).\n* Zswap writeback optimization: The current architecture pre-reserves\n  physical swap space for pages when they enter the zswap pool, giving\n  the kernel no flexibility at writeback time. With the virtual swap\n  implementation, the backends are decoupled, and physical swap space\n  is allocated on-demand at writeback time, at which point we can make\n  much smarter decisions: we can batch multiple zswap writeback\n  operations into a single IO request, allocating contiguous physical\n  swap slots for that request. We can even perform compressed writeback\n  (i.e writing these pages without decompressing them) (see [12]).\n\n\nV. References\n\n[1]: https://lore.kernel.org/all/CAJD7tkbCnXJ95Qow_aOjNX6NOMU5ovMSHRC+95U4wtW6cM+puw@mail.gmail.com/\n[2]: https://lwn.net/Articles/932077/\n[3]: https://www.youtube.com/watch?v=Hwqw_TBGEhg\n[4]: https://lore.kernel.org/all/Zqe_Nab-Df1CN7iW@infradead.org/\n[5]: https://lore.kernel.org/lkml/CAF8kJuN-4UE0skVHvjUzpGefavkLULMonjgkXUZSBVJrcGFXCA@mail.gmail.com/\n[6]: https://lore.kernel.org/linux-mm/87o78mzp24.fsf@yhuang6-desk2.ccr.corp.intel.com/\n[7]: https://lore.kernel.org/all/CAGsJ_4ysCN6f7qt=6gvee1x3ttbOnifGneqcRm9Hoeun=uFQ2w@mail.gmail.com/\n[8]: https://lore.kernel.org/linux-mm/4DA25039.3020700@redhat.com/\n[9]: https://lore.kernel.org/all/CA+ZsKJ7DCE8PMOSaVmsmYZL9poxK6rn0gvVXbjpqxMwxS2C9TQ@mail.gmail.com/\n[10]: https://lore.kernel.org/all/CACePvbUkMYMencuKfpDqtG1Ej7LiUS87VRAXb8sBn1yANikEmQ@mail.gmail.com/\n[11]: https://lore.kernel.org/all/CAMgjq7BvQ0ZXvyLGp2YP96+i+6COCBBJCYmjXHGBnfisCAb8VA@mail.gmail.com/\n[12]: https://lore.kernel.org/linux-mm/ZeZSDLWwDed0CgT3@casper.infradead.org/\n[13]: https://lore.kernel.org/all/20251121-ghost-v1-1-cfc0efcf3855@kernel.org/\n[14]: https://lore.kernel.org/linux-mm/20251202170222.GD430226@cmpxchg.org/\n\nNhat Pham (20):\n  mm/swap: decouple swap cache from physical swap infrastructure\n  swap: rearrange the swap header file\n  mm: swap: add an abstract API for locking out swapoff\n  zswap: add new helpers for zswap entry operations\n  mm/swap: add a new function to check if a swap entry is in swap\n    cached.\n  mm: swap: add a separate type for physical swap slots\n  mm: create scaffolds for the new virtual swap implementation\n  zswap: prepare zswap for swap virtualization\n  mm: swap: allocate a virtual swap slot for each swapped out page\n  swap: move swap cache to virtual swap descriptor\n  zswap: move zswap entry management to the virtual swap descriptor\n  swap: implement the swap_cgroup API using virtual swap\n  swap: manage swap entry lifecycle at the virtual swap layer\n  mm: swap: decouple virtual swap slot from backing store\n  zswap: do not start zswap shrinker if there is no physical swap slots\n  swap: do not unnecesarily pin readahead swap entries\n  swapfile: remove zeromap bitmap\n  memcg: swap: only charge physical swap slots\n  swap: simplify swapoff using virtual swap\n  swapfile: replace the swap map with bitmaps\n\n Documentation/mm/swap-table.rst |   69 --\n MAINTAINERS                     |    2 +\n include/linux/cpuhotplug.h      |    1 +\n include/linux/mm_types.h        |   16 +\n include/linux/shmem_fs.h        |    7 +-\n include/linux/swap.h            |  135 ++-\n include/linux/swap_cgroup.h     |   13 -\n include/linux/swapops.h         |   25 +\n include/linux/zswap.h           |   17 +-\n kernel/power/swap.c             |    6 +-\n mm/Makefile                     |    5 +-\n mm/huge_memory.c                |   11 +-\n mm/internal.h                   |   12 +-\n mm/memcontrol-v1.c              |    6 +\n mm/memcontrol.c                 |  142 ++-\n mm/memory.c                     |  101 +-\n mm/migrate.c                    |   13 +-\n mm/mincore.c                    |   15 +-\n mm/page_io.c                    |   83 +-\n mm/shmem.c                      |  215 +---\n mm/swap.h                       |  157 +--\n mm/swap_cgroup.c                |  172 ---\n mm/swap_state.c                 |  306 +----\n mm/swap_table.h                 |   78 +-\n mm/swapfile.c                   | 1518 ++++-------------------\n mm/userfaultfd.c                |   18 +-\n mm/vmscan.c                     |   28 +-\n mm/vswap.c                      | 2025 +++++++++++++++++++++++++++++++\n mm/zswap.c                      |  142 +--\n 29 files changed, 2853 insertions(+), 2485 deletions(-)\n delete mode 100644 Documentation/mm/swap-table.rst\n delete mode 100644 mm/swap_cgroup.c\n create mode 100644 mm/vswap.c\n\n\nbase-commit: 05f7e89ab9731565d8a62e3b5d1ec206485eeb0b\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed concerns about lock contention issues on the virtual swap layer by implementing a cluster-based allocation algorithm for virtual swap slots, inspired by Kairui Song and Chris Li's implementation, as well as Johannes Weiner's suggestions.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "My sincerest apologies - it seems like the cover letter (and just the\ncover letter) fails to be sent out, for some reason. I'm trying to figure\nout what happened - it works when I send the entire patch series to\nmyself...\n\nAnyway, resending this (in-reply-to patch 1 of the series):\n\nChangelog:\n* RFC v2 -> v3:\n    * Implement a cluster-based allocation algorithm for virtual swap\n      slots, inspired by Kairui Song and Chris Li's implementation, as\n      well as Johannes Weiner's suggestions. This eliminates the lock\n\t  contention issues on the virtual swap layer.\n    * Re-use swap table for the reverse mapping.\n    * Remove CONFIG_VIRTUAL_SWAP.\n    * Reducing the size of the swap descriptor from 48 bytes to 24\n      bytes, i.e another 50% reduction in memory overhead from v2.\n    * Remove swap cache and zswap tree and use the swap descriptor\n      for this.\n    * Remove zeromap, and replace the swap_map bytemap with 2 bitmaps\n      (one for allocated slots, and one for bad slots).\n    * Rebase on top of 6.19 (7d0a66e4bb9081d75c82ec4957c50034cb0ea449)\n\t* Update cover letter to include new benchmark results and discussion\n\t  on overhead in various cases.\n* RFC v1 -> RFC v2:\n    * Use a single atomic type (swap_refs) for reference counting\n      purpose. This brings the size of the swap descriptor from 64 B\n      down to 48 B (25% reduction). Suggested by Yosry Ahmed.\n    * Zeromap bitmap is removed in the virtual swap implementation.\n      This saves one bit per phyiscal swapfile slot.\n    * Rearrange the patches and the code change to make things more\n      reviewable. Suggested by Johannes Weiner.\n    * Update the cover letter a bit.\n\nThis patch series implements the virtual swap space idea, based on Yosry's\nproposals at LSFMMBPF 2023 (see [1], [2], [3]), as well as valuable\ninputs from Johannes Weiner. The same idea (with different\nimplementation details) has been floated by Rik van Riel since at least\n2011 (see [8]).\n\nThis patch series is based on 6.19. There are a couple more\nswap-related changes in the mm-stable branch that I would need to\ncoordinate with, but I would like to send this out as an update, to show\nthat the lock contention issues that plagued earlier versions have been\nresolved and performance on the kernel build benchmark is now on-par with\nbaseline. Furthermore, memory overhead has been substantially reduced\ncompared to the last RFC version.\n\n\nI. Motivation\n\nCurrently, when an anon page is swapped out, a slot in a backing swap\ndevice is allocated and stored in the page table entries that refer to\nthe original page. This slot is also used as the \"key\" to find the\nswapped out content, as well as the index to swap data structures, such\nas the swap cache, or the swap cgroup mapping. Tying a swap entry to its\nbacking slot in this way is performant and efficient when swap is purely\njust disk space, and swapoff is rare.\n\nHowever, the advent of many swap optimizations has exposed major\ndrawbacks of this design. The first problem is that we occupy a physical\nslot in the swap space, even for pages that are NEVER expected to hit\nthe disk: pages compressed and stored in the zswap pool, zero-filled\npages, or pages rejected by both of these optimizations when zswap\nwriteback is disabled. This is the arguably central shortcoming of\nzswap:\n* In deployments when no disk space can be afforded for swap (such as\n  mobile and embedded devices), users cannot adopt zswap, and are forced\n  to use zram. This is confusing for users, and creates extra burdens\n  for developers, having to develop and maintain similar features for\n  two separate swap backends (writeback, cgroup charging, THP support,\n  etc.). For instance, see the discussion in [4].\n* Resource-wise, it is hugely wasteful in terms of disk usage. At Meta,\n  we have swapfile in the order of tens to hundreds of GBs, which are\n  mostly unused and only exist to enable zswap usage and zero-filled\n  pages swap optimizations.\n* Tying zswap (and more generally, other in-memory swap backends) to\n  the current physical swapfile infrastructure makes zswap implicitly\n  statically sized. This does not make sense, as unlike disk swap, in\n  which we consume a limited resource (disk space or swapfile space) to\n  save another resource (memory), zswap consume the same resource it is\n  saving (memory). The more we zswap, the more memory we have available,\n  not less. We are not rationing a limited resource when we limit\n  the size of he zswap pool, but rather we are capping the resource\n  (memory) saving potential of zswap. Under memory pressure, using\n  more zswap is almost always better than the alternative (disk IOs, or\n  even worse, OOMs), and dynamically sizing the zswap pool on demand\n  allows the system to flexibly respond to these precarious scenarios.\n* Operationally, static provisioning the swapfile for zswap pose\n  significant challenges, because the sysadmin has to prescribe how\n  much swap is needed a priori, for each combination of\n  (memory size x disk space x workload usage). It is even more\n  complicated when we take into account the variance of memory\n  compression, which changes the reclaim dynamics (and as a result,\n  swap space size requirement). The problem is further exarcebated for\n  users who rely on swap utilization (and exhaustion) as an OOM signal.\n\n  All of these factors make it very difficult to configure the swapfile\n  for zswap: too small of a swapfile and we risk preventable OOMs and\n  limit the memory saving potentials of zswap; too big of a swapfile\n  and we waste disk space and memory due to swap metadata overhead.\n  This dilemma becomes more drastic in high memory systems, which can\n  have up to TBs worth of memory.\n\nPast attempts to decouple disk and compressed swap backends, namely the\nghost swapfile approach (see [13]), as well as the alternative\ncompressed swap backend zram, have mainly focused on eliminating the\ndisk space usage of compressed backends. We want a solution that not\nonly tackles that same problem, but also achieve the dyamicization of\nswap space to maximize the memory saving potentials while reducing\noperational and static memory overhead.\n\nFinally, any swap redesign should support efficient backend transfer,\ni.e without having to perform the expensive page table walk to\nupdate all the PTEs that refer to the swap entry:\n* The main motivation for this requirement is zswap writeback. To quote\n  Johannes (from [14]): \"Combining compression with disk swap is\n  extremely powerful, because it dramatically reduces the worst aspects\n  of both: it reduces the memory footprint of compression by shedding\n  the coldest data to disk; it reduces the IO latencies and flash wear\n  of disk swap through the writeback cache. In practice, this reduces\n  *average event rates of the entire reclaim/paging/IO stack*.\"\n* Another motivation is to simplify swapoff, which is both complicated\n  and expensive in the current design, precisely because we are storing\n  an encoding of the backend positional information in the page table,\n  and thus requires a full page table walk to remove these references.\n\n\nII. High Level Design Overview\n\nTo fix the aforementioned issues, we need an abstraction that separates\na swap entry from its physical backing storage. IOW, we need to\n\\u201cvirtualize\\u201d the swap space: swap clients will work with a dynamically\nallocated virtual swap slot, storing it in page table entries, and\nusing it to index into various swap-related data structures. The\nbacking storage is decoupled from the virtual swap slot, and the newly\nintroduced layer will \\u201cresolve\\u201d the virtual swap slot to the actual\nstorage. This layer also manages other metadata of the swap entry, such\nas its lifetime information (swap count), via a dynamically allocated,\nper-swap-entry descriptor:\n\nstruct swp_desc {\n        union {\n                swp_slot_t         slot;                 /*     0     8 */\n                struct zswap_entry * zswap_entry;        /*     0     8 */\n        };                                               /*     0     8 */\n        union {\n                struct folio *     swap_cache;           /*     8     8 */\n                void *             shadow;               /*     8     8 */\n        };                                               /*     8     8 */\n        unsigned int               swap_count;           /*    16     4 */\n        unsigned short             memcgid:16;           /*    20: 0  2 */\n        bool                       in_swapcache:1;       /*    22: 0  1 */\n\n        /* Bitfield combined with previous fields */\n\n        enum swap_type             type:2;               /*    20:17  4 */\n\n        /* size: 24, cachelines: 1, members: 6 */\n        /* bit_padding: 13 bits */\n        /* last cacheline: 24 bytes */\n};\n\n(output from pahole).\n\nThis design allows us to:\n* Decouple zswap (and zeromapped swap entry) from backing swapfile:\n  simply associate the virtual swap slot with one of the supported\n  backends: a zswap entry, a zero-filled swap page, a slot on the\n  swapfile, or an in-memory page.\n* Simplify and optimize swapoff: we only have to fault the page in and\n  have the virtual swap slot points to the page instead of the on-disk\n  physical swap slot. No need to perform any page table walking.\n\nThe size of the virtual swap descriptor is 24 bytes. Note that this is\nnot all \"new\" overhead, as the swap descriptor will replace:\n* the swap_cgroup arrays (one per swap type) in the old design, which\n  is a massive source of static memory overhead. With the new design,\n  it is only allocated for used clusters.\n* the swap tables, which holds the swap cache and workingset shadows.\n* the zeromap bitmap, which is a bitmap of physical swap slots to\n  indicate whether the swapped out page is zero-filled or not.\n* huge chunk of the swap_map. The swap_map is now replaced by 2 bitmaps,\n  one for allocated slots, and one for bad slots, representing 3 possible\n  states of a slot on the swapfile: allocated, free, and bad.\n* the zswap tree.\n\nSo, in terms of additional memory overhead:\n* For zswap entries, the added memory overhead is rather minimal. The\n  new indirection pointer neatly replaces the existing zswap tree.\n  We really only incur less than one word of overhead for swap count\n  blow up (since we no longer use swap continuation) and the swap type.\n* For physical swap entries, the new design will impose fewer than 3 words\n  memory overhead. However, as noted above this overhead is only for\n  actively used swap entries, whereas in the current design the overhead is\n  static (including the swap cgroup array for example).\n\n  The primary victim of this overhead will be zram users. However, as\n  zswap now no longer takes up disk space, zram users can consider\n  switching to zswap (which, as a bonus, has a lot of useful features\n  out of the box, such as cgroup tracking, dynamic zswap pool sizing,\n  LRU-ordering writeback, etc.).\n\nFor a more concrete example, suppose we have a 32 GB swapfile (i.e.\n8,388,608 swap entries), and we use zswap.\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 0.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 48.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 96.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 121.00 MB\n* Vswap total overhead: 144.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 153.00 MB\n* Vswap total overhead: 193.00 MB\n\nSo even in the worst case scenario for virtual swap, i.e when we\nsomehow have an oracle to correctly size the swapfile for zswap\npool to 32 GB, the added overhead is only 40 MB, which is a mere\n0.12% of the total swapfile :)\n\nIn practice, the overhead will be closer to the 50-75% usage case, as\nsystems tend to leave swap headroom for pathological events or sudden\nspikes in memory requirements. The added overhead in these cases are\npractically neglible. And in deployments where swapfiles for zswap\nare previously sparsely used, switching over to virtual swap will\nactually reduce memory overhead.\n\nDoing the same math for the disk swap, which is the worst case for\nvirtual swap in terms of swap backends:\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 2.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 41.00 MB\n* Vswap total overhead: 66.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 130.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 73.00 MB\n* Vswap total overhead: 194.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 259.00 MB\n\nThe added overhead is 170MB, which is 0.5% of the total swapfile size,\nagain in the worst case when we have a sizing oracle.\n\nPlease see the attached patches for more implementation details.\n\n\nIII. Usage and Benchmarking\n\nThis patch series introduce no new syscalls or userspace API. Existing\nuserspace setups will work as-is, except we no longer have to create a\nswapfile or set memory.swap.max if we want to use zswap, as zswap is no\nlonger tied to physical swap. The zswap pool will be automatically and\ndynamically sized based on memory usage and reclaim dynamics.\n\nTo measure the performance of the new implementation, I have run the\nfollowing benchmarks:\n\n1. Kernel building: 52 workers (one per processor), memory.max = 3G.\n\nUsing zswap as the backend:\n\nBaseline:\nreal: mean: 185.2s, stdev: 0.93s\nsys: mean: 683.7s, stdev: 33.77s\n\nVswap:\nreal: mean: 184.88s, stdev: 0.57s\nsys: mean: 675.14s, stdev: 32.8s\n\nWe actually see a slight improvement in systime (by 1.5%) :) This is\nlikely because we no longer have to perform swap charging for zswap\nentries, and virtual swap allocator is simpler than that of physical\nswap.\n\nUsing SSD swap as the backend:\n\nBaseline:\nreal: mean: 200.3s, stdev: 2.33s\nsys: mean: 489.88s, stdev: 9.62s\n\nVswap:\nreal: mean: 201.47s, stdev: 2.98s\nsys: mean: 487.36s, stdev: 5.53s\n\nThe performance is neck-to-neck.\n\n\nIV. Future Use Cases\n\nWhile the patch series focus on two applications (decoupling swap\nbackends and swapoff optimization/simplification), this new,\nfuture-proof design also allows us to implement new swap features more\neasily and efficiently:\n\n* Multi-tier swapping (as mentioned in [5]), with transparent\n  transferring (promotion/demotion) of pages across tiers (see [8] and\n  [9]). Similar to swapoff, with the old design we would need to\n  perform the expensive page table walk.\n* Swapfile compaction to alleviate fragmentation (as proposed by Ying\n  Huang in [6]).\n* Mixed backing THP swapin (see [7]): Once you have pinned down the\n  backing store of THPs, then you can dispatch each range of subpages\n  to appropriate backend swapin handler.\n* Swapping a folio out with discontiguous physical swap slots\n  (see [10]).\n* Zswap writeback optimization: The current architecture pre-reserves\n  physical swap space for pages when they enter the zswap pool, giving\n  the kernel no flexibility at writeback time. With the virtual swap\n  implementation, the backends are decoupled, and physical swap space\n  is allocated on-demand at writeback time, at which point we can make\n  much smarter decisions: we can batch multiple zswap writeback\n  operations into a single IO request, allocating contiguous physical\n  swap slots for that request. We can even perform compressed writeback\n  (i.e writing these pages without decompressing them) (see [12]).\n\n\nV. References\n\n[1]: https://lore.kernel.org/all/CAJD7tkbCnXJ95Qow_aOjNX6NOMU5ovMSHRC+95U4wtW6cM+puw@mail.gmail.com/\n[2]: https://lwn.net/Articles/932077/\n[3]: https://www.youtube.com/watch?v=Hwqw_TBGEhg\n[4]: https://lore.kernel.org/all/Zqe_Nab-Df1CN7iW@infradead.org/\n[5]: https://lore.kernel.org/lkml/CAF8kJuN-4UE0skVHvjUzpGefavkLULMonjgkXUZSBVJrcGFXCA@mail.gmail.com/\n[6]: https://lore.kernel.org/linux-mm/87o78mzp24.fsf@yhuang6-desk2.ccr.corp.intel.com/\n[7]: https://lore.kernel.org/all/CAGsJ_4ysCN6f7qt=6gvee1x3ttbOnifGneqcRm9Hoeun=uFQ2w@mail.gmail.com/\n[8]: https://lore.kernel.org/linux-mm/4DA25039.3020700@redhat.com/\n[9]: https://lore.kernel.org/all/CA+ZsKJ7DCE8PMOSaVmsmYZL9poxK6rn0gvVXbjpqxMwxS2C9TQ@mail.gmail.com/\n[10]: https://lore.kernel.org/all/CACePvbUkMYMencuKfpDqtG1Ej7LiUS87VRAXb8sBn1yANikEmQ@mail.gmail.com/\n[11]: https://lore.kernel.org/all/CAMgjq7BvQ0ZXvyLGp2YP96+i+6COCBBJCYmjXHGBnfisCAb8VA@mail.gmail.com/\n[12]: https://lore.kernel.org/linux-mm/ZeZSDLWwDed0CgT3@casper.infradead.org/\n[13]: https://lore.kernel.org/all/20251121-ghost-v1-1-cfc0efcf3855@kernel.org/\n[14]: https://lore.kernel.org/linux-mm/20251202170222.GD430226@cmpxchg.org/\n\nNhat Pham (20):\n  mm/swap: decouple swap cache from physical swap infrastructure\n  swap: rearrange the swap header file\n  mm: swap: add an abstract API for locking out swapoff\n  zswap: add new helpers for zswap entry operations\n  mm/swap: add a new function to check if a swap entry is in swap\n    cached.\n  mm: swap: add a separate type for physical swap slots\n  mm: create scaffolds for the new virtual swap implementation\n  zswap: prepare zswap for swap virtualization\n  mm: swap: allocate a virtual swap slot for each swapped out page\n  swap: move swap cache to virtual swap descriptor\n  zswap: move zswap entry management to the virtual swap descriptor\n  swap: implement the swap_cgroup API using virtual swap\n  swap: manage swap entry lifecycle at the virtual swap layer\n  mm: swap: decouple virtual swap slot from backing store\n  zswap: do not start zswap shrinker if there is no physical swap slots\n  swap: do not unnecesarily pin readahead swap entries\n  swapfile: remove zeromap bitmap\n  memcg: swap: only charge physical swap slots\n  swap: simplify swapoff using virtual swap\n  swapfile: replace the swap map with bitmaps\n\n Documentation/mm/swap-table.rst |   69 --\n MAINTAINERS                     |    2 +\n include/linux/cpuhotplug.h      |    1 +\n include/linux/mm_types.h        |   16 +\n include/linux/shmem_fs.h        |    7 +-\n include/linux/swap.h            |  135 ++-\n include/linux/swap_cgroup.h     |   13 -\n include/linux/swapops.h         |   25 +\n include/linux/zswap.h           |   17 +-\n kernel/power/swap.c             |    6 +-\n mm/Makefile                     |    5 +-\n mm/huge_memory.c                |   11 +-\n mm/internal.h                   |   12 +-\n mm/memcontrol-v1.c              |    6 +\n mm/memcontrol.c                 |  142 ++-\n mm/memory.c                     |  101 +-\n mm/migrate.c                    |   13 +-\n mm/mincore.c                    |   15 +-\n mm/page_io.c                    |   83 +-\n mm/shmem.c                      |  215 +---\n mm/swap.h                       |  157 +--\n mm/swap_cgroup.c                |  172 ---\n mm/swap_state.c                 |  306 +----\n mm/swap_table.h                 |   78 +-\n mm/swapfile.c                   | 1518 ++++-------------------\n mm/userfaultfd.c                |   18 +-\n mm/vmscan.c                     |   28 +-\n mm/vswap.c                      | 2025 +++++++++++++++++++++++++++++++\n mm/zswap.c                      |  142 +--\n 29 files changed, 2853 insertions(+), 2485 deletions(-)\n delete mode 100644 Documentation/mm/swap-table.rst\n delete mode 100644 mm/swap_cgroup.c\n create mode 100644 mm/vswap.c\n\n\nbase-commit: 05f7e89ab9731565d8a62e3b5d1ec206485eeb0b\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author addressed concerns about lock contention issues on the virtual swap layer by implementing a cluster-based allocation algorithm for virtual swap slots, inspired by Kairui Song and Chris Li's implementation, as well as Johannes Weiner's suggestions.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "My sincerest apologies - it seems like the cover letter (and just the\ncover letter) fails to be sent out, for some reason. I'm trying to figure\nout what happened - it works when I send the entire patch series to\nmyself...\n\nAnyway, resending this (in-reply-to patch 1 of the series):\n\nChangelog:\n* RFC v2 -> v3:\n    * Implement a cluster-based allocation algorithm for virtual swap\n      slots, inspired by Kairui Song and Chris Li's implementation, as\n      well as Johannes Weiner's suggestions. This eliminates the lock\n\t  contention issues on the virtual swap layer.\n    * Re-use swap table for the reverse mapping.\n    * Remove CONFIG_VIRTUAL_SWAP.\n    * Reducing the size of the swap descriptor from 48 bytes to 24\n      bytes, i.e another 50% reduction in memory overhead from v2.\n    * Remove swap cache and zswap tree and use the swap descriptor\n      for this.\n    * Remove zeromap, and replace the swap_map bytemap with 2 bitmaps\n      (one for allocated slots, and one for bad slots).\n    * Rebase on top of 6.19 (7d0a66e4bb9081d75c82ec4957c50034cb0ea449)\n\t* Update cover letter to include new benchmark results and discussion\n\t  on overhead in various cases.\n* RFC v1 -> RFC v2:\n    * Use a single atomic type (swap_refs) for reference counting\n      purpose. This brings the size of the swap descriptor from 64 B\n      down to 48 B (25% reduction). Suggested by Yosry Ahmed.\n    * Zeromap bitmap is removed in the virtual swap implementation.\n      This saves one bit per phyiscal swapfile slot.\n    * Rearrange the patches and the code change to make things more\n      reviewable. Suggested by Johannes Weiner.\n    * Update the cover letter a bit.\n\nThis patch series implements the virtual swap space idea, based on Yosry's\nproposals at LSFMMBPF 2023 (see [1], [2], [3]), as well as valuable\ninputs from Johannes Weiner. The same idea (with different\nimplementation details) has been floated by Rik van Riel since at least\n2011 (see [8]).\n\nThis patch series is based on 6.19. There are a couple more\nswap-related changes in the mm-stable branch that I would need to\ncoordinate with, but I would like to send this out as an update, to show\nthat the lock contention issues that plagued earlier versions have been\nresolved and performance on the kernel build benchmark is now on-par with\nbaseline. Furthermore, memory overhead has been substantially reduced\ncompared to the last RFC version.\n\n\nI. Motivation\n\nCurrently, when an anon page is swapped out, a slot in a backing swap\ndevice is allocated and stored in the page table entries that refer to\nthe original page. This slot is also used as the \"key\" to find the\nswapped out content, as well as the index to swap data structures, such\nas the swap cache, or the swap cgroup mapping. Tying a swap entry to its\nbacking slot in this way is performant and efficient when swap is purely\njust disk space, and swapoff is rare.\n\nHowever, the advent of many swap optimizations has exposed major\ndrawbacks of this design. The first problem is that we occupy a physical\nslot in the swap space, even for pages that are NEVER expected to hit\nthe disk: pages compressed and stored in the zswap pool, zero-filled\npages, or pages rejected by both of these optimizations when zswap\nwriteback is disabled. This is the arguably central shortcoming of\nzswap:\n* In deployments when no disk space can be afforded for swap (such as\n  mobile and embedded devices), users cannot adopt zswap, and are forced\n  to use zram. This is confusing for users, and creates extra burdens\n  for developers, having to develop and maintain similar features for\n  two separate swap backends (writeback, cgroup charging, THP support,\n  etc.). For instance, see the discussion in [4].\n* Resource-wise, it is hugely wasteful in terms of disk usage. At Meta,\n  we have swapfile in the order of tens to hundreds of GBs, which are\n  mostly unused and only exist to enable zswap usage and zero-filled\n  pages swap optimizations.\n* Tying zswap (and more generally, other in-memory swap backends) to\n  the current physical swapfile infrastructure makes zswap implicitly\n  statically sized. This does not make sense, as unlike disk swap, in\n  which we consume a limited resource (disk space or swapfile space) to\n  save another resource (memory), zswap consume the same resource it is\n  saving (memory). The more we zswap, the more memory we have available,\n  not less. We are not rationing a limited resource when we limit\n  the size of he zswap pool, but rather we are capping the resource\n  (memory) saving potential of zswap. Under memory pressure, using\n  more zswap is almost always better than the alternative (disk IOs, or\n  even worse, OOMs), and dynamically sizing the zswap pool on demand\n  allows the system to flexibly respond to these precarious scenarios.\n* Operationally, static provisioning the swapfile for zswap pose\n  significant challenges, because the sysadmin has to prescribe how\n  much swap is needed a priori, for each combination of\n  (memory size x disk space x workload usage). It is even more\n  complicated when we take into account the variance of memory\n  compression, which changes the reclaim dynamics (and as a result,\n  swap space size requirement). The problem is further exarcebated for\n  users who rely on swap utilization (and exhaustion) as an OOM signal.\n\n  All of these factors make it very difficult to configure the swapfile\n  for zswap: too small of a swapfile and we risk preventable OOMs and\n  limit the memory saving potentials of zswap; too big of a swapfile\n  and we waste disk space and memory due to swap metadata overhead.\n  This dilemma becomes more drastic in high memory systems, which can\n  have up to TBs worth of memory.\n\nPast attempts to decouple disk and compressed swap backends, namely the\nghost swapfile approach (see [13]), as well as the alternative\ncompressed swap backend zram, have mainly focused on eliminating the\ndisk space usage of compressed backends. We want a solution that not\nonly tackles that same problem, but also achieve the dyamicization of\nswap space to maximize the memory saving potentials while reducing\noperational and static memory overhead.\n\nFinally, any swap redesign should support efficient backend transfer,\ni.e without having to perform the expensive page table walk to\nupdate all the PTEs that refer to the swap entry:\n* The main motivation for this requirement is zswap writeback. To quote\n  Johannes (from [14]): \"Combining compression with disk swap is\n  extremely powerful, because it dramatically reduces the worst aspects\n  of both: it reduces the memory footprint of compression by shedding\n  the coldest data to disk; it reduces the IO latencies and flash wear\n  of disk swap through the writeback cache. In practice, this reduces\n  *average event rates of the entire reclaim/paging/IO stack*.\"\n* Another motivation is to simplify swapoff, which is both complicated\n  and expensive in the current design, precisely because we are storing\n  an encoding of the backend positional information in the page table,\n  and thus requires a full page table walk to remove these references.\n\n\nII. High Level Design Overview\n\nTo fix the aforementioned issues, we need an abstraction that separates\na swap entry from its physical backing storage. IOW, we need to\n\\u201cvirtualize\\u201d the swap space: swap clients will work with a dynamically\nallocated virtual swap slot, storing it in page table entries, and\nusing it to index into various swap-related data structures. The\nbacking storage is decoupled from the virtual swap slot, and the newly\nintroduced layer will \\u201cresolve\\u201d the virtual swap slot to the actual\nstorage. This layer also manages other metadata of the swap entry, such\nas its lifetime information (swap count), via a dynamically allocated,\nper-swap-entry descriptor:\n\nstruct swp_desc {\n        union {\n                swp_slot_t         slot;                 /*     0     8 */\n                struct zswap_entry * zswap_entry;        /*     0     8 */\n        };                                               /*     0     8 */\n        union {\n                struct folio *     swap_cache;           /*     8     8 */\n                void *             shadow;               /*     8     8 */\n        };                                               /*     8     8 */\n        unsigned int               swap_count;           /*    16     4 */\n        unsigned short             memcgid:16;           /*    20: 0  2 */\n        bool                       in_swapcache:1;       /*    22: 0  1 */\n\n        /* Bitfield combined with previous fields */\n\n        enum swap_type             type:2;               /*    20:17  4 */\n\n        /* size: 24, cachelines: 1, members: 6 */\n        /* bit_padding: 13 bits */\n        /* last cacheline: 24 bytes */\n};\n\n(output from pahole).\n\nThis design allows us to:\n* Decouple zswap (and zeromapped swap entry) from backing swapfile:\n  simply associate the virtual swap slot with one of the supported\n  backends: a zswap entry, a zero-filled swap page, a slot on the\n  swapfile, or an in-memory page.\n* Simplify and optimize swapoff: we only have to fault the page in and\n  have the virtual swap slot points to the page instead of the on-disk\n  physical swap slot. No need to perform any page table walking.\n\nThe size of the virtual swap descriptor is 24 bytes. Note that this is\nnot all \"new\" overhead, as the swap descriptor will replace:\n* the swap_cgroup arrays (one per swap type) in the old design, which\n  is a massive source of static memory overhead. With the new design,\n  it is only allocated for used clusters.\n* the swap tables, which holds the swap cache and workingset shadows.\n* the zeromap bitmap, which is a bitmap of physical swap slots to\n  indicate whether the swapped out page is zero-filled or not.\n* huge chunk of the swap_map. The swap_map is now replaced by 2 bitmaps,\n  one for allocated slots, and one for bad slots, representing 3 possible\n  states of a slot on the swapfile: allocated, free, and bad.\n* the zswap tree.\n\nSo, in terms of additional memory overhead:\n* For zswap entries, the added memory overhead is rather minimal. The\n  new indirection pointer neatly replaces the existing zswap tree.\n  We really only incur less than one word of overhead for swap count\n  blow up (since we no longer use swap continuation) and the swap type.\n* For physical swap entries, the new design will impose fewer than 3 words\n  memory overhead. However, as noted above this overhead is only for\n  actively used swap entries, whereas in the current design the overhead is\n  static (including the swap cgroup array for example).\n\n  The primary victim of this overhead will be zram users. However, as\n  zswap now no longer takes up disk space, zram users can consider\n  switching to zswap (which, as a bonus, has a lot of useful features\n  out of the box, such as cgroup tracking, dynamic zswap pool sizing,\n  LRU-ordering writeback, etc.).\n\nFor a more concrete example, suppose we have a 32 GB swapfile (i.e.\n8,388,608 swap entries), and we use zswap.\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 0.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 48.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 96.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 121.00 MB\n* Vswap total overhead: 144.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 153.00 MB\n* Vswap total overhead: 193.00 MB\n\nSo even in the worst case scenario for virtual swap, i.e when we\nsomehow have an oracle to correctly size the swapfile for zswap\npool to 32 GB, the added overhead is only 40 MB, which is a mere\n0.12% of the total swapfile :)\n\nIn practice, the overhead will be closer to the 50-75% usage case, as\nsystems tend to leave swap headroom for pathological events or sudden\nspikes in memory requirements. The added overhead in these cases are\npractically neglible. And in deployments where swapfiles for zswap\nare previously sparsely used, switching over to virtual swap will\nactually reduce memory overhead.\n\nDoing the same math for the disk swap, which is the worst case for\nvirtual swap in terms of swap backends:\n\n0% usage, or 0 entries: 0.00 MB\n* Old design total overhead: 25.00 MB\n* Vswap total overhead: 2.00 MB\n\n25% usage, or 2,097,152 entries:\n* Old design total overhead: 41.00 MB\n* Vswap total overhead: 66.25 MB\n\n50% usage, or 4,194,304 entries:\n* Old design total overhead: 57.00 MB\n* Vswap total overhead: 130.50 MB\n\n75% usage, or 6,291,456 entries:\n* Old design total overhead: 73.00 MB\n* Vswap total overhead: 194.75 MB\n\n100% usage, or 8,388,608 entries:\n* Old design total overhead: 89.00 MB\n* Vswap total overhead: 259.00 MB\n\nThe added overhead is 170MB, which is 0.5% of the total swapfile size,\nagain in the worst case when we have a sizing oracle.\n\nPlease see the attached patches for more implementation details.\n\n\nIII. Usage and Benchmarking\n\nThis patch series introduce no new syscalls or userspace API. Existing\nuserspace setups will work as-is, except we no longer have to create a\nswapfile or set memory.swap.max if we want to use zswap, as zswap is no\nlonger tied to physical swap. The zswap pool will be automatically and\ndynamically sized based on memory usage and reclaim dynamics.\n\nTo measure the performance of the new implementation, I have run the\nfollowing benchmarks:\n\n1. Kernel building: 52 workers (one per processor), memory.max = 3G.\n\nUsing zswap as the backend:\n\nBaseline:\nreal: mean: 185.2s, stdev: 0.93s\nsys: mean: 683.7s, stdev: 33.77s\n\nVswap:\nreal: mean: 184.88s, stdev: 0.57s\nsys: mean: 675.14s, stdev: 32.8s\n\nWe actually see a slight improvement in systime (by 1.5%) :) This is\nlikely because we no longer have to perform swap charging for zswap\nentries, and virtual swap allocator is simpler than that of physical\nswap.\n\nUsing SSD swap as the backend:\n\nBaseline:\nreal: mean: 200.3s, stdev: 2.33s\nsys: mean: 489.88s, stdev: 9.62s\n\nVswap:\nreal: mean: 201.47s, stdev: 2.98s\nsys: mean: 487.36s, stdev: 5.53s\n\nThe performance is neck-to-neck.\n\n\nIV. Future Use Cases\n\nWhile the patch series focus on two applications (decoupling swap\nbackends and swapoff optimization/simplification), this new,\nfuture-proof design also allows us to implement new swap features more\neasily and efficiently:\n\n* Multi-tier swapping (as mentioned in [5]), with transparent\n  transferring (promotion/demotion) of pages across tiers (see [8] and\n  [9]). Similar to swapoff, with the old design we would need to\n  perform the expensive page table walk.\n* Swapfile compaction to alleviate fragmentation (as proposed by Ying\n  Huang in [6]).\n* Mixed backing THP swapin (see [7]): Once you have pinned down the\n  backing store of THPs, then you can dispatch each range of subpages\n  to appropriate backend swapin handler.\n* Swapping a folio out with discontiguous physical swap slots\n  (see [10]).\n* Zswap writeback optimization: The current architecture pre-reserves\n  physical swap space for pages when they enter the zswap pool, giving\n  the kernel no flexibility at writeback time. With the virtual swap\n  implementation, the backends are decoupled, and physical swap space\n  is allocated on-demand at writeback time, at which point we can make\n  much smarter decisions: we can batch multiple zswap writeback\n  operations into a single IO request, allocating contiguous physical\n  swap slots for that request. We can even perform compressed writeback\n  (i.e writing these pages without decompressing them) (see [12]).\n\n\nV. References\n\n[1]: https://lore.kernel.org/all/CAJD7tkbCnXJ95Qow_aOjNX6NOMU5ovMSHRC+95U4wtW6cM+puw@mail.gmail.com/\n[2]: https://lwn.net/Articles/932077/\n[3]: https://www.youtube.com/watch?v=Hwqw_TBGEhg\n[4]: https://lore.kernel.org/all/Zqe_Nab-Df1CN7iW@infradead.org/\n[5]: https://lore.kernel.org/lkml/CAF8kJuN-4UE0skVHvjUzpGefavkLULMonjgkXUZSBVJrcGFXCA@mail.gmail.com/\n[6]: https://lore.kernel.org/linux-mm/87o78mzp24.fsf@yhuang6-desk2.ccr.corp.intel.com/\n[7]: https://lore.kernel.org/all/CAGsJ_4ysCN6f7qt=6gvee1x3ttbOnifGneqcRm9Hoeun=uFQ2w@mail.gmail.com/\n[8]: https://lore.kernel.org/linux-mm/4DA25039.3020700@redhat.com/\n[9]: https://lore.kernel.org/all/CA+ZsKJ7DCE8PMOSaVmsmYZL9poxK6rn0gvVXbjpqxMwxS2C9TQ@mail.gmail.com/\n[10]: https://lore.kernel.org/all/CACePvbUkMYMencuKfpDqtG1Ej7LiUS87VRAXb8sBn1yANikEmQ@mail.gmail.com/\n[11]: https://lore.kernel.org/all/CAMgjq7BvQ0ZXvyLGp2YP96+i+6COCBBJCYmjXHGBnfisCAb8VA@mail.gmail.com/\n[12]: https://lore.kernel.org/linux-mm/ZeZSDLWwDed0CgT3@casper.infradead.org/\n[13]: https://lore.kernel.org/all/20251121-ghost-v1-1-cfc0efcf3855@kernel.org/\n[14]: https://lore.kernel.org/linux-mm/20251202170222.GD430226@cmpxchg.org/\n\nNhat Pham (20):\n  mm/swap: decouple swap cache from physical swap infrastructure\n  swap: rearrange the swap header file\n  mm: swap: add an abstract API for locking out swapoff\n  zswap: add new helpers for zswap entry operations\n  mm/swap: add a new function to check if a swap entry is in swap\n    cached.\n  mm: swap: add a separate type for physical swap slots\n  mm: create scaffolds for the new virtual swap implementation\n  zswap: prepare zswap for swap virtualization\n  mm: swap: allocate a virtual swap slot for each swapped out page\n  swap: move swap cache to virtual swap descriptor\n  zswap: move zswap entry management to the virtual swap descriptor\n  swap: implement the swap_cgroup API using virtual swap\n  swap: manage swap entry lifecycle at the virtual swap layer\n  mm: swap: decouple virtual swap slot from backing store\n  zswap: do not start zswap shrinker if there is no physical swap slots\n  swap: do not unnecesarily pin readahead swap entries\n  swapfile: remove zeromap bitmap\n  memcg: swap: only charge physical swap slots\n  swap: simplify swapoff using virtual swap\n  swapfile: replace the swap map with bitmaps\n\n Documentation/mm/swap-table.rst |   69 --\n MAINTAINERS                     |    2 +\n include/linux/cpuhotplug.h      |    1 +\n include/linux/mm_types.h        |   16 +\n include/linux/shmem_fs.h        |    7 +-\n include/linux/swap.h            |  135 ++-\n include/linux/swap_cgroup.h     |   13 -\n include/linux/swapops.h         |   25 +\n include/linux/zswap.h           |   17 +-\n kernel/power/swap.c             |    6 +-\n mm/Makefile                     |    5 +-\n mm/huge_memory.c                |   11 +-\n mm/internal.h                   |   12 +-\n mm/memcontrol-v1.c              |    6 +\n mm/memcontrol.c                 |  142 ++-\n mm/memory.c                     |  101 +-\n mm/migrate.c                    |   13 +-\n mm/mincore.c                    |   15 +-\n mm/page_io.c                    |   83 +-\n mm/shmem.c                      |  215 +---\n mm/swap.h                       |  157 +--\n mm/swap_cgroup.c                |  172 ---\n mm/swap_state.c                 |  306 +----\n mm/swap_table.h                 |   78 +-\n mm/swapfile.c                   | 1518 ++++-------------------\n mm/userfaultfd.c                |   18 +-\n mm/vmscan.c                     |   28 +-\n mm/vswap.c                      | 2025 +++++++++++++++++++++++++++++++\n mm/zswap.c                      |  142 +--\n 29 files changed, 2853 insertions(+), 2485 deletions(-)\n delete mode 100644 Documentation/mm/swap-table.rst\n delete mode 100644 mm/swap_cgroup.c\n create mode 100644 mm/vswap.c\n\n\nbase-commit: 05f7e89ab9731565d8a62e3b5d1ec206485eeb0b\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-08",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that he received the original patch series, but there is no specific technical issue or concern raised in this comment.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "For the record I did receive your original V3 cover letter from the\nlinux-mm mailing list.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li questioned the increased per-swap slot entry overhead in the new implementation, noting that it jumps from 8 bytes (dynamic) to 24 bytes, which he considers an unnecessary cost compared to alternative solutions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "increased overhead",
                "unnecessary cost"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Is the per swap slot entry overhead 24 bytes in your implementation?\nThe current swap overhead is 3 static +8 dynamic, your 24 dynamic is a\nbig jump. You can argue that 8->24 is not a big jump . But it is an\nunnecessary price compared to the alternatives, which is 8 dynamic +\n4(optional redirect).",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the patch series does not apply to mm-unstable nor mm-stable, and he has a compile error on Fedora 43 due to an initialization from pointer to non-enclosed address space in mm/vswap.c. He requested that the maintainer be informed about this issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "compile_error",
                "patch_series_applicability"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Ah, you need to mention that in the first line to Andrew. Spell out\nthis series is not for Andrew to consume in the MM series. It can't\nany way because it does not apply to mm-unstable nor mm-stable.\n\nBTW, I have the following compile error with this series (fedora 43).\nSame config compile fine on v6.19.\n\nIn file included from ./include/linux/local_lock.h:5,\n                 from ./include/linux/mmzone.h:24,\n                 from ./include/linux/gfp.h:7,\n                 from ./include/linux/mm.h:7,\n                 from mm/vswap.c:7:\nmm/vswap.c: In function vswap_cpu_dead:\n./include/linux/percpu-defs.h:221:45: error: initialization from\npointer to non-enclosed address space\n  221 |         const void __percpu *__vpp_verify = (typeof((ptr) +\n0))NULL;    \\\n      |                                             ^\n./include/linux/local_lock_internal.h:105:40: note: in definition of\nmacro __local_lock_acquire\n  105 |                 __l = (local_lock_t *)(lock);\n         \\\n      |                                        ^~~~\n./include/linux/local_lock.h:17:41: note: in expansion of macro\n__local_lock\n   17 | #define local_lock(lock)                __local_lock(this_cpu_ptr(lock))\n      |                                         ^~~~~~~~~~~~\n./include/linux/percpu-defs.h:245:9: note: in expansion of macro\n__verify_pcpu_ptr\n  245 |         __verify_pcpu_ptr(ptr);\n         \\\n      |         ^~~~~~~~~~~~~~~~~\n./include/linux/percpu-defs.h:256:27: note: in expansion of macro raw_cpu_ptr\n  256 | #define this_cpu_ptr(ptr) raw_cpu_ptr(ptr)\n      |                           ^~~~~~~~~~~\n./include/linux/local_lock.h:17:54: note: in expansion of macro\nthis_cpu_ptr\n   17 | #define local_lock(lock)\n__local_lock(this_cpu_ptr(lock))\n      |\n^~~~~~~~~~~~\nmm/vswap.c:1518:9: note: in expansion of macro local_lock\n 1518 |         local_lock(&percpu_cluster->lock);\n      |         ^~~~~~~~~~",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li requested additional information on user space times to provide a more comprehensive view, and asked how many runs were performed for a specific standard deviation value of 32.8 seconds.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request_for_additional_info",
                "clarification_needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Can you show your user space time as well to complete the picture?\n\nHow many runs do you have for stdev 32.8s?",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li requested that the patch also include zram swap test data, citing that Android heavily uses zram for swapping.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Please include zram swap test data as well. Android heavily uses zram\nfor swapping.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "reviewer expressed concern that performance differences may not be fully captured by current tests, requested additional confirmation from others on performance measurement and suggested pushing swap testing to stress the system within OOM limits",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance",
                "testing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I strongly suspect there is some performance difference that hasn't\nbeen covered by your test yet. Need more conformation by others on the\nperformance measurement. The swap testing is tricky. You want to push\nto stress barely within the OOM limit. Need more data.\n\nChris",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "reviewer noted that the patch does not address the issue of swap cache being managed by multiple locks, which can lead to lock ordering violations and requested the use of a single global lock for swap cache accesses",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "lock ordering violation",
                "requested change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Chris,\n\nOn Mon, Feb 09, 2026 at 04:20:21AM -0800, Chris Li wrote:",
              "reply_to": "Chris Li",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "reviewer disputed the claim that the patch reduces network overhead, pointing out that it actually consolidates and eliminates multiple data structures",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "disagreement",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, this is not the net overhead.\n\nThe descriptor consolidates and eliminates several other data\nstructures.\n\nHere is the more detailed breakdown:",
              "reply_to": "Chris Li",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Dan Carpenter",
              "summary": "The reviewer pointed out a build warning in the vswap_alloc_swap_slot() function due to a potential dereference of a NULL folio pointer at line 733, and requested that the issue be fixed in a separate patch or commit.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "build warning",
                "potential dereference"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Nhat,\n\nkernel test robot noticed the following build warnings:\n\nurl:    https://github.com/intel-lab-lkp/linux/commits/Nhat-Pham/mm-swap-decouple-swap-cache-from-physical-swap-infrastructure/20260209-120606\nbase:   05f7e89ab9731565d8a62e3b5d1ec206485eeb0b\npatch link:    https://lore.kernel.org/r/20260208215839.87595-15-nphamcs%40gmail.com\npatch subject: [PATCH v3 14/20] mm: swap: decouple virtual swap slot from backing store\nconfig: powerpc-randconfig-r073-20260209 (https://download.01.org/0day-ci/archive/20260209/202602092300.lZO4Ee4N-lkp@intel.com/config)\ncompiler: powerpc-linux-gcc (GCC) 15.2.0\nsmatch version: v0.5.0-8994-gd50c5a4c\n\nIf you fix the issue in a separate patch/commit (i.e. not just a new version of\nthe same patch/commit), kindly add following tags\n| Reported-by: kernel test robot <lkp@intel.com>\n| Reported-by: Dan Carpenter <dan.carpenter@linaro.org>\n| Closes: https://lore.kernel.org/r/202602092300.lZO4Ee4N-lkp@intel.com/\n\nsmatch warnings:\nmm/vswap.c:733 vswap_alloc_swap_slot() warn: variable dereferenced before check 'folio' (see line 701)\n\nvim +/folio +733 mm/vswap.c\n\n19a5fe94e9aae4 Nhat Pham 2026-02-08  694  bool vswap_alloc_swap_slot(struct folio *folio)\n19a5fe94e9aae4 Nhat Pham 2026-02-08  695  {\n19a5fe94e9aae4 Nhat Pham 2026-02-08  696  \tint i, nr = folio_nr_pages(folio);\n19a5fe94e9aae4 Nhat Pham 2026-02-08  697  \tstruct vswap_cluster *cluster = NULL;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  698  \tstruct swap_info_struct *si;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  699  \tstruct swap_cluster_info *ci;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  700  \tswp_slot_t slot = { .val = 0 };\n19a5fe94e9aae4 Nhat Pham 2026-02-08 @701  \tswp_entry_t entry = folio->swap;\n\nfolio dereference here\n\n19a5fe94e9aae4 Nhat Pham 2026-02-08  702  \tstruct swp_desc *desc;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  703  \tbool fallback = false;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  704  \n19a5fe94e9aae4 Nhat Pham 2026-02-08  705  \t/*\n19a5fe94e9aae4 Nhat Pham 2026-02-08  706  \t * We might have already allocated a backing physical swap slot in past\n19a5fe94e9aae4 Nhat Pham 2026-02-08  707  \t * attempts (for instance, when we disable zswap). If the entire range is\n19a5fe94e9aae4 Nhat Pham 2026-02-08  708  \t * already swapfile-backed we can skip swapfile case.\n19a5fe94e9aae4 Nhat Pham 2026-02-08  709  \t */\n19a5fe94e9aae4 Nhat Pham 2026-02-08  710  \tif (vswap_swapfile_backed(entry, nr))\n19a5fe94e9aae4 Nhat Pham 2026-02-08  711  \t\treturn true;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  712  \n19a5fe94e9aae4 Nhat Pham 2026-02-08  713  \tif (swap_slot_alloc(&slot, folio_order(folio)))\n\nand here\n\n19a5fe94e9aae4 Nhat Pham 2026-02-08  714  \t\treturn false;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  715  \n19a5fe94e9aae4 Nhat Pham 2026-02-08  716  \tif (!slot.val)\n19a5fe94e9aae4 Nhat Pham 2026-02-08  717  \t\treturn false;\n19a5fe94e9aae4 Nhat Pham 2026-02-08  718  \n7f88e3ea20f231 Nhat Pham 2026-02-08  719  \t/* establish the vrtual <-> physical swap slots linkages. */\n7f88e3ea20f231 Nhat Pham 2026-02-08  720  \tsi = __swap_slot_to_info(slot);\n7f88e3ea20f231 Nhat Pham 2026-02-08  721  \tci = swap_cluster_lock(si, swp_slot_offset(slot));\n7f88e3ea20f231 Nhat Pham 2026-02-08  722  \tvswap_rmap_set(ci, slot, entry.val, nr);\n7f88e3ea20f231 Nhat Pham 2026-02-08  723  \tswap_cluster_unlock(ci);\n7f88e3ea20f231 Nhat Pham 2026-02-08  724  \n7f88e3ea20f231 Nhat Pham 2026-02-08  725  \trcu_read_lock();\n7f88e3ea20f231 Nhat Pham 2026-02-08  726  \tfor (i = 0; i < nr; i++) {\n7f88e3ea20f231 Nhat Pham 2026-02-08  727  \t\tdesc = vswap_iter(&cluster, entry.val + i);\n7f88e3ea20f231 Nhat Pham 2026-02-08  728  \t\tVM_WARN_ON(!desc);\n7f88e3ea20f231 Nhat Pham 2026-02-08  729  \n19a5fe94e9aae4 Nhat Pham 2026-02-08  730  \t\tif (desc->type == VSWAP_FOLIO) {\n19a5fe94e9aae4 Nhat Pham 2026-02-08  731  \t\t\t/* case 1: fallback from zswap store failure */\n19a5fe94e9aae4 Nhat Pham 2026-02-08  732  \t\t\tfallback = true;\n19a5fe94e9aae4 Nhat Pham 2026-02-08 @733  \t\t\tif (!folio)\n\nSo it can't be NULL here.\n\n19a5fe94e9aae4 Nhat Pham 2026-02-08  734  \t\t\t\tfolio = desc->swap_cache;\n\nSo we'll never do this assignment and it will never become NULL.\n\n19a5fe94e9aae4 Nhat Pham 2026-02-08  735  \t\t\telse\n19a5fe94e9aae4 Nhat Pham 2026-02-08  736  \t\t\t\tVM_WARN_ON(folio != desc->swap_cache);\n19a5fe94e9aae4 Nhat Pham 2026-02-08  737  \t\t} else {\n19a5fe94e9aae4 Nhat Pham 2026-02-08  738  \t\t\t/*\n19a5fe94e9aae4 Nhat Pham 2026-02-08  739  \t\t\t * Case 2: zswap writeback.\n19a5fe94e9aae4 Nhat Pham 2026-02-08  740  \t\t\t *\n19a5fe94e9aae4 Nhat Pham 2026-02-08  741  \t\t\t * No need to free zswap entry here - it will be freed once zswap\n19a5fe94e9aae4 Nhat Pham 2026-02-08  742  \t\t\t * writeback suceeds.\n19a5fe94e9aae4 Nhat Pham 2026-02-08  743  \t\t\t */\n19a5fe94e9aae4 Nhat Pham 2026-02-08  744  \t\t\tVM_WARN_ON(desc->type != VSWAP_ZSWAP);\n19a5fe94e9aae4 Nhat Pham 2026-02-08  745  \t\t\tVM_WARN_ON(fallback);\n19a5fe94e9aae4 Nhat Pham 2026-02-08  746  \t\t}\n19a5fe94e9aae4 Nhat Pham 2026-02-08  747  \t\tdesc->type = VSWAP_SWAPFILE;\n7f88e3ea20f231 Nhat Pham 2026-02-08  748  \t\tdesc->slot.val = slot.val + i;\n7f88e3ea20f231 Nhat Pham 2026-02-08  749  \t}\n7f88e3ea20f231 Nhat Pham 2026-02-08  750  \tspin_unlock(&cluster->lock);\n7f88e3ea20f231 Nhat Pham 2026-02-08  751  \trcu_read_unlock();\n19a5fe94e9aae4 Nhat Pham 2026-02-08  752  \treturn true;\n7f88e3ea20f231 Nhat Pham 2026-02-08  753  }\n\n-- \n0-DAY CI Kernel Test Service\nhttps://github.com/intel/lkp-tests/wiki",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "syzbot ci",
              "summary": "The reviewer detected a possible deadlock in the vswap_iter function, where the task is trying to acquire the cluster->lock while already holding the mm->mmap_lock and rcu_read_lock locks, indicating a lock ordering violation.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "deadlock",
                "lock ordering violation"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "syzbot ci has tested the following series\n\n[v3] Virtual Swap Space\nhttps://lore.kernel.org/all/20260208215839.87595-1-nphamcs@gmail.com\n* [PATCH v3 01/20] mm/swap: decouple swap cache from physical swap infrastructure\n* [PATCH v3 02/20] swap: rearrange the swap header file\n* [PATCH v3 03/20] mm: swap: add an abstract API for locking out swapoff\n* [PATCH v3 04/20] zswap: add new helpers for zswap entry operations\n* [PATCH v3 05/20] mm/swap: add a new function to check if a swap entry is in swap cached.\n* [PATCH v3 06/20] mm: swap: add a separate type for physical swap slots\n* [PATCH v3 07/20] mm: create scaffolds for the new virtual swap implementation\n* [PATCH v3 08/20] zswap: prepare zswap for swap virtualization\n* [PATCH v3 09/20] mm: swap: allocate a virtual swap slot for each swapped out page\n* [PATCH v3 10/20] swap: move swap cache to virtual swap descriptor\n* [PATCH v3 11/20] zswap: move zswap entry management to the virtual swap descriptor\n* [PATCH v3 12/20] swap: implement the swap_cgroup API using virtual swap\n* [PATCH v3 13/20] swap: manage swap entry lifecycle at the virtual swap layer\n* [PATCH v3 14/20] mm: swap: decouple virtual swap slot from backing store\n* [PATCH v3 15/20] zswap: do not start zswap shrinker if there is no physical swap slots\n* [PATCH v3 16/20] swap: do not unnecesarily pin readahead swap entries\n* [PATCH v3 17/20] swapfile: remove zeromap bitmap\n* [PATCH v3 18/20] memcg: swap: only charge physical swap slots\n* [PATCH v3 19/20] swap: simplify swapoff using virtual swap\n* [PATCH v3 20/20] swapfile: replace the swap map with bitmaps\n\nand found the following issue:\npossible deadlock in vswap_iter\n\nFull report is available here:\nhttps://ci.syzbot.org/series/b9defda6-daec-4c41-bbf9-7d3b7fabd7cb\n\n***\n\npossible deadlock in vswap_iter\n\ntree:      bpf\nURL:       https://kernel.googlesource.com/pub/scm/linux/kernel/git/bpf/bpf.git\nbase:      05f7e89ab9731565d8a62e3b5d1ec206485eeb0b\narch:      amd64\ncompiler:  Debian clang version 21.1.8 (++20251221033036+2078da43e25a-1~exp1~20251221153213.50), Debian LLD 21.1.8\nconfig:    https://ci.syzbot.org/builds/f444cfbe-4ce0-4917-94aa-3a8bd96ee376/config\nC repro:   https://ci.syzbot.org/findings/7b8c50b1-47d6-42e0-bcfc-814e7b3bb596/c_repro\nsyz repro: https://ci.syzbot.org/findings/7b8c50b1-47d6-42e0-bcfc-814e7b3bb596/syz_repro\n\nloop0: detected capacity change from 0 to 764\n============================================\nWARNING: possible recursive locking detected\nsyzkaller #0 Not tainted\n--------------------------------------------\nsyz-executor625/5806 is trying to acquire lock:\nffff88811884c018 (&cluster->lock){+.+.}-{3:3}, at: spin_lock include/linux/spinlock.h:351 [inline]\nffff88811884c018 (&cluster->lock){+.+.}-{3:3}, at: vswap_iter+0xfa/0x1b0 mm/vswap.c:274\n\nbut task is already holding lock:\nffff88811884c018 (&cluster->lock){+.+.}-{3:3}, at: spin_lock_irq include/linux/spinlock.h:376 [inline]\nffff88811884c018 (&cluster->lock){+.+.}-{3:3}, at: swap_cache_lock_irq+0xe2/0x190 mm/vswap.c:1586\n\nother info that might help us debug this:\n Possible unsafe locking scenario:\n\n       CPU0\n       ----\n  lock(&cluster->lock);\n  lock(&cluster->lock);\n\n *** DEADLOCK ***\n\n May be due to missing lock nesting notation\n\n3 locks held by syz-executor625/5806:\n #0: ffff888174bc2800 (&mm->mmap_lock){++++}-{4:4}, at: mmap_read_lock include/linux/mmap_lock.h:391 [inline]\n #0: ffff888174bc2800 (&mm->mmap_lock){++++}-{4:4}, at: madvise_lock+0x152/0x2e0 mm/madvise.c:1789\n #1: ffff88811884c018 (&cluster->lock){+.+.}-{3:3}, at: spin_lock_irq include/linux/spinlock.h:376 [inline]\n #1: ffff88811884c018 (&cluster->lock){+.+.}-{3:3}, at: swap_cache_lock_irq+0xe2/0x190 mm/vswap.c:1586\n #2: ffffffff8e55a360 (rcu_read_lock){....}-{1:3}, at: rcu_lock_acquire include/linux/rcupdate.h:331 [inline]\n #2: ffffffff8e55a360 (rcu_read_lock){....}-{1:3}, at: rcu_read_lock include/linux/rcupdate.h:867 [inline]\n #2: ffffffff8e55a360 (rcu_read_lock){....}-{1:3}, at: vswap_cgroup_record+0x40/0x290 mm/vswap.c:1925\n\nstack backtrace:\n\n\n***\n\nIf these findings have caused you to resend the series or submit a\nseparate fix, please add the following tag to your commit message:\n  Tested-by: syzbot@syzkaller.appspotmail.com\n\n---\nThis report is generated by a bot. It may contain errors.\nsyzbot ci engineers can be reached at syzkaller@googlegroups.com.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song expressed concerns that the patch fundamentally changes the swap workflow and introduces many behavior changes at once, which may lead to performance or memory usage regressions for some workloads.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested_changes",
                "performance_regression"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I really do think we better make this optional, not a replacement or\nmandatory. There are many hard to evaluate effects as this\nfundamentally changes the swap workflow with a lot of behavior changes\nat once. e.g. it seems the folio will be reactivated instead of\nsplitted if the physical swap device is fragmented; slot is allocated\nat IO and not at unmap, and maybe many others. Just like zswap is\noptional. Some common workloads would see an obvious performance or\nmemory usage regression following this design, see below.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song expressed concern that the patch's simplification of the swap table format may ultimately lead to reimplementing it, suggesting a potential long-term maintenance burden.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential long-term maintenance issue"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Honestly if you keep reducing that you might just end up\nreimplementing the swap table format :)",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that having a 1:1 virtual swap setup is sufficient in most cases, and the static overhead will be trivial due to lack of fragmentation issues when physical memory size matches swap space.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "reconsidered opinion",
                "acknowledged potential"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "So I thought about it again, this one seems not to be an issue. In\nmost cases, having a 1:1 virtual swap setup is enough, and very soon\nthe static overhead will be really trivial. There won't even be any\nfragmentation issue either, since if the physical memory size is\nidentical to swap space, then you can always find a matching part. And\nbesides, dynamic growth of swap files is actually very doable and\nuseful, that will make physical swap files adjustable at runtime, so\nusers won't need to waste a swap type id to extend physical swap\nspace.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer noted that the new swapoff implementation does not provide the same performance as the old one and cannot guarantee metadata release, suggesting it is not a clean swapoff.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance",
                "metadata release"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The swapoff here is not really a clean swapoff, minor faults will\nstill be triggered afterwards, and metadata is not released. So this\nnew swapoff cannot really guarantee the same performance as the old\nswapoff. And on the other hand we can already just read everything\ninto the swap cache then ignore the page table walk with the older\ndesign too, that's just not a clean swapoff.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that introducing a standalone bit for swap cache, similar to the problematic SWAP_HAS_CACHE flag, may cause new issues and requested an alternative solution.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "A standalone bit for swapcache looks like the old SWAP_HAS_CACHE that\ncauses many issues...",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that having a struct larger than 8 bytes in the swap table would limit lock design, suggesting to leverage atomic operations like CAS on swap entries once they are small and unified, similar to how atomic_long_t is used. He also pointed out that consolidating the lock scope to folio for many swap operations might be feasible soon.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Having a struct larger than 8 bytes means you can't load it\natomically, that limits your lock design. About a year ago Chris\nshared with me an idea to use CAS on swap entries once they are small\nand unified, that's why swap table is using atomic_long_t and have\nhelpers like __swap_table_xchg, we are not making good use of them yet\nthough. Meanwhile we have already consolidated the lock scope to folio\nin many places, holding the folio lock then doing the CAS without\ntouching cluster lock at all for many swap operations might be\nfeasible soon.\n\nE.g. we already have a cluster-lockless version of swap check in swap table p3:\nhttps://lore.kernel.org/linux-mm/20260128-swap-table-p3-v2-11-fe0b67ef0215@tencent.com/\n\nThat might also greatly simplify the locking on IO and migration\nperformance between swap devices.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song expressed concern that the proposed swap table design results in significantly higher memory usage compared to existing solutions, particularly for large swap devices (e.g., 128G), and suggested making it optional and minimal.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hmm.. With the swap table we will have a stable 8 bytes per slot in\nall cases, in current mm-stable we use 11 bytes (8 bytes dyn and 3\nbytes static), and in the posted p3 we already get 10 bytes (8 bytes\ndyn and 2 bytes static). P4 or follow up was already demonstrated\nlast year with working code, and it makes everything dynamic\n(8 bytes fully dyn, I'll rebase and send that once p3 is merged).\n\nSo with mm-stable and follow up, for 32G swap device:\n\n0% usage, or 0/8,388,608 entries: 0.00 MB\n* mm-stable total overhead: 25.50 MB (which is swap table p2)\n* swap-table p3 overhead: 17.50 MB\n* swap-table p4 overhead: 0.50 MB\n* Vswap total overhead: 2.00 MB\n\n100% usage, or 8,388,608/8,388,608 entries:\n* mm-stable total overhead: 89.5 MB (which is swap table p2)\n* swap-table p3 overhead: 81.5 MB\n* swap-table p4 overhead: 64.5 MB\n* Vswap total overhead: 259.00 MB\n\nThat 3 - 4 times more memory usage, quite a trade off. With a\n128G device, which is not something rare, it would be 1G of memory.\nSwap table p3 / p4 is about 320M / 256M, and we do have a way to cut\nthat down close to be <1 byte or 3 byte per page with swap table\ncompaction, which was discussed in LSFMM last year, or even 1 bit\nwhich was once suggested by Baolin, that would make it much smaller\ndown to <24MB (This is just an idea for now, but the compaction is\nvery doable as we already have \"LRU\"s for swap clusters in swap\nallocator).\n\nI don't think it looks good as a mandatory overhead. We do have a huge\nuser base of swap over many different kinds of devices, it was not\nlong ago two new kernel bugzilla issue  or bug reported was sent to\nthe maillist about swap over disk, and I'm still trying to investigate\none of them which seems to be actually a page LRU issue and not swap\nproblem..  OK a little off topic, anyway, I'm not saying that we don't\nwant more features, as I mentioned above, it would be better if this\ncan be optional and minimal. See more test info below.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "reviewer noted that the patch decouples swap cache from physical swap infrastructure and reduces the lock scope, but questioned whether this is comparable to zswap's reduced callpath",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "comparative analysis",
                "questioning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Congrats! Yeah, I guess that's because vswap has a smaller lock scope\nthan zswap with a reduced callpath?",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that the vswap patch series has a significant regression in freeing time under global pressure, with up to 200% slower performance compared to mainline and swap table patches. The reviewer suspects that the double free or decoupling of swap/underlying slots might be causing this issue and recommends implementing a flexible way to handle freeing without a mandatory layer.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "regression",
                "slow performance"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Thanks for the bench, but please also test with global pressure too.\nOne mistake I made when working on the prototype of swap tables was\nonly focusing on cgroup memory pressure, which is really not how\neveryone uses Linux, and that's why I reworked it for a long time to\ntweak the RCU allocation / freeing of swap table pages so there won't\nbe any regression even for lowend and global pressure. That's kind of\ncritical for devices like Android.\n\nI did an overnight bench on this with global pressure, comparing to\nmainline 6.19 and swap table p3 (I do include such test for each swap\ntable serie, p2 / p3 is close so I just rebase and latest p3 on top of\nyour base commit just to be fair and that's easier for me too) and it\ndoesn't look that good.\n\nTest machine setup for vm-scalability:\n# lscpu | grep \"Model name\"\nModel name:          AMD EPYC 7K62 48-Core Processor\n\n# free -m\n              total        used        free      shared  buff/cache   available\nMem:          31582         909       26388           8        4284       29989\nSwap:         40959          41       40918\n\nThe swap setup follows the recommendation from Huang\n(https://lore.kernel.org/linux-mm/87ed474kvx.fsf@yhuang6-desk2.ccr.corp.intel.com/).\n\nTest (average of 18 test run):\nvm-scalability/usemem --init-time -O -y -x -n 1 56G\n\n6.19:\nThroughput: 618.49 MB/s (stdev 31.3)\nFree latency: 5754780.50us (stdev 69542.7)\n\nswap-table-p3 (3.8%, 0.5% better):\nThroughput: 642.02 MB/s (stdev 25.1)\nFree latency: 5728544.16us (stdev 48592.51)\n\nvswap (3.2%, 244% worse):\nThroughput: 598.67 MB/s (stdev 25.1)\nFree latency: 13987175.66us (stdev 125148.57)\n\nThat's a huge regression with freeing. I have a vm-scatiliby test\nmatrix, not every setup has such significant >200% regression, but on\naverage the freeing time is about at least 15 - 50% slower (for\nexample /data/vm-scalability/usemem --init-time -O -y -x -n 32 1536M\nthe regression is about 2583221.62us vs 2153735.59us). Throughput is\nall lower too.\n\nFreeing is important as it was causing many problems before, it's the\nreason why we had a swap slot freeing cache years ago (and later we\nremoved that since the freeing cache causes more problems and swap\nallocator already improved it better than having the cache). People\neven tried to optimize that:\nhttps://lore.kernel.org/linux-mm/20250909065349.574894-1-liulei.rjpt@vivo.com/\n(This seems a already fixed downstream issue, solved by swap allocator\nor swap table). Some workloads might amplify the free latency greatly\nand cause serious lags as shown above.\n\nAnother thing I personally cares about is how swap works on my daily\nlaptop :), building the kernel in a 2G test VM using NVME as swap,\nwhich is a very practical workload I do everyday, the result is also\nnot good (average of 8 test run, make -j12):\n#free -m\n               total        used        free      shared  buff/cache   available\nMem:            1465         216        1026           0         300        1248\nSwap:           4095          36        4059\n\n6.19 systime:\n109.6s\nswap-table p3:\n108.9s\nvswap systime:\n118.7s\n\nOn a build server, it's also slower (make -j48 with 4G memory VM and\nNVME swap, average of 10 testrun):\n# free -m\n               total        used        free      shared  buff/cache   available\nMem:            3877        1444        2019         737        1376        2432\nSwap:          32767        1886       30881\n\n# lscpu | grep \"Model name\"\nModel name:                              Intel(R) Xeon(R) Platinum\n8255C CPU @ 2.50GHz\n\n6.19 systime:\n435.601s\nswap-table p3:\n432.793s\nvswap systime:\n455.652s\n\nIn conclusion it's about 4.3 - 8.3% slower for common workloads under\nglobal pressure, and there is a up to 200% regression on freeing. ZRAM\nshows an even larger workload regression but I'll skip that part since\nyour series is focusing on zswap now. Redis is also ~20% slower\ncompared to mm-stable (327515.00 RPS vs 405827.81 RPS), that's mostly\ndue to swap-table-p2 in mm-stable so I didn't do further comparisons.\n\nSo if that's not a bug with this series, I think the double free or\ndecoupling of swap / underlying slots might be the problem with the\nfreeing regression shown above. That's really a serious issue, and the\nglobal pressure might be a critical issue too as the metadata is much\nlarger, and is already causing regressions for very common workloads.\nLow end users could hit the min watermark easily and could have\nserious jitters or allocation failures.\n\nThat's part of the issue I've found, so I really do think we need a\nflexible way to implementa that and not have a mandatory layer. After\nswap table P4 we should be able to figure out a way to fit all needs,\nwith a clean defined set of swap API, metadata and layers, as was\ndiscussed at LSFMM last year.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that there was an issue with the patch series not being delivered to the mailing list, and promised to be more patient in the future.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgment of a non-technical issue",
                "promise to improve process"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I have no idea what happened to be honest. It did not show up on lore\nfor a couple of hours, and my coworkers did not receive the cover\nletter email initially. I did not receive any error message or logs\neither - git send-email returns Success to me, and when I checked on\nthe web gmail client (since I used a gmail email account), the whole\nseries is there.\n\nI tried re-sending a couple times, to no avail. Then, in a couple of\nhours, all of these attempts showed up.\n\nAnyway, this is my bad - I'll be more patient next time. If it does\nnot show up for a couple of hours then I'll do some more digging.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author acknowledged a mistake in an old cover letter and promised to correct it in future versions.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged mistake",
                "promised correction"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oh yeah I forgot to update that. That was from an old cover letter of\nan old version that never got sent out - I'll correct that in future\nversions\n\n(if you scroll down to the bottom of the cover letter you should see\nthe correct base, which should be 6.19).",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged a potential compilation issue and asked reviewer to share their configurations so they can reproduce and fix it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a potential issue",
                "asked for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ah that's strange. It compiled on all of my setups (I tested with a couple\ndifferent ones), but I must have missed some cases. Would you mind\nsharing your configs so that I can reproduce this compilation error?\n\n(although I'm sure kernel test robot will scream at me soon, which\nusually includes configs that cause the compilation issue).",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author is addressing a concern about the inclusion of user time in performance metrics, acknowledging that others only report system time and agreeing to omit user time in future versions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Will do next time! I used to include user time as well, but I noticed\nthat folks (for e.g see [1]) only include systime, not even real time,\nso I figure nobody cares about user time :)\n\n(I still include real time because some of my past work improves sys\ntime but regresses real time, so I figure that's relevant).\n\n[1]: https://lore.kernel.org/linux-mm/20260128-swap-table-p3-v2-0-fe0b67ef0215@tencent.com/\n\nBut yeah no big deal. I'll dig through my logs to see if I still have\nthe numbers, but if not I'll include it in next version.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged a concern about lock contention and agreed to address it by restructuring the swap cache code in future versions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a performance issue",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Very fair point :) I will say though - the kernel build test, with\nmemory.max limit sets, does generate a sizable amount of swapping, and\ndoes OOM if you don't set up swap. Take my words for now, but I will\ntry to include average per-run (z)swap activity stats (zswpout zswpin\netc.) in future versions if you're interested :)\n\nI've been trying to running more stress tests to trigger crashes and\nperformance regression. One of the big reasons why I haven't sent\nanything til now is to fix obvious performance issues (the\naforementioned lock contention) and bugs. It's a complicated piece of\nwork.\n\nAs always, would love to receive code/design feedback from you (and\nKairui, and other swap reviewers), and I would appreciate very much if\nother swap folks can play with the patch series on their setup as well\nfor performance testing, or let me know if there is any particular\ncase that they're interested in :)\n\nThanks for your review, Chris!",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "reviewer noted that the patch does not address the issue of swap cache being accessed concurrently by multiple threads, and requested the use of a spinlock to protect access to the swap cache",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concurrency",
                "spinlock"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hello Kairui,\n\nOn Wed, Feb 11, 2026 at 01:59:34AM +0800, Kairui Song wrote:",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "reviewer noted that the patch introduces similar data structures for swap cache and shadowed pages, which may be redundant or unnecessary",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "redundancy",
                "unnecessary"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah, it turns out we need the same data points to describe and track\na swapped out page ;)",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "reviewer noted that the patch does not properly address address space separation, specifically that entries in the compressed pool should not consume disk space and vice versa",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The issue is address space separation. We don't want things inside the\ncompressed pool to consume disk space; nor do we want entries that\nlive on disk to take usable space away from the compressed pool.\n\nThe regression reports are fair, thanks for highlighting those. And\nwhether to make this optional is also a fair discussion.\n\nBut some of the numbers comparisons really strike me as apples to\noranges comparisons. It seems to miss the core issue this series is\ntrying to address.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Reviewer questioned the design of the patch series, suggesting that it prioritizes performance over simplicity and ease of use, specifically criticizing the try_to_unuse() scans as expensive and unnecessary.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That seems very academic to me. The goal is to relinquish disk space,\nand these patches make that a lot faster.\n\nLet's put it the other way round: if today we had a fast swapoff read\nsequence with lazy minor faults to resolve page tables, would we\naccept patches that implement the expensive try_to_unuse() scans and\nmake it mandatory? Considering the worst-case runtime it can cause?\n\nI don't think so. We have this scan because the page table references\nare pointing to disk slots, and this is the only way to free them.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "reviewer noted that vswap_free() cannot release the disk slot while the swp_entry_t is still in circulation, and requested a mechanism to handle this scenario",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "How can you relinquish the disk slot as long as the swp_entry_t is in\ncirculation?",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledges that maintaining two swap implementations would make the patch series unreadable, unreviewable, and unmaintainable, but does not commit to a single implementation yet.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges need for improvement",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ideally, if we can close the performance gap and have only one\nversion, then that would be the best :)\n\nProblem with making it optional, or maintaining effectively two swap\nimplementations, is that it will make the patch series unreadable and\nunreviewable, and the code base unmaintanable :) You'll have x2 the\namount of code to reason about and test, much more merge conflicts at\nrebase and cherry-pick time. And any improvement to one version takes\nextra work to graft onto the other version.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged Kairui Song's swap table format and cluster-based allocator are good, but clarified that this patch series aims to separate physical and virtual swap address spaces for new use cases.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "There's nothing wrong with that ;)\n\nI like the swap table format (and your cluster-based swap allocator) a\nlot. This patch series does not aim to remove that design - I just\nwant to separate the address space of physical and virtual swaps to\nenable new use cases...",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author acknowledges that dynamically sizing swap space is necessary due to varying server machines, services, and access characteristics, confirming that a static approach would not work.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges production issue",
                "confirms dynamic sizing needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I mean, it is a real production issue :) We have a variety of server\nmachines and services. Each of the former has its own memory and drive\nsize. Each of the latter has its own access characteristics,\ncompressibility, latency tolerance (and hence would prefer a different\nswapping solutions - zswap, disk swap, zswap x disk swap). Coupled\nwith the fact that now multiple services can cooccur on one host, and\none services can be deployed on different kinds of hosts, statically\nsizing the swapfile becomes operationally impossible and leaves a lot\nof wins on the table. So swap space has to be dynamic.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that dynamic growth of swap files is a missing feature and expressed concerns about the current design, specifically mentioning the reliance on a tree structure similar to vswap's radix tree (xarray). They hinted at potential issues with combining hacks to solve individual problems.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a problem",
                "expressed concerns"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"dynamic growth of swap files\", do you mean dynamically adjusting\nthe size of the swapfile? then that capacity does not exist right now,\nand I don't see a good design laid out for it... At the very least,\nthe swap allocator needs to be dynamic in nature. I assume it's going\nto look something very similar to vswap's current attempt, which\nrelies on a tree structure (radix tree i.e xarray). Sounds familiar?\n;)\n\nI feel like each of the problem I mention in this cover letter can be\nsolved partially with some amount of hacks, but none of them will\nsolve it all. And once you slaps all the hacks together, you just get\nvirtual swap, potentially shoved within specific backend codebase\n(zswap or zram). That's not... ideal.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that the swap cache cannot be freed because page table entries still refer to slots on the physical swap device, and agreed that locking the swap device in place is unavoidable.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a limitation",
                "agreed with an implication"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't understand your point regarding the \"reading everything into\nswap cache\". Yes, you can do that, but you would still lock the swap\ndevice in place, because the page table entries still refer to slots\non the physical swap device - you cannot free the swap device, nor\nspace on disk, not even the swapfile's metadata (especially since the\nswap cache is now intertwined with the physical swap layer).",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that their patch was based on an older kernel version and suggested incorporating recent swap table work into their own line of work, specifically considering the swap cache synchronization scheme and memory overhead reduction tricks.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging prior work",
                "suggesting incorporation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah this was based on 6.19, which did not have your swap cache change yet :)\n\nI have taken a look at your latest swap table work in mm-stable, and I\nthink most of that can conceptually incorporated in to this line of\nwork as well.\n\nChiefly, the new swap cache synchronization scheme (i.e whoever puts\nthe folio in swap cache first gets exclusive rights) still works in\nvirtual swap world (and hence, the removal of swap cache pin, which is\none bit in the virtual swap descriptor).\n\nSimilarly, do you think we cannot hold the folio lock in place of the\ncluster lock in the virtual swap world? Same for a lot of the memory\noverhead reduction tricks (such as using shadow for cgroup id instead\nof a separate swap_cgroup unsigned short field). I think comparing the\ntwo this way is a bit apples-to-oranges (especially given the new\nfeatures enabled by vswap).\n\n[...]",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledges a need for further debugging and investigation of an issue, but does not commit to fixing it in the current patch series.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a need for further debugging",
                "does not commit to fixing"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Side note - I might have missed this. If it's still ongoing, would\nlove to help debug this :)",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that the zswap code path can merge swap operations and release-acquire locks, but considered it secondary to their main goal of enabling new features.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a related benefit",
                "prioritized feature enablement over performance"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ah yeah that too. I neglected to mention this, but with vswap you can\nmerge several swap operations in zswap code path and no longer have to\nrelease-then-reacquire the swap locks, since zswap entries live in the\nsame lock scope as swap cache entries.\n\nIt's more of a side note either way, because my main goal with this\npatch series is to enable new features. Getting a performance win is\nalways nice of course :)",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that reviewer Kairui Song questioned the performance impact of the patch, explained that swap performance is dominated by IO work in their experiments, and asked if they missed something regarding concurrency.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "asked for clarification",
                "acknowledged question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hmm this one I don't think I can reproduce without your laptop ;)\n\nJokes aside, I did try to run the kernel build with disk swapping, and\nthe performance is on par with baseline. Swap performance with NVME\nswap tends to be dominated by IO work in my experiments. Do you think\nI missed something here? Maybe it's the concurrency difference (since\nI always run with -j$(nproc), i.e the number of workers == the number\nof processors).",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged a need for further testing and reproduction of reported issues, specifically starting with the 'usemem' case.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a need for further testing",
                "will start with usemem first"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'll see if I can reproduce the issues! I'll start with usemem one\nfirst, as that seems easier to reproduce...",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledges the possibility of a non-crashing bug in their patch and agrees to review the reviewer's test case before proceeding.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges potential bug",
                "agrees to study test case"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It could be a non-crashing bug that subtly regresses certain swap\noperations, but yeah let me study your test case first!",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that the reviewer's numbers only included system time and asked them to ignore those comments, but still emphasized the importance of considering IO wait time in a production system.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "emphasized"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ah I just noticed that your numbers include only systime. Ignore my IO\ncomments then.\n\n(I still think in real production system, with disk swapping enabled,\nthen IO wait time is going to be really important. If you're going to\nuse disk swap, then this affects real time just as much if not more\nthan kernel CPU overhead).",
              "reply_to": "",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the swap cache is currently implemented using XArray, but questioned whether this data structure can be improved for lookup efficiency, suggesting a re-evaluation of its design.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Johannes,\n\nOn Mon, Feb 9, 2026 at 6:36PM Johannes Weiner <hannes@cmpxchg.org> wrote:",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li pointed out that the patch adds new members to existing structures and increases their size without explaining why these changes are necessary, specifically mentioning the increase in size of the swap_map from 1 byte to 4 bytes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested explanation for structural changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Adding members previously not there and making some members bigger\nalong the way. For example, the swap_map from 1 byte to a 4 byte\ncount.",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "reviewer pointed out that the patch caused a significant increase in per-swap entry overhead, specifically mentioning it increased to 24 bytes, and requested clarification on whether this is acceptable for virtual swap (VS)",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "disagreement",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It seems you did not finish your sentence before sending your reply.\n\nAnyway, I saw the total per swap entry overhead bump to 24 bytes\ndynamic. Let me know what is the correct number for VS if you\ndisagree.\n\nChris",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li expressed appreciation for the performance test and only commented on its results, not providing any technical feedback or suggestions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no specific technical concerns raised"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Kairui,\n\nThank you so much for the performance test.\n\nI will only comment on the performance number in this sub email thread.\n\nOn Tue, Feb 10, 2026 at 10:00AM Kairui Song <ryncsn@gmail.com> wrote:",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the Virtual Swap (VS) series has a significant per-swap-entry metadata overhead, which was previously addressed by reverting the swap table; however, VS resulted in worse memory and CPU performance compared to the swap table.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance concerns",
                "metadata overhead"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Agree. That has been my main complaint about VS is the per swap entry\nmetadata overhead. This VS series reverted the swap table, but memory\nand CPU performance is worse than swap table.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li expressed strong disagreement with the patch, citing performance concerns and stating it's a 'deal breaker' due to lack of similarity in performance with baseline or swap table P3.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance",
                "disagreement"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Now that is a deal breaker for me. Not the similar performance with\nbaseline or swap table P3.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the patch introduces a 4-8% performance regression due to favoring swap table implementations, and considers this statically significant.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance_regression",
                "statically_significant"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "At 4-8% I would consider it a statically significant performance\nregression to favor swap table implementations.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li agrees with the approach to decouple swap cache from physical swap infrastructure, suggesting that getting the fundamental infrastructure right first is crucial before implementing more complex features like online growing of swapfile size.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreement",
                "shared view"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Agree. That matches my view, get the fundamental infrastructure for\nswap right first (swap table), then do those fancier feature\nenhancement like online growing the size of swapfile.\n\nChris",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Reviewer noted that the patch introduces a global lock to synchronize swap cache accesses, which is temporary and will be replaced by swap clusters in the virtual swap layer; however, he requested more information on how this temporary solution works and what are its implications.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested clarification",
                "implications of temporary solution"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Chris,\n\nOn Tue, Feb 10, 2026 at 01:24:03PM -0800, Chris Li wrote:",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Reviewer noted that the patch description does not include a clear explanation of how swap cache will be managed in the virtual swap layer and requested that Nhat's cover letter be trimmed to address specific questions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested clarification",
                "asked for trimming"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I did. I trimmed the quote of Nhat's cover letter to the parts\naddressing your questions. If you use gmail, click the three dots:",
              "reply_to": "Chris Li",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li provided additional debugging information without raising any specific technical concerns or objections about the patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No problem. Just want to provide more data points if that helps you\ndebug your email issue.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested simplifying the explanation of virtual swap space memory overhead by referencing a specific value ('24B dynamic') and omitting detailed tables.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested improvement"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think the \"24B dynamic\" sums up the VS memory overhead pretty well\nwithout going into the detail tables. You can drive from case\ndiscussion from that.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li reported an issue where a config.gz file attachment was causing errors during stress testing of the patch series, and suggested checking if a newer GCC version might be contributing to the problem.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "issue",
                "suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "See attached config.gz. It is also possible the newer gcc version\ncontributes to that error. Anyway, that is preventing me from stress\ntesting your series on my setup.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that measuring stddev for 33 seconds is insufficient to achieve a 1.5% resolution, as it falls within the noise range and suggested taking more samples.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The stddev is 33 seconds. Measure 5 times then average result is not\nenough sample to get your to 1.5% resolution (8 seconds), which fall\ninto the range of noise.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested including user space time to better measure swap pressure, but noted that the absolute zswapout count is not currently needed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested additional information",
                "no clear objection or approval"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Including the user space time will help determine the level of swap\npressure as well. I don't need the absolutely zswapout count just yet.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that he cannot proceed with further testing until a compiling error is fixed, and requested the patch author to address this issue first.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I understand Kairui has some measurements that show regressions.\n\nIf you can fix the compiling error I can do some stress testing myself\nto provide more data points.\n\nThanks\n\nChris",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the patch's requirement to use a single global lock for swap cache accesses should be made runtime optional for other types of swap, such as zram, which does not benefit from this feature.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "runtime optionality",
                "zram"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I second that this should be run time optional for other types of\nswap. It should not be mandatory for other swap that does not benefit\nfrom it. e.g. zram.\n\nChris",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, and explained that this is a temporary measure until the virtual swap layer is introduced.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Thu, Feb 12, 2026 at 4:23AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged a concern about CC'ing too many people and promised to use a script or trim the list in future patches.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged concern",
                "promised fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I hope so... did I miss someone? If so, my apologies - I manually add\nthem one at a time to be completely honest. The list is huge...\n\nI'll probably use a script to convert that huge output next time into \"--cc\".\n\n(Or are you suggesting I should not send it out to everyone? I can try\nto trim the list, but tbh it touches areas that I'm not familiar with,\nso I figure I should just cc everyone).",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author acknowledges that they should have done a proper CC list from the start, apologizes for missing someone, and promises to be more careful in the future.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "apology",
                "promise"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ok let's try... this :) Probably should have done it from the start,\nbut better late than never...\n\nNot sure who was missing from the first run - my apologies if I did\nthat.... I'll be more careful with huge cc list next time and just\nscriptify it.",
              "reply_to": "",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David expressed frustration at missing his patch revisions, but did not provide any specific technical feedback on the code changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "frustration",
                "lack of technical feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I stumbled over this patch set while scrolling through the mailing list \nafter a while (now that my inbox is \"mostly\" cleaned up) and wondered \nwhy no revision ended in my inbox :)",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David requested that the patch author include a CC list in the cover letter, which can be used to send emails using '--cc-cover' with git send-email.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I usually add them as\n\nCc:\n\nto the cover letter and then use something like \"--cc-cover \" with git \nsend-email.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, and provided a brief explanation of why this is necessary.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Thu, Feb 12, 2026 at 9:41AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "The author acknowledged that their patch series was too large and may have caused issues with email delivery, but did not indicate a plan to revise or restructure the patches in response to this feedback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged issue",
                "did not commit to revising"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oh TIL. Thanks, David!\n\nYeah this is the biggest patch series I've ever sent out. Most of my\npast patches are contained in one or two files, so usually only the\nmaintainers and contributors are pulled in, and the cc list never\nexceeds 15-20 cc's. So I've been getting away with just manually\npreparing a send command, do a quick eyeball check, then send things\nout.\n\nThat system breaks down hard this case (the email debacle aside, which\nI still haven't figured out - still looking at gmail as the prime\nsuspect...).",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David noted that the patch series should not blindly CC all maintainers and reviewers, but rather consider the specific needs of each subsystem, to avoid flooding them with patches or annoying them with unnecessary notifications.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "considered practicality",
                "acknowledged complexity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It's usually not as easy as copying the output to the cover letter via Cc:.\n\nSometimes you want to CC all maintainers+reviewers of some subsystem, \nsometimes only the maintainers (heads-up, mostly simplistic unrelated \nchanges that don't need any real subsystem-specific review).\n\nFine line between flooding people with patches or annoying people with \npatches :)",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author acknowledged that the PTE zapping path is causing a regression and identified two specific issues: unnecessary xarray lookups to resolve the backend for superfluous checks, and not batching even though there's no good reason not to. They plan to fix these issues in future patches.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a problem",
                "planned to fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Kairui - a quick update.\n\nTook me awhile to get a host that matches your memory spec:\n\nfree -m\n               total        used        free      shared  buff/cache   available\nMem:           31609        5778        7634          20       18664       25831\nSwap:          65535           1       65534\n\nI think I managed to reproduce your observations (average over 5 runs):\n\nBaseline (6.19)\n\nreal: mean: 191.19s, stdev: 4.53s\nuser: mean: 46.98s, stdev: 0.15s\nsys: mean: 127.97s, stdev: 3.95s\naverage throughput: 382057 KB/s\naverage free time: 8179978 usecs\n\nVswap:\n\nreal: mean: 199.85s, stdev: 6.09s\nuser: mean: 46.51s, stdev: 0.25s\nsys: mean: 137.24s, stdev: 6.46s\naverage throughput: 367437 KB/s\naverage free time: 9887107.6 usecs\n\n(command is time ./usemem --init-time -w -O -s 10 -n 1 56g)\n\nI think I figured out where the bulk of the regression lay - it's in\nthe PTE zapping path. In a nutshell, we're not batching in the case\nwhere these PTEs are backed by virtual swap entries with zswap\nbackends (even though there is not a good reason not to batch), and\nunnecessarily performing unnecesary xarray lookups to resolve the\nbackend for some superfluous checks (2 xarray lookups for every PTE,\nwhich is wasted work because as noted earlier, we ended up not\nbatching anyway).\n\nJust by simply fixing this issue, the gap is much closer\n\nreal: mean: 192.24s, stdev: 4.82s\nuser: mean: 46.42s, stdev: 0.27s\nsys: mean: 129.84s, stdev: 4.59s\naverage throughput: 380670 KB/s\naverage free time: 8583381.4 usecs\n\nI also discovered a couple more inefficiencies in vswap free path.\nHopefully once we fix those, the gap will be non-existent.",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham (author)",
              "summary": "Author Nhat Pham is addressing Kairui Song's feedback about the swap free path being inefficient, specifically issues with resolving backends and batching consecutive free operations. The author has run benchmarks to demonstrate the improvement provided by their patch and asks Kairui to apply it on top of the vswap series and report any discrepancies in results.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "author is providing additional information",
                "asking for further testing"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Kairui, could you apply this patch on top of the vswap series and run it\non your test suite? It runs fairly well on my system (I actually rerun\nthe benchmark on a different host to double check as well), but I'd love\nto get some data from your ends as well.\n\nIf there are serious discrepancies, could you also include your build\nconfig etc.? There might be differences in our setups, but since I\nmanaged to reproduce the free time regression on my first try I figured\nI should just fix it first :)\n\n---------------\n\nFix two issues that make the swap free path inefficient:\n\n1. At the PTE zapping step, we are unnecessarily resolving the backends,\n   and fall back to batch size of 1, even though virtual swap\n   infrastructure now already supports freeing of mixed backend ranges\n   (as long the PTEs contain virtually contiguous swap slots).\n2. Optimize vswap_free() by batching consecutive free operations, and\n   avoid releasing locks unnecessarily (most notably, when we release\n   non-disk-swap backends).\n\nPer a report from Kairui Song ([1]), I have run the following benchmark:\n\nfree -m\n               total        used        free      shared  buff/cache   available\nMem:           31596        5094       11667          19       15302       26502\nSwap:          65535          33       65502\n\nRunning the usemem benchmark with n = 1, 56G for 5 times, and average\nout the result:\n\nBaseline (6.19):\n\nreal: mean: 190.93s, stdev: 5.09s\nuser: mean: 46.62s, stdev: 0.27s\nsys: mean: 128.51s, stdev: 5.17s\nthroughput: mean: 382093 KB/s, stdev: 11173.6 KB/s\nfree time: mean: 7916690.2 usecs, stdev: 88923.0 usecs\n\nVSS without this patch:\nreal: mean: 194.59s, stdev: 7.61s\nuser: mean: 46.71s, stdev: 0.46s\nsys: mean: 131.97s, stdev: 7.93s\nthroughput: mean: 379236.4 KB/s, stdev: 15912.26 KB/s\nfree time: mean: 10115572.2 usecs, stdev: 108318.35 usecs\n\nVSS with this patch:\nreal: mean: 187.66s, stdev: 5.67s\nuser: mean: 46.5s, stdev: 0.16s\nsys: mean: 125.3s, stdev: 5.58s\nthroughput: mean: 387506.4 KB/s, stdev: 12556.56 KB/s\nfree time: mean: 7029733.8 usecs, stdev: 124661.34 usecs\n\n[1]: https://lore.kernel.org/linux-mm/CAMgjq7AQNGK-a=AOgvn4-V+zGO21QMbMTVbrYSW_R2oDSLoC+A@mail.gmail.com/\n\nSigned-off-by: Nhat Pham <nphamcs@gmail.com>\n---\n include/linux/memcontrol.h |   6 +\n mm/internal.h              |  18 ++-\n mm/madvise.c               |   2 +-\n mm/memcontrol.c            |   2 +-\n mm/memory.c                |   8 +-\n mm/vswap.c                 | 294 ++++++++++++++++++-------------------\n 6 files changed, 165 insertions(+), 165 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 0651865a4564f..0f7f5489e1675 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -827,6 +827,7 @@ static inline unsigned short mem_cgroup_id(struct mem_cgroup *memcg)\n \treturn memcg->id.id;\n }\n struct mem_cgroup *mem_cgroup_from_id(unsigned short id);\n+void mem_cgroup_id_put_many(struct mem_cgroup *memcg, unsigned int n);\n \n #ifdef CONFIG_SHRINKER_DEBUG\n static inline unsigned long mem_cgroup_ino(struct mem_cgroup *memcg)\n@@ -1289,6 +1290,11 @@ static inline struct mem_cgroup *mem_cgroup_from_id(unsigned short id)\n \treturn NULL;\n }\n \n+static inline void mem_cgroup_id_put_many(struct mem_cgroup *memcg,\n+\t\t\t\t\t  unsigned int n)\n+{\n+}\n+\n #ifdef CONFIG_SHRINKER_DEBUG\n static inline unsigned long mem_cgroup_ino(struct mem_cgroup *memcg)\n {\ndiff --git a/mm/internal.h b/mm/internal.h\nindex cfe97501e4885..df991f601702c 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -327,8 +327,6 @@ static inline swp_entry_t swap_nth(swp_entry_t entry, long n)\n \treturn (swp_entry_t) { entry.val + n };\n }\n \n-swp_entry_t swap_move(swp_entry_t entry, long delta);\n-\n /**\n  * pte_move_swp_offset - Move the swap entry offset field of a swap pte\n  *\t forward or backward by delta\n@@ -342,7 +340,7 @@ swp_entry_t swap_move(swp_entry_t entry, long delta);\n static inline pte_t pte_move_swp_offset(pte_t pte, long delta)\n {\n \tsoftleaf_t entry = softleaf_from_pte(pte);\n-\tpte_t new = swp_entry_to_pte(swap_move(entry, delta));\n+\tpte_t new = swp_entry_to_pte(swap_nth(entry, delta));\n \n \tif (pte_swp_soft_dirty(pte))\n \t\tnew = pte_swp_mksoft_dirty(new);\n@@ -372,6 +370,7 @@ static inline pte_t pte_next_swp_offset(pte_t pte)\n  * @start_ptep: Page table pointer for the first entry.\n  * @max_nr: The maximum number of table entries to consider.\n  * @pte: Page table entry for the first entry.\n+ * @free_batch: Whether the batch will be passed to free_swap_and_cache_nr().\n  *\n  * Detect a batch of contiguous swap entries: consecutive (non-present) PTEs\n  * containing swap entries all with consecutive offsets and targeting the same\n@@ -382,13 +381,15 @@ static inline pte_t pte_next_swp_offset(pte_t pte)\n  *\n  * Return: the number of table entries in the batch.\n  */\n-static inline int swap_pte_batch(pte_t *start_ptep, int max_nr, pte_t pte)\n+static inline int swap_pte_batch(pte_t *start_ptep, int max_nr, pte_t pte,\n+\t\t\t\t bool free_batch)\n {\n \tpte_t expected_pte = pte_next_swp_offset(pte);\n \tconst pte_t *end_ptep = start_ptep + max_nr;\n \tconst softleaf_t entry = softleaf_from_pte(pte);\n \tpte_t *ptep = start_ptep + 1;\n \tunsigned short cgroup_id;\n+\tint nr;\n \n \tVM_WARN_ON(max_nr < 1);\n \tVM_WARN_ON(!softleaf_is_swap(entry));\n@@ -408,7 +409,14 @@ static inline int swap_pte_batch(pte_t *start_ptep, int max_nr, pte_t pte)\n \t\tptep++;\n \t}\n \n-\treturn ptep - start_ptep;\n+\tnr = ptep - start_ptep;\n+\t/*\n+\t * free_swap_and_cache_nr can handle mixed backends, as long as virtual\n+\t * swap entries backing these PTEs are contiguous.\n+\t */\n+\tif (!free_batch && !vswap_can_swapin_thp(entry, nr))\n+\t\treturn 1;\n+\treturn nr;\n }\n #endif /* CONFIG_MMU */\n \ndiff --git a/mm/madvise.c b/mm/madvise.c\nindex b617b1be0f535..441da03c5d2b9 100644\n--- a/mm/madvise.c\n+++ b/mm/madvise.c\n@@ -692,7 +692,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,\n \n \t\t\tif (softleaf_is_swap(entry)) {\n \t\t\t\tmax_nr = (end - addr) / PAGE_SIZE;\n-\t\t\t\tnr = swap_pte_batch(pte, max_nr, ptent);\n+\t\t\t\tnr = swap_pte_batch(pte, max_nr, ptent, true);\n \t\t\t\tnr_swap -= nr;\n \t\t\t\tfree_swap_and_cache_nr(entry, nr);\n \t\t\t\tclear_not_present_full_ptes(mm, addr, pte, nr, tlb->fullmm);\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 50be8066bebec..bfa25eaffa12a 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -3597,7 +3597,7 @@ void __maybe_unused mem_cgroup_id_get_many(struct mem_cgroup *memcg,\n \trefcount_add(n, &memcg->id.ref);\n }\n \n-static void mem_cgroup_id_put_many(struct mem_cgroup *memcg, unsigned int n)\n+void mem_cgroup_id_put_many(struct mem_cgroup *memcg, unsigned int n)\n {\n \tif (refcount_sub_and_test(n, &memcg->id.ref)) {\n \t\tmem_cgroup_id_remove(memcg);\ndiff --git a/mm/memory.c b/mm/memory.c\nindex a16bf84ebaaf9..59645ad238e22 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -1742,7 +1742,7 @@ static inline int zap_nonpresent_ptes(struct mmu_gather *tlb,\n \t\tif (!should_zap_cows(details))\n \t\t\treturn 1;\n \n-\t\tnr = swap_pte_batch(pte, max_nr, ptent);\n+\t\tnr = swap_pte_batch(pte, max_nr, ptent, true);\n \t\trss[MM_SWAPENTS] -= nr;\n \t\tfree_swap_and_cache_nr(entry, nr);\n \t} else if (softleaf_is_migration(entry)) {\n@@ -4491,7 +4491,7 @@ static bool can_swapin_thp(struct vm_fault *vmf, pte_t *ptep, int nr_pages)\n \tif (!pte_same(pte, pte_move_swp_offset(vmf->orig_pte, -idx)))\n \t\treturn false;\n \tentry = softleaf_from_pte(pte);\n-\tif (swap_pte_batch(ptep, nr_pages, pte) != nr_pages)\n+\tif (swap_pte_batch(ptep, nr_pages, pte, false) != nr_pages)\n \t\treturn false;\n \n \t/*\n@@ -4877,7 +4877,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\tpte_t folio_pte = ptep_get(folio_ptep);\n \n \t\tif (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||\n-\t\t    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)\n+\t\t    swap_pte_batch(folio_ptep, nr, folio_pte, false) != nr)\n \t\t\tgoto out_nomap;\n \n \t\tpage_idx = idx;\n@@ -4906,7 +4906,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \t\tfolio_ptep = vmf->pte - idx;\n \t\tfolio_pte = ptep_get(folio_ptep);\n \t\tif (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||\n-\t\t    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)\n+\t\t    swap_pte_batch(folio_ptep, nr, folio_pte, false) != nr)\n \t\t\tgoto check_folio;\n \n \t\tpage_idx = idx;\ndiff --git a/mm/vswap.c b/mm/vswap.c\nindex 2a071d5ae173c..047c6476ef23c 100644\n--- a/mm/vswap.c\n+++ b/mm/vswap.c\n@@ -481,18 +481,18 @@ static void vswap_cluster_free(struct vswap_cluster *cluster)\n \tkvfree_rcu(cluster, rcu);\n }\n \n-static inline void release_vswap_slot(struct vswap_cluster *cluster,\n-\t\tunsigned long index)\n+static inline void release_vswap_slot_nr(struct vswap_cluster *cluster,\n+\t\tunsigned long index, int nr)\n {\n \tunsigned long slot_index = VSWAP_IDX_WITHIN_CLUSTER_VAL(index);\n \n \tVM_WARN_ON(!spin_is_locked(&cluster->lock));\n-\tcluster->count--;\n+\tcluster->count -= nr;\n \n-\tbitmap_clear(cluster->bitmap, slot_index, 1);\n+\tbitmap_clear(cluster->bitmap, slot_index, nr);\n \n \t/* we only free uncached empty clusters */\n-\tif (refcount_dec_and_test(&cluster->refcnt))\n+\tif (refcount_sub_and_test(nr, &cluster->refcnt))\n \t\tvswap_cluster_free(cluster);\n \telse if (cluster->full && cluster_is_alloc_candidate(cluster)) {\n \t\tcluster->full = false;\n@@ -505,7 +505,7 @@ static inline void release_vswap_slot(struct vswap_cluster *cluster,\n \t\t}\n \t}\n \n-\tatomic_dec(&vswap_used);\n+\tatomic_sub(nr, &vswap_used);\n }\n \n /*\n@@ -527,23 +527,29 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n }\n \n /*\n- * Caller needs to handle races with other operations themselves.\n+ * release_backing - release the backend storage for a given range of virtual\n+ * swap slots.\n+ *\n+ * Entered with the cluster locked, but might drop the lock in between.\n+ * This is because several operations, such as releasing physical swap slots\n+ * (i.e swap_slot_free_nr()) require the cluster to be unlocked to avoid\n+ * deadlocks.\n  *\n- * Specifically, this function is safe to be called in contexts where the swap\n- * entry has been added to the swap cache and the associated folio is locked.\n- * We cannot race with other accessors, and the swap entry is guaranteed to be\n- * valid the whole time (since swap cache implies one refcount).\n+ * This is safe, because:\n+ *\n+ * 1. The swap entry to be freed has refcnt (swap count and swapcache pin)\n+ *    down to 0, so no one can change its internal state\n  *\n- * We cannot assume that the backends will be of the same type,\n- * contiguous, etc. We might have a large folio coalesced from subpages with\n- * mixed backend, which is only rectified when it is reclaimed.\n+ * 2. The swap entry to be freed still holds a refcnt to the cluster, keeping\n+ *    the cluster itself valid.\n+ *\n+ * We will exit the function with the cluster re-locked.\n  */\n- static void release_backing(swp_entry_t entry, int nr)\n+static void release_backing(struct vswap_cluster *cluster, swp_entry_t entry,\n+\t\tint nr)\n {\n-\tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n \tunsigned long flush_nr, phys_swap_start = 0, phys_swap_end = 0;\n-\tunsigned long phys_swap_released = 0;\n \tunsigned int phys_swap_type = 0;\n \tbool need_flushing_phys_swap = false;\n \tswp_slot_t flush_slot;\n@@ -551,9 +557,8 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \n \tVM_WARN_ON(!entry.val);\n \n-\trcu_read_lock();\n \tfor (i = 0; i < nr; i++) {\n-\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n \t\tVM_WARN_ON(!desc);\n \n \t\t/*\n@@ -573,7 +578,6 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \t\tif (desc->type == VSWAP_ZSWAP && desc->zswap_entry) {\n \t\t\tzswap_entry_free(desc->zswap_entry);\n \t\t} else if (desc->type == VSWAP_SWAPFILE) {\n-\t\t\tphys_swap_released++;\n \t\t\tif (!phys_swap_start) {\n \t\t\t\t/* start a new contiguous range of phys swap */\n \t\t\t\tphys_swap_start = swp_slot_offset(desc->slot);\n@@ -589,56 +593,49 @@ void vswap_rmap_set(struct swap_cluster_info *ci, swp_slot_t slot,\n \n \t\tif (need_flushing_phys_swap) {\n \t\t\tspin_unlock(&cluster->lock);\n-\t\t\tcluster = NULL;\n \t\t\tswap_slot_free_nr(flush_slot, flush_nr);\n+\t\t\tmem_cgroup_uncharge_swap(entry, flush_nr);\n+\t\t\tspin_lock(&cluster->lock);\n \t\t\tneed_flushing_phys_swap = false;\n \t\t}\n \t}\n-\tif (cluster)\n-\t\tspin_unlock(&cluster->lock);\n-\trcu_read_unlock();\n \n \t/* Flush any remaining physical swap range */\n \tif (phys_swap_start) {\n \t\tflush_slot = swp_slot(phys_swap_type, phys_swap_start);\n \t\tflush_nr = phys_swap_end - phys_swap_start;\n+\t\tspin_unlock(&cluster->lock);\n \t\tswap_slot_free_nr(flush_slot, flush_nr);\n+\t\tmem_cgroup_uncharge_swap(entry, flush_nr);\n+\t\tspin_lock(&cluster->lock);\n \t}\n+}\n \n-\tif (phys_swap_released)\n-\t\tmem_cgroup_uncharge_swap(entry, phys_swap_released);\n- }\n+static void __vswap_swap_cgroup_clear(struct vswap_cluster *cluster,\n+\t\tswp_entry_t entry, unsigned int nr_ents);\n \n /*\n- * Entered with the cluster locked, but might unlock the cluster.\n- * This is because several operations, such as releasing physical swap slots\n- * (i.e swap_slot_free_nr()) require the cluster to be unlocked to avoid\n- * deadlocks.\n- *\n- * This is safe, because:\n- *\n- * 1. The swap entry to be freed has refcnt (swap count and swapcache pin)\n- *    down to 0, so no one can change its internal state\n- *\n- * 2. The swap entry to be freed still holds a refcnt to the cluster, keeping\n- *    the cluster itself valid.\n- *\n- * We will exit the function with the cluster re-locked.\n+ * Entered with the cluster locked. We will exit the function with the cluster\n+ * still locked.\n  */\n-static void vswap_free(struct vswap_cluster *cluster, struct swp_desc *desc,\n-\tswp_entry_t entry)\n+static void vswap_free_nr(struct vswap_cluster *cluster, swp_entry_t entry,\n+\t\tint nr)\n {\n-\t/* Clear shadow if present */\n-\tif (xa_is_value(desc->shadow))\n-\t\tdesc->shadow = NULL;\n-\tspin_unlock(&cluster->lock);\n+\tstruct swp_desc *desc;\n+\tint i;\n \n-\trelease_backing(entry, 1);\n-\tmem_cgroup_clear_swap(entry, 1);\n+\tfor (i = 0; i < nr; i++) {\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n+\t\t/* Clear shadow if present */\n+\t\tif (xa_is_value(desc->shadow))\n+\t\t\tdesc->shadow = NULL;\n+\t}\n \n-\t/* erase forward mapping and release the virtual slot for reallocation */\n-\tspin_lock(&cluster->lock);\n-\trelease_vswap_slot(cluster, entry.val);\n+\trelease_backing(cluster, entry, nr);\n+\t__vswap_swap_cgroup_clear(cluster, entry, nr);\n+\n+\t/* erase forward mapping and release the virtual slots for reallocation */\n+\trelease_vswap_slot_nr(cluster, entry.val, nr);\n }\n \n /**\n@@ -820,18 +817,32 @@ static bool vswap_free_nr_any_cache_only(swp_entry_t entry, int nr)\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n \tbool ret = false;\n-\tint i;\n+\tswp_entry_t free_start;\n+\tint i, free_nr = 0;\n \n+\tfree_start.val = 0;\n \trcu_read_lock();\n \tfor (i = 0; i < nr; i++) {\n+\t\t/* flush pending free batch at cluster boundary */\n+\t\tif (free_nr && !VSWAP_IDX_WITHIN_CLUSTER_VAL(entry.val)) {\n+\t\t\tvswap_free_nr(cluster, free_start, free_nr);\n+\t\t\tfree_nr = 0;\n+\t\t}\n \t\tdesc = vswap_iter(&cluster, entry.val);\n \t\tVM_WARN_ON(!desc);\n \t\tret |= (desc->swap_count == 1 && desc->in_swapcache);\n \t\tdesc->swap_count--;\n-\t\tif (!desc->swap_count && !desc->in_swapcache)\n-\t\t\tvswap_free(cluster, desc, entry);\n+\t\tif (!desc->swap_count && !desc->in_swapcache) {\n+\t\t\tif (!free_nr++)\n+\t\t\t\tfree_start = entry;\n+\t\t} else if (free_nr) {\n+\t\t\tvswap_free_nr(cluster, free_start, free_nr);\n+\t\t\tfree_nr = 0;\n+\t\t}\n \t\tentry.val++;\n \t}\n+\tif (free_nr)\n+\t\tvswap_free_nr(cluster, free_start, free_nr);\n \tif (cluster)\n \t\tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n@@ -954,19 +965,33 @@ void swapcache_clear(swp_entry_t entry, int nr)\n {\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n-\tint i;\n+\tswp_entry_t free_start;\n+\tint i, free_nr = 0;\n \n \tif (!nr)\n \t\treturn;\n \n+\tfree_start.val = 0;\n \trcu_read_lock();\n \tfor (i = 0; i < nr; i++) {\n+\t\t/* flush pending free batch at cluster boundary */\n+\t\tif (free_nr && !VSWAP_IDX_WITHIN_CLUSTER_VAL(entry.val)) {\n+\t\t\tvswap_free_nr(cluster, free_start, free_nr);\n+\t\t\tfree_nr = 0;\n+\t\t}\n \t\tdesc = vswap_iter(&cluster, entry.val);\n \t\tdesc->in_swapcache = false;\n-\t\tif (!desc->swap_count)\n-\t\t\tvswap_free(cluster, desc, entry);\n+\t\tif (!desc->swap_count) {\n+\t\t\tif (!free_nr++)\n+\t\t\t\tfree_start = entry;\n+\t\t} else if (free_nr) {\n+\t\t\tvswap_free_nr(cluster, free_start, free_nr);\n+\t\t\tfree_nr = 0;\n+\t\t}\n \t\tentry.val++;\n \t}\n+\tif (free_nr)\n+\t\tvswap_free_nr(cluster, free_start, free_nr);\n \tif (cluster)\n \t\tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n@@ -1107,11 +1132,13 @@ void vswap_store_folio(swp_entry_t entry, struct folio *folio)\n \tVM_BUG_ON(!folio_test_locked(folio));\n \tVM_BUG_ON(folio->swap.val != entry.val);\n \n-\trelease_backing(entry, nr);\n-\n \trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tVM_WARN_ON(!desc);\n+\trelease_backing(cluster, entry, nr);\n+\n \tfor (i = 0; i < nr; i++) {\n-\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n \t\tVM_WARN_ON(!desc);\n \t\tdesc->type = VSWAP_FOLIO;\n \t\tdesc->swap_cache = folio;\n@@ -1136,11 +1163,13 @@ void swap_zeromap_folio_set(struct folio *folio)\n \tVM_BUG_ON(!folio_test_locked(folio));\n \tVM_BUG_ON(!entry.val);\n \n-\trelease_backing(entry, nr);\n-\n \trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tVM_WARN_ON(!desc);\n+\trelease_backing(cluster, entry, nr);\n+\n \tfor (i = 0; i < nr; i++) {\n-\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n \t\tVM_WARN_ON(!desc);\n \t\tdesc->type = VSWAP_ZERO;\n \t}\n@@ -1261,89 +1290,6 @@ bool vswap_can_swapin_thp(swp_entry_t entry, int nr)\n \t\t(type == VSWAP_ZERO || type == VSWAP_SWAPFILE);\n }\n \n-/**\n- * swap_move - increment the swap slot by delta, checking the backing state and\n- *             return 0 if the backing state does not match (i.e wrong backing\n- *             state type, or wrong offset on the backing stores).\n- * @entry: the original virtual swap slot.\n- * @delta: the offset to increment the original slot.\n- *\n- * Note that this function is racy unless we can pin the backing state of these\n- * swap slots down with swapcache_prepare().\n- *\n- * Caller should only rely on this function as a best-effort hint otherwise,\n- * and should double-check after ensuring the whole range is pinned down.\n- *\n- * Return: the incremented virtual swap slot if the backing state matches, or\n- *         0 if the backing state does not match.\n- */\n-swp_entry_t swap_move(swp_entry_t entry, long delta)\n-{\n-\tstruct vswap_cluster *cluster = NULL;\n-\tstruct swp_desc *desc, *next_desc;\n-\tswp_entry_t next_entry;\n-\tstruct folio *folio = NULL, *next_folio = NULL;\n-\tenum swap_type type, next_type;\n-\tswp_slot_t slot = {0}, next_slot = {0};\n-\n-\tnext_entry.val = entry.val + delta;\n-\n-\trcu_read_lock();\n-\n-\t/* Look up first descriptor and get its type and backing store */\n-\tdesc = vswap_iter(&cluster, entry.val);\n-\tif (!desc) {\n-\t\trcu_read_unlock();\n-\t\treturn (swp_entry_t){0};\n-\t}\n-\n-\ttype = desc->type;\n-\tif (type == VSWAP_ZSWAP) {\n-\t\t/* zswap not supported for move */\n-\t\tspin_unlock(&cluster->lock);\n-\t\trcu_read_unlock();\n-\t\treturn (swp_entry_t){0};\n-\t}\n-\tif (type == VSWAP_FOLIO)\n-\t\tfolio = desc->swap_cache;\n-\telse if (type == VSWAP_SWAPFILE)\n-\t\tslot = desc->slot;\n-\n-\t/* Look up second descriptor and get its type and backing store */\n-\tnext_desc = vswap_iter(&cluster, next_entry.val);\n-\tif (!next_desc) {\n-\t\trcu_read_unlock();\n-\t\treturn (swp_entry_t){0};\n-\t}\n-\n-\tnext_type = next_desc->type;\n-\tif (next_type == VSWAP_FOLIO)\n-\t\tnext_folio = next_desc->swap_cache;\n-\telse if (next_type == VSWAP_SWAPFILE)\n-\t\tnext_slot = next_desc->slot;\n-\n-\tif (cluster)\n-\t\tspin_unlock(&cluster->lock);\n-\n-\trcu_read_unlock();\n-\n-\t/* Check if types match */\n-\tif (next_type != type)\n-\t\treturn (swp_entry_t){0};\n-\n-\t/* Check backing state consistency */\n-\tif (type == VSWAP_SWAPFILE &&\n-\t\t\t(swp_slot_type(next_slot) != swp_slot_type(slot) ||\n-\t\t\t\tswp_slot_offset(next_slot) !=\n-\t\t\t\t\t\t\tswp_slot_offset(slot) + delta))\n-\t\treturn (swp_entry_t){0};\n-\n-\tif (type == VSWAP_FOLIO && next_folio != folio)\n-\t\treturn (swp_entry_t){0};\n-\n-\treturn next_entry;\n-}\n-\n /*\n  * Return the count of contiguous swap entries that share the same\n  * VSWAP_ZERO status as the starting entry. If is_zeromap is not NULL,\n@@ -1863,11 +1809,10 @@ void zswap_entry_store(swp_entry_t swpentry, struct zswap_entry *entry)\n \tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n \n-\trelease_backing(swpentry, 1);\n-\n \trcu_read_lock();\n \tdesc = vswap_iter(&cluster, swpentry.val);\n \tVM_WARN_ON(!desc);\n+\trelease_backing(cluster, swpentry, 1);\n \tdesc->zswap_entry = entry;\n \tdesc->type = VSWAP_ZSWAP;\n \tspin_unlock(&cluster->lock);\n@@ -1914,17 +1859,22 @@ bool zswap_empty(swp_entry_t swpentry)\n #endif /* CONFIG_ZSWAP */\n \n #ifdef CONFIG_MEMCG\n-static unsigned short vswap_cgroup_record(swp_entry_t entry,\n-\t\t\t\tunsigned short memcgid, unsigned int nr_ents)\n+/*\n+ * __vswap_cgroup_record - record mem_cgroup for a set of swap entries\n+ *\n+ * Entered with the cluster locked. We will exit the function with the cluster\n+ * still locked.\n+ */\n+static unsigned short __vswap_cgroup_record(struct vswap_cluster *cluster,\n+\t\t\t\tswp_entry_t entry, unsigned short memcgid,\n+\t\t\t\tunsigned int nr_ents)\n {\n-\tstruct vswap_cluster *cluster = NULL;\n \tstruct swp_desc *desc;\n \tunsigned short oldid, iter = 0;\n \tint i;\n \n-\trcu_read_lock();\n \tfor (i = 0; i < nr_ents; i++) {\n-\t\tdesc = vswap_iter(&cluster, entry.val + i);\n+\t\tdesc = __vswap_iter(cluster, entry.val + i);\n \t\tVM_WARN_ON(!desc);\n \t\toldid = desc->memcgid;\n \t\tdesc->memcgid = memcgid;\n@@ -1932,6 +1882,37 @@ static unsigned short vswap_cgroup_record(swp_entry_t entry,\n \t\t\titer = oldid;\n \t\tVM_WARN_ON(iter != oldid);\n \t}\n+\n+\treturn oldid;\n+}\n+\n+/*\n+ * Clear swap cgroup for a range of swap entries.\n+ * Entered with the cluster locked. Caller must be under rcu_read_lock().\n+ */\n+static void __vswap_swap_cgroup_clear(struct vswap_cluster *cluster,\n+\t\t\t\t      swp_entry_t entry, unsigned int nr_ents)\n+{\n+\tunsigned short id;\n+\tstruct mem_cgroup *memcg;\n+\n+\tid = __vswap_cgroup_record(cluster, entry, 0, nr_ents);\n+\tmemcg = mem_cgroup_from_id(id);\n+\tif (memcg)\n+\t\tmem_cgroup_id_put_many(memcg, nr_ents);\n+}\n+\n+static unsigned short vswap_cgroup_record(swp_entry_t entry,\n+\t\t\t\tunsigned short memcgid, unsigned int nr_ents)\n+{\n+\tstruct vswap_cluster *cluster = NULL;\n+\tstruct swp_desc *desc;\n+\tunsigned short oldid;\n+\n+\trcu_read_lock();\n+\tdesc = vswap_iter(&cluster, entry.val);\n+\tVM_WARN_ON(!desc);\n+\toldid = __vswap_cgroup_record(cluster, entry, memcgid, nr_ents);\n \tspin_unlock(&cluster->lock);\n \trcu_read_unlock();\n \n@@ -1999,6 +1980,11 @@ unsigned short lookup_swap_cgroup_id(swp_entry_t entry)\n \trcu_read_unlock();\n \treturn ret;\n }\n+#else /* !CONFIG_MEMCG */\n+static void __vswap_swap_cgroup_clear(struct vswap_cluster *cluster,\n+\t\t\t\t      swp_entry_t entry, unsigned int nr_ents)\n+{\n+}\n #endif /* CONFIG_MEMCG */\n \n int vswap_init(void)\n-- \n2.47.3",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Rik van Riel",
      "primary_email": "riel@surriel.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Shakeel Butt",
      "primary_email": "shakeel.butt@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] arm64: remove HAVE_CMPXCHG_LOCAL",
          "message_id": "aZjiHt7h2z3Ye81_@linux.dev",
          "url": "https://lore.kernel.org/all/aZjiHt7h2z3Ye81_@linux.dev/",
          "date": "2026-02-20T23:27:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dev Jain",
              "summary": "Reviewer noted that the issue isn't with LL/SC/LSE but rather preempt_disable()/enable() in this_cpu_* functions, and suggested keeping the code while only removing the HAVE_CMPXCHG_LOCAL config selection.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Thanks. This concurs with my investigation on [1]. The problem\nisn't really LL/SC/LSE but preempt_disable()/enable() in\nthis_cpu_* [1, 2].\n\nI think you should only remove the selection of the config,\nbut keep the code? We may want to switch this on again if\nthe real issue gets solved.\n\n[1] https://lore.kernel.org/all/5a6782f3-d758-4d9c-975b-5ae4b5d80d4e@arm.com/\n[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/",
              "reply_to": "Jisheng Zhang",
              "message_date": "2026-02-16",
              "analysis_source": "llm"
            },
            {
              "author": "Will Deacon",
              "summary": "Reviewer Will Deacon expressed concern that the patch is system-dependent and may not be suitable for all architectures, particularly those that struggle with atomic operations.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "system-dependency",
                "micro-optimization"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That is _entirely_ dependent on the system, so this isn't the right\napproach. I also don't think it's something we particularly want to\nmicro-optimise to accomodate systems that suck at atomics.\n\nWill",
              "reply_to": "Jisheng Zhang",
              "message_date": "2026-02-16",
              "analysis_source": "llm"
            },
            {
              "author": "Dev Jain",
              "summary": "Reviewer Dev Jain suspects that the performance regression is caused by preempt_disable() in _pcp_protect_return, and suggests testing this hypothesis on other hardware to confirm whether disabling preempt_disable/enable improves performance.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "hypothesis"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Will,\n\nAs I mention in the other email, the suspect is not the atomics, but\npreempt_disable(). On Apple M3, the regression reported in [1] resolves\nby removing preempt_disable/enable in _pcp_protect_return. To prove\nthis another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the\nregression worsened, indicating that at least on Apple M3 the\natomics are faster.\n\nIt may help to confirm this hypothesis on other hardware - perhaps\nJisheng can test with this change on his hardware and confirm\nwhether he gets the same performance improvement.\n\nBy coincidence, Yang Shi has been discussing the this_cpu_* overhead\nat [2].\n\n[1] https://lore.kernel.org/all/1052a452-9ba3-4da7-be47-7d27d27b3d1d@arm.com/\n[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/",
              "reply_to": "Will Deacon",
              "message_date": "2026-02-16",
              "analysis_source": "llm"
            },
            {
              "author": "Catalin Marinas",
              "summary": "Reviewer Catalin Marinas suggested replacing preempt disabling with local_irq_save() in the arm64 code to still use LSE atomics, citing a potential issue where another CPU can access and modify the current CPU's variable via per_cpu_ptr().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Then why don't we replace the preempt disabling with local_irq_save()\nin the arm64 code and still use the LSE atomics?\n\nIIUC (lots of macro indirection), the generic cmpxchg is not atomic, so\nanother CPU is allowed to mess this up if it accesses current CPU's\nvariable via per_cpu_ptr().\n\n-- \nCatalin",
              "reply_to": "Dev Jain",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Will Deacon",
              "summary": "Reviewer Will Deacon suggested improving preempt_disable() performance as it is used in multiple places and proposed that users can also modify their .config to change preemption mode",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance improvement",
                "alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Even better, work on making preempt_disable() faster as it's used in many\nother places. Of course, if people want to hack the .config, they could\nalso change the preemption mode...\n\nWill",
              "reply_to": "Catalin Marinas",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Catalin Marinas",
              "summary": "The reviewer noted that preempt_enable_notrace() can unconditionally call __schedule(), and suggested a simple change to the preempt_schedule_notrace() function to avoid this issue, or alternatively changing the preemption model to make the macros no-ops.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, that would be good. It's the preempt_enable_notrace() path that\nends up calling preempt_schedule_notrace() -> __schedule() pretty much\nunconditionally. Not sure what would go wrong but some simple change\nlike this (can be done at a higher in the preempt macros to even avoid\ngetting here):\n\ndiff --git a/kernel/sched/core.c b/kernel/sched/core.c\nindex 854984967fe2..d9a5d6438303 100644\n--- a/kernel/sched/core.c\n+++ b/kernel/sched/core.c\n@@ -7119,7 +7119,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n \tif (likely(!preemptible()))\n \t\treturn;\n \n-\tdo {\n+\twhile (need_resched()) {\n \t\t/*\n \t\t * Because the function tracer can trace preempt_count_sub()\n \t\t * and it also uses preempt_enable/disable_notrace(), if\n@@ -7146,7 +7146,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n \n \t\tpreempt_latency_stop(1);\n \t\tpreempt_enable_no_resched_notrace();\n-\t} while (need_resched());\n+\t}\n }\n EXPORT_SYMBOL_GPL(preempt_schedule_notrace);\n \n\nOf course, changing the preemption model solves this by making the\nmacros no-ops but I assume people want to keep preemption on.\n\n-- \nCatalin",
              "reply_to": "Will Deacon",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph (Ampere)",
              "summary": "Reviewer noted that the performance of cmpxchg varies by platform and kernel config, citing measurements from 2 years ago on Ampere processors where it did not cause a regression, and suggested that preempt_enable/disable overhead is not incurred in production systems due to PREEMPT_FULL not being enabled",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yang Shi is on vacation but we have a patchset that removes\npreempt_enable/disable from this_cpu operations on ARM64.\n\nThe performance of cmpxchg varies by platform in use and with the kernel\nconfig. The measurements that I did 2 years ago indicated that the cmpxchg\nuse with Ampere processors did not cause a regression.\n\nNote that distro kernels often do not enable PREEMPT_FULL and therefore\npreempt_disable/enable overhead is not incurred in production systems.\n\nPREEMPT_VOLUNTARY does not use preemption for this_cpu ops.",
              "reply_to": "Dev Jain",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "K Nayak",
              "summary": "Reviewer K Nayak noted that the patch removes HAVE_CMPXCHG_LOCAL without considering the implications of cmpxchg128_local() on arm64, which is still using this feature.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "patch needs reevaluation",
                "consideration for arm64"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hello Catalin,\n\nOn 2/17/2026 10:18 PM, Catalin Marinas wrote:",
              "reply_to": "Catalin Marinas",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "K Nayak",
              "summary": "Reviewer K Nayak pointed out that the arm64 implementation's use of a separate 'need_resched' bit within the 'ti->preempt_count' union is not unconditional, as it still checks __preempt_count_dec_and_test() before calling into __schedule().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "What do you mean by unconditionally? We always check\n__preempt_count_dec_and_test() before calling into __schedule().\n\nOn x86, We use MSB of preempt_count to indicate a resched and\nset_preempt_need_resched() would just clear this MSB.\n\nIf the preempt_count() turns 0, we immediately go into schedule\nor  or the next preempt_enable() -> __preempt_count_dec_and_test()\nwould see the entire preempt_count being clear and will call into\nschedule.\n\nThe arm64 implementation seems to be doing something similar too\nwith a separate \"ti->preempt.need_resched\" bit which is part of\nthe \"ti->preempt_count\"'s union so it isn't really unconditional.",
              "reply_to": "Catalin Marinas",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "K Nayak",
              "summary": "Reviewer K Nayak pointed out that the patch introduces a redundant check for need_resched() state, which is already communicated by __preempt_count_dec_and_test(), and requested further investigation.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "redundant check",
                "further investigation"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Essentially you are simply checking it twice now on entry since\nneed_resched() state would have already been communicated by\n__preempt_count_dec_and_test().",
              "reply_to": "Catalin Marinas",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Catalin Marinas",
              "summary": "Reviewer Catalin Marinas noted that the patch removes HAVE_CMPXCHG_LOCAL, but this change is not sufficient to enable cmpxchg_local on arm64, as it requires additional changes in the arch/arm64/include/asm/percpu.h file.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "additional changes required"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Prateek,\n\nOn Wed, Feb 18, 2026 at 09:31:19AM +0530, K Prateek Nayak wrote:",
              "reply_to": "K Nayak",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Catalin Marinas",
              "summary": "Reviewer Catalin Marinas noted that the patch's performance improvement is due to an additional pointer chase and preempt_count update, not the removal of HAVE_CMPXCHG_LOCAL as claimed, and suggested exploring alternative optimizations.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Ah, yes, you are right. I got the polarity of need_resched in\nthread_info wrong (we should have named it no_need_to_resched).\n\nSo in the common case, the overhead is caused by the additional\npointer chase and preempt_count update, on top of the cpu offset read.\nNot sure we can squeeze any more cycles out of these without some\nlarge overhaul like:\n\nhttps://git.kernel.org/mark/c/84ee5f23f93d4a650e828f831da9ed29c54623c5\n\nor Yang's per-CPU page tables. Well, there are more ideas like in-kernel\nrestartable sequences but they move the overhead elsewhere.\n\nThanks.\n\n-- \nCatalin",
              "reply_to": "K Nayak",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that mod_node_page_state() can be called in NMI context, but the generic disable/enable irq implementation is not safe against NMIs on newer ARM architectures, and requested further consideration.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "lock ordering violation",
                "NMI context"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Please note that mod_node_page_state() can be called in NMI context and\ngeneric disable/enable irq are not safe against NMIs (newer arm arch supports\nNMI).",
              "reply_to": "Jisheng Zhang",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Jisheng Zhang (author)",
              "summary": "Author asked for clarification on whether reviewer thinks cmpxchg_local is better than generic disable/enable irq version on newer arm64 systems.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Will,\n\nI read this as an implication that the cmpxchg_local version is better\nthan generic disable/enable irq version on the newer arm64 systems. Is my\nunderstanding correct?",
              "reply_to": "Dev Jain",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Jisheng Zhang (author)",
              "summary": "Author acknowledged that removing preempt_disable/enable from _pcp_protect_return improves performance, but the HAVE_CMPXCHG_LOCAL version is still slower than the generic implementation on two test platforms.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a performance issue",
                "planned to re-evaluate"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Dev,\n\nThanks for the hints. I tried to remove the preempt_disable/enable from\n_pcp_protect_return, it improves, but the HAVE_CMPXCHG_LOCAL version is\nstill worse than generic disable/enable irq version on CA55 and CA73.",
              "reply_to": "Dev Jain",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Jisheng Zhang (author)",
              "summary": "Author acknowledges that arm, powerpc, and mips architectures may have issues with mod_node_page_state() in NMI context, but does not explicitly address the safety concern or plan a fix.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledges issue",
                "does not commit to fixing"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "hmm, interesting...\n\nfgrep HAVE_NMI arch/*/Kconfig\nthen\nfgrep HAVE_CMPXCHG_LOCAL arch/*/Kconfig\n\nshows that only x86, arm64, s390 and loongarch are safe, while arm,\npowerpc and mips enable HAVE_NMI but missing HAVE_CMPXCHG_LOCAL, so\nthey rely on generic generic disable/enable irq version, so you imply\nthat these three arch are not safe considering mod_node_page_state()\nin NMI context.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that vmstat updates may require NMI-safe cmpxchg operations, similar to memcg stats, and questioned whether adding complexity for certain architectures is necessary.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "complexity",
                "nmi-safe"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes it seems like it. For memcg stats, we use ARCH_HAVE_NMI_SAFE_CMPXCHG and\nARCH_HAS_NMI_SAFE_THIS_CPU_OPS config options to correctly handle the updates\nfrom NMI context. Maybe we need something similar for vmstat as well.\n\nSo arm, powerpc and mips does not have ARCH_HAS_NMI_SAFE_THIS_CPU_OPS but\npowerpc does have ARCH_HAVE_NMI_SAFE_CMPXCHG and arm has\nit for CPU_V7, CPU_V7M & CPU_V6K models.\n\nI wonder if we need to add complexity for these archs.",
              "reply_to": "Jisheng Zhang",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Heiko Carstens",
              "summary": "Reviewer noted that the patch's removal of HAVE_CMPXCHG_LOCAL may be problematic due to the enforcement of PREEMPT_LAZY in newer kernels, which can lead to full preempt_disable()/preempt_enable() overhead for this_cpu_* operations.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "PREEMPT_LAZY",
                "PREEMPT_BUILD"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Note that with commit 7dadeaa6e851 (\"sched: Further restrict the preemption\nmodes\") at least PREEMPT_LAZY is enforced, which comes together with\nPREEMPT_BUILD and full preempt_disable()/preempt_enable() overhead for\nthis_cpu_* ops for every \"up-to-date\" architecture (except x86).\n\nPREEMPT_NONE and PREEMPT_VOLUNTARY are gone for those architectures.",
              "reply_to": "Christoph (Ampere)",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 0/4] mm: zone lock tracepoint instrumentation",
          "message_id": "aZjg6PWn_xhZV7Nb@linux.dev",
          "url": "https://lore.kernel.org/all/aZjg6PWn_xhZV7Nb@linux.dev/",
          "date": "2026-02-20T22:36:39Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing a concern about direct zone lock acquire/release operations being replaced with the newly introduced wrappers in preparation for future tracepoint instrumentation. The changes are purely mechanical substitutions, and locking semantics and ordering remain unchanged. No functional change intended.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing concerns about the lack of visibility into zone lock contention and its impact on allocation and reclaim latency. They acknowledge that generic lock tracepoints are insufficient for detailed analysis and explain their approach to adding dedicated tracepoint instrumentation, which will enable holder/waiter analysis and lock hold time measurements without affecting the fast path when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging a technical issue",
                "explaining an alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the compatibility of zone lock wrappers with existing code that uses compact_lock_irqsave(), which operates directly on spinlock_t pointers. The author introduced a new struct compact_lock to abstract the underlying lock type, allowing it to dispatch to the appropriate lock/unlock helper based on the lock type.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to zone lock acquire/release operations, explaining that the implementation follows the mmap_lock tracepoint pattern and does not affect the fast path when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to improve flow, recommending that zone lock wrappers and tracepoints be introduced together in a single patch before mechanically converting users to the wrappers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the function should not return a value and suggested replacing it with an if-else statement for clarity.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested moving zone lock wrapper changes that don't use compact_* to a later patch, arguing they are not directly relevant to the current patch's additions and would fit better in the final patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change",
                "suggested reordering"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to squash patches (1) and (2), stating that it's just a matter of personal preference, but also mentioned that the series ordering is good as is.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "personal preference"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author addressed a concern about the patch series structure, explaining that he intentionally split behavior-preserving refactoring from instrumentation changes to keep the instrumentation isolated and make it easier to review and bisect. He does not plan to reorder the patches as suggested.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "author provided explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the patch order is not as important as the technical changes, and expressed appreciation for the patches",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "appreciation",
                "acknowledgment of priority"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v5] mm: move pgscan, pgsteal, pgrefill to node stats",
          "message_id": "aZjdZv1eJc4HanML@linux.dev",
          "url": "https://lore.kernel.org/all/aZjdZv1eJc4HanML@linux.dev/",
          "date": "2026-02-20T22:19:03Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Zi Yan",
              "summary": "Reviewer Zi Yan noted that the patch introduces a potential issue where the global counters are accumulated differently due to the change in how mod_lruvec_state() updates the counters, and requested further investigation into this change.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential issue with global counter accumulation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "Acked-by: Zi Yan <ziy@nvidia.com>\n\nBest Regards,\nYan, Zi",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-20",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-20",
              "analysis_source": "heuristic"
            },
            {
              "author": "Michal Hocko",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-23",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 2/2] mm: memcontrol: switch to native NR_VMALLOC vmstat counter",
          "message_id": "aZjdCfE1tww_WKwh@linux.dev",
          "url": "https://lore.kernel.org/all/aZjdCfE1tww_WKwh@linux.dev/",
          "date": "2026-02-20T22:16:03Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Johannes Weiner (author)",
              "summary": "The author is addressing a concern about the removal of MEMCG_VMALLOC from memcg_stat_items and memory_stats[]. They acknowledged that it was an oversight and will re-add NR_VMALLOC to these arrays in v2, ensuring consistent accounting.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "re-add in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Eliminates the custom memcg counter and results in a single,\nconsolidated accounting call in vmalloc code.\n\nSigned-off-by: Johannes Weiner <hannes@cmpxchg.org>\n---\n include/linux/memcontrol.h |  1 -\n mm/memcontrol.c            |  4 ++--\n mm/vmalloc.c               | 16 ++++------------\n 3 files changed, 6 insertions(+), 15 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 67f154de10bc..c7cc4e50e59a 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -35,7 +35,6 @@ enum memcg_stat_item {\n \tMEMCG_SWAP = NR_VM_NODE_STAT_ITEMS,\n \tMEMCG_SOCK,\n \tMEMCG_PERCPU_B,\n-\tMEMCG_VMALLOC,\n \tMEMCG_KMEM,\n \tMEMCG_ZSWAP_B,\n \tMEMCG_ZSWAPPED,\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 129eed3ff5bb..fef5bdd887e0 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -317,6 +317,7 @@ static const unsigned int memcg_node_stat_items[] = {\n \tNR_SHMEM_THPS,\n \tNR_FILE_THPS,\n \tNR_ANON_THPS,\n+\tNR_VMALLOC,\n \tNR_KERNEL_STACK_KB,\n \tNR_PAGETABLE,\n \tNR_SECONDARY_PAGETABLE,\n@@ -339,7 +340,6 @@ static const unsigned int memcg_stat_items[] = {\n \tMEMCG_SWAP,\n \tMEMCG_SOCK,\n \tMEMCG_PERCPU_B,\n-\tMEMCG_VMALLOC,\n \tMEMCG_KMEM,\n \tMEMCG_ZSWAP_B,\n \tMEMCG_ZSWAPPED,\n@@ -1359,7 +1359,7 @@ static const struct memory_stat memory_stats[] = {\n \t{ \"sec_pagetables\",\t\tNR_SECONDARY_PAGETABLE\t\t},\n \t{ \"percpu\",\t\t\tMEMCG_PERCPU_B\t\t\t},\n \t{ \"sock\",\t\t\tMEMCG_SOCK\t\t\t},\n-\t{ \"vmalloc\",\t\t\tMEMCG_VMALLOC\t\t\t},\n+\t{ \"vmalloc\",\t\t\tNR_VMALLOC\t\t\t},\n \t{ \"shmem\",\t\t\tNR_SHMEM\t\t\t},\n #ifdef CONFIG_ZSWAP\n \t{ \"zswap\",\t\t\tMEMCG_ZSWAP_B\t\t\t},\ndiff --git a/mm/vmalloc.c b/mm/vmalloc.c\nindex a49a46de9c4f..8773bc0c4734 100644\n--- a/mm/vmalloc.c\n+++ b/mm/vmalloc.c\n@@ -3446,9 +3446,6 @@ void vfree(const void *addr)\n \n \tif (unlikely(vm->flags & VM_FLUSH_RESET_PERMS))\n \t\tvm_reset_perms(vm);\n-\t/* All pages of vm should be charged to same memcg, so use first one. */\n-\tif (vm->nr_pages && !(vm->flags & VM_MAP_PUT_PAGES))\n-\t\tmod_memcg_page_state(vm->pages[0], MEMCG_VMALLOC, -vm->nr_pages);\n \tfor (i = 0; i < vm->nr_pages; i++) {\n \t\tstruct page *page = vm->pages[i];\n \n@@ -3458,7 +3455,7 @@ void vfree(const void *addr)\n \t\t * can be freed as an array of order-0 allocations\n \t\t */\n \t\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n-\t\t\tdec_node_page_state(page, NR_VMALLOC);\n+\t\t\tmod_lruvec_page_state(page, NR_VMALLOC, -1);\n \t\t__free_page(page);\n \t\tcond_resched();\n \t}\n@@ -3649,7 +3646,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n \t\t\tcontinue;\n \t\t}\n \n-\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n+\t\tmod_lruvec_page_state(page, NR_VMALLOC, 1 << large_order);\n \n \t\tsplit_page(page, large_order);\n \t\tfor (i = 0; i < (1U << large_order); i++)\n@@ -3696,7 +3693,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n \t\t\t\t\t\t\tpages + nr_allocated);\n \n \t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n-\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n+\t\t\t\tmod_lruvec_page_state(pages[i], NR_VMALLOC, 1);\n \n \t\t\tnr_allocated += nr;\n \n@@ -3722,7 +3719,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n \t\tif (unlikely(!page))\n \t\t\tbreak;\n \n-\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n+\t\tmod_lruvec_page_state(page, NR_VMALLOC, 1 << order);\n \n \t\t/*\n \t\t * High-order allocations must be able to be treated as\n@@ -3866,11 +3863,6 @@ static void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,\n \t\t\tvmalloc_gfp_adjust(gfp_mask, page_order), node,\n \t\t\tpage_order, nr_small_pages, area->pages);\n \n-\t/* All pages of vm should be charged to same memcg, so use first one. */\n-\tif (gfp_mask & __GFP_ACCOUNT && area->nr_pages)\n-\t\tmod_memcg_page_state(area->pages[0], MEMCG_VMALLOC,\n-\t\t\t\t     area->nr_pages);\n-\n \t/*\n \t * If not enough pages were obtained to accomplish an\n \t * allocation request, free them via vfree() if any.\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt pointed out that the NR_VMALLOC vmstat counter is not being updated correctly in the vmap_notify_list, specifically when a new allocation is made, and requested that the update be moved to the correct location.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "update_vmstat_counter"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n[...]",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt pointed out that the mod_node_page_state() function requires a struct pglist_data pointer as its first argument, and suggested using page_pgdat(page) to obtain this pointer.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "mod_node_page_state() takes 'struct pglist_data *pgdat', you need to use\npage_pgdat(page) as first param.",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that the patch description should be updated to include an Acked-by tag for themselves and requested it be added.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change",
                "update patch description"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "Same here.\n\nWith above fixes, you can add:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-20",
              "analysis_source": "heuristic"
            },
            {
              "author": "Uladzislau Rezki",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-23",
              "analysis_source": "heuristic"
            },
            {
              "author": "Uladzislau Rezki",
              "summary": "The reviewer suggested moving *_node_page_stat() calls to a single, consistent location in vm_area_alloc_pages(), specifically at the end or before splitting high-order pages, to avoid unnecessary looping over small pages.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()?\n\nOr mod_node_page_state in first place should be invoked on high-order\npage before split(to avoid of looping over small pages afterword)?\n\nI mean it would be good to place to the one solid place. If it is possible\nof course.\n\n--\nUladzislau Rezk",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner (author)",
              "summary": "Author acknowledged a mistake in their patch compilation and apologized for it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "apology",
                "acknowledgment of error"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Good catch, my apologies. Serves me right for not compiling\nincrementally.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 1/2] mm: vmalloc: streamline vmalloc memory accounting",
          "message_id": "aZjaxAi-AzyOYzNT@linux.dev",
          "url": "https://lore.kernel.org/all/aZjaxAi-AzyOYzNT@linux.dev/",
          "date": "2026-02-20T22:09:35Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Johannes Weiner (author)",
              "summary": "The author addressed a concern about the removal of the custom memcg counter and its replacement with a single consolidated accounting call in vmalloc code, explaining that this change eliminates the custom counter and results in a single accounting call.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Eliminates the custom memcg counter and results in a single,\nconsolidated accounting call in vmalloc code.\n\nSigned-off-by: Johannes Weiner <hannes@cmpxchg.org>\n---\n include/linux/memcontrol.h |  1 -\n mm/memcontrol.c            |  4 ++--\n mm/vmalloc.c               | 16 ++++------------\n 3 files changed, 6 insertions(+), 15 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 67f154de10bc..c7cc4e50e59a 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -35,7 +35,6 @@ enum memcg_stat_item {\n \tMEMCG_SWAP = NR_VM_NODE_STAT_ITEMS,\n \tMEMCG_SOCK,\n \tMEMCG_PERCPU_B,\n-\tMEMCG_VMALLOC,\n \tMEMCG_KMEM,\n \tMEMCG_ZSWAP_B,\n \tMEMCG_ZSWAPPED,\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 129eed3ff5bb..fef5bdd887e0 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -317,6 +317,7 @@ static const unsigned int memcg_node_stat_items[] = {\n \tNR_SHMEM_THPS,\n \tNR_FILE_THPS,\n \tNR_ANON_THPS,\n+\tNR_VMALLOC,\n \tNR_KERNEL_STACK_KB,\n \tNR_PAGETABLE,\n \tNR_SECONDARY_PAGETABLE,\n@@ -339,7 +340,6 @@ static const unsigned int memcg_stat_items[] = {\n \tMEMCG_SWAP,\n \tMEMCG_SOCK,\n \tMEMCG_PERCPU_B,\n-\tMEMCG_VMALLOC,\n \tMEMCG_KMEM,\n \tMEMCG_ZSWAP_B,\n \tMEMCG_ZSWAPPED,\n@@ -1359,7 +1359,7 @@ static const struct memory_stat memory_stats[] = {\n \t{ \"sec_pagetables\",\t\tNR_SECONDARY_PAGETABLE\t\t},\n \t{ \"percpu\",\t\t\tMEMCG_PERCPU_B\t\t\t},\n \t{ \"sock\",\t\t\tMEMCG_SOCK\t\t\t},\n-\t{ \"vmalloc\",\t\t\tMEMCG_VMALLOC\t\t\t},\n+\t{ \"vmalloc\",\t\t\tNR_VMALLOC\t\t\t},\n \t{ \"shmem\",\t\t\tNR_SHMEM\t\t\t},\n #ifdef CONFIG_ZSWAP\n \t{ \"zswap\",\t\t\tMEMCG_ZSWAP_B\t\t\t},\ndiff --git a/mm/vmalloc.c b/mm/vmalloc.c\nindex a49a46de9c4f..8773bc0c4734 100644\n--- a/mm/vmalloc.c\n+++ b/mm/vmalloc.c\n@@ -3446,9 +3446,6 @@ void vfree(const void *addr)\n \n \tif (unlikely(vm->flags & VM_FLUSH_RESET_PERMS))\n \t\tvm_reset_perms(vm);\n-\t/* All pages of vm should be charged to same memcg, so use first one. */\n-\tif (vm->nr_pages && !(vm->flags & VM_MAP_PUT_PAGES))\n-\t\tmod_memcg_page_state(vm->pages[0], MEMCG_VMALLOC, -vm->nr_pages);\n \tfor (i = 0; i < vm->nr_pages; i++) {\n \t\tstruct page *page = vm->pages[i];\n \n@@ -3458,7 +3455,7 @@ void vfree(const void *addr)\n \t\t * can be freed as an array of order-0 allocations\n \t\t */\n \t\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n-\t\t\tdec_node_page_state(page, NR_VMALLOC);\n+\t\t\tmod_lruvec_page_state(page, NR_VMALLOC, -1);\n \t\t__free_page(page);\n \t\tcond_resched();\n \t}\n@@ -3649,7 +3646,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n \t\t\tcontinue;\n \t\t}\n \n-\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n+\t\tmod_lruvec_page_state(page, NR_VMALLOC, 1 << large_order);\n \n \t\tsplit_page(page, large_order);\n \t\tfor (i = 0; i < (1U << large_order); i++)\n@@ -3696,7 +3693,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n \t\t\t\t\t\t\tpages + nr_allocated);\n \n \t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n-\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n+\t\t\t\tmod_lruvec_page_state(pages[i], NR_VMALLOC, 1);\n \n \t\t\tnr_allocated += nr;\n \n@@ -3722,7 +3719,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n \t\tif (unlikely(!page))\n \t\t\tbreak;\n \n-\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n+\t\tmod_lruvec_page_state(page, NR_VMALLOC, 1 << order);\n \n \t\t/*\n \t\t * High-order allocations must be able to be treated as\n@@ -3866,11 +3863,6 @@ static void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,\n \t\t\tvmalloc_gfp_adjust(gfp_mask, page_order), node,\n \t\t\tpage_order, nr_small_pages, area->pages);\n \n-\t/* All pages of vm should be charged to same memcg, so use first one. */\n-\tif (gfp_mask & __GFP_ACCOUNT && area->nr_pages)\n-\t\tmod_memcg_page_state(area->pages[0], MEMCG_VMALLOC,\n-\t\t\t\t     area->nr_pages);\n-\n \t/*\n \t * If not enough pages were obtained to accomplish an\n \t * allocation request, free them via vfree() if any.\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that the patch does not handle the case where a node is going down and requested the addition of a check to ensure that the global_node_page_state() call is safe before calling try_to_unmap().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n[...]",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "The reviewer pointed out that the function mod_node_page_state() requires a struct pglist_data pointer as its first argument, and suggested using page_pgdat(page) to obtain this pointer.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "mod_node_page_state() takes 'struct pglist_data *pgdat', you need to use\npage_pgdat(page) as first param.",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt approved the patch after the suggested fixes were applied and added their Acked-by tag.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK",
                "APPROVAL"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "Same here.\n\nWith above fixes, you can add:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-20",
              "analysis_source": "heuristic"
            },
            {
              "author": "Uladzislau Rezki",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-23",
              "analysis_source": "heuristic"
            },
            {
              "author": "Uladzislau Rezki",
              "summary": "Reviewer suggested moving *_node_page_stat() calls to a single, consistent location in vm_area_alloc_pages(), specifically at the end or before splitting high-order pages",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()?\n\nOr mod_node_page_state in first place should be invoked on high-order\npage before split(to avoid of looping over small pages afterword)?\n\nI mean it would be good to place to the one solid place. If it is possible\nof course.\n\n--\nUladzislau Rezk",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner (author)",
              "summary": "Author acknowledged a mistake in their patch and apologized for it, indicating no fix is planned as the issue was simply a compilation error.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "apology",
                "acknowledgment of mistake"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Good catch, my apologies. Serves me right for not compiling\nincrementally.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Usama Arif",
      "primary_email": "usama.arif@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    }
  ]
}