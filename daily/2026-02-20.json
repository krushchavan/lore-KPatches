{
  "date": "2026-02-20",
  "report_file": "2026-02-20.html",
  "status": "complete",
  "last_updated": "2026-02-26 01:16 UTC",
  "llm_backends": [],
  "generation_time_seconds": 133.82781314849854,
  "developer_reports": [
    {
      "name": "Alexandre Ghiti",
      "primary_email": "alexghiti@rivosinc.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Boris Burkov",
      "primary_email": "boris@bur.io",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/1] btrfs: set BTRFS_ROOT_ORPHAN_CLEANUP during subvol create",
          "message_id": "718a3b0c2275324b9e287af7e4434f55a4a45901.1771529877.git.boris@bur.io",
          "url": "https://lore.kernel.org/all/718a3b0c2275324b9e287af7e4434f55a4a45901.1771529877.git.boris@bur.io/",
          "date": "2026-02-19T19:38:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-19",
          "patch_summary": "We have recently observed a number of subvolumes with broken dentries. ls-ing the parent dir looks like:\n\ndrwxrwxrwt 1 root root 16 Jan 23 16:49 . drwxr-xr-x 1 root root 24 Jan 23 16:48 .. d????????? ? ?    ?     ?            ? broken_subvol\n\nand similarly stat-ing the file fails.\n\nIn this state, deleting the subvol fails with ENOENT, but attempting to create a new file or subvol over it errors out with EEXIST and even aborts the fs. Which leaves us a bit stuck.\n\ndmesg contains a single notable error message reading: \"could not do orphan cleanup -2\"\n\n2 is ENOENT and the error comes from the failure handling path of btrfs_orphan_cleanup(), with the stack leading back up to btrfs_lookup().",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Please include the stack trace in the changelog. What do you mean by \"first does\"? The first call to btrfs_orphan_cleanup()? Also please use full function name, orphan_cleanup() -> btrfs_orphan_cleanup() orphan_cleanup -> btrfs_orphan_cleanup() I find this confusing:   1 -> 1 + N ? Shouldn't it be 1 -> 2, meaning the igrab() increased i_count from 1 to 2? Where does this decrement of parent->d_lockref.count happens exactly? I don't see it immediately in iput(), or iput_final(). Please put the full call chain. Looks reasonable to me.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Thu, Feb 19, 2026 at 7:38â€¯PM Boris Burkov <boris@bur.io> wrote:\n>\n> We have recently observed a number of subvolumes with broken dentries.\n> ls-ing the parent dir looks like:\n>\n> drwxrwxrwt 1 root root 16 Jan 23 16:49 .\n> drwxr-xr-x 1 root root 24 Jan 23 16:48 ..\n> d????????? ? ?    ?     ?            ? broken_subvol\n>\n> and similarly stat-ing the file fails.\n>\n> In this state, deleting the subvol fails with ENOENT, but attempting to\n> create a new file or subvol over it errors out with EEXIST and even\n> aborts the fs. Which leaves us a bit stuck.\n>\n> dmesg contains a single notable error message reading:\n> \"could not do orphan cleanup -2\"\n>\n> 2 is ENOENT and the error comes from the failure handling path of\n> btrfs_orphan_cleanup(), with the stack leading back up to\n> btrfs_lookup().\n\nPlease include the stack trace in the changelog.\n\n>\n> After some detailed inspection of the internal state, it became clear\n> that:\n> - there are no orphan items for the subvol\n> - the subvol is otherwise healthy looking, it is not half-deleted or\n>   anything, there is no drop progress, etc.\n> - the subvol was created a while ago and first does orphan_cleanup()\n\nWhat do you mean by \"first does\"? The first call to btrfs_orphan_cleanup()?\n\nAlso please use full function name, orphan_cleanup() -> btrfs_orphan_cleanup()\n\n>   much later\n> - after orphan_cleanup fails, btrfs_lookup_dentry() returns -ENOENT,\n\norphan_cleanup -> btrfs_orphan_cleanup()\n\n>   which results in a negative dentry for the subvolume via\n>   d_splice_alias(NULL, dentry), leading to the observed behavior. The\n>   bug can be mitigated by dropping the dentry cache, at which point we\n>   can successfully delete the subvolume if we want.\n>\n> btrfs_orphan_cleanup() does test_and_set_bit(BTRFS_ROOT_ORPHAN_CLEANUP)\n> on the root when it runs, so it cannot run more than once on a given\n> root, so something else must run concurrently. However, the obvious\n> routes to deleting an orphan when nlinks goes to 0 should not be able to\n> run without first doing a lookup into the subvolume, which should run\n> btrfs_orphan_cleanup() and set the bit.\n>\n> The final important observation is that create_subvol() calls\n> d_instantiate_new() but does not set BTRFS_ROOT_ORPHAN_CLEANUP, so if\n> the dentry cache gets dropped, the next lookup into the subvolume will\n> make a real call into btrfs_orphan_cleanup() for the first time. This\n> opens up the possibility of concurrently deleting the inode/orphan items\n> but most typical evict() paths will be holding a reference on the parent\n> dentry (child dentry holds parent->d_lockref.count via dget in\n> d_alloc(), released in __dentry_kill()) and prevent the parent from\n> being removed from the dentry cache.\n>\n> The one exception is delayed iputs. Ordered extent creation calls\n> igrab() on the inode. If the file is unlinked and closed while those\n> refs are held, iput() in __dentry_kill() decrements i_count but does\n> not trigger eviction (i_count > 0). The child dentry is freed and the\n> subvol dentry's d_lockref.count drops to 0, making it evictable while\n> the inode is still alive.\n>\n> Since there are two races (the race between writeback and unlink and\n> the race between lookup and delayed iputs), and there are too many moving\n> parts, the following two diagrams show the complete picture. The first\n> sets us up in a condition where we can evict the subvol dentry and there\n> is a delayed iput on the inode:\n>\n> T1 (task)                    T2 (writeback)                   T3 (OE workqueue)\n>\n> write() // dirty pages\n>                               btrfs_writepages()\n>                                 btrfs_run_delalloc_range()\n>                                   cow_file_range()\n>                                     btrfs_alloc_ordered_extent()\n>                                       igrab() // i_count: 1 -> 1+N\n\nI find this confusing:   1 -> 1 + N ?\n\nShouldn't it be 1 -> 2, meaning the igrab() increased i_count from 1 to 2?\n\n> btrfs_unlink_inode()\n>   btrfs_orphan_add()\n> close()\n>   __dentry_kill()\n>     dentry_unlink_inode()\n>       iput() // 1+N -> N\n>     --parent->d_lockref.count\n\nWhere does this decrement of parent->d_lockref.count happens exactly?\nI don't see it immediately in iput(), or iput_final(). Please put the\nfull call chain.\n\n\n>     // count -> 0\n>                                                                 finish_ordered_fn()\n>                                                                   btrfs_finish_ordered_io()\n>                                                                     btrfs_put_ordered_extent()\n>                                                                       btrfs_add_delayed_iput()\n>\n> Once the delayed iput is pending and the subvol dentry is evictable,\n> the shrinker can free it, causing the next lookup to go through\n> btrfs_lookup() and call btrfs_orphan_cleanup() for the first time.\n> If the cleaner kthread processes the delayed iput concurrently, the\n> two race:\n>\n>   T1 (shrinker)              T2 (cleaner kthread)                          T3 (lookup)\n>\n>   super_cache_scan()\n>     prune_dcache_sb()\n>       __dentry_kill()\n>       // subvol dentry freed\n>                               btrfs_run_delayed_iputs()\n>                                 iput()  // i_count -> 0\n>                                   evict()  // sets I_FREEING\n>                                     btrfs_evict_inode()\n>                                       // truncation loop\n>                                                                             btrfs_lookup()\n>                                                                               btrfs_lookup_dentry()\n>                                                                                 btrfs_orphan_cleanup()\n>                                                                                   // first call (bit never set)\n>                                                                                   btrfs_iget()\n>                                                                                     // blocks on I_FREEING\n>\n>                                       btrfs_orphan_del()\n>                                       // inode freed\n>                                                                                     // returns -ENOENT\n>                                                                                   btrfs_del_orphan_item()\n>                                                                                     // -ENOENT\n>                                                                                 // \"could not do orphan cleanup -2\"\n>                                                                             d_splice_alias(NULL, dentry)\n>                                                                             // negative dentry for valid subvol\n>\n> The most straightforward fix is to ensure the invariant that a dentry\n> for a subvolume can exist if and only if that subvolume has\n> BTRFS_ROOT_ORPHAN_CLEANUP set on its root (and is known to have no\n> orphans or ran btrfs_orphan_cleanup()).\n\nLooks reasonable to me.\n\nThanks!\n\n>\n> Signed-off-by: Boris Burkov <boris@bur.io>\n> ---\n>  fs/btrfs/ioctl.c | 7 +++++++\n>  1 file changed, 7 insertions(+)\n>\n> diff --git a/fs/btrfs/ioctl.c b/fs/btrfs/ioctl.c\n> index b8db877be61cc..77f7db18c6ca5 100644\n> --- a/fs/btrfs/ioctl.c\n> +++ b/fs/btrfs/ioctl.c\n> @@ -672,6 +672,13 @@ static noinline int create_subvol(struct mnt_idmap *idmap,\n>                 goto out;\n>         }\n>\n> +       /*\n> +        * Subvolumes have orphans cleaned on first dentry lookup. A new\n> +        * subvolume cannot have any orphans, so we should set the bit before we\n> +        * add the subvolume dentry to the dentry cache, so that it is in the\n> +        * same state as a subvolume after first lookup.\n> +        */\n> +       set_bit(BTRFS_ROOT_ORPHAN_CLEANUP, &new_root->state);\n>         d_instantiate_new(dentry, new_inode_args.inode);\n>         new_inode_args.inode = NULL;\n>\n> --\n> 2.47.3\n>\n>\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Dmitry Ilvokhin",
      "primary_email": "d@ilvokhin.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 4/4] mm: add tracepoints for zone lock",
          "message_id": "1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Add tracepoint instrumentation to zone lock acquire/release operations via the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a lightweight inline helper checks whether the tracepoint is enabled and calls into an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 3/4] mm: convert compaction to zone lock wrappers",
          "message_id": "3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Compaction uses compact_lock_irqsave(), which currently operates on a raw spinlock_t pointer so that it can be used for both zone->lock and lru_lock. Since zone lock operations are now wrapped, compact_lock_irqsave() can no longer operate directly on a spinlock_t when the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The structure carries a lock type enum and a union holding either a zone pointer or a raw spinlock_t pointer, and dispatches to the appropriate lock/unlock helper.\n\nNo functional change intended.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Cheatham, Benjamin",
              "summary": "Nit: You could remove the helpers above and just do the calls directly in this function, though it would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay since they have the __acquires() annotations. You don't need the return statement here (and you shouldn't be returning a value at all). It may be cleaner to just do an if-else statement here instead. I would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you didn't change it due to location but I would argue it isn't really relevant to what's being added in this patch and fits better in the last.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "nits"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> Compaction uses compact_lock_irqsave(), which currently operates\n> on a raw spinlock_t pointer so that it can be used for both\n> zone->lock and lru_lock. Since zone lock operations are now wrapped,\n> compact_lock_irqsave() can no longer operate directly on a spinlock_t\n> when the lock belongs to a zone.\n> \n> Introduce struct compact_lock to abstract the underlying lock type. The\n> structure carries a lock type enum and a union holding either a zone\n> pointer or a raw spinlock_t pointer, and dispatches to the appropriate\n> lock/unlock helper.\n> \n> No functional change intended.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> ---\n>  mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n>  1 file changed, 89 insertions(+), 19 deletions(-)\n> \n> diff --git a/mm/compaction.c b/mm/compaction.c\n> index 1e8f8eca318c..1b000d2b95b2 100644\n> --- a/mm/compaction.c\n> +++ b/mm/compaction.c\n> @@ -24,6 +24,7 @@\n>  #include <linux/page_owner.h>\n>  #include <linux/psi.h>\n>  #include <linux/cpuset.h>\n> +#include <linux/zone_lock.h>\n>  #include \"internal.h\"\n>  \n>  #ifdef CONFIG_COMPACTION\n> @@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n>  }\n>  #endif /* CONFIG_COMPACTION */\n>  \n> +enum compact_lock_type {\n> +\tCOMPACT_LOCK_ZONE,\n> +\tCOMPACT_LOCK_RAW_SPINLOCK,\n> +};\n> +\n> +struct compact_lock {\n> +\tenum compact_lock_type type;\n> +\tunion {\n> +\t\tstruct zone *zone;\n> +\t\tspinlock_t *lock; /* Reference to lru lock */\n> +\t};\n> +};\n> +\n> +static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n> +\t\t\t\t\t    unsigned long *flags)\n> +{\n> +\treturn zone_trylock_irqsave(zone, *flags);\n> +}\n> +\n> +static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n> +\t\t\t\t\t   unsigned long *flags)\n> +{\n> +\treturn spin_trylock_irqsave(lock, *flags);\n> +}\n> +\n> +static bool compact_do_trylock_irqsave(struct compact_lock lock,\n> +\t\t\t\t       unsigned long *flags)\n> +{\n> +\tif (lock.type == COMPACT_LOCK_ZONE)\n> +\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n> +\n> +\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n> +}\n\nNit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n> +\n> +static void compact_do_zone_lock_irqsave(struct zone *zone,\n> +\t\t\t\t\t unsigned long *flags)\n> +__acquires(zone->lock)\n> +{\n> +\tzone_lock_irqsave(zone, *flags);\n> +}\n> +\n> +static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n> +\t\t\t\t\tunsigned long *flags)\n> +__acquires(lock)\n> +{\n> +\tspin_lock_irqsave(lock, *flags);\n> +}\n> +\n> +static void compact_do_lock_irqsave(struct compact_lock lock,\n> +\t\t\t\t    unsigned long *flags)\n> +{\n> +\tif (lock.type == COMPACT_LOCK_ZONE) {\n> +\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n> +\t\treturn;\n> +\t}\n> +\n> +\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n> +}\n> +\n>  /*\n>   * Compaction requires the taking of some coarse locks that are potentially\n>   * very heavily contended. For async compaction, trylock and record if the\n> @@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n>   *\n>   * Always returns true which makes it easier to track lock state in callers.\n>   */\n> -static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n> -\t\t\t\t\t\tstruct compact_control *cc)\n> -\t__acquires(lock)\n> +static bool compact_lock_irqsave(struct compact_lock lock,\n> +\t\t\t\t unsigned long *flags,\n> +\t\t\t\t struct compact_control *cc)\n>  {\n>  \t/* Track if the lock is contended in async mode */\n>  \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n> -\t\tif (spin_trylock_irqsave(lock, *flags))\n> +\t\tif (compact_do_trylock_irqsave(lock, flags))\n>  \t\t\treturn true;\n>  \n>  \t\tcc->contended = true;\n>  \t}\n>  \n> -\tspin_lock_irqsave(lock, *flags);\n> +\tcompact_do_lock_irqsave(lock, flags);\n>  \treturn true;\n>  }\n>  \n> @@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n>   * Returns true if compaction should abort due to fatal signal pending.\n>   * Returns false when compaction can continue.\n>   */\n> -static bool compact_unlock_should_abort(spinlock_t *lock,\n> -\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n> +static bool compact_unlock_should_abort(struct zone *zone,\n> +\t\t\t\t\tunsigned long flags,\n> +\t\t\t\t\tbool *locked,\n> +\t\t\t\t\tstruct compact_control *cc)\n>  {\n>  \tif (*locked) {\n> -\t\tspin_unlock_irqrestore(lock, flags);\n> +\t\tzone_unlock_irqrestore(zone, flags);\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.\n\n>  \t\t*locked = false;\n>  \t}\n>  \n> @@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n>  \t\t * contention, to give chance to IRQs. Abort if fatal signal\n>  \t\t * pending.\n>  \t\t */\n> -\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n> -\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n> -\t\t\t\t\t\t\t\t&locked, cc))\n> +\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n> +\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n>  \t\t\tbreak;\n>  \n>  \t\tnr_scanned++;\n> @@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n>  \n>  \t\t/* If we already hold the lock, we can skip some rechecking. */\n>  \t\tif (!locked) {\n> -\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n> -\t\t\t\t\t\t\t\t&flags, cc);\n> +\t\t\tstruct compact_lock zol = {\n> +\t\t\t\t.type = COMPACT_LOCK_ZONE,\n> +\t\t\t\t.zone = cc->zone,\n> +\t\t\t};\n> +\n> +\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n>  \n>  \t\t\t/* Recheck this is a buddy page under lock */\n>  \t\t\tif (!PageBuddy(page))\n> @@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n>  \t}\n>  \n>  \tif (locked)\n> -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> +\t\tzone_unlock_irqrestore(cc->zone, flags);\n>  \n>  \t/*\n>  \t * Be careful to not go outside of the pageblock.\n> @@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n>  \n>  \t\t/* If we already hold the lock, we can skip some rechecking */\n>  \t\tif (lruvec != locked) {\n> +\t\t\tstruct compact_lock zol = {\n> +\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n> +\t\t\t\t.lock = &lruvec->lru_lock,\n> +\t\t\t};\n> +\n>  \t\t\tif (locked)\n>  \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n>  \n> -\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n> +\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n>  \t\t\tlocked = lruvec;\n>  \n>  \t\t\tlruvec_memcg_debug(lruvec, folio);\n> @@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n>  \t\tif (!area->nr_free)\n>  \t\t\tcontinue;\n>  \n> -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> +\t\tzone_lock_irqsave(cc->zone, flags);\n>  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n>  \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n>  \t\t\tunsigned long pfn;\n> @@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n>  \t\t\t}\n>  \t\t}\n>  \n> -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> +\t\tzone_unlock_irqrestore(cc->zone, flags);\n>  \n>  \t\t/* Skip fast search if enough freepages isolated */\n>  \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n> @@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n>  \t\tif (!area->nr_free)\n>  \t\t\tcontinue;\n>  \n> -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> +\t\tzone_lock_irqsave(cc->zone, flags);\n>  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n>  \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n>  \t\t\tunsigned long free_pfn;\n> @@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n>  \t\t\t\tbreak;\n>  \t\t\t}\n>  \t\t}\n> -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> +\t\tzone_unlock_irqrestore(cc->zone, flags);\n>  \t}\n>  \n>  \tcc->total_migrate_scanned += nr_scanned;\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Yes, I agree, there is no much value in this wrappers, will remove them, Yes, agree, will fix in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "nits"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:10:05PM -0600, Cheatham, Benjamin wrote:\n> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> > Compaction uses compact_lock_irqsave(), which currently operates\n> > on a raw spinlock_t pointer so that it can be used for both\n> > zone->lock and lru_lock. Since zone lock operations are now wrapped,\n> > compact_lock_irqsave() can no longer operate directly on a spinlock_t\n> > when the lock belongs to a zone.\n> > \n> > Introduce struct compact_lock to abstract the underlying lock type. The\n> > structure carries a lock type enum and a union holding either a zone\n> > pointer or a raw spinlock_t pointer, and dispatches to the appropriate\n> > lock/unlock helper.\n> > \n> > No functional change intended.\n> > \n> > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > ---\n> >  mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n> >  1 file changed, 89 insertions(+), 19 deletions(-)\n> > \n> > diff --git a/mm/compaction.c b/mm/compaction.c\n> > index 1e8f8eca318c..1b000d2b95b2 100644\n> > --- a/mm/compaction.c\n> > +++ b/mm/compaction.c\n> > @@ -24,6 +24,7 @@\n> >  #include <linux/page_owner.h>\n> >  #include <linux/psi.h>\n> >  #include <linux/cpuset.h>\n> > +#include <linux/zone_lock.h>\n> >  #include \"internal.h\"\n> >  \n> >  #ifdef CONFIG_COMPACTION\n> > @@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n> >  }\n> >  #endif /* CONFIG_COMPACTION */\n> >  \n> > +enum compact_lock_type {\n> > +\tCOMPACT_LOCK_ZONE,\n> > +\tCOMPACT_LOCK_RAW_SPINLOCK,\n> > +};\n> > +\n> > +struct compact_lock {\n> > +\tenum compact_lock_type type;\n> > +\tunion {\n> > +\t\tstruct zone *zone;\n> > +\t\tspinlock_t *lock; /* Reference to lru lock */\n> > +\t};\n> > +};\n> > +\n> > +static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n> > +\t\t\t\t\t    unsigned long *flags)\n> > +{\n> > +\treturn zone_trylock_irqsave(zone, *flags);\n> > +}\n> > +\n> > +static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n> > +\t\t\t\t\t   unsigned long *flags)\n> > +{\n> > +\treturn spin_trylock_irqsave(lock, *flags);\n> > +}\n> > +\n> > +static bool compact_do_trylock_irqsave(struct compact_lock lock,\n> > +\t\t\t\t       unsigned long *flags)\n> > +{\n> > +\tif (lock.type == COMPACT_LOCK_ZONE)\n> > +\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n> > +\n> > +\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n> > +}\n> \n> Nit: You could remove the helpers above and just do the calls directly in this function, though\n> it would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\n> since they have the __acquires() annotations.\n\nYes, I agree, there is no much value in this wrappers, will remove them,\nthanks!\n\n> > +\n> > +static void compact_do_zone_lock_irqsave(struct zone *zone,\n> > +\t\t\t\t\t unsigned long *flags)\n> > +__acquires(zone->lock)\n> > +{\n> > +\tzone_lock_irqsave(zone, *flags);\n> > +}\n> > +\n> > +static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n> > +\t\t\t\t\tunsigned long *flags)\n> > +__acquires(lock)\n> > +{\n> > +\tspin_lock_irqsave(lock, *flags);\n> > +}\n> > +\n> > +static void compact_do_lock_irqsave(struct compact_lock lock,\n> > +\t\t\t\t    unsigned long *flags)\n> > +{\n> > +\tif (lock.type == COMPACT_LOCK_ZONE) {\n> > +\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n> > +\t\treturn;\n> > +\t}\n> > +\n> > +\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n> \n> You don't need the return statement here (and you shouldn't be returning a value at all).\n\nYes, agree, will fix in v2.\n\n> \n> It may be cleaner to just do an if-else statement here instead.\n> \n> > +}\n> > +\n> >  /*\n> >   * Compaction requires the taking of some coarse locks that are potentially\n> >   * very heavily contended. For async compaction, trylock and record if the\n> > @@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n> >   *\n> >   * Always returns true which makes it easier to track lock state in callers.\n> >   */\n> > -static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n> > -\t\t\t\t\t\tstruct compact_control *cc)\n> > -\t__acquires(lock)\n> > +static bool compact_lock_irqsave(struct compact_lock lock,\n> > +\t\t\t\t unsigned long *flags,\n> > +\t\t\t\t struct compact_control *cc)\n> >  {\n> >  \t/* Track if the lock is contended in async mode */\n> >  \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n> > -\t\tif (spin_trylock_irqsave(lock, *flags))\n> > +\t\tif (compact_do_trylock_irqsave(lock, flags))\n> >  \t\t\treturn true;\n> >  \n> >  \t\tcc->contended = true;\n> >  \t}\n> >  \n> > -\tspin_lock_irqsave(lock, *flags);\n> > +\tcompact_do_lock_irqsave(lock, flags);\n> >  \treturn true;\n> >  }\n> >  \n> > @@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n> >   * Returns true if compaction should abort due to fatal signal pending.\n> >   * Returns false when compaction can continue.\n> >   */\n> > -static bool compact_unlock_should_abort(spinlock_t *lock,\n> > -\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n> > +static bool compact_unlock_should_abort(struct zone *zone,\n> > +\t\t\t\t\tunsigned long flags,\n> > +\t\t\t\t\tbool *locked,\n> > +\t\t\t\t\tstruct compact_control *cc)\n> >  {\n> >  \tif (*locked) {\n> > -\t\tspin_unlock_irqrestore(lock, flags);\n> > +\t\tzone_unlock_irqrestore(zone, flags);\n> \n> I would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\n> didn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\n> and fits better in the last.\n\nThanks for the suggestion. Totally makes sense to me, will do in v2 as well.\n\n> \n> >  \t\t*locked = false;\n> >  \t}\n> >  \n> > @@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n> >  \t\t * contention, to give chance to IRQs. Abort if fatal signal\n> >  \t\t * pending.\n> >  \t\t */\n> > -\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n> > -\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n> > -\t\t\t\t\t\t\t\t&locked, cc))\n> > +\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n> > +\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n> >  \t\t\tbreak;\n> >  \n> >  \t\tnr_scanned++;\n> > @@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n> >  \n> >  \t\t/* If we already hold the lock, we can skip some rechecking. */\n> >  \t\tif (!locked) {\n> > -\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n> > -\t\t\t\t\t\t\t\t&flags, cc);\n> > +\t\t\tstruct compact_lock zol = {\n> > +\t\t\t\t.type = COMPACT_LOCK_ZONE,\n> > +\t\t\t\t.zone = cc->zone,\n> > +\t\t\t};\n> > +\n> > +\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n> >  \n> >  \t\t\t/* Recheck this is a buddy page under lock */\n> >  \t\t\tif (!PageBuddy(page))\n> > @@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n> >  \t}\n> >  \n> >  \tif (locked)\n> > -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> > +\t\tzone_unlock_irqrestore(cc->zone, flags);\n> >  \n> >  \t/*\n> >  \t * Be careful to not go outside of the pageblock.\n> > @@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n> >  \n> >  \t\t/* If we already hold the lock, we can skip some rechecking */\n> >  \t\tif (lruvec != locked) {\n> > +\t\t\tstruct compact_lock zol = {\n> > +\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n> > +\t\t\t\t.lock = &lruvec->lru_lock,\n> > +\t\t\t};\n> > +\n> >  \t\t\tif (locked)\n> >  \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n> >  \n> > -\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n> > +\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n> >  \t\t\tlocked = lruvec;\n> >  \n> >  \t\t\tlruvec_memcg_debug(lruvec, folio);\n> > @@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n> >  \t\tif (!area->nr_free)\n> >  \t\t\tcontinue;\n> >  \n> > -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> > +\t\tzone_lock_irqsave(cc->zone, flags);\n> >  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n> >  \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n> >  \t\t\tunsigned long pfn;\n> > @@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n> >  \t\t\t}\n> >  \t\t}\n> >  \n> > -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> > +\t\tzone_unlock_irqrestore(cc->zone, flags);\n> >  \n> >  \t\t/* Skip fast search if enough freepages isolated */\n> >  \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n> > @@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n> >  \t\tif (!area->nr_free)\n> >  \t\t\tcontinue;\n> >  \n> > -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> > +\t\tzone_lock_irqsave(cc->zone, flags);\n> >  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n> >  \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n> >  \t\t\tunsigned long free_pfn;\n> > @@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n> >  \t\t\t\tbreak;\n> >  \t\t\t}\n> >  \t\t}\n> > -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> > +\t\tzone_unlock_irqrestore(cc->zone, flags);\n> >  \t}\n> >  \n> >  \tcc->total_migrate_scanned += nr_scanned;\n> \n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/4] mm: zone lock tracepoint instrumentation",
          "message_id": "cover.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/cover.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Zone lock contention can significantly impact allocation and reclaim latency, as it is a central synchronization point in the page allocator and reclaim paths. Improved visibility into its behavior is therefore important for diagnosing performance issues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable zone lock contention. Deeper analysis of lock holders and waiters is currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints cover the slow path, they do not provide sufficient visibility into lock hold times. In particular, the lack of a release-side event makes it difficult to identify long lock holders and correlate them with waiters. As a result, distinguishing between short bursts of contention and pathological long hold times requires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to zone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock hold time measurements without affecting the fast path when tracing is disabled.\n\nThe series is structured as follows:\n\n1. Introduce zone lock wrappers. 2. Mechanically convert zone lock users to the wrappers. 3. Convert compaction to use the wrappers (requires minor restructuring of compact_lock_irqsave()). 4. Add zone lock tracepoints.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Cheatham, Benjamin",
              "summary": "I think you can improve the flow of this series if reorder as follows: 1. Introduce zone lock wrappers 4. Add zone lock tracepoints 2. Mechanically convert zone lock users to the wrappers 3. Convert compaction to use the wrappers... and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in patch 1 by the time they get to patch 4.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> Zone lock contention can significantly impact allocation and\n> reclaim latency, as it is a central synchronization point in\n> the page allocator and reclaim paths. Improved visibility into\n> its behavior is therefore important for diagnosing performance\n> issues in memory-intensive workloads.\n> \n> On some production workloads at Meta, we have observed noticeable\n> zone lock contention. Deeper analysis of lock holders and waiters\n> is currently difficult with existing instrumentation.\n> \n> While generic lock contention_begin/contention_end tracepoints\n> cover the slow path, they do not provide sufficient visibility\n> into lock hold times. In particular, the lack of a release-side\n> event makes it difficult to identify long lock holders and\n> correlate them with waiters. As a result, distinguishing between\n> short bursts of contention and pathological long hold times\n> requires additional instrumentation.\n> \n> This patch series adds dedicated tracepoint instrumentation to\n> zone lock, following the existing mmap_lock tracing model.\n> \n> The goal is to enable detailed holder/waiter analysis and lock\n> hold time measurements without affecting the fast path when\n> tracing is disabled.\n> \n> The series is structured as follows:\n> \n>   1. Introduce zone lock wrappers.\n>   2. Mechanically convert zone lock users to the wrappers.\n>   3. Convert compaction to use the wrappers (requires minor\n>      restructuring of compact_lock_irqsave()).\n>   4. Add zone lock tracepoints.\n\nI think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "I don't think this suggestion will make anything better. This just seems like a different taste. If I make a suggestion, I would request to squash (1) and (2) i.e. patch containing wrappers and their use together but that is just my taste and would be a nit. The series ordering is good as is.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:\n> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> > Zone lock contention can significantly impact allocation and\n> > reclaim latency, as it is a central synchronization point in\n> > the page allocator and reclaim paths. Improved visibility into\n> > its behavior is therefore important for diagnosing performance\n> > issues in memory-intensive workloads.\n> > \n> > On some production workloads at Meta, we have observed noticeable\n> > zone lock contention. Deeper analysis of lock holders and waiters\n> > is currently difficult with existing instrumentation.\n> > \n> > While generic lock contention_begin/contention_end tracepoints\n> > cover the slow path, they do not provide sufficient visibility\n> > into lock hold times. In particular, the lack of a release-side\n> > event makes it difficult to identify long lock holders and\n> > correlate them with waiters. As a result, distinguishing between\n> > short bursts of contention and pathological long hold times\n> > requires additional instrumentation.\n> > \n> > This patch series adds dedicated tracepoint instrumentation to\n> > zone lock, following the existing mmap_lock tracing model.\n> > \n> > The goal is to enable detailed holder/waiter analysis and lock\n> > hold time measurements without affecting the fast path when\n> > tracing is disabled.\n> > \n> > The series is structured as follows:\n> > \n> >   1. Introduce zone lock wrappers.\n> >   2. Mechanically convert zone lock users to the wrappers.\n> >   3. Convert compaction to use the wrappers (requires minor\n> >      restructuring of compact_lock_irqsave()).\n> >   4. Add zone lock tracepoints.\n> \n> I think you can improve the flow of this series if reorder as follows:\n> \t1. Introduce zone lock wrappers\n> \t4. Add zone lock tracepoints\n> \t2. Mechanically convert zone lock users to the wrappers\n> \t3. Convert compaction to use the wrappers...\n> \n> and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\n> wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\n> patch 1 by the time they get to patch 4.\n\nI don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "I structured the series intentionally to keep all behavior-preserving refactoring separate from the actual instrumentation change. In particular, I had to split the conversion into two patches to separate the purely mechanical changes from the compaction restructuring. With the current order, tracepoints addition remains a single, atomic functional change on top of a fully converted tree. This keeps the instrumentation isolated from the refactoring and with an intention to make bisection and review of the behavioral change easier. Reordering as suggested would mix instrumentation with intermediate refactoring states, which I'd prefer to avoid. I hope this reasoning makes sense, but I'm happy to discuss if there are strong objections.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:\n> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> > Zone lock contention can significantly impact allocation and\n> > reclaim latency, as it is a central synchronization point in\n> > the page allocator and reclaim paths. Improved visibility into\n> > its behavior is therefore important for diagnosing performance\n> > issues in memory-intensive workloads.\n> > \n> > On some production workloads at Meta, we have observed noticeable\n> > zone lock contention. Deeper analysis of lock holders and waiters\n> > is currently difficult with existing instrumentation.\n> > \n> > While generic lock contention_begin/contention_end tracepoints\n> > cover the slow path, they do not provide sufficient visibility\n> > into lock hold times. In particular, the lack of a release-side\n> > event makes it difficult to identify long lock holders and\n> > correlate them with waiters. As a result, distinguishing between\n> > short bursts of contention and pathological long hold times\n> > requires additional instrumentation.\n> > \n> > This patch series adds dedicated tracepoint instrumentation to\n> > zone lock, following the existing mmap_lock tracing model.\n> > \n> > The goal is to enable detailed holder/waiter analysis and lock\n> > hold time measurements without affecting the fast path when\n> > tracing is disabled.\n> > \n> > The series is structured as follows:\n> > \n> >   1. Introduce zone lock wrappers.\n> >   2. Mechanically convert zone lock users to the wrappers.\n> >   3. Convert compaction to use the wrappers (requires minor\n> >      restructuring of compact_lock_irqsave()).\n> >   4. Add zone lock tracepoints.\n> \n> I think you can improve the flow of this series if reorder as follows:\n> \t1. Introduce zone lock wrappers\n> \t4. Add zone lock tracepoints\n> \t2. Mechanically convert zone lock users to the wrappers\n> \t3. Convert compaction to use the wrappers...\n> \n> and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\n> wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\n> patch 1 by the time they get to patch 4.\n\nHi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.\n\n> \n> Thanks,\n> Ben\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "No that's fine, I figured as much. I just wasn't sure that was more important to you than what (I thought) was a better reading order for the series.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "\n\nOn 2/23/2026 10:46 AM, Dmitry Ilvokhin wrote:\n> [You don't often get email from d@ilvokhin.com. Learn why this is important at https://aka.ms/LearnAboutSenderIdentification ]\n> \n> On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:\n>> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n>>> Zone lock contention can significantly impact allocation and\n>>> reclaim latency, as it is a central synchronization point in\n>>> the page allocator and reclaim paths. Improved visibility into\n>>> its behavior is therefore important for diagnosing performance\n>>> issues in memory-intensive workloads.\n>>>\n>>> On some production workloads at Meta, we have observed noticeable\n>>> zone lock contention. Deeper analysis of lock holders and waiters\n>>> is currently difficult with existing instrumentation.\n>>>\n>>> While generic lock contention_begin/contention_end tracepoints\n>>> cover the slow path, they do not provide sufficient visibility\n>>> into lock hold times. In particular, the lack of a release-side\n>>> event makes it difficult to identify long lock holders and\n>>> correlate them with waiters. As a result, distinguishing between\n>>> short bursts of contention and pathological long hold times\n>>> requires additional instrumentation.\n>>>\n>>> This patch series adds dedicated tracepoint instrumentation to\n>>> zone lock, following the existing mmap_lock tracing model.\n>>>\n>>> The goal is to enable detailed holder/waiter analysis and lock\n>>> hold time measurements without affecting the fast path when\n>>> tracing is disabled.\n>>>\n>>> The series is structured as follows:\n>>>\n>>>   1. Introduce zone lock wrappers.\n>>>   2. Mechanically convert zone lock users to the wrappers.\n>>>   3. Convert compaction to use the wrappers (requires minor\n>>>      restructuring of compact_lock_irqsave()).\n>>>   4. Add zone lock tracepoints.\n>>\n>> I think you can improve the flow of this series if reorder as follows:\n>>       1. Introduce zone lock wrappers\n>>       4. Add zone lock tracepoints\n>>       2. Mechanically convert zone lock users to the wrappers\n>>       3. Convert compaction to use the wrappers...\n>>\n>> and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\n>> wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\n>> patch 1 by the time they get to patch 4.\n> \n> Hi Ben,\n> \n> Thanks for the suggestion.\n> \n> I structured the series intentionally to keep all behavior-preserving\n> refactoring separate from the actual instrumentation change.\n> \n> In particular, I had to split the conversion into two patches to\n> separate the purely mechanical changes from the compaction\n> restructuring. With the current order, tracepoints addition remains a\n> single, atomic functional change on top of a fully converted tree. This\n> keeps the instrumentation isolated from the refactoring and with an\n> intention to make bisection and review of the behavioral change easier.\n> \n> Reordering as suggested would mix instrumentation with intermediate\n> refactoring states, which I'd prefer to avoid.\n> \n> I hope this reasoning makes sense, but I'm happy to discuss if there are\n> strong objections.\n\nNo that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen\n\n> \n>>\n>> Thanks,\n>> Ben\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 2/4] mm: convert zone lock users to wrappers",
          "message_id": "7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Replace direct zone lock acquire/release operations with the newly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change intended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be handled separately in the following patch due to additional non-trivial modifications.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Wed, Feb 11, 2026 at 03:22:14PM +0000, Dmitry Ilvokhin wrote:\n> Replace direct zone lock acquire/release operations with the\n> newly introduced wrappers.\n> \n> The changes are purely mechanical substitutions. No functional change\n> intended. Locking semantics and ordering remain unchanged.\n> \n> The compaction path is left unchanged for now and will be\n> handled separately in the following patch due to additional\n> non-trivial modifications.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Add thin wrappers around zone lock acquire/release operations. This prepares the code for future tracepoint instrumentation without modifying individual call sites.\n\nCentralizing zone lock operations behind wrappers allows future instrumentation or debugging hooks to be added without touching all users.\n\nNo functional change intended. The wrappers are introduced in preparation for subsequent patches and are not yet used.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Any reason you used macros for above two and inlined functions for remaining?",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> Add thin wrappers around zone lock acquire/release operations. This\n> prepares the code for future tracepoint instrumentation without\n> modifying individual call sites.\n> \n> Centralizing zone lock operations behind wrappers allows future\n> instrumentation or debugging hooks to be added without touching\n> all users.\n> \n> No functional change intended. The wrappers are introduced in\n> preparation for subsequent patches and are not yet used.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> ---\n>  MAINTAINERS               |  1 +\n>  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n>  2 files changed, 39 insertions(+)\n>  create mode 100644 include/linux/zone_lock.h\n> \n> diff --git a/MAINTAINERS b/MAINTAINERS\n> index b4088f7290be..680c9ae02d7e 100644\n> --- a/MAINTAINERS\n> +++ b/MAINTAINERS\n> @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n>  F:\tinclude/linux/ptdump.h\n>  F:\tinclude/linux/vmpressure.h\n>  F:\tinclude/linux/vmstat.h\n> +F:\tinclude/linux/zone_lock.h\n>  F:\tkernel/fork.c\n>  F:\tmm/Kconfig\n>  F:\tmm/debug.c\n> diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> new file mode 100644\n> index 000000000000..c531e26280e6\n> --- /dev/null\n> +++ b/include/linux/zone_lock.h\n> @@ -0,0 +1,38 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +#ifndef _LINUX_ZONE_LOCK_H\n> +#define _LINUX_ZONE_LOCK_H\n> +\n> +#include <linux/mmzone.h>\n> +#include <linux/spinlock.h>\n> +\n> +static inline void zone_lock_init(struct zone *zone)\n> +{\n> +\tspin_lock_init(&zone->lock);\n> +}\n> +\n> +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> +do {\t\t\t\t\t\t\t\t\\\n> +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> +} while (0)\n> +\n> +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> +({\t\t\t\t\t\t\t\t\\\n> +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> +})\n\nAny reason you used macros for above two and inlined functions for remaining?\n\n> +\n> +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n> +{\n> +\tspin_unlock_irqrestore(&zone->lock, flags);\n> +}\n> +\n> +static inline void zone_lock_irq(struct zone *zone)\n> +{\n> +\tspin_lock_irq(&zone->lock);\n> +}\n> +\n> +static inline void zone_unlock_irq(struct zone *zone)\n> +{\n> +\tspin_unlock_irq(&zone->lock);\n> +}\n> +\n> +#endif /* _LINUX_ZONE_LOCK_H */\n> -- \n> 2.47.3\n> \n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The reason for using macros in those two cases is that they need to modify the flags variable passed by the caller, just like spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same convention here. If we used normal inline functions instead, we would need to pass a pointer to flags, which would change the call sites and diverge from the existing *_irqsave() locking pattern. There is also a difference between zone_lock_irqsave() and zone_trylock_irqsave() implementations: the former is implemented as a do { } while (0) macro since it does not return a value, while the latter uses a GCC extension in order to return the trylock result. This matches spin_lock_* convention as well.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > Add thin wrappers around zone lock acquire/release operations. This\n> > prepares the code for future tracepoint instrumentation without\n> > modifying individual call sites.\n> > \n> > Centralizing zone lock operations behind wrappers allows future\n> > instrumentation or debugging hooks to be added without touching\n> > all users.\n> > \n> > No functional change intended. The wrappers are introduced in\n> > preparation for subsequent patches and are not yet used.\n> > \n> > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > ---\n> >  MAINTAINERS               |  1 +\n> >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> >  2 files changed, 39 insertions(+)\n> >  create mode 100644 include/linux/zone_lock.h\n> > \n> > diff --git a/MAINTAINERS b/MAINTAINERS\n> > index b4088f7290be..680c9ae02d7e 100644\n> > --- a/MAINTAINERS\n> > +++ b/MAINTAINERS\n> > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> >  F:\tinclude/linux/ptdump.h\n> >  F:\tinclude/linux/vmpressure.h\n> >  F:\tinclude/linux/vmstat.h\n> > +F:\tinclude/linux/zone_lock.h\n> >  F:\tkernel/fork.c\n> >  F:\tmm/Kconfig\n> >  F:\tmm/debug.c\n> > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > new file mode 100644\n> > index 000000000000..c531e26280e6\n> > --- /dev/null\n> > +++ b/include/linux/zone_lock.h\n> > @@ -0,0 +1,38 @@\n> > +/* SPDX-License-Identifier: GPL-2.0 */\n> > +#ifndef _LINUX_ZONE_LOCK_H\n> > +#define _LINUX_ZONE_LOCK_H\n> > +\n> > +#include <linux/mmzone.h>\n> > +#include <linux/spinlock.h>\n> > +\n> > +static inline void zone_lock_init(struct zone *zone)\n> > +{\n> > +\tspin_lock_init(&zone->lock);\n> > +}\n> > +\n> > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > +do {\t\t\t\t\t\t\t\t\\\n> > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > +} while (0)\n> > +\n> > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > +({\t\t\t\t\t\t\t\t\\\n> > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > +})\n> \n> Any reason you used macros for above two and inlined functions for remaining?\n>\n\nThe reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.\n\n> > +\n> > +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n> > +{\n> > +\tspin_unlock_irqrestore(&zone->lock, flags);\n> > +}\n> > +\n> > +static inline void zone_lock_irq(struct zone *zone)\n> > +{\n> > +\tspin_lock_irq(&zone->lock);\n> > +}\n> > +\n> > +static inline void zone_unlock_irq(struct zone *zone)\n> > +{\n> > +\tspin_unlock_irq(&zone->lock);\n> > +}\n> > +\n> > +#endif /* _LINUX_ZONE_LOCK_H */\n> > -- \n> > 2.47.3\n> > \n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Cool, thanks for the explanation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 03:18:04PM +0000, Dmitry Ilvokhin wrote:\n> On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> > On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > > Add thin wrappers around zone lock acquire/release operations. This\n> > > prepares the code for future tracepoint instrumentation without\n> > > modifying individual call sites.\n> > > \n> > > Centralizing zone lock operations behind wrappers allows future\n> > > instrumentation or debugging hooks to be added without touching\n> > > all users.\n> > > \n> > > No functional change intended. The wrappers are introduced in\n> > > preparation for subsequent patches and are not yet used.\n> > > \n> > > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > > ---\n> > >  MAINTAINERS               |  1 +\n> > >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> > >  2 files changed, 39 insertions(+)\n> > >  create mode 100644 include/linux/zone_lock.h\n> > > \n> > > diff --git a/MAINTAINERS b/MAINTAINERS\n> > > index b4088f7290be..680c9ae02d7e 100644\n> > > --- a/MAINTAINERS\n> > > +++ b/MAINTAINERS\n> > > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> > >  F:\tinclude/linux/ptdump.h\n> > >  F:\tinclude/linux/vmpressure.h\n> > >  F:\tinclude/linux/vmstat.h\n> > > +F:\tinclude/linux/zone_lock.h\n> > >  F:\tkernel/fork.c\n> > >  F:\tmm/Kconfig\n> > >  F:\tmm/debug.c\n> > > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > > new file mode 100644\n> > > index 000000000000..c531e26280e6\n> > > --- /dev/null\n> > > +++ b/include/linux/zone_lock.h\n> > > @@ -0,0 +1,38 @@\n> > > +/* SPDX-License-Identifier: GPL-2.0 */\n> > > +#ifndef _LINUX_ZONE_LOCK_H\n> > > +#define _LINUX_ZONE_LOCK_H\n> > > +\n> > > +#include <linux/mmzone.h>\n> > > +#include <linux/spinlock.h>\n> > > +\n> > > +static inline void zone_lock_init(struct zone *zone)\n> > > +{\n> > > +\tspin_lock_init(&zone->lock);\n> > > +}\n> > > +\n> > > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > > +do {\t\t\t\t\t\t\t\t\\\n> > > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +} while (0)\n> > > +\n> > > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > > +({\t\t\t\t\t\t\t\t\\\n> > > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +})\n> > \n> > Any reason you used macros for above two and inlined functions for remaining?\n> >\n> \n> The reason for using macros in those two cases is that they need to\n> modify the flags variable passed by the caller, just like\n> spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\n> convention here.\n> \n> If we used normal inline functions instead, we would need to pass a\n> pointer to flags, which would change the call sites and diverge from the\n> existing *_irqsave() locking pattern.\n> \n> There is also a difference between zone_lock_irqsave() and\n> zone_trylock_irqsave() implementations: the former is implemented as a\n> do { } while (0) macro since it does not return a value, while the\n> latter uses a GCC extension in order to return the trylock result. This\n> matches spin_lock_* convention as well.\n> \n\nCool, thanks for the explanation.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> Add thin wrappers around zone lock acquire/release operations. This\n> prepares the code for future tracepoint instrumentation without\n> modifying individual call sites.\n> \n> Centralizing zone lock operations behind wrappers allows future\n> instrumentation or debugging hooks to be added without touching\n> all users.\n> \n> No functional change intended. The wrappers are introduced in\n> preparation for subsequent patches and are not yet used.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Steven Rostedt",
              "summary": "Have you thought about adding guards as well. It could make the code simpler: (Not tested) #include <linux/cleanup.h> [..] DEFINE_LOCK_GUARD_1(zonelock_irqsave, struct zone *, zone_lock_irqsave(_T->lock, _T->flags), zone_unlock_irqrestore(_T->lock, _T->flags), unsigned long flags) DECLARE_LOCK_GUARD_1_ATTRS(zonelock_irqsave, __acquires(_T), __releases(*(struct zone ***)_T)) #define class_zonelock_irqsave_constructor(_T) WITH_LOCK_GUARD_1_ATTRS(zonelock_irqsave, _T) DEFINE_LOCK_GUARD_1(zonelock_irq, struct zone *, zone_lock_irq(_T->lock), zone_unlock_irq(_T->lock)) DECLARE_LOCK_GUARD_1_ATTRS(zonelock_irq, __acquires(_T), __releases(*(struct zone ***)_T)) #define...",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Wed, 11 Feb 2026 15:22:13 +0000\nDmitry Ilvokhin <d@ilvokhin.com> wrote:\n\n\n> diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> new file mode 100644\n> index 000000000000..c531e26280e6\n> --- /dev/null\n> +++ b/include/linux/zone_lock.h\n> @@ -0,0 +1,38 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +#ifndef _LINUX_ZONE_LOCK_H\n> +#define _LINUX_ZONE_LOCK_H\n> +\n> +#include <linux/mmzone.h>\n> +#include <linux/spinlock.h>\n> +\n> +static inline void zone_lock_init(struct zone *zone)\n> +{\n> +\tspin_lock_init(&zone->lock);\n> +}\n> +\n> +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> +do {\t\t\t\t\t\t\t\t\\\n> +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> +} while (0)\n> +\n> +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> +({\t\t\t\t\t\t\t\t\\\n> +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> +})\n> +\n> +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n> +{\n> +\tspin_unlock_irqrestore(&zone->lock, flags);\n> +}\n> +\n> +static inline void zone_lock_irq(struct zone *zone)\n> +{\n> +\tspin_lock_irq(&zone->lock);\n> +}\n> +\n> +static inline void zone_unlock_irq(struct zone *zone)\n> +{\n> +\tspin_unlock_irq(&zone->lock);\n> +}\n> +\n> +#endif /* _LINUX_ZONE_LOCK_H */\n\nHave you thought about adding guards as well. It could make the code simpler:\n\n  (Not tested)\n\n#include <linux/cleanup.h>\n[..]\n\nDEFINE_LOCK_GUARD_1(zonelock_irqsave, struct zone *,\n\t\t    zone_lock_irqsave(_T->lock, _T->flags),\n\t\t    zone_unlock_irqrestore(_T->lock, _T->flags),\n\t\t    unsigned long flags)\nDECLARE_LOCK_GUARD_1_ATTRS(zonelock_irqsave, __acquires(_T), __releases(*(struct zone ***)_T))\n#define class_zonelock_irqsave_constructor(_T) WITH_LOCK_GUARD_1_ATTRS(zonelock_irqsave, _T)\n\nDEFINE_LOCK_GUARD_1(zonelock_irq, struct zone *,\n\t\t    zone_lock_irq(_T->lock),\n\t\t    zone_unlock_irq(_T->lock))\nDECLARE_LOCK_GUARD_1_ATTRS(zonelock_irq, __acquires(_T), __releases(*(struct zone ***)_T))\n#define class_zonelock_irq_constructor(_T) WITH_LOCK_GUARD_1_ATTRS(zonelock_irq, _T)\n\nThen you could even remove the \"flags\" variables from the C code, and some goto unlocks.\n\n-- Steve\n\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Gregory Price",
      "primary_email": "gourry@gourry.net",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 0/3] pull region-specific logic into new files",
          "message_id": "20260211204206.2171525-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260211204206.2171525-1-gourry@gourry.net/",
          "date": "2026-02-11T20:42:11Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "cxl/core/region.c presently contains logic to handle cxl_region, cxl_pmem_region, and cxl_dax_region.  The cxl_pmem_region and cxl_dax_region management code deserves new files to make it clear that this logic applies to a specific types of regions.\n\nThis also breaks up development space so fewer conflicts can occur, and it becomes clear where changes are actually happening.\n\nI snuck in a cleanup.h fixup for devm_cxl_add_dax_region to tidy up some of the existing functions.\n\nv2 -> v3:  renamed from x_region to region_x because it's prettier added cleanup.h small nits asked for by Jonathan (commas)",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v3 3/3] cxl/core: use cleanup.h for devm_cxl_add_dax_region",
          "message_id": "aZfv6qQR5DoZ7Chp@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZfv6qQR5DoZ7Chp@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-20T05:23:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Jeff Layton",
      "primary_email": "jlayton@kernel.org",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/3] sunrpc: cache infrastructure scalability improvements",
          "message_id": "20260220-sunrpc-cache-v1-0-47d04014c245@kernel.org",
          "url": "https://lore.kernel.org/all/20260220-sunrpc-cache-v1-0-47d04014c245@kernel.org/",
          "date": "2026-02-20T12:26:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "I've been working on trying to retrofit a netlink upcall into the sunrpc cache infrastructure. While crawling over that code, I noticed that it relies on both a global spinlock and waitqueue. The first two patches convert those to be per-cache_detail instead.\n\nThe last patch splits up the cache_detail->queue into two lists: one to hold cache_readers and one for cache_requests. This simplifies the code, and the new sequence number that helps the readers track position may help with implementing netlink upcalls.\n\nPlease consider these for v7.1.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Chuck Lever",
              "summary": "From: Chuck Lever <chuck.lever@oracle.com> Applied to nfsd-testing, thanks! [1/3] sunrpc: convert queue_lock from global spinlock to per-cache_detail lock commit: 8da8f32e9a2702259cdf97e2f8f492ef9c79db65 [2/3] sunrpc: convert queue_wait from global to per-cache_detail waitqueue commit: 802261d8b58dd2f41a52a0c92776e0fb45619efe [3/3] sunrpc: split cache_detail...",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "applied"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nOn Fri, 20 Feb 2026 07:26:02 -0500, Jeff Layton wrote:\n> I've been working on trying to retrofit a netlink upcall into the sunrpc\n> cache infrastructure. While crawling over that code, I noticed that it\n> relies on both a global spinlock and waitqueue. The first two patches\n> convert those to be per-cache_detail instead.\n> \n> The last patch splits up the cache_detail->queue into two lists: one to\n> hold cache_readers and one for cache_requests. This simplifies the code,\n> and the new sequence number that helps the readers track position may\n> help with implementing netlink upcalls.\n> \n> [...]\n\nApplied to nfsd-testing, thanks!\n\n[1/3] sunrpc: convert queue_lock from global spinlock to per-cache_detail lock\n      commit: 8da8f32e9a2702259cdf97e2f8f492ef9c79db65\n[2/3] sunrpc: convert queue_wait from global to per-cache_detail waitqueue\n      commit: 802261d8b58dd2f41a52a0c92776e0fb45619efe\n[3/3] sunrpc: split cache_detail queue into request and reader lists\n      commit: 0eb3d9dc71ada02909e4dfe9cb54e703ec717ed4\n\n--\nChuck Lever\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v1 1/2] NFSD: Defer sub-object cleanup in export put callbacks",
          "message_id": "ae5f1ee0c43eda94f86bc60b1b223c86e0f24805.camel@kernel.org",
          "url": "https://lore.kernel.org/all/ae5f1ee0c43eda94f86bc60b1b223c86e0f24805.camel@kernel.org/",
          "date": "2026-02-20T15:50:11Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Olga Kornievskaia",
              "summary": "I can reproduce the problem and verify that the 2 patches applied I no longer see it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Tested-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 10:50â€¯AM Jeff Layton <jlayton@kernel.org> wrote:\n>\n> On Thu, 2026-02-19 at 16:50 -0500, Chuck Lever wrote:\n> > From: Chuck Lever <chuck.lever@oracle.com>\n> >\n> > svc_export_put() calls path_put() and auth_domain_put() immediately\n> > when the last reference drops, before the RCU grace period. RCU\n> > readers in e_show() and c_show() access both ex_path (via\n> > seq_path/d_path) and ex_client->name (via seq_escape) without\n> > holding a reference. If cache_clean removes the entry and drops the\n> > last reference concurrently, the sub-objects are freed while still\n> > in use, producing a NULL pointer dereference in d_path.\n> >\n> > Commit 2530766492ec (\"nfsd: fix UAF when access ex_uuid or\n> > ex_stats\") moved kfree of ex_uuid and ex_stats into the\n> > call_rcu callback, but left path_put() and auth_domain_put() running\n> > before the grace period because both may sleep and call_rcu\n> > callbacks execute in softirq context.\n> >\n> > Replace call_rcu/kfree_rcu with queue_rcu_work(), which defers the\n> > callback until after the RCU grace period and executes it in process\n> > context where sleeping is permitted. This allows path_put() and\n> > auth_domain_put() to be moved into the deferred callback alongside\n> > the other resource releases. Apply the same fix to expkey_put(),\n> > which has the identical pattern with ek_path and ek_client.\n> >\n> > A dedicated workqueue scopes the shutdown drain to only NFSD\n> > export release work items; flushing the shared\n> > system_unbound_wq would stall on unrelated work from other\n> > subsystems. nfsd_export_shutdown() uses rcu_barrier() followed\n> > by flush_workqueue() to ensure all deferred release callbacks\n> > complete before the export caches are destroyed.\n> >\n> > Reported-by: Misbah Anjum N <misanjum@linux.ibm.com>\n> > Closes: https://lore.kernel.org/linux-nfs/dcd371d3a95815a84ba7de52cef447b8@linux.ibm.com/\n> > Fixes: c224edca7af0 (\"nfsd: no need get cache ref when protected by rcu\")\n> > Fixes: 1b10f0b603c0 (\"SUNRPC: no need get cache ref when protected by rcu\")\n> > Signed-off-by: Chuck Lever <chuck.lever@oracle.com>\n\nTested-by: Olga Kornievskaia <okorniev@redhat.com>\n\nI can reproduce the problem and verify that the 2 patches applied I no\nlonger see it.\n\n> > ---\n> >  fs/nfsd/export.c | 63 +++++++++++++++++++++++++++++++++++++++++-------\n> >  fs/nfsd/export.h |  7 ++++--\n> >  fs/nfsd/nfsctl.c |  8 +++++-\n> >  3 files changed, 66 insertions(+), 12 deletions(-)\n> >\n> > diff --git a/fs/nfsd/export.c b/fs/nfsd/export.c\n> > index 04b18f0f402f..53fe66784ed2 100644\n> > --- a/fs/nfsd/export.c\n> > +++ b/fs/nfsd/export.c\n> > @@ -36,19 +36,30 @@\n> >   * second map contains a reference to the entry in the first map.\n> >   */\n> >\n> > +static struct workqueue_struct *nfsd_export_wq;\n> > +\n> >  #define      EXPKEY_HASHBITS         8\n> >  #define      EXPKEY_HASHMAX          (1 << EXPKEY_HASHBITS)\n> >  #define      EXPKEY_HASHMASK         (EXPKEY_HASHMAX -1)\n> >\n> > -static void expkey_put(struct kref *ref)\n> > +static void expkey_release(struct work_struct *work)\n> >  {\n> > -     struct svc_expkey *key = container_of(ref, struct svc_expkey, h.ref);\n> > +     struct svc_expkey *key = container_of(to_rcu_work(work),\n> > +                                           struct svc_expkey, ek_rwork);\n> >\n> >       if (test_bit(CACHE_VALID, &key->h.flags) &&\n> >           !test_bit(CACHE_NEGATIVE, &key->h.flags))\n> >               path_put(&key->ek_path);\n> >       auth_domain_put(key->ek_client);\n> > -     kfree_rcu(key, ek_rcu);\n> > +     kfree(key);\n> > +}\n> > +\n> > +static void expkey_put(struct kref *ref)\n> > +{\n> > +     struct svc_expkey *key = container_of(ref, struct svc_expkey, h.ref);\n> > +\n> > +     INIT_RCU_WORK(&key->ek_rwork, expkey_release);\n> > +     queue_rcu_work(nfsd_export_wq, &key->ek_rwork);\n> >  }\n> >\n> >  static int expkey_upcall(struct cache_detail *cd, struct cache_head *h)\n> > @@ -353,11 +364,13 @@ static void export_stats_destroy(struct export_stats *stats)\n> >                                           EXP_STATS_COUNTERS_NUM);\n> >  }\n> >\n> > -static void svc_export_release(struct rcu_head *rcu_head)\n> > +static void svc_export_release(struct work_struct *work)\n> >  {\n> > -     struct svc_export *exp = container_of(rcu_head, struct svc_export,\n> > -                     ex_rcu);\n> > +     struct svc_export *exp = container_of(to_rcu_work(work),\n> > +                                           struct svc_export, ex_rwork);\n> >\n> > +     path_put(&exp->ex_path);\n> > +     auth_domain_put(exp->ex_client);\n> >       nfsd4_fslocs_free(&exp->ex_fslocs);\n> >       export_stats_destroy(exp->ex_stats);\n> >       kfree(exp->ex_stats);\n> > @@ -369,9 +382,8 @@ static void svc_export_put(struct kref *ref)\n> >  {\n> >       struct svc_export *exp = container_of(ref, struct svc_export, h.ref);\n> >\n> > -     path_put(&exp->ex_path);\n> > -     auth_domain_put(exp->ex_client);\n> > -     call_rcu(&exp->ex_rcu, svc_export_release);\n> > +     INIT_RCU_WORK(&exp->ex_rwork, svc_export_release);\n> > +     queue_rcu_work(nfsd_export_wq, &exp->ex_rwork);\n> >  }\n> >\n> >  static int svc_export_upcall(struct cache_detail *cd, struct cache_head *h)\n> > @@ -1481,6 +1493,36 @@ const struct seq_operations nfs_exports_op = {\n> >       .show   = e_show,\n> >  };\n> >\n> > +/**\n> > + * nfsd_export_wq_init - allocate the export release workqueue\n> > + *\n> > + * Called once at module load. The workqueue runs deferred svc_export and\n> > + * svc_expkey release work scheduled by queue_rcu_work() in the cache put\n> > + * callbacks.\n> > + *\n> > + * Return values:\n> > + *   %0: workqueue allocated\n> > + *   %-ENOMEM: allocation failed\n> > + */\n> > +int nfsd_export_wq_init(void)\n> > +{\n> > +     nfsd_export_wq = alloc_workqueue(\"nfsd_export\", WQ_UNBOUND, 0);\n> > +     if (!nfsd_export_wq)\n> > +             return -ENOMEM;\n> > +     return 0;\n> > +}\n> > +\n> > +/**\n> > + * nfsd_export_wq_shutdown - drain and free the export release workqueue\n> > + *\n> > + * Called once at module unload. Per-namespace teardown in\n> > + * nfsd_export_shutdown() has already drained all deferred work.\n> > + */\n> > +void nfsd_export_wq_shutdown(void)\n> > +{\n> > +     destroy_workqueue(nfsd_export_wq);\n> > +}\n> > +\n> >  /*\n> >   * Initialize the exports module.\n> >   */\n> > @@ -1542,6 +1584,9 @@ nfsd_export_shutdown(struct net *net)\n> >\n> >       cache_unregister_net(nn->svc_expkey_cache, net);\n> >       cache_unregister_net(nn->svc_export_cache, net);\n> > +     /* Drain deferred export and expkey release work. */\n> > +     rcu_barrier();\n> > +     flush_workqueue(nfsd_export_wq);\n> >       cache_destroy_net(nn->svc_expkey_cache, net);\n> >       cache_destroy_net(nn->svc_export_cache, net);\n> >       svcauth_unix_purge(net);\n> > diff --git a/fs/nfsd/export.h b/fs/nfsd/export.h\n> > index d2b09cd76145..b05399374574 100644\n> > --- a/fs/nfsd/export.h\n> > +++ b/fs/nfsd/export.h\n> > @@ -7,6 +7,7 @@\n> >\n> >  #include <linux/sunrpc/cache.h>\n> >  #include <linux/percpu_counter.h>\n> > +#include <linux/workqueue.h>\n> >  #include <uapi/linux/nfsd/export.h>\n> >  #include <linux/nfs4.h>\n> >\n> > @@ -75,7 +76,7 @@ struct svc_export {\n> >       u32                     ex_layout_types;\n> >       struct nfsd4_deviceid_map *ex_devid_map;\n> >       struct cache_detail     *cd;\n> > -     struct rcu_head         ex_rcu;\n> > +     struct rcu_work         ex_rwork;\n> >       unsigned long           ex_xprtsec_modes;\n> >       struct export_stats     *ex_stats;\n> >  };\n> > @@ -92,7 +93,7 @@ struct svc_expkey {\n> >       u32                     ek_fsid[6];\n> >\n> >       struct path             ek_path;\n> > -     struct rcu_head         ek_rcu;\n> > +     struct rcu_work         ek_rwork;\n> >  };\n> >\n> >  #define EX_ISSYNC(exp)               (!((exp)->ex_flags & NFSEXP_ASYNC))\n> > @@ -110,6 +111,8 @@ __be32 check_nfsd_access(struct svc_export *exp, struct svc_rqst *rqstp,\n> >  /*\n> >   * Function declarations\n> >   */\n> > +int                  nfsd_export_wq_init(void);\n> > +void                 nfsd_export_wq_shutdown(void);\n> >  int                  nfsd_export_init(struct net *);\n> >  void                 nfsd_export_shutdown(struct net *);\n> >  void                 nfsd_export_flush(struct net *);\n> > diff --git a/fs/nfsd/nfsctl.c b/fs/nfsd/nfsctl.c\n> > index 664a3275c511..4166f59908f4 100644\n> > --- a/fs/nfsd/nfsctl.c\n> > +++ b/fs/nfsd/nfsctl.c\n> > @@ -2308,9 +2308,12 @@ static int __init init_nfsd(void)\n> >       if (retval)\n> >               goto out_free_pnfs;\n> >       nfsd_lockd_init();      /* lockd->nfsd callbacks */\n> > +     retval = nfsd_export_wq_init();\n> > +     if (retval)\n> > +             goto out_free_lockd;\n> >       retval = register_pernet_subsys(&nfsd_net_ops);\n> >       if (retval < 0)\n> > -             goto out_free_lockd;\n> > +             goto out_free_export_wq;\n> >       retval = register_cld_notifier();\n> >       if (retval)\n> >               goto out_free_subsys;\n> > @@ -2339,6 +2342,8 @@ static int __init init_nfsd(void)\n> >       unregister_cld_notifier();\n> >  out_free_subsys:\n> >       unregister_pernet_subsys(&nfsd_net_ops);\n> > +out_free_export_wq:\n> > +     nfsd_export_wq_shutdown();\n> >  out_free_lockd:\n> >       nfsd_lockd_shutdown();\n> >       nfsd_drc_slab_free();\n> > @@ -2359,6 +2364,7 @@ static void __exit exit_nfsd(void)\n> >       nfsd4_destroy_laundry_wq();\n> >       unregister_cld_notifier();\n> >       unregister_pernet_subsys(&nfsd_net_ops);\n> > +     nfsd_export_wq_shutdown();\n> >       nfsd_drc_slab_free();\n> >       nfsd_lockd_shutdown();\n> >       nfsd4_free_slabs();\n>\n> Looks good.\n>\n> Reviwed-by: Jeff Layton <jlayton@kernel.org>\n>\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Chuck Lever",
              "summary": "Excellent, thank you!",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "\n\nOn Wed, Feb 25, 2026, at 1:29 PM, Olga Kornievskaia wrote:\n> On Fri, Feb 20, 2026 at 10:50â€¯AM Jeff Layton <jlayton@kernel.org> wrote:\n>>\n>> On Thu, 2026-02-19 at 16:50 -0500, Chuck Lever wrote:\n>> > From: Chuck Lever <chuck.lever@oracle.com>\n>> >\n>> > svc_export_put() calls path_put() and auth_domain_put() immediately\n>> > when the last reference drops, before the RCU grace period. RCU\n>> > readers in e_show() and c_show() access both ex_path (via\n>> > seq_path/d_path) and ex_client->name (via seq_escape) without\n>> > holding a reference. If cache_clean removes the entry and drops the\n>> > last reference concurrently, the sub-objects are freed while still\n>> > in use, producing a NULL pointer dereference in d_path.\n>> >\n>> > Commit 2530766492ec (\"nfsd: fix UAF when access ex_uuid or\n>> > ex_stats\") moved kfree of ex_uuid and ex_stats into the\n>> > call_rcu callback, but left path_put() and auth_domain_put() running\n>> > before the grace period because both may sleep and call_rcu\n>> > callbacks execute in softirq context.\n>> >\n>> > Replace call_rcu/kfree_rcu with queue_rcu_work(), which defers the\n>> > callback until after the RCU grace period and executes it in process\n>> > context where sleeping is permitted. This allows path_put() and\n>> > auth_domain_put() to be moved into the deferred callback alongside\n>> > the other resource releases. Apply the same fix to expkey_put(),\n>> > which has the identical pattern with ek_path and ek_client.\n>> >\n>> > A dedicated workqueue scopes the shutdown drain to only NFSD\n>> > export release work items; flushing the shared\n>> > system_unbound_wq would stall on unrelated work from other\n>> > subsystems. nfsd_export_shutdown() uses rcu_barrier() followed\n>> > by flush_workqueue() to ensure all deferred release callbacks\n>> > complete before the export caches are destroyed.\n>> >\n>> > Reported-by: Misbah Anjum N <misanjum@linux.ibm.com>\n>> > Closes: https://lore.kernel.org/linux-nfs/dcd371d3a95815a84ba7de52cef447b8@linux.ibm.com/\n>> > Fixes: c224edca7af0 (\"nfsd: no need get cache ref when protected by rcu\")\n>> > Fixes: 1b10f0b603c0 (\"SUNRPC: no need get cache ref when protected by rcu\")\n>> > Signed-off-by: Chuck Lever <chuck.lever@oracle.com>\n>\n> Tested-by: Olga Kornievskaia <okorniev@redhat.com>\n>\n> I can reproduce the problem and verify that the 2 patches applied I no\n> longer see it.\n\nExcellent, thank you!\n\n\n>> > ---\n>> >  fs/nfsd/export.c | 63 +++++++++++++++++++++++++++++++++++++++++-------\n>> >  fs/nfsd/export.h |  7 ++++--\n>> >  fs/nfsd/nfsctl.c |  8 +++++-\n>> >  3 files changed, 66 insertions(+), 12 deletions(-)\n>> >\n>> > diff --git a/fs/nfsd/export.c b/fs/nfsd/export.c\n>> > index 04b18f0f402f..53fe66784ed2 100644\n>> > --- a/fs/nfsd/export.c\n>> > +++ b/fs/nfsd/export.c\n>> > @@ -36,19 +36,30 @@\n>> >   * second map contains a reference to the entry in the first map.\n>> >   */\n>> >\n>> > +static struct workqueue_struct *nfsd_export_wq;\n>> > +\n>> >  #define      EXPKEY_HASHBITS         8\n>> >  #define      EXPKEY_HASHMAX          (1 << EXPKEY_HASHBITS)\n>> >  #define      EXPKEY_HASHMASK         (EXPKEY_HASHMAX -1)\n>> >\n>> > -static void expkey_put(struct kref *ref)\n>> > +static void expkey_release(struct work_struct *work)\n>> >  {\n>> > -     struct svc_expkey *key = container_of(ref, struct svc_expkey, h.ref);\n>> > +     struct svc_expkey *key = container_of(to_rcu_work(work),\n>> > +                                           struct svc_expkey, ek_rwork);\n>> >\n>> >       if (test_bit(CACHE_VALID, &key->h.flags) &&\n>> >           !test_bit(CACHE_NEGATIVE, &key->h.flags))\n>> >               path_put(&key->ek_path);\n>> >       auth_domain_put(key->ek_client);\n>> > -     kfree_rcu(key, ek_rcu);\n>> > +     kfree(key);\n>> > +}\n>> > +\n>> > +static void expkey_put(struct kref *ref)\n>> > +{\n>> > +     struct svc_expkey *key = container_of(ref, struct svc_expkey, h.ref);\n>> > +\n>> > +     INIT_RCU_WORK(&key->ek_rwork, expkey_release);\n>> > +     queue_rcu_work(nfsd_export_wq, &key->ek_rwork);\n>> >  }\n>> >\n>> >  static int expkey_upcall(struct cache_detail *cd, struct cache_head *h)\n>> > @@ -353,11 +364,13 @@ static void export_stats_destroy(struct export_stats *stats)\n>> >                                           EXP_STATS_COUNTERS_NUM);\n>> >  }\n>> >\n>> > -static void svc_export_release(struct rcu_head *rcu_head)\n>> > +static void svc_export_release(struct work_struct *work)\n>> >  {\n>> > -     struct svc_export *exp = container_of(rcu_head, struct svc_export,\n>> > -                     ex_rcu);\n>> > +     struct svc_export *exp = container_of(to_rcu_work(work),\n>> > +                                           struct svc_export, ex_rwork);\n>> >\n>> > +     path_put(&exp->ex_path);\n>> > +     auth_domain_put(exp->ex_client);\n>> >       nfsd4_fslocs_free(&exp->ex_fslocs);\n>> >       export_stats_destroy(exp->ex_stats);\n>> >       kfree(exp->ex_stats);\n>> > @@ -369,9 +382,8 @@ static void svc_export_put(struct kref *ref)\n>> >  {\n>> >       struct svc_export *exp = container_of(ref, struct svc_export, h.ref);\n>> >\n>> > -     path_put(&exp->ex_path);\n>> > -     auth_domain_put(exp->ex_client);\n>> > -     call_rcu(&exp->ex_rcu, svc_export_release);\n>> > +     INIT_RCU_WORK(&exp->ex_rwork, svc_export_release);\n>> > +     queue_rcu_work(nfsd_export_wq, &exp->ex_rwork);\n>> >  }\n>> >\n>> >  static int svc_export_upcall(struct cache_detail *cd, struct cache_head *h)\n>> > @@ -1481,6 +1493,36 @@ const struct seq_operations nfs_exports_op = {\n>> >       .show   = e_show,\n>> >  };\n>> >\n>> > +/**\n>> > + * nfsd_export_wq_init - allocate the export release workqueue\n>> > + *\n>> > + * Called once at module load. The workqueue runs deferred svc_export and\n>> > + * svc_expkey release work scheduled by queue_rcu_work() in the cache put\n>> > + * callbacks.\n>> > + *\n>> > + * Return values:\n>> > + *   %0: workqueue allocated\n>> > + *   %-ENOMEM: allocation failed\n>> > + */\n>> > +int nfsd_export_wq_init(void)\n>> > +{\n>> > +     nfsd_export_wq = alloc_workqueue(\"nfsd_export\", WQ_UNBOUND, 0);\n>> > +     if (!nfsd_export_wq)\n>> > +             return -ENOMEM;\n>> > +     return 0;\n>> > +}\n>> > +\n>> > +/**\n>> > + * nfsd_export_wq_shutdown - drain and free the export release workqueue\n>> > + *\n>> > + * Called once at module unload. Per-namespace teardown in\n>> > + * nfsd_export_shutdown() has already drained all deferred work.\n>> > + */\n>> > +void nfsd_export_wq_shutdown(void)\n>> > +{\n>> > +     destroy_workqueue(nfsd_export_wq);\n>> > +}\n>> > +\n>> >  /*\n>> >   * Initialize the exports module.\n>> >   */\n>> > @@ -1542,6 +1584,9 @@ nfsd_export_shutdown(struct net *net)\n>> >\n>> >       cache_unregister_net(nn->svc_expkey_cache, net);\n>> >       cache_unregister_net(nn->svc_export_cache, net);\n>> > +     /* Drain deferred export and expkey release work. */\n>> > +     rcu_barrier();\n>> > +     flush_workqueue(nfsd_export_wq);\n>> >       cache_destroy_net(nn->svc_expkey_cache, net);\n>> >       cache_destroy_net(nn->svc_export_cache, net);\n>> >       svcauth_unix_purge(net);\n>> > diff --git a/fs/nfsd/export.h b/fs/nfsd/export.h\n>> > index d2b09cd76145..b05399374574 100644\n>> > --- a/fs/nfsd/export.h\n>> > +++ b/fs/nfsd/export.h\n>> > @@ -7,6 +7,7 @@\n>> >\n>> >  #include <linux/sunrpc/cache.h>\n>> >  #include <linux/percpu_counter.h>\n>> > +#include <linux/workqueue.h>\n>> >  #include <uapi/linux/nfsd/export.h>\n>> >  #include <linux/nfs4.h>\n>> >\n>> > @@ -75,7 +76,7 @@ struct svc_export {\n>> >       u32                     ex_layout_types;\n>> >       struct nfsd4_deviceid_map *ex_devid_map;\n>> >       struct cache_detail     *cd;\n>> > -     struct rcu_head         ex_rcu;\n>> > +     struct rcu_work         ex_rwork;\n>> >       unsigned long           ex_xprtsec_modes;\n>> >       struct export_stats     *ex_stats;\n>> >  };\n>> > @@ -92,7 +93,7 @@ struct svc_expkey {\n>> >       u32                     ek_fsid[6];\n>> >\n>> >       struct path             ek_path;\n>> > -     struct rcu_head         ek_rcu;\n>> > +     struct rcu_work         ek_rwork;\n>> >  };\n>> >\n>> >  #define EX_ISSYNC(exp)               (!((exp)->ex_flags & NFSEXP_ASYNC))\n>> > @@ -110,6 +111,8 @@ __be32 check_nfsd_access(struct svc_export *exp, struct svc_rqst *rqstp,\n>> >  /*\n>> >   * Function declarations\n>> >   */\n>> > +int                  nfsd_export_wq_init(void);\n>> > +void                 nfsd_export_wq_shutdown(void);\n>> >  int                  nfsd_export_init(struct net *);\n>> >  void                 nfsd_export_shutdown(struct net *);\n>> >  void                 nfsd_export_flush(struct net *);\n>> > diff --git a/fs/nfsd/nfsctl.c b/fs/nfsd/nfsctl.c\n>> > index 664a3275c511..4166f59908f4 100644\n>> > --- a/fs/nfsd/nfsctl.c\n>> > +++ b/fs/nfsd/nfsctl.c\n>> > @@ -2308,9 +2308,12 @@ static int __init init_nfsd(void)\n>> >       if (retval)\n>> >               goto out_free_pnfs;\n>> >       nfsd_lockd_init();      /* lockd->nfsd callbacks */\n>> > +     retval = nfsd_export_wq_init();\n>> > +     if (retval)\n>> > +             goto out_free_lockd;\n>> >       retval = register_pernet_subsys(&nfsd_net_ops);\n>> >       if (retval < 0)\n>> > -             goto out_free_lockd;\n>> > +             goto out_free_export_wq;\n>> >       retval = register_cld_notifier();\n>> >       if (retval)\n>> >               goto out_free_subsys;\n>> > @@ -2339,6 +2342,8 @@ static int __init init_nfsd(void)\n>> >       unregister_cld_notifier();\n>> >  out_free_subsys:\n>> >       unregister_pernet_subsys(&nfsd_net_ops);\n>> > +out_free_export_wq:\n>> > +     nfsd_export_wq_shutdown();\n>> >  out_free_lockd:\n>> >       nfsd_lockd_shutdown();\n>> >       nfsd_drc_slab_free();\n>> > @@ -2359,6 +2364,7 @@ static void __exit exit_nfsd(void)\n>> >       nfsd4_destroy_laundry_wq();\n>> >       unregister_cld_notifier();\n>> >       unregister_pernet_subsys(&nfsd_net_ops);\n>> > +     nfsd_export_wq_shutdown();\n>> >       nfsd_drc_slab_free();\n>> >       nfsd_lockd_shutdown();\n>> >       nfsd4_free_slabs();\n>>\n>> Looks good.\n>>\n>> Reviwed-by: Jeff Layton <jlayton@kernel.org>\n>>\n\n-- \nChuck Lever\n\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v1 2/2] NFSD: Hold net reference for the lifetime of /proc/fs/nfs/exports fd",
          "message_id": "2fa166bf4183cbc049350dc892eeb6656d9ed081.camel@kernel.org",
          "url": "https://lore.kernel.org/all/2fa166bf4183cbc049350dc892eeb6656d9ed081.camel@kernel.org/",
          "date": "2026-02-20T15:52:23Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Olga Kornievskaia",
              "summary": "Gave Tested-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Tested-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 10:53â€¯AM Jeff Layton <jlayton@kernel.org> wrote:\n>\n> On Thu, 2026-02-19 at 16:50 -0500, Chuck Lever wrote:\n> > From: Chuck Lever <chuck.lever@oracle.com>\n> >\n> > The /proc/fs/nfs/exports proc entry is created at module init\n> > and persists for the module's lifetime. exports_proc_open()\n> > captures the caller's current network namespace and stores\n> > its svc_export_cache in seq->private, but takes no reference\n> > on the namespace. If the namespace is subsequently torn down\n> > (e.g. container destruction after the opener does setns() to a\n> > different namespace), nfsd_net_exit() calls nfsd_export_shutdown()\n> > which frees the cache. Subsequent reads on the still-open fd\n> > dereference the freed cache_detail, walking a freed hash table.\n> >\n> > Hold a reference on the struct net for the lifetime of the open\n> > file descriptor. This prevents nfsd_net_exit() from running --\n> > and thus prevents nfsd_export_shutdown() from freeing the cache\n> > -- while any exports fd is open. cache_detail already stores\n> > its net pointer (cd->net, set by cache_create_net()), so\n> > exports_release() can retrieve it without additional per-file\n> > storage.\n> >\n> > Reported-by: Misbah Anjum N <misanjum@linux.ibm.com>\n> > Closes: https://lore.kernel.org/linux-nfs/dcd371d3a95815a84ba7de52cef447b8@linux.ibm.com/\n> > Fixes: 96d851c4d28d (\"nfsd: use proper net while reading \"exports\" file\")\n> > Signed-off-by: Chuck Lever <chuck.lever@oracle.com>\n\nTested-by: Olga Kornievskaia <okorniev@redhat.com>\n\n> > ---\n> >  fs/nfsd/nfsctl.c | 14 ++++++++++++--\n> >  1 file changed, 12 insertions(+), 2 deletions(-)\n> >\n> > diff --git a/fs/nfsd/nfsctl.c b/fs/nfsd/nfsctl.c\n> > index 4166f59908f4..3d5a676e1d14 100644\n> > --- a/fs/nfsd/nfsctl.c\n> > +++ b/fs/nfsd/nfsctl.c\n> > @@ -149,9 +149,19 @@ static int exports_net_open(struct net *net, struct file *file)\n> >\n> >       seq = file->private_data;\n> >       seq->private = nn->svc_export_cache;\n> > +     get_net(net);\n> >       return 0;\n> >  }\n> >\n> > +static int exports_release(struct inode *inode, struct file *file)\n> > +{\n> > +     struct seq_file *seq = file->private_data;\n> > +     struct cache_detail *cd = seq->private;\n> > +\n> > +     put_net(cd->net);\n> > +     return seq_release(inode, file);\n> > +}\n> > +\n> >  static int exports_nfsd_open(struct inode *inode, struct file *file)\n> >  {\n> >       return exports_net_open(inode->i_sb->s_fs_info, file);\n> > @@ -161,7 +171,7 @@ static const struct file_operations exports_nfsd_operations = {\n> >       .open           = exports_nfsd_open,\n> >       .read           = seq_read,\n> >       .llseek         = seq_lseek,\n> > -     .release        = seq_release,\n> > +     .release        = exports_release,\n> >  };\n> >\n> >  static int export_features_show(struct seq_file *m, void *v)\n> > @@ -1376,7 +1386,7 @@ static const struct proc_ops exports_proc_ops = {\n> >       .proc_open      = exports_proc_open,\n> >       .proc_read      = seq_read,\n> >       .proc_lseek     = seq_lseek,\n> > -     .proc_release   = seq_release,\n> > +     .proc_release   = exports_release,\n> >  };\n> >\n> >  static int create_proc_exports_entry(void)\n>\n> Reviewed-by: Jeff Layton <jlayton@kernel.org>\n>\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joanne Koong",
      "primary_email": "joannelkoong@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] io_uring/rsrc: clean up buffer cloning arg validation (for 6.18-stable tree)",
          "message_id": "CAJnrk1YA9hk5Mv0BXFe+TcWLXsNLpWtcA-gy+k03zDt4f0z7zg@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1YA9hk5Mv0BXFe+TcWLXsNLpWtcA-gy+k03zDt4f0z7zg@mail.gmail.com/",
          "date": "2026-02-20T18:20:08Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Commit id upstream: b8201b50e403815f941d1c6581a27fdbfe7d0fd4 (\"io_uring/rsrc: clean up buffer cloning arg validation\") Link to the patch: https://lore.kernel.org/io-uring/20251204215116.2642044-1-joannelkoong@gmail.com/#t Kernel version to apply it to: 6.18-stable tree\n\nHi stable@,",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Jens Axboe",
              "summary": "FWIW, this is approved on my end. CC Greg.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/20/26 11:19 AM, Joanne Koong wrote:\n> Commit id upstream: b8201b50e403815f941d1c6581a27fdbfe7d0fd4\n> (\"io_uring/rsrc: clean up buffer cloning arg validation\")\n> Link to the patch:\n> https://lore.kernel.org/io-uring/20251204215116.2642044-1-joannelkoong@gmail.com/#t\n> Kernel version to apply it to: 6.18-stable tree\n> \n> Hi stable@,\n> \n> Chris Mason recently detected that this patch is a required dependency\n> for commit 5b804b8f1e0d (\"io_uring/rsrc: fix lost entries after cloned\n> range\") in the 6.18-stable tree [1]. Without this patch, the changes\n> in commit 5b804b8f1e0d use an incorrect value for nbufs when it\n> assigns \"i = nbufs\" [2].\n> \n> Could you please apply this patch to the 6.18-stable tree as a\n> dependency fix needed for commit 5b804b8f1e0d?\t\n> \n> Thanks,\n> Joanne\n> \n> [1] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=linux-6.18.y&id=5b804b8f1e0d66413774d43f7a4b78bba0ca6272\n> [2] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/rsrc.c?h=linux-6.18.y#n1252.\n\nFWIW, this is approved on my end. CC Greg.\n\n\n-- \nJens Axboe\n\n",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Greg Kroah-Hartman",
              "summary": "Now queued up, thanks.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "queued"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Sun, Feb 22, 2026 at 08:33:24AM -0700, Jens Axboe wrote:\n> On 2/20/26 11:19 AM, Joanne Koong wrote:\n> > Commit id upstream: b8201b50e403815f941d1c6581a27fdbfe7d0fd4\n> > (\"io_uring/rsrc: clean up buffer cloning arg validation\")\n> > Link to the patch:\n> > https://lore.kernel.org/io-uring/20251204215116.2642044-1-joannelkoong@gmail.com/#t\n> > Kernel version to apply it to: 6.18-stable tree\n> > \n> > Hi stable@,\n> > \n> > Chris Mason recently detected that this patch is a required dependency\n> > for commit 5b804b8f1e0d (\"io_uring/rsrc: fix lost entries after cloned\n> > range\") in the 6.18-stable tree [1]. Without this patch, the changes\n> > in commit 5b804b8f1e0d use an incorrect value for nbufs when it\n> > assigns \"i = nbufs\" [2].\n> > \n> > Could you please apply this patch to the 6.18-stable tree as a\n> > dependency fix needed for commit 5b804b8f1e0d?\t\n> > \n> > Thanks,\n> > Joanne\n> > \n> > [1] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=linux-6.18.y&id=5b804b8f1e0d66413774d43f7a4b78bba0ca6272\n> > [2] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/rsrc.c?h=linux-6.18.y#n1252.\n> \n> FWIW, this is approved on my end. CC Greg.\n\nNow queued up, thanks.\n\ngreg k-h\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v1 0/1] iomap: don't mark folio uptodate if read IO has bytes pending",
          "message_id": "20260219003911.344478-1-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260219003911.344478-1-joannelkoong@gmail.com/",
          "date": "2026-02-19T00:41:04Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-19",
          "patch_summary": "This is a fix for this scenario:",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v1 11/11] io_uring/cmd: set selected buffer index in __io_uring_cmd_done()",
          "message_id": "20260210002852.1394504-12-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260210002852.1394504-12-joannelkoong@gmail.com/",
          "date": "2026-02-10T00:31:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-10",
          "patch_summary": "When uring_cmd operations select a buffer, the completion queue entry should indicate which buffer was selected.\n\nSet IORING_CQE_F_BUFFER on the completed entry and encode the buffer index if a buffer was selected.\n\nThis will be needed for fuse, which needs to relay to userspace which selected buffer contains the data.",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v1 1/1] iomap: don't mark folio uptodate if read IO has bytes pending",
          "message_id": "CAJnrk1aJJqafDkxMypUym6iFQ-HkaSxneOe6Sc746AwrmrDK4Q@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1aJJqafDkxMypUym6iFQ-HkaSxneOe6Sc746AwrmrDK4Q@mail.gmail.com/",
          "date": "2026-02-20T22:13:46Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [syzbot] [iomap?] WARNING in ifs_free",
          "message_id": "CAJnrk1bk7jN8SfHny9nVWZZS6tP8bnQbMZHTCuFma6-YuMugAg@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1bk7jN8SfHny9nVWZZS6tP8bnQbMZHTCuFma6-YuMugAg@mail.gmail.com/",
          "date": "2026-02-20T00:47:10Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Christoph Hellwig",
              "summary": "I'd honestly rather have ntfs3 come along and explain what their doing.  They've copy and pasted large chunks of the buffered read code for now reason, which already annoys me and I'd rather not paper over random misuses.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Thu, Feb 19, 2026 at 04:46:58PM -0800, Joanne Koong wrote:\n> The folio is uptodate but the ifs uptodate bitmap is not reflected as\n> fully uptodate. I think this is because ntfs3 handles writes for\n> compressed files through its own interface that doesn't go through\n> iomap where it calls folio_mark_uptodate() but the ifs bitmap doesn't\n> get updated. fuse-blk servers that operate in writethrough mode run\n> into something like this as well [2].\n> \n> This doesn't lead to any data corruption issues. Should we get rid of\n> the  WARN_ON_ONCE(ifs_is_fully_uptodate(folio, ifs) !=\n> folio_test_uptodate(folio))? The alternative is to make a modified\n> version of the functionality in \"iomap_set_range_uptodate()\" a public\n> api callable by subsystems.\n\nI'd honestly rather have ntfs3 come along and explain what their\ndoing.  They've copy and pasted large chunks of the buffered\nread code for now reason, which already annoys me and I'd rather\nnot paper over random misuses.\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Johannes Weiner",
      "primary_email": "hannes@cmpxchg.org",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/2] mm: vmalloc: streamline vmalloc memory accounting",
          "message_id": "20260220191035.3703800-1-hannes@cmpxchg.org",
          "url": "https://lore.kernel.org/all/20260220191035.3703800-1-hannes@cmpxchg.org/",
          "date": "2026-02-20T19:10:37Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Use a vmstat counter instead of a custom, open-coded atomic. This has the added benefit of making the data available per-node, and prepares for cleaning up the memcg accounting as well.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "mod_node_page_state() takes 'struct pglist_data *pgdat', you need to use page_pgdat(page) as first param. Same here. With above fixes, you can add:",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n[...]\n>  static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)\n>  {\n>  \tstruct rb_node *n = root->rb_node;\n> @@ -3463,11 +3457,11 @@ void vfree(const void *addr)\n>  \t\t * High-order allocs for huge vmallocs are split, so\n>  \t\t * can be freed as an array of order-0 allocations\n>  \t\t */\n> +\t\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> +\t\t\tdec_node_page_state(page, NR_VMALLOC);\n>  \t\t__free_page(page);\n>  \t\tcond_resched();\n>  \t}\n> -\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> -\t\tatomic_long_sub(vm->nr_pages, &nr_vmalloc_pages);\n>  \tkvfree(vm->pages);\n>  \tkfree(vm);\n>  }\n> @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\t\tcontinue;\n>  \t\t}\n>  \n> +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n\nmod_node_page_state() takes 'struct pglist_data *pgdat', you need to use\npage_pgdat(page) as first param.\n\n> +\n>  \t\tsplit_page(page, large_order);\n>  \t\tfor (i = 0; i < (1U << large_order); i++)\n>  \t\t\tpages[nr_allocated + i] = page + i;\n> @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \tif (!order) {\n>  \t\twhile (nr_allocated < nr_pages) {\n>  \t\t\tunsigned int nr, nr_pages_request;\n> +\t\t\tint i;\n>  \n>  \t\t\t/*\n>  \t\t\t * A maximum allowed request is hard-coded and is 100\n> @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\t\t\t\t\t\tnr_pages_request,\n>  \t\t\t\t\t\t\tpages + nr_allocated);\n>  \n> +\t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n> +\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n> +\n>  \t\t\tnr_allocated += nr;\n>  \n>  \t\t\t/*\n> @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\tif (unlikely(!page))\n>  \t\t\tbreak;\n>  \n> +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n\nSame here.\n\nWith above fixes, you can add:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Uladzislau Rezki",
              "summary": "Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()? Or mod_node_page_state in first place should be invoked on high-order page before split(to avoid of looping over small pages afterword)? I mean it would be good to place to the one solid place. If it is possible of course.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> Use a vmstat counter instead of a custom, open-coded atomic. This has\n> the added benefit of making the data available per-node, and prepares\n> for cleaning up the memcg accounting as well.\n> \n> Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>\n> ---\n>  fs/proc/meminfo.c       |  3 ++-\n>  include/linux/mmzone.h  |  1 +\n>  include/linux/vmalloc.h |  3 ---\n>  mm/vmalloc.c            | 19 ++++++++++---------\n>  mm/vmstat.c             |  1 +\n>  5 files changed, 14 insertions(+), 13 deletions(-)\n> \n> diff --git a/fs/proc/meminfo.c b/fs/proc/meminfo.c\n> index a458f1e112fd..549793f44726 100644\n> --- a/fs/proc/meminfo.c\n> +++ b/fs/proc/meminfo.c\n> @@ -126,7 +126,8 @@ static int meminfo_proc_show(struct seq_file *m, void *v)\n>  \tshow_val_kb(m, \"Committed_AS:   \", committed);\n>  \tseq_printf(m, \"VmallocTotal:   %8lu kB\\n\",\n>  \t\t   (unsigned long)VMALLOC_TOTAL >> 10);\n> -\tshow_val_kb(m, \"VmallocUsed:    \", vmalloc_nr_pages());\n> +\tshow_val_kb(m, \"VmallocUsed:    \",\n> +\t\t    global_node_page_state(NR_VMALLOC));\n>  \tshow_val_kb(m, \"VmallocChunk:   \", 0ul);\n>  \tshow_val_kb(m, \"Percpu:         \", pcpu_nr_pages());\n>  \n> diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> index fc5d6c88d2f0..64df797d45c6 100644\n> --- a/include/linux/mmzone.h\n> +++ b/include/linux/mmzone.h\n> @@ -220,6 +220,7 @@ enum node_stat_item {\n>  \tNR_KERNEL_MISC_RECLAIMABLE,\t/* reclaimable non-slab kernel pages */\n>  \tNR_FOLL_PIN_ACQUIRED,\t/* via: pin_user_page(), gup flag: FOLL_PIN */\n>  \tNR_FOLL_PIN_RELEASED,\t/* pages returned via unpin_user_page() */\n> +\tNR_VMALLOC,\n>  \tNR_KERNEL_STACK_KB,\t/* measured in KiB */\n>  #if IS_ENABLED(CONFIG_SHADOW_CALL_STACK)\n>  \tNR_KERNEL_SCS_KB,\t/* measured in KiB */\n> diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h\n> index e8e94f90d686..3b02c0c6b371 100644\n> --- a/include/linux/vmalloc.h\n> +++ b/include/linux/vmalloc.h\n> @@ -286,8 +286,6 @@ int unregister_vmap_purge_notifier(struct notifier_block *nb);\n>  #ifdef CONFIG_MMU\n>  #define VMALLOC_TOTAL (VMALLOC_END - VMALLOC_START)\n>  \n> -unsigned long vmalloc_nr_pages(void);\n> -\n>  int vm_area_map_pages(struct vm_struct *area, unsigned long start,\n>  \t\t      unsigned long end, struct page **pages);\n>  void vm_area_unmap_pages(struct vm_struct *area, unsigned long start,\n> @@ -304,7 +302,6 @@ static inline void set_vm_flush_reset_perms(void *addr)\n>  #else  /* !CONFIG_MMU */\n>  #define VMALLOC_TOTAL 0UL\n>  \n> -static inline unsigned long vmalloc_nr_pages(void) { return 0; }\n>  static inline void set_vm_flush_reset_perms(void *addr) {}\n>  #endif /* CONFIG_MMU */\n>  \n> diff --git a/mm/vmalloc.c b/mm/vmalloc.c\n> index e286c2d2068c..a49a46de9c4f 100644\n> --- a/mm/vmalloc.c\n> +++ b/mm/vmalloc.c\n> @@ -1063,14 +1063,8 @@ static BLOCKING_NOTIFIER_HEAD(vmap_notify_list);\n>  static void drain_vmap_area_work(struct work_struct *work);\n>  static DECLARE_WORK(drain_vmap_work, drain_vmap_area_work);\n>  \n> -static __cacheline_aligned_in_smp atomic_long_t nr_vmalloc_pages;\n>  static __cacheline_aligned_in_smp atomic_long_t vmap_lazy_nr;\n>  \n> -unsigned long vmalloc_nr_pages(void)\n> -{\n> -\treturn atomic_long_read(&nr_vmalloc_pages);\n> -}\n> -\n>  static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)\n>  {\n>  \tstruct rb_node *n = root->rb_node;\n> @@ -3463,11 +3457,11 @@ void vfree(const void *addr)\n>  \t\t * High-order allocs for huge vmallocs are split, so\n>  \t\t * can be freed as an array of order-0 allocations\n>  \t\t */\n> +\t\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> +\t\t\tdec_node_page_state(page, NR_VMALLOC);\n>  \t\t__free_page(page);\n>  \t\tcond_resched();\n>  \t}\n> -\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> -\t\tatomic_long_sub(vm->nr_pages, &nr_vmalloc_pages);\n>  \tkvfree(vm->pages);\n>  \tkfree(vm);\n>  }\n> @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\t\tcontinue;\n>  \t\t}\n>  \n> +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> +\n>  \t\tsplit_page(page, large_order);\n>  \t\tfor (i = 0; i < (1U << large_order); i++)\n>  \t\t\tpages[nr_allocated + i] = page + i;\n> @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \tif (!order) {\n>  \t\twhile (nr_allocated < nr_pages) {\n>  \t\t\tunsigned int nr, nr_pages_request;\n> +\t\t\tint i;\n>  \n>  \t\t\t/*\n>  \t\t\t * A maximum allowed request is hard-coded and is 100\n> @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\t\t\t\t\t\tnr_pages_request,\n>  \t\t\t\t\t\t\tpages + nr_allocated);\n>  \n> +\t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n> +\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n> +\n>  \t\t\tnr_allocated += nr;\n>  \n>  \t\t\t/*\n> @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\tif (unlikely(!page))\n>  \t\t\tbreak;\n>  \n> +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n> +\n>  \t\t/*\nCan we move *_node_page_stat() to the end of the vm_area_alloc_pages()?\n\nOr mod_node_page_state in first place should be invoked on high-order\npage before split(to avoid of looping over small pages afterword)?\n\nI mean it would be good to place to the one solid place. If it is possible\nof course.\n\n--\nUladzislau Rezk\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Johannes Weiner (author)",
              "summary": "Good catch, my apologies. Serves me right for not compiling incrementally.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 02:09:28PM -0800, Shakeel Butt wrote:\n> On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> [...]\n> >  static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)\n> >  {\n> >  \tstruct rb_node *n = root->rb_node;\n> > @@ -3463,11 +3457,11 @@ void vfree(const void *addr)\n> >  \t\t * High-order allocs for huge vmallocs are split, so\n> >  \t\t * can be freed as an array of order-0 allocations\n> >  \t\t */\n> > +\t\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> > +\t\t\tdec_node_page_state(page, NR_VMALLOC);\n> >  \t\t__free_page(page);\n> >  \t\tcond_resched();\n> >  \t}\n> > -\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> > -\t\tatomic_long_sub(vm->nr_pages, &nr_vmalloc_pages);\n> >  \tkvfree(vm->pages);\n> >  \tkfree(vm);\n> >  }\n> > @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \t\t\tcontinue;\n> >  \t\t}\n> >  \n> > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> \n> mod_node_page_state() takes 'struct pglist_data *pgdat', you need to use\n> page_pgdat(page) as first param.\n\nGood catch, my apologies. Serves me right for not compiling\nincrementally.\n\n> With above fixes, you can add:\n> \n> Acked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\nThanks! I'll send out v2.\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Johannes Weiner (author)",
              "summary": "Note that the top one in the fast path IS called before the split. We're accounting in the same step size as the page allocator can give us. In the fallback paths (bulk allocator, and one-by-one loop), the issue is that the individual pages could be coming from different nodes, so they need to bump different counters. One possible solution would be to remember the last node and accumulate until it differs, then flush: fallback_loop() { page = alloc_pages(); nid = page_to_nid(page); if (nid != last_nid) { if (node_count) { mod_node_page_state(...); node_count = 0; } last_nid = nid; } } if (node_count) mod_node_page_state(...); But it IS the slow path, and these are fairly cheap per-cpu counters. Especially compared to the cost of calling into the allocator.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 04:30:32PM +0100, Uladzislau Rezki wrote:\n> On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> > @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \t\t\tcontinue;\n> >  \t\t}\n> >  \n> > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> > +\n> >  \t\tsplit_page(page, large_order);\n> >  \t\tfor (i = 0; i < (1U << large_order); i++)\n> >  \t\t\tpages[nr_allocated + i] = page + i;\n> > @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \tif (!order) {\n> >  \t\twhile (nr_allocated < nr_pages) {\n> >  \t\t\tunsigned int nr, nr_pages_request;\n> > +\t\t\tint i;\n> >  \n> >  \t\t\t/*\n> >  \t\t\t * A maximum allowed request is hard-coded and is 100\n> > @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \t\t\t\t\t\t\tnr_pages_request,\n> >  \t\t\t\t\t\t\tpages + nr_allocated);\n> >  \n> > +\t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n> > +\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n> > +\n> >  \t\t\tnr_allocated += nr;\n> >  \n> >  \t\t\t/*\n> > @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \t\tif (unlikely(!page))\n> >  \t\t\tbreak;\n> >  \n> > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n> > +\n> >  \t\t/*\n> Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()?\n> \n> Or mod_node_page_state in first place should be invoked on high-order\n> page before split(to avoid of looping over small pages afterword)?\n> \n> I mean it would be good to place to the one solid place. If it is possible\n> of course.\n\nNote that the top one in the fast path IS called before the\nsplit. We're accounting in the same step size as the page allocator\ncan give us.\n\nIn the fallback paths (bulk allocator, and one-by-one loop), the issue\nis that the individual pages could be coming from different nodes, so\nthey need to bump different counters. One possible solution would be\nto remember the last node and accumulate until it differs, then flush:\n\nfallback_loop() {\n\tpage = alloc_pages();\n\tnid = page_to_nid(page);\n\tif (nid != last_nid) {\n\t\tif (node_count) {\n\t\t\tmod_node_page_state(...);\n\t\t\tnode_count = 0;\n\t\t}\n\t\tlast_nid = nid;\n\t}\n}\n\nif (node_count)\n\tmod_node_page_state(...);\n\nBut it IS the slow path, and these are fairly cheap per-cpu\ncounters. Especially compared to the cost of calling into the\nallocator. So I'm not sure it's worth it... What do you think?\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Uladzislau Rezki",
              "summary": "I see. I agree it is easier to keep original solution. I see that Andrew took it, but just in case:",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Mon, Feb 23, 2026 at 03:19:20PM -0500, Johannes Weiner wrote:\n> On Mon, Feb 23, 2026 at 04:30:32PM +0100, Uladzislau Rezki wrote:\n> > On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> > > @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \t\t\tcontinue;\n> > >  \t\t}\n> > >  \n> > > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> > > +\n> > >  \t\tsplit_page(page, large_order);\n> > >  \t\tfor (i = 0; i < (1U << large_order); i++)\n> > >  \t\t\tpages[nr_allocated + i] = page + i;\n> > > @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \tif (!order) {\n> > >  \t\twhile (nr_allocated < nr_pages) {\n> > >  \t\t\tunsigned int nr, nr_pages_request;\n> > > +\t\t\tint i;\n> > >  \n> > >  \t\t\t/*\n> > >  \t\t\t * A maximum allowed request is hard-coded and is 100\n> > > @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \t\t\t\t\t\t\tnr_pages_request,\n> > >  \t\t\t\t\t\t\tpages + nr_allocated);\n> > >  \n> > > +\t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n> > > +\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n> > > +\n> > >  \t\t\tnr_allocated += nr;\n> > >  \n> > >  \t\t\t/*\n> > > @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \t\tif (unlikely(!page))\n> > >  \t\t\tbreak;\n> > >  \n> > > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n> > > +\n> > >  \t\t/*\n> > Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()?\n> > \n> > Or mod_node_page_state in first place should be invoked on high-order\n> > page before split(to avoid of looping over small pages afterword)?\n> > \n> > I mean it would be good to place to the one solid place. If it is possible\n> > of course.\n> \n> Note that the top one in the fast path IS called before the\n> split. We're accounting in the same step size as the page allocator\n> can give us.\n> \n> In the fallback paths (bulk allocator, and one-by-one loop), the issue\n> is that the individual pages could be coming from different nodes, so\n> they need to bump different counters. One possible solution would be\n> to remember the last node and accumulate until it differs, then flush:\n> \n> fallback_loop() {\n> \tpage = alloc_pages();\n> \tnid = page_to_nid(page);\n> \tif (nid != last_nid) {\n> \t\tif (node_count) {\n> \t\t\tmod_node_page_state(...);\n> \t\t\tnode_count = 0;\n> \t\t}\n> \t\tlast_nid = nid;\n> \t}\n> }\n> \n> if (node_count)\n> \tmod_node_page_state(...);\n> \n> But it IS the slow path, and these are fairly cheap per-cpu\n> counters. Especially compared to the cost of calling into the\n> allocator. So I'm not sure it's worth it... What do you think?\n>\nI see. I agree it is easier to keep original solution. I see that\nAndrew took it, but just in case:\n\nReviewed-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\n\n--\nUladzislau Rezki\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] Improving MGLRU",
          "message_id": "aZim2hT0nNjcRYVG@cmpxchg.org",
          "url": "https://lore.kernel.org/all/aZim2hT0nNjcRYVG@cmpxchg.org/",
          "date": "2026-02-20T18:24:35Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Kairui Song",
              "summary": "A bit more than that. When there is no swap, MGLRU still performs worse in some workloads like MongoDB. From what I've noticed that's because the PID protection is a bit too passive, and there is a force protection in sort_folio which sometimes seems too aggressive. Active/Inactive LRU will just move a folio to head if it's accessed twice while in RAM, but MGLRU won't do so, as result hotter file folios are evicted equally as the colder one until the PID gets triggered, or still gets protected even if it hasn't been used for a while. And by the time PID finally gets triggered, the workload might has changed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Sat, Feb 21, 2026 at 2:24â€¯AM Johannes Weiner <hannes@cmpxchg.org> wrote:\n>\n> On Fri, Feb 20, 2026 at 01:25:33AM +0800, Kairui Song wrote:\n> > Hi All,\n> >\n> > Apologies I forgot to add the proper tag in the previous email so\n> > resending this.\n> >\n> > MGLRU has been introduced in the mainline for years, but we still have two LRUs\n> > today. There are many reasons MGLRU is still not the only LRU implementation in\n> > the kernel.\n> >\n> > And I've been looking at a few major issues here:\n> >\n> > 1. Page flag usage: MGLRU uses many more flags (3+ more) than Active/Inactive\n> > LRU.\n> > 2. Regressions: MGLRU might cause regression, even though in many workloads it\n> > outperforms Active/Inactive by a lot.\n> > 3. Metrics: MGLRU makes some metrics work differently, for example: PSI,\n> > /proc/meminfo.\n> > 4. Some reclaim behavior is less controllable.\n>\n> I would be very interested in discussing this topic as well.\n\nThanks, glad to hear that!\n\n>\n> > 2. Regressions: Currently regression is a more major problem for us.\n> >    From our perspective, almost all regressions are caused by an under- or\n> >    overprotected file cache. MGLRU's PID protection either gets too aggressive\n> >    or too passive or just have a too long latency. To fix that, I'd propose a\n> >    LFU-like design and relax the PID's aggressiveness to make it much more\n> >    proactive and effective for file folios. The idea is always use 3 bits in\n> >    the page flags to count the referenced time (which would also replace\n> >    PG_workingset and PG_referenced). Initial tests showed a 30% reduction of\n> >    refaults, and many regressions are gone. A flow chart of how the MGLRU idea\n> >    might work:\n>\n> Are you referring to refaults on the page cache side, or swapins?\n>\n> Last time we evaluated MGLRU on Meta workloads, we noticed that it\n> tends to do better with zswap, but worse with disk swap. It seemed to\n> just prefer reclaiming anon, period.\n>\n> For the balancing between anon and file to work well in all\n> situations, it needs to have a notion of backend speed and factor in\n> the respective cost of misses on each side.\n\nA bit more than that. When there is no swap, MGLRU still performs\nworse in some workloads like MongoDB. From what I've noticed that's\nbecause the PID protection is a bit too passive, and there is a force\nprotection in sort_folio which sometimes seems too aggressive.\nActive/Inactive LRU will just move a folio to head if it's accessed\ntwice while in RAM, but MGLRU won't do so, as result hotter file\nfolios are evicted equally as the colder one until the PID gets\ntriggered, or still gets protected even if it hasn't been used for a\nwhile. And by the time PID finally gets triggered, the workload might\nhas changed. This is fixable using the approach I mentioned though,\nand it seems to be better than the Active/Inactive in all our known\ncases after that, whether that is a good fix worth discussion.\n\nI also notice Ridong has a series to apply a \"heat\" based reclaim,\nwhich also looks interesting.\n\n> >    Can we just ignore the shadow for anon folios? MGLRU basically activates\n> >    anon folios unconditionally, especially if we combined with the LFU like\n> >    idea above we might only want to track the 3 bit count, and get rid of\n> >    the extra bit usage in the shadow. The eviction performance might be even\n> >    better, and other components like swap table [3] will have more bits to use\n> >    for better performance and more features.\n>\n> On the face of it, both of these sounds problematic to me. Why are\n> anon pages special cased?\n>\n> The cost of reclaiming a page is:\n>\n>     reuse frequency * cost of a miss\n>\n> The *type* of the page is not all that meaningful for workload\n> performance. The wait time is qualitatively the same.\n>\n> If you assume every refaulting anon is hot, it'll fall apart when the\n> anon set is huge and has little locality.\n\nSorry I didn't make it clear. For MGLRU currently it already ignored\nthe shadow distance for re-activation. And yeah, basically all anons\nare activated on fault, which turns out to be quite nice? None MGLRU\nusers considered that as a problem and in fact the performance looks\ngood.\n\nOf course we can restore the old behavior to test the folio\nagainst some distance (gen distance or eviction distance), or push it\nfurther to only keep the reference bit (not completely ignore the\nshadow, just only keep the reference bits, if the LFU + PID still\nworks well without the distance), and gain more performance and bits\nto use.\n\nBTW I tried to restore the refault distance behavior for both anon and\nfile folios sometime ago:\nhttps://lwn.net/Articles/945266/\n\nFor file folios it indeed looked better, anon folios seems unchanged.\nBut later tests showed that it doesn't apply to all cases, and I think\nsomething better can be used as suggested in this topic.\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-21",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v5] mm: move pgscan, pgsteal, pgrefill to node stats",
          "message_id": "aZiv2ASYc46m7K_c@cmpxchg.org",
          "url": "https://lore.kernel.org/all/aZiv2ASYc46m7K_c@cmpxchg.org/",
          "date": "2026-02-20T19:02:50Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joshua Hahn",
      "primary_email": "joshua.hahnjy@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "JP Kobryn",
      "primary_email": "inwardvessel@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v5] mm: move pgscan, pgsteal, pgrefill to node stats",
          "message_id": "20260219235846.161910-1-jp.kobryn@linux.dev",
          "url": "https://lore.kernel.org/all/20260219235846.161910-1-jp.kobryn@linux.dev/",
          "date": "2026-02-19T23:59:27Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-19",
          "patch_summary": "There are situations where reclaim kicks in on a system with free memory. One possible cause is a NUMA imbalance scenario where one or more nodes are under pressure. It would help if we could easily identify such nodes.\n\nMove the pgscan, pgsteal, and pgrefill counters from vm_event_item to node_stat_item to provide per-node reclaim visibility. With these counters as node stats, the values are now displayed in the per-node section of /proc/zoneinfo, which allows for quick identification of the affected nodes.\n\n/proc/vmstat continues to report the same counters, aggregated across all nodes. But the ordering of these items within the readout changes as they move from the vm events section to the node stats section.\n\nMemcg accounting of these counters is preserved. The relocated counters remain visible in memory.stat alongside the existing aggregate pgscan and pgsteal counters.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Zi Yan",
              "summary": "Gave Acked-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On 19 Feb 2026, at 18:58, JP Kobryn (Meta) wrote:\n\n> There are situations where reclaim kicks in on a system with free memory.\n> One possible cause is a NUMA imbalance scenario where one or more nodes are\n> under pressure. It would help if we could easily identify such nodes.\n>\n> Move the pgscan, pgsteal, and pgrefill counters from vm_event_item to\n> node_stat_item to provide per-node reclaim visibility. With these counters\n> as node stats, the values are now displayed in the per-node section of\n> /proc/zoneinfo, which allows for quick identification of the affected\n> nodes.\n>\n> /proc/vmstat continues to report the same counters, aggregated across all\n> nodes. But the ordering of these items within the readout changes as they\n> move from the vm events section to the node stats section.\n>\n> Memcg accounting of these counters is preserved. The relocated counters\n> remain visible in memory.stat alongside the existing aggregate pgscan and\n> pgsteal counters.\n>\n> However, this change affects how the global counters are accumulated.\n> Previously, the global event count update was gated on !cgroup_reclaim(),\n> excluding memcg-based reclaim from /proc/vmstat. Now that\n> mod_lruvec_state() is being used to update the counters, the global\n> counters will include all reclaim. This is consistent with how pgdemote\n> counters are already tracked.\n>\n> Finally, the virtio_balloon driver is updated to use\n> global_node_page_state() to fetch the counters, as they are no longer\n> accessible through the vm_events array.\n>\n> Signed-off-by: JP Kobryn <jp.kobryn@linux.dev>\n> Suggested-by: Johannes Weiner <hannes@cmpxchg.org>\n> Acked-by: Michael S. Tsirkin <mst@redhat.com>\n> Reviewed-by: Vlastimil Babka (SUSE) <vbabka@kernel.org>\n> ---\n> v5:\n> \t- rebase onto mm/mm-new\n>\n> v4: https://lore.kernel.org/linux-mm/20260219171124.19053-1-jp.kobryn@linux.dev/\n> \t- remove unused memcg var from scan_folios()\n>\n> v3: https://lore.kernel.org/linux-mm/20260218222652.108411-1-jp.kobryn@linux.dev/\n> \t- additionally move PGREFILL to node stats\n>\n> v2: https://lore.kernel.org/linux-mm/20260218032941.225439-1-jp.kobryn@linux.dev/\n> \t- update commit message\n> \t- add entries to memory_stats array\n> \t- add switch cases in memcg_page_state_output_unit()\n>\n> v1: https://lore.kernel.org/linux-mm/20260212045109.255391-3-inwardvessel@gmail.com/\n>\n>  drivers/virtio/virtio_balloon.c |  8 ++---\n>  include/linux/mmzone.h          | 13 ++++++++\n>  include/linux/vm_event_item.h   | 13 --------\n>  mm/memcontrol.c                 | 56 +++++++++++++++++++++++----------\n>  mm/vmscan.c                     | 39 ++++++++---------------\n>  mm/vmstat.c                     | 26 +++++++--------\n>  6 files changed, 82 insertions(+), 73 deletions(-)\n>\n\nAcked-by: Zi Yan <ziy@nvidia.com>\n\nBest Regards,\nYan, Zi\n",
              "reply_to": "",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Johannes Weiner",
              "summary": "Gave Acked-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Thu, Feb 19, 2026 at 03:58:46PM -0800, JP Kobryn (Meta) wrote:\n> There are situations where reclaim kicks in on a system with free memory.\n> One possible cause is a NUMA imbalance scenario where one or more nodes are\n> under pressure. It would help if we could easily identify such nodes.\n> \n> Move the pgscan, pgsteal, and pgrefill counters from vm_event_item to\n> node_stat_item to provide per-node reclaim visibility. With these counters\n> as node stats, the values are now displayed in the per-node section of\n> /proc/zoneinfo, which allows for quick identification of the affected\n> nodes.\n> \n> /proc/vmstat continues to report the same counters, aggregated across all\n> nodes. But the ordering of these items within the readout changes as they\n> move from the vm events section to the node stats section.\n> \n> Memcg accounting of these counters is preserved. The relocated counters\n> remain visible in memory.stat alongside the existing aggregate pgscan and\n> pgsteal counters.\n> \n> However, this change affects how the global counters are accumulated.\n> Previously, the global event count update was gated on !cgroup_reclaim(),\n> excluding memcg-based reclaim from /proc/vmstat. Now that\n> mod_lruvec_state() is being used to update the counters, the global\n> counters will include all reclaim. This is consistent with how pgdemote\n> counters are already tracked.\n> \n> Finally, the virtio_balloon driver is updated to use\n> global_node_page_state() to fetch the counters, as they are no longer\n> accessible through the vm_events array.\n> \n> Signed-off-by: JP Kobryn <jp.kobryn@linux.dev>\n> Suggested-by: Johannes Weiner <hannes@cmpxchg.org>\n> Acked-by: Michael S. Tsirkin <mst@redhat.com>\n> Reviewed-by: Vlastimil Babka (SUSE) <vbabka@kernel.org>\n\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Yeah this difference always confused me.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Thu, Feb 19, 2026 at 03:58:46PM -0800, JP Kobryn (Meta) wrote:\n> There are situations where reclaim kicks in on a system with free memory.\n> One possible cause is a NUMA imbalance scenario where one or more nodes are\n> under pressure. It would help if we could easily identify such nodes.\n> \n> Move the pgscan, pgsteal, and pgrefill counters from vm_event_item to\n> node_stat_item to provide per-node reclaim visibility. With these counters\n> as node stats, the values are now displayed in the per-node section of\n> /proc/zoneinfo, which allows for quick identification of the affected\n> nodes.\n> \n> /proc/vmstat continues to report the same counters, aggregated across all\n> nodes. But the ordering of these items within the readout changes as they\n> move from the vm events section to the node stats section.\n> \n> Memcg accounting of these counters is preserved. The relocated counters\n> remain visible in memory.stat alongside the existing aggregate pgscan and\n> pgsteal counters.\n> \n> However, this change affects how the global counters are accumulated.\n> Previously, the global event count update was gated on !cgroup_reclaim(),\n> excluding memcg-based reclaim from /proc/vmstat. Now that\n> mod_lruvec_state() is being used to update the counters, the global\n> counters will include all reclaim. This is consistent with how pgdemote\n> counters are already tracked.\n\nYeah this difference always confused me.\n\n> \n> Finally, the virtio_balloon driver is updated to use\n> global_node_page_state() to fetch the counters, as they are no longer\n> accessible through the vm_events array.\n> \n> Signed-off-by: JP Kobryn <jp.kobryn@linux.dev>\n> Suggested-by: Johannes Weiner <hannes@cmpxchg.org>\n> Acked-by: Michael S. Tsirkin <mst@redhat.com>\n> Reviewed-by: Vlastimil Babka (SUSE) <vbabka@kernel.org>\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Michal Hocko",
              "summary": "Gave Acked-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Thu 19-02-26 15:58:46, JP Kobryn (Meta) wrote:\n> There are situations where reclaim kicks in on a system with free memory.\n> One possible cause is a NUMA imbalance scenario where one or more nodes are\n> under pressure. It would help if we could easily identify such nodes.\n> \n> Move the pgscan, pgsteal, and pgrefill counters from vm_event_item to\n> node_stat_item to provide per-node reclaim visibility. With these counters\n> as node stats, the values are now displayed in the per-node section of\n> /proc/zoneinfo, which allows for quick identification of the affected\n> nodes.\n> \n> /proc/vmstat continues to report the same counters, aggregated across all\n> nodes. But the ordering of these items within the readout changes as they\n> move from the vm events section to the node stats section.\n> \n> Memcg accounting of these counters is preserved. The relocated counters\n> remain visible in memory.stat alongside the existing aggregate pgscan and\n> pgsteal counters.\n> \n> However, this change affects how the global counters are accumulated.\n> Previously, the global event count update was gated on !cgroup_reclaim(),\n> excluding memcg-based reclaim from /proc/vmstat. Now that\n> mod_lruvec_state() is being used to update the counters, the global\n> counters will include all reclaim. This is consistent with how pgdemote\n> counters are already tracked.\n> \n> Finally, the virtio_balloon driver is updated to use\n> global_node_page_state() to fetch the counters, as they are no longer\n> accessible through the vm_events array.\n> \n> Signed-off-by: JP Kobryn <jp.kobryn@linux.dev>\n> Suggested-by: Johannes Weiner <hannes@cmpxchg.org>\n> Acked-by: Michael S. Tsirkin <mst@redhat.com>\n> Reviewed-by: Vlastimil Babka (SUSE) <vbabka@kernel.org>\n\nAcked-by: Michal Hocko <mhocko@suse.com>\nThanks\n\n> ---\n> v5:\n> \t- rebase onto mm/mm-new\n> \n> v4: https://lore.kernel.org/linux-mm/20260219171124.19053-1-jp.kobryn@linux.dev/\n> \t- remove unused memcg var from scan_folios()\n> \n> v3: https://lore.kernel.org/linux-mm/20260218222652.108411-1-jp.kobryn@linux.dev/\n> \t- additionally move PGREFILL to node stats\n> \n> v2: https://lore.kernel.org/linux-mm/20260218032941.225439-1-jp.kobryn@linux.dev/\n> \t- update commit message\n> \t- add entries to memory_stats array\n> \t- add switch cases in memcg_page_state_output_unit()\n> \n> v1: https://lore.kernel.org/linux-mm/20260212045109.255391-3-inwardvessel@gmail.com/\n> \n>  drivers/virtio/virtio_balloon.c |  8 ++---\n>  include/linux/mmzone.h          | 13 ++++++++\n>  include/linux/vm_event_item.h   | 13 --------\n>  mm/memcontrol.c                 | 56 +++++++++++++++++++++++----------\n>  mm/vmscan.c                     | 39 ++++++++---------------\n>  mm/vmstat.c                     | 26 +++++++--------\n>  6 files changed, 82 insertions(+), 73 deletions(-)\n> \n> diff --git a/drivers/virtio/virtio_balloon.c b/drivers/virtio/virtio_balloon.c\n> index 4e549abe59ff..ab945532ceef 100644\n> --- a/drivers/virtio/virtio_balloon.c\n> +++ b/drivers/virtio/virtio_balloon.c\n> @@ -369,13 +369,13 @@ static inline unsigned int update_balloon_vm_stats(struct virtio_balloon *vb)\n>  \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_ALLOC_STALL, stall);\n>  \n>  \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_ASYNC_SCAN,\n> -\t\t    pages_to_bytes(events[PGSCAN_KSWAPD]));\n> +\t\t    pages_to_bytes(global_node_page_state(PGSCAN_KSWAPD)));\n>  \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_DIRECT_SCAN,\n> -\t\t    pages_to_bytes(events[PGSCAN_DIRECT]));\n> +\t\t    pages_to_bytes(global_node_page_state(PGSCAN_DIRECT)));\n>  \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_ASYNC_RECLAIM,\n> -\t\t    pages_to_bytes(events[PGSTEAL_KSWAPD]));\n> +\t\t    pages_to_bytes(global_node_page_state(PGSTEAL_KSWAPD)));\n>  \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_DIRECT_RECLAIM,\n> -\t\t    pages_to_bytes(events[PGSTEAL_DIRECT]));\n> +\t\t    pages_to_bytes(global_node_page_state(PGSTEAL_DIRECT)));\n>  \n>  #ifdef CONFIG_HUGETLB_PAGE\n>  \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_HTLB_PGALLOC,\n> diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> index 3e51190a55e4..546bca95ca40 100644\n> --- a/include/linux/mmzone.h\n> +++ b/include/linux/mmzone.h\n> @@ -255,6 +255,19 @@ enum node_stat_item {\n>  \tPGDEMOTE_DIRECT,\n>  \tPGDEMOTE_KHUGEPAGED,\n>  \tPGDEMOTE_PROACTIVE,\n> +\tPGSTEAL_KSWAPD,\n> +\tPGSTEAL_DIRECT,\n> +\tPGSTEAL_KHUGEPAGED,\n> +\tPGSTEAL_PROACTIVE,\n> +\tPGSTEAL_ANON,\n> +\tPGSTEAL_FILE,\n> +\tPGSCAN_KSWAPD,\n> +\tPGSCAN_DIRECT,\n> +\tPGSCAN_KHUGEPAGED,\n> +\tPGSCAN_PROACTIVE,\n> +\tPGSCAN_ANON,\n> +\tPGSCAN_FILE,\n> +\tPGREFILL,\n>  #ifdef CONFIG_HUGETLB_PAGE\n>  \tNR_HUGETLB,\n>  #endif\n> diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\n> index 22a139f82d75..03fe95f5a020 100644\n> --- a/include/linux/vm_event_item.h\n> +++ b/include/linux/vm_event_item.h\n> @@ -38,21 +38,8 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n>  \t\tPGFREE, PGACTIVATE, PGDEACTIVATE, PGLAZYFREE,\n>  \t\tPGFAULT, PGMAJFAULT,\n>  \t\tPGLAZYFREED,\n> -\t\tPGREFILL,\n>  \t\tPGREUSE,\n> -\t\tPGSTEAL_KSWAPD,\n> -\t\tPGSTEAL_DIRECT,\n> -\t\tPGSTEAL_KHUGEPAGED,\n> -\t\tPGSTEAL_PROACTIVE,\n> -\t\tPGSCAN_KSWAPD,\n> -\t\tPGSCAN_DIRECT,\n> -\t\tPGSCAN_KHUGEPAGED,\n> -\t\tPGSCAN_PROACTIVE,\n>  \t\tPGSCAN_DIRECT_THROTTLE,\n> -\t\tPGSCAN_ANON,\n> -\t\tPGSCAN_FILE,\n> -\t\tPGSTEAL_ANON,\n> -\t\tPGSTEAL_FILE,\n>  #ifdef CONFIG_NUMA\n>  \t\tPGSCAN_ZONE_RECLAIM_SUCCESS,\n>  \t\tPGSCAN_ZONE_RECLAIM_FAILED,\n> diff --git a/mm/memcontrol.c b/mm/memcontrol.c\n> index 6fb9c999347b..0d834c47706f 100644\n> --- a/mm/memcontrol.c\n> +++ b/mm/memcontrol.c\n> @@ -331,6 +331,19 @@ static const unsigned int memcg_node_stat_items[] = {\n>  \tPGDEMOTE_DIRECT,\n>  \tPGDEMOTE_KHUGEPAGED,\n>  \tPGDEMOTE_PROACTIVE,\n> +\tPGSTEAL_KSWAPD,\n> +\tPGSTEAL_DIRECT,\n> +\tPGSTEAL_KHUGEPAGED,\n> +\tPGSTEAL_PROACTIVE,\n> +\tPGSTEAL_ANON,\n> +\tPGSTEAL_FILE,\n> +\tPGSCAN_KSWAPD,\n> +\tPGSCAN_DIRECT,\n> +\tPGSCAN_KHUGEPAGED,\n> +\tPGSCAN_PROACTIVE,\n> +\tPGSCAN_ANON,\n> +\tPGSCAN_FILE,\n> +\tPGREFILL,\n>  #ifdef CONFIG_HUGETLB_PAGE\n>  \tNR_HUGETLB,\n>  #endif\n> @@ -444,17 +457,8 @@ static const unsigned int memcg_vm_event_stat[] = {\n>  #endif\n>  \tPSWPIN,\n>  \tPSWPOUT,\n> -\tPGSCAN_KSWAPD,\n> -\tPGSCAN_DIRECT,\n> -\tPGSCAN_KHUGEPAGED,\n> -\tPGSCAN_PROACTIVE,\n> -\tPGSTEAL_KSWAPD,\n> -\tPGSTEAL_DIRECT,\n> -\tPGSTEAL_KHUGEPAGED,\n> -\tPGSTEAL_PROACTIVE,\n>  \tPGFAULT,\n>  \tPGMAJFAULT,\n> -\tPGREFILL,\n>  \tPGACTIVATE,\n>  \tPGDEACTIVATE,\n>  \tPGLAZYFREE,\n> @@ -1401,6 +1405,15 @@ static const struct memory_stat memory_stats[] = {\n>  \t{ \"pgdemote_direct\",\t\tPGDEMOTE_DIRECT\t\t},\n>  \t{ \"pgdemote_khugepaged\",\tPGDEMOTE_KHUGEPAGED\t},\n>  \t{ \"pgdemote_proactive\",\t\tPGDEMOTE_PROACTIVE\t},\n> +\t{ \"pgsteal_kswapd\",\t\tPGSTEAL_KSWAPD\t\t},\n> +\t{ \"pgsteal_direct\",\t\tPGSTEAL_DIRECT\t\t},\n> +\t{ \"pgsteal_khugepaged\",\t\tPGSTEAL_KHUGEPAGED\t},\n> +\t{ \"pgsteal_proactive\",\t\tPGSTEAL_PROACTIVE\t},\n> +\t{ \"pgscan_kswapd\",\t\tPGSCAN_KSWAPD\t\t},\n> +\t{ \"pgscan_direct\",\t\tPGSCAN_DIRECT\t\t},\n> +\t{ \"pgscan_khugepaged\",\t\tPGSCAN_KHUGEPAGED\t},\n> +\t{ \"pgscan_proactive\",\t\tPGSCAN_PROACTIVE\t},\n> +\t{ \"pgrefill\",\t\t\tPGREFILL\t\t},\n>  #ifdef CONFIG_NUMA_BALANCING\n>  \t{ \"pgpromote_success\",\t\tPGPROMOTE_SUCCESS\t},\n>  #endif\n> @@ -1444,6 +1457,15 @@ static int memcg_page_state_output_unit(int item)\n>  \tcase PGDEMOTE_DIRECT:\n>  \tcase PGDEMOTE_KHUGEPAGED:\n>  \tcase PGDEMOTE_PROACTIVE:\n> +\tcase PGSTEAL_KSWAPD:\n> +\tcase PGSTEAL_DIRECT:\n> +\tcase PGSTEAL_KHUGEPAGED:\n> +\tcase PGSTEAL_PROACTIVE:\n> +\tcase PGSCAN_KSWAPD:\n> +\tcase PGSCAN_DIRECT:\n> +\tcase PGSCAN_KHUGEPAGED:\n> +\tcase PGSCAN_PROACTIVE:\n> +\tcase PGREFILL:\n>  #ifdef CONFIG_NUMA_BALANCING\n>  \tcase PGPROMOTE_SUCCESS:\n>  #endif\n> @@ -1562,15 +1584,15 @@ static void memcg_stat_format(struct mem_cgroup *memcg, struct seq_buf *s)\n>  \n>  \t/* Accumulated memory events */\n>  \tmemcg_seq_buf_print_stat(s, NULL, \"pgscan\", ' ',\n> -\t\t\t\t memcg_events(memcg, PGSCAN_KSWAPD) +\n> -\t\t\t\t memcg_events(memcg, PGSCAN_DIRECT) +\n> -\t\t\t\t memcg_events(memcg, PGSCAN_PROACTIVE) +\n> -\t\t\t\t memcg_events(memcg, PGSCAN_KHUGEPAGED));\n> +\t\t\t\t memcg_page_state(memcg, PGSCAN_KSWAPD) +\n> +\t\t\t\t memcg_page_state(memcg, PGSCAN_DIRECT) +\n> +\t\t\t\t memcg_page_state(memcg, PGSCAN_PROACTIVE) +\n> +\t\t\t\t memcg_page_state(memcg, PGSCAN_KHUGEPAGED));\n>  \tmemcg_seq_buf_print_stat(s, NULL, \"pgsteal\", ' ',\n> -\t\t\t\t memcg_events(memcg, PGSTEAL_KSWAPD) +\n> -\t\t\t\t memcg_events(memcg, PGSTEAL_DIRECT) +\n> -\t\t\t\t memcg_events(memcg, PGSTEAL_PROACTIVE) +\n> -\t\t\t\t memcg_events(memcg, PGSTEAL_KHUGEPAGED));\n> +\t\t\t\t memcg_page_state(memcg, PGSTEAL_KSWAPD) +\n> +\t\t\t\t memcg_page_state(memcg, PGSTEAL_DIRECT) +\n> +\t\t\t\t memcg_page_state(memcg, PGSTEAL_PROACTIVE) +\n> +\t\t\t\t memcg_page_state(memcg, PGSTEAL_KHUGEPAGED));\n>  \n>  \tfor (i = 0; i < ARRAY_SIZE(memcg_vm_event_stat); i++) {\n>  #ifdef CONFIG_MEMCG_V1\n> diff --git a/mm/vmscan.c b/mm/vmscan.c\n> index 5fa6e6bd6540..c3dc7c7befac 100644\n> --- a/mm/vmscan.c\n> +++ b/mm/vmscan.c\n> @@ -1984,7 +1984,7 @@ static unsigned long shrink_inactive_list(unsigned long nr_to_scan,\n>  \tunsigned long nr_taken;\n>  \tstruct reclaim_stat stat;\n>  \tbool file = is_file_lru(lru);\n> -\tenum vm_event_item item;\n> +\tenum node_stat_item item;\n>  \tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n>  \tbool stalled = false;\n>  \n> @@ -2010,10 +2010,8 @@ static unsigned long shrink_inactive_list(unsigned long nr_to_scan,\n>  \n>  \t__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, nr_taken);\n>  \titem = PGSCAN_KSWAPD + reclaimer_offset(sc);\n> -\tif (!cgroup_reclaim(sc))\n> -\t\t__count_vm_events(item, nr_scanned);\n> -\tcount_memcg_events(lruvec_memcg(lruvec), item, nr_scanned);\n> -\t__count_vm_events(PGSCAN_ANON + file, nr_scanned);\n> +\tmod_lruvec_state(lruvec, item, nr_scanned);\n> +\tmod_lruvec_state(lruvec, PGSCAN_ANON + file, nr_scanned);\n>  \n>  \tspin_unlock_irq(&lruvec->lru_lock);\n>  \n> @@ -2030,10 +2028,8 @@ static unsigned long shrink_inactive_list(unsigned long nr_to_scan,\n>  \t\t\t\t\tstat.nr_demoted);\n>  \t__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);\n>  \titem = PGSTEAL_KSWAPD + reclaimer_offset(sc);\n> -\tif (!cgroup_reclaim(sc))\n> -\t\t__count_vm_events(item, nr_reclaimed);\n> -\tcount_memcg_events(lruvec_memcg(lruvec), item, nr_reclaimed);\n> -\t__count_vm_events(PGSTEAL_ANON + file, nr_reclaimed);\n> +\tmod_lruvec_state(lruvec, item, nr_reclaimed);\n> +\tmod_lruvec_state(lruvec, PGSTEAL_ANON + file, nr_reclaimed);\n>  \n>  \tlru_note_cost_unlock_irq(lruvec, file, stat.nr_pageout,\n>  \t\t\t\t\tnr_scanned - nr_reclaimed);\n> @@ -2120,9 +2116,7 @@ static void shrink_active_list(unsigned long nr_to_scan,\n>  \n>  \t__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, nr_taken);\n>  \n> -\tif (!cgroup_reclaim(sc))\n> -\t\t__count_vm_events(PGREFILL, nr_scanned);\n> -\tcount_memcg_events(lruvec_memcg(lruvec), PGREFILL, nr_scanned);\n> +\tmod_lruvec_state(lruvec, PGREFILL, nr_scanned);\n>  \n>  \tspin_unlock_irq(&lruvec->lru_lock);\n>  \n> @@ -4537,7 +4531,7 @@ static int scan_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n>  {\n>  \tint i;\n>  \tint gen;\n> -\tenum vm_event_item item;\n> +\tenum node_stat_item item;\n>  \tint sorted = 0;\n>  \tint scanned = 0;\n>  \tint isolated = 0;\n> @@ -4545,7 +4539,6 @@ static int scan_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n>  \tint scan_batch = min(nr_to_scan, MAX_LRU_BATCH);\n>  \tint remaining = scan_batch;\n>  \tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n> -\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n>  \n>  \tVM_WARN_ON_ONCE(!list_empty(list));\n>  \n> @@ -4596,13 +4589,9 @@ static int scan_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n>  \t}\n>  \n>  \titem = PGSCAN_KSWAPD + reclaimer_offset(sc);\n> -\tif (!cgroup_reclaim(sc)) {\n> -\t\t__count_vm_events(item, isolated);\n> -\t\t__count_vm_events(PGREFILL, sorted);\n> -\t}\n> -\tcount_memcg_events(memcg, item, isolated);\n> -\tcount_memcg_events(memcg, PGREFILL, sorted);\n> -\t__count_vm_events(PGSCAN_ANON + type, isolated);\n> +\tmod_lruvec_state(lruvec, item, isolated);\n> +\tmod_lruvec_state(lruvec, PGREFILL, sorted);\n> +\tmod_lruvec_state(lruvec, PGSCAN_ANON + type, isolated);\n>  \ttrace_mm_vmscan_lru_isolate(sc->reclaim_idx, sc->order, scan_batch,\n>  \t\t\t\tscanned, skipped, isolated,\n>  \t\t\t\ttype ? LRU_INACTIVE_FILE : LRU_INACTIVE_ANON);\n> @@ -4705,7 +4694,7 @@ static int evict_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n>  \tLIST_HEAD(clean);\n>  \tstruct folio *folio;\n>  \tstruct folio *next;\n> -\tenum vm_event_item item;\n> +\tenum node_stat_item item;\n>  \tstruct reclaim_stat stat;\n>  \tstruct lru_gen_mm_walk *walk;\n>  \tbool skip_retry = false;\n> @@ -4769,10 +4758,8 @@ static int evict_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n>  \t\t\t\t\tstat.nr_demoted);\n>  \n>  \titem = PGSTEAL_KSWAPD + reclaimer_offset(sc);\n> -\tif (!cgroup_reclaim(sc))\n> -\t\t__count_vm_events(item, reclaimed);\n> -\tcount_memcg_events(memcg, item, reclaimed);\n> -\t__count_vm_events(PGSTEAL_ANON + type, reclaimed);\n> +\tmod_lruvec_state(lruvec, item, reclaimed);\n> +\tmod_lruvec_state(lruvec, PGSTEAL_ANON + type, reclaimed);\n>  \n>  \tspin_unlock_irq(&lruvec->lru_lock);\n>  \n> diff --git a/mm/vmstat.c b/mm/vmstat.c\n> index 86b14b0f77b5..44bbb7752f11 100644\n> --- a/mm/vmstat.c\n> +++ b/mm/vmstat.c\n> @@ -1276,6 +1276,19 @@ const char * const vmstat_text[] = {\n>  \t[I(PGDEMOTE_DIRECT)]\t\t\t= \"pgdemote_direct\",\n>  \t[I(PGDEMOTE_KHUGEPAGED)]\t\t= \"pgdemote_khugepaged\",\n>  \t[I(PGDEMOTE_PROACTIVE)]\t\t\t= \"pgdemote_proactive\",\n> +\t[I(PGSTEAL_KSWAPD)]\t\t\t= \"pgsteal_kswapd\",\n> +\t[I(PGSTEAL_DIRECT)]\t\t\t= \"pgsteal_direct\",\n> +\t[I(PGSTEAL_KHUGEPAGED)]\t\t\t= \"pgsteal_khugepaged\",\n> +\t[I(PGSTEAL_PROACTIVE)]\t\t\t= \"pgsteal_proactive\",\n> +\t[I(PGSTEAL_ANON)]\t\t\t= \"pgsteal_anon\",\n> +\t[I(PGSTEAL_FILE)]\t\t\t= \"pgsteal_file\",\n> +\t[I(PGSCAN_KSWAPD)]\t\t\t= \"pgscan_kswapd\",\n> +\t[I(PGSCAN_DIRECT)]\t\t\t= \"pgscan_direct\",\n> +\t[I(PGSCAN_KHUGEPAGED)]\t\t\t= \"pgscan_khugepaged\",\n> +\t[I(PGSCAN_PROACTIVE)]\t\t\t= \"pgscan_proactive\",\n> +\t[I(PGSCAN_ANON)]\t\t\t= \"pgscan_anon\",\n> +\t[I(PGSCAN_FILE)]\t\t\t= \"pgscan_file\",\n> +\t[I(PGREFILL)]\t\t\t\t= \"pgrefill\",\n>  #ifdef CONFIG_HUGETLB_PAGE\n>  \t[I(NR_HUGETLB)]\t\t\t\t= \"nr_hugetlb\",\n>  #endif\n> @@ -1318,21 +1331,8 @@ const char * const vmstat_text[] = {\n>  \t[I(PGMAJFAULT)]\t\t\t\t= \"pgmajfault\",\n>  \t[I(PGLAZYFREED)]\t\t\t= \"pglazyfreed\",\n>  \n> -\t[I(PGREFILL)]\t\t\t\t= \"pgrefill\",\n>  \t[I(PGREUSE)]\t\t\t\t= \"pgreuse\",\n> -\t[I(PGSTEAL_KSWAPD)]\t\t\t= \"pgsteal_kswapd\",\n> -\t[I(PGSTEAL_DIRECT)]\t\t\t= \"pgsteal_direct\",\n> -\t[I(PGSTEAL_KHUGEPAGED)]\t\t\t= \"pgsteal_khugepaged\",\n> -\t[I(PGSTEAL_PROACTIVE)]\t\t\t= \"pgsteal_proactive\",\n> -\t[I(PGSCAN_KSWAPD)]\t\t\t= \"pgscan_kswapd\",\n> -\t[I(PGSCAN_DIRECT)]\t\t\t= \"pgscan_direct\",\n> -\t[I(PGSCAN_KHUGEPAGED)]\t\t\t= \"pgscan_khugepaged\",\n> -\t[I(PGSCAN_PROACTIVE)]\t\t\t= \"pgscan_proactive\",\n>  \t[I(PGSCAN_DIRECT_THROTTLE)]\t\t= \"pgscan_direct_throttle\",\n> -\t[I(PGSCAN_ANON)]\t\t\t= \"pgscan_anon\",\n> -\t[I(PGSCAN_FILE)]\t\t\t= \"pgscan_file\",\n> -\t[I(PGSTEAL_ANON)]\t\t\t= \"pgsteal_anon\",\n> -\t[I(PGSTEAL_FILE)]\t\t\t= \"pgsteal_file\",\n>  \n>  #ifdef CONFIG_NUMA\n>  \t[I(PGSCAN_ZONE_RECLAIM_SUCCESS)]\t= \"zone_reclaim_success\",\n> -- \n> 2.47.3\n\n-- \nMichal Hocko\nSUSE Labs\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Kiryl Shutsemau",
      "primary_email": "kas@kernel.org",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86",
          "message_id": "aZiBgbAoe1FQ5nO-@thinkstation",
          "url": "https://lore.kernel.org/all/aZiBgbAoe1FQ5nO-@thinkstation/",
          "date": "2026-02-20T15:50:13Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86",
          "message_id": "aZhOnSVao9yFJML7@thinkstation",
          "url": "https://lore.kernel.org/all/aZhOnSVao9yFJML7@thinkstation/",
          "date": "2026-02-20T12:10:50Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Kalesh Singh",
              "summary": "I think personality(2) may be too late? By the time a process invokes it, the initial userspace mappings (executable, linker for init, etc) are already established with the default granularity. To handle this, I've been using an early_param to enforce the larger VMA alignment system-wide right from boot. Perhaps, something for global enforcement (Kconfig/early param) and a prctl/personality flag for per-process opt in? This makes sense for maintaining ABI compatibility. Userspace allocators might want to optimize their layouts to match PG_SIZE while still being able to operate at PTE_SIZE when needed.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "preference expressed"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 4:10â€¯AM Kiryl Shutsemau <kas@kernel.org> wrote:\n>\n> On Thu, Feb 19, 2026 at 03:24:37PM -0800, Kalesh Singh wrote:\n> > On Thu, Feb 19, 2026 at 7:39â€¯AM David Hildenbrand (Arm)\n> > <david@kernel.org> wrote:\n> > >\n> > > On 2/19/26 16:08, Kiryl Shutsemau wrote:\n> > > > No, there's no new hardware (that I know of). I want to explore what page size\n> > > > means.\n> > > >\n> > > > The kernel uses the same value - PAGE_SIZE - for two things:\n> > > >\n> > > >    - the order-0 buddy allocation size;\n> > > >\n> > > >    - the granularity of virtual address space mapping;\n> > > >\n> > > > I think we can benefit from separating these two meanings and allowing\n> > > > order-0 allocations to be larger than the virtual address space covered by a\n> > > > PTE entry.\n> > > >\n> > > > The main motivation is scalability. Managing memory on multi-terabyte\n> > > > machines in 4k is suboptimal, to say the least.\n> > > >\n> > > > Potential benefits of the approach (assuming 64k pages):\n> > > >\n> > > >    - The order-0 page size cuts struct page overhead by a factor of 16. From\n> > > >      ~1.6% of RAM to ~0.1%;\n> > > >\n> > > >    - TLB wins on machines with TLB coalescing as long as mapping is naturally\n> > > >      aligned;\n> > > >\n> > > >    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;\n> > > >\n> > > >    - 1G pages are within possibility for the buddy allocator - order-14\n> > > >      allocation. It can open the road to 1G THPs.\n> > > >\n> > > >    - As with THP, fewer pages - less pressure on the LRU lock;\n> > > >\n> > > >    - ...\n> > > >\n> > > > The trade-off is memory waste (similar to what we have on architectures with\n> > > > native 64k pages today) and complexity, mostly in the core-MM code.\n> > > >\n> > > > == Design considerations ==\n> > > >\n> > > > I want to split PAGE_SIZE into two distinct values:\n> > > >\n> > > >    - PTE_SIZE defines the virtual address space granularity;\n> > > >\n> > > >    - PG_SIZE defines the size of the order-0 buddy allocation;\n> > > >\n> > > > PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code\n> > > > requires conversion, and keep existing code working while conversion is in\n> > > > progress.\n> > > >\n> > > > The same split happens for other page-related macros: mask, shift,\n> > > > alignment helpers, etc.\n> > > >\n> > > > PFNs are in PTE_SIZE units.\n> > > >\n> > > > The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE\n> > > > units.\n> > > >\n> > > > Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes\n> > > > for userspace. But we might want to communicate PG_SIZE to userspace to\n> > > > get the optimal results for userspace that cares.\n> > > >\n> > > > PTE_SIZE granularity requires a substantial rework of page fault and VMA\n> > > > handling:\n> > > >\n> > > >    - A struct page pointer and pgprot_t are not enough to create a PTE entry.\n> > > >      We also need the offset within the page we are creating the PTE for.\n> > > >\n> > > >    - Since the VMA start can be aligned arbitrarily with respect to the\n> > > >      underlying page, vma->vm_pgoff has to be changed to vma->vm_pteoff,\n> > > >      which is in PTE_SIZE units.\n> > > >\n> > > >    - The page fault handler needs to handle PTE_SIZE < PG_SIZE, including\n> > > >      misaligned cases;\n> > > >\n> > > > Page faults into file mappings are relatively simple to handle as we\n> > > > always have the page cache to refer to. So you can map only the part of the\n> > > > page that fits in the page table, similarly to fault-around.\n> > > >\n> > > > Anonymous and file-CoW faults should also be simple as long as the VMA is\n> > > > aligned to PG_SIZE in both the virtual address space and with respect to\n> > > > vm_pgoff. We might waste some memory on the ends of the VMA, but it is\n> > > > tolerable.\n> > > >\n> > > > Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping\n> > > > pages across a page table boundary. In the worst case, a page is mapped across\n> > > > a PGD entry boundary and PTEs for the page have to be put in two separate\n> > > > subtrees of page tables.\n> > > >\n> > > > A naive implementation would map different pages on different sides of a\n> > > > page table boundary and accept the waste of one page per page table crossing.\n> > > > The hope is that misaligned mappings are rare, but this is suboptimal.\n> > > >\n> > > > mremap(2) is the ultimate stress test for the design.\n> > > >\n> > > > On x86, page tables are allocated from the buddy allocator and if PG_SIZE\n> > > > is greater than 4 KB, we need a way to pack multiple page tables into a\n> > > > single page. We could use the slab allocator for this, but it would\n> > > > require relocating the page-table metadata out of struct page.\n> > >\n> > > When discussing per-process page sizes with Ryan and Dev, I mentioned\n> > > that having a larger emulated page size could be interesting for other\n> > > architectures as well.\n> > >\n> > > That is, we would emulate a 64K page size on Intel for user space as\n> > > well, but let the OS work with 4K pages.\n> > >\n> > > We'd only allocate+map large folios into user space + pagecache, but\n> > > still allow for page tables etc. to not waste memory.\n> > >\n> > > So \"most\" of your allocations in the system would actually be at least\n> > > 64k, reducing zone lock contention etc.\n> > >\n> > >\n> > > It doesn't solve all the problems you wanted to tackle on your list\n> > > (e.g., \"struct page\" overhead, which will be sorted out by memdescs).\n> >\n> > Hi Kiryl,\n> >\n> > I'd be interested to discuss this at LSFMM.\n> >\n> > On Android, we have a separate but related use case: we emulate the\n> > userspace page size on x86, primarily to enable app developers to\n> > conduct compatibility testing of their apps for 16KB Android devices.\n> > [1]\n> >\n> > It mainly works by enforcing a larger granularity on the VMAs to\n> > emulate a userspace page size, somewhat similar to what David\n> > mentioned, while the underlying kernel still operates on a 4KB\n> > granularity. [2]\n> >\n> > IIUC the current design would not enfore the larger granularity /\n> > alignment for VMAs to avoid breaking ABI. However, I'd be interest to\n> > discuss whether it can be extended to cover this usecase as well.\n>\n> I don't want to break ABI, but might add a knob (maybe personality(2) ?)\n> for enforcement to see what breaks.\n\nI think personality(2) may be too late? By the time a process invokes\nit, the initial userspace mappings (executable, linker for init, etc)\nare already established with the default granularity.\n\nTo handle this, I've been using an early_param to enforce the larger\nVMA alignment system-wide right from boot.\n\nPerhaps, something for global enforcement (Kconfig/early param) and a\nprctl/personality flag for per-process opt in?\n\n>\n> In general, I would prefer to advertise a new value to userspace that\n> would mean preferred virtual address space granularity.\n\nThis makes sense for maintaining ABI compatibility. Userspace\nallocators might want to optimize their layouts to match PG_SIZE while\nstill being able to operate at PTE_SIZE when needed.\n\n-- Kalesh\n\n>\n> >\n> > [1]  https://developer.android.com/guide/practices/page-sizes#16kb-emulator\n> > [2] https://source.android.com/docs/core/architecture/16kb-page-size/getting-started-cf-x86-64-pgagnostic\n> >\n> > Thanks,\n> > Kalesh\n> >\n> >\n> >\n> >\n> > >\n> > > --\n> > > Cheers,\n> > >\n> > > David\n> > >\n>\n> --\n>   Kiryl Shutsemau / Kirill A. Shutemov\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86",
          "message_id": "aZhErt9DZcWI24_v@thinkstation",
          "url": "https://lore.kernel.org/all/aZhErt9DZcWI24_v@thinkstation/",
          "date": "2026-02-20T12:07:42Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "David (Arm)",
              "summary": "I'd assume that many applications nowadays can deal with differing page sizes (thanks to some other architectures paving the way). But yes, some real legacy stuff, or stuff that ever only cared about intel still hardcodes pagesize=4k. In Meta's fleet, I'd be quite interesting how much conversion there would have to be done. For legacy apps, you could still run them as 4k pagesize on the same system, of course. I still have to wrap my head around the sub-page mapping here as well. It's scary. Re mapcount: I think if any part of the page is mapped, it would be considered mapped -> mapcount += 1. I'd assume that would work. Devil is in the detail with these things before we have memdescs. E.g., page table have a dedicated type (PGTY_table) and store separate metadata in the ptdesc.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/20/26 13:07, Kiryl Shutsemau wrote:\n> On Fri, Feb 20, 2026 at 11:24:37AM +0100, David Hildenbrand (Arm) wrote:\n>>>\n>>> Just to clarify, do you want it to be enforced on userspace ABI.\n>>> Like, all mappings are 64k aligned?\n>>\n>> Right, see the proposal from Dev on the list.\n>>\n>>  From user-space POV, the pagesize would be 64K for these emulated processes.\n>> That is, VMAs must be suitable aligned etc.\n> \n> Well, it will drastically limit the adoption. We have too much legacy\n> stuff on x86.\n\nI'd assume that many applications nowadays can deal with differing page \nsizes (thanks to some other architectures paving the way).\n\nBut yes, some real legacy stuff, or stuff that ever only cared about \nintel still hardcodes pagesize=4k.\n\nIn Meta's fleet, I'd be quite interesting how much conversion there \nwould have to be done.\n\nFor legacy apps, you could still run them as 4k pagesize on the same \nsystem, of course.\n\n> \n>>>\n>>> Waste of memory for page table is solvable and pretty straight forward.\n>>> Most of such cases can be solve mechanically by switching to slab.\n>>\n>> Well, yes, like Willy says, there are already similar custom solutions for\n>> s390x and ppc.\n>>\n>> Pasha talked recently about the memory waste of 16k kernel stacks and how we\n>> would want to reduce that to 4k. In your proposal, it would be 64k, unless\n>> you somehow manage to allocate multiple kernel stacks from the same 64k\n>> page. My head hurts thinking about whether that could work, maybe it could\n>> (no idea about guard pages in there, though).\n> \n> Kernel stack is allocated from vmalloc. I think mapping them with\n> sub-page granularity should be doable.\n\nI still have to wrap my head around the sub-page mapping here as well. \nIt's scary.\n\nRe mapcount: I think if any part of the page is mapped, it would be \nconsidered mapped -> mapcount += 1.\n\n> \n> BTW, do you see any reason why slab-allocated stack wouldn't work for\n> large base page sizes? There's no requirement for it be aligned to page\n> or PTE, right?\n\nI'd assume that would work. Devil is in the detail with these things \nbefore we have memdescs.\n\nE.g., page table have a dedicated type (PGTY_table) and store separate \nmetadata in the ptdesc. For kernel stack there was once a proposal to \nhave a type but it is not upstream.\n\n> \n>> Let's take a look at the history of page size usage on Arm (people can feel\n>> free to correct me):\n>>\n>> (1) Most distros were using 64k on Arm.\n>>\n>> (2) People realized that 64k was suboptimal many use cases (memory\n>>      waste for stacks, pagecache, etc) and started to switch to 4k. I\n>>      remember that mostly HPC-centric users sticked to 64k, but there was\n>>      also demand from others to be able to stay on 64k.\n>>\n>> (3) Arm improved performance on a 4k kernel by adding cont-pte support,\n>>      trying to get closer to 64k native performance.\n>>\n>> (4) Achieving 64k native performance is hard, which is why per-process\n>>      page sizes are being explored to get the best out of both worlds\n>>      (use 64k page size only where it really matters for performance).\n>>\n>> Arm clearly has the added benefit of actually benefiting from hardware\n>> support for 64k.\n>>\n>> IIUC, what you are proposing feels a bit like traveling back in time when it\n>> comes to the memory waste problem that Arm users encountered.\n>>\n>> Where do you see the big difference to 64k on Arm in your proposal? Would\n>> you currently also be running 64k Arm in production and the memory waste etc\n>> is acceptable?\n> \n> That's the point. I don't see a big difference to 64k Arm. I want to\n> bring this option to x86: at some machine size it makes sense trade\n> memory consumption for scalability. I am targeting it to machines with\n> over 2TiB of RAM.\n> \n> BTW, we do run 64k Arm in our fleet. There's some growing pains, but it\n> looks good in general We have no plans to switch to 4k (or 16k) at the\n> moment. 512M THPs also look good on some workloads.\n\nOkay, that's valuable information, thanks!\n\nBeing able to remove the sub-page mapping part (or being able to just \nhide it somewhere deep down in arch code) would make this a lot easier \nto digest.\n\n-- \nCheers,\n\nDavid\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Kalesh Singh",
              "summary": "On Fri, Feb 20, 2026 at 8:30â€¯AM David Hildenbrand (Arm) I think most issues will stem from linkers setting the default ELF segment alignment (max-page-size) for x86 to 4096. So those ELFs will not load correctly or at all on the larger emulated granularity.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 8:30â€¯AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:\n>\n> On 2/20/26 13:07, Kiryl Shutsemau wrote:\n> > On Fri, Feb 20, 2026 at 11:24:37AM +0100, David Hildenbrand (Arm) wrote:\n> >>>\n> >>> Just to clarify, do you want it to be enforced on userspace ABI.\n> >>> Like, all mappings are 64k aligned?\n> >>\n> >> Right, see the proposal from Dev on the list.\n> >>\n> >>  From user-space POV, the pagesize would be 64K for these emulated processes.\n> >> That is, VMAs must be suitable aligned etc.\n> >\n> > Well, it will drastically limit the adoption. We have too much legacy\n> > stuff on x86.\n>\n> I'd assume that many applications nowadays can deal with differing page\n> sizes (thanks to some other architectures paving the way).\n>\n> But yes, some real legacy stuff, or stuff that ever only cared about\n> intel still hardcodes pagesize=4k.\n\nI think most issues will stem from linkers setting the default ELF\nsegment alignment (max-page-size) for x86 to 4096. So those ELFs will\nnot load correctly or at all on the larger emulated granularity.\n\n-- Kalesh\n\n>\n> In Meta's fleet, I'd be quite interesting how much conversion there\n> would have to be done.\n>\n> For legacy apps, you could still run them as 4k pagesize on the same\n> system, of course.\n>\n> >\n> >>>\n> >>> Waste of memory for page table is solvable and pretty straight forward.\n> >>> Most of such cases can be solve mechanically by switching to slab.\n> >>\n> >> Well, yes, like Willy says, there are already similar custom solutions for\n> >> s390x and ppc.\n> >>\n> >> Pasha talked recently about the memory waste of 16k kernel stacks and how we\n> >> would want to reduce that to 4k. In your proposal, it would be 64k, unless\n> >> you somehow manage to allocate multiple kernel stacks from the same 64k\n> >> page. My head hurts thinking about whether that could work, maybe it could\n> >> (no idea about guard pages in there, though).\n> >\n> > Kernel stack is allocated from vmalloc. I think mapping them with\n> > sub-page granularity should be doable.\n>\n> I still have to wrap my head around the sub-page mapping here as well.\n> It's scary.\n>\n> Re mapcount: I think if any part of the page is mapped, it would be\n> considered mapped -> mapcount += 1.\n>\n> >\n> > BTW, do you see any reason why slab-allocated stack wouldn't work for\n> > large base page sizes? There's no requirement for it be aligned to page\n> > or PTE, right?\n>\n> I'd assume that would work. Devil is in the detail with these things\n> before we have memdescs.\n>\n> E.g., page table have a dedicated type (PGTY_table) and store separate\n> metadata in the ptdesc. For kernel stack there was once a proposal to\n> have a type but it is not upstream.\n>\n> >\n> >> Let's take a look at the history of page size usage on Arm (people can feel\n> >> free to correct me):\n> >>\n> >> (1) Most distros were using 64k on Arm.\n> >>\n> >> (2) People realized that 64k was suboptimal many use cases (memory\n> >>      waste for stacks, pagecache, etc) and started to switch to 4k. I\n> >>      remember that mostly HPC-centric users sticked to 64k, but there was\n> >>      also demand from others to be able to stay on 64k.\n> >>\n> >> (3) Arm improved performance on a 4k kernel by adding cont-pte support,\n> >>      trying to get closer to 64k native performance.\n> >>\n> >> (4) Achieving 64k native performance is hard, which is why per-process\n> >>      page sizes are being explored to get the best out of both worlds\n> >>      (use 64k page size only where it really matters for performance).\n> >>\n> >> Arm clearly has the added benefit of actually benefiting from hardware\n> >> support for 64k.\n> >>\n> >> IIUC, what you are proposing feels a bit like traveling back in time when it\n> >> comes to the memory waste problem that Arm users encountered.\n> >>\n> >> Where do you see the big difference to 64k on Arm in your proposal? Would\n> >> you currently also be running 64k Arm in production and the memory waste etc\n> >> is acceptable?\n> >\n> > That's the point. I don't see a big difference to 64k Arm. I want to\n> > bring this option to x86: at some machine size it makes sense trade\n> > memory consumption for scalability. I am targeting it to machines with\n> > over 2TiB of RAM.\n> >\n> > BTW, we do run 64k Arm in our fleet. There's some growing pains, but it\n> > looks good in general We have no plans to switch to 4k (or 16k) at the\n> > moment. 512M THPs also look good on some workloads.\n>\n> Okay, that's valuable information, thanks!\n>\n> Being able to remove the sub-page mapping part (or being able to just\n> hide it somewhere deep down in arch code) would make this a lot easier\n> to digest.\n>\n> --\n> Cheers,\n>\n> David\n>\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "David (Arm)",
              "summary": "Right, I assume that they will have to be thought about that, and possibly, some binaries/libraries recompiled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/20/26 20:33, Kalesh Singh wrote:\n> On Fri, Feb 20, 2026 at 8:30\\u202fAM David Hildenbrand (Arm)\n> <david@kernel.org> wrote:\n>>\n>> On 2/20/26 13:07, Kiryl Shutsemau wrote:\n>>>\n>>> Well, it will drastically limit the adoption. We have too much legacy\n>>> stuff on x86.\n>>\n>> I'd assume that many applications nowadays can deal with differing page\n>> sizes (thanks to some other architectures paving the way).\n>>\n>> But yes, some real legacy stuff, or stuff that ever only cared about\n>> intel still hardcodes pagesize=4k.\n> \n> I think most issues will stem from linkers setting the default ELF\n> segment alignment (max-page-size) for x86 to 4096. So those ELFs will\n> not load correctly or at all on the larger emulated granularity.\n\nRight, I assume that they will have to be thought about that, and \npossibly, some binaries/libraries recompiled.\n\n-- \nCheers,\n\nDavid\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "I think backward compatibility is important and I believe we can get there without ABI break. And optimize from there. BTW, x86-64 SysV ABI allows for 64k page size: Systems are permitted to use any power-of-two page size between 4KB and 64KB, inclusive. But it doesn't work in practice.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 12:04:10PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/20/26 20:33, Kalesh Singh wrote:\n> > On Fri, Feb 20, 2026 at 8:30\\u202fAM David Hildenbrand (Arm)\n> > <david@kernel.org> wrote:\n> > > \n> > > On 2/20/26 13:07, Kiryl Shutsemau wrote:\n> > > > \n> > > > Well, it will drastically limit the adoption. We have too much legacy\n> > > > stuff on x86.\n> > > \n> > > I'd assume that many applications nowadays can deal with differing page\n> > > sizes (thanks to some other architectures paving the way).\n> > > \n> > > But yes, some real legacy stuff, or stuff that ever only cared about\n> > > intel still hardcodes pagesize=4k.\n> > \n> > I think most issues will stem from linkers setting the default ELF\n> > segment alignment (max-page-size) for x86 to 4096. So those ELFs will\n> > not load correctly or at all on the larger emulated granularity.\n> \n> Right, I assume that they will have to be thought about that, and possibly,\n> some binaries/libraries recompiled.\n\nI think backward compatibility is important and I believe we can get\nthere without ABI break. And optimize from there.\n\nBTW, x86-64 SysV ABI allows for 64k page size:\n\n\tSystems are permitted to use any power-of-two page size between\n\t4KB and 64KB, inclusive.\n\nBut it doesn't work in practice.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "David (Arm)",
              "summary": "Even in well controlled environments you would run in a hyperscaler?",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/23/26 12:13, Kiryl Shutsemau wrote:\n> On Mon, Feb 23, 2026 at 12:04:10PM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/20/26 20:33, Kalesh Singh wrote:\n>>> On Fri, Feb 20, 2026 at 8:30\\u202fAM David Hildenbrand (Arm)\n>>> <david@kernel.org> wrote:\n>>>\n>>> I think most issues will stem from linkers setting the default ELF\n>>> segment alignment (max-page-size) for x86 to 4096. So those ELFs will\n>>> not load correctly or at all on the larger emulated granularity.\n>>\n>> Right, I assume that they will have to be thought about that, and possibly,\n>> some binaries/libraries recompiled.\n> \n> I think backward compatibility is important and I believe we can get\n> there without ABI break. And optimize from there.\n> \n> BTW, x86-64 SysV ABI allows for 64k page size:\n> \n> \tSystems are permitted to use any power-of-two page size between\n> \t4KB and 64KB, inclusive.\n> \n> But it doesn't work in practice.\n\nEven in well controlled environments you would run in a hyperscaler?\n\n-- \nCheers,\n\nDavid\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "I have not invested much time into investigating this. I intentionally targeted compatible version assuming it will be better received by upstream. I want it to be usable outside specially cured userspace. 64k might not be good fit for a desktop, but 16k can be a different story.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 12:27:33PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/23/26 12:13, Kiryl Shutsemau wrote:\n> > On Mon, Feb 23, 2026 at 12:04:10PM +0100, David Hildenbrand (Arm) wrote:\n> > > On 2/20/26 20:33, Kalesh Singh wrote:\n> > > > On Fri, Feb 20, 2026 at 8:30\\u202fAM David Hildenbrand (Arm)\n> > > > <david@kernel.org> wrote:\n> > > > \n> > > > I think most issues will stem from linkers setting the default ELF\n> > > > segment alignment (max-page-size) for x86 to 4096. So those ELFs will\n> > > > not load correctly or at all on the larger emulated granularity.\n> > > \n> > > Right, I assume that they will have to be thought about that, and possibly,\n> > > some binaries/libraries recompiled.\n> > \n> > I think backward compatibility is important and I believe we can get\n> > there without ABI break. And optimize from there.\n> > \n> > BTW, x86-64 SysV ABI allows for 64k page size:\n> > \n> > \tSystems are permitted to use any power-of-two page size between\n> > \t4KB and 64KB, inclusive.\n> > \n> > But it doesn't work in practice.\n> \n> Even in well controlled environments you would run in a hyperscaler?\n\nI have not invested much time into investigating this.\n\nI intentionally targeted compatible version assuming it will be better\nreceived by upstream. I want it to be usable outside specially cured\nuserspace. 64k might not be good fit for a desktop, but 16k can be a\ndifferent story.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dave Hansen",
              "summary": "I think what Kirill is trying to say is that \"it breaks userspace\". ;) A hyperscaler (or other \"embedded\" environment) might be willing or able to go fix up userspace breakage. I would suspect our high frequency trading friends would be all over this if it shaved a microsecond off their receive times. The more important question is what it breaks and how badly it breaks things.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/23/26 03:27, David Hildenbrand (Arm) wrote:\n...\n>> BTW, x86-64 SysV ABI allows for 64k page size:\n>>\n>> Systems are permitted to use any power-of-two page size between\n>> 4KB and 64KB, inclusive.\n>>\n>> But it doesn't work in practice.\n> \n> Even in well controlled environments you would run in a hyperscaler?\n\nI think what Kirill is trying to say is that \"it breaks userspace\". ;)\n\nA hyperscaler (or other \"embedded\" environment) might be willing or able\nto go fix up userspace breakage. I would suspect our high frequency\ntrading friends would be all over this if it shaved a microsecond off\ntheir receive times.\n\nThe more important question is what it breaks and how badly it breaks\nthings. 5-level paging, for instance, broke some JITs that historically\nused the new (>48) upper virtual address bits for metadata. The gains\nfrom 5-level paging were big enough and the userspace breakage was\nconfined and fixable enough that 5-level paging was viable.\n\nI'm not sure which side a larger base page side will fall on, though. Is\nit going to be an out-of-tree hack that a few folks use, or will it be\nmore like 5-level paging and be good enough that it goes into mainline?\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "David (Arm)",
              "summary": "Yes. Probably similar to Intel proposing an actual 64k page size. Expected. :) Just thinking about VMAs spanning partial pages makes me shiver. Or A single page spanning multiple VMAs. I haven't seen the code yet, but I am certain that I will not like it. I'm happy to be proven wrong :)",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/23/26 16:14, Dave Hansen wrote:\n> On 2/23/26 03:27, David Hildenbrand (Arm) wrote:\n> ...\n>>> BTW, x86-64 SysV ABI allows for 64k page size:\n>>>\n>>>  Systems are permitted to use any power-of-two page size between\n>>>  4KB and 64KB, inclusive.\n>>>\n>>> But it doesn't work in practice.\n>>\n>> Even in well controlled environments you would run in a hyperscaler?\n> \n> I think what Kirill is trying to say is that \"it breaks userspace\". ;)\n\nYes. Probably similar to Intel proposing an actual 64k page size. \nExpected. :)\n\n> \n> A hyperscaler (or other \"embedded\" environment) might be willing or able\n> to go fix up userspace breakage. I would suspect our high frequency\n> trading friends would be all over this if it shaved a microsecond off\n> their receive times.\n> \n> The more important question is what it breaks and how badly it breaks\n> things. 5-level paging, for instance, broke some JITs that historically\n> used the new (>48) upper virtual address bits for metadata. The gains\n> from 5-level paging were big enough and the userspace breakage was\n> confined and fixable enough that 5-level paging was viable.\n> \n> I'm not sure which side a larger base page side will fall on, though. Is\n> it going to be an out-of-tree hack that a few folks use, or will it be\n> more like 5-level paging and be good enough that it goes into mainline?\n\nJust thinking about VMAs spanning partial pages makes me shiver. Or A \nsingle page spanning multiple VMAs.\n\nI haven't seen the code yet, but I am certain that I will not like it.\n\nI'm happy to be proven wrong :)\n\n-- \nCheers,\n\nDavid\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Hate to break it to you, but we have it now upstream :P THP can span multiple VMAs. And can be partially mapped. The only new thing is that we allow this for order-0 page now. And you cannot realistically recover wasted memory -- no deferred split. I will do my best, but no promises :)",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 04:31:56PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/23/26 16:14, Dave Hansen wrote:\n> > On 2/23/26 03:27, David Hildenbrand (Arm) wrote:\n> > ...\n> > > > BTW, x86-64 SysV ABI allows for 64k page size:\n> > > > \n> > > >  Systems are permitted to use any power-of-two page size between\n> > > >  4KB and 64KB, inclusive.\n> > > > \n> > > > But it doesn't work in practice.\n> > > \n> > > Even in well controlled environments you would run in a hyperscaler?\n> > \n> > I think what Kirill is trying to say is that \"it breaks userspace\". ;)\n> \n> Yes. Probably similar to Intel proposing an actual 64k page size. Expected.\n> :)\n> \n> > \n> > A hyperscaler (or other \"embedded\" environment) might be willing or able\n> > to go fix up userspace breakage. I would suspect our high frequency\n> > trading friends would be all over this if it shaved a microsecond off\n> > their receive times.\n> > \n> > The more important question is what it breaks and how badly it breaks\n> > things. 5-level paging, for instance, broke some JITs that historically\n> > used the new (>48) upper virtual address bits for metadata. The gains\n> > from 5-level paging were big enough and the userspace breakage was\n> > confined and fixable enough that 5-level paging was viable.\n> > \n> > I'm not sure which side a larger base page side will fall on, though. Is\n> > it going to be an out-of-tree hack that a few folks use, or will it be\n> > more like 5-level paging and be good enough that it goes into mainline?\n> \n> Just thinking about VMAs spanning partial pages makes me shiver. Or A\n> single page spanning multiple VMAs.\n\nHate to break it to you, but we have it now upstream :P\n\nTHP can span multiple VMAs. And can be partially mapped.\n\nThe only new thing is that we allow this for order-0 page now. And you\ncannot realistically recover wasted memory -- no deferred split.\n\n> I haven't seen the code yet, but I am certain that I will not like it.\n> \n> I'm happy to be proven wrong :)\n\nI will do my best, but no promises :)\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "David (Arm)",
              "summary": "Single mapcount, single anon-exclusive flag. Completely different story :P",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": ">>\n>> Just thinking about VMAs spanning partial pages makes me shiver. Or A\n>> single page spanning multiple VMAs.\n> \n> Hate to break it to you, but we have it now upstream :P\n> \n> THP can span multiple VMAs. And can be partially mapped.\n\nSingle mapcount, single anon-exclusive flag.\n\nCompletely different story :P\n\n-- \nCheers,\n\nDavid\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Leo Martins",
      "primary_email": "loemra.dev@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Mark Harmstone",
      "primary_email": "mark@harmstone.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix chunk map leak in btrfs_map_block() after btrfs_translate_remap()",
          "message_id": "20260220131002.6269-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260220131002.6269-1-mark@harmstone.com/",
          "date": "2026-02-20T13:10:10Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "If the call to btrfs_translate_remap() in btrfs_map_block() returns an error code, we were leaking the chunk map. Fix it by jumping to out rather than returning directly.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "It's a bug fix so this is more a Reported-by rather than a Suggested-by. Otherwise,",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 1:10â€¯PM Mark Harmstone <mark@harmstone.com> wrote:\n>\n> If the call to btrfs_translate_remap() in btrfs_map_block() returns an\n> error code, we were leaking the chunk map. Fix it by jumping to out\n> rather than returning directly.\n>\n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 18ba64992871 (\"btrfs: redirect I/O for remapped block groups\")\n> Suggested-by: Chris Mason <clm@fb.com>\n\nIt's a bug fix so this is more a Reported-by rather than a Suggested-by.\n\nOtherwise,\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\nThanks.\n\n> Link: https://lore.kernel.org/linux-btrfs/20260125125830.2352988-1-clm@meta.com/\n> ---\n>  fs/btrfs/volumes.c | 2 +-\n>  1 file changed, 1 insertion(+), 1 deletion(-)\n>\n> diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\n> index 83e2834ea273..1bd3464ccdd8 100644\n> --- a/fs/btrfs/volumes.c\n> +++ b/fs/btrfs/volumes.c\n> @@ -7082,7 +7082,7 @@ int btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n>\n>                 ret = btrfs_translate_remap(fs_info, &new_logical, length);\n>                 if (ret)\n> -                       return ret;\n> +                       goto out;\n>\n>                 if (new_logical != logical) {\n>                         btrfs_free_chunk_map(map);\n> --\n> 2.52.0\n>\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "David Sterba",
              "summary": "If it's a but then it's Reported-by Please sort the tags according to https://btrfs.readthedocs.io/en/latest/dev/Developer-s-FAQ.html#ordering as it's been for a long time and saves me editing each patch. Thanks.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:09:54PM +0000, Mark Harmstone wrote:\n> If the call to btrfs_translate_remap() in btrfs_map_block() returns an\n> error code, we were leaking the chunk map. Fix it by jumping to out\n> rather than returning directly.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 18ba64992871 (\"btrfs: redirect I/O for remapped block groups\")\n> Suggested-by: Chris Mason <clm@fb.com>\n\nIf it's a but then it's Reported-by\n\n> Link: https://lore.kernel.org/linux-btrfs/20260125125830.2352988-1-clm@meta.com/\n\nPlease sort the tags according to\nhttps://btrfs.readthedocs.io/en/latest/dev/Developer-s-FAQ.html#ordering\n\nas it's been for a long time and saves me editing each patch. Thanks.\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "Okay, I've edited the patches and pushed them with the tags in the right order.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 20/02/2026 3.58 pm, David Sterba wrote:\n> On Fri, Feb 20, 2026 at 01:09:54PM +0000, Mark Harmstone wrote:\n>> If the call to btrfs_translate_remap() in btrfs_map_block() returns an\n>> error code, we were leaking the chunk map. Fix it by jumping to out\n>> rather than returning directly.\n>>\n>> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n>> Fixes: 18ba64992871 (\"btrfs: redirect I/O for remapped block groups\")\n>> Suggested-by: Chris Mason <clm@fb.com>\n> \n> If it's a but then it's Reported-by\n> \n>> Link: https://lore.kernel.org/linux-btrfs/20260125125830.2352988-1-clm@meta.com/\n> \n> Please sort the tags according to\n> https://btrfs.readthedocs.io/en/latest/dev/Developer-s-FAQ.html#ordering\n> \n> as it's been for a long time and saves me editing each patch. Thanks.\n\nOkay, I've edited the patches and pushed them with the tags in the right \norder.\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix chunk map leak in btrfs_map_block() after btrfs_chunk_map_num_copies()",
          "message_id": "20260220130209.5020-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260220130209.5020-1-mark@harmstone.com/",
          "date": "2026-02-20T13:02:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Fix a chunk map leak in btrfs_map_block(): if we return early with -EINVAL, we're not freeing the chunk map that we've just looked up.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Side note: if there's a Fixes tag, we don't need a CC stable tag anymore nowadays.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 1:02â€¯PM Mark Harmstone <mark@harmstone.com> wrote:\n>\n> Fix a chunk map leak in btrfs_map_block(): if we return early with -EINVAL,\n> we're not freeing the chunk map that we've just looked up.\n>\n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Cc: stable@vger.kernel.org\n> Fixes: 0ae653fbec2b (\"btrfs: reduce chunk_map lookups in btrfs_map_block()\")\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\nSide note: if there's a Fixes tag, we don't need a CC stable tag\nanymore nowadays.\n\n\n\n> ---\n>  fs/btrfs/volumes.c | 6 ++++--\n>  1 file changed, 4 insertions(+), 2 deletions(-)\n>\n> diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\n> index 1bd3464ccdd8..a1f0fccd552c 100644\n> --- a/fs/btrfs/volumes.c\n> +++ b/fs/btrfs/volumes.c\n> @@ -7096,8 +7096,10 @@ int btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n>         }\n>\n>         num_copies = btrfs_chunk_map_num_copies(map);\n> -       if (io_geom.mirror_num > num_copies)\n> -               return -EINVAL;\n> +       if (io_geom.mirror_num > num_copies) {\n> +               ret = -EINVAL;\n> +               goto out;\n> +       }\n>\n>         map_offset = logical - map->start;\n>         io_geom.raid56_full_stripe_start = (u64)-1;\n> --\n> 2.52.0\n>\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Greg KH",
              "summary": "Not true at all, please read: https://www.kernel.org/doc/html/latest/process/stable-kernel-rules.html You HAVE to have a cc: stable if you know you want it to be added to a stable tree.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:07:27PM +0000, Filipe Manana wrote:\n> On Fri, Feb 20, 2026 at 1:02\\u202fPM Mark Harmstone <mark@harmstone.com> wrote:\n> >\n> > Fix a chunk map leak in btrfs_map_block(): if we return early with -EINVAL,\n> > we're not freeing the chunk map that we've just looked up.\n> >\n> > Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> > Cc: stable@vger.kernel.org\n> > Fixes: 0ae653fbec2b (\"btrfs: reduce chunk_map lookups in btrfs_map_block()\")\n> \n> Reviewed-by: Filipe Manana <fdmanana@suse.com>\n> \n> Side note: if there's a Fixes tag, we don't need a CC stable tag\n> anymore nowadays.\n\nNot true at all, please read:\n\n    https://www.kernel.org/doc/html/latest/process/stable-kernel-rules.html\n\nYou HAVE to have a cc: stable if you know you want it to be added to a\nstable tree.  If you do not do that, you are at the mercy of \"when Greg\nand Sasha get bored and attempt to pick up things that maintainers\nforgot about\".\n\nthanks,\n\ngreg k-h\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix chunk offset error message in check_dev_extent_item()",
          "message_id": "20260220113013.30254-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260220113013.30254-1-mark@harmstone.com/",
          "date": "2026-02-20T11:30:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Fix a copy-paste bug in an error message in check_dev_extent_item(): we're reporting an incorrect offset, but actually printing the objectid.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Qu Wenruo",
              "summary": "åœ¨ 2026/2/20 22:00, Mark Harmstone å†™é“:",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\nåœ¨ 2026/2/20 22:00, Mark Harmstone å†™é“:\n> Fix a copy-paste bug in an error message in check_dev_extent_item():\n> we're reporting an incorrect offset, but actually printing the objectid.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 008e2512dc56 (\"btrfs: tree-checker: add dev extent item checks\")\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\n\nThanks,\nQu\n\n> ---\n>   fs/btrfs/tree-checker.c | 2 +-\n>   1 file changed, 1 insertion(+), 1 deletion(-)\n> \n> diff --git a/fs/btrfs/tree-checker.c b/fs/btrfs/tree-checker.c\n> index ac4c4573ee39..133510f99fc5 100644\n> --- a/fs/btrfs/tree-checker.c\n> +++ b/fs/btrfs/tree-checker.c\n> @@ -1899,7 +1899,7 @@ static int check_dev_extent_item(const struct extent_buffer *leaf,\n>   \t\t\t\t sectorsize))) {\n>   \t\tgeneric_err(leaf, slot,\n>   \t\t\t    \"invalid dev extent chunk offset, has %llu not aligned to %u\",\n> -\t\t\t    btrfs_dev_extent_chunk_objectid(leaf, de),\n> +\t\t\t    btrfs_dev_extent_chunk_offset(leaf, de),\n>   \t\t\t    sectorsize);\n>   \t\treturn -EUCLEAN;\n>   \t}\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-21",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix unlikely in btrfs_insert_one_raid_extent()",
          "message_id": "20260218130006.9563-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260218130006.9563-1-mark@harmstone.com/",
          "date": "2026-02-18T13:00:13Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-18",
          "patch_summary": "Fix the unlikely added to btrfs_insert_one_raid_extent() by commit a929904c: the exclamation point is in the wrong place, so we are telling the compiler that allocation failure is actually expected.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "We use the Fixes tag for things that must be backported, which are aither bug fixes or rather severe performance regressions (i.e. things that affect users), and this doesn't fit into these categories. Otherwise:",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Wed, Feb 18, 2026 at 1:01â€¯PM Mark Harmstone <mark@harmstone.com> wrote:\n>\n> Fix the unlikely added to btrfs_insert_one_raid_extent() by commit\n> a929904c: the exclamation point is in the wrong place, so we are telling\n> the compiler that allocation failure is actually expected.\n>\n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: a929904cf73b (\"btrfs: add unlikely annotations to branches leading to transaction abort\")\n\nWe use the Fixes tag for things that must be backported, which are\naither bug fixes or rather severe performance regressions (i.e. things\nthat affect users), and this doesn't fit into these categories.\n\nOtherwise:\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\n\n> ---\n>  fs/btrfs/raid-stripe-tree.c | 2 +-\n>  1 file changed, 1 insertion(+), 1 deletion(-)\n>\n> diff --git a/fs/btrfs/raid-stripe-tree.c b/fs/btrfs/raid-stripe-tree.c\n> index 2987cb7c686e..638c4ad572c9 100644\n> --- a/fs/btrfs/raid-stripe-tree.c\n> +++ b/fs/btrfs/raid-stripe-tree.c\n> @@ -300,7 +300,7 @@ int btrfs_insert_one_raid_extent(struct btrfs_trans_handle *trans,\n>         int ret;\n>\n>         stripe_extent = kzalloc(item_size, GFP_NOFS);\n> -       if (!unlikely(stripe_extent)) {\n> +       if (unlikely(!stripe_extent)) {\n>                 btrfs_abort_transaction(trans, -ENOMEM);\n>                 btrfs_end_transaction(trans);\n>                 return -ENOMEM;\n> --\n> 2.52.0\n>\n>\n",
              "reply_to": "",
              "message_date": "2026-02-18",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "Okay - thanks Filipe, I'll push it with the Fixes line taken out.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 18/02/2026 2.08 pm, Filipe Manana wrote:\n> On Wed, Feb 18, 2026 at 1:01\\u202fPM Mark Harmstone <mark@harmstone.com> wrote:\n>>\n>> Fix the unlikely added to btrfs_insert_one_raid_extent() by commit\n>> a929904c: the exclamation point is in the wrong place, so we are telling\n>> the compiler that allocation failure is actually expected.\n>>\n>> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n>> Fixes: a929904cf73b (\"btrfs: add unlikely annotations to branches leading to transaction abort\")\n> \n> We use the Fixes tag for things that must be backported, which are\n> aither bug fixes or rather severe performance regressions (i.e. things\n> that affect users), and this doesn't fit into these categories.\n\nOkay - thanks Filipe, I'll push it with the Fixes line taken out.\n\n> Otherwise:\n> \n> Reviewed-by: Filipe Manana <fdmanana@suse.com>\n> \n> \n>> ---\n>>   fs/btrfs/raid-stripe-tree.c | 2 +-\n>>   1 file changed, 1 insertion(+), 1 deletion(-)\n>>\n>> diff --git a/fs/btrfs/raid-stripe-tree.c b/fs/btrfs/raid-stripe-tree.c\n>> index 2987cb7c686e..638c4ad572c9 100644\n>> --- a/fs/btrfs/raid-stripe-tree.c\n>> +++ b/fs/btrfs/raid-stripe-tree.c\n>> @@ -300,7 +300,7 @@ int btrfs_insert_one_raid_extent(struct btrfs_trans_handle *trans,\n>>          int ret;\n>>\n>>          stripe_extent = kzalloc(item_size, GFP_NOFS);\n>> -       if (!unlikely(stripe_extent)) {\n>> +       if (unlikely(!stripe_extent)) {\n>>                  btrfs_abort_transaction(trans, -ENOMEM);\n>>                  btrfs_end_transaction(trans);\n>>                  return -ENOMEM;\n>> --\n>> 2.52.0\n>>\n>>\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] btrfs: fix chunk map leak in btrfs_map_block() after btrfs_translate_remap()",
          "message_id": "85740194-bcd5-486f-b7a2-f31613f85c9f@harmstone.com",
          "url": "https://lore.kernel.org/all/85740194-bcd5-486f-b7a2-f31613f85c9f@harmstone.com/",
          "date": "2026-02-20T16:27:35Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] btrfs: fix unlikely in btrfs_insert_one_raid_extent()",
          "message_id": "6b37545b-80ee-4fef-bd55-5b6d9996716f@harmstone.com",
          "url": "https://lore.kernel.org/all/6b37545b-80ee-4fef-bd55-5b6d9996716f@harmstone.com/",
          "date": "2026-02-20T12:15:58Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Nhat Pham",
      "primary_email": "nphamcs@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] vswap: fix poor batching behavior of vswap free path",
          "message_id": "20260220210539.989603-1-nphamcs@gmail.com",
          "url": "https://lore.kernel.org/all/20260220210539.989603-1-nphamcs@gmail.com/",
          "date": "2026-02-20T22:54:20Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Kairui, could you apply this patch on top of the vswap series and run it on your test suite? It runs fairly well on my system (I actually rerun the benchmark on a different host to double check as well), but I'd love to get some data from your ends as well.\n\nIf there are serious discrepancies, could you also include your build config etc.? There might be differences in our setups, but since I managed to reproduce the free time regression on my first try I figured I should just fix it first :)\n\n---------------\n\nFix two issues that make the swap free path inefficient:\n\n1. At the PTE zapping step, we are unnecessarily resolving the backends, and fall back to batch size of 1, even though virtual swap infrastructure now already supports freeing of mixed backend ranges (as long the PTEs contain virtually contiguous swap slots). 2. Optimize vswap_free() by batching consecutive free operations, and avoid releasing locks unnecessarily (most notably, when we release non-disk-swap backends).\n\nPer a report from Kairui Song ([1]), I have run the following benchmark:",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 00/20] Virtual Swap Space",
          "message_id": "20260208223900.428408-1-nphamcs@gmail.com",
          "url": "https://lore.kernel.org/all/20260208223900.428408-1-nphamcs@gmail.com/",
          "date": "2026-02-09T00:32:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-09",
          "patch_summary": "My sincerest apologies - it seems like the cover letter (and just the cover letter) fails to be sent out, for some reason. I'm trying to figure out what happened - it works when I send the entire patch series to myself...\n\nAnyway, resending this (in-reply-to patch 1 of the series):",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Rik van Riel",
      "primary_email": "riel@surriel.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Shakeel Butt",
      "primary_email": "shakeel.butt@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] arm64: remove HAVE_CMPXCHG_LOCAL",
          "message_id": "aZjiHt7h2z3Ye81_@linux.dev",
          "url": "https://lore.kernel.org/all/aZjiHt7h2z3Ye81_@linux.dev/",
          "date": "2026-02-20T23:27:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 0/4] mm: zone lock tracepoint instrumentation",
          "message_id": "aZjg6PWn_xhZV7Nb@linux.dev",
          "url": "https://lore.kernel.org/all/aZjg6PWn_xhZV7Nb@linux.dev/",
          "date": "2026-02-20T22:36:39Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v5] mm: move pgscan, pgsteal, pgrefill to node stats",
          "message_id": "aZjdZv1eJc4HanML@linux.dev",
          "url": "https://lore.kernel.org/all/aZjdZv1eJc4HanML@linux.dev/",
          "date": "2026-02-20T22:19:03Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 2/2] mm: memcontrol: switch to native NR_VMALLOC vmstat counter",
          "message_id": "aZjdCfE1tww_WKwh@linux.dev",
          "url": "https://lore.kernel.org/all/aZjdCfE1tww_WKwh@linux.dev/",
          "date": "2026-02-20T22:16:03Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 1/2] mm: vmalloc: streamline vmalloc memory accounting",
          "message_id": "aZjaxAi-AzyOYzNT@linux.dev",
          "url": "https://lore.kernel.org/all/aZjaxAi-AzyOYzNT@linux.dev/",
          "date": "2026-02-20T22:09:35Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Johannes Weiner",
              "summary": "Good catch, my apologies. Serves me right for not compiling incrementally.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 02:09:28PM -0800, Shakeel Butt wrote:\n> On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> [...]\n> >  static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)\n> >  {\n> >  \tstruct rb_node *n = root->rb_node;\n> > @@ -3463,11 +3457,11 @@ void vfree(const void *addr)\n> >  \t\t * High-order allocs for huge vmallocs are split, so\n> >  \t\t * can be freed as an array of order-0 allocations\n> >  \t\t */\n> > +\t\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> > +\t\t\tdec_node_page_state(page, NR_VMALLOC);\n> >  \t\t__free_page(page);\n> >  \t\tcond_resched();\n> >  \t}\n> > -\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> > -\t\tatomic_long_sub(vm->nr_pages, &nr_vmalloc_pages);\n> >  \tkvfree(vm->pages);\n> >  \tkfree(vm);\n> >  }\n> > @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \t\t\tcontinue;\n> >  \t\t}\n> >  \n> > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> \n> mod_node_page_state() takes 'struct pglist_data *pgdat', you need to use\n> page_pgdat(page) as first param.\n\nGood catch, my apologies. Serves me right for not compiling\nincrementally.\n\n> With above fixes, you can add:\n> \n> Acked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\nThanks! I'll send out v2.\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Usama Arif",
      "primary_email": "usama.arif@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    }
  ]
}