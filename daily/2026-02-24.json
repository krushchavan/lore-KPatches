{
  "date": "2026-02-24",
  "report_file": "2026-02-24.html",
  "status": "complete",
  "last_updated": "2026-02-26 01:21 UTC",
  "llm_backends": [],
  "generation_time_seconds": 157.10112190246582,
  "developer_reports": [
    {
      "name": "Alexandre Ghiti",
      "primary_email": "alexghiti@rivosinc.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Boris Burkov",
      "primary_email": "boris@bur.io",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 1/1] btrfs: set BTRFS_ROOT_ORPHAN_CLEANUP during subvol create",
          "message_id": "14fc2404e55d99e9d3a4f95e3e825678dc2422a0.1771971643.git.boris@bur.io",
          "url": "https://lore.kernel.org/all/14fc2404e55d99e9d3a4f95e3e825678dc2422a0.1771971643.git.boris@bur.io/",
          "date": "2026-02-24T22:25:39Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "We have recently observed a number of subvolumes with broken dentries. ls-ing the parent dir looks like:\n\ndrwxrwxrwt 1 root root 16 Jan 23 16:49 . drwxr-xr-x 1 root root 24 Jan 23 16:48 .. d????????? ? ?    ?     ?            ? broken_subvol\n\nand similarly stat-ing the file fails.\n\nIn this state, deleting the subvol fails with ENOENT, but attempting to create a new file or subvol over it errors out with EEXIST and even aborts the fs. Which leaves us a bit stuck.\n\ndmesg contains a single notable error message reading: \"could not do orphan cleanup -2\"\n\n2 is ENOENT and the error comes from the failure handling path of btrfs_orphan_cleanup(), with the stack leading back up to btrfs_lookup().\n\nbtrfs_lookup btrfs_lookup_dentry btrfs_orphan_cleanup // prints that message and returns -ENOENT",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "So my previous comment from v1 still stands: \"Where does this decrement of parent->d_lockref.count happens exactly? I don't see it immediately in iput(), or iput_final(). Please put the full call chain.\"",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 10:25 PM Boris Burkov <boris@bur.io> wrote:\n>\n> We have recently observed a number of subvolumes with broken dentries.\n> ls-ing the parent dir looks like:\n>\n> drwxrwxrwt 1 root root 16 Jan 23 16:49 .\n> drwxr-xr-x 1 root root 24 Jan 23 16:48 ..\n> d????????? ? ?    ?     ?            ? broken_subvol\n>\n> and similarly stat-ing the file fails.\n>\n> In this state, deleting the subvol fails with ENOENT, but attempting to\n> create a new file or subvol over it errors out with EEXIST and even\n> aborts the fs. Which leaves us a bit stuck.\n>\n> dmesg contains a single notable error message reading:\n> \"could not do orphan cleanup -2\"\n>\n> 2 is ENOENT and the error comes from the failure handling path of\n> btrfs_orphan_cleanup(), with the stack leading back up to\n> btrfs_lookup().\n>\n> btrfs_lookup\n> btrfs_lookup_dentry\n> btrfs_orphan_cleanup // prints that message and returns -ENOENT\n>\n> After some detailed inspection of the internal state, it became clear\n> that:\n> - there are no orphan items for the subvol\n> - the subvol is otherwise healthy looking, it is not half-deleted or\n>   anything, there is no drop progress, etc.\n> - the subvol was created a while ago and does the meaningful first\n>   btrfs_orphan_cleanup() call that sets BTRFS_ROOT_ORPHAN_CLEANUP much\n>   later.\n> - after btrfs_orphan_cleanup() fails, btrfs_lookup_dentry() returns -ENOENT,\n>   which results in a negative dentry for the subvolume via\n>   d_splice_alias(NULL, dentry), leading to the observed behavior. The\n>   bug can be mitigated by dropping the dentry cache, at which point we\n>   can successfully delete the subvolume if we want.\n>\n> i.e.,\n> btrfs_lookup()\n>   btrfs_lookup_dentry()\n>     if (!sb_rdonly(inode->vfs_inode)->vfs_inode)\n>     btrfs_orphan_cleanup(sub_root)\n>       test_and_set_bit(BTRFS_ROOT_ORPHAN_CLEANUP)\n>       btrfs_search_slot() // finds orphan item for inode N\n>       ...\n>       prints \"could not do orphan cleanup -2\"\n>   if (inode == ERR_PTR(-ENOENT))\n>     inode = NULL;\n>   return d_splice_alias(NULL, dentry) // NEGATIVE DENTRY for valid subvolume\n>\n> btrfs_orphan_cleanup() does test_and_set_bit(BTRFS_ROOT_ORPHAN_CLEANUP)\n> on the root when it runs, so it cannot run more than once on a given\n> root, so something else must run concurrently. However, the obvious\n> routes to deleting an orphan when nlinks goes to 0 should not be able to\n> run without first doing a lookup into the subvolume, which should run\n> btrfs_orphan_cleanup() and set the bit.\n>\n> The final important observation is that create_subvol() calls\n> d_instantiate_new() but does not set BTRFS_ROOT_ORPHAN_CLEANUP, so if\n> the dentry cache gets dropped, the next lookup into the subvolume will\n> make a real call into btrfs_orphan_cleanup() for the first time. This\n> opens up the possibility of concurrently deleting the inode/orphan items\n> but most typical evict() paths will be holding a reference on the parent\n> dentry (child dentry holds parent->d_lockref.count via dget in\n> d_alloc(), released in __dentry_kill()) and prevent the parent from\n> being removed from the dentry cache.\n>\n> The one exception is delayed iputs. Ordered extent creation calls\n> igrab() on the inode. If the file is unlinked and closed while those\n> refs are held, iput() in __dentry_kill() decrements i_count but does\n> not trigger eviction (i_count > 0). The child dentry is freed and the\n> subvol dentry's d_lockref.count drops to 0, making it evictable while\n> the inode is still alive.\n>\n> Since there are two races (the race between writeback and unlink and\n> the race between lookup and delayed iputs), and there are too many moving\n> parts, the following three diagrams show the complete picture.\n> (Only the second and third are races)\n>\n> Phase 1:\n> Create Subvol in dentry cache without BTRFS_ROOT_ORPHAN_CLEANUP set\n>\n> btrfs_mksubvol()\n>   lookup_one_len()\n>     __lookup_slow()\n>       d_alloc_parallel()\n>         __d_alloc() // d_lockref.count = 1\n>   create_subvol(dentry)\n>     // doesn't touch the bit..\n>     d_instantiate_new(dentry, inode) // dentry in cache with d_lockref.count == 1\n>\n> Phase 2:\n> Create a delayed iput for a file in the subvol but leave the subvol in\n> state where its dentry can be evicted (d_lockref.count == 0)\n>\n> T1 (task)                    T2 (writeback)                   T3 (OE workqueue)\n>\n> write() // dirty pages\n>                               btrfs_writepages()\n>                                 btrfs_run_delalloc_range()\n>                                   cow_file_range()\n>                                     btrfs_alloc_ordered_extent()\n>                                       igrab() // i_count: 1 -> 2\n> btrfs_unlink_inode()\n>   btrfs_orphan_add()\n> close()\n>   __fput()\n>     dput()\n>       finish_dput()\n>         __dentry_kill()\n>           dentry_unlink_inode()\n>             iput() // 2 -> 1\n>           --parent->d_lockref.count // 1 -> 0; evictable\n\nSo my previous comment from v1 still stands:\n\n\"Where does this decrement of parent->d_lockref.count happens exactly?\n\nI don't see it immediately in iput(), or iput_final(). Please put the\nfull call chain.\"\n\nThanks.\n\n\n\n>                                                                 finish_ordered_fn()\n>                                                                   btrfs_finish_ordered_io()\n>                                                                     btrfs_put_ordered_extent()\n>                                                                       btrfs_add_delayed_iput()\n>\n> Phase 3:\n> Once the delayed iput is pending and the subvol dentry is evictable,\n> the shrinker can free it, causing the next lookup to go through\n> btrfs_lookup() and call btrfs_orphan_cleanup() for the first time.\n> If the cleaner kthread processes the delayed iput concurrently, the\n> two race:\n>\n>   T1 (shrinker)              T2 (cleaner kthread)                          T3 (lookup)\n>\n>   super_cache_scan()\n>     prune_dcache_sb()\n>       __dentry_kill()\n>       // subvol dentry freed\n>                               btrfs_run_delayed_iputs()\n>                                 iput()  // i_count -> 0\n>                                   evict()  // sets I_FREEING\n>                                     btrfs_evict_inode()\n>                                       // truncation loop\n>                                                                             btrfs_lookup()\n>                                                                               btrfs_lookup_dentry()\n>                                                                                 btrfs_orphan_cleanup()\n>                                                                                   // first call (bit never set)\n>                                                                                   btrfs_iget()\n>                                                                                     // blocks on I_FREEING\n>\n>                                       btrfs_orphan_del()\n>                                       // inode freed\n>                                                                                     // returns -ENOENT\n>                                                                                   btrfs_del_orphan_item()\n>                                                                                     // -ENOENT\n>                                                                                 // \"could not do orphan cleanup -2\"\n>                                                                             d_splice_alias(NULL, dentry)\n>                                                                             // negative dentry for valid subvol\n>\n> The most straightforward fix is to ensure the invariant that a dentry\n> for a subvolume can exist if and only if that subvolume has\n> BTRFS_ROOT_ORPHAN_CLEANUP set on its root (and is known to have no\n> orphans or ran btrfs_orphan_cleanup()).\n>\n> Signed-off-by: Boris Burkov <boris@bur.io>\n> ---\n> Changelog:\n> v2:\n> - fixed some typographical errors in the commit message.\n> - improved the commit message with more callstacks / details.\n>\n> ---\n>  fs/btrfs/ioctl.c | 7 +++++++\n>  1 file changed, 7 insertions(+)\n>\n> diff --git a/fs/btrfs/ioctl.c b/fs/btrfs/ioctl.c\n> index b8db877be61cc..77f7db18c6ca5 100644\n> --- a/fs/btrfs/ioctl.c\n> +++ b/fs/btrfs/ioctl.c\n> @@ -672,6 +672,13 @@ static noinline int create_subvol(struct mnt_idmap *idmap,\n>                 goto out;\n>         }\n>\n> +       /*\n> +        * Subvolumes have orphans cleaned on first dentry lookup. A new\n> +        * subvolume cannot have any orphans, so we should set the bit before we\n> +        * add the subvolume dentry to the dentry cache, so that it is in the\n> +        * same state as a subvolume after first lookup.\n> +        */\n> +       set_bit(BTRFS_ROOT_ORPHAN_CLEANUP, &new_root->state);\n>         d_instantiate_new(dentry, new_inode_args.inode);\n>         new_inode_args.inode = NULL;\n>\n> --\n> 2.47.3\n>\n>\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Boris Burkov (author)",
              "summary": "Sorry, I should have replied in greater detail. I added some callstack context above dput but didn't clarify anything about __dentry_kill where the real details are. On current for-next I see teh decrement at fs/dcache.c:690 in __dentry_kill() inside a conditional: if (parent && --parent->d_lockref.count) { ... } I have never figured out a perfect way to mix function calls and statements in these race diagrams with nesting and such, but I probably should have written out the conditional? I tried to have it nested at the \"stuff inside __dentry_kill level\" but after iput (which I also wanted to put to show the inode ref count) Let me know if you have any suggestions for how I can change it to make it more clear!",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Wed, Feb 25, 2026 at 12:21:43PM +0000, Filipe Manana wrote:\n> On Tue, Feb 24, 2026 at 10:25\\u202fPM Boris Burkov <boris@bur.io> wrote:\n> >\n> > We have recently observed a number of subvolumes with broken dentries.\n> > ls-ing the parent dir looks like:\n> >\n> > drwxrwxrwt 1 root root 16 Jan 23 16:49 .\n> > drwxr-xr-x 1 root root 24 Jan 23 16:48 ..\n> > d????????? ? ?    ?     ?            ? broken_subvol\n> >\n> > and similarly stat-ing the file fails.\n> >\n> > In this state, deleting the subvol fails with ENOENT, but attempting to\n> > create a new file or subvol over it errors out with EEXIST and even\n> > aborts the fs. Which leaves us a bit stuck.\n> >\n> > dmesg contains a single notable error message reading:\n> > \"could not do orphan cleanup -2\"\n> >\n> > 2 is ENOENT and the error comes from the failure handling path of\n> > btrfs_orphan_cleanup(), with the stack leading back up to\n> > btrfs_lookup().\n> >\n> > btrfs_lookup\n> > btrfs_lookup_dentry\n> > btrfs_orphan_cleanup // prints that message and returns -ENOENT\n> >\n> > After some detailed inspection of the internal state, it became clear\n> > that:\n> > - there are no orphan items for the subvol\n> > - the subvol is otherwise healthy looking, it is not half-deleted or\n> >   anything, there is no drop progress, etc.\n> > - the subvol was created a while ago and does the meaningful first\n> >   btrfs_orphan_cleanup() call that sets BTRFS_ROOT_ORPHAN_CLEANUP much\n> >   later.\n> > - after btrfs_orphan_cleanup() fails, btrfs_lookup_dentry() returns -ENOENT,\n> >   which results in a negative dentry for the subvolume via\n> >   d_splice_alias(NULL, dentry), leading to the observed behavior. The\n> >   bug can be mitigated by dropping the dentry cache, at which point we\n> >   can successfully delete the subvolume if we want.\n> >\n> > i.e.,\n> > btrfs_lookup()\n> >   btrfs_lookup_dentry()\n> >     if (!sb_rdonly(inode->vfs_inode)->vfs_inode)\n> >     btrfs_orphan_cleanup(sub_root)\n> >       test_and_set_bit(BTRFS_ROOT_ORPHAN_CLEANUP)\n> >       btrfs_search_slot() // finds orphan item for inode N\n> >       ...\n> >       prints \"could not do orphan cleanup -2\"\n> >   if (inode == ERR_PTR(-ENOENT))\n> >     inode = NULL;\n> >   return d_splice_alias(NULL, dentry) // NEGATIVE DENTRY for valid subvolume\n> >\n> > btrfs_orphan_cleanup() does test_and_set_bit(BTRFS_ROOT_ORPHAN_CLEANUP)\n> > on the root when it runs, so it cannot run more than once on a given\n> > root, so something else must run concurrently. However, the obvious\n> > routes to deleting an orphan when nlinks goes to 0 should not be able to\n> > run without first doing a lookup into the subvolume, which should run\n> > btrfs_orphan_cleanup() and set the bit.\n> >\n> > The final important observation is that create_subvol() calls\n> > d_instantiate_new() but does not set BTRFS_ROOT_ORPHAN_CLEANUP, so if\n> > the dentry cache gets dropped, the next lookup into the subvolume will\n> > make a real call into btrfs_orphan_cleanup() for the first time. This\n> > opens up the possibility of concurrently deleting the inode/orphan items\n> > but most typical evict() paths will be holding a reference on the parent\n> > dentry (child dentry holds parent->d_lockref.count via dget in\n> > d_alloc(), released in __dentry_kill()) and prevent the parent from\n> > being removed from the dentry cache.\n> >\n> > The one exception is delayed iputs. Ordered extent creation calls\n> > igrab() on the inode. If the file is unlinked and closed while those\n> > refs are held, iput() in __dentry_kill() decrements i_count but does\n> > not trigger eviction (i_count > 0). The child dentry is freed and the\n> > subvol dentry's d_lockref.count drops to 0, making it evictable while\n> > the inode is still alive.\n> >\n> > Since there are two races (the race between writeback and unlink and\n> > the race between lookup and delayed iputs), and there are too many moving\n> > parts, the following three diagrams show the complete picture.\n> > (Only the second and third are races)\n> >\n> > Phase 1:\n> > Create Subvol in dentry cache without BTRFS_ROOT_ORPHAN_CLEANUP set\n> >\n> > btrfs_mksubvol()\n> >   lookup_one_len()\n> >     __lookup_slow()\n> >       d_alloc_parallel()\n> >         __d_alloc() // d_lockref.count = 1\n> >   create_subvol(dentry)\n> >     // doesn't touch the bit..\n> >     d_instantiate_new(dentry, inode) // dentry in cache with d_lockref.count == 1\n> >\n> > Phase 2:\n> > Create a delayed iput for a file in the subvol but leave the subvol in\n> > state where its dentry can be evicted (d_lockref.count == 0)\n> >\n> > T1 (task)                    T2 (writeback)                   T3 (OE workqueue)\n> >\n> > write() // dirty pages\n> >                               btrfs_writepages()\n> >                                 btrfs_run_delalloc_range()\n> >                                   cow_file_range()\n> >                                     btrfs_alloc_ordered_extent()\n> >                                       igrab() // i_count: 1 -> 2\n> > btrfs_unlink_inode()\n> >   btrfs_orphan_add()\n> > close()\n> >   __fput()\n> >     dput()\n> >       finish_dput()\n> >         __dentry_kill()\n> >           dentry_unlink_inode()\n> >             iput() // 2 -> 1\n> >           --parent->d_lockref.count // 1 -> 0; evictable\n> \n> So my previous comment from v1 still stands:\n> \n> \"Where does this decrement of parent->d_lockref.count happens exactly?\n> \n> I don't see it immediately in iput(), or iput_final(). Please put the\n> full call chain.\"\n\nSorry, I should have replied in greater detail. I added some callstack\ncontext above dput but didn't clarify anything about __dentry_kill where\nthe real details are.\n\nOn current for-next I see teh decrement at fs/dcache.c:690 in\n__dentry_kill() inside a conditional:\n\n  if (parent && --parent->d_lockref.count) {\n  ...\n  }\n\nI have never figured out a perfect way to mix function calls and\nstatements in these race diagrams with nesting and such, but I probably\nshould have written out the conditional? I tried to have it nested at\nthe \"stuff inside __dentry_kill level\" but after iput (which I also\nwanted to put to show the inode ref count)\n\nLet me know if you have any suggestions for how I can change it to make\nit more clear!\n\nThanks,\nBoris\n\n> \n> Thanks.\n> \n> \n> \n> >                                                                 finish_ordered_fn()\n> >                                                                   btrfs_finish_ordered_io()\n> >                                                                     btrfs_put_ordered_extent()\n> >                                                                       btrfs_add_delayed_iput()\n> >\n> > Phase 3:\n> > Once the delayed iput is pending and the subvol dentry is evictable,\n> > the shrinker can free it, causing the next lookup to go through\n> > btrfs_lookup() and call btrfs_orphan_cleanup() for the first time.\n> > If the cleaner kthread processes the delayed iput concurrently, the\n> > two race:\n> >\n> >   T1 (shrinker)              T2 (cleaner kthread)                          T3 (lookup)\n> >\n> >   super_cache_scan()\n> >     prune_dcache_sb()\n> >       __dentry_kill()\n> >       // subvol dentry freed\n> >                               btrfs_run_delayed_iputs()\n> >                                 iput()  // i_count -> 0\n> >                                   evict()  // sets I_FREEING\n> >                                     btrfs_evict_inode()\n> >                                       // truncation loop\n> >                                                                             btrfs_lookup()\n> >                                                                               btrfs_lookup_dentry()\n> >                                                                                 btrfs_orphan_cleanup()\n> >                                                                                   // first call (bit never set)\n> >                                                                                   btrfs_iget()\n> >                                                                                     // blocks on I_FREEING\n> >\n> >                                       btrfs_orphan_del()\n> >                                       // inode freed\n> >                                                                                     // returns -ENOENT\n> >                                                                                   btrfs_del_orphan_item()\n> >                                                                                     // -ENOENT\n> >                                                                                 // \"could not do orphan cleanup -2\"\n> >                                                                             d_splice_alias(NULL, dentry)\n> >                                                                             // negative dentry for valid subvol\n> >\n> > The most straightforward fix is to ensure the invariant that a dentry\n> > for a subvolume can exist if and only if that subvolume has\n> > BTRFS_ROOT_ORPHAN_CLEANUP set on its root (and is known to have no\n> > orphans or ran btrfs_orphan_cleanup()).\n> >\n> > Signed-off-by: Boris Burkov <boris@bur.io>\n> > ---\n> > Changelog:\n> > v2:\n> > - fixed some typographical errors in the commit message.\n> > - improved the commit message with more callstacks / details.\n> >\n> > ---\n> >  fs/btrfs/ioctl.c | 7 +++++++\n> >  1 file changed, 7 insertions(+)\n> >\n> > diff --git a/fs/btrfs/ioctl.c b/fs/btrfs/ioctl.c\n> > index b8db877be61cc..77f7db18c6ca5 100644\n> > --- a/fs/btrfs/ioctl.c\n> > +++ b/fs/btrfs/ioctl.c\n> > @@ -672,6 +672,13 @@ static noinline int create_subvol(struct mnt_idmap *idmap,\n> >                 goto out;\n> >         }\n> >\n> > +       /*\n> > +        * Subvolumes have orphans cleaned on first dentry lookup. A new\n> > +        * subvolume cannot have any orphans, so we should set the bit before we\n> > +        * add the subvolume dentry to the dentry cache, so that it is in the\n> > +        * same state as a subvolume after first lookup.\n> > +        */\n> > +       set_bit(BTRFS_ROOT_ORPHAN_CLEANUP, &new_root->state);\n> >         d_instantiate_new(dentry, new_inode_args.inode);\n> >         new_inode_args.inode = NULL;\n> >\n> > --\n> > 2.47.3\n> >\n> >\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Filipe Manana",
              "summary": "Ok, you can add:",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Wed, Feb 25, 2026 at 5:08 PM Boris Burkov <boris@bur.io> wrote:\n>\n> On Wed, Feb 25, 2026 at 12:21:43PM +0000, Filipe Manana wrote:\n> > On Tue, Feb 24, 2026 at 10:25 PM Boris Burkov <boris@bur.io> wrote:\n> > >\n> > > We have recently observed a number of subvolumes with broken dentries.\n> > > ls-ing the parent dir looks like:\n> > >\n> > > drwxrwxrwt 1 root root 16 Jan 23 16:49 .\n> > > drwxr-xr-x 1 root root 24 Jan 23 16:48 ..\n> > > d????????? ? ?    ?     ?            ? broken_subvol\n> > >\n> > > and similarly stat-ing the file fails.\n> > >\n> > > In this state, deleting the subvol fails with ENOENT, but attempting to\n> > > create a new file or subvol over it errors out with EEXIST and even\n> > > aborts the fs. Which leaves us a bit stuck.\n> > >\n> > > dmesg contains a single notable error message reading:\n> > > \"could not do orphan cleanup -2\"\n> > >\n> > > 2 is ENOENT and the error comes from the failure handling path of\n> > > btrfs_orphan_cleanup(), with the stack leading back up to\n> > > btrfs_lookup().\n> > >\n> > > btrfs_lookup\n> > > btrfs_lookup_dentry\n> > > btrfs_orphan_cleanup // prints that message and returns -ENOENT\n> > >\n> > > After some detailed inspection of the internal state, it became clear\n> > > that:\n> > > - there are no orphan items for the subvol\n> > > - the subvol is otherwise healthy looking, it is not half-deleted or\n> > >   anything, there is no drop progress, etc.\n> > > - the subvol was created a while ago and does the meaningful first\n> > >   btrfs_orphan_cleanup() call that sets BTRFS_ROOT_ORPHAN_CLEANUP much\n> > >   later.\n> > > - after btrfs_orphan_cleanup() fails, btrfs_lookup_dentry() returns -ENOENT,\n> > >   which results in a negative dentry for the subvolume via\n> > >   d_splice_alias(NULL, dentry), leading to the observed behavior. The\n> > >   bug can be mitigated by dropping the dentry cache, at which point we\n> > >   can successfully delete the subvolume if we want.\n> > >\n> > > i.e.,\n> > > btrfs_lookup()\n> > >   btrfs_lookup_dentry()\n> > >     if (!sb_rdonly(inode->vfs_inode)->vfs_inode)\n> > >     btrfs_orphan_cleanup(sub_root)\n> > >       test_and_set_bit(BTRFS_ROOT_ORPHAN_CLEANUP)\n> > >       btrfs_search_slot() // finds orphan item for inode N\n> > >       ...\n> > >       prints \"could not do orphan cleanup -2\"\n> > >   if (inode == ERR_PTR(-ENOENT))\n> > >     inode = NULL;\n> > >   return d_splice_alias(NULL, dentry) // NEGATIVE DENTRY for valid subvolume\n> > >\n> > > btrfs_orphan_cleanup() does test_and_set_bit(BTRFS_ROOT_ORPHAN_CLEANUP)\n> > > on the root when it runs, so it cannot run more than once on a given\n> > > root, so something else must run concurrently. However, the obvious\n> > > routes to deleting an orphan when nlinks goes to 0 should not be able to\n> > > run without first doing a lookup into the subvolume, which should run\n> > > btrfs_orphan_cleanup() and set the bit.\n> > >\n> > > The final important observation is that create_subvol() calls\n> > > d_instantiate_new() but does not set BTRFS_ROOT_ORPHAN_CLEANUP, so if\n> > > the dentry cache gets dropped, the next lookup into the subvolume will\n> > > make a real call into btrfs_orphan_cleanup() for the first time. This\n> > > opens up the possibility of concurrently deleting the inode/orphan items\n> > > but most typical evict() paths will be holding a reference on the parent\n> > > dentry (child dentry holds parent->d_lockref.count via dget in\n> > > d_alloc(), released in __dentry_kill()) and prevent the parent from\n> > > being removed from the dentry cache.\n> > >\n> > > The one exception is delayed iputs. Ordered extent creation calls\n> > > igrab() on the inode. If the file is unlinked and closed while those\n> > > refs are held, iput() in __dentry_kill() decrements i_count but does\n> > > not trigger eviction (i_count > 0). The child dentry is freed and the\n> > > subvol dentry's d_lockref.count drops to 0, making it evictable while\n> > > the inode is still alive.\n> > >\n> > > Since there are two races (the race between writeback and unlink and\n> > > the race between lookup and delayed iputs), and there are too many moving\n> > > parts, the following three diagrams show the complete picture.\n> > > (Only the second and third are races)\n> > >\n> > > Phase 1:\n> > > Create Subvol in dentry cache without BTRFS_ROOT_ORPHAN_CLEANUP set\n> > >\n> > > btrfs_mksubvol()\n> > >   lookup_one_len()\n> > >     __lookup_slow()\n> > >       d_alloc_parallel()\n> > >         __d_alloc() // d_lockref.count = 1\n> > >   create_subvol(dentry)\n> > >     // doesn't touch the bit..\n> > >     d_instantiate_new(dentry, inode) // dentry in cache with d_lockref.count == 1\n> > >\n> > > Phase 2:\n> > > Create a delayed iput for a file in the subvol but leave the subvol in\n> > > state where its dentry can be evicted (d_lockref.count == 0)\n> > >\n> > > T1 (task)                    T2 (writeback)                   T3 (OE workqueue)\n> > >\n> > > write() // dirty pages\n> > >                               btrfs_writepages()\n> > >                                 btrfs_run_delalloc_range()\n> > >                                   cow_file_range()\n> > >                                     btrfs_alloc_ordered_extent()\n> > >                                       igrab() // i_count: 1 -> 2\n> > > btrfs_unlink_inode()\n> > >   btrfs_orphan_add()\n> > > close()\n> > >   __fput()\n> > >     dput()\n> > >       finish_dput()\n> > >         __dentry_kill()\n> > >           dentry_unlink_inode()\n> > >             iput() // 2 -> 1\n> > >           --parent->d_lockref.count // 1 -> 0; evictable\n> >\n> > So my previous comment from v1 still stands:\n> >\n> > \"Where does this decrement of parent->d_lockref.count happens exactly?\n> >\n> > I don't see it immediately in iput(), or iput_final(). Please put the\n> > full call chain.\"\n>\n> Sorry, I should have replied in greater detail. I added some callstack\n> context above dput but didn't clarify anything about __dentry_kill where\n> the real details are.\n>\n> On current for-next I see teh decrement at fs/dcache.c:690 in\n> __dentry_kill() inside a conditional:\n>\n>   if (parent && --parent->d_lockref.count) {\n>   ...\n>   }\n>\n> I have never figured out a perfect way to mix function calls and\n> statements in these race diagrams with nesting and such, but I probably\n> should have written out the conditional? I tried to have it nested at\n> the \"stuff inside __dentry_kill level\" but after iput (which I also\n> wanted to put to show the inode ref count)\n>\n> Let me know if you have any suggestions for how I can change it to make\n> it more clear!\n\nOk, you can add:\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\nThanks.\n\n>\n> Thanks,\n> Boris\n>\n> >\n> > Thanks.\n> >\n> >\n> >\n> > >                                                                 finish_ordered_fn()\n> > >                                                                   btrfs_finish_ordered_io()\n> > >                                                                     btrfs_put_ordered_extent()\n> > >                                                                       btrfs_add_delayed_iput()\n> > >\n> > > Phase 3:\n> > > Once the delayed iput is pending and the subvol dentry is evictable,\n> > > the shrinker can free it, causing the next lookup to go through\n> > > btrfs_lookup() and call btrfs_orphan_cleanup() for the first time.\n> > > If the cleaner kthread processes the delayed iput concurrently, the\n> > > two race:\n> > >\n> > >   T1 (shrinker)              T2 (cleaner kthread)                          T3 (lookup)\n> > >\n> > >   super_cache_scan()\n> > >     prune_dcache_sb()\n> > >       __dentry_kill()\n> > >       // subvol dentry freed\n> > >                               btrfs_run_delayed_iputs()\n> > >                                 iput()  // i_count -> 0\n> > >                                   evict()  // sets I_FREEING\n> > >                                     btrfs_evict_inode()\n> > >                                       // truncation loop\n> > >                                                                             btrfs_lookup()\n> > >                                                                               btrfs_lookup_dentry()\n> > >                                                                                 btrfs_orphan_cleanup()\n> > >                                                                                   // first call (bit never set)\n> > >                                                                                   btrfs_iget()\n> > >                                                                                     // blocks on I_FREEING\n> > >\n> > >                                       btrfs_orphan_del()\n> > >                                       // inode freed\n> > >                                                                                     // returns -ENOENT\n> > >                                                                                   btrfs_del_orphan_item()\n> > >                                                                                     // -ENOENT\n> > >                                                                                 // \"could not do orphan cleanup -2\"\n> > >                                                                             d_splice_alias(NULL, dentry)\n> > >                                                                             // negative dentry for valid subvol\n> > >\n> > > The most straightforward fix is to ensure the invariant that a dentry\n> > > for a subvolume can exist if and only if that subvolume has\n> > > BTRFS_ROOT_ORPHAN_CLEANUP set on its root (and is known to have no\n> > > orphans or ran btrfs_orphan_cleanup()).\n> > >\n> > > Signed-off-by: Boris Burkov <boris@bur.io>\n> > > ---\n> > > Changelog:\n> > > v2:\n> > > - fixed some typographical errors in the commit message.\n> > > - improved the commit message with more callstacks / details.\n> > >\n> > > ---\n> > >  fs/btrfs/ioctl.c | 7 +++++++\n> > >  1 file changed, 7 insertions(+)\n> > >\n> > > diff --git a/fs/btrfs/ioctl.c b/fs/btrfs/ioctl.c\n> > > index b8db877be61cc..77f7db18c6ca5 100644\n> > > --- a/fs/btrfs/ioctl.c\n> > > +++ b/fs/btrfs/ioctl.c\n> > > @@ -672,6 +672,13 @@ static noinline int create_subvol(struct mnt_idmap *idmap,\n> > >                 goto out;\n> > >         }\n> > >\n> > > +       /*\n> > > +        * Subvolumes have orphans cleaned on first dentry lookup. A new\n> > > +        * subvolume cannot have any orphans, so we should set the bit before we\n> > > +        * add the subvolume dentry to the dentry cache, so that it is in the\n> > > +        * same state as a subvolume after first lookup.\n> > > +        */\n> > > +       set_bit(BTRFS_ROOT_ORPHAN_CLEANUP, &new_root->state);\n> > >         d_instantiate_new(dentry, new_inode_args.inode);\n> > >         new_inode_args.inode = NULL;\n> > >\n> > > --\n> > > 2.47.3\n> > >\n> > >\n\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Dmitry Ilvokhin",
      "primary_email": "d@ilvokhin.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 4/4] mm: add tracepoints for zone lock",
          "message_id": "1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Add tracepoint instrumentation to zone lock acquire/release operations via the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a lightweight inline helper checks whether the tracepoint is enabled and calls into an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 3/4] mm: convert compaction to zone lock wrappers",
          "message_id": "3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Compaction uses compact_lock_irqsave(), which currently operates on a raw spinlock_t pointer so that it can be used for both zone->lock and lru_lock. Since zone lock operations are now wrapped, compact_lock_irqsave() can no longer operate directly on a spinlock_t when the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The structure carries a lock type enum and a union holding either a zone pointer or a raw spinlock_t pointer, and dispatches to the appropriate lock/unlock helper.\n\nNo functional change intended.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Cheatham, Benjamin",
              "summary": "Nit: You could remove the helpers above and just do the calls directly in this function, though it would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay since they have the __acquires() annotations. You don't need the return statement here (and you shouldn't be returning a value at all). It may be cleaner to just do an if-else statement here instead. I would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you didn't change it due to location but I would argue it isn't really relevant to what's being added in this patch and fits better in the last.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "nits"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> Compaction uses compact_lock_irqsave(), which currently operates\n> on a raw spinlock_t pointer so that it can be used for both\n> zone->lock and lru_lock. Since zone lock operations are now wrapped,\n> compact_lock_irqsave() can no longer operate directly on a spinlock_t\n> when the lock belongs to a zone.\n> \n> Introduce struct compact_lock to abstract the underlying lock type. The\n> structure carries a lock type enum and a union holding either a zone\n> pointer or a raw spinlock_t pointer, and dispatches to the appropriate\n> lock/unlock helper.\n> \n> No functional change intended.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> ---\n>  mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n>  1 file changed, 89 insertions(+), 19 deletions(-)\n> \n> diff --git a/mm/compaction.c b/mm/compaction.c\n> index 1e8f8eca318c..1b000d2b95b2 100644\n> --- a/mm/compaction.c\n> +++ b/mm/compaction.c\n> @@ -24,6 +24,7 @@\n>  #include <linux/page_owner.h>\n>  #include <linux/psi.h>\n>  #include <linux/cpuset.h>\n> +#include <linux/zone_lock.h>\n>  #include \"internal.h\"\n>  \n>  #ifdef CONFIG_COMPACTION\n> @@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n>  }\n>  #endif /* CONFIG_COMPACTION */\n>  \n> +enum compact_lock_type {\n> +\tCOMPACT_LOCK_ZONE,\n> +\tCOMPACT_LOCK_RAW_SPINLOCK,\n> +};\n> +\n> +struct compact_lock {\n> +\tenum compact_lock_type type;\n> +\tunion {\n> +\t\tstruct zone *zone;\n> +\t\tspinlock_t *lock; /* Reference to lru lock */\n> +\t};\n> +};\n> +\n> +static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n> +\t\t\t\t\t    unsigned long *flags)\n> +{\n> +\treturn zone_trylock_irqsave(zone, *flags);\n> +}\n> +\n> +static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n> +\t\t\t\t\t   unsigned long *flags)\n> +{\n> +\treturn spin_trylock_irqsave(lock, *flags);\n> +}\n> +\n> +static bool compact_do_trylock_irqsave(struct compact_lock lock,\n> +\t\t\t\t       unsigned long *flags)\n> +{\n> +\tif (lock.type == COMPACT_LOCK_ZONE)\n> +\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n> +\n> +\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n> +}\n\nNit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n> +\n> +static void compact_do_zone_lock_irqsave(struct zone *zone,\n> +\t\t\t\t\t unsigned long *flags)\n> +__acquires(zone->lock)\n> +{\n> +\tzone_lock_irqsave(zone, *flags);\n> +}\n> +\n> +static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n> +\t\t\t\t\tunsigned long *flags)\n> +__acquires(lock)\n> +{\n> +\tspin_lock_irqsave(lock, *flags);\n> +}\n> +\n> +static void compact_do_lock_irqsave(struct compact_lock lock,\n> +\t\t\t\t    unsigned long *flags)\n> +{\n> +\tif (lock.type == COMPACT_LOCK_ZONE) {\n> +\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n> +\t\treturn;\n> +\t}\n> +\n> +\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n> +}\n> +\n>  /*\n>   * Compaction requires the taking of some coarse locks that are potentially\n>   * very heavily contended. For async compaction, trylock and record if the\n> @@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n>   *\n>   * Always returns true which makes it easier to track lock state in callers.\n>   */\n> -static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n> -\t\t\t\t\t\tstruct compact_control *cc)\n> -\t__acquires(lock)\n> +static bool compact_lock_irqsave(struct compact_lock lock,\n> +\t\t\t\t unsigned long *flags,\n> +\t\t\t\t struct compact_control *cc)\n>  {\n>  \t/* Track if the lock is contended in async mode */\n>  \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n> -\t\tif (spin_trylock_irqsave(lock, *flags))\n> +\t\tif (compact_do_trylock_irqsave(lock, flags))\n>  \t\t\treturn true;\n>  \n>  \t\tcc->contended = true;\n>  \t}\n>  \n> -\tspin_lock_irqsave(lock, *flags);\n> +\tcompact_do_lock_irqsave(lock, flags);\n>  \treturn true;\n>  }\n>  \n> @@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n>   * Returns true if compaction should abort due to fatal signal pending.\n>   * Returns false when compaction can continue.\n>   */\n> -static bool compact_unlock_should_abort(spinlock_t *lock,\n> -\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n> +static bool compact_unlock_should_abort(struct zone *zone,\n> +\t\t\t\t\tunsigned long flags,\n> +\t\t\t\t\tbool *locked,\n> +\t\t\t\t\tstruct compact_control *cc)\n>  {\n>  \tif (*locked) {\n> -\t\tspin_unlock_irqrestore(lock, flags);\n> +\t\tzone_unlock_irqrestore(zone, flags);\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.\n\n>  \t\t*locked = false;\n>  \t}\n>  \n> @@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n>  \t\t * contention, to give chance to IRQs. Abort if fatal signal\n>  \t\t * pending.\n>  \t\t */\n> -\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n> -\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n> -\t\t\t\t\t\t\t\t&locked, cc))\n> +\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n> +\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n>  \t\t\tbreak;\n>  \n>  \t\tnr_scanned++;\n> @@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n>  \n>  \t\t/* If we already hold the lock, we can skip some rechecking. */\n>  \t\tif (!locked) {\n> -\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n> -\t\t\t\t\t\t\t\t&flags, cc);\n> +\t\t\tstruct compact_lock zol = {\n> +\t\t\t\t.type = COMPACT_LOCK_ZONE,\n> +\t\t\t\t.zone = cc->zone,\n> +\t\t\t};\n> +\n> +\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n>  \n>  \t\t\t/* Recheck this is a buddy page under lock */\n>  \t\t\tif (!PageBuddy(page))\n> @@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n>  \t}\n>  \n>  \tif (locked)\n> -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> +\t\tzone_unlock_irqrestore(cc->zone, flags);\n>  \n>  \t/*\n>  \t * Be careful to not go outside of the pageblock.\n> @@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n>  \n>  \t\t/* If we already hold the lock, we can skip some rechecking */\n>  \t\tif (lruvec != locked) {\n> +\t\t\tstruct compact_lock zol = {\n> +\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n> +\t\t\t\t.lock = &lruvec->lru_lock,\n> +\t\t\t};\n> +\n>  \t\t\tif (locked)\n>  \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n>  \n> -\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n> +\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n>  \t\t\tlocked = lruvec;\n>  \n>  \t\t\tlruvec_memcg_debug(lruvec, folio);\n> @@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n>  \t\tif (!area->nr_free)\n>  \t\t\tcontinue;\n>  \n> -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> +\t\tzone_lock_irqsave(cc->zone, flags);\n>  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n>  \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n>  \t\t\tunsigned long pfn;\n> @@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n>  \t\t\t}\n>  \t\t}\n>  \n> -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> +\t\tzone_unlock_irqrestore(cc->zone, flags);\n>  \n>  \t\t/* Skip fast search if enough freepages isolated */\n>  \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n> @@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n>  \t\tif (!area->nr_free)\n>  \t\t\tcontinue;\n>  \n> -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> +\t\tzone_lock_irqsave(cc->zone, flags);\n>  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n>  \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n>  \t\t\tunsigned long free_pfn;\n> @@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n>  \t\t\t\tbreak;\n>  \t\t\t}\n>  \t\t}\n> -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> +\t\tzone_unlock_irqrestore(cc->zone, flags);\n>  \t}\n>  \n>  \tcc->total_migrate_scanned += nr_scanned;\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Yes, I agree, there is no much value in this wrappers, will remove them, Yes, agree, will fix in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "nits"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:10:05PM -0600, Cheatham, Benjamin wrote:\n> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> > Compaction uses compact_lock_irqsave(), which currently operates\n> > on a raw spinlock_t pointer so that it can be used for both\n> > zone->lock and lru_lock. Since zone lock operations are now wrapped,\n> > compact_lock_irqsave() can no longer operate directly on a spinlock_t\n> > when the lock belongs to a zone.\n> > \n> > Introduce struct compact_lock to abstract the underlying lock type. The\n> > structure carries a lock type enum and a union holding either a zone\n> > pointer or a raw spinlock_t pointer, and dispatches to the appropriate\n> > lock/unlock helper.\n> > \n> > No functional change intended.\n> > \n> > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > ---\n> >  mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n> >  1 file changed, 89 insertions(+), 19 deletions(-)\n> > \n> > diff --git a/mm/compaction.c b/mm/compaction.c\n> > index 1e8f8eca318c..1b000d2b95b2 100644\n> > --- a/mm/compaction.c\n> > +++ b/mm/compaction.c\n> > @@ -24,6 +24,7 @@\n> >  #include <linux/page_owner.h>\n> >  #include <linux/psi.h>\n> >  #include <linux/cpuset.h>\n> > +#include <linux/zone_lock.h>\n> >  #include \"internal.h\"\n> >  \n> >  #ifdef CONFIG_COMPACTION\n> > @@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n> >  }\n> >  #endif /* CONFIG_COMPACTION */\n> >  \n> > +enum compact_lock_type {\n> > +\tCOMPACT_LOCK_ZONE,\n> > +\tCOMPACT_LOCK_RAW_SPINLOCK,\n> > +};\n> > +\n> > +struct compact_lock {\n> > +\tenum compact_lock_type type;\n> > +\tunion {\n> > +\t\tstruct zone *zone;\n> > +\t\tspinlock_t *lock; /* Reference to lru lock */\n> > +\t};\n> > +};\n> > +\n> > +static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n> > +\t\t\t\t\t    unsigned long *flags)\n> > +{\n> > +\treturn zone_trylock_irqsave(zone, *flags);\n> > +}\n> > +\n> > +static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n> > +\t\t\t\t\t   unsigned long *flags)\n> > +{\n> > +\treturn spin_trylock_irqsave(lock, *flags);\n> > +}\n> > +\n> > +static bool compact_do_trylock_irqsave(struct compact_lock lock,\n> > +\t\t\t\t       unsigned long *flags)\n> > +{\n> > +\tif (lock.type == COMPACT_LOCK_ZONE)\n> > +\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n> > +\n> > +\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n> > +}\n> \n> Nit: You could remove the helpers above and just do the calls directly in this function, though\n> it would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\n> since they have the __acquires() annotations.\n\nYes, I agree, there is no much value in this wrappers, will remove them,\nthanks!\n\n> > +\n> > +static void compact_do_zone_lock_irqsave(struct zone *zone,\n> > +\t\t\t\t\t unsigned long *flags)\n> > +__acquires(zone->lock)\n> > +{\n> > +\tzone_lock_irqsave(zone, *flags);\n> > +}\n> > +\n> > +static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n> > +\t\t\t\t\tunsigned long *flags)\n> > +__acquires(lock)\n> > +{\n> > +\tspin_lock_irqsave(lock, *flags);\n> > +}\n> > +\n> > +static void compact_do_lock_irqsave(struct compact_lock lock,\n> > +\t\t\t\t    unsigned long *flags)\n> > +{\n> > +\tif (lock.type == COMPACT_LOCK_ZONE) {\n> > +\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n> > +\t\treturn;\n> > +\t}\n> > +\n> > +\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n> \n> You don't need the return statement here (and you shouldn't be returning a value at all).\n\nYes, agree, will fix in v2.\n\n> \n> It may be cleaner to just do an if-else statement here instead.\n> \n> > +}\n> > +\n> >  /*\n> >   * Compaction requires the taking of some coarse locks that are potentially\n> >   * very heavily contended. For async compaction, trylock and record if the\n> > @@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n> >   *\n> >   * Always returns true which makes it easier to track lock state in callers.\n> >   */\n> > -static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n> > -\t\t\t\t\t\tstruct compact_control *cc)\n> > -\t__acquires(lock)\n> > +static bool compact_lock_irqsave(struct compact_lock lock,\n> > +\t\t\t\t unsigned long *flags,\n> > +\t\t\t\t struct compact_control *cc)\n> >  {\n> >  \t/* Track if the lock is contended in async mode */\n> >  \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n> > -\t\tif (spin_trylock_irqsave(lock, *flags))\n> > +\t\tif (compact_do_trylock_irqsave(lock, flags))\n> >  \t\t\treturn true;\n> >  \n> >  \t\tcc->contended = true;\n> >  \t}\n> >  \n> > -\tspin_lock_irqsave(lock, *flags);\n> > +\tcompact_do_lock_irqsave(lock, flags);\n> >  \treturn true;\n> >  }\n> >  \n> > @@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n> >   * Returns true if compaction should abort due to fatal signal pending.\n> >   * Returns false when compaction can continue.\n> >   */\n> > -static bool compact_unlock_should_abort(spinlock_t *lock,\n> > -\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n> > +static bool compact_unlock_should_abort(struct zone *zone,\n> > +\t\t\t\t\tunsigned long flags,\n> > +\t\t\t\t\tbool *locked,\n> > +\t\t\t\t\tstruct compact_control *cc)\n> >  {\n> >  \tif (*locked) {\n> > -\t\tspin_unlock_irqrestore(lock, flags);\n> > +\t\tzone_unlock_irqrestore(zone, flags);\n> \n> I would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\n> didn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\n> and fits better in the last.\n\nThanks for the suggestion. Totally makes sense to me, will do in v2 as well.\n\n> \n> >  \t\t*locked = false;\n> >  \t}\n> >  \n> > @@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n> >  \t\t * contention, to give chance to IRQs. Abort if fatal signal\n> >  \t\t * pending.\n> >  \t\t */\n> > -\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n> > -\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n> > -\t\t\t\t\t\t\t\t&locked, cc))\n> > +\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n> > +\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n> >  \t\t\tbreak;\n> >  \n> >  \t\tnr_scanned++;\n> > @@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n> >  \n> >  \t\t/* If we already hold the lock, we can skip some rechecking. */\n> >  \t\tif (!locked) {\n> > -\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n> > -\t\t\t\t\t\t\t\t&flags, cc);\n> > +\t\t\tstruct compact_lock zol = {\n> > +\t\t\t\t.type = COMPACT_LOCK_ZONE,\n> > +\t\t\t\t.zone = cc->zone,\n> > +\t\t\t};\n> > +\n> > +\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n> >  \n> >  \t\t\t/* Recheck this is a buddy page under lock */\n> >  \t\t\tif (!PageBuddy(page))\n> > @@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n> >  \t}\n> >  \n> >  \tif (locked)\n> > -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> > +\t\tzone_unlock_irqrestore(cc->zone, flags);\n> >  \n> >  \t/*\n> >  \t * Be careful to not go outside of the pageblock.\n> > @@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n> >  \n> >  \t\t/* If we already hold the lock, we can skip some rechecking */\n> >  \t\tif (lruvec != locked) {\n> > +\t\t\tstruct compact_lock zol = {\n> > +\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n> > +\t\t\t\t.lock = &lruvec->lru_lock,\n> > +\t\t\t};\n> > +\n> >  \t\t\tif (locked)\n> >  \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n> >  \n> > -\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n> > +\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n> >  \t\t\tlocked = lruvec;\n> >  \n> >  \t\t\tlruvec_memcg_debug(lruvec, folio);\n> > @@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n> >  \t\tif (!area->nr_free)\n> >  \t\t\tcontinue;\n> >  \n> > -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> > +\t\tzone_lock_irqsave(cc->zone, flags);\n> >  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n> >  \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n> >  \t\t\tunsigned long pfn;\n> > @@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n> >  \t\t\t}\n> >  \t\t}\n> >  \n> > -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> > +\t\tzone_unlock_irqrestore(cc->zone, flags);\n> >  \n> >  \t\t/* Skip fast search if enough freepages isolated */\n> >  \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n> > @@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n> >  \t\tif (!area->nr_free)\n> >  \t\t\tcontinue;\n> >  \n> > -\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n> > +\t\tzone_lock_irqsave(cc->zone, flags);\n> >  \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n> >  \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n> >  \t\t\tunsigned long free_pfn;\n> > @@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n> >  \t\t\t\tbreak;\n> >  \t\t\t}\n> >  \t\t}\n> > -\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n> > +\t\tzone_unlock_irqrestore(cc->zone, flags);\n> >  \t}\n> >  \n> >  \tcc->total_migrate_scanned += nr_scanned;\n> \n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/4] mm: zone lock tracepoint instrumentation",
          "message_id": "cover.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/cover.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Zone lock contention can significantly impact allocation and reclaim latency, as it is a central synchronization point in the page allocator and reclaim paths. Improved visibility into its behavior is therefore important for diagnosing performance issues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable zone lock contention. Deeper analysis of lock holders and waiters is currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints cover the slow path, they do not provide sufficient visibility into lock hold times. In particular, the lack of a release-side event makes it difficult to identify long lock holders and correlate them with waiters. As a result, distinguishing between short bursts of contention and pathological long hold times requires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to zone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock hold time measurements without affecting the fast path when tracing is disabled.\n\nThe series is structured as follows:\n\n1. Introduce zone lock wrappers. 2. Mechanically convert zone lock users to the wrappers. 3. Convert compaction to use the wrappers (requires minor restructuring of compact_lock_irqsave()). 4. Add zone lock tracepoints.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Cheatham, Benjamin",
              "summary": "I think you can improve the flow of this series if reorder as follows: 1. Introduce zone lock wrappers 4. Add zone lock tracepoints 2. Mechanically convert zone lock users to the wrappers 3. Convert compaction to use the wrappers... and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in patch 1 by the time they get to patch 4.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> Zone lock contention can significantly impact allocation and\n> reclaim latency, as it is a central synchronization point in\n> the page allocator and reclaim paths. Improved visibility into\n> its behavior is therefore important for diagnosing performance\n> issues in memory-intensive workloads.\n> \n> On some production workloads at Meta, we have observed noticeable\n> zone lock contention. Deeper analysis of lock holders and waiters\n> is currently difficult with existing instrumentation.\n> \n> While generic lock contention_begin/contention_end tracepoints\n> cover the slow path, they do not provide sufficient visibility\n> into lock hold times. In particular, the lack of a release-side\n> event makes it difficult to identify long lock holders and\n> correlate them with waiters. As a result, distinguishing between\n> short bursts of contention and pathological long hold times\n> requires additional instrumentation.\n> \n> This patch series adds dedicated tracepoint instrumentation to\n> zone lock, following the existing mmap_lock tracing model.\n> \n> The goal is to enable detailed holder/waiter analysis and lock\n> hold time measurements without affecting the fast path when\n> tracing is disabled.\n> \n> The series is structured as follows:\n> \n>   1. Introduce zone lock wrappers.\n>   2. Mechanically convert zone lock users to the wrappers.\n>   3. Convert compaction to use the wrappers (requires minor\n>      restructuring of compact_lock_irqsave()).\n>   4. Add zone lock tracepoints.\n\nI think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "I don't think this suggestion will make anything better. This just seems like a different taste. If I make a suggestion, I would request to squash (1) and (2) i.e. patch containing wrappers and their use together but that is just my taste and would be a nit. The series ordering is good as is.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:\n> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> > Zone lock contention can significantly impact allocation and\n> > reclaim latency, as it is a central synchronization point in\n> > the page allocator and reclaim paths. Improved visibility into\n> > its behavior is therefore important for diagnosing performance\n> > issues in memory-intensive workloads.\n> > \n> > On some production workloads at Meta, we have observed noticeable\n> > zone lock contention. Deeper analysis of lock holders and waiters\n> > is currently difficult with existing instrumentation.\n> > \n> > While generic lock contention_begin/contention_end tracepoints\n> > cover the slow path, they do not provide sufficient visibility\n> > into lock hold times. In particular, the lack of a release-side\n> > event makes it difficult to identify long lock holders and\n> > correlate them with waiters. As a result, distinguishing between\n> > short bursts of contention and pathological long hold times\n> > requires additional instrumentation.\n> > \n> > This patch series adds dedicated tracepoint instrumentation to\n> > zone lock, following the existing mmap_lock tracing model.\n> > \n> > The goal is to enable detailed holder/waiter analysis and lock\n> > hold time measurements without affecting the fast path when\n> > tracing is disabled.\n> > \n> > The series is structured as follows:\n> > \n> >   1. Introduce zone lock wrappers.\n> >   2. Mechanically convert zone lock users to the wrappers.\n> >   3. Convert compaction to use the wrappers (requires minor\n> >      restructuring of compact_lock_irqsave()).\n> >   4. Add zone lock tracepoints.\n> \n> I think you can improve the flow of this series if reorder as follows:\n> \t1. Introduce zone lock wrappers\n> \t4. Add zone lock tracepoints\n> \t2. Mechanically convert zone lock users to the wrappers\n> \t3. Convert compaction to use the wrappers...\n> \n> and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\n> wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\n> patch 1 by the time they get to patch 4.\n\nI don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "I structured the series intentionally to keep all behavior-preserving refactoring separate from the actual instrumentation change. In particular, I had to split the conversion into two patches to separate the purely mechanical changes from the compaction restructuring. With the current order, tracepoints addition remains a single, atomic functional change on top of a fully converted tree. This keeps the instrumentation isolated from the refactoring and with an intention to make bisection and review of the behavioral change easier. Reordering as suggested would mix instrumentation with intermediate refactoring states, which I'd prefer to avoid. I hope this reasoning makes sense, but I'm happy to discuss if there are strong objections.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:\n> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> > Zone lock contention can significantly impact allocation and\n> > reclaim latency, as it is a central synchronization point in\n> > the page allocator and reclaim paths. Improved visibility into\n> > its behavior is therefore important for diagnosing performance\n> > issues in memory-intensive workloads.\n> > \n> > On some production workloads at Meta, we have observed noticeable\n> > zone lock contention. Deeper analysis of lock holders and waiters\n> > is currently difficult with existing instrumentation.\n> > \n> > While generic lock contention_begin/contention_end tracepoints\n> > cover the slow path, they do not provide sufficient visibility\n> > into lock hold times. In particular, the lack of a release-side\n> > event makes it difficult to identify long lock holders and\n> > correlate them with waiters. As a result, distinguishing between\n> > short bursts of contention and pathological long hold times\n> > requires additional instrumentation.\n> > \n> > This patch series adds dedicated tracepoint instrumentation to\n> > zone lock, following the existing mmap_lock tracing model.\n> > \n> > The goal is to enable detailed holder/waiter analysis and lock\n> > hold time measurements without affecting the fast path when\n> > tracing is disabled.\n> > \n> > The series is structured as follows:\n> > \n> >   1. Introduce zone lock wrappers.\n> >   2. Mechanically convert zone lock users to the wrappers.\n> >   3. Convert compaction to use the wrappers (requires minor\n> >      restructuring of compact_lock_irqsave()).\n> >   4. Add zone lock tracepoints.\n> \n> I think you can improve the flow of this series if reorder as follows:\n> \t1. Introduce zone lock wrappers\n> \t4. Add zone lock tracepoints\n> \t2. Mechanically convert zone lock users to the wrappers\n> \t3. Convert compaction to use the wrappers...\n> \n> and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\n> wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\n> patch 1 by the time they get to patch 4.\n\nHi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.\n\n> \n> Thanks,\n> Ben\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "No that's fine, I figured as much. I just wasn't sure that was more important to you than what (I thought) was a better reading order for the series.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "\n\nOn 2/23/2026 10:46 AM, Dmitry Ilvokhin wrote:\n> [You don't often get email from d@ilvokhin.com. Learn why this is important at https://aka.ms/LearnAboutSenderIdentification ]\n> \n> On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:\n>> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n>>> Zone lock contention can significantly impact allocation and\n>>> reclaim latency, as it is a central synchronization point in\n>>> the page allocator and reclaim paths. Improved visibility into\n>>> its behavior is therefore important for diagnosing performance\n>>> issues in memory-intensive workloads.\n>>>\n>>> On some production workloads at Meta, we have observed noticeable\n>>> zone lock contention. Deeper analysis of lock holders and waiters\n>>> is currently difficult with existing instrumentation.\n>>>\n>>> While generic lock contention_begin/contention_end tracepoints\n>>> cover the slow path, they do not provide sufficient visibility\n>>> into lock hold times. In particular, the lack of a release-side\n>>> event makes it difficult to identify long lock holders and\n>>> correlate them with waiters. As a result, distinguishing between\n>>> short bursts of contention and pathological long hold times\n>>> requires additional instrumentation.\n>>>\n>>> This patch series adds dedicated tracepoint instrumentation to\n>>> zone lock, following the existing mmap_lock tracing model.\n>>>\n>>> The goal is to enable detailed holder/waiter analysis and lock\n>>> hold time measurements without affecting the fast path when\n>>> tracing is disabled.\n>>>\n>>> The series is structured as follows:\n>>>\n>>>   1. Introduce zone lock wrappers.\n>>>   2. Mechanically convert zone lock users to the wrappers.\n>>>   3. Convert compaction to use the wrappers (requires minor\n>>>      restructuring of compact_lock_irqsave()).\n>>>   4. Add zone lock tracepoints.\n>>\n>> I think you can improve the flow of this series if reorder as follows:\n>>       1. Introduce zone lock wrappers\n>>       4. Add zone lock tracepoints\n>>       2. Mechanically convert zone lock users to the wrappers\n>>       3. Convert compaction to use the wrappers...\n>>\n>> and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\n>> wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\n>> patch 1 by the time they get to patch 4.\n> \n> Hi Ben,\n> \n> Thanks for the suggestion.\n> \n> I structured the series intentionally to keep all behavior-preserving\n> refactoring separate from the actual instrumentation change.\n> \n> In particular, I had to split the conversion into two patches to\n> separate the purely mechanical changes from the compaction\n> restructuring. With the current order, tracepoints addition remains a\n> single, atomic functional change on top of a fully converted tree. This\n> keeps the instrumentation isolated from the refactoring and with an\n> intention to make bisection and review of the behavioral change easier.\n> \n> Reordering as suggested would mix instrumentation with intermediate\n> refactoring states, which I'd prefer to avoid.\n> \n> I hope this reasoning makes sense, but I'm happy to discuss if there are\n> strong objections.\n\nNo that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen\n\n> \n>>\n>> Thanks,\n>> Ben\n\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 2/4] mm: convert zone lock users to wrappers",
          "message_id": "7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Replace direct zone lock acquire/release operations with the newly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change intended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be handled separately in the following patch due to additional non-trivial modifications.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Wed, Feb 11, 2026 at 03:22:14PM +0000, Dmitry Ilvokhin wrote:\n> Replace direct zone lock acquire/release operations with the\n> newly introduced wrappers.\n> \n> The changes are purely mechanical substitutions. No functional change\n> intended. Locking semantics and ordering remain unchanged.\n> \n> The compaction path is left unchanged for now and will be\n> handled separately in the following patch due to additional\n> non-trivial modifications.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "Add thin wrappers around zone lock acquire/release operations. This prepares the code for future tracepoint instrumentation without modifying individual call sites.\n\nCentralizing zone lock operations behind wrappers allows future instrumentation or debugging hooks to be added without touching all users.\n\nNo functional change intended. The wrappers are introduced in preparation for subsequent patches and are not yet used.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Any reason you used macros for above two and inlined functions for remaining?",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> Add thin wrappers around zone lock acquire/release operations. This\n> prepares the code for future tracepoint instrumentation without\n> modifying individual call sites.\n> \n> Centralizing zone lock operations behind wrappers allows future\n> instrumentation or debugging hooks to be added without touching\n> all users.\n> \n> No functional change intended. The wrappers are introduced in\n> preparation for subsequent patches and are not yet used.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> ---\n>  MAINTAINERS               |  1 +\n>  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n>  2 files changed, 39 insertions(+)\n>  create mode 100644 include/linux/zone_lock.h\n> \n> diff --git a/MAINTAINERS b/MAINTAINERS\n> index b4088f7290be..680c9ae02d7e 100644\n> --- a/MAINTAINERS\n> +++ b/MAINTAINERS\n> @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n>  F:\tinclude/linux/ptdump.h\n>  F:\tinclude/linux/vmpressure.h\n>  F:\tinclude/linux/vmstat.h\n> +F:\tinclude/linux/zone_lock.h\n>  F:\tkernel/fork.c\n>  F:\tmm/Kconfig\n>  F:\tmm/debug.c\n> diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> new file mode 100644\n> index 000000000000..c531e26280e6\n> --- /dev/null\n> +++ b/include/linux/zone_lock.h\n> @@ -0,0 +1,38 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +#ifndef _LINUX_ZONE_LOCK_H\n> +#define _LINUX_ZONE_LOCK_H\n> +\n> +#include <linux/mmzone.h>\n> +#include <linux/spinlock.h>\n> +\n> +static inline void zone_lock_init(struct zone *zone)\n> +{\n> +\tspin_lock_init(&zone->lock);\n> +}\n> +\n> +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> +do {\t\t\t\t\t\t\t\t\\\n> +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> +} while (0)\n> +\n> +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> +({\t\t\t\t\t\t\t\t\\\n> +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> +})\n\nAny reason you used macros for above two and inlined functions for remaining?\n\n> +\n> +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n> +{\n> +\tspin_unlock_irqrestore(&zone->lock, flags);\n> +}\n> +\n> +static inline void zone_lock_irq(struct zone *zone)\n> +{\n> +\tspin_lock_irq(&zone->lock);\n> +}\n> +\n> +static inline void zone_unlock_irq(struct zone *zone)\n> +{\n> +\tspin_unlock_irq(&zone->lock);\n> +}\n> +\n> +#endif /* _LINUX_ZONE_LOCK_H */\n> -- \n> 2.47.3\n> \n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The reason for using macros in those two cases is that they need to modify the flags variable passed by the caller, just like spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same convention here. If we used normal inline functions instead, we would need to pass a pointer to flags, which would change the call sites and diverge from the existing *_irqsave() locking pattern. There is also a difference between zone_lock_irqsave() and zone_trylock_irqsave() implementations: the former is implemented as a do { } while (0) macro since it does not return a value, while the latter uses a GCC extension in order to return the trylock result. This matches spin_lock_* convention as well.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > Add thin wrappers around zone lock acquire/release operations. This\n> > prepares the code for future tracepoint instrumentation without\n> > modifying individual call sites.\n> > \n> > Centralizing zone lock operations behind wrappers allows future\n> > instrumentation or debugging hooks to be added without touching\n> > all users.\n> > \n> > No functional change intended. The wrappers are introduced in\n> > preparation for subsequent patches and are not yet used.\n> > \n> > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > ---\n> >  MAINTAINERS               |  1 +\n> >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> >  2 files changed, 39 insertions(+)\n> >  create mode 100644 include/linux/zone_lock.h\n> > \n> > diff --git a/MAINTAINERS b/MAINTAINERS\n> > index b4088f7290be..680c9ae02d7e 100644\n> > --- a/MAINTAINERS\n> > +++ b/MAINTAINERS\n> > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> >  F:\tinclude/linux/ptdump.h\n> >  F:\tinclude/linux/vmpressure.h\n> >  F:\tinclude/linux/vmstat.h\n> > +F:\tinclude/linux/zone_lock.h\n> >  F:\tkernel/fork.c\n> >  F:\tmm/Kconfig\n> >  F:\tmm/debug.c\n> > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > new file mode 100644\n> > index 000000000000..c531e26280e6\n> > --- /dev/null\n> > +++ b/include/linux/zone_lock.h\n> > @@ -0,0 +1,38 @@\n> > +/* SPDX-License-Identifier: GPL-2.0 */\n> > +#ifndef _LINUX_ZONE_LOCK_H\n> > +#define _LINUX_ZONE_LOCK_H\n> > +\n> > +#include <linux/mmzone.h>\n> > +#include <linux/spinlock.h>\n> > +\n> > +static inline void zone_lock_init(struct zone *zone)\n> > +{\n> > +\tspin_lock_init(&zone->lock);\n> > +}\n> > +\n> > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > +do {\t\t\t\t\t\t\t\t\\\n> > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > +} while (0)\n> > +\n> > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > +({\t\t\t\t\t\t\t\t\\\n> > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > +})\n> \n> Any reason you used macros for above two and inlined functions for remaining?\n>\n\nThe reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.\n\n> > +\n> > +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n> > +{\n> > +\tspin_unlock_irqrestore(&zone->lock, flags);\n> > +}\n> > +\n> > +static inline void zone_lock_irq(struct zone *zone)\n> > +{\n> > +\tspin_lock_irq(&zone->lock);\n> > +}\n> > +\n> > +static inline void zone_unlock_irq(struct zone *zone)\n> > +{\n> > +\tspin_unlock_irq(&zone->lock);\n> > +}\n> > +\n> > +#endif /* _LINUX_ZONE_LOCK_H */\n> > -- \n> > 2.47.3\n> > \n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Cool, thanks for the explanation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 03:18:04PM +0000, Dmitry Ilvokhin wrote:\n> On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> > On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > > Add thin wrappers around zone lock acquire/release operations. This\n> > > prepares the code for future tracepoint instrumentation without\n> > > modifying individual call sites.\n> > > \n> > > Centralizing zone lock operations behind wrappers allows future\n> > > instrumentation or debugging hooks to be added without touching\n> > > all users.\n> > > \n> > > No functional change intended. The wrappers are introduced in\n> > > preparation for subsequent patches and are not yet used.\n> > > \n> > > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > > ---\n> > >  MAINTAINERS               |  1 +\n> > >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> > >  2 files changed, 39 insertions(+)\n> > >  create mode 100644 include/linux/zone_lock.h\n> > > \n> > > diff --git a/MAINTAINERS b/MAINTAINERS\n> > > index b4088f7290be..680c9ae02d7e 100644\n> > > --- a/MAINTAINERS\n> > > +++ b/MAINTAINERS\n> > > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> > >  F:\tinclude/linux/ptdump.h\n> > >  F:\tinclude/linux/vmpressure.h\n> > >  F:\tinclude/linux/vmstat.h\n> > > +F:\tinclude/linux/zone_lock.h\n> > >  F:\tkernel/fork.c\n> > >  F:\tmm/Kconfig\n> > >  F:\tmm/debug.c\n> > > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > > new file mode 100644\n> > > index 000000000000..c531e26280e6\n> > > --- /dev/null\n> > > +++ b/include/linux/zone_lock.h\n> > > @@ -0,0 +1,38 @@\n> > > +/* SPDX-License-Identifier: GPL-2.0 */\n> > > +#ifndef _LINUX_ZONE_LOCK_H\n> > > +#define _LINUX_ZONE_LOCK_H\n> > > +\n> > > +#include <linux/mmzone.h>\n> > > +#include <linux/spinlock.h>\n> > > +\n> > > +static inline void zone_lock_init(struct zone *zone)\n> > > +{\n> > > +\tspin_lock_init(&zone->lock);\n> > > +}\n> > > +\n> > > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > > +do {\t\t\t\t\t\t\t\t\\\n> > > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +} while (0)\n> > > +\n> > > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > > +({\t\t\t\t\t\t\t\t\\\n> > > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +})\n> > \n> > Any reason you used macros for above two and inlined functions for remaining?\n> >\n> \n> The reason for using macros in those two cases is that they need to\n> modify the flags variable passed by the caller, just like\n> spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\n> convention here.\n> \n> If we used normal inline functions instead, we would need to pass a\n> pointer to flags, which would change the call sites and diverge from the\n> existing *_irqsave() locking pattern.\n> \n> There is also a difference between zone_lock_irqsave() and\n> zone_trylock_irqsave() implementations: the former is implemented as a\n> do { } while (0) macro since it does not return a value, while the\n> latter uses a GCC extension in order to return the trylock result. This\n> matches spin_lock_* convention as well.\n> \n\nCool, thanks for the explanation.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> Add thin wrappers around zone lock acquire/release operations. This\n> prepares the code for future tracepoint instrumentation without\n> modifying individual call sites.\n> \n> Centralizing zone lock operations behind wrappers allows future\n> instrumentation or debugging hooks to be added without touching\n> all users.\n> \n> No functional change intended. The wrappers are introduced in\n> preparation for subsequent patches and are not yet used.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Steven Rostedt",
              "summary": "Have you thought about adding guards as well. It could make the code simpler: (Not tested) #include <linux/cleanup.h> [..] DEFINE_LOCK_GUARD_1(zonelock_irqsave, struct zone *, zone_lock_irqsave(_T->lock, _T->flags), zone_unlock_irqrestore(_T->lock, _T->flags), unsigned long flags) DECLARE_LOCK_GUARD_1_ATTRS(zonelock_irqsave, __acquires(_T), __releases(*(struct zone ***)_T)) #define class_zonelock_irqsave_constructor(_T) WITH_LOCK_GUARD_1_ATTRS(zonelock_irqsave, _T) DEFINE_LOCK_GUARD_1(zonelock_irq, struct zone *, zone_lock_irq(_T->lock), zone_unlock_irq(_T->lock)) DECLARE_LOCK_GUARD_1_ATTRS(zonelock_irq, __acquires(_T), __releases(*(struct zone ***)_T)) #define...",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Wed, 11 Feb 2026 15:22:13 +0000\nDmitry Ilvokhin <d@ilvokhin.com> wrote:\n\n\n> diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> new file mode 100644\n> index 000000000000..c531e26280e6\n> --- /dev/null\n> +++ b/include/linux/zone_lock.h\n> @@ -0,0 +1,38 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +#ifndef _LINUX_ZONE_LOCK_H\n> +#define _LINUX_ZONE_LOCK_H\n> +\n> +#include <linux/mmzone.h>\n> +#include <linux/spinlock.h>\n> +\n> +static inline void zone_lock_init(struct zone *zone)\n> +{\n> +\tspin_lock_init(&zone->lock);\n> +}\n> +\n> +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> +do {\t\t\t\t\t\t\t\t\\\n> +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> +} while (0)\n> +\n> +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> +({\t\t\t\t\t\t\t\t\\\n> +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> +})\n> +\n> +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n> +{\n> +\tspin_unlock_irqrestore(&zone->lock, flags);\n> +}\n> +\n> +static inline void zone_lock_irq(struct zone *zone)\n> +{\n> +\tspin_lock_irq(&zone->lock);\n> +}\n> +\n> +static inline void zone_unlock_irq(struct zone *zone)\n> +{\n> +\tspin_unlock_irq(&zone->lock);\n> +}\n> +\n> +#endif /* _LINUX_ZONE_LOCK_H */\n\nHave you thought about adding guards as well. It could make the code simpler:\n\n  (Not tested)\n\n#include <linux/cleanup.h>\n[..]\n\nDEFINE_LOCK_GUARD_1(zonelock_irqsave, struct zone *,\n\t\t    zone_lock_irqsave(_T->lock, _T->flags),\n\t\t    zone_unlock_irqrestore(_T->lock, _T->flags),\n\t\t    unsigned long flags)\nDECLARE_LOCK_GUARD_1_ATTRS(zonelock_irqsave, __acquires(_T), __releases(*(struct zone ***)_T))\n#define class_zonelock_irqsave_constructor(_T) WITH_LOCK_GUARD_1_ATTRS(zonelock_irqsave, _T)\n\nDEFINE_LOCK_GUARD_1(zonelock_irq, struct zone *,\n\t\t    zone_lock_irq(_T->lock),\n\t\t    zone_unlock_irq(_T->lock))\nDECLARE_LOCK_GUARD_1_ATTRS(zonelock_irq, __acquires(_T), __releases(*(struct zone ***)_T))\n#define class_zonelock_irq_constructor(_T) WITH_LOCK_GUARD_1_ATTRS(zonelock_irq, _T)\n\nThen you could even remove the \"flags\" variables from the C code, and some goto unlocks.\n\n-- Steve\n\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 3/4] mm: convert compaction to zone lock wrappers",
          "message_id": "aZ3I0ADTAdCN6UmN@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aZ3I0ADTAdCN6UmN@shell.ilvokhin.com/",
          "date": "2026-02-24T15:50:43Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "aZ3BLKzhIIZvkbwL@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aZ3BLKzhIIZvkbwL@shell.ilvokhin.com/",
          "date": "2026-02-24T15:18:08Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Cool, thanks for the explanation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 03:18:04PM +0000, Dmitry Ilvokhin wrote:\n> On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> > On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > > Add thin wrappers around zone lock acquire/release operations. This\n> > > prepares the code for future tracepoint instrumentation without\n> > > modifying individual call sites.\n> > > \n> > > Centralizing zone lock operations behind wrappers allows future\n> > > instrumentation or debugging hooks to be added without touching\n> > > all users.\n> > > \n> > > No functional change intended. The wrappers are introduced in\n> > > preparation for subsequent patches and are not yet used.\n> > > \n> > > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > > ---\n> > >  MAINTAINERS               |  1 +\n> > >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> > >  2 files changed, 39 insertions(+)\n> > >  create mode 100644 include/linux/zone_lock.h\n> > > \n> > > diff --git a/MAINTAINERS b/MAINTAINERS\n> > > index b4088f7290be..680c9ae02d7e 100644\n> > > --- a/MAINTAINERS\n> > > +++ b/MAINTAINERS\n> > > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> > >  F:\tinclude/linux/ptdump.h\n> > >  F:\tinclude/linux/vmpressure.h\n> > >  F:\tinclude/linux/vmstat.h\n> > > +F:\tinclude/linux/zone_lock.h\n> > >  F:\tkernel/fork.c\n> > >  F:\tmm/Kconfig\n> > >  F:\tmm/debug.c\n> > > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > > new file mode 100644\n> > > index 000000000000..c531e26280e6\n> > > --- /dev/null\n> > > +++ b/include/linux/zone_lock.h\n> > > @@ -0,0 +1,38 @@\n> > > +/* SPDX-License-Identifier: GPL-2.0 */\n> > > +#ifndef _LINUX_ZONE_LOCK_H\n> > > +#define _LINUX_ZONE_LOCK_H\n> > > +\n> > > +#include <linux/mmzone.h>\n> > > +#include <linux/spinlock.h>\n> > > +\n> > > +static inline void zone_lock_init(struct zone *zone)\n> > > +{\n> > > +\tspin_lock_init(&zone->lock);\n> > > +}\n> > > +\n> > > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > > +do {\t\t\t\t\t\t\t\t\\\n> > > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +} while (0)\n> > > +\n> > > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > > +({\t\t\t\t\t\t\t\t\\\n> > > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +})\n> > \n> > Any reason you used macros for above two and inlined functions for remaining?\n> >\n> \n> The reason for using macros in those two cases is that they need to\n> modify the flags variable passed by the caller, just like\n> spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\n> convention here.\n> \n> If we used normal inline functions instead, we would need to pass a\n> pointer to flags, which would change the call sites and diverge from the\n> existing *_irqsave() locking pattern.\n> \n> There is also a difference between zone_lock_irqsave() and\n> zone_trylock_irqsave() implementations: the former is implemented as a\n> do { } while (0) macro since it does not return a value, while the\n> latter uses a GCC extension in order to return the trylock result. This\n> matches spin_lock_* convention as well.\n> \n\nCool, thanks for the explanation.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Gregory Price",
      "primary_email": "gourry@gourry.net",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[RFC PATCH v4 27/27] cxl: add cxl_compression PCI driver",
          "message_id": "20260222084842.1824063-28-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260222084842.1824063-28-gourry@gourry.net/",
          "date": "2026-02-22T08:50:38Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-22",
          "patch_summary": "Add a generic CXL type-3 driver for compressed memory controllers.\n\nThe driver provides an alternative PCI binding that converts CXL RAM regions to private-node sysram and registers them with the CRAM subsystem for transparent demotion/promotion.\n\nProbe flow: 1. cxl_pci_type3_probe_init() for standard CXL device setup 2. Discover/convert auto-RAM regions or create a RAM region 3. Convert to private-node sysram via devm_cxl_add_sysram() 4. Register with CRAM via cram_register_private_node()\n\nPage flush pipeline: When a CRAM folio is freed, the CRAM free_folio   callback buffers it into a per-CPU RCU-protected flush buffer to offload the operation.\n\nA periodic kthread swaps the per-CPU buffers under RCU, then sends batched Sanitize-Zero commands so the device can zero pages.\n\nA flush_record bitmap tracks in-flight pages to avoid re-buffering on the second free_folio entry after folio_put().\n\nOverflow from full buffers is handled by a per-CPU workqueue fallback.\n\nWatermark interrupts: MSI-X vector 12 - delivers \"Low\" watermark interrupts MSI-X vector 13 - delivers \"High\" watermark interrupts This adjusts CRAM pressure: Low  - increases pressure. High - reduces pressure.",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/2] cxl/region: fix region leak when attach_target fails in cxl_add_to_region",
          "message_id": "20260221043013.1420169-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260221043013.1420169-1-gourry@gourry.net/",
          "date": "2026-02-21T04:30:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-21",
          "patch_summary": "cxl_add_to_region() ignores the return value of attach_target().  When attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO), the auto-discovered region remains registered with its HPA resource consumed but never reaches COMMIT state.  Subsequent region creation attempts fail with -ENOSPC because the HPA range is already reserved.\n\nTrack whether this call to cxl_add_to_region() created the region, and call drop_region() on attach_target() failure to unregister it and release the HPA resource.  Pre-existing regions are left alone since other endpoints may already be attached.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "BAH - disregard this patch, it uses drop_region which is introduced by Alejandro here: https://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 11:30:12PM -0500, Gregory Price wrote:\n> cxl_add_to_region() ignores the return value of attach_target().  When\n> attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO),\n> the auto-discovered region remains registered with its HPA resource\n> consumed but never reaches COMMIT state.  Subsequent region creation\n> attempts fail with -ENOSPC because the HPA range is already reserved.\n> \n> Track whether this call to cxl_add_to_region() created the region, and\n> call drop_region() on attach_target() failure to unregister it and\n> release the HPA resource.  Pre-existing regions are left alone since\n> other endpoints may already be attached.\n> \n> Signed-off-by: Gregory Price <gourry@gourry.net>\n\nBAH - disregard this patch, it uses drop_region which is introduced by\nAlejandro here:\n\nhttps://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/\n\n",
              "reply_to": "",
              "message_date": "2026-02-21",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Alison Schofield",
              "summary": "I see you dropping this, perhaps just for the moment, because the drop_region() you wanted to use is not available yet. This looks a lot like https://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/ cxl/region: Unregister auto-created region when assembly fails When auto-created region assembly fails the region remains registered but disabled. The region continues to reserve its memory resource, preventing DAX from registering the memory. Unregister the region on assembly failure to release the resource. And the review comments on that one, or at least on that thread in general, was to leave all the broken things in place. I didn't agree with that, and hope to see this version move ahead when you have the drop_region you need.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Fri, Feb 20, 2026 at 11:30:12PM -0500, Gregory Price wrote:\n> cxl_add_to_region() ignores the return value of attach_target().  When\n> attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO),\n> the auto-discovered region remains registered with its HPA resource\n> consumed but never reaches COMMIT state.  Subsequent region creation\n> attempts fail with -ENOSPC because the HPA range is already reserved.\n> \n> Track whether this call to cxl_add_to_region() created the region, and\n> call drop_region() on attach_target() failure to unregister it and\n> release the HPA resource.  Pre-existing regions are left alone since\n> other endpoints may already be attached.\n\nI see you dropping this, perhaps just for the moment, because\nthe drop_region() you wanted to use is not available yet.\n\nThis looks a lot like \n\thttps://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/\n\tcxl/region: Unregister auto-created region when assembly fails\n\tWhen auto-created region assembly fails the region remains registered\n\tbut disabled. The region continues to reserve its memory resource,\n\tpreventing DAX from registering the memory.\n\tUnregister the region on assembly failure to release the resource.\n\nAnd the review comments on that one, or at least on that thread in\ngeneral, was to leave all the broken things in place.\nI didn't agree with that, and hope to see this version move ahead\nwhen you have the drop_region you need.\n\n-- Alison\n\n\n\n\n\n\n> \n> Signed-off-by: Gregory Price <gourry@gourry.net>\n> ---\n>  drivers/cxl/core/region.c | 15 ++++++++++++---\n>  1 file changed, 12 insertions(+), 3 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 385be9cb44cd..276046d49f88 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -3923,6 +3923,7 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n>  \tstruct cxl_region_context ctx;\n>  \tstruct cxl_region_params *p;\n>  \tbool attach = false;\n> +\tbool newly_created = false;\n>  \tint rc;\n>  \n>  \tctx = (struct cxl_region_context) {\n> @@ -3946,15 +3947,23 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n>  \tmutex_lock(&cxlrd->range_lock);\n>  \tstruct cxl_region *cxlr __free(put_cxl_region) =\n>  \t\tcxl_find_region_by_range(cxlrd, &ctx.hpa_range);\n> -\tif (!cxlr)\n> +\tif (!cxlr) {\n>  \t\tcxlr = construct_region(cxlrd, &ctx);\n> +\t\tnewly_created = !IS_ERR(cxlr);\n> +\t}\n>  \tmutex_unlock(&cxlrd->range_lock);\n>  \n>  \trc = PTR_ERR_OR_ZERO(cxlr);\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> -\tattach_target(cxlr, cxled, -1, TASK_UNINTERRUPTIBLE);\n> +\trc = attach_target(cxlr, cxled, -1, TASK_UNINTERRUPTIBLE);\n> +\tif (rc) {\n> +\t\t/* If endpoint was just created, tear it down to release HPA */\n> +\t\tif (newly_created)\n> +\t\t\tdrop_region(cxlrd, cxlr);\n> +\t\treturn rc;\n> +\t}\n>  \n>  \tscoped_guard(rwsem_read, &cxl_rwsem.region) {\n>  \t\tp = &cxlr->params;\n> @@ -3972,7 +3981,7 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n>  \t\t\t\tp->res);\n>  \t}\n>  \n> -\treturn rc;\n> +\treturn 0;\n>  }\n>  EXPORT_SYMBOL_NS_GPL(cxl_add_to_region, \"CXL\");\n>  \n> -- \n> 2.47.3\n> \n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Yeah it's not a particularly useful cleanup in the current infrastructure because nothing actually uses this pattern (yet). The important note here is the difference between auto-regions and manually created regions.  For auto-regions, you might have another endpoint show up looking for the partially created region - and then just go off and create it anyway because it thinks it was first. But in my driver, i'm explicitly converting these auto-regions into other things, and if that fails it causes *all other* region creation to fail - even if it wasn't actually dependent on that original region. This is only an issue if you have two devices unbind/bind cycling at the same time - i.e. echo 0000:d0:00.00 > cxl_pci/unbind echo 0000:e0:00.00 > cxl_pci/unbind echo 0000:d0:00.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 11:48:42AM -0800, Alison Schofield wrote:\n> On Fri, Feb 20, 2026 at 11:30:12PM -0500, Gregory Price wrote:\n> > cxl_add_to_region() ignores the return value of attach_target().  When\n> > attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO),\n> > the auto-discovered region remains registered with its HPA resource\n> > consumed but never reaches COMMIT state.  Subsequent region creation\n> > attempts fail with -ENOSPC because the HPA range is already reserved.\n> > \n> > Track whether this call to cxl_add_to_region() created the region, and\n> > call drop_region() on attach_target() failure to unregister it and\n> > release the HPA resource.  Pre-existing regions are left alone since\n> > other endpoints may already be attached.\n> \n> I see you dropping this, perhaps just for the moment, because\n> the drop_region() you wanted to use is not available yet.\n> \n\nYeah it's not a particularly useful cleanup in the current\ninfrastructure because nothing actually uses this pattern (yet).\n\n> This looks a lot like \n> \thttps://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/\n> \tcxl/region: Unregister auto-created region when assembly fails\n> \tWhen auto-created region assembly fails the region remains registered\n> \tbut disabled. The region continues to reserve its memory resource,\n> \tpreventing DAX from registering the memory.\n> \tUnregister the region on assembly failure to release the resource.\n> \n> And the review comments on that one, or at least on that thread in\n> general, was to leave all the broken things in place.\n> I didn't agree with that, and hope to see this version move ahead\n> when you have the drop_region you need.\n> \n> \n\nThe important note here is the difference between auto-regions and\nmanually created regions.  For auto-regions, you might have another\nendpoint show up looking for the partially created region - and then\njust go off and create it anyway because it thinks it was first.\n\nBut in my driver, i'm explicitly converting these auto-regions into\nother things, and if that fails it causes *all other* region creation to\nfail - even if it wasn't actually dependent on that original region.\n\nThis is only an issue if you have two devices unbind/bind cycling at\nthe same time - i.e.\n\n   echo 0000:d0:00.00 > cxl_pci/unbind\n   echo 0000:e0:00.00 > cxl_pci/unbind\n   echo 0000:d0:00.00 > mydriver/bind\n   echo 0000:e0:00.00 > mydriver/bind\n\nIf the platform has pre-programmed and locked the decoders, and one of\nthe two devices fails to probe and leaves a hanging partially\ncreated region, the other device will fail too.\n\nIt's a pretty narrow failure scenario.\n\n~Gregory\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Alison Schofield",
              "summary": "That's by design, and that'll eventually fail too. But - is see how your case is different. Thanks for the explanation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 03:15:16PM -0500, Gregory Price wrote:\n> On Mon, Feb 23, 2026 at 11:48:42AM -0800, Alison Schofield wrote:\n> > On Fri, Feb 20, 2026 at 11:30:12PM -0500, Gregory Price wrote:\n> > > cxl_add_to_region() ignores the return value of attach_target().  When\n> > > attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO),\n> > > the auto-discovered region remains registered with its HPA resource\n> > > consumed but never reaches COMMIT state.  Subsequent region creation\n> > > attempts fail with -ENOSPC because the HPA range is already reserved.\n> > > \n> > > Track whether this call to cxl_add_to_region() created the region, and\n> > > call drop_region() on attach_target() failure to unregister it and\n> > > release the HPA resource.  Pre-existing regions are left alone since\n> > > other endpoints may already be attached.\n> > \n> > I see you dropping this, perhaps just for the moment, because\n> > the drop_region() you wanted to use is not available yet.\n> > \n> \n> Yeah it's not a particularly useful cleanup in the current\n> infrastructure because nothing actually uses this pattern (yet).\n> \n> > This looks a lot like \n> > \thttps://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/\n> > \tcxl/region: Unregister auto-created region when assembly fails\n> > \tWhen auto-created region assembly fails the region remains registered\n> > \tbut disabled. The region continues to reserve its memory resource,\n> > \tpreventing DAX from registering the memory.\n> > \tUnregister the region on assembly failure to release the resource.\n> > \n> > And the review comments on that one, or at least on that thread in\n> > general, was to leave all the broken things in place.\n> > I didn't agree with that, and hope to see this version move ahead\n> > when you have the drop_region you need.\n> > \n> > \n> \n> The important note here is the difference between auto-regions and\n> manually created regions.  For auto-regions, you might have another\n> endpoint show up looking for the partially created region - and then\n> just go off and create it anyway because it thinks it was first.\n\nThat's by design, and that'll eventually fail too.\n\nBut - is see how your case is different. Thanks for the explanation.\n\n> \n> But in my driver, i'm explicitly converting these auto-regions into\n> other things, and if that fails it causes *all other* region creation to\n> fail - even if it wasn't actually dependent on that original region.\n> \n> This is only an issue if you have two devices unbind/bind cycling at\n> the same time - i.e.\n> \n>    echo 0000:d0:00.00 > cxl_pci/unbind\n>    echo 0000:e0:00.00 > cxl_pci/unbind\n>    echo 0000:d0:00.00 > mydriver/bind\n>    echo 0000:e0:00.00 > mydriver/bind\n> \n> If the platform has pre-programmed and locked the decoders, and one of\n> the two devices fails to probe and leaves a hanging partially\n> created region, the other device will fail too.\n> \n> It's a pretty narrow failure scenario.\n> \n> ~Gregory\n> \n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Feel free to add it to this series. I have started to send individual series as you know but the part changing the region creation will require more work than the already sent. About this fix, it looks good to me, although I have to admit I'm a bit lost after following the discussion Allison points to. If we want to keep the state of failure for forensics, not sure if the debugging/tracing or default error info in this case will be enough. In any case:",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\nOn 2/21/26 05:17, Gregory Price wrote:\n> On Fri, Feb 20, 2026 at 11:30:12PM -0500, Gregory Price wrote:\n>> cxl_add_to_region() ignores the return value of attach_target().  When\n>> attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO),\n>> the auto-discovered region remains registered with its HPA resource\n>> consumed but never reaches COMMIT state.  Subsequent region creation\n>> attempts fail with -ENOSPC because the HPA range is already reserved.\n>>\n>> Track whether this call to cxl_add_to_region() created the region, and\n>> call drop_region() on attach_target() failure to unregister it and\n>> release the HPA resource.  Pre-existing regions are left alone since\n>> other endpoints may already be attached.\n>>\n>> Signed-off-by: Gregory Price <gourry@gourry.net>\n> BAH - disregard this patch, it uses drop_region which is introduced by\n> Alejandro here:\n>\n> https://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/\n>\nFeel free to add it to this series. I have started to send individual \nseries as you know but the part changing the region creation will \nrequire more work than the already sent.\n\nAbout this fix, it looks good to me, although I have to admit I'm a bit \nlost after following the discussion Allison points to. If we want to \nkeep the state of failure for forensics, not sure if the \ndebugging/tracing or default error info in this case will be enough.\n\nIn any case:\n\nReviewed-by: Alejandro Lucero <alucerop@amd.com>\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Yeah i don't quite follow the want to keep the objects around, it seems to cause more issues than it solves - but then i also don't think this is going to be a particularly common problem",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 04:15:33PM +0000, Alejandro Lucero Palau wrote:\n> \n> On 2/21/26 05:17, Gregory Price wrote:\n> > On Fri, Feb 20, 2026 at 11:30:12PM -0500, Gregory Price wrote:\n> > > cxl_add_to_region() ignores the return value of attach_target().  When\n> > > attach_target() fails (e.g. cxl_port_setup_targets() returns -ENXIO),\n> > > the auto-discovered region remains registered with its HPA resource\n> > > consumed but never reaches COMMIT state.  Subsequent region creation\n> > > attempts fail with -ENOSPC because the HPA range is already reserved.\n> > > \n> > > Track whether this call to cxl_add_to_region() created the region, and\n> > > call drop_region() on attach_target() failure to unregister it and\n> > > release the HPA resource.  Pre-existing regions are left alone since\n> > > other endpoints may already be attached.\n> > > \n> > > Signed-off-by: Gregory Price <gourry@gourry.net>\n> > BAH - disregard this patch, it uses drop_region which is introduced by\n> > Alejandro here:\n> > \n> > https://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/\n> > \n> Feel free to add it to this series. I have started to send individual series\n> as you know but the part changing the region creation will require more work\n> than the already sent.\n> \n> About this fix, it looks good to me, although I have to admit I'm a bit lost\n> after following the discussion Allison points to. If we want to keep the\n> state of failure for forensics, not sure if the debugging/tracing or default\n> error info in this case will be enough.\n> \n> In any case:\n> \n> Reviewed-by: Alejandro Lucero <alucerop@amd.com>\n> \n\nYeah i don't quite follow the want to keep the objects around, it seems\nto cause more issues than it solves - but then i also don't think this\nis going to be a particularly common problem\n\n~Gregory\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH 0/6] mm/memcontrol: Make memcg limits tier-aware",
          "message_id": "aZ3ysV-k1UisnPRG@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3ysV-k1UisnPRG@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T18:49:25Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Kaiyang Zhao",
              "summary": "recently released a preprint paper on arXiv that includes case studies with a few of Meta's production workloads using a prototype version of the patches. The results confirmed that co-colocated workloads can have working set sizes exceeding the limited top-tier memory capacity given today's server memory shapes and workload stacking settings, causing contention of top-tier...",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 01:49:21PM -0500, Gregory Price wrote:\n> \n> > \n> > > Is this typical in real life configurations?\n> > \n> > I would say so. I think that the two examples above are realistic\n> > scenarios that cloud providers and hyperscalers might face on tiered systems.\n> > \n> \n> The answer is unequivocally yes.\n> \n> Lacking tier-awareness is actually a huge blocker for deploying mixed\n> workloads on large, dense memory systems with multiple tiers (2+).\n\nHello! I'm the author of the RFC in 2024. Just want to add that we've\nrecently released a preprint paper on arXiv that includes case studies\nwith a few of Meta's production workloads using a prototype version of\nthe patches.\n\nThe results confirmed that co-colocated workloads can have working set\nsizes exceeding the limited top-tier memory capacity given today's\nserver memory shapes and workload stacking settings, causing contention\nof top-tier memory. Workloads see significant variations in tail\nlatency and throughput depending on the share of top-tier tier memory\nthey get, which this patch set will alleviate.\n\nBest,\nKaiyang\n\n[1] https://arxiv.org/pdf/2602.08800\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC][RFC PATCH v4 00/27] Private Memory Nodes (w/ Compressed RAM)",
          "message_id": "aZ3X3Jni0HZXZMVl@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3X3Jni0HZXZMVl@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T16:54:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH v5 00/10] mm: Hot page tracking and promotion infrastructure",
          "message_id": "aZ3D_8GJit3FYhQc@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3D_8GJit3FYhQc@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T15:30:11Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Bharata Rao",
              "summary": "It consistently performs that way. Here are the numbers from another run: ================================ Time in seconds         4329.22 Mop/s total             90400.27 pgpromote_success       41967282 pgpromote_candidate     0 pgpromote_candidate_nrl 41968339 pgdemote_kswapd         0 numa_pte_updates        42253854 numa_hint_faults        42019449 ================================ grep -E \"pgpromote|pgdemote\" /sys/devices/system/node/node0/vmstat pgpromote_success 20996597 pgpromote_candidate 0 pgpromote_candidate_nrl 41968339 (*) pgdemote_kswapd 0...",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 24-Feb-26 9:00 PM, Gregory Price wrote:\n>>\n>> For pghot-default, with target_nid alternating between the available\n>> toptier nodes 0 and 1, the numbers catch up with pghot-precise and base\n>> NUMAB2 case as seen below:\n>> ================================\n>> Time in seconds         4337.98\n>> Mop/s total             90217.86\n>>\n>> pgpromote_success       42170085\n>> pgpromote_candidate     0\n>> pgpromote_candidate_nrl 42171963\n>> pgdemote_kswapd         0\n>> numa_pte_updates        42338538\n>> numa_hint_faults        42185662\n>> ================================\n>>\n> \n> Fascinating! Thank you for the quick follow up.\n> \n> I wonder if this was a lucky run, it almost seems *too* perfect.\n\nIt consistently performs that way. Here are the numbers from another\nrun:\n\n================================\nTime in seconds         4329.22\nMop/s total             90400.27\n\npgpromote_success       41967282\npgpromote_candidate     0\npgpromote_candidate_nrl 41968339\npgdemote_kswapd         0\nnuma_pte_updates        42253854\nnuma_hint_faults        42019449\n================================\n\ngrep -E \"pgpromote|pgdemote\" /sys/devices/system/node/node0/vmstat\npgpromote_success 20996597\npgpromote_candidate 0\npgpromote_candidate_nrl 41968339 (*)\npgdemote_kswapd 0\npgdemote_direct 0\npgdemote_khugepaged 0\npgdemote_proactive 0\n\ngrep -E \"pgpromote|pgdemote\" /sys/devices/system/node/node1/vmstat\npgpromote_success 20970685\npgpromote_candidate 0\npgpromote_candidate_nrl 0\npgdemote_kswapd 0\npgdemote_direct 0\npgdemote_khugepaged 0\npgdemote_proactive 0\n\n\n(*) The round-robin b/n nodes 0 and 1 happens after this metric is\nattributed to the original default target_nid. Hence nrl metric\ngets populated for node 0 only.\n\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC][RFC PATCH v4 00/27] Private Memory Nodes (w/ Compressed RAM)",
          "message_id": "aZ3BEn_73Rk8Fn7L@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3BEn_73Rk8Fn7L@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T15:17:43Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "This gave me something to chew on I think this can be done without introducing N_MEMORY_PRIVATE and just checking:   NODE_DATA(target_nid)->private meaning these nodes can just be N_MEMORY with the...",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 10:17:38AM -0500, Gregory Price wrote:\n>    - Changing validations on node states\n>      - mempolicy checks N_MEMORY membership, so you have to hack\n>        N_MEMORY onto ZONE_DEVICE\n>        (or teach it about a new node state... N_MEMORY_PRIVATE)\n> \n\nThis gave me something to chew on\n\nI think this can be done without introducing N_MEMORY_PRIVATE and just\nchecking:   NODE_DATA(target_nid)->private\n\nmeaning these nodes can just be N_MEMORY with the same isolations.\n\nI'll look at this a bit more.\n\n~Gregory\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Matthew Brost",
              "summary": "I was about to chime in with essentially the same comment about DRM. Switching over to core-managed MM is a massive shift and is likely infeasible, or so extreme that we\\u2019d end up throwing away any the existing driver and starting from scratch. At least for Xe, our MM code is baked into all meaningful components of the driver. It\\u2019s also a unified driver that has to work on iGPU, dGPU over PCIe, dGPU over a coherent bus once we get there, devices with GPU pagefaults, and devices without GPU pagefaults. It also has to support both 3D and compute user-space stacks, etc. So requirements of what it needs to support is quite large. IIRC, Christian once mentioned that AMD was exploring using NUMA and udma-buf rather than DRM GEMs for MM on coherent-bus devices.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 10:17:38AM -0500, Gregory Price wrote:\n> On Tue, Feb 24, 2026 at 05:19:11PM +1100, Alistair Popple wrote:\n> > On 2026-02-22 at 19:48 +1100, Gregory Price <gourry@gourry.net> wrote...\n> > \n> > Based on our discussion at LPC I believe one of the primary motivators here was\n> > to re-use the existing mm buddy allocator rather than writing your own. I remain\n> > to be convinced that alone is justification enough for doing all this - DRM for\n> > example already has quite a nice standalone buddy allocator (drm_buddy.c) that\n> > could presumably be used, or adapted for use, by any device driver.\n> >\n> > The interesting part of this series (which I have skimmed but not read in\n> > detail) is how device memory gets exposed to userspace - this is something that\n> > existing ZONE_DEVICE implementations don't address, instead leaving it up to\n> > drivers and associated userspace stacks to deal with allocation, migration, etc.\n> > \n> \n> I agree that buddy-access alone is insufficient justification, it\n> started off that way - but if you want mempolicy/NUMA UAPI access,\n> it turns into \"Re-use all of MM\" - and that means using the buddy.\n> \n> I also expected ZONE_DEVICE vs NODE_DATA to be the primary discussion,\n> \n> I raise replacing it as a thought experiment, but not the proposal.\n> \n> The idea that drm/ is going to switch to private nodes is outside the\n> realm of reality, but part of that is because of years of infrastructure\n> built on the assumption that re-using mm/ is infeasible.\n\nI was about to chime in with essentially the same comment about DRM.\nSwitching over to core-managed MM is a massive shift and is likely\ninfeasible, or so extreme that we\\u2019d end up throwing away any the\nexisting driver and starting from scratch. At least for Xe, our MM code\nis baked into all meaningful components of the driver. It\\u2019s also a\nunified driver that has to work on iGPU, dGPU over PCIe, dGPU over a\ncoherent bus once we get there, devices with GPU pagefaults, and devices\nwithout GPU pagefaults. It also has to support both 3D and compute\nuser-space stacks, etc. So requirements of what it needs to support is\nquite large.\n\nIIRC, Christian once mentioned that AMD was exploring using NUMA and\nudma-buf rather than DRM GEMs for MM on coherent-bus devices. I would\nthink AMDGPU has nearly all the same requirements as Xe, aside from\nsupporting both 3D and compute stacks, since AMDKFD currently handles\ncompute. It might be worth getting Christian\\u2019s input on this RFC as he\nlikely has better insight then myself on DRM's future here.\n\nMatt\n\n> \n> But, lets talk about DEVICE_COHERENT\n> \n> ---\n> \n> DEVICE_COHERENT is the odd-man out among ZONE_DEVICE modes. The others\n> use softleaf entries and don't allow direct mappings.\n> \n> (DEVICE_PRIVATE sort of does if you squint, but you can also view that\n>  a bit like PROT_NONE or read-only controls to force migrations).\n> \n> If you take DEVICE_COHERENT and:\n> \n> - Move pgmap out of the struct page (page_ext, NODE_DATA, etc) to free\n>   the LRU list_head\n> - Put pages in the buddy (free lists, watermarks, managed_pages) or add\n>   pgmap->device_alloc() at every allocation callsite / buddy hook\n> - Add LRU support (aging, reclaim, compaction)\n> - Add isolated gating (new GFP flag and adjusted zonelist filtering)\n> - Add new dev_pagemap_ops callbacks for the various mm/ features\n> - Audit evey folio_is_zone_device() to distinguish zone device modes\n> \n> ... you've built N_MEMORY_PRIVATE inside ZONE_DEVICE. Except now\n> page_zone(page) returns ZONE_DEVICE - so you inherit the wrong\n> defaults at every existing ZONE_DEVICE check. \n> \n> Skip-sites become things to opt-out of instead of opting into.\n> \n> You just end up with\n> \n> if (folio_is_zone_device(folio))\n>     if (folio_is_my_special_zone_device())\n>     else ....\n> \n> and this just generalizes to\n> \n> if (folio_is_private_managed(folio))\n>     folio_managed_my_hooked_operation()\n> \n> So you get the same code, but have added more complexity to ZONE_DEVICE.\n> \n> I don't think that's needed if we just recognize ZONE is the wrong\n> abstraction to be operating on.\n> \n> Honestly, even ZONE_MOVABLE becomes pointless with N_MEMORY_PRIVATE\n> if you disallow longterm pinning - because the managing service handles\n> allocations (it has to inject GFP_PRIVATE to get access) or selectively\n> enables the mm/ services it knows are safe (mempolicy).\n> \n> Even if you allow longterm pinning, if your service controls what does\n> the pinning it can still be reclaimable - just manually (killing\n> processes) instead of letting hotplug do it via migration.\n> \n> If your service only allocates movable pages - your ZONE_NORMAL is\n> effectively ZONE_MOVABLE.  \n> \n> In some cases we use ZONE_MOVABLE to prevent the kernel from allocating\n> memory onto devices (like CXL).  This means struct page is forced to\n> take up DRAM or use memmap_on_memory - meaning you lose high-value\n> capacity or sacrifice contiguity (less huge page support).\n> \n> This entire problem can evaporate if you can just use ZONE_NORMAL.\n> \n> There are a lot of benefits to just re-using the buddy like this.\n> \n> Zones are the wrong abstraction and cause more problems.\n> \n> > >   free_folio           - mirrors ZONE_DEVICE's\n> > >   folio_split          - mirrors ZONE_DEVICE's\n> > >   migrate_to           - ... same as ZONE_DEVICE\n> > >   handle_fault         - mirrors the ZONE_DEVICE ...\n> > >   memory_failure       - parallels memory_failure_dev_pagemap(),\n> > \n> > One does not have to squint too hard to see that the above is not so different\n> > from what ZONE_DEVICE provides today via dev_pagemap_ops(). So I think I think\n> > it would be worth outlining why the existing ZONE_DEVICE mechanism can't be\n> > extended to provide these kind of services.\n> > \n> > This seems to add a bunch of code just to use NODE_DATA instead of page->pgmap,\n> > without really explaining why just extending dev_pagemap_ops wouldn't work. The\n> > obvious reason is that if you want to support things like reclaim, compaction,\n> > etc. these pages need to be on the LRU, which is a little bit hard when that\n> > field is also used by the pgmap pointer for ZONE_DEVICE pages.\n> > \n> \n> You don't have to squint because it was deliberate :]\n> \n> The callback similarity is the feature - they're the same logical\n> operations.  The difference is the direction of the defaults.\n> \n> Extending ZONE_DEVICE into these areas requires the same set of hooks,\n> plus distinguishing \"old ZONE_DEVICE\" from \"new ZONE_DEVICE\".\n> \n> Where there are new injection sites, it's because ZONE_DEVICE opts\n> out of ever touching that code in some other silently implied way.\n> \n> For example, reclaim/compaction doesn't run because ZONE_DEVICE doesn't\n> add to managed_pages (among other reasons).\n> \n> You'd have to go figure out how to hack those things into ZONE_DEVICE \n> *and then* opt every *other* ZONE_DEVICE mode *back out*.\n> \n> So you still end up with something like this anyway:\n> \n> static inline bool folio_managed_handle_fault(struct folio *folio,\n>                                               struct vm_fault *vmf,\n>                                               enum pgtable_level level,\n>                                               vm_fault_t *ret)\n> {\n>         /* Zone device pages use swap entries; handled in do_swap_page */\n>         if (folio_is_zone_device(folio))\n>                 return false;\n> \n>         if (folio_is_private_node(folio))\n> \t\t...\n>         return false;\n> }\n> \n> \n> > example page_ext could be used.  Or I hear struct page may go away in place of\n> > folios any day now, so maybe that gives us space for both :-)\n> > \n> \n> If NUMA is the interface we want, then NODE_DATA is the right direction\n> regardless of struct page's future or what zone it lives in.\n> \n> There's no reason to keep per-page pgmap w/ device-to-node mappings.\n> \n> You can have one driver manage multiple devices with the same numa node\n> if it uses the same owner context (PFN already differentiates devices).\n> \n> The existing code allows for this.\n> \n> > The above also looks pretty similar to the existing ZONE_DEVICE methods for\n> > doing this which is another reason to argue for just building up the feature set\n> > of the existing boondoggle rather than adding another thingymebob.\n> >\n> > It seems the key thing we are looking for is:\n> > \n> > 1) A userspace API to allocate/manage device memory (ie. move_pages(), mbind(),\n> > etc.)\n> > \n> > 2) Allowing reclaim/LRU list processing of device memory.\n> > \n> > From my perspective both of these are interesting and I look forward to the\n> > discussion (hopefully I can make it to LSFMM). Mostly I'm interested in the\n> > implementation as this does on the surface seem to sprinkle around and duplicate\n> > a lot of hooks similar to what ZONE_DEVICE already provides.\n> > \n> \n> On (1): ZONE_DEVICE NUMA UAPI is harder than it looks from the surface\n> \n> Much of the kernel mm/ infrastructure is written on top of the buddy and\n> expects N_MEMORY to be the sole arbiter of \"Where to Acquire Pages\".\n> \n> Mempolicy depends on:\n>    - Buddy support or a new alloc hook around the buddy\n> \n>    - Migration support (mbind() after allocation migrates)\n>      - Migration also deeply assumes buddy and LRU support\n> \n>    - Changing validations on node states\n>      - mempolicy checks N_MEMORY membership, so you have to hack\n>        N_MEMORY onto ZONE_DEVICE\n>        (or teach it about a new node state... N_MEMORY_PRIVATE)\n> \n> \n> Getting mempolicy to work with N_MEMORY_PRIVATE amounts to adding 2\n> lines of code in vma_alloc_folio_noprof:\n> \n> struct folio *vma_alloc_folio_noprof(gfp_t gfp, int order,\n>                                      struct vm_area_struct *vma,\n> \t\t\t\t     unsigned long addr)\n> {\n>         if (pol->flags & MPOL_F_PRIVATE)\n>                 gfp |= __GFP_PRIVATE;\n> \n>         folio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n> \t/* Woo! I faulted a DEVICE PAGE! */\n> }\n> \n> But this requires the pages to be managed by the buddy.\n> \n> The rest of the mempolicy support is around keeping sane nodemasks when\n> things like cpuset.mems rebinds occur and validating you don't end up\n> with private nodes that don't support mempolicy in your nodemask.\n> \n> You have to do all of this anyway, but with the added bonus of fighting\n> with the overloaded nature of ZONE_DEVICE at every step.\n> \n> ==========\n> \n> On (2): Assume you solve LRU. \n> \n> Zone Device has no free lists, managed_pages, or watermarks.\n> \n> kswapd can't run, compaction has no targets, vmscan's pressure model\n> doesn't function.  These all come for free when the pages are\n> buddy-managed on a real zone.  Why re-invent the wheel?\n> \n> ==========\n> \n> So you really have two options here:\n> \n> a) Put pages in the buddy, or\n> \n> b) Add pgmap->device_alloc() callbacks at every allocation site that\n>    could target a node:\n>      - vma_alloc_folio\n>      - alloc_migration_target\n>      - alloc_demote_folio\n>      - alloc_pages_node\n>      - alloc_contig_pages\n>      - list goes on\n> \n> Or more likely - hooking get_page_from_freelist.  Which at that\n> point... just use the buddy?  You're already deep in the hot path.\n> \n> > \n> > For basic allocation I agree this is the case. But there's no reason some device\n> > allocator library couldn't be written. Or in fact as pointed out above reuse the\n> > already existing one in drm_buddy.c.  So would be interested to hear arguments\n> > for why allocation has to be done by the mm allocator and/or why an allocation\n> > library wouldn't work here given DRM already has them.\n> > \n> \n> Using the buddy underpins the rest of mm/ services we want to re-use.\n> \n> That's basically it.  Otherwise you have to inject hooks into every\n> surface that touches the buddy...\n> \n> ... or in the buddy (get_page_from_freelist), at which point why not\n> just use the buddy?\n> \n> ~Gregory\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "I also think the usage patterns don't quite match (today). GPUs seem to care very much about specific size allocations, contiguity, how users get swapped in/out, how reclaim occurs, specific shutdown procedures - etc. A private node service just wants to be the arbiter of who can access the memory, but it may not really care to have extremely deep control over the actual management of said memory. Maybe there is a world where GPUs trend in that direction, but it's certainly not where they are today. But trying to generalize DRM's infrastructure seems bad.  At best we end up with two mm/ implementations - not good at all.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Wed, Feb 25, 2026 at 02:21:54PM -0800, Matthew Brost wrote:\n> On Tue, Feb 24, 2026 at 10:17:38AM -0500, Gregory Price wrote:\n> > \n> > The idea that drm/ is going to switch to private nodes is outside the\n> > realm of reality, but part of that is because of years of infrastructure\n> > built on the assumption that re-using mm/ is infeasible.\n> \n> I was about to chime in with essentially the same comment about DRM.\n> Switching over to core-managed MM is a massive shift and is likely\n> infeasible, or so extreme that we\\u2019d end up throwing away any the\n> existing driver and starting from scratch. At least for Xe, our MM code\n> is baked into all meaningful components of the driver. It\\u2019s also a\n> unified driver that has to work on iGPU, dGPU over PCIe, dGPU over a\n> coherent bus once we get there, devices with GPU pagefaults, and devices\n> without GPU pagefaults. It also has to support both 3D and compute\n> user-space stacks, etc. So requirements of what it needs to support is\n> quite large.\n>\n> IIRC, Christian once mentioned that AMD was exploring using NUMA and\n> udma-buf rather than DRM GEMs for MM on coherent-bus devices. I would\n> think AMDGPU has nearly all the same requirements as Xe, aside from\n> supporting both 3D and compute stacks, since AMDKFD currently handles\n> compute. It might be worth getting Christian\\u2019s input on this RFC as he\n> likely has better insight then myself on DRM's future here.\n> \n\nI also think the usage patterns don't quite match (today).\n\nGPUs seem to care very much about specific size allocations, contiguity,\nhow users get swapped in/out, how reclaim occurs, specific shutdown\nprocedures - etc.\n\nA private node service just wants to be the arbiter of who can access\nthe memory, but it may not really care to have extremely deep control\nover the actual management of said memory.\n\nMaybe there is a world where GPUs trend in that direction, but it's\ncertainly not where they are today.\n\nBut trying to generalize DRM's infrastructure seems bad.  At best we\nend up with two mm/ implementations - not good at all.\n\n\nI do think this fundamentally changes how NUMA gets used by userspace,\nbut I think userspace should stop reasoning about nodes for memory\nplacement beyond simple cpu-socket-dram mappings </opinion>.\n\n(using mm/mempolicy.c just makes your code less portable by design)\n\n---\n\nAs a side note, This infrastructure is not just limited to devices,\nand I probably should have pointed this out in the cover.\n\nWe could create service-dedicated memory pools directly from DRAM.\n\nSomething I was exploring this week:  Private-CMA\n\nHack off a chunk of DRAM at boot, hand it to a driver to hotplug as a\nprivate node in ZONE_NORMAL with MIGRATE_CMA, and add that node as a\nvalid demotion target.\n\nYou get:\n\n1) A node of general purpose memory full of (reasonably) cold data\n2) Tracked by CMA\n3) The CMA is dedicated to a single service\n4) And the memory can be pinned for DMA\n\nRight now CMA is somewhat of a free-for-all and if you have multiple CMA\nusers you can end up in situations where even CMA fragments.\n\nSplitting up users might be nice - but you need some kind of delimiting\nmechanism for that.  A node seems just about right.\n\n~Gregory\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v1 0/3] cxl region changes for Type2 support",
          "message_id": "aZ3MoKZgs26C2PrZ@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3MoKZgs26C2PrZ@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T16:07:00Z",
          "in_reply_to": null,
          "ack_type": "Tested-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v1 3/3] cxl/region: Factor out interleave granularity setup",
          "message_id": "aZ3MdkPQf-5aXZ9j@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3MdkPQf-5aXZ9j@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T16:06:18Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v1 2/3] cxl/region: Factor out interleave ways setup",
          "message_id": "aZ3MUyOy2JgavERa@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3MUyOy2JgavERa@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T16:05:43Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v1 1/3] cxl: Make region type based on endpoint type",
          "message_id": "aZ3MHVcheVVoooiC@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3MHVcheVVoooiC@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T16:04:48Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Jeff Layton",
      "primary_email": "jlayton@kernel.org",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] nfsd: fix heap overflow in NFSv4.0 LOCK replay cache",
          "message_id": "20260224-v4-0-lock-overflow-v1-1-22beeaf5cf6b@kernel.org",
          "url": "https://lore.kernel.org/all/20260224-v4-0-lock-overflow-v1-1-22beeaf5cf6b@kernel.org/",
          "date": "2026-02-24T16:33:44Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The NFSv4.0 replay cache uses a fixed 112-byte inline buffer (rp_ibuf[NFSD4_REPLAY_ISIZE]) to store encoded operation responses. This size was calculated based on OPEN responses and does not account for LOCK denied responses, which include the conflicting lock owner as a variable-length field up to 1024 bytes (NFS4_OPAQUE_LIMIT).\n\nWhen a LOCK operation is denied due to a conflict with an existing lock that has a large owner, nfsd4_encode_operation() copies the full encoded response into the undersized replay buffer via read_bytes_from_xdr_buf() with no bounds check. This results in a slab-out-of-bounds write of up to 944 bytes past the end of the buffer, corrupting adjacent heap memory.\n\nThis can be triggered remotely by an unauthenticated attacker with two cooperating NFSv4.0 clients: one sets a lock with a large owner string, then the other requests a conflicting lock to provoke the denial.\n\nWe could fix this by increasing NFSD4_REPLAY_ISIZE to allow for a full opaque, but that would increase the size of every stateowner, when most lockowners are not that large.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Chuck Lever",
              "summary": "From: Chuck Lever <chuck.lever@oracle.com> Applied to nfsd-testing, thanks! [1/1] nfsd: fix heap overflow in NFSv4.0 LOCK replay cache commit: 1e8e9913672a31c6fdd0d237cd3cec88435bd66e",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "applied"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nOn Tue, 24 Feb 2026 11:33:35 -0500, Jeff Layton wrote:\n> The NFSv4.0 replay cache uses a fixed 112-byte inline buffer\n> (rp_ibuf[NFSD4_REPLAY_ISIZE]) to store encoded operation responses.\n> This size was calculated based on OPEN responses and does not account\n> for LOCK denied responses, which include the conflicting lock owner as\n> a variable-length field up to 1024 bytes (NFS4_OPAQUE_LIMIT).\n> \n> When a LOCK operation is denied due to a conflict with an existing lock\n> that has a large owner, nfsd4_encode_operation() copies the full encoded\n> response into the undersized replay buffer via read_bytes_from_xdr_buf()\n> with no bounds check. This results in a slab-out-of-bounds write of up\n> to 944 bytes past the end of the buffer, corrupting adjacent heap memory.\n> \n> [...]\n\nApplied to nfsd-testing, thanks!\n\n[1/1] nfsd: fix heap overflow in NFSv4.0 LOCK replay cache\n      commit: 1e8e9913672a31c6fdd0d237cd3cec88435bd66e\n\n--\nChuck Lever\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] nfsd: convert global state_lock to per-net deleg_lock",
          "message_id": "20260224-nfsd-deleg-lock-v1-1-1df17c1daa47@kernel.org",
          "url": "https://lore.kernel.org/all/20260224-nfsd-deleg-lock-v1-1-1df17c1daa47@kernel.org/",
          "date": "2026-02-24T13:28:24Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Replace the global state_lock spinlock with a per-nfsd_net deleg_lock. The state_lock was only used to protect delegation lifecycle operations (the del_recall_lru list and delegation hash/unhash), all of which are scoped to a single network namespace. Making the lock per-net removes a source of unnecessary contention between containers.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Chuck Lever",
              "summary": "From: Chuck Lever <chuck.lever@oracle.com> Applied to nfsd-testing with minor changes, thanks! [1/1] nfsd: convert global state_lock to per-net deleg_lock commit: 87d8659010fe5ba78759ad7b8780656f1c3d350a",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "applied"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nOn Tue, 24 Feb 2026 08:28:11 -0500, Jeff Layton wrote:\n> Replace the global state_lock spinlock with a per-nfsd_net deleg_lock.\n> The state_lock was only used to protect delegation lifecycle operations\n> (the del_recall_lru list and delegation hash/unhash), all of which are\n> scoped to a single network namespace. Making the lock per-net removes\n> a source of unnecessary contention between containers.\n> \n> \n> [...]\n\nApplied to nfsd-testing with minor changes, thanks!\n\n[1/1] nfsd: convert global state_lock to per-net deleg_lock\n      commit: 87d8659010fe5ba78759ad7b8780656f1c3d350a\n\n--\nChuck Lever\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] nfsd: use dynamic allocation for oversized NFSv4.0 replay cache",
          "message_id": "f16c1806a705e08252b1b39ea44b1de1e6be17d6.camel@kernel.org",
          "url": "https://lore.kernel.org/all/f16c1806a705e08252b1b39ea44b1de1e6be17d6.camel@kernel.org/",
          "date": "2026-02-24T19:59:19Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] nfsd: use dynamic allocation for oversized NFSv4.0 replay cache",
          "message_id": "5c6b6e52619caf720912639697af5b388a3ea79a.camel@kernel.org",
          "url": "https://lore.kernel.org/all/5c6b6e52619caf720912639697af5b388a3ea79a.camel@kernel.org/",
          "date": "2026-02-24T19:51:44Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Chuck Lever",
              "summary": "I don't disagree at all. My concern is handling replay compliantly. Maybe there's another approach.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/24/26 2:51 PM, Jeff Layton wrote:\n> On Tue, 2026-02-24 at 14:42 -0500, Chuck Lever wrote:\n>> On 2/24/26 2:39 PM, Jeff Layton wrote:\n>>> On Tue, 2026-02-24 at 14:33 -0500, Chuck Lever wrote:\n>>>> From: Chuck Lever <chuck.lever@oracle.com>\n>>>>\n>>>> Commit 1e8e9913672a (\"nfsd: fix heap overflow in NFSv4.0 LOCK\n>>>> replay cache\") capped the replay cache copy at NFSD4_REPLAY_ISIZE\n>>>> to prevent a heap overflow, but set rp_buflen to zero when the\n>>>> encoded response exceeded the inline buffer. A retransmitted LOCK\n>>>> reaching the replay path then produced only a status code with no\n>>>> operation body, resulting in a malformed XDR response.\n>>>>\n>>>> When the encoded response exceeds the 112-byte inline rp_ibuf, a\n>>>> buffer is kmalloc'd to hold it. If the allocation fails, rp_buflen\n>>>> remains zero, preserving the behavior from the capped-copy fix.\n>>>> The buffer is freed when the stateowner is released or when a\n>>>> subsequent operation's response fits in the inline buffer.\n>>>>\n>>>> Fixes: 1e8e9913672a (\"nfsd: fix heap overflow in NFSv4.0 LOCK replay cache\")\n>>>> Signed-off-by: Chuck Lever <chuck.lever@oracle.com>\n>>>> ---\n>>>>  fs/nfsd/nfs4state.c | 16 ++++++++++++++++\n>>>>  fs/nfsd/nfs4xdr.c   | 23 ++++++++++++++++-------\n>>>>  fs/nfsd/state.h     | 12 +++++++-----\n>>>>  3 files changed, 39 insertions(+), 12 deletions(-)\n>>>>\n>>>> diff --git a/fs/nfsd/nfs4state.c b/fs/nfsd/nfs4state.c\n>>>> index ba49f49bb93b..b4d0e82b2690 100644\n>>>> --- a/fs/nfsd/nfs4state.c\n>>>> +++ b/fs/nfsd/nfs4state.c\n>>>> @@ -1496,8 +1496,24 @@ release_all_access(struct nfs4_ol_stateid *stp)\n>>>>  \t}\n>>>>  }\n>>>>  \n>>>> +/**\n>>>> + * nfs4_replay_free_cache - release dynamically allocated replay buffer\n>>>> + * @rp: replay cache to reset\n>>>> + *\n>>>> + * If @rp->rp_buf points to a kmalloc'd buffer, free it and reset\n>>>> + * rp_buf to the inline rp_ibuf. Always zeroes rp_buflen.\n>>>> + */\n>>>> +void nfs4_replay_free_cache(struct nfs4_replay *rp)\n>>>> +{\n>>>> +\tif (rp->rp_buf != rp->rp_ibuf)\n>>>> +\t\tkfree(rp->rp_buf);\n>>>> +\trp->rp_buf = rp->rp_ibuf;\n>>>> +\trp->rp_buflen = 0;\n>>>> +}\n>>>> +\n>>>>  static inline void nfs4_free_stateowner(struct nfs4_stateowner *sop)\n>>>>  {\n>>>> +\tnfs4_replay_free_cache(&sop->so_replay);\n>>>>  \tkfree(sop->so_owner.data);\n>>>>  \tsop->so_ops->so_free(sop);\n>>>>  }\n>>>> diff --git a/fs/nfsd/nfs4xdr.c b/fs/nfsd/nfs4xdr.c\n>>>> index 690f7a3122ec..2a0946c630e1 100644\n>>>> --- a/fs/nfsd/nfs4xdr.c\n>>>> +++ b/fs/nfsd/nfs4xdr.c\n>>>> @@ -6282,14 +6282,23 @@ nfsd4_encode_operation(struct nfsd4_compoundres *resp, struct nfsd4_op *op)\n>>>>  \t\tint len = xdr->buf->len - (op_status_offset + XDR_UNIT);\n>>>>  \n>>>>  \t\tso->so_replay.rp_status = op->status;\n>>>> -\t\tif (len <= NFSD4_REPLAY_ISIZE) {\n>>>> -\t\t\tso->so_replay.rp_buflen = len;\n>>>> -\t\t\tread_bytes_from_xdr_buf(xdr->buf,\n>>>> -\t\t\t\t\t\top_status_offset + XDR_UNIT,\n>>>> -\t\t\t\t\t\tso->so_replay.rp_buf, len);\n>>>> -\t\t} else {\n>>>> -\t\t\tso->so_replay.rp_buflen = 0;\n>>>> +\t\tif (len > NFSD4_REPLAY_ISIZE) {\n>>>> +\t\t\tchar *buf = kmalloc(len, GFP_KERNEL);\n>>>> +\n>>>> +\t\t\tnfs4_replay_free_cache(&so->so_replay);\n>>>> +\t\t\tif (buf) {\n>>>> +\t\t\t\tso->so_replay.rp_buf = buf;\n>>>> +\t\t\t} else {\n>>>> +\t\t\t\t/* rp_buflen already zeroed; skip caching */\n>>>> +\t\t\t\tgoto status;\n>>>> +\t\t\t}\n>>>> +\t\t} else if (so->so_replay.rp_buf != so->so_replay.rp_ibuf) {\n>>>> +\t\t\tnfs4_replay_free_cache(&so->so_replay);\n>>>>  \t\t}\n>>>> +\t\tso->so_replay.rp_buflen = len;\n>>>> +\t\tread_bytes_from_xdr_buf(xdr->buf,\n>>>> +\t\t\t\t\top_status_offset + XDR_UNIT,\n>>>> +\t\t\t\t\tso->so_replay.rp_buf, len);\n>>>>  \t}\n>>>>  status:\n>>>>  \top->status = nfsd4_map_status(op->status,\n>>>> diff --git a/fs/nfsd/state.h b/fs/nfsd/state.h\n>>>> index 3159c7b67f50..9b05462da4cc 100644\n>>>> --- a/fs/nfsd/state.h\n>>>> +++ b/fs/nfsd/state.h\n>>>> @@ -554,10 +554,10 @@ struct nfs4_client_reclaim {\n>>>>   *   ~32(deleg. ace) = 112 bytes\n>>>>   *\n>>>>   * Some responses can exceed this. A LOCK denial includes the conflicting\n>>>> - * lock owner, which can be up to 1024 bytes (NFS4_OPAQUE_LIMIT). Responses\n>>>> - * larger than REPLAY_ISIZE are not cached in rp_ibuf; only rp_status is\n>>>> - * saved. Enlarging this constant increases the size of every\n>>>> - * nfs4_stateowner.\n>>>> + * lock owner, which can be up to 1024 bytes (NFS4_OPAQUE_LIMIT). When a\n>>>> + * response exceeds REPLAY_ISIZE, a buffer is dynamically allocated. If\n>>>> + * that allocation fails, only rp_status is saved. Enlarging this constant\n>>>> + * increases the size of every nfs4_stateowner.\n>>>>   */\n>>>>  \n>>>>  #define NFSD4_REPLAY_ISIZE       112 \n>>>> @@ -569,12 +569,14 @@ struct nfs4_client_reclaim {\n>>>>  struct nfs4_replay {\n>>>>  \t__be32\t\t\trp_status;\n>>>>  \tunsigned int\t\trp_buflen;\n>>>> -\tchar\t\t\t*rp_buf;\n>>>> +\tchar\t\t\t*rp_buf; /* rp_ibuf or kmalloc'd */\n>>>>  \tstruct knfsd_fh\t\trp_openfh;\n>>>>  \tint\t\t\trp_locked;\n>>>>  \tchar\t\t\trp_ibuf[NFSD4_REPLAY_ISIZE];\n>>>>  };\n>>>>  \n>>>> +extern void nfs4_replay_free_cache(struct nfs4_replay *rp);\n>>>> +\n>>>>  struct nfs4_stateowner;\n>>>>  \n>>>>  struct nfs4_stateowner_operations {\n>>>\n>>>\n>>> Certainly a reasonable approach if we care about full correctness when\n>>> dealing with a large lockowner on NFSv4.0. Do we?\n>>\n>> The idea would be to either:\n>>\n>> o Backport your fix and not this update, or\n>> o Squash these two together, and backport both\n>>\n>> Admittedly this is a narrow corner case for a minor version that is\n>> destined for the scrap heap.\n>>\n> \n> Right. I ask because I looked at this approach when I was fixing this,\n> and decided it wasn't worthwhile. I certainly won't stand in your way\n> if you decide you want to handle long lockowner blobs, but I doubt any\n> legitimate user will ever care.\n\nI don't disagree at all. My concern is handling replay compliantly.\nMaybe there's another approach.\n\n\n-- \nChuck Lever\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "I think that the only other way is to grow NFSD4_REPLAY_ISIZE, and doing dynamic allocation is preferable to that, IMO. To be clear: I don't have a problem with your patch. It just didn't seem worthwhile to me. If you think it's worth fixing though, then go for it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, 2026-02-24 at 14:53 -0500, Chuck Lever wrote:\n> On 2/24/26 2:51 PM, Jeff Layton wrote:\n> > On Tue, 2026-02-24 at 14:42 -0500, Chuck Lever wrote:\n> > > On 2/24/26 2:39 PM, Jeff Layton wrote:\n> > > > On Tue, 2026-02-24 at 14:33 -0500, Chuck Lever wrote:\n> > > > > From: Chuck Lever <chuck.lever@oracle.com>\n> > > > > \n> > > > > Commit 1e8e9913672a (\"nfsd: fix heap overflow in NFSv4.0 LOCK\n> > > > > replay cache\") capped the replay cache copy at NFSD4_REPLAY_ISIZE\n> > > > > to prevent a heap overflow, but set rp_buflen to zero when the\n> > > > > encoded response exceeded the inline buffer. A retransmitted LOCK\n> > > > > reaching the replay path then produced only a status code with no\n> > > > > operation body, resulting in a malformed XDR response.\n> > > > > \n> > > > > When the encoded response exceeds the 112-byte inline rp_ibuf, a\n> > > > > buffer is kmalloc'd to hold it. If the allocation fails, rp_buflen\n> > > > > remains zero, preserving the behavior from the capped-copy fix.\n> > > > > The buffer is freed when the stateowner is released or when a\n> > > > > subsequent operation's response fits in the inline buffer.\n> > > > > \n> > > > > Fixes: 1e8e9913672a (\"nfsd: fix heap overflow in NFSv4.0 LOCK replay cache\")\n> > > > > Signed-off-by: Chuck Lever <chuck.lever@oracle.com>\n> > > > > ---\n> > > > >  fs/nfsd/nfs4state.c | 16 ++++++++++++++++\n> > > > >  fs/nfsd/nfs4xdr.c   | 23 ++++++++++++++++-------\n> > > > >  fs/nfsd/state.h     | 12 +++++++-----\n> > > > >  3 files changed, 39 insertions(+), 12 deletions(-)\n> > > > > \n> > > > > diff --git a/fs/nfsd/nfs4state.c b/fs/nfsd/nfs4state.c\n> > > > > index ba49f49bb93b..b4d0e82b2690 100644\n> > > > > --- a/fs/nfsd/nfs4state.c\n> > > > > +++ b/fs/nfsd/nfs4state.c\n> > > > > @@ -1496,8 +1496,24 @@ release_all_access(struct nfs4_ol_stateid *stp)\n> > > > >  \t}\n> > > > >  }\n> > > > >  \n> > > > > +/**\n> > > > > + * nfs4_replay_free_cache - release dynamically allocated replay buffer\n> > > > > + * @rp: replay cache to reset\n> > > > > + *\n> > > > > + * If @rp->rp_buf points to a kmalloc'd buffer, free it and reset\n> > > > > + * rp_buf to the inline rp_ibuf. Always zeroes rp_buflen.\n> > > > > + */\n> > > > > +void nfs4_replay_free_cache(struct nfs4_replay *rp)\n> > > > > +{\n> > > > > +\tif (rp->rp_buf != rp->rp_ibuf)\n> > > > > +\t\tkfree(rp->rp_buf);\n> > > > > +\trp->rp_buf = rp->rp_ibuf;\n> > > > > +\trp->rp_buflen = 0;\n> > > > > +}\n> > > > > +\n> > > > >  static inline void nfs4_free_stateowner(struct nfs4_stateowner *sop)\n> > > > >  {\n> > > > > +\tnfs4_replay_free_cache(&sop->so_replay);\n> > > > >  \tkfree(sop->so_owner.data);\n> > > > >  \tsop->so_ops->so_free(sop);\n> > > > >  }\n> > > > > diff --git a/fs/nfsd/nfs4xdr.c b/fs/nfsd/nfs4xdr.c\n> > > > > index 690f7a3122ec..2a0946c630e1 100644\n> > > > > --- a/fs/nfsd/nfs4xdr.c\n> > > > > +++ b/fs/nfsd/nfs4xdr.c\n> > > > > @@ -6282,14 +6282,23 @@ nfsd4_encode_operation(struct nfsd4_compoundres *resp, struct nfsd4_op *op)\n> > > > >  \t\tint len = xdr->buf->len - (op_status_offset + XDR_UNIT);\n> > > > >  \n> > > > >  \t\tso->so_replay.rp_status = op->status;\n> > > > > -\t\tif (len <= NFSD4_REPLAY_ISIZE) {\n> > > > > -\t\t\tso->so_replay.rp_buflen = len;\n> > > > > -\t\t\tread_bytes_from_xdr_buf(xdr->buf,\n> > > > > -\t\t\t\t\t\top_status_offset + XDR_UNIT,\n> > > > > -\t\t\t\t\t\tso->so_replay.rp_buf, len);\n> > > > > -\t\t} else {\n> > > > > -\t\t\tso->so_replay.rp_buflen = 0;\n> > > > > +\t\tif (len > NFSD4_REPLAY_ISIZE) {\n> > > > > +\t\t\tchar *buf = kmalloc(len, GFP_KERNEL);\n> > > > > +\n> > > > > +\t\t\tnfs4_replay_free_cache(&so->so_replay);\n> > > > > +\t\t\tif (buf) {\n> > > > > +\t\t\t\tso->so_replay.rp_buf = buf;\n> > > > > +\t\t\t} else {\n> > > > > +\t\t\t\t/* rp_buflen already zeroed; skip caching */\n> > > > > +\t\t\t\tgoto status;\n> > > > > +\t\t\t}\n> > > > > +\t\t} else if (so->so_replay.rp_buf != so->so_replay.rp_ibuf) {\n> > > > > +\t\t\tnfs4_replay_free_cache(&so->so_replay);\n> > > > >  \t\t}\n> > > > > +\t\tso->so_replay.rp_buflen = len;\n> > > > > +\t\tread_bytes_from_xdr_buf(xdr->buf,\n> > > > > +\t\t\t\t\top_status_offset + XDR_UNIT,\n> > > > > +\t\t\t\t\tso->so_replay.rp_buf, len);\n> > > > >  \t}\n> > > > >  status:\n> > > > >  \top->status = nfsd4_map_status(op->status,\n> > > > > diff --git a/fs/nfsd/state.h b/fs/nfsd/state.h\n> > > > > index 3159c7b67f50..9b05462da4cc 100644\n> > > > > --- a/fs/nfsd/state.h\n> > > > > +++ b/fs/nfsd/state.h\n> > > > > @@ -554,10 +554,10 @@ struct nfs4_client_reclaim {\n> > > > >   *   ~32(deleg. ace) = 112 bytes\n> > > > >   *\n> > > > >   * Some responses can exceed this. A LOCK denial includes the conflicting\n> > > > > - * lock owner, which can be up to 1024 bytes (NFS4_OPAQUE_LIMIT). Responses\n> > > > > - * larger than REPLAY_ISIZE are not cached in rp_ibuf; only rp_status is\n> > > > > - * saved. Enlarging this constant increases the size of every\n> > > > > - * nfs4_stateowner.\n> > > > > + * lock owner, which can be up to 1024 bytes (NFS4_OPAQUE_LIMIT). When a\n> > > > > + * response exceeds REPLAY_ISIZE, a buffer is dynamically allocated. If\n> > > > > + * that allocation fails, only rp_status is saved. Enlarging this constant\n> > > > > + * increases the size of every nfs4_stateowner.\n> > > > >   */\n> > > > >  \n> > > > >  #define NFSD4_REPLAY_ISIZE       112 \n> > > > > @@ -569,12 +569,14 @@ struct nfs4_client_reclaim {\n> > > > >  struct nfs4_replay {\n> > > > >  \t__be32\t\t\trp_status;\n> > > > >  \tunsigned int\t\trp_buflen;\n> > > > > -\tchar\t\t\t*rp_buf;\n> > > > > +\tchar\t\t\t*rp_buf; /* rp_ibuf or kmalloc'd */\n> > > > >  \tstruct knfsd_fh\t\trp_openfh;\n> > > > >  \tint\t\t\trp_locked;\n> > > > >  \tchar\t\t\trp_ibuf[NFSD4_REPLAY_ISIZE];\n> > > > >  };\n> > > > >  \n> > > > > +extern void nfs4_replay_free_cache(struct nfs4_replay *rp);\n> > > > > +\n> > > > >  struct nfs4_stateowner;\n> > > > >  \n> > > > >  struct nfs4_stateowner_operations {\n> > > > \n> > > > \n> > > > Certainly a reasonable approach if we care about full correctness when\n> > > > dealing with a large lockowner on NFSv4.0. Do we?\n> > > \n> > > The idea would be to either:\n> > > \n> > > o Backport your fix and not this update, or\n> > > o Squash these two together, and backport both\n> > > \n> > > Admittedly this is a narrow corner case for a minor version that is\n> > > destined for the scrap heap.\n> > > \n> > \n> > Right. I ask because I looked at this approach when I was fixing this,\n> > and decided it wasn't worthwhile. I certainly won't stand in your way\n> > if you decide you want to handle long lockowner blobs, but I doubt any\n> > legitimate user will ever care.\n> \n> I don't disagree at all. My concern is handling replay compliantly.\n> Maybe there's another approach.\n> \n\nI think that the only other way is to grow NFSD4_REPLAY_ISIZE, and\ndoing dynamic allocation is preferable to that, IMO.\n\nTo be clear: I don't have a problem with your patch. It just didn't\nseem worthwhile to me. If you think it's worth fixing though, then go\nfor it.\n-- \nJeff Layton <jlayton@kernel.org>\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] nfsd: use dynamic allocation for oversized NFSv4.0 replay cache",
          "message_id": "887c1ca78b34974160dee3ce7f25d6d077da93ab.camel@kernel.org",
          "url": "https://lore.kernel.org/all/887c1ca78b34974160dee3ce7f25d6d077da93ab.camel@kernel.org/",
          "date": "2026-02-24T19:39:52Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Chuck Lever",
              "summary": "The idea would be to either: o Backport your fix and not this update, or o Squash these two together, and backport both Admittedly this is a narrow corner case for a minor version that is destined for the scrap heap.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/24/26 2:39 PM, Jeff Layton wrote:\n> On Tue, 2026-02-24 at 14:33 -0500, Chuck Lever wrote:\n>> From: Chuck Lever <chuck.lever@oracle.com>\n>>\n>> Commit 1e8e9913672a (\"nfsd: fix heap overflow in NFSv4.0 LOCK\n>> replay cache\") capped the replay cache copy at NFSD4_REPLAY_ISIZE\n>> to prevent a heap overflow, but set rp_buflen to zero when the\n>> encoded response exceeded the inline buffer. A retransmitted LOCK\n>> reaching the replay path then produced only a status code with no\n>> operation body, resulting in a malformed XDR response.\n>>\n>> When the encoded response exceeds the 112-byte inline rp_ibuf, a\n>> buffer is kmalloc'd to hold it. If the allocation fails, rp_buflen\n>> remains zero, preserving the behavior from the capped-copy fix.\n>> The buffer is freed when the stateowner is released or when a\n>> subsequent operation's response fits in the inline buffer.\n>>\n>> Fixes: 1e8e9913672a (\"nfsd: fix heap overflow in NFSv4.0 LOCK replay cache\")\n>> Signed-off-by: Chuck Lever <chuck.lever@oracle.com>\n>> ---\n>>  fs/nfsd/nfs4state.c | 16 ++++++++++++++++\n>>  fs/nfsd/nfs4xdr.c   | 23 ++++++++++++++++-------\n>>  fs/nfsd/state.h     | 12 +++++++-----\n>>  3 files changed, 39 insertions(+), 12 deletions(-)\n>>\n>> diff --git a/fs/nfsd/nfs4state.c b/fs/nfsd/nfs4state.c\n>> index ba49f49bb93b..b4d0e82b2690 100644\n>> --- a/fs/nfsd/nfs4state.c\n>> +++ b/fs/nfsd/nfs4state.c\n>> @@ -1496,8 +1496,24 @@ release_all_access(struct nfs4_ol_stateid *stp)\n>>  \t}\n>>  }\n>>  \n>> +/**\n>> + * nfs4_replay_free_cache - release dynamically allocated replay buffer\n>> + * @rp: replay cache to reset\n>> + *\n>> + * If @rp->rp_buf points to a kmalloc'd buffer, free it and reset\n>> + * rp_buf to the inline rp_ibuf. Always zeroes rp_buflen.\n>> + */\n>> +void nfs4_replay_free_cache(struct nfs4_replay *rp)\n>> +{\n>> +\tif (rp->rp_buf != rp->rp_ibuf)\n>> +\t\tkfree(rp->rp_buf);\n>> +\trp->rp_buf = rp->rp_ibuf;\n>> +\trp->rp_buflen = 0;\n>> +}\n>> +\n>>  static inline void nfs4_free_stateowner(struct nfs4_stateowner *sop)\n>>  {\n>> +\tnfs4_replay_free_cache(&sop->so_replay);\n>>  \tkfree(sop->so_owner.data);\n>>  \tsop->so_ops->so_free(sop);\n>>  }\n>> diff --git a/fs/nfsd/nfs4xdr.c b/fs/nfsd/nfs4xdr.c\n>> index 690f7a3122ec..2a0946c630e1 100644\n>> --- a/fs/nfsd/nfs4xdr.c\n>> +++ b/fs/nfsd/nfs4xdr.c\n>> @@ -6282,14 +6282,23 @@ nfsd4_encode_operation(struct nfsd4_compoundres *resp, struct nfsd4_op *op)\n>>  \t\tint len = xdr->buf->len - (op_status_offset + XDR_UNIT);\n>>  \n>>  \t\tso->so_replay.rp_status = op->status;\n>> -\t\tif (len <= NFSD4_REPLAY_ISIZE) {\n>> -\t\t\tso->so_replay.rp_buflen = len;\n>> -\t\t\tread_bytes_from_xdr_buf(xdr->buf,\n>> -\t\t\t\t\t\top_status_offset + XDR_UNIT,\n>> -\t\t\t\t\t\tso->so_replay.rp_buf, len);\n>> -\t\t} else {\n>> -\t\t\tso->so_replay.rp_buflen = 0;\n>> +\t\tif (len > NFSD4_REPLAY_ISIZE) {\n>> +\t\t\tchar *buf = kmalloc(len, GFP_KERNEL);\n>> +\n>> +\t\t\tnfs4_replay_free_cache(&so->so_replay);\n>> +\t\t\tif (buf) {\n>> +\t\t\t\tso->so_replay.rp_buf = buf;\n>> +\t\t\t} else {\n>> +\t\t\t\t/* rp_buflen already zeroed; skip caching */\n>> +\t\t\t\tgoto status;\n>> +\t\t\t}\n>> +\t\t} else if (so->so_replay.rp_buf != so->so_replay.rp_ibuf) {\n>> +\t\t\tnfs4_replay_free_cache(&so->so_replay);\n>>  \t\t}\n>> +\t\tso->so_replay.rp_buflen = len;\n>> +\t\tread_bytes_from_xdr_buf(xdr->buf,\n>> +\t\t\t\t\top_status_offset + XDR_UNIT,\n>> +\t\t\t\t\tso->so_replay.rp_buf, len);\n>>  \t}\n>>  status:\n>>  \top->status = nfsd4_map_status(op->status,\n>> diff --git a/fs/nfsd/state.h b/fs/nfsd/state.h\n>> index 3159c7b67f50..9b05462da4cc 100644\n>> --- a/fs/nfsd/state.h\n>> +++ b/fs/nfsd/state.h\n>> @@ -554,10 +554,10 @@ struct nfs4_client_reclaim {\n>>   *   ~32(deleg. ace) = 112 bytes\n>>   *\n>>   * Some responses can exceed this. A LOCK denial includes the conflicting\n>> - * lock owner, which can be up to 1024 bytes (NFS4_OPAQUE_LIMIT). Responses\n>> - * larger than REPLAY_ISIZE are not cached in rp_ibuf; only rp_status is\n>> - * saved. Enlarging this constant increases the size of every\n>> - * nfs4_stateowner.\n>> + * lock owner, which can be up to 1024 bytes (NFS4_OPAQUE_LIMIT). When a\n>> + * response exceeds REPLAY_ISIZE, a buffer is dynamically allocated. If\n>> + * that allocation fails, only rp_status is saved. Enlarging this constant\n>> + * increases the size of every nfs4_stateowner.\n>>   */\n>>  \n>>  #define NFSD4_REPLAY_ISIZE       112 \n>> @@ -569,12 +569,14 @@ struct nfs4_client_reclaim {\n>>  struct nfs4_replay {\n>>  \t__be32\t\t\trp_status;\n>>  \tunsigned int\t\trp_buflen;\n>> -\tchar\t\t\t*rp_buf;\n>> +\tchar\t\t\t*rp_buf; /* rp_ibuf or kmalloc'd */\n>>  \tstruct knfsd_fh\t\trp_openfh;\n>>  \tint\t\t\trp_locked;\n>>  \tchar\t\t\trp_ibuf[NFSD4_REPLAY_ISIZE];\n>>  };\n>>  \n>> +extern void nfs4_replay_free_cache(struct nfs4_replay *rp);\n>> +\n>>  struct nfs4_stateowner;\n>>  \n>>  struct nfs4_stateowner_operations {\n> \n> \n> Certainly a reasonable approach if we care about full correctness when\n> dealing with a large lockowner on NFSv4.0. Do we?\n\nThe idea would be to either:\n\no Backport your fix and not this update, or\no Squash these two together, and backport both\n\nAdmittedly this is a narrow corner case for a minor version that is\ndestined for the scrap heap.\n\n\n-- \nChuck Lever\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "Right. I ask because I looked at this approach when I was fixing this, and decided it wasn't worthwhile. I certainly won't stand in your way if you decide you want to handle long lockowner blobs, but I doubt any legitimate user will ever care.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, 2026-02-24 at 14:42 -0500, Chuck Lever wrote:\n> On 2/24/26 2:39 PM, Jeff Layton wrote:\n> > On Tue, 2026-02-24 at 14:33 -0500, Chuck Lever wrote:\n> > > From: Chuck Lever <chuck.lever@oracle.com>\n> > > \n> > > Commit 1e8e9913672a (\"nfsd: fix heap overflow in NFSv4.0 LOCK\n> > > replay cache\") capped the replay cache copy at NFSD4_REPLAY_ISIZE\n> > > to prevent a heap overflow, but set rp_buflen to zero when the\n> > > encoded response exceeded the inline buffer. A retransmitted LOCK\n> > > reaching the replay path then produced only a status code with no\n> > > operation body, resulting in a malformed XDR response.\n> > > \n> > > When the encoded response exceeds the 112-byte inline rp_ibuf, a\n> > > buffer is kmalloc'd to hold it. If the allocation fails, rp_buflen\n> > > remains zero, preserving the behavior from the capped-copy fix.\n> > > The buffer is freed when the stateowner is released or when a\n> > > subsequent operation's response fits in the inline buffer.\n> > > \n> > > Fixes: 1e8e9913672a (\"nfsd: fix heap overflow in NFSv4.0 LOCK replay cache\")\n> > > Signed-off-by: Chuck Lever <chuck.lever@oracle.com>\n> > > ---\n> > >  fs/nfsd/nfs4state.c | 16 ++++++++++++++++\n> > >  fs/nfsd/nfs4xdr.c   | 23 ++++++++++++++++-------\n> > >  fs/nfsd/state.h     | 12 +++++++-----\n> > >  3 files changed, 39 insertions(+), 12 deletions(-)\n> > > \n> > > diff --git a/fs/nfsd/nfs4state.c b/fs/nfsd/nfs4state.c\n> > > index ba49f49bb93b..b4d0e82b2690 100644\n> > > --- a/fs/nfsd/nfs4state.c\n> > > +++ b/fs/nfsd/nfs4state.c\n> > > @@ -1496,8 +1496,24 @@ release_all_access(struct nfs4_ol_stateid *stp)\n> > >  \t}\n> > >  }\n> > >  \n> > > +/**\n> > > + * nfs4_replay_free_cache - release dynamically allocated replay buffer\n> > > + * @rp: replay cache to reset\n> > > + *\n> > > + * If @rp->rp_buf points to a kmalloc'd buffer, free it and reset\n> > > + * rp_buf to the inline rp_ibuf. Always zeroes rp_buflen.\n> > > + */\n> > > +void nfs4_replay_free_cache(struct nfs4_replay *rp)\n> > > +{\n> > > +\tif (rp->rp_buf != rp->rp_ibuf)\n> > > +\t\tkfree(rp->rp_buf);\n> > > +\trp->rp_buf = rp->rp_ibuf;\n> > > +\trp->rp_buflen = 0;\n> > > +}\n> > > +\n> > >  static inline void nfs4_free_stateowner(struct nfs4_stateowner *sop)\n> > >  {\n> > > +\tnfs4_replay_free_cache(&sop->so_replay);\n> > >  \tkfree(sop->so_owner.data);\n> > >  \tsop->so_ops->so_free(sop);\n> > >  }\n> > > diff --git a/fs/nfsd/nfs4xdr.c b/fs/nfsd/nfs4xdr.c\n> > > index 690f7a3122ec..2a0946c630e1 100644\n> > > --- a/fs/nfsd/nfs4xdr.c\n> > > +++ b/fs/nfsd/nfs4xdr.c\n> > > @@ -6282,14 +6282,23 @@ nfsd4_encode_operation(struct nfsd4_compoundres *resp, struct nfsd4_op *op)\n> > >  \t\tint len = xdr->buf->len - (op_status_offset + XDR_UNIT);\n> > >  \n> > >  \t\tso->so_replay.rp_status = op->status;\n> > > -\t\tif (len <= NFSD4_REPLAY_ISIZE) {\n> > > -\t\t\tso->so_replay.rp_buflen = len;\n> > > -\t\t\tread_bytes_from_xdr_buf(xdr->buf,\n> > > -\t\t\t\t\t\top_status_offset + XDR_UNIT,\n> > > -\t\t\t\t\t\tso->so_replay.rp_buf, len);\n> > > -\t\t} else {\n> > > -\t\t\tso->so_replay.rp_buflen = 0;\n> > > +\t\tif (len > NFSD4_REPLAY_ISIZE) {\n> > > +\t\t\tchar *buf = kmalloc(len, GFP_KERNEL);\n> > > +\n> > > +\t\t\tnfs4_replay_free_cache(&so->so_replay);\n> > > +\t\t\tif (buf) {\n> > > +\t\t\t\tso->so_replay.rp_buf = buf;\n> > > +\t\t\t} else {\n> > > +\t\t\t\t/* rp_buflen already zeroed; skip caching */\n> > > +\t\t\t\tgoto status;\n> > > +\t\t\t}\n> > > +\t\t} else if (so->so_replay.rp_buf != so->so_replay.rp_ibuf) {\n> > > +\t\t\tnfs4_replay_free_cache(&so->so_replay);\n> > >  \t\t}\n> > > +\t\tso->so_replay.rp_buflen = len;\n> > > +\t\tread_bytes_from_xdr_buf(xdr->buf,\n> > > +\t\t\t\t\top_status_offset + XDR_UNIT,\n> > > +\t\t\t\t\tso->so_replay.rp_buf, len);\n> > >  \t}\n> > >  status:\n> > >  \top->status = nfsd4_map_status(op->status,\n> > > diff --git a/fs/nfsd/state.h b/fs/nfsd/state.h\n> > > index 3159c7b67f50..9b05462da4cc 100644\n> > > --- a/fs/nfsd/state.h\n> > > +++ b/fs/nfsd/state.h\n> > > @@ -554,10 +554,10 @@ struct nfs4_client_reclaim {\n> > >   *   ~32(deleg. ace) = 112 bytes\n> > >   *\n> > >   * Some responses can exceed this. A LOCK denial includes the conflicting\n> > > - * lock owner, which can be up to 1024 bytes (NFS4_OPAQUE_LIMIT). Responses\n> > > - * larger than REPLAY_ISIZE are not cached in rp_ibuf; only rp_status is\n> > > - * saved. Enlarging this constant increases the size of every\n> > > - * nfs4_stateowner.\n> > > + * lock owner, which can be up to 1024 bytes (NFS4_OPAQUE_LIMIT). When a\n> > > + * response exceeds REPLAY_ISIZE, a buffer is dynamically allocated. If\n> > > + * that allocation fails, only rp_status is saved. Enlarging this constant\n> > > + * increases the size of every nfs4_stateowner.\n> > >   */\n> > >  \n> > >  #define NFSD4_REPLAY_ISIZE       112 \n> > > @@ -569,12 +569,14 @@ struct nfs4_client_reclaim {\n> > >  struct nfs4_replay {\n> > >  \t__be32\t\t\trp_status;\n> > >  \tunsigned int\t\trp_buflen;\n> > > -\tchar\t\t\t*rp_buf;\n> > > +\tchar\t\t\t*rp_buf; /* rp_ibuf or kmalloc'd */\n> > >  \tstruct knfsd_fh\t\trp_openfh;\n> > >  \tint\t\t\trp_locked;\n> > >  \tchar\t\t\trp_ibuf[NFSD4_REPLAY_ISIZE];\n> > >  };\n> > >  \n> > > +extern void nfs4_replay_free_cache(struct nfs4_replay *rp);\n> > > +\n> > >  struct nfs4_stateowner;\n> > >  \n> > >  struct nfs4_stateowner_operations {\n> > \n> > \n> > Certainly a reasonable approach if we care about full correctness when\n> > dealing with a large lockowner on NFSv4.0. Do we?\n> \n> The idea would be to either:\n> \n> o Backport your fix and not this update, or\n> o Squash these two together, and backport both\n> \n> Admittedly this is a narrow corner case for a minor version that is\n> destined for the scrap heap.\n> \n\nRight. I ask because I looked at this approach when I was fixing this,\nand decided it wasn't worthwhile. I certainly won't stand in your way\nif you decide you want to handle long lockowner blobs, but I doubt any\nlegitimate user will ever care.\n-- \nJeff Layton <jlayton@kernel.org>\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Chuck Lever",
              "summary": "I don't disagree at all. My concern is handling replay compliantly. Maybe there's another approach.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/24/26 2:51 PM, Jeff Layton wrote:\n> On Tue, 2026-02-24 at 14:42 -0500, Chuck Lever wrote:\n>> On 2/24/26 2:39 PM, Jeff Layton wrote:\n>>> On Tue, 2026-02-24 at 14:33 -0500, Chuck Lever wrote:\n>>>> From: Chuck Lever <chuck.lever@oracle.com>\n>>>>\n>>>> Commit 1e8e9913672a (\"nfsd: fix heap overflow in NFSv4.0 LOCK\n>>>> replay cache\") capped the replay cache copy at NFSD4_REPLAY_ISIZE\n>>>> to prevent a heap overflow, but set rp_buflen to zero when the\n>>>> encoded response exceeded the inline buffer. A retransmitted LOCK\n>>>> reaching the replay path then produced only a status code with no\n>>>> operation body, resulting in a malformed XDR response.\n>>>>\n>>>> When the encoded response exceeds the 112-byte inline rp_ibuf, a\n>>>> buffer is kmalloc'd to hold it. If the allocation fails, rp_buflen\n>>>> remains zero, preserving the behavior from the capped-copy fix.\n>>>> The buffer is freed when the stateowner is released or when a\n>>>> subsequent operation's response fits in the inline buffer.\n>>>>\n>>>> Fixes: 1e8e9913672a (\"nfsd: fix heap overflow in NFSv4.0 LOCK replay cache\")\n>>>> Signed-off-by: Chuck Lever <chuck.lever@oracle.com>\n>>>> ---\n>>>>  fs/nfsd/nfs4state.c | 16 ++++++++++++++++\n>>>>  fs/nfsd/nfs4xdr.c   | 23 ++++++++++++++++-------\n>>>>  fs/nfsd/state.h     | 12 +++++++-----\n>>>>  3 files changed, 39 insertions(+), 12 deletions(-)\n>>>>\n>>>> diff --git a/fs/nfsd/nfs4state.c b/fs/nfsd/nfs4state.c\n>>>> index ba49f49bb93b..b4d0e82b2690 100644\n>>>> --- a/fs/nfsd/nfs4state.c\n>>>> +++ b/fs/nfsd/nfs4state.c\n>>>> @@ -1496,8 +1496,24 @@ release_all_access(struct nfs4_ol_stateid *stp)\n>>>>  \t}\n>>>>  }\n>>>>  \n>>>> +/**\n>>>> + * nfs4_replay_free_cache - release dynamically allocated replay buffer\n>>>> + * @rp: replay cache to reset\n>>>> + *\n>>>> + * If @rp->rp_buf points to a kmalloc'd buffer, free it and reset\n>>>> + * rp_buf to the inline rp_ibuf. Always zeroes rp_buflen.\n>>>> + */\n>>>> +void nfs4_replay_free_cache(struct nfs4_replay *rp)\n>>>> +{\n>>>> +\tif (rp->rp_buf != rp->rp_ibuf)\n>>>> +\t\tkfree(rp->rp_buf);\n>>>> +\trp->rp_buf = rp->rp_ibuf;\n>>>> +\trp->rp_buflen = 0;\n>>>> +}\n>>>> +\n>>>>  static inline void nfs4_free_stateowner(struct nfs4_stateowner *sop)\n>>>>  {\n>>>> +\tnfs4_replay_free_cache(&sop->so_replay);\n>>>>  \tkfree(sop->so_owner.data);\n>>>>  \tsop->so_ops->so_free(sop);\n>>>>  }\n>>>> diff --git a/fs/nfsd/nfs4xdr.c b/fs/nfsd/nfs4xdr.c\n>>>> index 690f7a3122ec..2a0946c630e1 100644\n>>>> --- a/fs/nfsd/nfs4xdr.c\n>>>> +++ b/fs/nfsd/nfs4xdr.c\n>>>> @@ -6282,14 +6282,23 @@ nfsd4_encode_operation(struct nfsd4_compoundres *resp, struct nfsd4_op *op)\n>>>>  \t\tint len = xdr->buf->len - (op_status_offset + XDR_UNIT);\n>>>>  \n>>>>  \t\tso->so_replay.rp_status = op->status;\n>>>> -\t\tif (len <= NFSD4_REPLAY_ISIZE) {\n>>>> -\t\t\tso->so_replay.rp_buflen = len;\n>>>> -\t\t\tread_bytes_from_xdr_buf(xdr->buf,\n>>>> -\t\t\t\t\t\top_status_offset + XDR_UNIT,\n>>>> -\t\t\t\t\t\tso->so_replay.rp_buf, len);\n>>>> -\t\t} else {\n>>>> -\t\t\tso->so_replay.rp_buflen = 0;\n>>>> +\t\tif (len > NFSD4_REPLAY_ISIZE) {\n>>>> +\t\t\tchar *buf = kmalloc(len, GFP_KERNEL);\n>>>> +\n>>>> +\t\t\tnfs4_replay_free_cache(&so->so_replay);\n>>>> +\t\t\tif (buf) {\n>>>> +\t\t\t\tso->so_replay.rp_buf = buf;\n>>>> +\t\t\t} else {\n>>>> +\t\t\t\t/* rp_buflen already zeroed; skip caching */\n>>>> +\t\t\t\tgoto status;\n>>>> +\t\t\t}\n>>>> +\t\t} else if (so->so_replay.rp_buf != so->so_replay.rp_ibuf) {\n>>>> +\t\t\tnfs4_replay_free_cache(&so->so_replay);\n>>>>  \t\t}\n>>>> +\t\tso->so_replay.rp_buflen = len;\n>>>> +\t\tread_bytes_from_xdr_buf(xdr->buf,\n>>>> +\t\t\t\t\top_status_offset + XDR_UNIT,\n>>>> +\t\t\t\t\tso->so_replay.rp_buf, len);\n>>>>  \t}\n>>>>  status:\n>>>>  \top->status = nfsd4_map_status(op->status,\n>>>> diff --git a/fs/nfsd/state.h b/fs/nfsd/state.h\n>>>> index 3159c7b67f50..9b05462da4cc 100644\n>>>> --- a/fs/nfsd/state.h\n>>>> +++ b/fs/nfsd/state.h\n>>>> @@ -554,10 +554,10 @@ struct nfs4_client_reclaim {\n>>>>   *   ~32(deleg. ace) = 112 bytes\n>>>>   *\n>>>>   * Some responses can exceed this. A LOCK denial includes the conflicting\n>>>> - * lock owner, which can be up to 1024 bytes (NFS4_OPAQUE_LIMIT). Responses\n>>>> - * larger than REPLAY_ISIZE are not cached in rp_ibuf; only rp_status is\n>>>> - * saved. Enlarging this constant increases the size of every\n>>>> - * nfs4_stateowner.\n>>>> + * lock owner, which can be up to 1024 bytes (NFS4_OPAQUE_LIMIT). When a\n>>>> + * response exceeds REPLAY_ISIZE, a buffer is dynamically allocated. If\n>>>> + * that allocation fails, only rp_status is saved. Enlarging this constant\n>>>> + * increases the size of every nfs4_stateowner.\n>>>>   */\n>>>>  \n>>>>  #define NFSD4_REPLAY_ISIZE       112 \n>>>> @@ -569,12 +569,14 @@ struct nfs4_client_reclaim {\n>>>>  struct nfs4_replay {\n>>>>  \t__be32\t\t\trp_status;\n>>>>  \tunsigned int\t\trp_buflen;\n>>>> -\tchar\t\t\t*rp_buf;\n>>>> +\tchar\t\t\t*rp_buf; /* rp_ibuf or kmalloc'd */\n>>>>  \tstruct knfsd_fh\t\trp_openfh;\n>>>>  \tint\t\t\trp_locked;\n>>>>  \tchar\t\t\trp_ibuf[NFSD4_REPLAY_ISIZE];\n>>>>  };\n>>>>  \n>>>> +extern void nfs4_replay_free_cache(struct nfs4_replay *rp);\n>>>> +\n>>>>  struct nfs4_stateowner;\n>>>>  \n>>>>  struct nfs4_stateowner_operations {\n>>>\n>>>\n>>> Certainly a reasonable approach if we care about full correctness when\n>>> dealing with a large lockowner on NFSv4.0. Do we?\n>>\n>> The idea would be to either:\n>>\n>> o Backport your fix and not this update, or\n>> o Squash these two together, and backport both\n>>\n>> Admittedly this is a narrow corner case for a minor version that is\n>> destined for the scrap heap.\n>>\n> \n> Right. I ask because I looked at this approach when I was fixing this,\n> and decided it wasn't worthwhile. I certainly won't stand in your way\n> if you decide you want to handle long lockowner blobs, but I doubt any\n> legitimate user will ever care.\n\nI don't disagree at all. My concern is handling replay compliantly.\nMaybe there's another approach.\n\n\n-- \nChuck Lever\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "I think that the only other way is to grow NFSD4_REPLAY_ISIZE, and doing dynamic allocation is preferable to that, IMO. To be clear: I don't have a problem with your patch. It just didn't seem worthwhile to me. If you think it's worth fixing though, then go for it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, 2026-02-24 at 14:53 -0500, Chuck Lever wrote:\n> On 2/24/26 2:51 PM, Jeff Layton wrote:\n> > On Tue, 2026-02-24 at 14:42 -0500, Chuck Lever wrote:\n> > > On 2/24/26 2:39 PM, Jeff Layton wrote:\n> > > > On Tue, 2026-02-24 at 14:33 -0500, Chuck Lever wrote:\n> > > > > From: Chuck Lever <chuck.lever@oracle.com>\n> > > > > \n> > > > > Commit 1e8e9913672a (\"nfsd: fix heap overflow in NFSv4.0 LOCK\n> > > > > replay cache\") capped the replay cache copy at NFSD4_REPLAY_ISIZE\n> > > > > to prevent a heap overflow, but set rp_buflen to zero when the\n> > > > > encoded response exceeded the inline buffer. A retransmitted LOCK\n> > > > > reaching the replay path then produced only a status code with no\n> > > > > operation body, resulting in a malformed XDR response.\n> > > > > \n> > > > > When the encoded response exceeds the 112-byte inline rp_ibuf, a\n> > > > > buffer is kmalloc'd to hold it. If the allocation fails, rp_buflen\n> > > > > remains zero, preserving the behavior from the capped-copy fix.\n> > > > > The buffer is freed when the stateowner is released or when a\n> > > > > subsequent operation's response fits in the inline buffer.\n> > > > > \n> > > > > Fixes: 1e8e9913672a (\"nfsd: fix heap overflow in NFSv4.0 LOCK replay cache\")\n> > > > > Signed-off-by: Chuck Lever <chuck.lever@oracle.com>\n> > > > > ---\n> > > > >  fs/nfsd/nfs4state.c | 16 ++++++++++++++++\n> > > > >  fs/nfsd/nfs4xdr.c   | 23 ++++++++++++++++-------\n> > > > >  fs/nfsd/state.h     | 12 +++++++-----\n> > > > >  3 files changed, 39 insertions(+), 12 deletions(-)\n> > > > > \n> > > > > diff --git a/fs/nfsd/nfs4state.c b/fs/nfsd/nfs4state.c\n> > > > > index ba49f49bb93b..b4d0e82b2690 100644\n> > > > > --- a/fs/nfsd/nfs4state.c\n> > > > > +++ b/fs/nfsd/nfs4state.c\n> > > > > @@ -1496,8 +1496,24 @@ release_all_access(struct nfs4_ol_stateid *stp)\n> > > > >  \t}\n> > > > >  }\n> > > > >  \n> > > > > +/**\n> > > > > + * nfs4_replay_free_cache - release dynamically allocated replay buffer\n> > > > > + * @rp: replay cache to reset\n> > > > > + *\n> > > > > + * If @rp->rp_buf points to a kmalloc'd buffer, free it and reset\n> > > > > + * rp_buf to the inline rp_ibuf. Always zeroes rp_buflen.\n> > > > > + */\n> > > > > +void nfs4_replay_free_cache(struct nfs4_replay *rp)\n> > > > > +{\n> > > > > +\tif (rp->rp_buf != rp->rp_ibuf)\n> > > > > +\t\tkfree(rp->rp_buf);\n> > > > > +\trp->rp_buf = rp->rp_ibuf;\n> > > > > +\trp->rp_buflen = 0;\n> > > > > +}\n> > > > > +\n> > > > >  static inline void nfs4_free_stateowner(struct nfs4_stateowner *sop)\n> > > > >  {\n> > > > > +\tnfs4_replay_free_cache(&sop->so_replay);\n> > > > >  \tkfree(sop->so_owner.data);\n> > > > >  \tsop->so_ops->so_free(sop);\n> > > > >  }\n> > > > > diff --git a/fs/nfsd/nfs4xdr.c b/fs/nfsd/nfs4xdr.c\n> > > > > index 690f7a3122ec..2a0946c630e1 100644\n> > > > > --- a/fs/nfsd/nfs4xdr.c\n> > > > > +++ b/fs/nfsd/nfs4xdr.c\n> > > > > @@ -6282,14 +6282,23 @@ nfsd4_encode_operation(struct nfsd4_compoundres *resp, struct nfsd4_op *op)\n> > > > >  \t\tint len = xdr->buf->len - (op_status_offset + XDR_UNIT);\n> > > > >  \n> > > > >  \t\tso->so_replay.rp_status = op->status;\n> > > > > -\t\tif (len <= NFSD4_REPLAY_ISIZE) {\n> > > > > -\t\t\tso->so_replay.rp_buflen = len;\n> > > > > -\t\t\tread_bytes_from_xdr_buf(xdr->buf,\n> > > > > -\t\t\t\t\t\top_status_offset + XDR_UNIT,\n> > > > > -\t\t\t\t\t\tso->so_replay.rp_buf, len);\n> > > > > -\t\t} else {\n> > > > > -\t\t\tso->so_replay.rp_buflen = 0;\n> > > > > +\t\tif (len > NFSD4_REPLAY_ISIZE) {\n> > > > > +\t\t\tchar *buf = kmalloc(len, GFP_KERNEL);\n> > > > > +\n> > > > > +\t\t\tnfs4_replay_free_cache(&so->so_replay);\n> > > > > +\t\t\tif (buf) {\n> > > > > +\t\t\t\tso->so_replay.rp_buf = buf;\n> > > > > +\t\t\t} else {\n> > > > > +\t\t\t\t/* rp_buflen already zeroed; skip caching */\n> > > > > +\t\t\t\tgoto status;\n> > > > > +\t\t\t}\n> > > > > +\t\t} else if (so->so_replay.rp_buf != so->so_replay.rp_ibuf) {\n> > > > > +\t\t\tnfs4_replay_free_cache(&so->so_replay);\n> > > > >  \t\t}\n> > > > > +\t\tso->so_replay.rp_buflen = len;\n> > > > > +\t\tread_bytes_from_xdr_buf(xdr->buf,\n> > > > > +\t\t\t\t\top_status_offset + XDR_UNIT,\n> > > > > +\t\t\t\t\tso->so_replay.rp_buf, len);\n> > > > >  \t}\n> > > > >  status:\n> > > > >  \top->status = nfsd4_map_status(op->status,\n> > > > > diff --git a/fs/nfsd/state.h b/fs/nfsd/state.h\n> > > > > index 3159c7b67f50..9b05462da4cc 100644\n> > > > > --- a/fs/nfsd/state.h\n> > > > > +++ b/fs/nfsd/state.h\n> > > > > @@ -554,10 +554,10 @@ struct nfs4_client_reclaim {\n> > > > >   *   ~32(deleg. ace) = 112 bytes\n> > > > >   *\n> > > > >   * Some responses can exceed this. A LOCK denial includes the conflicting\n> > > > > - * lock owner, which can be up to 1024 bytes (NFS4_OPAQUE_LIMIT). Responses\n> > > > > - * larger than REPLAY_ISIZE are not cached in rp_ibuf; only rp_status is\n> > > > > - * saved. Enlarging this constant increases the size of every\n> > > > > - * nfs4_stateowner.\n> > > > > + * lock owner, which can be up to 1024 bytes (NFS4_OPAQUE_LIMIT). When a\n> > > > > + * response exceeds REPLAY_ISIZE, a buffer is dynamically allocated. If\n> > > > > + * that allocation fails, only rp_status is saved. Enlarging this constant\n> > > > > + * increases the size of every nfs4_stateowner.\n> > > > >   */\n> > > > >  \n> > > > >  #define NFSD4_REPLAY_ISIZE       112 \n> > > > > @@ -569,12 +569,14 @@ struct nfs4_client_reclaim {\n> > > > >  struct nfs4_replay {\n> > > > >  \t__be32\t\t\trp_status;\n> > > > >  \tunsigned int\t\trp_buflen;\n> > > > > -\tchar\t\t\t*rp_buf;\n> > > > > +\tchar\t\t\t*rp_buf; /* rp_ibuf or kmalloc'd */\n> > > > >  \tstruct knfsd_fh\t\trp_openfh;\n> > > > >  \tint\t\t\trp_locked;\n> > > > >  \tchar\t\t\trp_ibuf[NFSD4_REPLAY_ISIZE];\n> > > > >  };\n> > > > >  \n> > > > > +extern void nfs4_replay_free_cache(struct nfs4_replay *rp);\n> > > > > +\n> > > > >  struct nfs4_stateowner;\n> > > > >  \n> > > > >  struct nfs4_stateowner_operations {\n> > > > \n> > > > \n> > > > Certainly a reasonable approach if we care about full correctness when\n> > > > dealing with a large lockowner on NFSv4.0. Do we?\n> > > \n> > > The idea would be to either:\n> > > \n> > > o Backport your fix and not this update, or\n> > > o Squash these two together, and backport both\n> > > \n> > > Admittedly this is a narrow corner case for a minor version that is\n> > > destined for the scrap heap.\n> > > \n> > \n> > Right. I ask because I looked at this approach when I was fixing this,\n> > and decided it wasn't worthwhile. I certainly won't stand in your way\n> > if you decide you want to handle long lockowner blobs, but I doubt any\n> > legitimate user will ever care.\n> \n> I don't disagree at all. My concern is handling replay compliantly.\n> Maybe there's another approach.\n> \n\nI think that the only other way is to grow NFSD4_REPLAY_ISIZE, and\ndoing dynamic allocation is preferable to that, IMO.\n\nTo be clear: I don't have a problem with your patch. It just didn't\nseem worthwhile to me. If you think it's worth fixing though, then go\nfor it.\n-- \nJeff Layton <jlayton@kernel.org>\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joanne Koong",
      "primary_email": "joannelkoong@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v1 0/1] iomap: don't mark folio uptodate if read IO has bytes pending",
          "message_id": "20260219003911.344478-1-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260219003911.344478-1-joannelkoong@gmail.com/",
          "date": "2026-02-19T00:41:04Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-19",
          "patch_summary": "This is a fix for this scenario:",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v1 11/11] io_uring/cmd: set selected buffer index in __io_uring_cmd_done()",
          "message_id": "20260210002852.1394504-12-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260210002852.1394504-12-joannelkoong@gmail.com/",
          "date": "2026-02-10T00:31:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-10",
          "patch_summary": "When uring_cmd operations select a buffer, the completion queue entry should indicate which buffer was selected.\n\nSet IORING_CQE_F_BUFFER on the completed entry and encode the buffer index if a buffer was selected.\n\nThis will be needed for fuse, which needs to relay to userspace which selected buffer contains the data.",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] io_uring/rsrc: clean up buffer cloning arg validation (for 6.18-stable tree)",
          "message_id": "CAJnrk1YA9hk5Mv0BXFe+TcWLXsNLpWtcA-gy+k03zDt4f0z7zg@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1YA9hk5Mv0BXFe+TcWLXsNLpWtcA-gy+k03zDt4f0z7zg@mail.gmail.com/",
          "date": "2026-02-20T18:20:08Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-20",
          "patch_summary": "Commit id upstream: b8201b50e403815f941d1c6581a27fdbfe7d0fd4 (\"io_uring/rsrc: clean up buffer cloning arg validation\") Link to the patch: https://lore.kernel.org/io-uring/20251204215116.2642044-1-joannelkoong@gmail.com/#t Kernel version to apply it to: 6.18-stable tree\n\nHi stable@,",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Jens Axboe",
              "summary": "FWIW, this is approved on my end. CC Greg.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On 2/20/26 11:19 AM, Joanne Koong wrote:\n> Commit id upstream: b8201b50e403815f941d1c6581a27fdbfe7d0fd4\n> (\"io_uring/rsrc: clean up buffer cloning arg validation\")\n> Link to the patch:\n> https://lore.kernel.org/io-uring/20251204215116.2642044-1-joannelkoong@gmail.com/#t\n> Kernel version to apply it to: 6.18-stable tree\n> \n> Hi stable@,\n> \n> Chris Mason recently detected that this patch is a required dependency\n> for commit 5b804b8f1e0d (\"io_uring/rsrc: fix lost entries after cloned\n> range\") in the 6.18-stable tree [1]. Without this patch, the changes\n> in commit 5b804b8f1e0d use an incorrect value for nbufs when it\n> assigns \"i = nbufs\" [2].\n> \n> Could you please apply this patch to the 6.18-stable tree as a\n> dependency fix needed for commit 5b804b8f1e0d?\t\n> \n> Thanks,\n> Joanne\n> \n> [1] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=linux-6.18.y&id=5b804b8f1e0d66413774d43f7a4b78bba0ca6272\n> [2] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/rsrc.c?h=linux-6.18.y#n1252.\n\nFWIW, this is approved on my end. CC Greg.\n\n\n-- \nJens Axboe\n\n",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Greg Kroah-Hartman",
              "summary": "Now queued up, thanks.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "queued"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Sun, Feb 22, 2026 at 08:33:24AM -0700, Jens Axboe wrote:\n> On 2/20/26 11:19 AM, Joanne Koong wrote:\n> > Commit id upstream: b8201b50e403815f941d1c6581a27fdbfe7d0fd4\n> > (\"io_uring/rsrc: clean up buffer cloning arg validation\")\n> > Link to the patch:\n> > https://lore.kernel.org/io-uring/20251204215116.2642044-1-joannelkoong@gmail.com/#t\n> > Kernel version to apply it to: 6.18-stable tree\n> > \n> > Hi stable@,\n> > \n> > Chris Mason recently detected that this patch is a required dependency\n> > for commit 5b804b8f1e0d (\"io_uring/rsrc: fix lost entries after cloned\n> > range\") in the 6.18-stable tree [1]. Without this patch, the changes\n> > in commit 5b804b8f1e0d use an incorrect value for nbufs when it\n> > assigns \"i = nbufs\" [2].\n> > \n> > Could you please apply this patch to the 6.18-stable tree as a\n> > dependency fix needed for commit 5b804b8f1e0d?\t\n> > \n> > Thanks,\n> > Joanne\n> > \n> > [1] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=linux-6.18.y&id=5b804b8f1e0d66413774d43f7a4b78bba0ca6272\n> > [2] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/rsrc.c?h=linux-6.18.y#n1252.\n> \n> FWIW, this is approved on my end. CC Greg.\n\nNow queued up, thanks.\n\ngreg k-h\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v1 03/11] io_uring/kbuf: add support for kernel-managed buffer rings",
          "message_id": "CAJnrk1a1FAARebZ0Aqw18zxtOy8WTMb2UfcAK6jQaigXiZbTfQ@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1a1FAARebZ0Aqw18zxtOy8WTMb2UfcAK6jQaigXiZbTfQ@mail.gmail.com/",
          "date": "2026-02-24T22:20:09Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/5] fuse: flush pending FUSE_RELEASE requests before sending FUSE_DESTROY",
          "message_id": "CAJnrk1YCh=CsFmxGwnK37d-31ravAOR8uLH+CrhpFzPX=ZTxUw@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1YCh=CsFmxGwnK37d-31ravAOR8uLH+CrhpFzPX=ZTxUw@mail.gmail.com/",
          "date": "2026-02-24T20:03:55Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 2/5] fuse: quiet down complaints in fuse_conn_limit_write",
          "message_id": "CAJnrk1bEm=pe2M367CsbQNYyUEdXCVzAyboqqHnSCxx7fxZKZA@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1bEm=pe2M367CsbQNYyUEdXCVzAyboqqHnSCxx7fxZKZA@mail.gmail.com/",
          "date": "2026-02-24T20:09:39Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 1/5] fuse: flush pending FUSE_RELEASE requests before sending FUSE_DESTROY",
          "message_id": "CAJnrk1ZZ=1jF4DUF-NyedLP-BJM_5d3s0zfD4oHGyR51PM9E7Q@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1ZZ=1jF4DUF-NyedLP-BJM_5d3s0zfD4oHGyR51PM9E7Q@mail.gmail.com/",
          "date": "2026-02-24T19:33:24Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Darrick Wong",
              "summary": "I'm very confused by the comment style in this header file.  Some of them look like kerneldoc comments (albeit not documenting the sole parameter), but others are just regular C comments. <shrug> I sorta dislike kerneldoc's fussiness so I'll change it to a C comment so that I don't have to propagate this \"@param fc fuse connection\" verbosity. Yep, that was added for a previous iteration and can go away now. Right.  Fixed. How about I simplify it to: /* * Flush all pending requests before sending FUSE_DESTROY.  The * fuse server must reply to the flushed requests before * handling FUSE_DESTROY because unmount is about to release * its O_EXCL hold on the block device. */",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "nits"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 11:33:12AM -0800, Joanne Koong wrote:\n> On Mon, Feb 23, 2026 at 3:06\\u202fPM Darrick J. Wong <djwong@kernel.org> wrote:\n> >\n> > From: Darrick J. Wong <djwong@kernel.org>\n> >\n> > generic/488 fails with fuse2fs in the following fashion:\n> >\n> > generic/488       _check_generic_filesystem: filesystem on /dev/sdf is inconsistent\n> > (see /var/tmp/fstests/generic/488.full for details)\n> >\n> > This test opens a large number of files, unlinks them (which really just\n> > renames them to fuse hidden files), closes the program, unmounts the\n> > filesystem, and runs fsck to check that there aren't any inconsistencies\n> > in the filesystem.\n> >\n> > Unfortunately, the 488.full file shows that there are a lot of hidden\n> > files left over in the filesystem, with incorrect link counts.  Tracing\n> > fuse_request_* shows that there are a large number of FUSE_RELEASE\n> > commands that are queued up on behalf of the unlinked files at the time\n> > that fuse_conn_destroy calls fuse_abort_conn.  Had the connection not\n> > aborted, the fuse server would have responded to the RELEASE commands by\n> > removing the hidden files; instead they stick around.\n> >\n> > For upper-level fuse servers that don't use fuseblk mode this isn't a\n> > problem because libfuse responds to the connection going down by pruning\n> > its inode cache and calling the fuse server's ->release for any open\n> > files before calling the server's ->destroy function.\n> >\n> > For fuseblk servers this is a problem, however, because the kernel sends\n> > FUSE_DESTROY to the fuse server, and the fuse server has to write all of\n> > its pending changes to the block device before replying to the DESTROY\n> > request because the kernel releases its O_EXCL hold on the block device.\n> > This means that the kernel must flush all pending FUSE_RELEASE requests\n> > before issuing FUSE_DESTROY.\n> >\n> > For fuse-iomap servers this will also be a problem because iomap servers\n> > are expected to release all exclusively-held resources before unmount\n> > returns from the kernel.\n> >\n> > Create a function to push all the background requests to the queue\n> > before sending FUSE_DESTROY.  That way, all the pending file release\n> > events are processed by the fuse server before it tears itself down, and\n> > we don't end up with a corrupt filesystem.\n> >\n> > Note that multithreaded fuse servers will need to track the number of\n> > open files and defer a FUSE_DESTROY request until that number reaches\n> > zero.  An earlier version of this patch made the kernel wait for the\n> > RELEASE acknowledgements before sending DESTROY, but the kernel people\n> > weren't comfortable with adding blocking waits to unmount.\n> >\n> > Signed-off-by: \"Darrick J. Wong\" <djwong@kernel.org>\n> \n> Overall LGTM, left a few comments below\n> \n> Reviewed-by: Joanne Koong <joannelkoong@gmail.com>\n\nThanks!\n\n> > ---\n> >  fs/fuse/fuse_i.h |    5 +++++\n> >  fs/fuse/dev.c    |   19 +++++++++++++++++++\n> >  fs/fuse/inode.c  |   12 +++++++++++-\n> >  3 files changed, 35 insertions(+), 1 deletion(-)\n> >\n> >\n> > diff --git a/fs/fuse/fuse_i.h b/fs/fuse/fuse_i.h\n> > index 7f16049387d15e..1d4beca5c7018d 100644\n> > --- a/fs/fuse/fuse_i.h\n> > +++ b/fs/fuse/fuse_i.h\n> > @@ -1287,6 +1287,11 @@ void fuse_request_end(struct fuse_req *req);\n> >  void fuse_abort_conn(struct fuse_conn *fc);\n> >  void fuse_wait_aborted(struct fuse_conn *fc);\n> >\n> > +/**\n> > + * Flush all pending requests but do not wait for them.\n> > + */\n> \n> nit: /*  */ comment style\n\nI'm very confused by the comment style in this header file.  Some of\nthem look like kerneldoc comments (albeit not documenting the sole\nparameter), but others are just regular C comments.\n\n<shrug> I sorta dislike kerneldoc's fussiness so I'll change it to a C\ncomment so that I don't have to propagate this \"@param fc fuse\nconnection\" verbosity.\n\n> > +void fuse_flush_requests(struct fuse_conn *fc);\n> > +\n> >  /* Check if any requests timed out */\n> >  void fuse_check_timeout(struct work_struct *work);\n> >\n> > diff --git a/fs/fuse/dev.c b/fs/fuse/dev.c\n> > index 0b0241f47170d4..ac9d7a7b3f5e68 100644\n> > --- a/fs/fuse/dev.c\n> > +++ b/fs/fuse/dev.c\n> > @@ -24,6 +24,7 @@\n> >  #include <linux/splice.h>\n> >  #include <linux/sched.h>\n> >  #include <linux/seq_file.h>\n> > +#include <linux/nmi.h>\n> \n> I don't think you meant to add this?\n\nYep, that was added for a previous iteration and can go away now.\n\n> >\n> >  #include \"fuse_trace.h\"\n> >\n> > @@ -2430,6 +2431,24 @@ static void end_polls(struct fuse_conn *fc)\n> >         }\n> >  }\n> >\n> > +/*\n> > + * Flush all pending requests and wait for them.  Only call this function when\n> \n> I think you meant \"don't wait\" for them?\n\nRight.  Fixed.\n\n> > + * it is no longer possible for other threads to add requests.\n> > + */\n> > +void fuse_flush_requests(struct fuse_conn *fc)\n> > +{\n> > +       spin_lock(&fc->lock);\n> > +       spin_lock(&fc->bg_lock);\n> > +       if (fc->connected) {\n> > +               /* Push all the background requests to the queue. */\n> > +               fc->blocked = 0;\n> > +               fc->max_background = UINT_MAX;\n> > +               flush_bg_queue(fc);\n> > +       }\n> > +       spin_unlock(&fc->bg_lock);\n> > +       spin_unlock(&fc->lock);\n> > +}\n> > +\n> >  /*\n> >   * Abort all requests.\n> >   *\n> > diff --git a/fs/fuse/inode.c b/fs/fuse/inode.c\n> > index e57b8af06be93e..58c3351b467221 100644\n> > --- a/fs/fuse/inode.c\n> > +++ b/fs/fuse/inode.c\n> > @@ -2086,8 +2086,18 @@ void fuse_conn_destroy(struct fuse_mount *fm)\n> >  {\n> >         struct fuse_conn *fc = fm->fc;\n> >\n> > -       if (fc->destroy)\n> > +       if (fc->destroy) {\n> > +               /*\n> > +                * Flush all pending requests (most of which will be\n> > +                * FUSE_RELEASE) before sending FUSE_DESTROY, because the fuse\n> > +                * server must close the filesystem before replying to the\n> > +                * destroy message, because unmount is about to release its\n> > +                * O_EXCL hold on the block device.  We don't wait, so libfuse\n> > +                * has to do that for us.\n> \n> nit: imo the \"because the fuse server must close the filesystem before\n> replying to the destroy message, because...\" part is confusing. Even\n> if that weren't true, the pending requests would still have to be sent\n> before the destroy, no? i think it would be less confusing if that\n> part of the paragraph was removed. I think it might be better to\n> remove the \"we don't wait, so libfuse has to do that for us\" part too\n> or rewording it to something like \"flushed requests are sent before\n> the FUSE_DESTROY. Userspace is responsible for ensuring flushed\n> requests are handled before replying to the FUSE_DESTROY\".\n\nHow about I simplify it to:\n\n\t/*\n\t * Flush all pending requests before sending FUSE_DESTROY.  The\n\t * fuse server must reply to the flushed requests before\n\t * handling FUSE_DESTROY because unmount is about to release\n\t * its O_EXCL hold on the block device.\n\t */\n\n--D\n\n> \n> Thanks,\n> Joanne\n> \n> > +                */\n> > +               fuse_flush_requests(fc);\n> >                 fuse_send_destroy(fm);\n> > +       }\n> >\n> >         fuse_abort_conn(fc);\n> >         fuse_wait_aborted(fc);\n> >\n> \n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Oh I see your confusion now. Yeah the comment styles in this .h file are kind of all over the place. Most of the functions don't even have comments. This sounds a lot better to me.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "nits"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 11:57 AM Darrick J. Wong <djwong@kernel.org> wrote:\n>\n> On Tue, Feb 24, 2026 at 11:33:12AM -0800, Joanne Koong wrote:\n> > On Mon, Feb 23, 2026 at 3:06 PM Darrick J. Wong <djwong@kernel.org> wrote:\n> > >\n> > > From: Darrick J. Wong <djwong@kernel.org>\n> > >\n> > > generic/488 fails with fuse2fs in the following fashion:\n> > >\n> > > generic/488       _check_generic_filesystem: filesystem on /dev/sdf is inconsistent\n> > > (see /var/tmp/fstests/generic/488.full for details)\n> > >\n> > > This test opens a large number of files, unlinks them (which really just\n> > > renames them to fuse hidden files), closes the program, unmounts the\n> > > filesystem, and runs fsck to check that there aren't any inconsistencies\n> > > in the filesystem.\n> > >\n> > > Unfortunately, the 488.full file shows that there are a lot of hidden\n> > > files left over in the filesystem, with incorrect link counts.  Tracing\n> > > fuse_request_* shows that there are a large number of FUSE_RELEASE\n> > > commands that are queued up on behalf of the unlinked files at the time\n> > > that fuse_conn_destroy calls fuse_abort_conn.  Had the connection not\n> > > aborted, the fuse server would have responded to the RELEASE commands by\n> > > removing the hidden files; instead they stick around.\n> > >\n> > > For upper-level fuse servers that don't use fuseblk mode this isn't a\n> > > problem because libfuse responds to the connection going down by pruning\n> > > its inode cache and calling the fuse server's ->release for any open\n> > > files before calling the server's ->destroy function.\n> > >\n> > > For fuseblk servers this is a problem, however, because the kernel sends\n> > > FUSE_DESTROY to the fuse server, and the fuse server has to write all of\n> > > its pending changes to the block device before replying to the DESTROY\n> > > request because the kernel releases its O_EXCL hold on the block device.\n> > > This means that the kernel must flush all pending FUSE_RELEASE requests\n> > > before issuing FUSE_DESTROY.\n> > >\n> > > For fuse-iomap servers this will also be a problem because iomap servers\n> > > are expected to release all exclusively-held resources before unmount\n> > > returns from the kernel.\n> > >\n> > > Create a function to push all the background requests to the queue\n> > > before sending FUSE_DESTROY.  That way, all the pending file release\n> > > events are processed by the fuse server before it tears itself down, and\n> > > we don't end up with a corrupt filesystem.\n> > >\n> > > Note that multithreaded fuse servers will need to track the number of\n> > > open files and defer a FUSE_DESTROY request until that number reaches\n> > > zero.  An earlier version of this patch made the kernel wait for the\n> > > RELEASE acknowledgements before sending DESTROY, but the kernel people\n> > > weren't comfortable with adding blocking waits to unmount.\n> > >\n> > > Signed-off-by: \"Darrick J. Wong\" <djwong@kernel.org>\n> >\n> > Overall LGTM, left a few comments below\n> >\n> > Reviewed-by: Joanne Koong <joannelkoong@gmail.com>\n>\n> Thanks!\n>\n> > > ---\n> > >  fs/fuse/fuse_i.h |    5 +++++\n> > >  fs/fuse/dev.c    |   19 +++++++++++++++++++\n> > >  fs/fuse/inode.c  |   12 +++++++++++-\n> > >  3 files changed, 35 insertions(+), 1 deletion(-)\n> > >\n> > >\n> > > diff --git a/fs/fuse/fuse_i.h b/fs/fuse/fuse_i.h\n> > > index 7f16049387d15e..1d4beca5c7018d 100644\n> > > --- a/fs/fuse/fuse_i.h\n> > > +++ b/fs/fuse/fuse_i.h\n> > > @@ -1287,6 +1287,11 @@ void fuse_request_end(struct fuse_req *req);\n> > >  void fuse_abort_conn(struct fuse_conn *fc);\n> > >  void fuse_wait_aborted(struct fuse_conn *fc);\n> > >\n> > > +/**\n> > > + * Flush all pending requests but do not wait for them.\n> > > + */\n> >\n> > nit: /*  */ comment style\n>\n> I'm very confused by the comment style in this header file.  Some of\n> them look like kerneldoc comments (albeit not documenting the sole\n> parameter), but others are just regular C comments.\n\nOh I see your confusion now. Yeah the comment styles in this .h file\nare kind of all over the place. Most of the functions don't even have\ncomments.\n\n>\n> <shrug> I sorta dislike kerneldoc's fussiness so I'll change it to a C\n> comment so that I don't have to propagate this \"@param fc fuse\n> connection\" verbosity.\n>\n> > > +void fuse_flush_requests(struct fuse_conn *fc);\n> > > +\n> > >  /* Check if any requests timed out */\n> > >  void fuse_check_timeout(struct work_struct *work);\n> > >\n> > > diff --git a/fs/fuse/dev.c b/fs/fuse/dev.c\n> > > index 0b0241f47170d4..ac9d7a7b3f5e68 100644\n> > > --- a/fs/fuse/dev.c\n> > > +++ b/fs/fuse/dev.c\n> > > @@ -24,6 +24,7 @@\n> > >  #include <linux/splice.h>\n> > >  #include <linux/sched.h>\n> > >  #include <linux/seq_file.h>\n> > > +#include <linux/nmi.h>\n> >\n> > I don't think you meant to add this?\n>\n> Yep, that was added for a previous iteration and can go away now.\n>\n> > >\n> > >  #include \"fuse_trace.h\"\n> > >\n> > > @@ -2430,6 +2431,24 @@ static void end_polls(struct fuse_conn *fc)\n> > >         }\n> > >  }\n> > >\n> > > +/*\n> > > + * Flush all pending requests and wait for them.  Only call this function when\n> >\n> > I think you meant \"don't wait\" for them?\n>\n> Right.  Fixed.\n>\n> > > + * it is no longer possible for other threads to add requests.\n> > > + */\n> > > +void fuse_flush_requests(struct fuse_conn *fc)\n> > > +{\n> > > +       spin_lock(&fc->lock);\n> > > +       spin_lock(&fc->bg_lock);\n> > > +       if (fc->connected) {\n> > > +               /* Push all the background requests to the queue. */\n> > > +               fc->blocked = 0;\n> > > +               fc->max_background = UINT_MAX;\n> > > +               flush_bg_queue(fc);\n> > > +       }\n> > > +       spin_unlock(&fc->bg_lock);\n> > > +       spin_unlock(&fc->lock);\n> > > +}\n> > > +\n> > >  /*\n> > >   * Abort all requests.\n> > >   *\n> > > diff --git a/fs/fuse/inode.c b/fs/fuse/inode.c\n> > > index e57b8af06be93e..58c3351b467221 100644\n> > > --- a/fs/fuse/inode.c\n> > > +++ b/fs/fuse/inode.c\n> > > @@ -2086,8 +2086,18 @@ void fuse_conn_destroy(struct fuse_mount *fm)\n> > >  {\n> > >         struct fuse_conn *fc = fm->fc;\n> > >\n> > > -       if (fc->destroy)\n> > > +       if (fc->destroy) {\n> > > +               /*\n> > > +                * Flush all pending requests (most of which will be\n> > > +                * FUSE_RELEASE) before sending FUSE_DESTROY, because the fuse\n> > > +                * server must close the filesystem before replying to the\n> > > +                * destroy message, because unmount is about to release its\n> > > +                * O_EXCL hold on the block device.  We don't wait, so libfuse\n> > > +                * has to do that for us.\n> >\n> > nit: imo the \"because the fuse server must close the filesystem before\n> > replying to the destroy message, because...\" part is confusing. Even\n> > if that weren't true, the pending requests would still have to be sent\n> > before the destroy, no? i think it would be less confusing if that\n> > part of the paragraph was removed. I think it might be better to\n> > remove the \"we don't wait, so libfuse has to do that for us\" part too\n> > or rewording it to something like \"flushed requests are sent before\n> > the FUSE_DESTROY. Userspace is responsible for ensuring flushed\n> > requests are handled before replying to the FUSE_DESTROY\".\n>\n> How about I simplify it to:\n>\n>         /*\n>          * Flush all pending requests before sending FUSE_DESTROY.  The\n>          * fuse server must reply to the flushed requests before\n>          * handling FUSE_DESTROY because unmount is about to release\n>          * its O_EXCL hold on the block device.\n>          */\n\nThis sounds a lot better to me.\n\nThanks,\nJoanne\n>\n> --D\n>\n> >\n> > Thanks,\n> > Joanne\n> >\n> > > +                */\n> > > +               fuse_flush_requests(fc);\n> > >                 fuse_send_destroy(fm);\n> > > +       }\n> > >\n> > >         fuse_abort_conn(fc);\n> > >         fuse_wait_aborted(fc);\n> > >\n> >\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Johannes Weiner",
      "primary_email": "hannes@cmpxchg.org",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH RFC 08/15] mm, swap: store and check memcg info in the swap table",
          "message_id": "aZ3KrfD_6vfxjRcs@cmpxchg.org",
          "url": "https://lore.kernel.org/all/aZ3KrfD_6vfxjRcs@cmpxchg.org/",
          "date": "2026-02-24T15:58:43Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joshua Hahn",
      "primary_email": "joshua.hahnjy@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[RFC PATCH 0/6] mm/memcontrol: Make memcg limits tier-aware",
          "message_id": "20260223223830.586018-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260223223830.586018-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-24T00:19:15Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-24",
          "patch_summary": "Memory cgroups provide an interface that allow multiple workloads on a host to co-exist, and establish both weak and strong memory isolation guarantees. For large servers and small embedded systems alike, memcgs provide an effective way to provide a baseline quality of service for protected workloads.\n\nThis works, because for the most part, all memory is equal (except for zram / zswap). Restricting a cgroup's memory footprint restricts how much it can hurt other workloads competing for memory. Likewise, setting memory.low or memory.min limits can provide weak and strong guarantees to the performance of a cgroup.\n\nHowever, on systems with tiered memory (e.g. CXL / compressed memory), the quality of service guarantees that memcg limits enforced become less effective, as memcg has no awareness of the physical location of its charged memory. In other words, a workload that is well-behaved within its memcg limits may still be hurting the performance of other well-behaving workloads on the system by hogging more than its \"fair share\" of toptier memory.\n\nIntroduce tier-aware memcg limits, which scale memory.low/high to reflect the ratio of toptier:total memory the cgroup has access.\n\nTake the following scenario as an example: On a host with 3:1 toptier:lowtier, say 150G toptier, and 50Glowtier, setting a cgroup's limits to: memory.min:  15G memory.low:  20G memory.high: 40G memory.max:  50G\n\nWill be enforced at the toptier as: memory.min:          15G memory.toptier_low:  15G (20 * 150/200) memory.toptier_high: 30G (40 * 150/200) memory.max:          50G\n\nLet's say that there are 4 such cgroups on the host. Previously, it would be possible for 3 hosts to completely take over all of DRAM, while one cgroup could only access the lowtier memory. In the perspective of a tier-agnostic memcg limit enforcement, the three cgroups are all well-behaved, consuming within their memory limits.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Michal Hocko",
              "summary": "This assumes that the active workingset size of all workloads doesn't fit into the top tier right? Otherwise promotions would make sure to that we have the most active memory in the top tier. Is this typical in real life configurations? Or do you intend to limit memory consumption on particular tier even without an external pressure? Let's spend some more time with the interface first. You seem to be focusing only on the top tier with this interface, right? Is this really the right way to go long term? What makes you believe that we do not really hit the same issue with other tiers as well? Also do we want/need to duplicate all the limits for each/top tier? What is the reasoning for the switch to be runtime sysctl rather than boot-time or cgroup mount option?",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Mon 23-02-26 14:38:23, Joshua Hahn wrote:\n> Memory cgroups provide an interface that allow multiple workloads on a\n> host to co-exist, and establish both weak and strong memory isolation\n> guarantees. For large servers and small embedded systems alike, memcgs\n> provide an effective way to provide a baseline quality of service for\n> protected workloads.\n> \n> This works, because for the most part, all memory is equal (except for\n> zram / zswap). Restricting a cgroup's memory footprint restricts how\n> much it can hurt other workloads competing for memory. Likewise, setting\n> memory.low or memory.min limits can provide weak and strong guarantees\n> to the performance of a cgroup.\n> \n> However, on systems with tiered memory (e.g. CXL / compressed memory),\n> the quality of service guarantees that memcg limits enforced become less\n> effective, as memcg has no awareness of the physical location of its\n> charged memory. In other words, a workload that is well-behaved within\n> its memcg limits may still be hurting the performance of other\n> well-behaving workloads on the system by hogging more than its\n> \"fair share\" of toptier memory.\n\nThis assumes that the active workingset size of all workloads doesn't\nfit into the top tier right? Otherwise promotions would make sure to\nthat we have the most active memory in the top tier. Is this typical in\nreal life configurations?\n\nOr do you intend to limit memory consumption on particular tier even\nwithout an external pressure?\n\n> Introduce tier-aware memcg limits, which scale memory.low/high to\n> reflect the ratio of toptier:total memory the cgroup has access.\n> \n> Take the following scenario as an example:\n> On a host with 3:1 toptier:lowtier, say 150G toptier, and 50Glowtier,\n> setting a cgroup's limits to:\n> \tmemory.min:  15G\n> \tmemory.low:  20G\n> \tmemory.high: 40G\n> \tmemory.max:  50G\n> \n> Will be enforced at the toptier as:\n> \tmemory.min:          15G\n> \tmemory.toptier_low:  15G (20 * 150/200)\n> \tmemory.toptier_high: 30G (40 * 150/200)\n> \tmemory.max:          50G\n\nLet's spend some more time with the interface first. You seem to be\nfocusing only on the top tier with this interface, right? Is this really the\nright way to go long term? What makes you believe that we do not really\nhit the same issue with other tiers as well? Also do we want/need to\nduplicate all the limits for each/top tier? What is the reasoning for\nthe switch to be runtime sysctl rather than boot-time or cgroup mount\noption?\n\nI will likely have more questions but these are immediate ones after\nreading the cover. Please note I haven't really looked at the\nimplementation yet. I really want to understand usecases and interface\nfirst.\n-- \nMichal Hocko\nSUSE Labs\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "I hope that you are doing well! Thank you for taking the time to review my work and leaving your thoughts. I wanted to note that I hope to bring this discussion to LSFMMBPF as well, to discuss what the scope of the project should be, what usecases there are (as I will note below), how to make this scalable and sustainable for the future, etc. I'll send out a topic proposal later today. I had separated the series from the proposal because I imagined that this series would go through many versions, so it would be helpful to have the topic as a unified place for pre-conference discussions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "Hello Michal,\n\nI hope that you are doing well! Thank you for taking the time to review my\nwork and leaving your thoughts.\n\nI wanted to note that I hope to bring this discussion to LSFMMBPF as well,\nto discuss what the scope of the project should be, what usecases there\nare (as I will note below), how to make this scalable and sustainable\nfor the future, etc. I'll send out a topic proposal later today. I had\nseparated the series from the proposal because I imagined that this\nseries would go through many versions, so it would be helpful to have\nthe topic as a unified place for pre-conference discussions.\n\n> > Memory cgroups provide an interface that allow multiple workloads on a\n> > host to co-exist, and establish both weak and strong memory isolation\n> > guarantees. For large servers and small embedded systems alike, memcgs\n> > provide an effective way to provide a baseline quality of service for\n> > protected workloads.\n> > \n> > This works, because for the most part, all memory is equal (except for\n> > zram / zswap). Restricting a cgroup's memory footprint restricts how\n> > much it can hurt other workloads competing for memory. Likewise, setting\n> > memory.low or memory.min limits can provide weak and strong guarantees\n> > to the performance of a cgroup.\n> > \n> > However, on systems with tiered memory (e.g. CXL / compressed memory),\n> > the quality of service guarantees that memcg limits enforced become less\n> > effective, as memcg has no awareness of the physical location of its\n> > charged memory. In other words, a workload that is well-behaved within\n> > its memcg limits may still be hurting the performance of other\n> > well-behaving workloads on the system by hogging more than its\n> > \"fair share\" of toptier memory.\n\nI will split up your questions to answer them individually:\n\n> This assumes that the active workingset size of all workloads doesn't\n> fit into the top tier right?\n\nYes, for the scenario above, a workload that is violating its fair share\nof toptier memory mostly hurts other workloads if the aggregate working\nset size of all workloads exceeds the size of toptier memory.\n\n> Otherwise promotions would make sure to that we have the most active\n> memory in the top tier.\n\nThis is true. And for a lot of usecases, this is 100% the right thing to do.\nHowever, with this patch I want to encourage a different perspective,\nwhich is to think about things in a per-workload perspective, and not a\nper-system perspective.\n\nHaving hot memory in high tiers and cold memory in low tiers is only\nlogical, since we increase the system's throughput and make the most\noptimal choices for latency. However, what about systems that care about\nobjectives other than simply maximizing throughput?\n\nIn the original cover letter I offered an example of VM hosting services\nthat care less about maximizing host-wide throughput, but more on ensuring\na bottomline performance guarantee for all workloads running on the system.\nFor the users on these services, they don't care that the host their VM is\nrunning on is maximizing throughput; rather, they care that their VM meets\nthe performance guarantees that their provider promised. If there is no\nway to know or enforce which tier of memory their workload lands on, either\nthe bottomline guarantee becomes very underestimated, or users must deal\nwith a high variance in performance.\n\nHere's another example: Let's say there is a host with multiple workloads,\neach serving queries for a database. The host would like to guarantee the\nlowest maximum latency possible, while maximizing the total throughput\nof the system. Once again in this situation, without tier-aware memcg\nlimits the host can maximize throughput, but can only make severely\nunderestimated promises on the bottom line.\n\n> Is this typical in real life configurations?\n\nI would say so. I think that the two examples above are realistic\nscenarios that cloud providers and hyperscalers might face on tiered systems.\n\n> Or do you intend to limit memory consumption on particular tier even\n> without an external pressure?\n\nThis is a great question, and one that I hope to discuss at LSFMMBPF\nto see how people expect an interface like this to work.\n\nOver the past few weeks, I have been discussing this idea during the\nLinux Memory Hotness and Promotion biweekly calls with Gregory Price [1].\nOne of the proposals that we made there (but did not include in this\nseries) is the idea of \"fixed\" vs. \"opportunistic\" reclaim.\n\nFixed mode is what we have here -- start limiting toptier usage whenever\na workload goes above its fair slice of toptier.\nOpportunistic mode would allow workloads to use more toptier memory than\nits fair share, but only be restricted when toptier is pressured.\n\nWhat do you think about these two options? For the stated goal of this\nseries, which is to help maximize the bottom line for workloads, fair\nshare seemed to make sense. Implementing opportunistic mode changes\non top of this work would most likely just be another sysctl.\n\n> > Introduce tier-aware memcg limits, which scale memory.low/high to\n> > reflect the ratio of toptier:total memory the cgroup has access.\n> > \n> > Take the following scenario as an example:\n> > On a host with 3:1 toptier:lowtier, say 150G toptier, and 50Glowtier,\n> > setting a cgroup's limits to:\n> > \tmemory.min:  15G\n> > \tmemory.low:  20G\n> > \tmemory.high: 40G\n> > \tmemory.max:  50G\n> > \n> > Will be enforced at the toptier as:\n> > \tmemory.min:          15G\n> > \tmemory.toptier_low:  15G (20 * 150/200)\n> > \tmemory.toptier_high: 30G (40 * 150/200)\n> > \tmemory.max:          50G\n\nI will split up the following points to answer them individually as well:\n\n> Let's spend some more time with the interface first.\n\nThat sounds good with me, my goal was to bring this out as an RFC patchset\nso folks could look at the code and understand the motivation, and then send\nout the LSFMMBPF topic proposal. In retrospect I think I should have done\nit in the opposite order. I'm sorry if this caused any confusion.\n\n> You seem to be focusing only on the top tier with this interface, right?\n> Is this really the right way to go long term? What makes you believe that\n> we do not really hit the same issue with other tiers as well?\n\nYes, that's right. I'm not sure if this is the right way to go long-term\n(say, past the next 5 years). My thinking was that I can stick with doing\nthis for toptier vs. non-toptier memory for now, and deal with having\n3+ tiers in the future, when we start to have systems with that many tiers.\nAFAICT two-tiered systems are still ~relatively new, and I don't think\nthere are a lot of genuine usecases for enforcing mid-tier memory limits\nas of now. Of course, I would be excited to learn about these usecases\nand work this patchset to support them as well if anybody has them.\n\n> Also do we want/need to duplicate all the limits for each/top tier?\n\nSorry, I'm not sure that I completely understood this question. Are you\nreferring to the case where we have multiple nodes in the toptier?\nIf so, then all of those nodes are treated the same, and don't have\nunique limits. Or are you referring to the case where we have multiple\ntiers in the toptier? If so, I hope the answer above can answer this too.\n\n> What is the reasoning for the switch to be runtime sysctl rather than\n> boot-time or cgroup mount option?\n\nGood point : -) I don't think cgroup mount options are a good idea,\nsince this would mean that we can have a set of cgroups self-policing\ntheir toptier usage, while another cgroup allocates memory unrestricted.\nThis would punish the self-policing cgroup and we would lose the benefit\nof having a bottomline performance guarantee.\n\n> I will likely have more questions but these are immediate ones after\n> reading the cover. Please note I haven't really looked at the\n> implementation yet. I really want to understand usecases and interface\n> first.\n\nThat sounds good to me, thank you again for reviewing this work!\nI hope you have a great day : -)\nJoshua\n\n[1] https://lore.kernel.org/linux-mm/c8bc2dce-d4ec-c16e-8df4-2624c48cfc06@google.com/\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Gregory Price",
              "summary": "Just injecting a few points here (disclosure: I have been in the development loop for this feature) Yes / No.  This makes the assumption that you always want this. Barring a minimum Quality of Service mechanism (as Joshua explains) this reduces the usefulness of a secondary tier of memory. Services will just prefer not to be deployed to these kinds of machines because the performance variance is too high.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 08:13:56AM -0800, Joshua Hahn wrote:\n> ... snip ...\n\nJust injecting a few points here\n(disclosure: I have been in the development loop for this feature)\n\n> \n> > Otherwise promotions would make sure to that we have the most active\n> > memory in the top tier.\n> \n\nYes / No.  This makes the assumption that you always want this.\n\nBarring a minimum Quality of Service mechanism (as Joshua explains)\nthis reduces the usefulness of a secondary tier of memory.\n\nServices will just prefer not to be deployed to these kinds of\nmachines because the performance variance is too high.\n\n> \n> > Is this typical in real life configurations?\n> \n> I would say so. I think that the two examples above are realistic\n> scenarios that cloud providers and hyperscalers might face on tiered systems.\n> \n\nThe answer is unequivocally yes.\n\nLacking tier-awareness is actually a huge blocker for deploying mixed\nworkloads on large, dense memory systems with multiple tiers (2+).\n\nTechnically we're already at 4-ish tiers: DDR, CXL, ZSWAP, SWAP.\n\nWe have zswap/swap controls in cgroups already, we just lack that same\ncontrol for coherent memory tiers.  This tries to use the existing nobs\n(max/high/low/min) to do what they already do - just proportionally.\n\n~Gregory\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Kaiyang Zhao",
              "summary": "recently released a preprint paper on arXiv that includes case studies with a few of Meta's production workloads using a prototype version of the patches. The results confirmed that co-colocated workloads can have working set sizes exceeding the limited top-tier memory capacity given today's server memory shapes and workload stacking settings, causing contention of top-tier...",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 01:49:21PM -0500, Gregory Price wrote:\n> \n> > \n> > > Is this typical in real life configurations?\n> > \n> > I would say so. I think that the two examples above are realistic\n> > scenarios that cloud providers and hyperscalers might face on tiered systems.\n> > \n> \n> The answer is unequivocally yes.\n> \n> Lacking tier-awareness is actually a huge blocker for deploying mixed\n> workloads on large, dense memory systems with multiple tiers (2+).\n\nHello! I'm the author of the RFC in 2024. Just want to add that we've\nrecently released a preprint paper on arXiv that includes case studies\nwith a few of Meta's production workloads using a prototype version of\nthe patches.\n\nThe results confirmed that co-colocated workloads can have working set\nsizes exceeding the limited top-tier memory capacity given today's\nserver memory shapes and workload stacking settings, causing contention\nof top-tier memory. Workloads see significant variations in tail\nlatency and throughput depending on the share of top-tier tier memory\nthey get, which this patch set will alleviate.\n\nBest,\nKaiyang\n\n[1] https://arxiv.org/pdf/2602.08800\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH 0/6] mm/memcontrol: Make memcg limits tier-aware",
          "message_id": "20260224161357.2622501-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260224161357.2622501-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-24T18:06:59Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "Just injecting a few points here (disclosure: I have been in the development loop for this feature) Yes / No.  This makes the assumption that you always want this. Barring a minimum Quality of Service mechanism (as Joshua explains) this reduces the usefulness of a secondary tier of memory. Services will just prefer not to be deployed to these kinds of machines because the performance variance is too high.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 08:13:56AM -0800, Joshua Hahn wrote:\n> ... snip ...\n\nJust injecting a few points here\n(disclosure: I have been in the development loop for this feature)\n\n> \n> > Otherwise promotions would make sure to that we have the most active\n> > memory in the top tier.\n> \n\nYes / No.  This makes the assumption that you always want this.\n\nBarring a minimum Quality of Service mechanism (as Joshua explains)\nthis reduces the usefulness of a secondary tier of memory.\n\nServices will just prefer not to be deployed to these kinds of\nmachines because the performance variance is too high.\n\n> \n> > Is this typical in real life configurations?\n> \n> I would say so. I think that the two examples above are realistic\n> scenarios that cloud providers and hyperscalers might face on tiered systems.\n> \n\nThe answer is unequivocally yes.\n\nLacking tier-awareness is actually a huge blocker for deploying mixed\nworkloads on large, dense memory systems with multiple tiers (2+).\n\nTechnically we're already at 4-ish tiers: DDR, CXL, ZSWAP, SWAP.\n\nWe have zswap/swap controls in cgroups already, we just lack that same\ncontrol for coherent memory tiers.  This tries to use the existing nobs\n(max/high/low/min) to do what they already do - just proportionally.\n\n~Gregory\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Kaiyang Zhao",
              "summary": "recently released a preprint paper on arXiv that includes case studies with a few of Meta's production workloads using a prototype version of the patches. The results confirmed that co-colocated workloads can have working set sizes exceeding the limited top-tier memory capacity given today's server memory shapes and workload stacking settings, causing contention of top-tier...",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 01:49:21PM -0500, Gregory Price wrote:\n> \n> > \n> > > Is this typical in real life configurations?\n> > \n> > I would say so. I think that the two examples above are realistic\n> > scenarios that cloud providers and hyperscalers might face on tiered systems.\n> > \n> \n> The answer is unequivocally yes.\n> \n> Lacking tier-awareness is actually a huge blocker for deploying mixed\n> workloads on large, dense memory systems with multiple tiers (2+).\n\nHello! I'm the author of the RFC in 2024. Just want to add that we've\nrecently released a preprint paper on arXiv that includes case studies\nwith a few of Meta's production workloads using a prototype version of\nthe patches.\n\nThe results confirmed that co-colocated workloads can have working set\nsizes exceeding the limited top-tier memory capacity given today's\nserver memory shapes and workload stacking settings, causing contention\nof top-tier memory. Workloads see significant variations in tail\nlatency and throughput depending on the share of top-tier tier memory\nthey get, which this patch set will alleviate.\n\nBest,\nKaiyang\n\n[1] https://arxiv.org/pdf/2602.08800\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "JP Kobryn",
      "primary_email": "inwardvessel@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Kiryl Shutsemau",
      "primary_email": "kas@kernel.org",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Leo Martins",
      "primary_email": "loemra.dev@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 3/3] btrfs: add tracepoint for search slot restart tracking",
          "message_id": "18c04d9a68f64fa5e36dde196306170d0fb437d9.1771884128.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/18c04d9a68f64fa5e36dde196306170d0fb437d9.1771884128.git.loemra.dev@gmail.com/",
          "date": "2026-02-24T19:22:55Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Add a btrfs_search_slot_restart tracepoint that fires at each restart site in btrfs_search_slot(), recording the root, tree level, and reason for the restart. This enables tracking search slot restarts which contribute to COW amplification under memory pressure.\n\nThe four restart reasons are: - write_lock: insufficient write lock level, need to restart with higher lock - setup_nodes: node setup returned -EAGAIN - slot_zero: insertion at slot 0 requires higher write lock level - read_block: read_block_for_search returned -EAGAIN (block not cached or lock contention)\n\nCOW counts are already tracked by the existing trace_btrfs_cow_block() tracepoint. The per-restart-site tracepoint avoids counter overhead in the critical path when tracepoints are disabled, and provides richer per-event information that bpftrace scripts can aggregate into counts, histograms, and per-root breakdowns.",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 2/3] btrfs: inhibit extent buffer writeback to prevent COW amplification",
          "message_id": "cc847a35e26cc4dfad18c59e3c525cea507ff440.1771884128.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/cc847a35e26cc4dfad18c59e3c525cea507ff440.1771884128.git.loemra.dev@gmail.com/",
          "date": "2026-02-24T19:22:54Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Inhibit writeback on COW'd extent buffers for the lifetime of the transaction handle, preventing background writeback from setting BTRFS_HEADER_FLAG_WRITTEN and causing unnecessary re-COW.\n\nCOW amplification occurs when background writeback flushes an extent buffer that a transaction handle is still actively modifying. When lock_extent_buffer_for_io() transitions a buffer from dirty to writeback, it sets BTRFS_HEADER_FLAG_WRITTEN, marking the block as having been persisted to disk at its current bytenr. Once WRITTEN is set, should_cow_block() must either COW the block again or overwrite it in place, both of which are unnecessary overhead when the buffer is still being modified by the same handle that allocated it. By inhibiting background writeback on actively-used buffers, WRITTEN is never set while a transaction handle holds a reference to the buffer, avoiding this overhead entirely.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Sun YangKai",
              "summary": "I might be missing something here, but I'm curious whether this atomic counter can ever go above 1. If not, and it's strictly binary, perhaps using atomic_set(1/0) instead of atomic_inc/dec would make the intent clearer? Otherwise looks good. Thanks.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "\n\nOn 2026/2/25 03:22, Leo Martins wrote:\n> Inhibit writeback on COW'd extent buffers for the lifetime of the\n> transaction handle, preventing background writeback from setting\n> BTRFS_HEADER_FLAG_WRITTEN and causing unnecessary re-COW.\n> \n> COW amplification occurs when background writeback flushes an extent\n> buffer that a transaction handle is still actively modifying. When\n> lock_extent_buffer_for_io() transitions a buffer from dirty to\n> writeback, it sets BTRFS_HEADER_FLAG_WRITTEN, marking the block as\n> having been persisted to disk at its current bytenr. Once WRITTEN is\n> set, should_cow_block() must either COW the block again or overwrite\n> it in place, both of which are unnecessary overhead when the buffer\n> is still being modified by the same handle that allocated it. By\n> inhibiting background writeback on actively-used buffers, WRITTEN is\n> never set while a transaction handle holds a reference to the buffer,\n> avoiding this overhead entirely.\n> \n> Add an atomic_t writeback_inhibitors counter to struct extent_buffer,\n> which fits in an existing 6-byte hole without increasing struct size.\n> When a buffer is COW'd in btrfs_force_cow_block(), call\n> btrfs_inhibit_eb_writeback() to store the eb in the transaction\n> handle's writeback_inhibited_ebs xarray (keyed by eb->start), take a\n> reference, and increment writeback_inhibitors. The function handles\n> dedup (same eb inhibited twice by the same handle) and replacement\n> (different eb at the same logical address). Allocation failure is\n> graceful: the buffer simply falls back to the pre-existing behavior\n> where it may be written back and re-COW'd.\n> \n> In lock_extent_buffer_for_io(), when writeback_inhibitors is non-zero\n> and the writeback mode is WB_SYNC_NONE, skip the buffer. WB_SYNC_NONE\n> is used by the VM flusher threads for background and periodic\n> writeback, which are the only paths that cause COW amplification by\n> opportunistically writing out dirty extent buffers mid-transaction.\n> Skipping these is safe because the buffers remain dirty in the page\n> cache and will be written out at transaction commit time.\n> \n> WB_SYNC_ALL must always proceed regardless of writeback_inhibitors.\n> This is required for correctness in the fsync path: btrfs_sync_log()\n> writes log tree blocks via filemap_fdatawrite_range() (WB_SYNC_ALL)\n> while the transaction handle that inhibited those same blocks is still\n> active. Without the WB_SYNC_ALL bypass, those inhibited log tree\n> blocks would be silently skipped, resulting in an incomplete log on\n> disk and corruption on replay. btrfs_write_and_wait_transaction()\n> also uses WB_SYNC_ALL via filemap_fdatawrite_range(); for that path,\n> inhibitors are already cleared beforehand, but the bypass ensures\n> correctness regardless.\n> \n> Uninhibit in __btrfs_end_transaction() before atomic_dec(num_writers)\n> to prevent a race where the committer proceeds while buffers are still\n> inhibited. Also uninhibit in btrfs_commit_transaction() before writing\n> and in cleanup_transaction() for the error path.\n> \n> Signed-off-by: Leo Martins <loemra.dev@gmail.com>\n> Reviewed-by: Filipe Manana <fdmanana@suse.com>\n> ---\n>   fs/btrfs/ctree.c       |  4 +++\n>   fs/btrfs/extent_io.c   | 63 +++++++++++++++++++++++++++++++++++++++++-\n>   fs/btrfs/extent_io.h   |  6 ++++\n>   fs/btrfs/transaction.c | 19 +++++++++++++\n>   fs/btrfs/transaction.h |  3 ++\n>   5 files changed, 94 insertions(+), 1 deletion(-)\n> \n> diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\n> index 0e02b7b14adc..d4da65bb9096 100644\n> --- a/fs/btrfs/ctree.c\n> +++ b/fs/btrfs/ctree.c\n> @@ -590,6 +590,10 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,\n>   \t\tbtrfs_tree_unlock(buf);\n>   \tfree_extent_buffer_stale(buf);\n>   \tbtrfs_mark_buffer_dirty(trans, cow);\n> +\n> +\t/* Inhibit writeback on the COW'd buffer for this transaction handle. */\n> +\tbtrfs_inhibit_eb_writeback(trans, cow);\n> +\n>   \t*cow_ret = cow;\n>   \treturn 0;\n>   \n> diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c\n> index ff1fc699a6ca..e04e42a81978 100644\n> --- a/fs/btrfs/extent_io.c\n> +++ b/fs/btrfs/extent_io.c\n> @@ -1940,7 +1940,9 @@ static noinline_for_stack bool lock_extent_buffer_for_io(struct extent_buffer *e\n>   \t * of time.\n>   \t */\n>   \tspin_lock(&eb->refs_lock);\n> -\tif (test_and_clear_bit(EXTENT_BUFFER_DIRTY, &eb->bflags)) {\n> +\tif ((wbc->sync_mode == WB_SYNC_ALL ||\n> +\t     atomic_read(&eb->writeback_inhibitors) == 0) &&\n> +\t    test_and_clear_bit(EXTENT_BUFFER_DIRTY, &eb->bflags)) {\n>   \t\tXA_STATE(xas, &fs_info->buffer_tree, eb->start >> fs_info->nodesize_bits);\n>   \t\tunsigned long flags;\n>   \n> @@ -2999,6 +3001,64 @@ static inline void btrfs_release_extent_buffer(struct extent_buffer *eb)\n>   \tkmem_cache_free(extent_buffer_cache, eb);\n>   }\n>   \n> +/*\n> + * btrfs_inhibit_eb_writeback - Inhibit writeback on buffer during transaction.\n> + * @trans: transaction handle that will own the inhibitor\n> + * @eb: extent buffer to inhibit writeback on\n> + *\n> + * Attempts to track this extent buffer in the transaction's inhibited set.\n> + * If memory allocation fails, the buffer is simply not tracked. It may\n> + * be written back and need re-COW, which is the original behavior.\n> + * This is acceptable since inhibiting writeback is an optimization.\n> + */\n> +void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,\n> +\t\t\t\tstruct extent_buffer *eb)\n> +{\n> +\tunsigned long index = eb->start >> trans->fs_info->nodesize_bits;\n> +\tvoid *old;\n> +\n> +\t/* Check if already inhibited by this handle. */\n> +\told = xa_load(&trans->writeback_inhibited_ebs, index);\n> +\tif (old == eb)\n> +\t\treturn;\n> +\n> +\t/* Take reference for the xarray entry. */\n> +\trefcount_inc(&eb->refs);\n> +\n> +\told = xa_store(&trans->writeback_inhibited_ebs, index, eb, GFP_NOFS);\n> +\tif (xa_is_err(old)) {\n> +\t\t/* Allocation failed, just skip inhibiting this buffer. */\n> +\t\tfree_extent_buffer(eb);\n> +\t\treturn;\n> +\t}\n> +\n> +\t/* Handle replacement of different eb at same index. */\n> +\tif (old && old != eb) {\n> +\t\tstruct extent_buffer *old_eb = old;\n> +\n> +\t\tatomic_dec(&old_eb->writeback_inhibitors);\n> +\t\tfree_extent_buffer(old_eb);\n> +\t}\n> +\n> +\tatomic_inc(&eb->writeback_inhibitors);\n> +}\n> +\n> +/*\n> + * btrfs_uninhibit_all_eb_writeback - Uninhibit writeback on all buffers.\n> + * @trans: transaction handle to clean up\n> + */\n> +void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans)\n> +{\n> +\tstruct extent_buffer *eb;\n> +\tunsigned long index;\n> +\n> +\txa_for_each(&trans->writeback_inhibited_ebs, index, eb) {\n> +\t\tatomic_dec(&eb->writeback_inhibitors);\n> +\t\tfree_extent_buffer(eb);\n> +\t}\n> +\txa_destroy(&trans->writeback_inhibited_ebs);\n> +}\n> +\n>   static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info,\n>   \t\t\t\t\t\t   u64 start)\n>   {\n> @@ -3009,6 +3069,7 @@ static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info\n>   \teb->len = fs_info->nodesize;\n>   \teb->fs_info = fs_info;\n>   \tinit_rwsem(&eb->lock);\n> +\tatomic_set(&eb->writeback_inhibitors, 0);\n>   \n>   \tbtrfs_leak_debug_add_eb(eb);\n>   \n> diff --git a/fs/btrfs/extent_io.h b/fs/btrfs/extent_io.h\n> index 73571d5d3d5a..fb68fbd4866c 100644\n> --- a/fs/btrfs/extent_io.h\n> +++ b/fs/btrfs/extent_io.h\n> @@ -102,6 +102,8 @@ struct extent_buffer {\n>   \t/* >= 0 if eb belongs to a log tree, -1 otherwise */\n>   \ts8 log_index;\n>   \tu8 folio_shift;\n> +\t/* Inhibits WB_SYNC_NONE writeback when > 0. */\n> +\tatomic_t writeback_inhibitors;\n\nI might be missing something here, but I'm curious whether this atomic \ncounter can ever go above 1. If not, and it's strictly binary, perhaps \nusing atomic_set(1/0) instead of atomic_inc/dec would make the intent \nclearer?\n\nOtherwise looks good. Thanks.\n\n>   \tstruct rcu_head rcu_head;\n>   \n>   \tstruct rw_semaphore lock;\n> @@ -381,4 +383,8 @@ void btrfs_extent_buffer_leak_debug_check(struct btrfs_fs_info *fs_info);\n>   #define btrfs_extent_buffer_leak_debug_check(fs_info)\tdo {} while (0)\n>   #endif\n>   \n> +void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,\n> +\t\t\t       struct extent_buffer *eb);\n> +void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans);\n> +\n>   #endif\n> diff --git a/fs/btrfs/transaction.c b/fs/btrfs/transaction.c\n> index f4cc9e1a1b93..a9a22629b49d 100644\n> --- a/fs/btrfs/transaction.c\n> +++ b/fs/btrfs/transaction.c\n> @@ -15,6 +15,7 @@\n>   #include \"misc.h\"\n>   #include \"ctree.h\"\n>   #include \"disk-io.h\"\n> +#include \"extent_io.h\"\n>   #include \"transaction.h\"\n>   #include \"locking.h\"\n>   #include \"tree-log.h\"\n> @@ -688,6 +689,8 @@ start_transaction(struct btrfs_root *root, unsigned int num_items,\n>   \t\tgoto alloc_fail;\n>   \t}\n>   \n> +\txa_init(&h->writeback_inhibited_ebs);\n> +\n>   \t/*\n>   \t * If we are JOIN_NOLOCK we're already committing a transaction and\n>   \t * waiting on this guy, so we don't need to do the sb_start_intwrite\n> @@ -1083,6 +1086,13 @@ static int __btrfs_end_transaction(struct btrfs_trans_handle *trans,\n>   \tif (trans->type & __TRANS_FREEZABLE)\n>   \t\tsb_end_intwrite(info->sb);\n>   \n> +\t/*\n> +\t * Uninhibit extent buffer writeback before decrementing num_writers,\n> +\t * since the decrement wakes the committing thread which needs all\n> +\t * buffers uninhibited to write them to disk.\n> +\t */\n> +\tbtrfs_uninhibit_all_eb_writeback(trans);\n> +\n>   \tWARN_ON(cur_trans != info->running_transaction);\n>   \tWARN_ON(atomic_read(&cur_trans->num_writers) < 1);\n>   \tatomic_dec(&cur_trans->num_writers);\n> @@ -2110,6 +2120,7 @@ static void cleanup_transaction(struct btrfs_trans_handle *trans, int err)\n>   \tif (!test_bit(BTRFS_FS_RELOC_RUNNING, &fs_info->flags))\n>   \t\tbtrfs_scrub_cancel(fs_info);\n>   \n> +\tbtrfs_uninhibit_all_eb_writeback(trans);\n>   \tkmem_cache_free(btrfs_trans_handle_cachep, trans);\n>   }\n>   \n> @@ -2556,6 +2567,14 @@ int btrfs_commit_transaction(struct btrfs_trans_handle *trans)\n>   \t    fs_info->cleaner_kthread)\n>   \t\twake_up_process(fs_info->cleaner_kthread);\n>   \n> +\t/*\n> +\t * Uninhibit writeback on all extent buffers inhibited during this\n> +\t * transaction before writing them to disk. Inhibiting prevented\n> +\t * writeback while the transaction was building, but now we need\n> +\t * them written.\n> +\t */\n> +\tbtrfs_uninhibit_all_eb_writeback(trans);\n> +\n>   \tret = btrfs_write_and_wait_transaction(trans);\n>   \tif (unlikely(ret)) {\n>   \t\tbtrfs_err(fs_info, \"error while writing out transaction: %d\", ret);\n> diff --git a/fs/btrfs/transaction.h b/fs/btrfs/transaction.h\n> index 18ef069197e5..7d70fe486758 100644\n> --- a/fs/btrfs/transaction.h\n> +++ b/fs/btrfs/transaction.h\n> @@ -12,6 +12,7 @@\n>   #include <linux/time64.h>\n>   #include <linux/mutex.h>\n>   #include <linux/wait.h>\n> +#include <linux/xarray.h>\n>   #include \"btrfs_inode.h\"\n>   #include \"delayed-ref.h\"\n>   \n> @@ -162,6 +163,8 @@ struct btrfs_trans_handle {\n>   \tstruct btrfs_fs_info *fs_info;\n>   \tstruct list_head new_bgs;\n>   \tstruct btrfs_block_rsv delayed_rsv;\n> +\t/* Extent buffers with writeback inhibited by this handle. */\n> +\tstruct xarray writeback_inhibited_ebs;\n>   };\n>   \n>   /*\n\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 1/3] btrfs: skip COW for written extent buffers allocated in current transaction",
          "message_id": "4ce911a475b998ddf76951629ad203e6440ab0ca.1771884128.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/4ce911a475b998ddf76951629ad203e6440ab0ca.1771884128.git.loemra.dev@gmail.com/",
          "date": "2026-02-24T19:22:52Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "When memory pressure causes writeback of a recently COW'd buffer, btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent btrfs_search_slot() restarts then see the WRITTEN flag and re-COW the buffer unnecessarily, causing COW amplification that can exhaust block reservations and degrade throughput.\n\nOverwriting in place is crash-safe because the committed superblock does not reference buffers allocated in the current (uncommitted) transaction, so no on-disk tree points to this block yet.\n\nWhen should_cow_block() encounters a WRITTEN buffer whose generation matches the current transaction, instead of requesting a COW, re-dirty the buffer and re-register its range in the transaction's dirty_pages.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Sun YangKai",
              "summary": "should_cow_block(), making the overall logic easier to follow. And the commit message and comments are quite detailed and helpful.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\nOn 2026/2/25 03:22, Leo Martins wrote:\n> When memory pressure causes writeback of a recently COW'd buffer,\n> btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent\n> btrfs_search_slot() restarts then see the WRITTEN flag and re-COW\n> the buffer unnecessarily, causing COW amplification that can exhaust\n> block reservations and degrade throughput.\n> \n> Overwriting in place is crash-safe because the committed superblock\n> does not reference buffers allocated in the current (uncommitted)\n> transaction, so no on-disk tree points to this block yet.\n> \n> When should_cow_block() encounters a WRITTEN buffer whose generation\n> matches the current transaction, instead of requesting a COW, re-dirty\n> the buffer and re-register its range in the transaction's dirty_pages.\n> \n> Both are necessary because btrfs tracks dirty metadata through two\n> independent mechanisms. set_extent_buffer_dirty() sets the\n> EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY\n> mark, which is what background writeback (btree_write_cache_pages) uses\n> to find and write dirty buffers. The transaction's dirty_pages io tree\n> is a separate structure used by btrfs_write_and_wait_transaction() at\n> commit time to ensure all buffers allocated during the transaction are\n> persisted. The dirty_pages range was originally registered in\n> btrfs_init_new_buffer() when the block was first allocated. Normally\n> dirty_pages is only cleared at commit time by\n> btrfs_write_and_wait_transaction(), but if qgroups are enabled and\n> snapshots are being created, qgroup_account_snapshot() may have already\n> called btrfs_write_and_wait_transaction() and released the range before\n> the final commit-time call.\n> \n> Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()\n> correctly pins the block if it is freed later.\n> \n> Relax the lockdep assertion in btrfs_mark_buffer_dirty() from\n> btrfs_assert_tree_write_locked() to lockdep_assert_held() so that it\n> accepts either a read or write lock. should_cow_block() may be called\n> from btrfs_search_slot() when only a read lock is held (nodes above\n> write_lock_level are read-locked). The write lock assertion previously\n> documented the caller convention that buffer content was being modified\n> under exclusive access, but btrfs_mark_buffer_dirty() and\n> set_extent_buffer_dirty() themselves only perform independently\n> synchronized operations: atomic bit ops on bflags, folio_mark_dirty()\n> (kernel-internal folio locking), xarray mark updates (xarray spinlock),\n> and percpu counter updates. The read lock is sufficient because it\n> prevents lock_extent_buffer_for_io() from acquiring the write lock and\n> racing on the dirty state. Since rw_semaphore permits concurrent\n> readers, multiple threads can enter btrfs_mark_buffer_dirty()\n> simultaneously for the same buffer; this is safe because\n> test_and_set_bit(EXTENT_BUFFER_DIRTY) ensures only one thread performs\n> the full dirty state transition.\n> \n> Remove the CONFIG_BTRFS_DEBUG assertion in set_extent_buffer_dirty()\n> that checked folio_test_dirty() after marking the buffer dirty. This\n> assertion assumed exclusive access (only one thread in\n> set_extent_buffer_dirty() at a time), which held when the only caller\n> was btrfs_mark_buffer_dirty() under write lock. With concurrent readers\n> calling through should_cow_block(), a thread that loses the\n> test_and_set_bit race sees was_dirty=true and skips the folio dirty\n> marking, but the winning thread may not have called\n> btrfs_meta_folio_set_dirty() yet, causing the assertion to fire. This\n> is a benign race: the winning thread will complete the folio dirty\n> marking, and no writeback can clear it while readers hold their locks.\n> \n> Hoist the EXTENT_BUFFER_WRITEBACK, BTRFS_HEADER_FLAG_RELOC, and\n> BTRFS_ROOT_FORCE_COW checks before the WRITTEN block since they apply\n> regardless of whether the buffer has been written back. This\n> consolidates the exclusion logic and simplifies the WRITTEN path to\n> only handle log trees and zoned devices. Moving the RELOC checks\n> before the smp_mb__before_atomic() barrier is safe because both\n> btrfs_root_id() (immutable) and BTRFS_HEADER_FLAG_RELOC (set at COW\n> time under tree lock) are stable values not subject to concurrent\n> modification; the barrier is only needed for BTRFS_ROOT_FORCE_COW\n> which is set concurrently by create_pending_snapshot().\n> \n> Exclude cases where in-place overwrite is not safe:\n>   - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O\n>   - Zoned devices: require sequential writes\n>   - Log trees: log blocks are immediately referenced by a committed\n>     superblock via btrfs_sync_log(), so overwriting could corrupt the\n>     committed log\n>   - BTRFS_ROOT_FORCE_COW: snapshot in progress\n>   - BTRFS_HEADER_FLAG_RELOC: block being relocated\n> \n> Signed-off-by: Leo Martins <loemra.dev@gmail.com>\n\nThanks! Looks good now. Currently we only early return true in \nshould_cow_block(), making the overall logic easier to follow.\n\nAnd the commit message and comments are quite detailed and helpful.\n\nReviewed-by: Sun YangKai <sunk67188@gmail.com>\n\n> ---\n>   fs/btrfs/ctree.c     | 56 ++++++++++++++++++++++++++++++++++++++------\n>   fs/btrfs/disk-io.c   |  2 +-\n>   fs/btrfs/extent_io.c |  4 ----\n>   3 files changed, 50 insertions(+), 12 deletions(-)\n> \n> diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\n> index 7267b2502665..0e02b7b14adc 100644\n> --- a/fs/btrfs/ctree.c\n> +++ b/fs/btrfs/ctree.c\n> @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,\n>   \treturn ret;\n>   }\n>   \n> -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n> +static inline bool should_cow_block(struct btrfs_trans_handle *trans,\n>   \t\t\t\t    const struct btrfs_root *root,\n> -\t\t\t\t    const struct extent_buffer *buf)\n> +\t\t\t\t    struct extent_buffer *buf)\n>   {\n>   \tif (btrfs_is_testing(root->fs_info))\n>   \t\treturn false;\n> @@ -621,7 +621,11 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n>   \tif (btrfs_header_generation(buf) != trans->transid)\n>   \t\treturn true;\n>   \n> -\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))\n> +\tif (test_bit(EXTENT_BUFFER_WRITEBACK, &buf->bflags))\n> +\t\treturn true;\n> +\n> +\tif (btrfs_root_id(root) != BTRFS_TREE_RELOC_OBJECTID &&\n> +\t    btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC))\n>   \t\treturn true;\n>   \n>   \t/* Ensure we can see the FORCE_COW bit. */\n> @@ -629,11 +633,49 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n>   \tif (test_bit(BTRFS_ROOT_FORCE_COW, &root->state))\n>   \t\treturn true;\n>   \n> -\tif (btrfs_root_id(root) == BTRFS_TREE_RELOC_OBJECTID)\n> -\t\treturn false;\n> +\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {\n> +\t\t/*\n> +\t\t * The buffer was allocated in this transaction and has been\n> +\t\t * written back to disk (WRITTEN is set). Normally we'd COW\n> +\t\t * it again, but since the committed superblock doesn't\n> +\t\t * reference this buffer (it was allocated in this transaction),\n> +\t\t * we can safely overwrite it in place.\n> +\t\t *\n> +\t\t * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been\n> +\t\t * persisted at this bytenr and will be again after the\n> +\t\t * in-place update. This is important so that\n> +\t\t * btrfs_free_tree_block() correctly pins the block if it is\n> +\t\t * freed later (e.g., during tree rebalancing or FORCE_COW).\n> +\t\t *\n> +\t\t * Log trees and zoned devices cannot use this optimization:\n> +\t\t * - Log trees: log blocks are written and immediately\n> +\t\t *   referenced by a committed superblock via\n> +\t\t *   btrfs_sync_log(), bypassing the normal transaction\n> +\t\t *   commit. Overwriting in place could corrupt the\n> +\t\t *   committed log.\n> +\t\t * - Zoned devices: require sequential writes.\n> +\t\t */\n> +\t\tif (btrfs_root_id(root) == BTRFS_TREE_LOG_OBJECTID ||\n> +\t\t    btrfs_is_zoned(root->fs_info))\n> +\t\t\treturn true;\n>   \n> -\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC))\n> -\t\treturn true;\n> +\t\t/*\n> +\t\t * Re-register this block's range in the current transaction's\n> +\t\t * dirty_pages so that btrfs_write_and_wait_transaction()\n> +\t\t * writes it. The range was originally registered when the\n> +\t\t * block was allocated. Normally dirty_pages is only cleared\n> +\t\t * at commit time by btrfs_write_and_wait_transaction(), but\n> +\t\t * if qgroups are enabled and snapshots are being created,\n> +\t\t * qgroup_account_snapshot() may have already called\n> +\t\t * btrfs_write_and_wait_transaction() and released the range\n> +\t\t * before the final commit-time call.\n> +\t\t */\n> +\t\tbtrfs_set_extent_bit(&trans->transaction->dirty_pages,\n> +\t\t\t\t     buf->start,\n> +\t\t\t\t     buf->start + buf->len - 1,\n> +\t\t\t\t     EXTENT_DIRTY, NULL);\n> +\t\tbtrfs_mark_buffer_dirty(trans, buf);\n> +\t}\n>   \n>   \treturn false;\n>   }\n> diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c\n> index 32fffb0557e5..bee8f76fbfea 100644\n> --- a/fs/btrfs/disk-io.c\n> +++ b/fs/btrfs/disk-io.c\n> @@ -4491,7 +4491,7 @@ void btrfs_mark_buffer_dirty(struct btrfs_trans_handle *trans,\n>   #endif\n>   \t/* This is an active transaction (its state < TRANS_STATE_UNBLOCKED). */\n>   \tASSERT(trans->transid == fs_info->generation);\n> -\tbtrfs_assert_tree_write_locked(buf);\n> +\tlockdep_assert_held(&buf->lock);\n>   \tif (unlikely(transid != fs_info->generation)) {\n>   \t\tbtrfs_abort_transaction(trans, -EUCLEAN);\n>   \t\tbtrfs_crit(fs_info,\n> diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c\n> index dfc17c292217..ff1fc699a6ca 100644\n> --- a/fs/btrfs/extent_io.c\n> +++ b/fs/btrfs/extent_io.c\n> @@ -3791,10 +3791,6 @@ void set_extent_buffer_dirty(struct extent_buffer *eb)\n>   \t\t\t\t\t eb->len,\n>   \t\t\t\t\t eb->fs_info->dirty_metadata_batch);\n>   \t}\n> -#ifdef CONFIG_BTRFS_DEBUG\n> -\tfor (int i = 0; i < num_extent_folios(eb); i++)\n> -\t\tASSERT(folio_test_dirty(eb->folios[i]));\n> -#endif\n>   }\n>   \n>   void clear_extent_buffer_uptodate(struct extent_buffer *eb)\n\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 0/3] btrfs: fix COW amplification under memory pressure",
          "message_id": "cover.1771884128.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/cover.1771884128.git.loemra.dev@gmail.com/",
          "date": "2026-02-24T19:22:51Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "I've been investigating a pattern where COW amplification under memory pressure exhausts the transaction block reserve, leading to spurious ENOSPCs on filesystems with plenty of unallocated space.\n\nThe pattern is:\n\n1. btrfs_search_slot() begins tree traversal with cow=1 2. Node at level N needs COW (old generation or WRITTEN flag set) 3. btrfs_cow_block() allocates new block, copies data, updates parent pointer 4. Traversal hits a condition requiring restart (node not cached, lock contention, need higher write_lock_level) 5. btrfs_release_path() releases all locks and references 6. Memory pressure triggers writeback on the COW'd block 7. lock_extent_buffer_for_io() clears EXTENT_BUFFER_DIRTY and sets BTRFS_HEADER_FLAG_WRITTEN 8. goto again - traversal restarts from root 9. Traversal reaches the same tree node 10. should_cow_block() sees WRITTEN flag, returns true 11. btrfs_cow_block() allocates yet another new block, copies data, updates parent pointer again, consuming another reservation 12. Steps 4-11 repeat under sustained memory pressure\n\nThis series fixes the problem with two complementary approaches:\n\nPatch 1 implements Filipe's suggestion of overwriting in place. When should_cow_block() encounters a WRITTEN buffer whose generation matches the current transaction, it re-dirties the buffer and returns false instead of requesting a COW. This is crash-safe because the committed superblock does not reference buffers allocated in the current transaction. Log trees, zoned devices, FORCE_COW, relocation, and buffers mid-writeback are excluded. Beyond improving the amplification bug, this is a general optimization for the entire transaction lifetime: any time writeback runs during a transaction, revisiting the same path no longer triggers unnecessary COW, reducing extent allocation overhead, memory copies, and space usage per transaction.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "You can add to all the patches:",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Tue, Feb 24, 2026 at 7:23 PM Leo Martins <loemra.dev@gmail.com> wrote:\n>\n> I've been investigating a pattern where COW amplification under memory\n> pressure exhausts the transaction block reserve, leading to spurious\n> ENOSPCs on filesystems with plenty of unallocated space.\n>\n> The pattern is:\n>\n>  1. btrfs_search_slot() begins tree traversal with cow=1\n>  2. Node at level N needs COW (old generation or WRITTEN flag set)\n>  3. btrfs_cow_block() allocates new block, copies data, updates\n>     parent pointer\n>  4. Traversal hits a condition requiring restart (node not cached,\n>     lock contention, need higher write_lock_level)\n>  5. btrfs_release_path() releases all locks and references\n>  6. Memory pressure triggers writeback on the COW'd block\n>  7. lock_extent_buffer_for_io() clears EXTENT_BUFFER_DIRTY and sets\n>     BTRFS_HEADER_FLAG_WRITTEN\n>  8. goto again - traversal restarts from root\n>  9. Traversal reaches the same tree node\n>  10. should_cow_block() sees WRITTEN flag, returns true\n>  11. btrfs_cow_block() allocates yet another new block, copies data,\n>      updates parent pointer again, consuming another reservation\n>  12. Steps 4-11 repeat under sustained memory pressure\n>\n> This series fixes the problem with two complementary approaches:\n>\n> Patch 1 implements Filipe's suggestion of overwriting in place. When\n> should_cow_block() encounters a WRITTEN buffer whose generation matches\n> the current transaction, it re-dirties the buffer and returns false\n> instead of requesting a COW. This is crash-safe because the committed\n> superblock does not reference buffers allocated in the current\n> transaction. Log trees, zoned devices, FORCE_COW, relocation, and\n> buffers mid-writeback are excluded. Beyond improving the amplification bug,\n> this is a general optimization for the entire transaction lifetime: any\n> time writeback runs during a transaction, revisiting the same path no\n> longer triggers unnecessary COW, reducing extent allocation overhead,\n> memory copies, and space usage per transaction.\n>\n> Patch 2 inhibits writeback on COW'd buffers for the lifetime of the\n> transaction handle. This prevents WRITTEN from being set while a\n> handle holds a reference to the buffer, avoiding unnecessary re-COW\n> at its source. Only WB_SYNC_NONE (background/periodic flusher\n> writeback) is inhibited. WB_SYNC_ALL (data-integrity writeback from\n> btrfs_sync_log() and btrfs_write_and_wait_transaction()) always\n> proceeds, which is required for correctness in the fsync path where\n> log tree blocks must be written while the inhibiting handle is still\n> active.\n>\n> Both approaches are independently useful. The overwrite path is a\n> general optimization that eliminates unnecessary COW across the entire\n> transaction, not just within a single handle. Writeback inhibition\n> prevents the problem from occurring in the first place and is the\n> only fix for log trees and zoned devices where overwrite does not\n> apply. Together they provide defense in depth against COW\n> amplification.\n>\n> Patch 3 adds a tracepoint for tracking search slot restarts.\n>\n> Note: Boris's AS_KERNEL_FILE changes prevent cgroup-scoped reclaim\n> from targeting extent buffer pages, making this harder to trigger via\n> per-cgroup memory limits. I was able to reproduce the amplification\n> using global memory pressure (drop_caches, root cgroup memory.reclaim,\n> memory hog) with concurrent filesystem operations.\n>\n> Benchmark: concurrent filesystem operations under aggressive global\n> memory pressure. COW counts measured via bpftrace.\n>\n>   Approach                   Max COW count   Num operations\n>   -------                    -------------   --------------\n>   Baseline                              35   -\n>   Overwrite only                        19   ~same\n>   Writeback inhibition only              5   ~2x\n>   Combined (this series)                 4   ~2x\n>\n> Changes since v2:\n>\n> Patch 1:\n>  - Add smp_mb__before_atomic() before FORCE_COW check (Filipe, Sun YangKai)\n>  - Hoist WRITEBACK, RELOC, FORCE_COW checks before WRITTEN block (Sun YangKai)\n>  - Update dirty_pages comment for qgroup_account_snapshot() (Filipe)\n>  - Relax btrfs_mark_buffer_dirty() lockdep assertion to accept read locks\n>  - Use btrfs_mark_buffer_dirty() instead of set_extent_buffer_dirty()\n>  - Remove folio_test_dirty() debug assertion (concurrent reader race)\n>\n> Patch 2:\n>  - Fix comment style (Filipe)\n>\n> Patch 3:\n>  - Replace counters with per-restart-site tracepoint (Filipe)\n>\n> Changes since v1:\n>\n> The v1 patch used a per-btrfs_search_slot() xarray to track COW'd\n> buffers. Filipe pointed out this was too complex and too narrow in\n> scope, and suggested overwriting in place instead. Qu raised the\n> concern that other btrfs_cow_block() callers outside of search_slot\n> would not be covered. Boris suggested transaction-handle-level scope\n> as an alternative.\n>\n> v2 implemented both overwrite-in-place and writeback inhibition at\n> the transaction handle level. The per-search-slot xarray was replaced\n> by a per-transaction-handle xarray, which covers all\n> btrfs_force_cow_block() callers.\n>\n> Leo Martins (3):\n>   btrfs: skip COW for written extent buffers allocated in current\n>     transaction\n>   btrfs: inhibit extent buffer writeback to prevent COW amplification\n>   btrfs: add tracepoint for search slot restart tracking\n\nYou can add to all the patches:\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\nThanks.\n\n>\n>  fs/btrfs/ctree.c             | 70 +++++++++++++++++++++++++++++++-----\n>  fs/btrfs/disk-io.c           |  2 +-\n>  fs/btrfs/extent_io.c         | 67 +++++++++++++++++++++++++++++++---\n>  fs/btrfs/extent_io.h         |  6 ++++\n>  fs/btrfs/transaction.c       | 19 ++++++++++\n>  fs/btrfs/transaction.h       |  3 ++\n>  include/trace/events/btrfs.h | 24 +++++++++++++\n>  7 files changed, 176 insertions(+), 15 deletions(-)\n>\n> --\n> 2.47.3\n>\n>\n\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Mark Harmstone",
      "primary_email": "mark@harmstone.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix error message in btrfs_validate_super()",
          "message_id": "20260217185335.21013-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260217185335.21013-1-mark@harmstone.com/",
          "date": "2026-02-17T18:54:05Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-17",
          "patch_summary": "Fix the superblock offset mismatch error message in btrfs_validate_super(): we changed it so that it considers all the superblocks, but the message still assumes we're only looking at the first one.\n\nThe change from %u to %llu is because we're changing from a constant to a u64.",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Qu Wenruo",
              "summary": "在 2026/2/18 05:23, Mark Harmstone 写道:",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\n在 2026/2/18 05:23, Mark Harmstone 写道:\n> Fix the superblock offset mismatch error message in\n> btrfs_validate_super(): we changed it so that it considers all the\n> superblocks, but the message still assumes we're only looking at the\n> first one.\n> \n> The change from %u to %llu is because we're changing from a constant to\n> a u64.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 069ec957c35e (\"btrfs: Refactor btrfs_check_super_valid\")\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\n\nThanks,\nQu\n\n> ---\n>   fs/btrfs/disk-io.c | 4 ++--\n>   1 file changed, 2 insertions(+), 2 deletions(-)\n> \n> diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c\n> index 600287ac8eb7..f39008591631 100644\n> --- a/fs/btrfs/disk-io.c\n> +++ b/fs/btrfs/disk-io.c\n> @@ -2533,8 +2533,8 @@ int btrfs_validate_super(const struct btrfs_fs_info *fs_info,\n>   \n>   \tif (unlikely(mirror_num >= 0 &&\n>   \t\t     btrfs_super_bytenr(sb) != btrfs_sb_offset(mirror_num))) {\n> -\t\tbtrfs_err(fs_info, \"super offset mismatch %llu != %u\",\n> -\t\t\t  btrfs_super_bytenr(sb), BTRFS_SUPER_INFO_OFFSET);\n> +\t\tbtrfs_err(fs_info, \"super offset mismatch %llu != %llu\",\n> +\t\t\t  btrfs_super_bytenr(sb), btrfs_sb_offset(mirror_num));\n>   \t\tret = -EINVAL;\n>   \t}\n>   \n\n",
              "reply_to": "",
              "message_date": "2026-02-18",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "David Sterba",
              "summary": "FYI I've edited the subjects of the error message fixing patches to be more specific what is being fixed now that there are a few of them.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 17, 2026 at 06:53:19PM +0000, Mark Harmstone wrote:\n> Fix the superblock offset mismatch error message in\n> btrfs_validate_super(): we changed it so that it considers all the\n> superblocks, but the message still assumes we're only looking at the\n> first one.\n> \n> The change from %u to %llu is because we're changing from a constant to\n> a u64.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 069ec957c35e (\"btrfs: Refactor btrfs_check_super_valid\")\n\nFYI I've edited the subjects of the error message fixing patches to be\nmore specific what is being fixed now that there are a few of them.\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Nhat Pham",
      "primary_email": "nphamcs@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH RFC 00/15] mm, swap: swap table phase IV with dynamic ghost swapfile",
          "message_id": "CAKEwX=NjRGxjQuvAnRoom=Ac_YptspMk1pwoq-2on46f1meuyw@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAKEwX=NjRGxjQuvAnRoom=Ac_YptspMk1pwoq-2on46f1meuyw@mail.gmail.com/",
          "date": "2026-02-24T21:57:05Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Rik van Riel",
      "primary_email": "riel@surriel.com",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 3/5] mm: add a batched helper to clear the young flag for large folios",
          "message_id": "58e1883fe084d8284dac68dcd570f5a6c56c0abc.camel@surriel.com",
          "url": "https://lore.kernel.org/all/58e1883fe084d8284dac68dcd570f5a6c56c0abc.camel@surriel.com/",
          "date": "2026-02-24T22:05:10Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Baolin Wang",
              "summary": "It seems that people have different preferences regarding loop patterns. The current code keeps this loop pattern consistent with other similar functions in the same file, as previously suggested by David [1]. [1] https://lore.kernel.org/all/3d5cb9a4-6604-4302-a110-3d8ff91baa56@kernel.org/",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "\n\nOn 2/25/26 6:03 AM, Rik van Riel wrote:\n> On Tue, 2026-02-24 at 09:56 +0800, Baolin Wang wrote:\n>>\n>> +static inline int test_and_clear_young_ptes(struct vm_area_struct\n>> *vma,\n>> +\t\t\t\t\t unsigned long addr,\n>> pte_t *ptep,\n>> +\t\t\t\t\t unsigned int nr)\n>> +{\n>> +\tint young = 0;\n>> +\n>> +\tfor (;;) {\n>> +\t\tyoung |= ptep_test_and_clear_young(vma, addr, ptep);\n>> +\t\tif (--nr == 0)\n>> +\t\t\tbreak;\n>> +\t\tptep++;\n>> +\t\taddr += PAGE_SIZE;\n>> +\t}\n> \n> This may be a nitpick, but could the --nr thing be\n> stuck into the loop conditional?\n> \n> Something that looks like an infinite loop just\n> seems wrong for something so bounded.\n\nIt seems that people have different preferences regarding loop patterns.\n\nThe current code keeps this loop pattern consistent with other similar \nfunctions in the same file, as previously suggested by David [1].\n\n[1] \nhttps://lore.kernel.org/all/3d5cb9a4-6604-4302-a110-3d8ff91baa56@kernel.org/\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 2/5] mm: rmap: add a ZONE_DEVICE folio warning in folio_referenced()",
          "message_id": "b3c1c739c233ccb32945ccaffdaf25fd3f96dd59.camel@surriel.com",
          "url": "https://lore.kernel.org/all/b3c1c739c233ccb32945ccaffdaf25fd3f96dd59.camel@surriel.com/",
          "date": "2026-02-24T02:39:48Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Baolin Wang",
              "summary": "Yes, sounds reasonable. Will do in next version. Thanks for reviewing.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "\n\nOn 2/24/26 10:38 AM, Rik van Riel wrote:\n> On Tue, 2026-02-24 at 09:56 +0800, Baolin Wang wrote:\n>>\n>> diff --git a/mm/rmap.c b/mm/rmap.c\n>> index bff8f222004e..be785dfc9336 100644\n>> --- a/mm/rmap.c\n>> +++ b/mm/rmap.c\n>> @@ -1065,6 +1065,7 @@ int folio_referenced(struct folio *folio, int\n>> is_locked,\n>>  \t\t.invalid_vma = invalid_folio_referenced_vma,\n>>  \t};\n>>   \n>> +\tVM_WARN_ON_FOLIO(folio_is_zone_device(folio), folio);\n>>  \t*vm_flags = 0;\n>>  \tif (!pra.mapcount)\n>>  \t\treturn 0;\n> \n> Should be a VM_WARN_ON_ONCE_FOLIO so we do not cause\n> a softlockup if we try to print information about a\n> million ZONE_DEVICE pages?\n\nYes, sounds reasonable. Will do in next version. Thanks for reviewing.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "David (Arm)",
              "summary": "Feel free to add my",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On 2/24/26 06:49, Baolin Wang wrote:\n> \n> \n> On 2/24/26 10:38 AM, Rik van Riel wrote:\n>> On Tue, 2026-02-24 at 09:56 +0800, Baolin Wang wrote:\n>>>\n>>> diff --git a/mm/rmap.c b/mm/rmap.c\n>>> index bff8f222004e..be785dfc9336 100644\n>>> --- a/mm/rmap.c\n>>> +++ b/mm/rmap.c\n>>> @@ -1065,6 +1065,7 @@ int folio_referenced(struct folio *folio, int\n>>> is_locked,\n>>>  .invalid_vma = invalid_folio_referenced_vma,\n>>>  };\n>>>  + VM_WARN_ON_FOLIO(folio_is_zone_device(folio), folio);\n>>>  *vm_flags = 0;\n>>>  if (!pra.mapcount)\n>>>  return 0;\n>>\n>> Should be a VM_WARN_ON_ONCE_FOLIO so we do not cause\n>> a softlockup if we try to print information about a\n>> million ZONE_DEVICE pages?\n> \n> Yes, sounds reasonable. Will do in next version. Thanks for reviewing.\n\nFeel free to add my\n\nAcked-by: David Hildenbrand (Arm) <david@kernel.org>\n\nto that.\n\n-- \nCheers,\n\nDavid\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 4/5] mm: support batched checking of the young flag for MGLRU",
          "message_id": "5957cdb584cad9007a58f43fb5a1c3b737fb0159.camel@surriel.com",
          "url": "https://lore.kernel.org/all/5957cdb584cad9007a58f43fb5a1c3b737fb0159.camel@surriel.com/",
          "date": "2026-02-24T22:13:38Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 1/5] mm: use inline helper functions instead of ugly macros",
          "message_id": "01f4ffab0da9e4326a78f8b6eedce23dfb115e7a.camel@surriel.com",
          "url": "https://lore.kernel.org/all/01f4ffab0da9e4326a78f8b6eedce23dfb115e7a.camel@surriel.com/",
          "date": "2026-02-24T02:42:53Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Shakeel Butt",
      "primary_email": "shakeel.butt@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "aZ3EE4JVDghZSq59@linux.dev",
          "url": "https://lore.kernel.org/all/aZ3EE4JVDghZSq59@linux.dev/",
          "date": "2026-02-24T15:31:19Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] mm: allow __GFP_RETRY_MAYFAIL in vmalloc",
          "message_id": "aZ2zPzyoFUUNWdJ7@linux.dev",
          "url": "https://lore.kernel.org/all/aZ2zPzyoFUUNWdJ7@linux.dev/",
          "date": "2026-02-24T14:22:36Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Christoph Hellwig",
              "summary": "That is a good point.  Compared to other options documenting that it might would still seem like the best option after just not using __GFP_RETRY_MAYFAIL with vmalloc.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 06:22:15AM -0800, Shakeel Butt wrote:\n> > Why bother?  __GFP_RETRY_MAYFAIL has pretty lose semantics.  Trying\n> > too hard to allocate PTEs is not breaking the overall concept.\n> > \n> \n> One thing __GFP_RETRY_MAYFAIL is very clear about is to not trigger the\n> oom-killer which is not the case for GFP_KERNEL. There are users who explicitly\n> use __GFP_RETRY_MAYFAIL to avoid oom-killer.\n\nThat is a good point.  Compared to other options documenting that it\nmight would still seem like the best option after just not using\n__GFP_RETRY_MAYFAIL with vmalloc.\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            },
            {
              "author": "Michal Hocko",
              "summary": "yes https://lore.kernel.org/all/32bd9bed-a939-69c4-696d-f7f9a5fe31d8@redhat.com/T/#u",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue 24-02-26 06:22:15, Shakeel Butt wrote:\n> On Tue, Feb 24, 2026 at 06:03:13AM -0800, Christoph Hellwig wrote:\n> > On Tue, Feb 24, 2026 at 01:22:36PM +0100, Michal Hocko wrote:\n> > > One thing that we could do to improve __GFP_RETRY_MAYFAIL resp.\n> > > __GFP_NORETRY is to use NOWAIT allocation semantic for page table\n> > > allocations as those could be achieved by scoped allocation context.\n> > > This could cause pre-mature failure after the whole bunch of memory has\n> > > already been allocated for the backing pages but considering that page\n> > > table allocations should be more and more rare over system runtime it\n> > > might be just a reasonable workaround. WDYT?\n> > \n> > Why bother?  __GFP_RETRY_MAYFAIL has pretty lose semantics.  Trying\n> > too hard to allocate PTEs is not breaking the overall concept.\n> > \n> \n> One thing __GFP_RETRY_MAYFAIL is very clear about is to not trigger the\n> oom-killer which is not the case for GFP_KERNEL. There are users who explicitly\n> use __GFP_RETRY_MAYFAIL to avoid oom-killer.\n> \n> Mikulas, is that the reason you are using __GFP_RETRY_MAYFAIL in your use-case?\n\nyes https://lore.kernel.org/all/32bd9bed-a939-69c4-696d-f7f9a5fe31d8@redhat.com/T/#u\n-- \nMichal Hocko\nSUSE Labs\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH RFC 06/15] memcg, swap: reparent the swap entry on swapin if swapout cgroup is dead",
          "message_id": "aZ0oXHNMe7_3P9OT@linux.dev",
          "url": "https://lore.kernel.org/all/aZ0oXHNMe7_3P9OT@linux.dev/",
          "date": "2026-02-24T05:44:16Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": [
            {
              "author": "Kairui Song",
              "summary": "It's an issue for this series, if we want to track the folio->swap using folio->memcg to avoid an external array to record folio->swap's memcgid. It's the same. If the memcg is dead and a swap entry's memcgid record points to the dead memcg, then whoever reads this swap entry recharges the swapin folio.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "On Tue, Feb 24, 2026 at 1:44 PM Shakeel Butt <shakeel.butt@linux.dev> wrote:\n>\n> On Fri, Feb 20, 2026 at 07:42:07AM +0800, Kairui Song via B4 Relay wrote:\n> > From: Kairui Song <kasong@tencent.com>\n> >\n> > As a result this will always charge the swapin folio into the dead\n> > cgroup's parent cgroup, and ensure folio->swap belongs to folio_memcg.\n>\n> I directly jump to this patch and the opening statement is confusing. Please\n> make the commit message self contained.\n>\n> > This only affects some uncommon behavior if we move the process between\n> > memcg.\n> >\n> > When a process that previously swapped some memory is moved to another\n> > cgroup, and the cgroup where the swap occurred is dead, folios for\n> > swap in of old swap entries will be charged into the new cgroup.\n> > Combined with the lazy freeing of swap cache, this leads to a strange\n> > situation where the folio->swap entry belongs to a cgroup that is not\n> > folio->memcg.\n>\n> Why is this an issue (i.e. folio->swap's cgroup different from\n> folio->memcg)?\n\nIt's an issue for this series, if we want to track the folio->swap\nusing folio->memcg to avoid an external array to record folio->swap's\nmemcgid.\n\n>\n> >\n> > Swapin from dead zombie memcg might be rare in practise, cgroups are\n> > offlined only after the workload in it is gone, which requires zapping\n> > the page table first, and releases all swap entries. Shmem is\n> > a bit different, but shmem always has swap count == 1, and force\n> > releases the swap cache. So, for shmem charging into the new memcg and\n> > release entry does look more sensible.\n>\n> Is this behavior same for all types of memory backed by shmem (i.e. MAP_SHARED,\n> memfd etc)? What about cow anon memory shared between parent and child\n> processes?\n\nIt's the same. If the memcg is dead and a swap entry's memcgid record\npoints to the dead memcg, then whoever reads this swap entry recharges\nthe swapin folio.\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "heuristic"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "aZ3EQRZ1XRLsGlzX@linux.dev",
          "url": "https://lore.kernel.org/all/aZ3EQRZ1XRLsGlzX@linux.dev/",
          "date": "2026-02-24T15:32:03Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "heuristic",
          "review_comments": []
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Usama Arif",
      "primary_email": "usama.arif@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    }
  ]
}