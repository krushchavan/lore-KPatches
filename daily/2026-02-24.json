{
  "date": "2026-02-24",
  "report_file": "2026-02-24_ollama_llama3.1-8b.html",
  "status": "in_progress",
  "last_updated": "2026-02-25 23:22 UTC",
  "llm_backends": [
    [
      "ollama",
      "llama3.1:8b"
    ]
  ],
  "generation_time_seconds": 0.0,
  "developer_reports": [
    {
      "name": "Alexandre Ghiti",
      "primary_email": "alexghiti@rivosinc.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Boris Burkov",
      "primary_email": "boris@bur.io",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 1/1] btrfs: set BTRFS_ROOT_ORPHAN_CLEANUP during subvol create",
          "message_id": "14fc2404e55d99e9d3a4f95e3e825678dc2422a0.1771971643.git.boris@bur.io",
          "url": "https://lore.kernel.org/all/14fc2404e55d99e9d3a4f95e3e825678dc2422a0.1771971643.git.boris@bur.io/",
          "date": "2026-02-24T22:25:39Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch addresses a bug in Btrfs where subvolumes with broken dentries cause issues when deleting or creating new files/subvolumes. The problem arises from the failure of btrfs_orphan_cleanup() to set BTRFS_ROOT_ORPHAN_CLEANUP, leading to negative dentry creation and subsequent errors. The fix involves setting this flag during subvolume creation.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "Identified a potential issue with concurrent orphan cleanup and deletion of inodes. Suggested adding a lock to protect against this scenario.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concurrent deletion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Dmitry Ilvokhin",
      "primary_email": "d@ilvokhin.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 4/4] mm: add tracepoints for zone lock",
          "message_id": "1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/1d2a7778aeee03abf8a11528ce8d4926ca78e9b4.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations in the Linux kernel, allowing for future instrumentation or debugging hooks to be added without modifying individual call sites. The wrappers are not yet used but prepare the code for subsequent patches. This change is intended to centralize zone lock operations behind a common interface, making it easier to add functionality in the future.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing reviewer feedback about direct zone lock acquire/release operations not being replaced with the newly introduced wrappers, and has confirmed that this change will be made in the next patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "next patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing concerns about the lack of visibility into zone lock contention and its impact on performance, particularly in memory-intensive workloads. They explain that existing instrumentation does not provide sufficient information to diagnose issues and propose adding dedicated tracepoint instrumentation to the zone lock, following a similar model to mmap_lock tracing. The author also mentions minor restructuring required for compaction changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging technical concerns",
                "proposing additional instrumentation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the zone lock wrappers interfering with compact_lock_irqsave() by introducing a new struct compact_lock to abstract the underlying lock type, which will allow compact_lock_irqsave() to operate correctly on both zone locks and raw spinlocks. The author confirmed that no functional change is intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock wrappers, explaining that they followed the mmap_lock pattern and ensured the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to introduce zone lock wrappers and tracepoints together, before mechanically converting users to the wrappers, to improve understanding of the changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason\n\nreviewer pointed out that the zone_lock_irqsave() macro should not return a value and suggested replacing it with an if-else statement\n\nReviewer Cheatham suggested moving zone lock wrapper changes, which are not yet used, to a later patch where they fit better with other similar changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested_reorder"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to introduce zone lock wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2) together instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "nit"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged a suggestion from reviewer Cheatham about reordering patches in the series, explained that they intentionally structured the series to keep refactoring and instrumentation changes separate, and stated their preference for maintaining the current order.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged suggestion",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the patch's priority should be reassessed in favor of improving the reading order of the series, but ultimately accepted the current implementation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK",
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author addressed Shakeel Butt's concern about using macros for zone lock wrappers, explaining that it's necessary to modify the flags variable passed by the caller and maintain consistency with existing locking patterns.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "explained reasoning",
                "acknowledged feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged that the zone lock wrappers are not valuable and agreed to remove them.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "will remove"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, I agree, there is no much value in this wrappers, will remove them,\nthanks!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 3/4] mm: convert compaction to zone lock wrappers",
          "message_id": "3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations in the Linux kernel, allowing for future instrumentation or debugging hooks to be added without modifying individual call sites. The wrappers are not yet used but prepare the code for subsequent patches. This change is intended to centralize zone lock operations behind a common interface, making it easier to add functionality in the future.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing reviewer feedback about direct zone lock acquire/release operations not being replaced with the newly introduced wrappers, and has confirmed that this change will be made in the next patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "next patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing concerns about the lack of visibility into zone lock contention and its impact on performance, particularly in memory-intensive workloads. They explain that existing instrumentation does not provide sufficient information to diagnose issues and propose adding dedicated tracepoint instrumentation to the zone lock, following a similar model to mmap_lock tracing. The author also mentions minor restructuring required for compaction changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging technical concerns",
                "proposing additional instrumentation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the zone lock wrappers interfering with compact_lock_irqsave() by introducing a new struct compact_lock to abstract the underlying lock type, which will allow compact_lock_irqsave() to operate correctly on both zone locks and raw spinlocks. The author confirmed that no functional change is intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock wrappers, explaining that they followed the mmap_lock pattern and ensured the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to introduce zone lock wrappers and tracepoints together, before mechanically converting users to the wrappers, to improve understanding of the changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason\n\nreviewer pointed out that the zone_lock_irqsave() macro should not return a value and suggested replacing it with an if-else statement\n\nReviewer Cheatham suggested moving zone lock wrapper changes, which are not yet used, to a later patch where they fit better with other similar changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested_reorder"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to introduce zone lock wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2) together instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "nit"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged a suggestion from reviewer Cheatham about reordering patches in the series, explained that they intentionally structured the series to keep refactoring and instrumentation changes separate, and stated their preference for maintaining the current order.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged suggestion",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the patch's priority should be reassessed in favor of improving the reading order of the series, but ultimately accepted the current implementation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK",
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author addressed Shakeel Butt's concern about using macros for zone lock wrappers, explaining that it's necessary to modify the flags variable passed by the caller and maintain consistency with existing locking patterns.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "explained reasoning",
                "acknowledged feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged that the zone lock wrappers are not valuable and agreed to remove them.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "will remove"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, I agree, there is no much value in this wrappers, will remove them,\nthanks!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/4] mm: zone lock tracepoint instrumentation",
          "message_id": "cover.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/cover.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations in the Linux kernel, allowing for future instrumentation or debugging hooks to be added without modifying individual call sites. The wrappers are not yet used but prepare the code for subsequent patches. This change is intended to centralize zone lock operations behind a common interface, making it easier to add functionality in the future.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing reviewer feedback about direct zone lock acquire/release operations not being replaced with the newly introduced wrappers, and has confirmed that this change will be made in the next patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "next patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing concerns about the lack of visibility into zone lock contention and its impact on performance, particularly in memory-intensive workloads. They explain that existing instrumentation does not provide sufficient information to diagnose issues and propose adding dedicated tracepoint instrumentation to the zone lock, following a similar model to mmap_lock tracing. The author also mentions minor restructuring required for compaction changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging technical concerns",
                "proposing additional instrumentation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the zone lock wrappers interfering with compact_lock_irqsave() by introducing a new struct compact_lock to abstract the underlying lock type, which will allow compact_lock_irqsave() to operate correctly on both zone locks and raw spinlocks. The author confirmed that no functional change is intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock wrappers, explaining that they followed the mmap_lock pattern and ensured the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to introduce zone lock wrappers and tracepoints together, before mechanically converting users to the wrappers, to improve understanding of the changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason\n\nreviewer pointed out that the zone_lock_irqsave() macro should not return a value and suggested replacing it with an if-else statement\n\nReviewer Cheatham suggested moving zone lock wrapper changes, which are not yet used, to a later patch where they fit better with other similar changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested_reorder"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to introduce zone lock wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2) together instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "nit"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged a suggestion from reviewer Cheatham about reordering patches in the series, explained that they intentionally structured the series to keep refactoring and instrumentation changes separate, and stated their preference for maintaining the current order.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged suggestion",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the patch's priority should be reassessed in favor of improving the reading order of the series, but ultimately accepted the current implementation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK",
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:\n> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> > Zone lock contention can significantly impact allocation and\n> > reclaim latency, as it is a central synchronization point in\n> > the page allocator and reclaim paths. Improved visibility into\n> > its behavior is therefore important for diagnosing performance\n> > issues in memory-intensive workloads.\n> > \n> > On some production workloads at Meta, we have observed noticeable\n> > zone lock contention. Deeper analysis of lock holders and waiters\n> > is currently difficult with existing instrumentation.\n> > \n> > While generic lock contention_begin/contention_end tracepoints\n> > cover the slow path, they do not provide sufficient visibility\n> > into lock hold times. In particular, the lack of a release-side\n> > event makes it difficult to identify long lock holders and\n> > correlate them with waiters. As a result, distinguishing between\n> > short bursts of contention and pathological long hold times\n> > requires additional instrumentation.\n> > \n> > This patch series adds dedicated tracepoint instrumentation to\n> > zone lock, following the existing mmap_lock tracing model.\n> > \n> > The goal is to enable detailed holder/waiter analysis and lock\n> > hold time measurements without affecting the fast path when\n> > tracing is disabled.\n> > \n> > The series is structured as follows:\n> > \n> >   1. Introduce zone lock wrappers.\n> >   2. Mechanically convert zone lock users to the wrappers.\n> >   3. Convert compaction to use the wrappers (requires minor\n> >      restructuring of compact_lock_irqsave()).\n> >   4. Add zone lock tracepoints.\n> \n> I think you can improve the flow of this series if reorder as follows:\n> \t1. Introduce zone lock wrappers\n> \t4. Add zone lock tracepoints\n> \t2. Mechanically convert zone lock users to the wrappers\n> \t3. Convert compaction to use the wrappers...\n> \n> and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\n> wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\n> patch 1 by the time they get to patch 4.\n\nI don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.\n\n",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author addressed Shakeel Butt's concern about using macros for zone lock wrappers, explaining that it's necessary to modify the flags variable passed by the caller and maintain consistency with existing locking patterns.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "explained reasoning",
                "acknowledged feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:\n> On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:\n> > Zone lock contention can significantly impact allocation and\n> > reclaim latency, as it is a central synchronization point in\n> > the page allocator and reclaim paths. Improved visibility into\n> > its behavior is therefore important for diagnosing performance\n> > issues in memory-intensive workloads.\n> > \n> > On some production workloads at Meta, we have observed noticeable\n> > zone lock contention. Deeper analysis of lock holders and waiters\n> > is currently difficult with existing instrumentation.\n> > \n> > While generic lock contention_begin/contention_end tracepoints\n> > cover the slow path, they do not provide sufficient visibility\n> > into lock hold times. In particular, the lack of a release-side\n> > event makes it difficult to identify long lock holders and\n> > correlate them with waiters. As a result, distinguishing between\n> > short bursts of contention and pathological long hold times\n> > requires additional instrumentation.\n> > \n> > This patch series adds dedicated tracepoint instrumentation to\n> > zone lock, following the existing mmap_lock tracing model.\n> > \n> > The goal is to enable detailed holder/waiter analysis and lock\n> > hold time measurements without affecting the fast path when\n> > tracing is disabled.\n> > \n> > The series is structured as follows:\n> > \n> >   1. Introduce zone lock wrappers.\n> >   2. Mechanically convert zone lock users to the wrappers.\n> >   3. Convert compaction to use the wrappers (requires minor\n> >      restructuring of compact_lock_irqsave()).\n> >   4. Add zone lock tracepoints.\n> \n> I think you can improve the flow of this series if reorder as follows:\n> \t1. Introduce zone lock wrappers\n> \t4. Add zone lock tracepoints\n> \t2. Mechanically convert zone lock users to the wrappers\n> \t3. Convert compaction to use the wrappers...\n> \n> and possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\n> wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\n> patch 1 by the time they get to patch 4.\n\nI don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.\n\n",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged that the zone lock wrappers are not valuable and agreed to remove them.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "will remove"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, I agree, there is no much value in this wrappers, will remove them,\nthanks!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 2/4] mm: convert zone lock users to wrappers",
          "message_id": "7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/7d1ee95201a8870445556e61e47161f46ade8b3b.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations in the Linux kernel, allowing for future instrumentation or debugging hooks to be added without modifying individual call sites. The wrappers are not yet used but prepare the code for subsequent patches. This change is intended to centralize zone lock operations behind a common interface, making it easier to add functionality in the future.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing reviewer feedback about direct zone lock acquire/release operations not being replaced with the newly introduced wrappers, and has confirmed that this change will be made in the next patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "next patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing concerns about the lack of visibility into zone lock contention and its impact on performance, particularly in memory-intensive workloads. They explain that existing instrumentation does not provide sufficient information to diagnose issues and propose adding dedicated tracepoint instrumentation to the zone lock, following a similar model to mmap_lock tracing. The author also mentions minor restructuring required for compaction changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging technical concerns",
                "proposing additional instrumentation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the zone lock wrappers interfering with compact_lock_irqsave() by introducing a new struct compact_lock to abstract the underlying lock type, which will allow compact_lock_irqsave() to operate correctly on both zone locks and raw spinlocks. The author confirmed that no functional change is intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock wrappers, explaining that they followed the mmap_lock pattern and ensured the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to introduce zone lock wrappers and tracepoints together, before mechanically converting users to the wrappers, to improve understanding of the changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason\n\nreviewer pointed out that the zone_lock_irqsave() macro should not return a value and suggested replacing it with an if-else statement\n\nReviewer Cheatham suggested moving zone lock wrapper changes, which are not yet used, to a later patch where they fit better with other similar changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested_reorder"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to introduce zone lock wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2) together instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "nit"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged a suggestion from reviewer Cheatham about reordering patches in the series, explained that they intentionally structured the series to keep refactoring and instrumentation changes separate, and stated their preference for maintaining the current order.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged suggestion",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the patch's priority should be reassessed in favor of improving the reading order of the series, but ultimately accepted the current implementation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK",
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Wed, Feb 11, 2026 at 03:22:14PM +0000, Dmitry Ilvokhin wrote:\n> Replace direct zone lock acquire/release operations with the\n> newly introduced wrappers.\n> \n> The changes are purely mechanical substitutions. No functional change\n> intended. Locking semantics and ordering remain unchanged.\n> \n> The compaction path is left unchanged for now and will be\n> handled separately in the following patch due to additional\n> non-trivial modifications.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author addressed Shakeel Butt's concern about using macros for zone lock wrappers, explaining that it's necessary to modify the flags variable passed by the caller and maintain consistency with existing locking patterns.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "explained reasoning",
                "acknowledged feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Wed, Feb 11, 2026 at 03:22:14PM +0000, Dmitry Ilvokhin wrote:\n> Replace direct zone lock acquire/release operations with the\n> newly introduced wrappers.\n> \n> The changes are purely mechanical substitutions. No functional change\n> intended. Locking semantics and ordering remain unchanged.\n> \n> The compaction path is left unchanged for now and will be\n> handled separately in the following patch due to additional\n> non-trivial modifications.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged that the zone lock wrappers are not valuable and agreed to remove them.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "will remove"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, I agree, there is no much value in this wrappers, will remove them,\nthanks!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3826dd6dc55a9c5721ec3de85f019764a6cf3222.1770821420.git.d@ilvokhin.com/",
          "date": "2026-02-11T15:23:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-11",
          "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations in the Linux kernel, allowing for future instrumentation or debugging hooks to be added without modifying individual call sites. The wrappers are not yet used but prepare the code for subsequent patches. This change is intended to centralize zone lock operations behind a common interface, making it easier to add functionality in the future.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing reviewer feedback about direct zone lock acquire/release operations not being replaced with the newly introduced wrappers, and has confirmed that this change will be made in the next patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "next patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing concerns about the lack of visibility into zone lock contention and its impact on performance, particularly in memory-intensive workloads. They explain that existing instrumentation does not provide sufficient information to diagnose issues and propose adding dedicated tracepoint instrumentation to the zone lock, following a similar model to mmap_lock tracing. The author also mentions minor restructuring required for compaction changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging technical concerns",
                "proposing additional instrumentation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the zone lock wrappers interfering with compact_lock_irqsave() by introducing a new struct compact_lock to abstract the underlying lock type, which will allow compact_lock_irqsave() to operate correctly on both zone locks and raw spinlocks. The author confirmed that no functional change is intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock wrappers, explaining that they followed the mmap_lock pattern and ensured the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to introduce zone lock wrappers and tracepoints together, before mechanically converting users to the wrappers, to improve understanding of the changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason\n\nreviewer pointed out that the zone_lock_irqsave() macro should not return a value and suggested replacing it with an if-else statement\n\nReviewer Cheatham suggested moving zone lock wrapper changes, which are not yet used, to a later patch where they fit better with other similar changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested_reorder"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to introduce zone lock wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2) together instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "nit"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged a suggestion from reviewer Cheatham about reordering patches in the series, explained that they intentionally structured the series to keep refactoring and instrumentation changes separate, and stated their preference for maintaining the current order.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged suggestion",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the patch's priority should be reassessed in favor of improving the reading order of the series, but ultimately accepted the current implementation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK",
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> Add thin wrappers around zone lock acquire/release operations. This\n> prepares the code for future tracepoint instrumentation without\n> modifying individual call sites.\n> \n> Centralizing zone lock operations behind wrappers allows future\n> instrumentation or debugging hooks to be added without touching\n> all users.\n> \n> No functional change intended. The wrappers are introduced in\n> preparation for subsequent patches and are not yet used.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> ---\n>  MAINTAINERS               |  1 +\n>  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n>  2 files changed, 39 insertions(+)\n>  create mode 100644 include/linux/zone_lock.h\n> \n> diff --git a/MAINTAINERS b/MAINTAINERS\n> index b4088f7290be..680c9ae02d7e 100644\n> --- a/MAINTAINERS\n> +++ b/MAINTAINERS\n> @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n>  F:\tinclude/linux/ptdump.h\n>  F:\tinclude/linux/vmpressure.h\n>  F:\tinclude/linux/vmstat.h\n> +F:\tinclude/linux/zone_lock.h\n>  F:\tkernel/fork.c\n>  F:\tmm/Kconfig\n>  F:\tmm/debug.c\n> diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> new file mode 100644\n> index 000000000000..c531e26280e6\n> --- /dev/null\n> +++ b/include/linux/zone_lock.h\n> @@ -0,0 +1,38 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +#ifndef _LINUX_ZONE_LOCK_H\n> +#define _LINUX_ZONE_LOCK_H\n> +\n> +#include <linux/mmzone.h>\n> +#include <linux/spinlock.h>\n> +\n> +static inline void zone_lock_init(struct zone *zone)\n> +{\n> +\tspin_lock_init(&zone->lock);\n> +}\n> +\n> +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> +do {\t\t\t\t\t\t\t\t\\\n> +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> +} while (0)\n> +\n> +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> +({\t\t\t\t\t\t\t\t\\\n> +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> +})\n\nAny reason you used macros for above two and inlined functions for remaining?\n\n> +\n> +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n> +{\n> +\tspin_unlock_irqrestore(&zone->lock, flags);\n> +}\n> +\n> +static inline void zone_lock_irq(struct zone *zone)\n> +{\n> +\tspin_lock_irq(&zone->lock);\n> +}\n> +\n> +static inline void zone_unlock_irq(struct zone *zone)\n> +{\n> +\tspin_unlock_irq(&zone->lock);\n> +}\n> +\n> +#endif /* _LINUX_ZONE_LOCK_H */\n> -- \n> 2.47.3\n> \n\n\n---\n\nOn Tue, Feb 24, 2026 at 03:18:04PM +0000, Dmitry Ilvokhin wrote:\n> On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> > On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > > Add thin wrappers around zone lock acquire/release operations. This\n> > > prepares the code for future tracepoint instrumentation without\n> > > modifying individual call sites.\n> > > \n> > > Centralizing zone lock operations behind wrappers allows future\n> > > instrumentation or debugging hooks to be added without touching\n> > > all users.\n> > > \n> > > No functional change intended. The wrappers are introduced in\n> > > preparation for subsequent patches and are not yet used.\n> > > \n> > > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > > ---\n> > >  MAINTAINERS               |  1 +\n> > >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> > >  2 files changed, 39 insertions(+)\n> > >  create mode 100644 include/linux/zone_lock.h\n> > > \n> > > diff --git a/MAINTAINERS b/MAINTAINERS\n> > > index b4088f7290be..680c9ae02d7e 100644\n> > > --- a/MAINTAINERS\n> > > +++ b/MAINTAINERS\n> > > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> > >  F:\tinclude/linux/ptdump.h\n> > >  F:\tinclude/linux/vmpressure.h\n> > >  F:\tinclude/linux/vmstat.h\n> > > +F:\tinclude/linux/zone_lock.h\n> > >  F:\tkernel/fork.c\n> > >  F:\tmm/Kconfig\n> > >  F:\tmm/debug.c\n> > > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > > new file mode 100644\n> > > index 000000000000..c531e26280e6\n> > > --- /dev/null\n> > > +++ b/include/linux/zone_lock.h\n> > > @@ -0,0 +1,38 @@\n> > > +/* SPDX-License-Identifier: GPL-2.0 */\n> > > +#ifndef _LINUX_ZONE_LOCK_H\n> > > +#define _LINUX_ZONE_LOCK_H\n> > > +\n> > > +#include <linux/mmzone.h>\n> > > +#include <linux/spinlock.h>\n> > > +\n> > > +static inline void zone_lock_init(struct zone *zone)\n> > > +{\n> > > +\tspin_lock_init(&zone->lock);\n> > > +}\n> > > +\n> > > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > > +do {\t\t\t\t\t\t\t\t\\\n> > > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +} while (0)\n> > > +\n> > > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > > +({\t\t\t\t\t\t\t\t\\\n> > > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +})\n> > \n> > Any reason you used macros for above two and inlined functions for remaining?\n> >\n> \n> The reason for using macros in those two cases is that they need to\n> modify the flags variable passed by the caller, just like\n> spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\n> convention here.\n> \n> If we used normal inline functions instead, we would need to pass a\n> pointer to flags, which would change the call sites and diverge from the\n> existing *_irqsave() locking pattern.\n> \n> There is also a difference between zone_lock_irqsave() and\n> zone_trylock_irqsave() implementations: the former is implemented as a\n> do { } while (0) macro since it does not return a value, while the\n> latter uses a GCC extension in order to return the trylock result. This\n> matches spin_lock_* convention as well.\n> \n\nCool, thanks for the explanation.\n\n\n---\n\nOn Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> Add thin wrappers around zone lock acquire/release operations. This\n> prepares the code for future tracepoint instrumentation without\n> modifying individual call sites.\n> \n> Centralizing zone lock operations behind wrappers allows future\n> instrumentation or debugging hooks to be added without touching\n> all users.\n> \n> No functional change intended. The wrappers are introduced in\n> preparation for subsequent patches and are not yet used.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author addressed Shakeel Butt's concern about using macros for zone lock wrappers, explaining that it's necessary to modify the flags variable passed by the caller and maintain consistency with existing locking patterns.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "explained reasoning",
                "acknowledged feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> Add thin wrappers around zone lock acquire/release operations. This\n> prepares the code for future tracepoint instrumentation without\n> modifying individual call sites.\n> \n> Centralizing zone lock operations behind wrappers allows future\n> instrumentation or debugging hooks to be added without touching\n> all users.\n> \n> No functional change intended. The wrappers are introduced in\n> preparation for subsequent patches and are not yet used.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> ---\n>  MAINTAINERS               |  1 +\n>  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n>  2 files changed, 39 insertions(+)\n>  create mode 100644 include/linux/zone_lock.h\n> \n> diff --git a/MAINTAINERS b/MAINTAINERS\n> index b4088f7290be..680c9ae02d7e 100644\n> --- a/MAINTAINERS\n> +++ b/MAINTAINERS\n> @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n>  F:\tinclude/linux/ptdump.h\n>  F:\tinclude/linux/vmpressure.h\n>  F:\tinclude/linux/vmstat.h\n> +F:\tinclude/linux/zone_lock.h\n>  F:\tkernel/fork.c\n>  F:\tmm/Kconfig\n>  F:\tmm/debug.c\n> diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> new file mode 100644\n> index 000000000000..c531e26280e6\n> --- /dev/null\n> +++ b/include/linux/zone_lock.h\n> @@ -0,0 +1,38 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +#ifndef _LINUX_ZONE_LOCK_H\n> +#define _LINUX_ZONE_LOCK_H\n> +\n> +#include <linux/mmzone.h>\n> +#include <linux/spinlock.h>\n> +\n> +static inline void zone_lock_init(struct zone *zone)\n> +{\n> +\tspin_lock_init(&zone->lock);\n> +}\n> +\n> +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> +do {\t\t\t\t\t\t\t\t\\\n> +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> +} while (0)\n> +\n> +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> +({\t\t\t\t\t\t\t\t\\\n> +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> +})\n\nAny reason you used macros for above two and inlined functions for remaining?\n\n> +\n> +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n> +{\n> +\tspin_unlock_irqrestore(&zone->lock, flags);\n> +}\n> +\n> +static inline void zone_lock_irq(struct zone *zone)\n> +{\n> +\tspin_lock_irq(&zone->lock);\n> +}\n> +\n> +static inline void zone_unlock_irq(struct zone *zone)\n> +{\n> +\tspin_unlock_irq(&zone->lock);\n> +}\n> +\n> +#endif /* _LINUX_ZONE_LOCK_H */\n> -- \n> 2.47.3\n> \n\n\n---\n\nOn Tue, Feb 24, 2026 at 03:18:04PM +0000, Dmitry Ilvokhin wrote:\n> On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> > On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > > Add thin wrappers around zone lock acquire/release operations. This\n> > > prepares the code for future tracepoint instrumentation without\n> > > modifying individual call sites.\n> > > \n> > > Centralizing zone lock operations behind wrappers allows future\n> > > instrumentation or debugging hooks to be added without touching\n> > > all users.\n> > > \n> > > No functional change intended. The wrappers are introduced in\n> > > preparation for subsequent patches and are not yet used.\n> > > \n> > > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > > ---\n> > >  MAINTAINERS               |  1 +\n> > >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> > >  2 files changed, 39 insertions(+)\n> > >  create mode 100644 include/linux/zone_lock.h\n> > > \n> > > diff --git a/MAINTAINERS b/MAINTAINERS\n> > > index b4088f7290be..680c9ae02d7e 100644\n> > > --- a/MAINTAINERS\n> > > +++ b/MAINTAINERS\n> > > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> > >  F:\tinclude/linux/ptdump.h\n> > >  F:\tinclude/linux/vmpressure.h\n> > >  F:\tinclude/linux/vmstat.h\n> > > +F:\tinclude/linux/zone_lock.h\n> > >  F:\tkernel/fork.c\n> > >  F:\tmm/Kconfig\n> > >  F:\tmm/debug.c\n> > > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > > new file mode 100644\n> > > index 000000000000..c531e26280e6\n> > > --- /dev/null\n> > > +++ b/include/linux/zone_lock.h\n> > > @@ -0,0 +1,38 @@\n> > > +/* SPDX-License-Identifier: GPL-2.0 */\n> > > +#ifndef _LINUX_ZONE_LOCK_H\n> > > +#define _LINUX_ZONE_LOCK_H\n> > > +\n> > > +#include <linux/mmzone.h>\n> > > +#include <linux/spinlock.h>\n> > > +\n> > > +static inline void zone_lock_init(struct zone *zone)\n> > > +{\n> > > +\tspin_lock_init(&zone->lock);\n> > > +}\n> > > +\n> > > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > > +do {\t\t\t\t\t\t\t\t\\\n> > > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +} while (0)\n> > > +\n> > > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > > +({\t\t\t\t\t\t\t\t\\\n> > > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +})\n> > \n> > Any reason you used macros for above two and inlined functions for remaining?\n> >\n> \n> The reason for using macros in those two cases is that they need to\n> modify the flags variable passed by the caller, just like\n> spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\n> convention here.\n> \n> If we used normal inline functions instead, we would need to pass a\n> pointer to flags, which would change the call sites and diverge from the\n> existing *_irqsave() locking pattern.\n> \n> There is also a difference between zone_lock_irqsave() and\n> zone_trylock_irqsave() implementations: the former is implemented as a\n> do { } while (0) macro since it does not return a value, while the\n> latter uses a GCC extension in order to return the trylock result. This\n> matches spin_lock_* convention as well.\n> \n\nCool, thanks for the explanation.\n\n\n---\n\nOn Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> Add thin wrappers around zone lock acquire/release operations. This\n> prepares the code for future tracepoint instrumentation without\n> modifying individual call sites.\n> \n> Centralizing zone lock operations behind wrappers allows future\n> instrumentation or debugging hooks to be added without touching\n> all users.\n> \n> No functional change intended. The wrappers are introduced in\n> preparation for subsequent patches and are not yet used.\n> \n> Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged that the zone lock wrappers are not valuable and agreed to remove them.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "will remove"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, I agree, there is no much value in this wrappers, will remove them,\nthanks!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 3/4] mm: convert compaction to zone lock wrappers",
          "message_id": "aZ3I0ADTAdCN6UmN@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aZ3I0ADTAdCN6UmN@shell.ilvokhin.com/",
          "date": "2026-02-24T15:50:43Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations in the Linux kernel, allowing for future instrumentation or debugging hooks to be added without modifying individual call sites. The wrappers are not yet used but prepare the code for subsequent patches. This change is intended to centralize zone lock operations behind a common interface, making it easier to add functionality in the future.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing reviewer feedback about direct zone lock acquire/release operations not being replaced with the newly introduced wrappers, and has confirmed that this change will be made in the next patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "next patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing concerns about the lack of visibility into zone lock contention and its impact on performance, particularly in memory-intensive workloads. They explain that existing instrumentation does not provide sufficient information to diagnose issues and propose adding dedicated tracepoint instrumentation to the zone lock, following a similar model to mmap_lock tracing. The author also mentions minor restructuring required for compaction changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging technical concerns",
                "proposing additional instrumentation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the zone lock wrappers interfering with compact_lock_irqsave() by introducing a new struct compact_lock to abstract the underlying lock type, which will allow compact_lock_irqsave() to operate correctly on both zone locks and raw spinlocks. The author confirmed that no functional change is intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock wrappers, explaining that they followed the mmap_lock pattern and ensured the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to introduce zone lock wrappers and tracepoints together, before mechanically converting users to the wrappers, to improve understanding of the changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason\n\nreviewer pointed out that the zone_lock_irqsave() macro should not return a value and suggested replacing it with an if-else statement\n\nReviewer Cheatham suggested moving zone lock wrapper changes, which are not yet used, to a later patch where they fit better with other similar changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested_reorder"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to introduce zone lock wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2) together instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "nit"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged a suggestion from reviewer Cheatham about reordering patches in the series, explained that they intentionally structured the series to keep refactoring and instrumentation changes separate, and stated their preference for maintaining the current order.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged suggestion",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the patch's priority should be reassessed in favor of improving the reading order of the series, but ultimately accepted the current implementation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK",
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author addressed Shakeel Butt's concern about using macros for zone lock wrappers, explaining that it's necessary to modify the flags variable passed by the caller and maintain consistency with existing locking patterns.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "explained reasoning",
                "acknowledged feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged that the zone lock wrappers are not valuable and agreed to remove them.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "will remove"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, I agree, there is no much value in this wrappers, will remove them,\nthanks!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "aZ3BLKzhIIZvkbwL@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aZ3BLKzhIIZvkbwL@shell.ilvokhin.com/",
          "date": "2026-02-24T15:18:08Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations in the Linux kernel, allowing for future instrumentation or debugging hooks to be added without modifying individual call sites. The wrappers are not yet used but prepare the code for subsequent patches. This change is intended to centralize zone lock operations behind a common interface, making it easier to add functionality in the future.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing reviewer feedback about direct zone lock acquire/release operations not being replaced with the newly introduced wrappers, and has confirmed that this change will be made in the next patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "next patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing concerns about the lack of visibility into zone lock contention and its impact on performance, particularly in memory-intensive workloads. They explain that existing instrumentation does not provide sufficient information to diagnose issues and propose adding dedicated tracepoint instrumentation to the zone lock, following a similar model to mmap_lock tracing. The author also mentions minor restructuring required for compaction changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging technical concerns",
                "proposing additional instrumentation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the zone lock wrappers interfering with compact_lock_irqsave() by introducing a new struct compact_lock to abstract the underlying lock type, which will allow compact_lock_irqsave() to operate correctly on both zone locks and raw spinlocks. The author confirmed that no functional change is intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock wrappers, explaining that they followed the mmap_lock pattern and ensured the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to introduce zone lock wrappers and tracepoints together, before mechanically converting users to the wrappers, to improve understanding of the changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason\n\nreviewer pointed out that the zone_lock_irqsave() macro should not return a value and suggested replacing it with an if-else statement\n\nReviewer Cheatham suggested moving zone lock wrapper changes, which are not yet used, to a later patch where they fit better with other similar changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested_reorder"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to introduce zone lock wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2) together instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "nit"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged a suggestion from reviewer Cheatham about reordering patches in the series, explained that they intentionally structured the series to keep refactoring and instrumentation changes separate, and stated their preference for maintaining the current order.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged suggestion",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the patch's priority should be reassessed in favor of improving the reading order of the series, but ultimately accepted the current implementation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK",
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Tue, Feb 24, 2026 at 03:18:04PM +0000, Dmitry Ilvokhin wrote:\n> On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> > On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > > Add thin wrappers around zone lock acquire/release operations. This\n> > > prepares the code for future tracepoint instrumentation without\n> > > modifying individual call sites.\n> > > \n> > > Centralizing zone lock operations behind wrappers allows future\n> > > instrumentation or debugging hooks to be added without touching\n> > > all users.\n> > > \n> > > No functional change intended. The wrappers are introduced in\n> > > preparation for subsequent patches and are not yet used.\n> > > \n> > > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > > ---\n> > >  MAINTAINERS               |  1 +\n> > >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> > >  2 files changed, 39 insertions(+)\n> > >  create mode 100644 include/linux/zone_lock.h\n> > > \n> > > diff --git a/MAINTAINERS b/MAINTAINERS\n> > > index b4088f7290be..680c9ae02d7e 100644\n> > > --- a/MAINTAINERS\n> > > +++ b/MAINTAINERS\n> > > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> > >  F:\tinclude/linux/ptdump.h\n> > >  F:\tinclude/linux/vmpressure.h\n> > >  F:\tinclude/linux/vmstat.h\n> > > +F:\tinclude/linux/zone_lock.h\n> > >  F:\tkernel/fork.c\n> > >  F:\tmm/Kconfig\n> > >  F:\tmm/debug.c\n> > > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > > new file mode 100644\n> > > index 000000000000..c531e26280e6\n> > > --- /dev/null\n> > > +++ b/include/linux/zone_lock.h\n> > > @@ -0,0 +1,38 @@\n> > > +/* SPDX-License-Identifier: GPL-2.0 */\n> > > +#ifndef _LINUX_ZONE_LOCK_H\n> > > +#define _LINUX_ZONE_LOCK_H\n> > > +\n> > > +#include <linux/mmzone.h>\n> > > +#include <linux/spinlock.h>\n> > > +\n> > > +static inline void zone_lock_init(struct zone *zone)\n> > > +{\n> > > +\tspin_lock_init(&zone->lock);\n> > > +}\n> > > +\n> > > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > > +do {\t\t\t\t\t\t\t\t\\\n> > > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +} while (0)\n> > > +\n> > > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > > +({\t\t\t\t\t\t\t\t\\\n> > > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +})\n> > \n> > Any reason you used macros for above two and inlined functions for remaining?\n> >\n> \n> The reason for using macros in those two cases is that they need to\n> modify the flags variable passed by the caller, just like\n> spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\n> convention here.\n> \n> If we used normal inline functions instead, we would need to pass a\n> pointer to flags, which would change the call sites and diverge from the\n> existing *_irqsave() locking pattern.\n> \n> There is also a difference between zone_lock_irqsave() and\n> zone_trylock_irqsave() implementations: the former is implemented as a\n> do { } while (0) macro since it does not return a value, while the\n> latter uses a GCC extension in order to return the trylock result. This\n> matches spin_lock_* convention as well.\n> \n\nCool, thanks for the explanation.\n",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author addressed Shakeel Butt's concern about using macros for zone lock wrappers, explaining that it's necessary to modify the flags variable passed by the caller and maintain consistency with existing locking patterns.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "explained reasoning",
                "acknowledged feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Tue, Feb 24, 2026 at 03:18:04PM +0000, Dmitry Ilvokhin wrote:\n> On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:\n> > On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:\n> > > Add thin wrappers around zone lock acquire/release operations. This\n> > > prepares the code for future tracepoint instrumentation without\n> > > modifying individual call sites.\n> > > \n> > > Centralizing zone lock operations behind wrappers allows future\n> > > instrumentation or debugging hooks to be added without touching\n> > > all users.\n> > > \n> > > No functional change intended. The wrappers are introduced in\n> > > preparation for subsequent patches and are not yet used.\n> > > \n> > > Signed-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n> > > ---\n> > >  MAINTAINERS               |  1 +\n> > >  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++\n> > >  2 files changed, 39 insertions(+)\n> > >  create mode 100644 include/linux/zone_lock.h\n> > > \n> > > diff --git a/MAINTAINERS b/MAINTAINERS\n> > > index b4088f7290be..680c9ae02d7e 100644\n> > > --- a/MAINTAINERS\n> > > +++ b/MAINTAINERS\n> > > @@ -16498,6 +16498,7 @@ F:\tinclude/linux/pgtable.h\n> > >  F:\tinclude/linux/ptdump.h\n> > >  F:\tinclude/linux/vmpressure.h\n> > >  F:\tinclude/linux/vmstat.h\n> > > +F:\tinclude/linux/zone_lock.h\n> > >  F:\tkernel/fork.c\n> > >  F:\tmm/Kconfig\n> > >  F:\tmm/debug.c\n> > > diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\n> > > new file mode 100644\n> > > index 000000000000..c531e26280e6\n> > > --- /dev/null\n> > > +++ b/include/linux/zone_lock.h\n> > > @@ -0,0 +1,38 @@\n> > > +/* SPDX-License-Identifier: GPL-2.0 */\n> > > +#ifndef _LINUX_ZONE_LOCK_H\n> > > +#define _LINUX_ZONE_LOCK_H\n> > > +\n> > > +#include <linux/mmzone.h>\n> > > +#include <linux/spinlock.h>\n> > > +\n> > > +static inline void zone_lock_init(struct zone *zone)\n> > > +{\n> > > +\tspin_lock_init(&zone->lock);\n> > > +}\n> > > +\n> > > +#define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n> > > +do {\t\t\t\t\t\t\t\t\\\n> > > +\tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +} while (0)\n> > > +\n> > > +#define zone_trylock_irqsave(zone, flags)\t\t\t\\\n> > > +({\t\t\t\t\t\t\t\t\\\n> > > +\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n> > > +})\n> > \n> > Any reason you used macros for above two and inlined functions for remaining?\n> >\n> \n> The reason for using macros in those two cases is that they need to\n> modify the flags variable passed by the caller, just like\n> spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\n> convention here.\n> \n> If we used normal inline functions instead, we would need to pass a\n> pointer to flags, which would change the call sites and diverge from the\n> existing *_irqsave() locking pattern.\n> \n> There is also a difference between zone_lock_irqsave() and\n> zone_trylock_irqsave() implementations: the former is implemented as a\n> do { } while (0) macro since it does not return a value, while the\n> latter uses a GCC extension in order to return the trylock result. This\n> matches spin_lock_* convention as well.\n> \n\nCool, thanks for the explanation.\n",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged that the zone lock wrappers are not valuable and agreed to remove them.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "will remove"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, I agree, there is no much value in this wrappers, will remove them,\nthanks!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Gregory Price",
      "primary_email": "gourry@gourry.net",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[RFC PATCH v4 27/27] cxl: add cxl_compression PCI driver",
          "message_id": "20260222084842.1824063-28-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260222084842.1824063-28-gourry@gourry.net/",
          "date": "2026-02-22T08:50:38Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-22",
          "patch_summary": "This patch introduces a new PCI driver called cxl_compression, which is part of a larger series that adds support for Private Memory Nodes (PMNs) and Compressed RAM. The PMN feature allows for the creation of isolated NUMA nodes, while Compressed RAM enables the compression of memory pages to reduce memory usage. The cxl_compression driver is designed to work with CXL (Compute Express Link) devices, which are used to manage compressed memory. This patch adds the necessary infrastructure to support the cxl_compression driver and allows for the creation of PMNs and Compressed RAM services.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the node_private infrastructure being mutually exclusive with N_MEMORY, explained that it's intended for memory nodes not meant for general consumption, and confirmed that Zonelist construction changes are deferred to a subsequent commit.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY nodes are intended to contain general System RAM. Today, some\ndevice drivers hotplug their memory (marked Specific Purpose or Reserved)\nto get access to mm/ services, but don't intend it for general consumption.\n\nCreate N_MEMORY_PRIVATE for memory nodes whose memory is not intended for\ngeneral consumption. This state is mutually exclusive with N_MEMORY.\n\nAdd the node_private infrastructure for N_MEMORY_PRIVATE nodes:\n\n  - struct node_private: Per-node container stored in NODE_DATA(nid),\n    holding driver callbacks (ops), owner, and refcount.\n\n  - struct node_private_ops: Initial structure with void *reserved\n    placeholder and flags field.  Callbacks will be added by subsequent\n    commits as each consumer is wired up.\n\n  - folio_is_private_node() / page_is_private_node(): check if a\n    folio/page resides on a private node.\n\n  - folio_node_private_ops() / node_private_flags(): retrieve the ops\n    vtable or flags for a folio's node.\n\n  - Registration API: node_private_register()/unregister() for drivers\n    to register callbacks for private nodes. Only one driver callback\n    can be registered per node - attempting to register different ops\n    returns -EBUSY.\n\n  - sysfs attribute exposing N_MEMORY_PRIVATE node state.\n\nZonelist construction changes for private nodes are deferred to a\nsubsequent commit.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 197 ++++++++++++++++++++++++++++++++\n include/linux/mmzone.h       |   4 +\n include/linux/node_private.h | 210 +++++++++++++++++++++++++++++++++++\n include/linux/nodemask.h     |   1 +\n 4 files changed, 412 insertions(+)\n create mode 100644 include/linux/node_private.h\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 00cf4532f121..646dc48a23b5 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -22,6 +22,7 @@\n #include <linux/swap.h>\n #include <linux/slab.h>\n #include <linux/memblock.h>\n+#include <linux/node_private.h>\n \n static const struct bus_type node_subsys = {\n \t.name = \"node\",\n@@ -861,6 +862,198 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n \t\t\t   (void *)&nid, register_mem_block_under_node_hotplug);\n \treturn;\n }\n+\n+static DEFINE_MUTEX(node_private_lock);\n+static bool node_private_initialized;\n+\n+/**\n+ * node_private_register - Register a private node\n+ * @nid: Node identifier\n+ * @np: The node_private structure (driver-allocated, driver-owned)\n+ *\n+ * Register a driver for a private node. Only one driver can register\n+ * per node. If another driver has already registered (with different np),\n+ * -EBUSY is returned. Re-registration with the same np is allowed.\n+ *\n+ * The driver owns the node_private memory and must ensure it remains valid\n+ * until refcount reaches 0 after node_private_unregister().\n+ *\n+ * Returns 0 on success, negative errno on failure.\n+ */\n+int node_private_register(int nid, struct node_private *np)\n+{\n+\tstruct node_private *existing;\n+\tpg_data_t *pgdat;\n+\tint ret = 0;\n+\n+\tif (!np || !node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tif (!node_private_initialized)\n+\t\treturn -ENODEV;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\t/* N_MEMORY_PRIVATE and N_MEMORY are mutually exclusive */\n+\tif (node_state(nid, N_MEMORY)) {\n+\t\tret = -EBUSY;\n+\t\tgoto out;\n+\t}\n+\n+\tpgdat = NODE_DATA(nid);\n+\texisting = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t\t     lockdep_is_held(&node_private_lock));\n+\n+\t/* Only one source my register this node */\n+\tif (existing) {\n+\t\tif (existing != np) {\n+\t\t\tret = -EBUSY;\n+\t\t\tgoto out;\n+\t\t}\n+\t\tgoto out;\n+\t}\n+\n+\trefcount_set(&np->refcount, 1);\n+\tinit_completion(&np->released);\n+\n+\trcu_assign_pointer(pgdat->node_private, np);\n+\tpgdat->private = true;\n+\n+out:\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_register);\n+\n+/**\n+ * node_private_set_ops - Set service callbacks on a registered private node\n+ * @nid: Node identifier\n+ * @ops: Service callbacks and flags (driver-owned, must outlive registration)\n+ *\n+ * Validates flag dependencies and sets the ops on the node's node_private.\n+ * The node must already be registered via node_private_register().\n+ *\n+ * Returns 0 on success, -EINVAL for invalid flag combinations,\n+ * -ENODEV if no node_private is registered on @nid.\n+ */\n+int node_private_set_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!ops)\n+\t\treturn -EINVAL;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse\n+\t\tnp->ops = ops;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_set_ops);\n+\n+/**\n+ * node_private_clear_ops - Clear service callbacks from a private node\n+ * @nid: Node identifier\n+ * @ops: Expected ops pointer (must match current ops)\n+ *\n+ * Clears the ops only if @ops matches the currently registered ops,\n+ * preventing one service from accidentally clearing another's callbacks.\n+ *\n+ * Returns 0 on success, -ENODEV if no node_private is registered,\n+ * -EINVAL if @ops does not match.\n+ */\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse if (np->ops != ops)\n+\t\tret = -EINVAL;\n+\telse\n+\t\tnp->ops = NULL;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_clear_ops);\n+\n+/**\n+ * node_private_unregister - Unregister a private node\n+ * @nid: Node identifier\n+ *\n+ * Unregister the driver from a private node. Only succeeds if all memory\n+ * has been offlined and the node is no longer N_MEMORY_PRIVATE.\n+ * When successful, drops the refcount to 0 indicating the driver can\n+ * free its context.\n+ *\n+ * N_MEMORY_PRIVATE state is cleared by offline_pages() when the last\n+ * memory is offlined, not by this function.\n+ *\n+ * Return: 0 if unregistered, -EBUSY if N_MEMORY_PRIVATE is still set\n+ * (other memory blocks remain on this node).\n+ */\n+int node_private_unregister(int nid)\n+{\n+\tstruct node_private *np;\n+\tpg_data_t *pgdat;\n+\n+\tif (!node_possible(nid))\n+\t\treturn 0;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\tpgdat = NODE_DATA(nid);\n+\tnp = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Only unregister if all memory is offline and N_MEMORY_PRIVATE is\n+\t * cleared. N_MEMORY_PRIVATE is cleared by offline_pages() when the\n+\t * last memory block is offlined.\n+\t */\n+\tif (node_state(nid, N_MEMORY_PRIVATE)) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn -EBUSY;\n+\t}\n+\n+\trcu_assign_pointer(pgdat->node_private, NULL);\n+\tpgdat->private = false;\n+\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\n+\tsynchronize_rcu();\n+\n+\tif (!refcount_dec_and_test(&np->refcount))\n+\t\twait_for_completion(&np->released);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(node_private_unregister);\n+\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n /**\n@@ -959,6 +1152,7 @@ static struct node_attr node_state_attr[] = {\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n \t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\n \t\t\t\t\t   N_GENERIC_INITIATOR),\n@@ -972,6 +1166,7 @@ static struct attribute *node_state_attrs[] = {\n \t&node_state_attr[N_HIGH_MEMORY].attr.attr,\n #endif\n \t&node_state_attr[N_MEMORY].attr.attr,\n+\t&node_state_attr[N_MEMORY_PRIVATE].attr.attr,\n \t&node_state_attr[N_CPU].attr.attr,\n \t&node_state_attr[N_GENERIC_INITIATOR].attr.attr,\n \tNULL\n@@ -1007,5 +1202,7 @@ void __init node_dev_init(void)\n \t\t\tpanic(\"%s() failed to add node: %d\\n\", __func__, ret);\n \t}\n \n+\tnode_private_initialized = true;\n+\n \tregister_memory_blocks_under_nodes();\n }\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex b01cb1e49896..992eb1c5a2c6 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -25,6 +25,8 @@\n #include <linux/zswap.h>\n #include <asm/page.h>\n \n+struct node_private;\n+\n /* Free memory management - zoned buddy allocator.  */\n #ifndef CONFIG_ARCH_FORCE_MAX_ORDER\n #define MAX_PAGE_ORDER 10\n@@ -1514,6 +1516,8 @@ typedef struct pglist_data {\n \tatomic_long_t\t\tvm_stat[NR_VM_NODE_STAT_ITEMS];\n #ifdef CONFIG_NUMA\n \tstruct memory_tier __rcu *memtier;\n+\tstruct node_private __rcu *node_private;\n+\tbool private;\n #endif\n #ifdef CONFIG_MEMORY_FAILURE\n \tstruct memory_failure_stats mf_stats;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nnew file mode 100644\nindex 000000000000..6a70ec39d569\n--- /dev/null\n+++ b/include/linux/node_private.h\n@@ -0,0 +1,210 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_NODE_PRIVATE_H\n+#define _LINUX_NODE_PRIVATE_H\n+\n+#include <linux/completion.h>\n+#include <linux/mm.h>\n+#include <linux/nodemask.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+\n+struct page;\n+struct vm_area_struct;\n+struct vm_fault;\n+\n+/**\n+ * struct node_private_ops - Callbacks for private node services\n+ *\n+ * Services register these callbacks to intercept MM operations that affect\n+ * their private nodes.\n+ *\n+ * Flag bits control which MM subsystems may operate on folios on this node.\n+ *\n+ * The pgdat->node_private pointer is RCU-protected.  Callbacks fall into\n+ * three categories based on their calling context:\n+ *\n+ * Folio-referenced callbacks (RCU released before callback):\n+ *   The caller holds a reference to a folio on the private node, which\n+ *   pins the node's memory online and prevents node_private teardown.\n+ *\n+ * Refcounted callbacks (RCU released before callback):\n+ *   The caller has no folio on the private node (e.g., folios are on a\n+ *   source node being migrated TO this node).  A temporary refcount is\n+ *   taken on node_private under rcu_read_lock to keep the structure (and\n+ *   the service module) alive across the callback.  node_private_unregister\n+ *   waits for all temporary references to drain before returning.\n+ *\n+ * Non-folio callbacks (rcu_read_lock held during callback):\n+ *   No folio reference exists, so rcu_read_lock is held across the\n+ *   callback to prevent node_private from being freed.\n+ *   These callbacks MUST NOT sleep.\n+ *\n+ * @flags: Operation exclusion flags (NP_OPS_* constants).\n+ *\n+ */\n+struct node_private_ops {\n+\tunsigned long flags;\n+};\n+\n+/**\n+ * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n+ *\n+ * This structure is allocated by the driver and passed to node_private_register().\n+ * The driver owns the memory and must ensure it remains valid until after\n+ * node_private_unregister() returns with the reference count dropped to 0.\n+ *\n+ * @owner: Opaque driver identifier\n+ * @refcount: Reference count (1 = registered; temporary refs for non-folio\n+ *\t\tcallbacks that may sleep; 0 = fully released)\n+ * @released: Signaled when refcount drops to 0; unregister waits on this\n+ * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ */\n+struct node_private {\n+\tvoid *owner;\n+\trefcount_t refcount;\n+\tstruct completion released;\n+\tconst struct node_private_ops *ops;\n+};\n+\n+#ifdef CONFIG_NUMA\n+\n+#include <linux/mmzone.h>\n+\n+/**\n+ * folio_is_private_node - Check if folio is on an N_MEMORY_PRIVATE node\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio resides on a private node.\n+ */\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn node_state(folio_nid(folio), N_MEMORY_PRIVATE);\n+}\n+\n+/**\n+ * page_is_private_node - Check if page is on an N_MEMORY_PRIVATE node\n+ * @page: The page to check\n+ *\n+ * Returns true if the page resides on a private node.\n+ */\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\tconst struct node_private_ops *ops;\n+\tstruct node_private *np;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(folio_nid(folio))->node_private);\n+\tops = np ? np->ops : NULL;\n+\trcu_read_unlock();\n+\n+\treturn ops;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\tstruct node_private *np;\n+\tunsigned long flags;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tflags = (np && np->ops) ? np->ops->flags : 0;\n+\trcu_read_unlock();\n+\n+\treturn flags;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn node_private_flags(folio_nid(f)) & flag;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn node_private_flags(nid) & flag;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn node_private_flags(zone_to_nid(z)) & flag;\n+}\n+\n+#else /* !CONFIG_NUMA */\n+\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn false;\n+}\n+\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn false;\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\treturn NULL;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+#endif /* CONFIG_NUMA */\n+\n+#if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\n+\n+int node_private_register(int nid, struct node_private *np);\n+int node_private_unregister(int nid);\n+int node_private_set_ops(int nid, const struct node_private_ops *ops);\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n+\n+#else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n+\n+static inline int node_private_register(int nid, struct node_private *np)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_unregister(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline int node_private_set_ops(int nid,\n+\t\t\t\t       const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_clear_ops(int nid,\n+\t\t\t\t\t const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+#endif /* CONFIG_NUMA && CONFIG_MEMORY_HOTPLUG */\n+\n+#endif /* _LINUX_NODE_PRIVATE_H */\ndiff --git a/include/linux/nodemask.h b/include/linux/nodemask.h\nindex bd38648c998d..c9bcfd5a9a06 100644\n--- a/include/linux/nodemask.h\n+++ b/include/linux/nodemask.h\n@@ -391,6 +391,7 @@ enum node_states {\n \tN_HIGH_MEMORY = N_NORMAL_MEMORY,\n #endif\n \tN_MEMORY,\t\t/* The node has memory(regular, high, movable) */\n+\tN_MEMORY_PRIVATE,\t/* The node's memory is private */\n \tN_CPU,\t\t/* The node has one or more cpus */\n \tN_GENERIC_INITIATOR,\t/* The node has one or more Generic Initiators */\n \tNR_NODE_STATES\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about general allocations landing on private nodes without explicit permission by introducing __GFP_PRIVATE and updating cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE nodes unless this flag is set.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY_PRIVATE nodes hold device-managed memory that should not be\nused for general allocations. Without a gating mechanism, any allocation\ncould land on a private node if it appears in the task's mems_allowed.\n\nIntroduce __GFP_PRIVATE that explicitly opts in to allocation from\nN_MEMORY_PRIVATE nodes.\n\nAdd the GFP_PRIVATE compound mask (__GFP_PRIVATE | __GFP_THISNODE)\nfor callers that explicitly target private nodes to help prevent\nfallback allocations from DRAM.\n\nUpdate cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE\nnodes unless __GFP_PRIVATE is set.\n\nIn interrupt context, only N_MEMORY nodes are valid.\n\nUpdate cpuset_handle_hotplug() to include N_MEMORY_PRIVATE nodes in\nthe effective mems set, allowing cgroup-level control over private\nnode access.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/gfp_types.h      | 15 +++++++++++++--\n include/trace/events/mmflags.h |  4 ++--\n kernel/cgroup/cpuset.c         | 32 ++++++++++++++++++++++++++++----\n 3 files changed, 43 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/gfp_types.h b/include/linux/gfp_types.h\nindex 3de43b12209e..ac375f9a0fc2 100644\n--- a/include/linux/gfp_types.h\n+++ b/include/linux/gfp_types.h\n@@ -33,7 +33,7 @@ enum {\n \t___GFP_IO_BIT,\n \t___GFP_FS_BIT,\n \t___GFP_ZERO_BIT,\n-\t___GFP_UNUSED_BIT,\t/* 0x200u unused */\n+\t___GFP_PRIVATE_BIT,\n \t___GFP_DIRECT_RECLAIM_BIT,\n \t___GFP_KSWAPD_RECLAIM_BIT,\n \t___GFP_WRITE_BIT,\n@@ -69,7 +69,7 @@ enum {\n #define ___GFP_IO\t\tBIT(___GFP_IO_BIT)\n #define ___GFP_FS\t\tBIT(___GFP_FS_BIT)\n #define ___GFP_ZERO\t\tBIT(___GFP_ZERO_BIT)\n-/* 0x200u unused */\n+#define ___GFP_PRIVATE\t\tBIT(___GFP_PRIVATE_BIT)\n #define ___GFP_DIRECT_RECLAIM\tBIT(___GFP_DIRECT_RECLAIM_BIT)\n #define ___GFP_KSWAPD_RECLAIM\tBIT(___GFP_KSWAPD_RECLAIM_BIT)\n #define ___GFP_WRITE\t\tBIT(___GFP_WRITE_BIT)\n@@ -139,6 +139,11 @@ enum {\n  * %__GFP_ACCOUNT causes the allocation to be accounted to kmemcg.\n  *\n  * %__GFP_NO_OBJ_EXT causes slab allocation to have no object extension.\n+ *\n+ * %__GFP_PRIVATE allows allocation from N_MEMORY_PRIVATE nodes (e.g., compressed\n+ * memory, accelerator memory). Without this flag, allocations are restricted\n+ * to N_MEMORY nodes only. Used by migration/demotion paths when explicitly\n+ * targeting private nodes.\n  */\n #define __GFP_RECLAIMABLE ((__force gfp_t)___GFP_RECLAIMABLE)\n #define __GFP_WRITE\t((__force gfp_t)___GFP_WRITE)\n@@ -146,6 +151,7 @@ enum {\n #define __GFP_THISNODE\t((__force gfp_t)___GFP_THISNODE)\n #define __GFP_ACCOUNT\t((__force gfp_t)___GFP_ACCOUNT)\n #define __GFP_NO_OBJ_EXT   ((__force gfp_t)___GFP_NO_OBJ_EXT)\n+#define __GFP_PRIVATE\t((__force gfp_t)___GFP_PRIVATE)\n \n /**\n  * DOC: Watermark modifiers\n@@ -367,6 +373,10 @@ enum {\n  * available and will not wake kswapd/kcompactd on failure. The _LIGHT\n  * version does not attempt reclaim/compaction at all and is by default used\n  * in page fault path, while the non-light is used by khugepaged.\n+ *\n+ * %GFP_PRIVATE adds %__GFP_THISNODE by default to prevent any fallback\n+ * allocations to other nodes, given that the caller was already attempting\n+ * to access driver-managed memory explicitly.\n  */\n #define GFP_ATOMIC\t(__GFP_HIGH|__GFP_KSWAPD_RECLAIM)\n #define GFP_KERNEL\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n@@ -382,5 +392,6 @@ enum {\n #define GFP_TRANSHUGE_LIGHT\t((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \\\n \t\t\t __GFP_NOMEMALLOC | __GFP_NOWARN) & ~__GFP_RECLAIM)\n #define GFP_TRANSHUGE\t(GFP_TRANSHUGE_LIGHT | __GFP_DIRECT_RECLAIM)\n+#define GFP_PRIVATE\t(__GFP_PRIVATE | __GFP_THISNODE)\n \n #endif /* __LINUX_GFP_TYPES_H */\ndiff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h\nindex a6e5a44c9b42..f042cd848451 100644\n--- a/include/trace/events/mmflags.h\n+++ b/include/trace/events/mmflags.h\n@@ -37,7 +37,8 @@\n \tTRACE_GFP_EM(HARDWALL)\t\t\t\\\n \tTRACE_GFP_EM(THISNODE)\t\t\t\\\n \tTRACE_GFP_EM(ACCOUNT)\t\t\t\\\n-\tTRACE_GFP_EM(ZEROTAGS)\n+\tTRACE_GFP_EM(ZEROTAGS)\t\t\t\\\n+\tTRACE_GFP_EM(PRIVATE)\n \n #ifdef CONFIG_KASAN_HW_TAGS\n # define TRACE_GFP_FLAGS_KASAN\t\t\t\\\n@@ -73,7 +74,6 @@\n TRACE_GFP_FLAGS\n \n /* Just in case these are ever used */\n-TRACE_DEFINE_ENUM(___GFP_UNUSED_BIT);\n TRACE_DEFINE_ENUM(___GFP_LAST_BIT);\n \n #define gfpflag_string(flag) {(__force unsigned long)flag, #flag}\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 473aa9261e16..1a597f0c7c6c 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -444,21 +444,32 @@ static void guarantee_active_cpus(struct task_struct *tsk,\n }\n \n /*\n- * Return in *pmask the portion of a cpusets's mems_allowed that\n+ * Return in *pmask the portion of a cpuset's mems_allowed that\n  * are online, with memory.  If none are online with memory, walk\n  * up the cpuset hierarchy until we find one that does have some\n  * online mems.  The top cpuset always has some mems online.\n  *\n  * One way or another, we guarantee to return some non-empty subset\n- * of node_states[N_MEMORY].\n+ * of node_states[N_MEMORY].  N_MEMORY_PRIVATE nodes from the\n+ * original cpuset are preserved, but only N_MEMORY nodes are\n+ * pulled from ancestors.\n  *\n  * Call with callback_lock or cpuset_mutex held.\n  */\n static void guarantee_online_mems(struct cpuset *cs, nodemask_t *pmask)\n {\n+\tstruct cpuset *orig_cs = cs;\n+\tint nid;\n+\n \twhile (!nodes_intersects(cs->effective_mems, node_states[N_MEMORY]))\n \t\tcs = parent_cs(cs);\n+\n \tnodes_and(*pmask, cs->effective_mems, node_states[N_MEMORY]);\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_isset(nid, orig_cs->effective_mems))\n+\t\t\tnode_set(nid, *pmask);\n+\t}\n }\n \n /**\n@@ -4075,7 +4086,9 @@ static void cpuset_handle_hotplug(void)\n \n \t/* fetch the available cpus/mems and find out which changed how */\n \tcpumask_copy(&new_cpus, cpu_active_mask);\n-\tnew_mems = node_states[N_MEMORY];\n+\n+\t/* Include N_MEMORY_PRIVATE so cpuset controls access the same way */\n+\tnodes_or(new_mems, node_states[N_MEMORY], node_states[N_MEMORY_PRIVATE]);\n \n \t/*\n \t * If subpartitions_cpus is populated, it is likely that the check\n@@ -4488,10 +4501,21 @@ bool cpuset_node_allowed(struct cgroup *cgroup, int nid)\n  * __alloc_pages() will include all nodes.  If the slab allocator\n  * is passed an offline node, it will fall back to the local node.\n  * See kmem_cache_alloc_node().\n+ *\n+ *\n+ * Private nodes aren't eligible for these allocations, so skip them.\n+ * guarantee_online_mems guaranttes at least one N_MEMORY node is set.\n  */\n static int cpuset_spread_node(int *rotor)\n {\n-\treturn *rotor = next_node_in(*rotor, current->mems_allowed);\n+\tint node;\n+\n+\tdo {\n+\t\tnode = next_node_in(*rotor, current->mems_allowed);\n+\t\t*rotor = node;\n+\t} while (node_state(node, N_MEMORY_PRIVATE));\n+\n+\treturn node;\n }\n \n /**\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern that the open-coded cpuset filtering in mm/ does not account for N_MEMORY_PRIVATE nodes on systems without cpusets, which can lead to private-node zones leaking into allocation paths. The author added a new helper function numa_zone_allowed() and replaced the open-coded patterns with it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Various locations in mm/ open-code cpuset filtering with:\n\n  cpusets_enabled() && ALLOC_CPUSET && !__cpuset_zone_allowed()\n\nThis pattern does not account for N_MEMORY_PRIVATE nodes on systems\nwithout cpusets, so private-node zones can leak into allocation\npaths that should only see general-purpose memory.\n\nAdd numa_zone_allowed() which consolidates zone filtering. It checks\ncpuset membership when cpusets are enabled, and otherwise gates\nN_MEMORY_PRIVATE zones behind __GFP_PRIVATE globally.\n\nReplace the open-coded patterns in mm/ with the new helper.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/compaction.c |  6 ++----\n mm/hugetlb.c    |  2 +-\n mm/internal.h   |  7 +++++++\n mm/page_alloc.c | 31 ++++++++++++++++++++-----------\n mm/slub.c       |  3 ++-\n 5 files changed, 32 insertions(+), 17 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..6a65145b03d8 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -2829,10 +2829,8 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tenum compact_result status;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 51273baec9e5..f2b914ab5910 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -1353,7 +1353,7 @@ static struct folio *dequeue_hugetlb_folio_nodemask(struct hstate *h, gfp_t gfp_\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {\n \t\tstruct folio *folio;\n \n-\t\tif (!cpuset_zone_allowed(zone, gfp_mask))\n+\t\tif (!numa_zone_alloc_allowed(ALLOC_CPUSET, zone, gfp_mask))\n \t\t\tcontinue;\n \t\t/*\n \t\t * no need to ask again on the same node. Pool is node rather than\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 23ee14790227..97023748e6a9 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t      gfp_t gfp_mask);\n #else\n #define node_reclaim_mode 0\n \n@@ -1218,6 +1220,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t\t     gfp_t gfp_mask)\n+{\n+\treturn true;\n+}\n #endif\n \n static inline bool node_reclaim_enabled(void)\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 2facee0805da..47f2619d3840 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3690,6 +3690,21 @@ static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n \treturn node_distance(zone_to_nid(local_zone), zone_to_nid(zone)) <=\n \t\t\t\tnode_reclaim_distance;\n }\n+\n+/* Returns true if allocation from this zone is permitted */\n+bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone, gfp_t gfp_mask)\n+{\n+\t/* Gate N_MEMORY_PRIVATE zones behind __GFP_PRIVATE */\n+\tif (!(gfp_mask & __GFP_PRIVATE) &&\n+\t    node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn false;\n+\n+\t/* If cpusets is being used, check mems_allowed */\n+\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET))\n+\t\treturn cpuset_zone_allowed(zone, gfp_mask);\n+\n+\treturn true;\n+}\n #else\t/* CONFIG_NUMA */\n static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n {\n@@ -3781,10 +3796,8 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\tstruct page *page;\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \t\t/*\n \t\t * When allocating a page cache page for writing, we\n \t\t * want to get it from a node that is within its dirty\n@@ -4585,10 +4598,8 @@ should_reclaim_retry(gfp_t gfp_mask, unsigned order,\n \t\tunsigned long min_wmark = min_wmark_pages(zone);\n \t\tbool wmark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tavailable = reclaimable = zone_reclaimable_pages(zone);\n \t\tavailable += zone_page_state_snapshot(zone, NR_FREE_PAGES);\n@@ -5084,10 +5095,8 @@ unsigned long alloc_pages_bulk_noprof(gfp_t gfp, int preferred_nid,\n \tfor_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&\n-\t\t    !__cpuset_zone_allowed(zone, gfp)) {\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp))\n \t\t\tcontinue;\n-\t\t}\n \n \t\tif (nr_online_nodes > 1 && zone != zonelist_zone(ac.preferred_zoneref) &&\n \t\t    zone_to_nid(zone) != zonelist_node_idx(ac.preferred_zoneref)) {\ndiff --git a/mm/slub.c b/mm/slub.c\nindex 861592ac5425..e4bd6ede81d1 100644\n--- a/mm/slub.c\n+++ b/mm/slub.c\n@@ -3595,7 +3595,8 @@ static struct slab *get_any_partial(struct kmem_cache *s,\n \n \t\t\tn = get_node(s, zone_to_nid(zone));\n \n-\t\t\tif (n && cpuset_zone_allowed(zone, pc->flags) &&\n+\t\t\tif (n && numa_zone_alloc_allowed(ALLOC_CPUSET, zone,\n+\t\t\t\t\t\t   pc->flags) &&\n \t\t\t\t\tn->nr_partial > s->min_partial) {\n \t\t\t\tslab = get_partial_node(s, n, pc);\n \t\t\t\tif (slab) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about N_MEMORY fallback lists including N_MEMORY_PRIVATE nodes, explaining that this would allow allocations from private nodes in some scenarios and cause unnecessary iterations over ineligible nodes. The author provided a patch to fix the issue by adding private nodes as fallbacks for kernel allocations on behalf of the private node.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY fallback lists should not include N_MEMORY_PRIVATE nodes, at\nworst this would allow allocation from them in some scenarios, and at\nbest it causes iterations over nodes that aren't eligible.\n\nPrivate node primary fallback lists do include N_MEMORY nodes so\nkernel/slab allocations made on behalf of the private node can\nfall back to DRAM when __GFP_PRIVATE is not set.\n\nThe nofallback list contains only the node's own zones, restricting\n__GFP_THISNODE allocations to the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/page_alloc.c | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 47f2619d3840..5a1b35421d78 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5683,6 +5683,26 @@ static void build_zonelists(pg_data_t *pgdat)\n \tlocal_node = pgdat->node_id;\n \tprev_node = local_node;\n \n+\t/*\n+\t * Private nodes need N_MEMORY nodes as fallback for kernel allocations\n+\t * (e.g., slab objects allocated on behalf of this node).\n+\t */\n+\tif (node_state(local_node, N_MEMORY_PRIVATE)) {\n+\t\tnode_order[nr_nodes++] = local_node;\n+\t\tnode_set(local_node, used_mask);\n+\n+\t\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0)\n+\t\t\tnode_order[nr_nodes++] = node;\n+\n+\t\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n+\t\tbuild_thisnode_zonelists(pgdat);\n+\t\tpr_info(\"Fallback order for Node %d (private):\", local_node);\n+\t\tfor (node = 0; node < nr_nodes; node++)\n+\t\t\tpr_cont(\" %d\", node_order[node]);\n+\t\tpr_cont(\"\\n\");\n+\t\treturn;\n+\t}\n+\n \tmemset(node_order, 0, sizeof(node_order));\n \twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author addressed a concern about the need for a unified predicate to exclude both N_MEMORY_PRIVATE and ZONE_DEVICE folios from MM operations, and provided a patch that adds the folio_is_private_managed() function to achieve this.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix is needed",
                "provided a patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Multiple mm/ subsystems already skip operations for ZONE_DEVICE folios,\nand N_MEMORY_PRIVATE folios share the checkpoints for ZONE_DEVICE pages.\n\nAdd folio_is_private_managed() as a unified predicate that returns true\nfor folios on N_MEMORY_PRIVATE nodes or in ZONE_DEVICE.\n\nThis predicate replaces folio_is_zone_device at skip sites where both\nfolio types should be excluded from an MM operation.\n\nAt some locations, explicit zone_device vs private_node checks are more\nappropriate when the operations between the two fundamentally differ.\n\nThe !CONFIG_NUMA stubs fall through to folio_is_zone_device() only,\npreserving existing behavior when NUMA is disabled.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 6a70ec39d569..7687a4cf990c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -92,6 +92,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio) || folio_is_private_node(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n@@ -146,6 +156,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn false;\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about mlocking private node folios, explaining that they should not be locked and citing the existing folio_is_zone_device check as sufficient to handle this case. The author extended this check to include private nodes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "addressed_concern",
                "explained_reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nmlocked.  The existing folio_is_zone_device check is already correctly\nplaced to handle this - simply extend it for private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/mlock.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/mlock.c b/mm/mlock.c\nindex 2f699c3497a5..c56159253e45 100644\n--- a/mm/mlock.c\n+++ b/mm/mlock.c\n@@ -25,6 +25,7 @@\n #include <linux/memcontrol.h>\n #include <linux/mm_inline.h>\n #include <linux/secretmem.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -366,7 +367,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (is_huge_zero_pmd(*pmd))\n \t\t\tgoto out;\n \t\tfolio = pmd_folio(*pmd);\n-\t\tif (folio_is_zone_device(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)))\n \t\t\tgoto out;\n \t\tif (vma->vm_flags & VM_LOCKED)\n \t\t\tmlock_folio(folio);\n@@ -386,7 +387,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (!pte_present(ptent))\n \t\t\tcontinue;\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\tstep = folio_mlock_step(folio, pte, addr, end);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author acknowledged a concern that madvise and pageout operations should not interfere with device driver-managed private node folios, agreed to extend the zone_device check to cover private nodes, and made corresponding changes to mm/madvise.c.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "agreed to make changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nsubjectto madvise cold/pageout/free operations that would interfere\nwith the driver's memory management.\n\nExtend the existing zone_device check to cover private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/madvise.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/madvise.c b/mm/madvise.c\nindex b617b1be0f53..3aac105e840b 100644\n--- a/mm/madvise.c\n+++ b/mm/madvise.c\n@@ -32,6 +32,7 @@\n #include <linux/leafops.h>\n #include <linux/shmem_fs.h>\n #include <linux/mmu_notifier.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlb.h>\n \n@@ -475,7 +476,7 @@ static int madvise_cold_or_pageout_pte_range(pmd_t *pmd,\n \t\t\tcontinue;\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n@@ -704,7 +705,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,\n \t\t}\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private node folios participating in KSM merging by default, agreeing that this can interfere with driver operations. The author extended existing checks to exclude private node folios from KSM merging.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not participate in KSM merging by default.\nThe driver manages the memory lifecycle and KSM's page sharing can\ninterfere with driver operations.\n\nExtend the existing zone_device checks in get_mergeable_page and\nksm_next_page_pmd_entry to cover private node folios as well.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/ksm.c | 9 ++++++---\n 1 file changed, 6 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/ksm.c b/mm/ksm.c\nindex 2d89a7c8b4eb..c48e95a6fff9 100644\n--- a/mm/ksm.c\n+++ b/mm/ksm.c\n@@ -40,6 +40,7 @@\n #include <linux/oom.h>\n #include <linux/numa.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include \"internal.h\"\n@@ -808,7 +809,7 @@ static struct page *get_mergeable_page(struct ksm_rmap_item *rmap_item)\n \n \tfolio = folio_walk_start(&fw, vma, addr, 0);\n \tif (folio) {\n-\t\tif (!folio_is_zone_device(folio) &&\n+\t\tif (!folio_is_private_managed(folio) &&\n \t\t    folio_test_anon(folio)) {\n \t\t\tfolio_get(folio);\n \t\t\tpage = fw.page;\n@@ -2521,7 +2522,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\t\tgoto not_found_unlock;\n \t\t\tfolio = page_folio(page);\n \n-\t\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t\t    !folio_test_anon(folio))\n \t\t\t\tgoto not_found_unlock;\n \n \t\t\tpage += ((addr & (PMD_SIZE - 1)) >> PAGE_SHIFT);\n@@ -2545,7 +2547,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\tcontinue;\n \t\tfolio = page_folio(page);\n \n-\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t    !folio_test_anon(folio))\n \t\t\tcontinue;\n \t\tgoto found_unlock;\n \t}\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about collapse operations on private nodes potentially promoting pages to local nodes and inverting LRU order, agreeing that handling this like zone_device is the best approach for now.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "A collapse operation allocates a new large folio and migrates the\nsmaller folios into it.  This is an issue for private nodes:\n\n  1. The private node service may not support migration\n  2. Collapse may promotes pages from the private node to a local node,\n     which may result in an LRU inversion that defeats memory tiering.\n\nHandle this just like zone_device for now.\n\nIt may be possible to support this later for some private node services\nthat report explicit support for collapse (and migration).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/khugepaged.c | 7 ++++---\n 1 file changed, 4 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/khugepaged.c b/mm/khugepaged.c\nindex 97d1b2824386..36f6bc5da53c 100644\n--- a/mm/khugepaged.c\n+++ b/mm/khugepaged.c\n@@ -21,6 +21,7 @@\n #include <linux/shmem_fs.h>\n #include <linux/dax.h>\n #include <linux/ksm.h>\n+#include <linux/node_private.h>\n #include <linux/pgalloc.h>\n \n #include <asm/tlb.h>\n@@ -571,7 +572,7 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,\n \t\t\tgoto out;\n \t\t}\n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out;\n \t\t}\n@@ -1323,7 +1324,7 @@ static int hpage_collapse_scan_pmd(struct mm_struct *mm,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out_unmap;\n \t\t}\n@@ -1575,7 +1576,7 @@ int collapse_pte_mapped_thp(struct mm_struct *mm, unsigned long addr,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, ptent);\n-\t\tif (WARN_ON_ONCE(page && is_zone_device_page(page)))\n+\t\tif (WARN_ON_ONCE(page && page_is_private_managed(page)))\n \t\t\tpage = NULL;\n \t\t/*\n \t\t * Note that uprobe, debugger, or MAP_PRIVATE may change the\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about cleanup when a folio's refcount drops to zero, explaining that the service may need to perform cleanup before the page returns to the buddy allocator. They added a new function `folio_managed_on_free()` to wrap both zone_device and private node semantics for this operation. The function will return true if the folio is fully handled (zone_device) or false if the callback ran but the folio should continue through the normal free path (private_node).",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "added new function to address concern"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a folio's refcount drops to zero, the service may need to perform\ncleanup before the page returns to the buddy allocator (e.g. zeroing\npages to scrub stale compressed data / release compression ratio).\n\nAdd folio_managed_on_free() to wrap both zone_device and private node\nsemantics for this operation since they are the same.\n\nOne difference between zone_device and private node folios:\n  - private nodes may choose to either take a reference and return true\n    (\"handled\"), or return false to return it back to the buddy.\n\n  - zone_device returns the page to the buddy (always returns true)\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 30 ++++++++++++++++++++++++++++++\n mm/swap.c                    | 21 ++++++++++-----------\n 3 files changed, 46 insertions(+), 11 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7687a4cf990c..09ea7c4cb13c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -39,10 +39,16 @@ struct vm_fault;\n  *   callback to prevent node_private from being freed.\n  *   These callbacks MUST NOT sleep.\n  *\n+ * @free_folio: Called when a folio refcount drops to 0\n+ *   [folio-referenced callback]\n+ *   Returns: true if handled (skip return to buddy)\n+ *            false if no op (return to buddy)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n+\tbool (*free_folio)(struct folio *folio);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 97023748e6a9..658da41cdb8e 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1412,6 +1412,36 @@ int numa_migrate_check(struct folio *folio, struct vm_fault *vmf,\n void free_zone_device_folio(struct folio *folio);\n int migrate_device_coherent_folio(struct folio *folio);\n \n+/**\n+ * folio_managed_on_free - Notify managed-memory service that folio\n+ *                         refcount reached zero.\n+ * @folio: the folio being freed\n+ *\n+ * Returns true if the folio is fully handled (zone_device -- caller\n+ * must return immediately).  Returns false if the callback ran but\n+ * the folio should continue through the normal free path\n+ * (private_node -- pages go back to buddy).\n+ *\n+ * Returns false for normal folios (no-op).\n+ */\n+static inline bool folio_managed_on_free(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio)) {\n+\t\tfree_zone_device_folio(folio);\n+\t\treturn true;\n+\t}\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->free_folio) {\n+\t\t\tif (ops->free_folio(folio))\n+\t\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/swap.c b/mm/swap.c\nindex 2260dcd2775e..dca306e1ae6d 100644\n--- a/mm/swap.c\n+++ b/mm/swap.c\n@@ -37,6 +37,7 @@\n #include <linux/page_idle.h>\n #include <linux/local_lock.h>\n #include <linux/buffer_head.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -96,10 +97,9 @@ static void page_cache_release(struct folio *folio)\n \n void __folio_put(struct folio *folio)\n {\n-\tif (unlikely(folio_is_zone_device(folio))) {\n-\t\tfree_zone_device_folio(folio);\n-\t\treturn;\n-\t}\n+\tif (unlikely(folio_is_private_managed(folio)))\n+\t\tif (folio_managed_on_free(folio))\n+\t\t\treturn;\n \n \tif (folio_test_hugetlb(folio)) {\n \t\tfree_huge_folio(folio);\n@@ -961,19 +961,18 @@ void folios_put_refs(struct folio_batch *folios, unsigned int *refs)\n \t\tif (is_huge_zero_folio(folio))\n \t\t\tcontinue;\n \n-\t\tif (folio_is_zone_device(folio)) {\n+\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n+\t\t\tcontinue;\n+\n+\t\tif (unlikely(folio_is_private_managed(folio))) {\n \t\t\tif (lruvec) {\n \t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n \t\t\t\tlruvec = NULL;\n \t\t\t}\n-\t\t\tif (folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\t\tfree_zone_device_folio(folio);\n-\t\t\tcontinue;\n+\t\t\tif (folio_managed_on_free(folio))\n+\t\t\t\tcontinue;\n \t\t}\n \n-\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\tcontinue;\n-\n \t\t/* hugetlb has its own memcg */\n \t\tif (folio_test_hugetlb(folio)) {\n \t\t\tif (lruvec) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about notifying private node services when a THP folio is split by adding an optional callback to the ops struct and updating the folio_split path in huge_memory.c. The author confirmed that this change will be included in the next version of the patch series.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "confirmed a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private node services may need to update internal metadata when\na THP folio is split.  ZONE_DEVICE already has a split callback via\npgmap->ops; private nodes can provide the same capability.\n\nJust like zone_device, some private node services may want to know\nabout a folio being split.  Add this optional callback to the ops\nstruct and add a wrapper for zone_device and private node callback\ndispatch to be consolidated.\n\nWire this into __folio_split() where the zone_device check was made.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 33 +++++++++++++++++++++++++++++++++\n mm/huge_memory.c             |  6 ++++--\n 2 files changed, 37 insertions(+), 2 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 09ea7c4cb13c..f9dd2d25c8a5 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -3,6 +3,7 @@\n #define _LINUX_NODE_PRIVATE_H\n \n #include <linux/completion.h>\n+#include <linux/memremap.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -44,11 +45,19 @@ struct vm_fault;\n  *   Returns: true if handled (skip return to buddy)\n  *            false if no op (return to buddy)\n  *\n+ * @folio_split: Notification that a folio on this private node is being split.\n+ *    [folio-referenced callback]\n+ *     Called from the folio split path via folio_managed_split_cb().\n+ *     @folio is the original folio; @new_folio is the newly created folio,\n+ *     or NULL when called for the final (original) folio after all sub-folios\n+ *     have been split off.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n+\tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n \tunsigned long flags;\n };\n \n@@ -150,6 +159,24 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn node_private_flags(zone_to_nid(z)) & flag;\n }\n \n+static inline void node_private_split_cb(struct folio *folio,\n+\t\t\t\t\t struct folio *new_folio)\n+{\n+\tconst struct node_private_ops *ops = folio_node_private_ops(folio);\n+\n+\tif (ops && ops->folio_split)\n+\t\tops->folio_split(folio, new_folio);\n+}\n+\n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+\telse if (folio_is_private_node(original_folio))\n+\t\tnode_private_split_cb(original_folio, new_folio);\n+}\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -198,6 +225,12 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn false;\n }\n \n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+}\n #endif /* CONFIG_NUMA */\n \n #if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 40cf59301c21..2ecae494291a 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -24,6 +24,7 @@\n #include <linux/freezer.h>\n #include <linux/mman.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/debugfs.h>\n #include <linux/migrate.h>\n@@ -3850,7 +3851,7 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \n \t\t\tnext = folio_next(new_folio);\n \n-\t\t\tzone_device_private_split_cb(folio, new_folio);\n+\t\t\tfolio_managed_split_cb(folio, new_folio);\n \n \t\t\tfolio_ref_unfreeze(new_folio,\n \t\t\t\t\t   folio_cache_ref_count(new_folio) + 1);\n@@ -3889,7 +3890,8 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \t\t\tfolio_put_refs(new_folio, nr_pages);\n \t\t}\n \n-\t\tzone_device_private_split_cb(folio, NULL);\n+\t\tfolio_managed_split_cb(folio, NULL);\n+\n \t\t/*\n \t\t * Unfreeze @folio only after all page cache entries, which\n \t\t * used to point to it, have been updated with new folios.\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about user-driven migration to private nodes, explaining that ZONE_DEVICE always rejects it but private nodes should be able to opt in. They added the NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper to dispatch migration requests, allowing migrate_pages syscall to target private nodes.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "added new functionality",
                "acknowledged reviewer feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services may want to support user-driven migration\n(migrate_pages syscall, mbind) to allow data movement between regular\nand private nodes.\n\nZONE_DEVICE always rejects user migration, but private nodes should\nbe able to opt in.\n\nAdd NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper that\ndispatches migration requests.  Private nodes can either set the flag\nand provide a custom migrate_to callback for driver-managed migration.\n\nIn migrate_to_node(), allows GFP_PRIVATE when the destination node\nsupports NP_OPS_MIGRATION, enabling migrate_pages syscall to target\nprivate nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |   4 ++\n include/linux/migrate.h      |  10 +++\n include/linux/node_private.h | 122 +++++++++++++++++++++++++++++++++++\n mm/damon/paddr.c             |   3 +\n mm/internal.h                |  24 +++++++\n mm/mempolicy.c               |  10 +--\n mm/migrate.c                 |  49 ++++++++++----\n mm/rmap.c                    |   4 +-\n 8 files changed, 206 insertions(+), 20 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 646dc48a23b5..e587f5781135 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -949,6 +949,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \tif (!node_possible(nid))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MIGRATION) &&\n+\t    (!ops->migrate_to || !ops->folio_migrate))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 26ca00c325d9..7b2da3875ff2 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -71,6 +71,9 @@ void folio_migrate_flags(struct folio *newfolio, struct folio *folio);\n int folio_migrate_mapping(struct address_space *mapping,\n \t\tstruct folio *newfolio, struct folio *folio, int extra_count);\n int set_movable_ops(const struct movable_operations *ops, enum pagetype type);\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason);\n \n #else\n \n@@ -96,6 +99,13 @@ static inline int set_movable_ops(const struct movable_operations *ops, enum pag\n {\n \treturn -ENOSYS;\n }\n+static inline int migrate_folios_to_node(struct list_head *folios,\n+\t\t\t\t\t\t  int nid,\n+\t\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t\t  enum migrate_reason reason)\n+{\n+\treturn -ENOSYS;\n+}\n \n #endif /* CONFIG_MIGRATION */\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex f9dd2d25c8a5..0c5be1ee6e60 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -4,6 +4,7 @@\n \n #include <linux/completion.h>\n #include <linux/memremap.h>\n+#include <linux/migrate_mode.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -52,15 +53,40 @@ struct vm_fault;\n  *     or NULL when called for the final (original) folio after all sub-folios\n  *     have been split off.\n  *\n+ * @migrate_to: Migrate folios TO this node.\n+ *\t[refcounted callback]\n+ *\tReturns: 0 on full success, >0 = number of folios that failed to\n+ *\t\t migrate, <0 = error.  Matches migrate_pages() semantics.\n+ *\t\t @nr_succeeded is set to the number of successfully migrated\n+ *\t\t folios (may be NULL if caller doesn't need it).\n+ *\n+ * @folio_migrate: Post-migration notification that a folio on this private node\n+ *    changed physical location (on the same node or a different node).\n+ *    [folio-referenced callback]\n+ *     Called from migrate_folio_move() after data has been copied but before\n+ *     migration entries are replaced with real PTEs.  Both @src and @dst are\n+ *     locked.  Faults block in migration_entry_wait() until\n+ *     remove_migration_ptes() runs, so the service can safely update\n+ *     PFN-based metadata (compression tables, device page tables, DMA\n+ *     mappings, etc.) before any access through the page tables.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n \tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n+\tint (*migrate_to)(struct list_head *folios, int nid,\n+\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t  unsigned int *nr_succeeded);\n+\tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tunsigned long flags;\n };\n \n+/* Allow user/kernel migration; requires migrate_to and folio_migrate */\n+#define NP_OPS_MIGRATION\t\tBIT(0)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\n@@ -177,6 +203,81 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n \t\tnode_private_split_cb(original_folio, new_folio);\n }\n \n+#ifdef CONFIG_MEMORY_HOTPLUG\n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn -ENOENT;\n+\treturn node_private_has_flag(folio_nid(folio), NP_OPS_MIGRATION) ?\n+\t       folio_nid(folio) : -ENOENT;\n+}\n+\n+/**\n+ * folio_managed_allows_migrate - Check if a managed folio supports migration\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio can be migrated.  For zone_device folios, only\n+ * device_private and device_coherent support migration.  For private node\n+ * folios, migration requires NP_OPS_MIGRATION.  Normal folios always\n+ * return true.\n+ */\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\tif (folio_is_private_node(folio))\n+\t\treturn folio_private_flags(folio, NP_OPS_MIGRATION);\n+\treturn true;\n+}\n+\n+/**\n+ * node_private_migrate_to - Attempt service-specific migration to a private node\n+ * @folios: list of folios to migrate (may sleep)\n+ * @nid: target node\n+ * @mode: migration mode (MIGRATE_ASYNC, MIGRATE_SYNC, etc.)\n+ * @reason: migration reason (MR_DEMOTION, MR_SYSCALL, etc.)\n+ * @nr_succeeded: optional output for number of successfully migrated folios\n+ *\n+ * If @nid is an N_MEMORY_PRIVATE node with a migrate_to callback,\n+ * invokes the callback and returns the result with migrate_pages()\n+ * semantics (0 = full success, >0 = failure count, <0 = error).\n+ * Returns -ENODEV if the node is not private or the service is being\n+ * torn down.\n+ *\n+ * The source folios are on other nodes, so they do not pin the target\n+ * node's node_private.  A temporary refcount is taken under rcu_read_lock\n+ * to keep node_private (and the service module) alive across the callback.\n+ */\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\tint (*fn)(struct list_head *, int, enum migrate_mode,\n+\t\t  enum migrate_reason, unsigned int *);\n+\tstruct node_private *np;\n+\tint ret;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (!np || !np->ops || !np->ops->migrate_to ||\n+\t    !refcount_inc_not_zero(&np->refcount)) {\n+\t\trcu_read_unlock();\n+\t\treturn -ENODEV;\n+\t}\n+\tfn = np->ops->migrate_to;\n+\trcu_read_unlock();\n+\n+\tret = fn(folios, nid, mode, reason, nr_succeeded);\n+\n+\tif (refcount_dec_and_test(&np->refcount))\n+\t\tcomplete(&np->released);\n+\n+\treturn ret;\n+}\n+#endif /* CONFIG_MEMORY_HOTPLUG */\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -242,6 +343,27 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\treturn -ENOENT;\n+}\n+\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\treturn true;\n+}\n+\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\treturn -ENODEV;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/damon/paddr.c b/mm/damon/paddr.c\nindex 07a8aead439e..532b8e2c62b0 100644\n--- a/mm/damon/paddr.c\n+++ b/mm/damon/paddr.c\n@@ -277,6 +277,9 @@ static unsigned long damon_pa_migrate(struct damon_region *r,\n \t\telse\n \t\t\t*sz_filter_passed += folio_size(folio) / addr_unit;\n \n+\t\tif (!folio_managed_allows_migrate(folio))\n+\t\t\tgoto put_folio;\n+\n \t\tif (!folio_isolate_lru(folio))\n \t\t\tgoto put_folio;\n \t\tlist_add(&folio->lru, &folio_list);\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 658da41cdb8e..6ab4679fe943 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1442,6 +1442,30 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/**\n+ * folio_managed_migrate_notify - Notify service that a folio changed location\n+ * @src: the old folio (about to be freed)\n+ * @dst: the new folio (data already copied, migration entries still in place)\n+ *\n+ * Called from migrate_folio_move() after data has been copied but before\n+ * remove_migration_ptes() installs real PTEs pointing to @dst.  While\n+ * migration entries are in place, faults block in migration_entry_wait(),\n+ * so the service can safely update PFN-based metadata before any access\n+ * through the page tables.  Both @src and @dst are locked.\n+ */\n+static inline void folio_managed_migrate_notify(struct folio *src,\n+\t\t\t\t\t\tstruct folio *dst)\n+{\n+\tconst struct node_private_ops *ops;\n+\n+\tif (!folio_is_private_node(src))\n+\t\treturn;\n+\n+\tops = folio_node_private_ops(src);\n+\tif (ops && ops->folio_migrate)\n+\t\tops->folio_migrate(src, dst);\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 68a98ba57882..2b0f9762d171 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -111,6 +111,7 @@\n #include <linux/mmu_notifier.h>\n #include <linux/printk.h>\n #include <linux/leafops.h>\n+#include <linux/node_private.h>\n #include <linux/gcd.h>\n \n #include <asm/tlbflush.h>\n@@ -1282,11 +1283,6 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tLIST_HEAD(pagelist);\n \tlong nr_failed;\n \tlong err = 0;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = dest,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n \tnodes_clear(nmask);\n \tnode_set(source, nmask);\n@@ -1311,8 +1307,8 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tmmap_read_unlock(mm);\n \n \tif (!list_empty(&pagelist)) {\n-\t\terr = migrate_pages(&pagelist, alloc_migration_target, NULL,\n-\t\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\t\terr = migrate_folios_to_node(&pagelist, dest, MIGRATE_SYNC,\n+\t\t\t\t\t     MR_SYSCALL);\n \t\tif (err)\n \t\t\tputback_movable_pages(&pagelist);\n \t}\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 5169f9717f60..a54d4af04df3 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -43,6 +43,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/memory-tiers.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1387,6 +1388,8 @@ static int migrate_folio_move(free_folio_t put_new_folio, unsigned long private,\n \tif (old_page_state & PAGE_WAS_MLOCKED)\n \t\tlru_add_drain();\n \n+\tfolio_managed_migrate_notify(src, dst);\n+\n \tif (old_page_state & PAGE_WAS_MAPPED)\n \t\tremove_migration_ptes(src, dst, 0);\n \n@@ -2165,6 +2168,7 @@ int migrate_pages(struct list_head *from, new_folio_t get_new_folio,\n \n \treturn rc_gather;\n }\n+EXPORT_SYMBOL_GPL(migrate_pages);\n \n struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n {\n@@ -2204,6 +2208,31 @@ struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n \n \treturn __folio_alloc(gfp_mask, order, nid, mtc->nmask);\n }\n+EXPORT_SYMBOL_GPL(alloc_migration_target);\n+\n+static int __migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, NULL);\n+}\n+\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason)\n+{\n+\tif (node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_private_migrate_to(folios, nid, mode,\n+\t\t\t\t\t       reason, NULL);\n+\treturn __migrate_folios_to_node(folios, nid, mode, reason);\n+}\n \n #ifdef CONFIG_NUMA\n \n@@ -2221,14 +2250,8 @@ static int store_status(int __user *status, int start, int value, int nr)\n static int do_move_pages_to_node(struct list_head *pagelist, int node)\n {\n \tint err;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = node,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n-\terr = migrate_pages(pagelist, alloc_migration_target, NULL,\n-\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\terr = migrate_folios_to_node(pagelist, node, MIGRATE_SYNC, MR_SYSCALL);\n \tif (err)\n \t\tputback_movable_pages(pagelist);\n \treturn err;\n@@ -2240,7 +2263,7 @@ static int __add_folio_for_migration(struct folio *folio, int node,\n \tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\treturn -EFAULT;\n \n-\tif (folio_is_zone_device(folio))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn -ENOENT;\n \n \tif (folio_nid(folio) == node)\n@@ -2364,7 +2387,8 @@ static int do_pages_move(struct mm_struct *mm, nodemask_t task_nodes,\n \t\terr = -ENODEV;\n \t\tif (node < 0 || node >= MAX_NUMNODES)\n \t\t\tgoto out_flush;\n-\t\tif (!node_state(node, N_MEMORY))\n+\t\tif (!node_state(node, N_MEMORY) &&\n+\t\t    !node_state(node, N_MEMORY_PRIVATE))\n \t\t\tgoto out_flush;\n \n \t\terr = -EACCES;\n@@ -2449,8 +2473,8 @@ static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n \t\tif (folio) {\n \t\t\tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\t\t\terr = -EFAULT;\n-\t\t\telse if (folio_is_zone_device(folio))\n-\t\t\t\terr = -ENOENT;\n+\t\t\telse if (unlikely(folio_is_private_managed(folio)))\n+\t\t\t\terr = folio_managed_allows_user_migrate(folio);\n \t\t\telse\n \t\t\t\terr = folio_nid(folio);\n \t\t\tfolio_walk_end(&fw, vma);\n@@ -2660,6 +2684,9 @@ int migrate_misplaced_folio_prepare(struct folio *folio,\n \tint nr_pages = folio_nr_pages(folio);\n \tpg_data_t *pgdat = NODE_DATA(node);\n \n+\tif (!folio_managed_allows_migrate(folio))\n+\t\treturn -ENOENT;\n+\n \tif (folio_is_file_lru(folio)) {\n \t\t/*\n \t\t * Do not migrate file folios that are mapped in multiple\ndiff --git a/mm/rmap.c b/mm/rmap.c\nindex f955f02d570e..805f9ceb82f3 100644\n--- a/mm/rmap.c\n+++ b/mm/rmap.c\n@@ -72,6 +72,7 @@\n #include <linux/backing-dev.h>\n #include <linux/page_idle.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/userfaultfd_k.h>\n #include <linux/mm_inline.h>\n #include <linux/oom.h>\n@@ -2616,8 +2617,7 @@ void try_to_migrate(struct folio *folio, enum ttu_flags flags)\n \t\t\t\t\tTTU_SYNC | TTU_BATCH_FLUSH)))\n \t\treturn;\n \n-\tif (folio_is_zone_device(folio) &&\n-\t    (!folio_is_device_private(folio) && !folio_is_device_coherent(folio)))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn;\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about allowing userland to directly allocate from private nodes via set_mempolicy() and mbind(), but not wanting those nodes as normal allocable system memory in the fallback lists. The author added a flag NP_OPS_MEMPOLICY requiring NP_OPS_MIGRATION, updated sysfs 'has_memory' attribute, and modified mempolicy migration sites to use __GFP_PRIVATE.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private nodes want userland to directly allocate from the node\nvia set_mempolicy() and mbind() - but don't want that node as normal\nallocable system memory in the fallback lists.\n\nAdd NP_OPS_MEMPOLICY flag requiring NP_OPS_MIGRATION (since mbind can\ndrive migrations).  Only allow private nodes in policy nodemasks if\nall private nodes in the mask support NP_OPS_MEMPOLICY. This prevents\n__GFP_PRIVATE from unlocking nodes without NP_OPS_MEMPOLICY support.\n\nAdd __GFP_PRIVATE to mempolicy migration sites so moves to opted-in\nprivate nodes succeed.\n\nUpdate the sysfs \"has_memory\" attribute to include N_MEMORY_PRIVATE\nnodes with NP_OPS_MEMPOLICY set, allowing existing numactl userland\ntools to work without modification.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c            | 22 +++++++++++++-\n include/linux/node_private.h   | 40 +++++++++++++++++++++++++\n include/uapi/linux/mempolicy.h |  1 +\n mm/mempolicy.c                 | 54 ++++++++++++++++++++++++++++++----\n mm/page_alloc.c                |  5 ++++\n 5 files changed, 116 insertions(+), 6 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex e587f5781135..c08b5a948779 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -953,6 +953,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (!ops->migrate_to || !ops->folio_migrate))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\n@@ -1145,6 +1149,21 @@ static ssize_t show_node_state(struct device *dev,\n \t\t\t  nodemask_pr_args(&node_states[na->state]));\n }\n \n+/* has_memory includes N_MEMORY + N_MEMORY_PRIVATE that support mempolicy. */\n+static ssize_t show_has_memory(struct device *dev,\n+\t\t\t       struct device_attribute *attr, char *buf)\n+{\n+\tnodemask_t mask = node_states[N_MEMORY];\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_set(nid, mask);\n+\t}\n+\n+\treturn sysfs_emit(buf, \"%*pbl\\n\", nodemask_pr_args(&mask));\n+}\n+\n #define _NODE_ATTR(name, state) \\\n \t{ __ATTR(name, 0444, show_node_state, NULL), state }\n \n@@ -1155,7 +1174,8 @@ static struct node_attr node_state_attr[] = {\n #ifdef CONFIG_HIGHMEM\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n-\t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY] = { __ATTR(has_memory, 0444, show_has_memory, NULL),\n+\t\t       N_MEMORY },\n \t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 0c5be1ee6e60..e9b58afa366b 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -86,6 +86,8 @@ struct node_private_ops {\n \n /* Allow user/kernel migration; requires migrate_to and folio_migrate */\n #define NP_OPS_MIGRATION\t\tBIT(0)\n+/* Allow mempolicy-directed allocation and mbind migration to this node */\n+#define NP_OPS_MEMPOLICY\t\tBIT(1)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -276,6 +278,34 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \n \treturn ret;\n }\n+\n+static inline bool node_mpol_eligible(int nid)\n+{\n+\tbool ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_state(nid, N_MEMORY);\n+\n+\trcu_read_lock();\n+\tret = node_private_has_flag(nid, NP_OPS_MEMPOLICY);\n+\trcu_read_unlock();\n+\treturn ret;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\tint nid;\n+\tbool eligible = false;\n+\n+\tfor_each_node_mask(nid, *nodes) {\n+\t\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\t\tcontinue;\n+\t\tif (!node_mpol_eligible(nid))\n+\t\t\treturn false;\n+\t\teligible = true;\n+\t}\n+\treturn eligible;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -364,6 +394,16 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \treturn -ENODEV;\n }\n \n+static inline bool node_mpol_eligible(int nid)\n+{\n+\treturn false;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/include/uapi/linux/mempolicy.h b/include/uapi/linux/mempolicy.h\nindex 8fbbe613611a..b606eae983c8 100644\n--- a/include/uapi/linux/mempolicy.h\n+++ b/include/uapi/linux/mempolicy.h\n@@ -64,6 +64,7 @@ enum {\n #define MPOL_F_SHARED  (1 << 0)\t/* identify shared policies */\n #define MPOL_F_MOF\t(1 << 3) /* this policy wants migrate on fault */\n #define MPOL_F_MORON\t(1 << 4) /* Migrate On protnone Reference On Node */\n+#define MPOL_F_PRIVATE\t(1 << 5) /* policy targets private node; use __GFP_PRIVATE */\n \n /*\n  * Enabling zone reclaim means the page allocator will attempt to fulfill\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 2b0f9762d171..8ac014950e88 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -406,8 +406,6 @@ static int mpol_new_preferred(struct mempolicy *pol, const nodemask_t *nodes)\n static int mpol_set_nodemask(struct mempolicy *pol,\n \t\t     const nodemask_t *nodes, struct nodemask_scratch *nsc)\n {\n-\tint ret;\n-\n \t/*\n \t * Default (pol==NULL) resp. local memory policies are not a\n \t * subject of any remapping. They also do not need any special\n@@ -416,9 +414,12 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \tif (!pol || pol->mode == MPOL_LOCAL)\n \t\treturn 0;\n \n-\t/* Check N_MEMORY */\n+\t/* Check N_MEMORY and N_MEMORY_PRIVATE*/\n \tnodes_and(nsc->mask1,\n \t\t  cpuset_current_mems_allowed, node_states[N_MEMORY]);\n+\tnodes_and(nsc->mask2, cpuset_current_mems_allowed,\n+\t\t  node_states[N_MEMORY_PRIVATE]);\n+\tnodes_or(nsc->mask1, nsc->mask1, nsc->mask2);\n \n \tVM_BUG_ON(!nodes);\n \n@@ -432,8 +433,13 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \telse\n \t\tpol->w.cpuset_mems_allowed = cpuset_current_mems_allowed;\n \n-\tret = mpol_ops[pol->mode].create(pol, &nsc->mask2);\n-\treturn ret;\n+\t/* All private nodes in the mask must have NP_OPS_MEMPOLICY. */\n+\tif (nodes_private_mpol_allowed(&nsc->mask2))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse if (nodes_intersects(nsc->mask2, node_states[N_MEMORY_PRIVATE]))\n+\t\treturn -EINVAL;\n+\n+\treturn mpol_ops[pol->mode].create(pol, &nsc->mask2);\n }\n \n /*\n@@ -500,6 +506,7 @@ static void mpol_rebind_default(struct mempolicy *pol, const nodemask_t *nodes)\n static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n {\n \tnodemask_t tmp;\n+\tint nid;\n \n \tif (pol->flags & MPOL_F_STATIC_NODES)\n \t\tnodes_and(tmp, pol->w.user_nodemask, *nodes);\n@@ -514,6 +521,21 @@ static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n \tif (nodes_empty(tmp))\n \t\ttmp = *nodes;\n \n+\t/*\n+\t * Drop private nodes that don't have mempolicy support.\n+\t * cpusets guarantees at least one N_MEMORY node in effective_mems\n+\t * and mems_allowed, so dropping private nodes here is safe.\n+\t */\n+\tfor_each_node_mask(nid, tmp) {\n+\t\tif (node_state(nid, N_MEMORY_PRIVATE) &&\n+\t\t    !node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_clear(nid, tmp);\n+\t}\n+\tif (nodes_intersects(tmp, node_states[N_MEMORY_PRIVATE]))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse\n+\t\tpol->flags &= ~MPOL_F_PRIVATE;\n+\n \tpol->nodes = tmp;\n }\n \n@@ -661,6 +683,9 @@ static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)\n \t}\n \tif (!queue_folio_required(folio, qp))\n \t\treturn;\n+\tif (folio_is_private_node(folio) &&\n+\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\treturn;\n \tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||\n \t    !vma_migratable(walk->vma) ||\n \t    !migrate_folio_add(folio, qp->pagelist, qp->flags))\n@@ -717,6 +742,9 @@ static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n \t\tif (!folio || folio_is_zone_device(folio))\n \t\t\tcontinue;\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\t\tcontinue;\n \t\tif (folio_test_large(folio) && max_nr != 1)\n \t\t\tnr = folio_pte_batch(folio, pte, ptent, max_nr);\n \t\t/*\n@@ -1451,6 +1479,9 @@ static struct folio *alloc_migration_target_by_mpol(struct folio *src,\n \telse\n \t\tgfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL | __GFP_COMP;\n \n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \treturn folio_alloc_mpol(gfp, order, pol, ilx, nid);\n }\n #else\n@@ -2280,6 +2311,15 @@ static nodemask_t *policy_nodemask(gfp_t gfp, struct mempolicy *pol,\n \t\t\tnodemask = &pol->nodes;\n \t\tif (pol->home_node != NUMA_NO_NODE)\n \t\t\t*nid = pol->home_node;\n+\t\telse if ((pol->flags & MPOL_F_PRIVATE) &&\n+\t\t\t !node_isset(*nid, pol->nodes)) {\n+\t\t\t/*\n+\t\t\t * Private nodes are not in N_MEMORY nodes' zonelists.\n+\t\t\t * When the preferred nid (usually numa_node_id()) can't\n+\t\t\t * reach the policy nodes, start from a policy node.\n+\t\t\t */\n+\t\t\t*nid = first_node(pol->nodes);\n+\t\t}\n \t\t/*\n \t\t * __GFP_THISNODE shouldn't even be used with the bind policy\n \t\t * because we might easily break the expectation to stay on the\n@@ -2533,6 +2573,10 @@ struct folio *vma_alloc_folio_noprof(gfp_t gfp, int order, struct vm_area_struct\n \t\tgfp |= __GFP_NOWARN;\n \n \tpol = get_vma_policy(vma, addr, order, &ilx);\n+\n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \tfolio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n \tmpol_cond_put(pol);\n \treturn folio;\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 5a1b35421d78..ec6c1f8e85d8 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3849,8 +3849,13 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\t * if another process has NUMA bindings and is causing\n \t\t * kswapd wakeups on only some nodes. Avoid accidental\n \t\t * \"node_reclaim_mode\"-like behavior in this case.\n+\t\t *\n+\t\t * Nodes without kswapd (some private nodes) are never\n+\t\t * skipped - this causes some mempolicies to silently\n+\t\t * fall back to DRAM even if the node is eligible.\n \t\t */\n \t\tif (skip_kswapd_nodes &&\n+\t\t    zone->zone_pgdat->kswapd &&\n \t\t    !waitqueue_active(&zone->zone_pgdat->kswapd_wait)) {\n \t\t\tskipped_kswapd_nodes = true;\n \t\t\tcontinue;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private nodes being used as demotion targets in the memory-tiers subsystem, agreeing that they should be added to the demotion target mask and implementing backpressure support to allow vmscan to fall back to swap. The author also acknowledged that the current demotion logic induces LRU inversions and suggested re-doing it to allow less fallback and kick kswapd instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The memory-tier subsystem needs to know which private nodes should\nappear as demotion targets.\n\nAdd NP_OPS_DEMOTION (BIT(2)):\n   Node can be added as a demotion target by memory-tiers.\n\nAdd demotion backpressure support so private nodes can reject\nnew demotions cleanly, allowing vmscan to fall back to swap.\n\nIn the demotion path, try demotion to private nodes invididually,\nthen clear private nodes from the demotion target mask until a\nnon-private node is found, then fall back to the remaining mask.\nThis prevents LRU inversion while still allowing forward progress.\n\nThis is the closest match to the current behavior without making\nprivate nodes inaccessible or preventing forward progress. We\nshould probably completely re-do the demotion logic to allow less\nfallback and kick kswapd instead - right now we induce LRU\ninversions by simply falling back to any node in the demotion list.\n\nAdd memory_tier_refresh_demotion() export for services to trigger\nre-evaluation of demotion targets after changing their flags.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory-tiers.h |  9 +++++++\n include/linux/node_private.h | 22 +++++++++++++++++\n mm/internal.h                |  7 ++++++\n mm/memory-tiers.c            | 46 ++++++++++++++++++++++++++++++++----\n mm/page_alloc.c              | 12 +++++++---\n mm/vmscan.c                  | 30 ++++++++++++++++++++++-\n 6 files changed, 117 insertions(+), 9 deletions(-)\n\ndiff --git a/include/linux/memory-tiers.h b/include/linux/memory-tiers.h\nindex 3e1159f6762c..e1476432e359 100644\n--- a/include/linux/memory-tiers.h\n+++ b/include/linux/memory-tiers.h\n@@ -58,6 +58,7 @@ struct memory_dev_type *mt_get_memory_type(int adist);\n int next_demotion_node(int node);\n void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets);\n bool node_is_toptier(int node);\n+void memory_tier_refresh_demotion(void);\n #else\n static inline int next_demotion_node(int node)\n {\n@@ -73,6 +74,10 @@ static inline bool node_is_toptier(int node)\n {\n \treturn true;\n }\n+\n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n #endif\n \n #else\n@@ -106,6 +111,10 @@ static inline bool node_is_toptier(int node)\n \treturn true;\n }\n \n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n+\n static inline int register_mt_adistance_algorithm(struct notifier_block *nb)\n {\n \treturn 0;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e9b58afa366b..e254e36056cd 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -88,6 +88,8 @@ struct node_private_ops {\n #define NP_OPS_MIGRATION\t\tBIT(0)\n /* Allow mempolicy-directed allocation and mbind migration to this node */\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n+/* Node participates as a demotion target in memory-tiers */\n+#define NP_OPS_DEMOTION\t\t\tBIT(2)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -101,12 +103,14 @@ struct node_private_ops {\n  *\t\tcallbacks that may sleep; 0 = fully released)\n  * @released: Signaled when refcount drops to 0; unregister waits on this\n  * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ * @migration_blocked: Service signals migrations should pause\n  */\n struct node_private {\n \tvoid *owner;\n \trefcount_t refcount;\n \tstruct completion released;\n \tconst struct node_private_ops *ops;\n+\tbool migration_blocked;\n };\n \n #ifdef CONFIG_NUMA\n@@ -306,6 +310,19 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \t}\n \treturn eligible;\n }\n+\n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\tstruct node_private *np;\n+\tbool blocked;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tblocked = np && READ_ONCE(np->migration_blocked);\n+\trcu_read_unlock();\n+\n+\treturn blocked;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -404,6 +421,11 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \treturn false;\n }\n \n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 6ab4679fe943..5950e20d4023 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t  const nodemask_t *candidates);\n extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t      gfp_t gfp_mask);\n #else\n@@ -1220,6 +1222,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t\t const nodemask_t *candidates)\n+{\n+\treturn NUMA_NO_NODE;\n+}\n static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t\t     gfp_t gfp_mask)\n {\ndiff --git a/mm/memory-tiers.c b/mm/memory-tiers.c\nindex 9c742e18e48f..434190fdc078 100644\n--- a/mm/memory-tiers.c\n+++ b/mm/memory-tiers.c\n@@ -3,6 +3,7 @@\n #include <linux/lockdep.h>\n #include <linux/sysfs.h>\n #include <linux/kobject.h>\n+#include <linux/node_private.h>\n #include <linux/memory.h>\n #include <linux/memory-tiers.h>\n #include <linux/notifier.h>\n@@ -380,6 +381,8 @@ static void disable_all_demotion_targets(void)\n \t\tif (memtier)\n \t\t\tmemtier->lower_tier_mask = NODE_MASK_NONE;\n \t}\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE)\n+\t\tnode_demotion[node].preferred = NODE_MASK_NONE;\n \t/*\n \t * Ensure that the \"disable\" is visible across the system.\n \t * Readers will see either a combination of before+disable\n@@ -421,6 +424,7 @@ static void establish_demotion_targets(void)\n \tint target = NUMA_NO_NODE, node;\n \tint distance, best_distance;\n \tnodemask_t tier_nodes, lower_tier;\n+\tnodemask_t all_memory;\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n@@ -429,6 +433,13 @@ static void establish_demotion_targets(void)\n \n \tdisable_all_demotion_targets();\n \n+\t/* Include private nodes that have opted in to demotion. */\n+\tall_memory = node_states[N_MEMORY];\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(node, NP_OPS_DEMOTION))\n+\t\t\tnode_set(node, all_memory);\n+\t}\n+\n \tfor_each_node_state(node, N_MEMORY) {\n \t\tbest_distance = -1;\n \t\tnd = &node_demotion[node];\n@@ -442,12 +453,12 @@ static void establish_demotion_targets(void)\n \t\tmemtier = list_next_entry(memtier, list);\n \t\ttier_nodes = get_memtier_nodemask(memtier);\n \t\t/*\n-\t\t * find_next_best_node, use 'used' nodemask as a skip list.\n+\t\t * find_next_best_node_in, use 'used' nodemask as a skip list.\n \t\t * Add all memory nodes except the selected memory tier\n \t\t * nodelist to skip list so that we find the best node from the\n \t\t * memtier nodelist.\n \t\t */\n-\t\tnodes_andnot(tier_nodes, node_states[N_MEMORY], tier_nodes);\n+\t\tnodes_andnot(tier_nodes, all_memory, tier_nodes);\n \n \t\t/*\n \t\t * Find all the nodes in the memory tier node list of same best distance.\n@@ -455,7 +466,8 @@ static void establish_demotion_targets(void)\n \t\t * in the preferred mask when allocating pages during demotion.\n \t\t */\n \t\tdo {\n-\t\t\ttarget = find_next_best_node(node, &tier_nodes);\n+\t\t\ttarget = find_next_best_node_in(node, &tier_nodes,\n+\t\t\t\t\t\t\t&all_memory);\n \t\t\tif (target == NUMA_NO_NODE)\n \t\t\t\tbreak;\n \n@@ -495,7 +507,7 @@ static void establish_demotion_targets(void)\n \t * allocation to a set of nodes that is closer the above selected\n \t * preferred node.\n \t */\n-\tlower_tier = node_states[N_MEMORY];\n+\tlower_tier = all_memory;\n \tlist_for_each_entry(memtier, &memory_tiers, list) {\n \t\t/*\n \t\t * Keep removing current tier from lower_tier nodes,\n@@ -542,7 +554,7 @@ static struct memory_tier *set_node_memory_tier(int node)\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n-\tif (!node_state(node, N_MEMORY))\n+\tif (!node_state(node, N_MEMORY) && !node_state(node, N_MEMORY_PRIVATE))\n \t\treturn ERR_PTR(-EINVAL);\n \n \tmt_calc_adistance(node, &adist);\n@@ -865,6 +877,30 @@ int mt_calc_adistance(int node, int *adist)\n }\n EXPORT_SYMBOL_GPL(mt_calc_adistance);\n \n+/**\n+ * memory_tier_refresh_demotion() - Re-establish demotion targets\n+ *\n+ * Called by services after registering or unregistering ops->migrate_to on\n+ * a private node, so that establish_demotion_targets() picks up the change.\n+ */\n+void memory_tier_refresh_demotion(void)\n+{\n+\tint nid;\n+\n+\tmutex_lock(&memory_tier_lock);\n+\t/*\n+\t * Ensure private nodes are registered with a tier, otherwise\n+\t * they won't show up in any node's demotion targets nodemask.\n+\t */\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (!__node_get_memory_tier(nid))\n+\t\t\tset_node_memory_tier(nid);\n+\t}\n+\testablish_demotion_targets();\n+\tmutex_unlock(&memory_tier_lock);\n+}\n+EXPORT_SYMBOL_GPL(memory_tier_refresh_demotion);\n+\n static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\t\t\t\t      unsigned long action, void *_arg)\n {\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex ec6c1f8e85d8..e272dfdc6b00 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5589,7 +5589,8 @@ static int node_load[MAX_NUMNODES];\n  *\n  * Return: node id of the found node or %NUMA_NO_NODE if no node is found.\n  */\n-int find_next_best_node(int node, nodemask_t *used_node_mask)\n+int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t   const nodemask_t *candidates)\n {\n \tint n, val;\n \tint min_val = INT_MAX;\n@@ -5599,12 +5600,12 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \t * Use the local node if we haven't already, but for memoryless local\n \t * node, we should skip it and fall back to other nodes.\n \t */\n-\tif (!node_isset(node, *used_node_mask) && node_state(node, N_MEMORY)) {\n+\tif (!node_isset(node, *used_node_mask) && node_isset(node, *candidates)) {\n \t\tnode_set(node, *used_node_mask);\n \t\treturn node;\n \t}\n \n-\tfor_each_node_state(n, N_MEMORY) {\n+\tfor_each_node_mask(n, *candidates) {\n \n \t\t/* Don't want a node to appear more than once */\n \t\tif (node_isset(n, *used_node_mask))\n@@ -5636,6 +5637,11 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \treturn best_node;\n }\n \n+int find_next_best_node(int node, nodemask_t *used_node_mask)\n+{\n+\treturn find_next_best_node_in(node, used_node_mask,\n+\t\t\t\t      &node_states[N_MEMORY]);\n+}\n \n /*\n  * Build zonelists ordered by node and zones within node.\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 6113be4d3519..0f534428ea88 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -355,6 +356,10 @@ static bool can_demote(int nid, struct scan_control *sc,\n \tif (demotion_nid == NUMA_NO_NODE)\n \t\treturn false;\n \n+\t/* Don't demote when the target's service signals backpressure */\n+\tif (node_private_migration_blocked(demotion_nid))\n+\t\treturn false;\n+\n \t/* If demotion node isn't in the cgroup's mems_allowed, fall back */\n \treturn mem_cgroup_node_allowed(memcg, demotion_nid);\n }\n@@ -1022,8 +1027,10 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \t\t\t\t     struct pglist_data *pgdat)\n {\n \tint target_nid = next_demotion_node(pgdat->node_id);\n-\tunsigned int nr_succeeded;\n+\tint first_nid = target_nid;\n+\tunsigned int nr_succeeded = 0;\n \tnodemask_t allowed_mask;\n+\tint ret;\n \n \tstruct migration_target_control mtc = {\n \t\t/*\n@@ -1046,6 +1053,27 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \n \tnode_get_allowed_targets(pgdat, &allowed_mask);\n \n+\t/* Try private node targets until we find non-private node */\n+\twhile (node_state(target_nid, N_MEMORY_PRIVATE)) {\n+\t\tunsigned int nr = 0;\n+\n+\t\tret = node_private_migrate_to(demote_folios, target_nid,\n+\t\t\t\t\t      MIGRATE_ASYNC, MR_DEMOTION,\n+\t\t\t\t\t      &nr);\n+\t\tnr_succeeded += nr;\n+\t\tif (ret == 0 || list_empty(demote_folios))\n+\t\t\treturn nr_succeeded;\n+\n+\t\ttarget_nid = next_node_in(target_nid, allowed_mask);\n+\t\tif (target_nid == first_nid)\n+\t\t\treturn nr_succeeded;\n+\t\tif (!node_state(target_nid, N_MEMORY_PRIVATE))\n+\t\t\tbreak;\n+\t}\n+\n+\t/* target_nid is a non-private node; use standard migration */\n+\tmtc.nid = target_nid;\n+\n \t/* Demotion ignores all cpuset and mempolicy settings */\n \tmigrate_pages(demote_folios, alloc_demote_folio, NULL,\n \t\t      (unsigned long)&mtc, MIGRATE_ASYNC, MR_DEMOTION,\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about write faults on private nodes by adding a new operation flag NP_OPS_PROTECT_WRITE and modifying several functions to prevent PTEs from being upgraded to writable when the node is write-protected.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Services that intercept write faults (e.g., for promotion tracking)\nneed PTEs to stay read-only. This requires preventing mprotect\nfrom silently upgrade the PTE, bypassing the service's handle_fault\ncallback.\n\nAdd NP_OPS_PROTECT_WRITE and folio_managed_wrprotect().\n\nIn change_pte_range() and change_huge_pmd(), suppress PTE write-upgrade\nwhen MM_CP_TRY_CHANGE_WRITABLE is sees the folio is write-protected.\n\nIn handle_pte_fault() and do_huge_pmd_wp_page(), dispatch to the node's\nops->handle_fault callback when set, allowing the service to handle write\nfaults with promotion or other custom logic.\n\nNP_OPS_MEMPOLICY is incompatible with NP_OPS_PROTECT_WRITE to avoid the\nfootgun of binding a writable VMA to a write-protected node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++\n include/linux/node_private.h | 22 ++++++++\n mm/huge_memory.c             | 17 ++++++-\n mm/internal.h                | 99 ++++++++++++++++++++++++++++++++++++\n mm/memory.c                  | 15 ++++++\n mm/migrate.c                 | 14 +----\n mm/mprotect.c                |  4 +-\n 7 files changed, 159 insertions(+), 16 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex c08b5a948779..a4955b9b5b93 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -957,6 +957,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    (ops->flags & NP_OPS_PROTECT_WRITE))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e254e36056cd..27d6e5d84e61 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -70,6 +70,24 @@ struct vm_fault;\n  *     PFN-based metadata (compression tables, device page tables, DMA\n  *     mappings, etc.) before any access through the page tables.\n  *\n+ * @handle_fault: Handle fault on folio on this private node.\n+ *   [folio-referenced callback, PTL held on entry]\n+ *\n+ *   Called from handle_pte_fault() (PTE level) or do_huge_pmd_wp_page()\n+ *   (PMD level) after lock acquisition and entry verification.\n+ *   @folio is the faulting folio, @level indicates the page table level.\n+ *\n+ *   For PGTABLE_LEVEL_PTE: vmf->pte is mapped and vmf->ptl is the\n+ *   PTE lock.  Release via pte_unmap_unlock(vmf->pte, vmf->ptl).\n+ *\n+ *   For PGTABLE_LEVEL_PMD: vmf->pte is NULL and vmf->ptl is the\n+ *   PMD lock.  Release via spin_unlock(vmf->ptl).\n+ *\n+ *   The callback MUST release PTL on ALL paths.\n+ *   The caller will NOT touch the page table entry after this returns.\n+ *\n+ *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -81,6 +99,8 @@ struct node_private_ops {\n \t\t\t\t  enum migrate_reason reason,\n \t\t\t\t  unsigned int *nr_succeeded);\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n+\tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t   enum pgtable_level level);\n \tunsigned long flags;\n };\n \n@@ -90,6 +110,8 @@ struct node_private_ops {\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n /* Node participates as a demotion target in memory-tiers */\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n+/* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n+#define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 2ecae494291a..d9ba6593244d 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -2063,12 +2063,14 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tstruct page *page;\n \tunsigned long haddr = vmf->address & HPAGE_PMD_MASK;\n \tpmd_t orig_pmd = vmf->orig_pmd;\n+\tvm_fault_t ret;\n+\n \n \tvmf->ptl = pmd_lockptr(vma->vm_mm, vmf->pmd);\n \tVM_BUG_ON_VMA(!vma->anon_vma, vma);\n \n \tif (is_huge_zero_pmd(orig_pmd)) {\n-\t\tvm_fault_t ret = do_huge_zero_wp_pmd(vmf);\n+\t\tret = do_huge_zero_wp_pmd(vmf);\n \n \t\tif (!(ret & VM_FAULT_FALLBACK))\n \t\t\treturn ret;\n@@ -2088,6 +2090,13 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tfolio = page_folio(page);\n \tVM_BUG_ON_PAGE(!PageHead(page), page);\n \n+\t/* Private-managed write-protect: let the service handle the fault */\n+\tif (unlikely(folio_is_private_managed(folio))) {\n+\t\tif (folio_managed_handle_fault(folio, vmf,\n+\t\t\t\t\t      PGTABLE_LEVEL_PMD, &ret))\n+\t\t\treturn ret;\n+\t}\n+\n \t/* Early check when only holding the PT lock. */\n \tif (PageAnonExclusive(page))\n \t\tgoto reuse;\n@@ -2633,7 +2642,8 @@ int change_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,\n \n \t/* See change_pte_range(). */\n \tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) && !pmd_write(entry) &&\n-\t    can_change_pmd_writable(vma, addr, entry))\n+\t    can_change_pmd_writable(vma, addr, entry) &&\n+\t    !folio_managed_wrprotect(pmd_folio(entry)))\n \t\tentry = pmd_mkwrite(entry, vma);\n \n \tret = HPAGE_PMD_NR;\n@@ -4943,6 +4953,9 @@ void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n \tif (folio_test_dirty(folio) && softleaf_is_migration_dirty(entry))\n \t\tpmde = pmd_mkdirty(pmde);\n \n+\tif (folio_managed_wrprotect(folio))\n+\t\tpmde = pmd_wrprotect(pmde);\n+\n \tif (folio_is_device_private(folio)) {\n \t\tswp_entry_t entry;\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 5950e20d4023..ae4ff86e8dc6 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -11,6 +11,7 @@\n #include <linux/khugepaged.h>\n #include <linux/mm.h>\n #include <linux/mm_inline.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/pagewalk.h>\n #include <linux/rmap.h>\n@@ -18,6 +19,7 @@\n #include <linux/leafops.h>\n #include <linux/swap_cgroup.h>\n #include <linux/tracepoint-defs.h>\n+#include <linux/node_private.h>\n \n /* Internal core VMA manipulation functions. */\n #include \"vma.h\"\n@@ -1449,6 +1451,103 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/*\n+ * folio_managed_handle_fault - Dispatch fault on managed-memory folio\n+ * @folio: the faulting folio (must not be NULL)\n+ * @vmf: the vm_fault descriptor (PTL held: vmf->ptl locked)\n+ * @level: page table level (PGTABLE_LEVEL_PTE or PGTABLE_LEVEL_PMD)\n+ * @ret: output fault result if handled\n+ *\n+ * Called with PTL held.  If a handle_fault callback exists, it is invoked\n+ * with PTL still held.  The callback is responsible for releasing PTL on\n+ * all paths.\n+ *\n+ * Returns true if the service handled the fault (PTL released by callback,\n+ * caller returns *ret).  Returns false if no handler exists (PTL still held,\n+ * caller continues with normal fault handling).\n+ */\n+static inline bool folio_managed_handle_fault(struct folio *folio,\n+\t\t\t\t\t      struct vm_fault *vmf,\n+\t\t\t\t\t      enum pgtable_level level,\n+\t\t\t\t\t      vm_fault_t *ret)\n+{\n+\t/* Zone device pages use swap entries; handled in do_swap_page */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->handle_fault) {\n+\t\t\t*ret = ops->handle_fault(folio, vmf, level);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n+/**\n+ * folio_managed_wrprotect - Should this folio's mappings stay write-protected?\n+ * @folio: the folio to check\n+ *\n+ * Returns true if the folio is on a private node with NP_OPS_PROTECT_WRITE,\n+ * meaning page table entries (PTE or PMD) should not be made writable.\n+ * Write faults are intercepted by the service's handle_fault callback\n+ * to promote the folio to DRAM.\n+ *\n+ * Used by:\n+ *   - change_pte_range() / change_huge_pmd(): prevent mprotect write-upgrade\n+ *   - remove_migration_pte() / remove_migration_pmd(): strip write after migration\n+ *   - do_huge_pmd_wp_page(): dispatch to fault handler instead of reuse\n+ */\n+static inline bool folio_managed_wrprotect(struct folio *folio)\n+{\n+\treturn unlikely(folio_is_private_node(folio) &&\n+\t\t\tfolio_private_flags(folio, NP_OPS_PROTECT_WRITE));\n+}\n+\n+/**\n+ * folio_managed_fixup_migration_pte - Fixup PTE after migration for\n+ *                                     managed memory pages.\n+ * @new: the destination page\n+ * @pte: the PTE being installed (normal PTE built by caller)\n+ * @old_pte: the original PTE (before migration, for swap entry flags)\n+ * @vma: the VMA\n+ *\n+ * For MEMORY_DEVICE_PRIVATE pages: replaces the PTE with a device-private\n+ * swap entry, preserving soft_dirty and uffd_wp from old_pte.\n+ *\n+ * For N_MEMORY_PRIVATE pages with NP_OPS_PROTECT_WRITE: strips the write\n+ * bit so the next write triggers the fault handler for promotion.\n+ *\n+ * For normal pages: returns pte unmodified.\n+ */\n+static inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n+\t\t\t\t\t\t      pte_t pte,\n+\t\t\t\t\t\t      pte_t old_pte,\n+\t\t\t\t\t\t      struct vm_area_struct *vma)\n+{\n+\tif (unlikely(is_device_private_page(new))) {\n+\t\tsoftleaf_t entry;\n+\n+\t\tif (pte_write(pte))\n+\t\t\tentry = make_writable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\telse\n+\t\t\tentry = make_readable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\tpte = softleaf_to_pte(entry);\n+\t\tif (pte_swp_soft_dirty(old_pte))\n+\t\t\tpte = pte_swp_mksoft_dirty(pte);\n+\t\tif (pte_swp_uffd_wp(old_pte))\n+\t\t\tpte = pte_swp_mkuffd_wp(pte);\n+\t} else if (folio_managed_wrprotect(page_folio(new))) {\n+\t\tpte = pte_wrprotect(pte);\n+\t}\n+\treturn pte;\n+}\n+\n /**\n  * folio_managed_migrate_notify - Notify service that a folio changed location\n  * @src: the old folio (about to be freed)\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 2a55edc48a65..0f78988befef 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -6079,6 +6079,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n \t */\n+\tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n+\t\twritable = false;\n+\t\tignore_writable = true;\n+\t}\n \tif (folio && folio_test_large(folio))\n \t\tnuma_rebuild_large_mapping(vmf, vma, folio, pte, ignore_writable,\n \t\t\t\t\t   pte_write_upgrade);\n@@ -6228,6 +6232,7 @@ static void fix_spurious_fault(struct vm_fault *vmf,\n  */\n static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n {\n+\tstruct folio *folio;\n \tpte_t entry;\n \n \tif (unlikely(pmd_none(*vmf->pmd))) {\n@@ -6284,6 +6289,16 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n \t\tupdate_mmu_tlb(vmf->vma, vmf->address, vmf->pte);\n \t\tgoto unlock;\n \t}\n+\n+\tfolio = vm_normal_folio(vmf->vma, vmf->address, entry);\n+\tif (unlikely(folio && folio_is_private_managed(folio))) {\n+\t\tvm_fault_t fault_ret;\n+\n+\t\tif (folio_managed_handle_fault(folio, vmf, PGTABLE_LEVEL_PTE,\n+\t\t\t\t\t       &fault_ret))\n+\t\t\treturn fault_ret;\n+\t}\n+\n \tif (vmf->flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {\n \t\tif (!pte_write(entry))\n \t\t\treturn do_wp_page(vmf);\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex a54d4af04df3..f632e8b03504 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -398,19 +398,7 @@ static bool remove_migration_pte(struct folio *folio,\n \t\tif (folio_test_anon(folio) && !softleaf_is_migration_read(entry))\n \t\t\trmap_flags |= RMAP_EXCLUSIVE;\n \n-\t\tif (unlikely(is_device_private_page(new))) {\n-\t\t\tif (pte_write(pte))\n-\t\t\t\tentry = make_writable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\telse\n-\t\t\t\tentry = make_readable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\tpte = softleaf_to_pte(entry);\n-\t\t\tif (pte_swp_soft_dirty(old_pte))\n-\t\t\t\tpte = pte_swp_mksoft_dirty(pte);\n-\t\t\tif (pte_swp_uffd_wp(old_pte))\n-\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n-\t\t}\n+\t\tpte = folio_managed_fixup_migration_pte(new, pte, old_pte, vma);\n \n #ifdef CONFIG_HUGETLB_PAGE\n \t\tif (folio_test_hugetlb(folio)) {\ndiff --git a/mm/mprotect.c b/mm/mprotect.c\nindex 283889e4f1ce..830be609bc24 100644\n--- a/mm/mprotect.c\n+++ b/mm/mprotect.c\n@@ -30,6 +30,7 @@\n #include <linux/mm_inline.h>\n #include <linux/pgtable.h>\n #include <linux/userfaultfd_k.h>\n+#include <linux/node_private.h>\n #include <uapi/linux/mman.h>\n #include <asm/cacheflush.h>\n #include <asm/mmu_context.h>\n@@ -290,7 +291,8 @@ static long change_pte_range(struct mmu_gather *tlb,\n \t\t\t * COW or special handling is required.\n \t\t\t */\n \t\t\tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) &&\n-\t\t\t     !pte_write(ptent))\n+\t\t\t     !pte_write(ptent) &&\n+\t\t\t     !(folio && folio_managed_wrprotect(folio)))\n \t\t\t\tset_write_prot_commit_flush_ptes(vma, folio, page,\n \t\t\t\taddr, pte, oldpte, ptent, nr_ptes, tlb);\n \t\t\telse\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the reclaim policy for private nodes in boosted reclaim mode, explaining that it needs to allow swap and writepage operations. They proposed adding a reclaim_policy callback to struct node_private_ops and a struct node_reclaim_policy to configure these policies. The author also added zone_reclaim_allowed() to filter private nodes that have not opted into reclaim.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix is needed",
                "proposed changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services that drive kswapd via watermark_boost need\ncontrol over the reclaim policy.  There are three problems:\n\n1) Boosted reclaim suppresses may_swap and may_writepage.  When\n   demotion is not possible, swap is the only evict path, so kswapd\n   cannot make progress and pages are stranded.\n\n2) __setup_per_zone_wmarks() unconditionally zeros watermark_boost,\n   killing the service's pressure signal.\n\n3) Not all private nodes want reclaim to touch their pages.\n\nAdd a reclaim_policy callback to struct node_private_ops and a\nstruct node_reclaim_policy with:\n\n  - active:             set by the helper when a callback was invoked\n  - may_swap:           allow swap writeback during boosted reclaim\n  - may_writepage:      allow writepage during boosted reclaim\n  - managed_watermarks: service owns watermark_boost lifecycle\n\nWe do not allow disabling swap/writepage, as core MM may have\nexplicitly enabled them on a non-boosted pass.\n\nWe only allow enablign swap/writepage, so that the supression during\na boost can be overridden.  This allows a device to force evictions\neven when the system otherwise would not percieve pressure.\n\nThis is important for a service like compressed RAM, as device capacity\nmay differ from reported capacity, and device may want to relieve real\npressure (poor compression ratio) as opposed to percieved pressure\n(i.e. how many pages are in use).\n\nAdd zone_reclaim_allowed() to filter private nodes that have not\nopted into reclaim.\n\nRegular nodes fall through to cpuset_zone_allowed() unchanged.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 28 ++++++++++++++++++++++++++++\n mm/internal.h                | 36 ++++++++++++++++++++++++++++++++++++\n mm/page_alloc.c              | 11 ++++++++++-\n mm/vmscan.c                  | 25 +++++++++++++++++++++++--\n 4 files changed, 97 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 27d6e5d84e61..34be52383255 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -14,6 +14,24 @@ struct page;\n struct vm_area_struct;\n struct vm_fault;\n \n+/**\n+ * struct node_reclaim_policy - Reclaim policy overrides for private nodes\n+ * @active: set by node_private_reclaim_policy() when a callback was invoked\n+ * @may_swap: allow swap writeback during boosted reclaim\n+ * @may_writepage: allow writepage during boosted reclaim\n+ * @managed_watermarks: service owns watermark_boost lifecycle; kswapd must\n+ *                      not clear it after boosted reclaim\n+ *\n+ * Passed to the reclaim_policy callback so each private node service can\n+ * inject its own reclaim policy before kswapd runs boosted reclaim.\n+ */\n+struct node_reclaim_policy {\n+\tbool active;\n+\tbool may_swap;\n+\tbool may_writepage;\n+\tbool managed_watermarks;\n+};\n+\n /**\n  * struct node_private_ops - Callbacks for private node services\n  *\n@@ -88,6 +106,13 @@ struct vm_fault;\n  *\n  *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n  *\n+ * @reclaim_policy: Configure reclaim policy for boosted reclaim.\n+ *   [called hodling rcu_read_lock, MUST NOT sleep]\n+ *   Called by kswapd before boosted reclaim to let the service override\n+ *   may_swap / may_writepage.  If provided, the service also owns the\n+ *   watermark_boost lifecycle (kswapd will not clear it).\n+ *   If NULL, normal boost policy applies.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -101,6 +126,7 @@ struct node_private_ops {\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n+\tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n \tunsigned long flags;\n };\n \n@@ -112,6 +138,8 @@ struct node_private_ops {\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n /* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n+/* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n+#define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/internal.h b/mm/internal.h\nindex ae4ff86e8dc6..db32cb2d7a29 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1572,6 +1572,42 @@ static inline void folio_managed_migrate_notify(struct folio *src,\n \t\tops->folio_migrate(src, dst);\n }\n \n+/**\n+ * node_private_reclaim_policy - invoke the service's reclaim policy callback\n+ * @nid: NUMA node id\n+ * @policy: reclaim policy struct to fill in\n+ *\n+ * Called by kswapd before boosted reclaim.  Zeroes @policy, then if the\n+ * private node service provides a reclaim_policy callback, invokes it\n+ * and sets policy->active to true.\n+ */\n+#ifdef CONFIG_NUMA\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tstruct node_private *np;\n+\n+\tmemset(policy, 0, sizeof(*policy));\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (np && np->ops && np->ops->reclaim_policy) {\n+\t\tnp->ops->reclaim_policy(nid, policy);\n+\t\tpolicy->active = true;\n+\t}\n+\trcu_read_unlock();\n+}\n+#else\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tmemset(policy, 0, sizeof(*policy));\n+}\n+#endif\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e272dfdc6b00..9692048ab5fb 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -55,6 +55,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/node_private.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -6437,6 +6438,8 @@ static void __setup_per_zone_wmarks(void)\n \tunsigned long lowmem_pages = 0;\n \tstruct zone *zone;\n \tunsigned long flags;\n+\tstruct node_reclaim_policy rp;\n+\tint prev_nid = NUMA_NO_NODE;\n \n \t/* Calculate total number of !ZONE_HIGHMEM and !ZONE_MOVABLE pages */\n \tfor_each_zone(zone) {\n@@ -6446,6 +6449,7 @@ static void __setup_per_zone_wmarks(void)\n \n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n+\t\tint nid = zone_to_nid(zone);\n \n \t\tspin_lock_irqsave(&zone->lock, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n@@ -6482,7 +6486,12 @@ static void __setup_per_zone_wmarks(void)\n \t\t\t    mult_frac(zone_managed_pages(zone),\n \t\t\t\t      watermark_scale_factor, 10000));\n \n-\t\tzone->watermark_boost = 0;\n+\t\tif (nid != prev_nid) {\n+\t\t\tnode_private_reclaim_policy(nid, &rp);\n+\t\t\tprev_nid = nid;\n+\t\t}\n+\t\tif (!rp.managed_watermarks)\n+\t\t\tzone->watermark_boost = 0;\n \t\tzone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_HIGH] = low_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 0f534428ea88..07de666c1276 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -73,6 +73,13 @@\n #define CREATE_TRACE_POINTS\n #include <trace/events/vmscan.h>\n \n+static inline bool zone_reclaim_allowed(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn zone_private_flags(zone, NP_OPS_RECLAIM);\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n struct scan_control {\n \t/* How many pages shrink_list() should reclaim */\n \tunsigned long nr_to_reclaim;\n@@ -6274,7 +6281,7 @@ static void shrink_zones(struct zonelist *zonelist, struct scan_control *sc)\n \t\t * to global LRU.\n \t\t */\n \t\tif (!cgroup_reclaim(sc)) {\n-\t\t\tif (!cpuset_zone_allowed(zone,\n+\t\t\tif (!zone_reclaim_allowed(zone,\n \t\t\t\t\t\t GFP_KERNEL | __GFP_HARDWALL))\n \t\t\t\tcontinue;\n \n@@ -6992,6 +6999,7 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \tunsigned long zone_boosts[MAX_NR_ZONES] = { 0, };\n \tbool boosted;\n \tstruct zone *zone;\n+\tstruct node_reclaim_policy policy;\n \tstruct scan_control sc = {\n \t\t.gfp_mask = GFP_KERNEL,\n \t\t.order = order,\n@@ -7016,6 +7024,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t}\n \tboosted = nr_boost_reclaim;\n \n+\t/* Query/cache private node reclaim policy once per balance() */\n+\tnode_private_reclaim_policy(pgdat->node_id, &policy);\n+\n restart:\n \tset_reclaim_active(pgdat, highest_zoneidx);\n \tsc.priority = DEF_PRIORITY;\n@@ -7083,6 +7094,12 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\tsc.may_writepage = !laptop_mode && !nr_boost_reclaim;\n \t\tsc.may_swap = !nr_boost_reclaim;\n \n+\t\t/* Private nodes may enable swap/writepage when using boost */\n+\t\tif (policy.active) {\n+\t\t\tsc.may_swap |= policy.may_swap;\n+\t\t\tsc.may_writepage |= policy.may_writepage;\n+\t\t}\n+\n \t\t/*\n \t\t * Do some background aging, to give pages a chance to be\n \t\t * referenced before reclaiming. All pages are rotated\n@@ -7176,6 +7193,10 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\t\tif (!zone_boosts[i])\n \t\t\t\tcontinue;\n \n+\t\t\t/* Some private nodes may own the\\ boost lifecycle */\n+\t\t\tif (policy.managed_watermarks)\n+\t\t\t\tcontinue;\n+\n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n \t\t\tspin_lock_irqsave(&zone->lock, flags);\n@@ -7406,7 +7427,7 @@ void wakeup_kswapd(struct zone *zone, gfp_t gfp_flags, int order,\n \tif (!managed_zone(zone))\n \t\treturn;\n \n-\tif (!cpuset_zone_allowed(zone, gfp_flags))\n+\tif (!zone_reclaim_allowed(zone, gfp_flags))\n \t\treturn;\n \n \tpgdat = zone->zone_pgdat;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed the concern that the OOM killer may select an undeserving victim if it doesn't know whether killing a task can actually free memory on a private node. The author introduced NP_OPS_OOM_ELIGIBLE and helpers to check if a private node is OOM-eligible, and updated constrained_alloc() to use these checks.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The OOM killer must know whether killing a task can actually free\nmemory such that pressure is reduced.\n\nA private node only contributes to relieving pressure if it participates\nin both reclaim and demotion. Without this check, the check, the OOM\nkiller may select an undeserving victim.\n\nIntroduce NP_OPS_OOM_ELIGIBLE and helpers node_oom_eligible() and\nzone_oom_eligible().\n\nReplace cpuset_mems_allowed_intersects() in oom_cpuset_eligible()\nwith oom_mems_intersect() that iterates N_MEMORY nodes and skips\nineligible private nodes.\n\nUpdate constrained_alloc() to use zone_oom_eligible() for constraint\ndetection and node_oom_eligible() to exclude ineligible nodes from\ntotalpages accounting.\n\nRemove cpuset_mems_allowed_intersects() as it has no remaining callers.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cpuset.h       |  9 -------\n include/linux/node_private.h |  3 +++\n kernel/cgroup/cpuset.c       | 17 ------------\n mm/oom_kill.c                | 52 ++++++++++++++++++++++++++++++++----\n 4 files changed, 50 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/cpuset.h b/include/linux/cpuset.h\nindex 7b2f3f6b68a9..53ccfb00b277 100644\n--- a/include/linux/cpuset.h\n+++ b/include/linux/cpuset.h\n@@ -97,9 +97,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-extern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t  const struct task_struct *tsk2);\n-\n #ifdef CONFIG_CPUSETS_V1\n #define cpuset_memory_pressure_bump() \t\t\t\t\\\n \tdo {\t\t\t\t\t\t\t\\\n@@ -241,12 +238,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-static inline int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t\t const struct task_struct *tsk2)\n-{\n-\treturn 1;\n-}\n-\n static inline void cpuset_memory_pressure_bump(void) {}\n \n static inline void cpuset_task_status_allowed(struct seq_file *m,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34be52383255..34d862f09e24 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -141,6 +141,9 @@ struct node_private_ops {\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n+/* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n+#define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 1a597f0c7c6c..29789d544fd5 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -4530,23 +4530,6 @@ int cpuset_mem_spread_node(void)\n \treturn cpuset_spread_node(&current->cpuset_mem_spread_rotor);\n }\n \n-/**\n- * cpuset_mems_allowed_intersects - Does @tsk1's mems_allowed intersect @tsk2's?\n- * @tsk1: pointer to task_struct of some task.\n- * @tsk2: pointer to task_struct of some other task.\n- *\n- * Description: Return true if @tsk1's mems_allowed intersects the\n- * mems_allowed of @tsk2.  Used by the OOM killer to determine if\n- * one of the task's memory usage might impact the memory available\n- * to the other.\n- **/\n-\n-int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t   const struct task_struct *tsk2)\n-{\n-\treturn nodes_intersects(tsk1->mems_allowed, tsk2->mems_allowed);\n-}\n-\n /**\n  * cpuset_print_current_mems_allowed - prints current's cpuset and mems_allowed\n  *\ndiff --git a/mm/oom_kill.c b/mm/oom_kill.c\nindex 5eb11fbba704..cd0d65ccd1e8 100644\n--- a/mm/oom_kill.c\n+++ b/mm/oom_kill.c\n@@ -74,7 +74,45 @@ static inline bool is_memcg_oom(struct oom_control *oc)\n \treturn oc->memcg != NULL;\n }\n \n+/* Private nodes are only eligible if they support both reclaim and demotion */\n+static inline bool node_oom_eligible(int nid)\n+{\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn (node_private_flags(nid) & NP_OPS_OOM_ELIGIBLE) ==\n+\t\tNP_OPS_OOM_ELIGIBLE;\n+}\n+\n+static inline bool zone_oom_eligible(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (!node_oom_eligible(zone_to_nid(zone)))\n+\t\treturn false;\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n #ifdef CONFIG_NUMA\n+/*\n+ * Killing a task can only relieve system pressure if freed memory can be\n+ * demoted there and reclaim can operate on the node's pages, so we\n+ * omit private nodes that aren't eligible.\n+ */\n+static bool oom_mems_intersect(const struct task_struct *tsk1,\n+\t\t\t       const struct task_struct *tsk2)\n+{\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (!node_isset(nid, tsk1->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_isset(nid, tsk2->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_oom_eligible(nid))\n+\t\t\tcontinue;\n+\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n /**\n  * oom_cpuset_eligible() - check task eligibility for kill\n  * @start: task struct of which task to consider\n@@ -107,9 +145,10 @@ static bool oom_cpuset_eligible(struct task_struct *start,\n \t\t} else {\n \t\t\t/*\n \t\t\t * This is not a mempolicy constrained oom, so only\n-\t\t\t * check the mems of tsk's cpuset.\n+\t\t\t * check the mems of tsk's cpuset, excluding private\n+\t\t\t * nodes that do not participate in kernel reclaim.\n \t\t\t */\n-\t\t\tret = cpuset_mems_allowed_intersects(current, tsk);\n+\t\t\tret = oom_mems_intersect(current, tsk);\n \t\t}\n \t\tif (ret)\n \t\t\tbreak;\n@@ -291,16 +330,19 @@ static enum oom_constraint constrained_alloc(struct oom_control *oc)\n \t\treturn CONSTRAINT_MEMORY_POLICY;\n \t}\n \n-\t/* Check this allocation failure is caused by cpuset's wall function */\n+\t/* Check this allocation failure is caused by cpuset or private node constraints */\n \tfor_each_zone_zonelist_nodemask(zone, z, oc->zonelist,\n \t\t\thighest_zoneidx, oc->nodemask)\n-\t\tif (!cpuset_zone_allowed(zone, oc->gfp_mask))\n+\t\tif (!zone_oom_eligible(zone, oc->gfp_mask))\n \t\t\tcpuset_limited = true;\n \n \tif (cpuset_limited) {\n \t\toc->totalpages = total_swap_pages;\n-\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed)\n+\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed) {\n+\t\t\tif (!node_oom_eligible(nid))\n+\t\t\t\tcontinue;\n \t\t\toc->totalpages += node_present_pages(nid);\n+\t\t}\n \t\treturn CONSTRAINT_CPUSET;\n \t}\n \treturn CONSTRAINT_NONE;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private nodes engaging in NUMA balancing faults by introducing an opt-in method (NP_OPS_NUMA_BALANCING) and adding a helper function to filter for private nodes that have opted in. The author also added code to enforce write-protection on failed or skipped migrations.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "added new functionality"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Not all private nodes may wish to engage in NUMA balancing faults.\n\nAdd the NP_OPS_NUMA_BALANCING flag (BIT(5)) as an opt-in method.\n\nIntroduce folio_managed_allows_numa() helper:\n   ZONE_DEVICE folios always return false (never NUMA-scanned)\n   NP_OPS_NUMA_BALANCING filters for private nodes\n\nIn do_numa_page(), if a private-node folio with NP_OPS_PROTECT_WRITE\nis still on its node after a failed/skipped migration, enforce\nwrite-protection so the next write triggers handle_fault.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h | 16 ++++++++++++++++\n mm/memory.c                  | 11 +++++++++++\n mm/mempolicy.c               |  5 ++++-\n 4 files changed, 35 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex a4955b9b5b93..88aaac45e814 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -961,6 +961,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (ops->flags & NP_OPS_PROTECT_WRITE))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_NUMA_BALANCING) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34d862f09e24..5ac60db1f044 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -140,6 +140,8 @@ struct node_private_ops {\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n+/* Allow NUMA balancing to scan and migrate folios on this node */\n+#define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n@@ -263,6 +265,15 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n }\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\tif (!folio_is_private_managed(folio))\n+\t\treturn true;\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\treturn folio_private_flags(folio, NP_OPS_NUMA_BALANCING);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \tif (folio_is_zone_device(folio))\n@@ -443,6 +454,11 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\treturn !folio_is_zone_device(folio);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \treturn -ENOENT;\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 0f78988befef..88a581baae40 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -78,6 +78,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/pgalloc.h>\n #include <linux/uaccess.h>\n+#include <linux/node_private.h>\n \n #include <trace/events/kmem.h>\n \n@@ -6041,6 +6042,12 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \tif (!folio || folio_is_zone_device(folio))\n \t\tgoto out_map;\n \n+\t/*\n+\t * We do not need to check private-node folios here because the private\n+\t * memory service either never opted in to NUMA balancing, or it did\n+\t * and we need to restore private PTE controls on the failure path.\n+\t */\n+\n \tnid = folio_nid(folio);\n \tnr_pages = folio_nr_pages(folio);\n \n@@ -6078,6 +6085,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t/*\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n+\t *\n+\t * If the folio is still on a private node with NP_OPS_PROTECT_WRITE,\n+\t * enforce write-protection so the next write triggers handle_fault.\n+\t * This covers migration-failed and migration-skipped paths.\n \t */\n \tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n \t\twritable = false;\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 8ac014950e88..8a3a9916ab59 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -861,7 +861,10 @@ bool folio_can_map_prot_numa(struct folio *folio, struct vm_area_struct *vma,\n {\n \tint nid;\n \n-\tif (!folio || folio_is_zone_device(folio) || folio_test_ksm(folio))\n+\tif (!folio || folio_test_ksm(folio))\n+\t\treturn false;\n+\n+\tif (unlikely(!folio_managed_allows_numa(folio)))\n \t\treturn false;\n \n \t/* Also skip shared copy-on-write folios */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about compaction on private nodes, explaining that it requires migration and services may have PFN-based metadata to update. They added a folio_migrate callback, zone_supports_compaction function, and filtered three direct compaction zone loops. The service is responsible for starting kcompactd on its node.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node zones should not be compacted unless the service explicitly\nopts in - as compaction requires migration and services may have\nPFN-based metadata that needs updating.\n\nAdd a folio_migrate callback which fires from migrate_folio_move() for\neach relocated folio before faults are unblocked.\n\nAdd zone_supports_compaction() which returns true for normal zones and\nchecks NP_OPS_COMPACTION for N_MEMORY_PRIVATE zones.\n\nFilter three direct compaction zone loops:\n  - compaction_zonelist_suitable() (reclaimer eligibility)\n  - try_to_compact_pages()         (direct compaction)\n  - compact_node()                 (proactive/manual compaction)\n\nkcompactd paths are intentionally unfiltered -- the service is\nresponsible for starting kcompactd on its node.\n\nNP_OPS_COMPACTION requires NP_OPS_MIGRATION.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h |  2 ++\n mm/compaction.c              | 26 ++++++++++++++++++++++++++\n 3 files changed, 32 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 88aaac45e814..da523aca18fa 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -965,6 +965,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_COMPACTION) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 5ac60db1f044..fe0336773ddb 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -142,6 +142,8 @@ struct node_private_ops {\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n /* Allow NUMA balancing to scan and migrate folios on this node */\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n+/* Allow compaction to run on the node.  Service must start kcompactd. */\n+#define NP_OPS_COMPACTION\t\tBIT(6)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 6a65145b03d8..d8532b957ec6 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,9 +24,26 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/node_private.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n+\n+/*\n+ * Private node zones require NP_OPS_COMPACTION to opt in.  Normal zones\n+ * always support compaction.\n+ */\n+static inline bool zone_supports_compaction(struct zone *zone)\n+{\n+#ifdef CONFIG_NUMA\n+\tif (!node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn zone_private_flags(zone, NP_OPS_COMPACTION);\n+#else\n+\treturn true;\n+#endif\n+}\n+\n /*\n  * Fragmentation score check interval for proactive compaction purposes.\n  */\n@@ -2443,6 +2460,9 @@ bool compaction_zonelist_suitable(struct alloc_context *ac, int order,\n \t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tunsigned long available;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\t/*\n \t\t * Do not consider all the reclaimable memory because we do not\n \t\t * want to trash just for a single high order allocation which\n@@ -2832,6 +2852,9 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\n \t\t\trc = max_t(enum compact_result, COMPACT_DEFERRED, rc);\n@@ -2906,6 +2929,9 @@ static int compact_node(pg_data_t *pgdat, bool proactive)\n \t\tif (!populated_zone(zone))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (fatal_signal_pending(current))\n \t\t\treturn -EINTR;\n \n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private node folios being longterm-pinnable by default, explaining that this would prevent services from controlling the memory during pinning. They added an NP_OPS_LONGTERM_PIN flag for services to opt-in and modified the folio_is_longterm_pinnable() function in mm.h to check for this flag.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not be longterm-pinnable by default.\nA pinned folio is frozen in place, no migration, compaction, or\nreclaim, so the service loses control for the duration of the pin.\n\nSome services may depend on hot-unplugability and must disallow\nlongterm pinning.  Others (accelerators with shared CPU-device state)\nneed pinning to work.\n\nAdd NP_OPS_LONGTERM_PIN flag for services to opt in with. Hook into\nfolio_is_longterm_pinnable() in mm.h, which all GUP callers\nout-of-line helper, node_private_allows_longterm_pin(),  called\nonly for N_MEMORY_PRIVATE nodes.\n\nWithout the flag: folio_is_longterm_pinnable() returns false, migration\nfails (no __GFP_PRIVATE in GFP mask) and pin_user_pages(FOLL_LONGTERM)\nreturns -ENOMEM.\n\nWith the flag: pin succeeds and the folio stays on the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 15 +++++++++++++++\n include/linux/mm.h           | 22 ++++++++++++++++++++++\n include/linux/node_private.h |  2 ++\n 3 files changed, 39 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex da523aca18fa..5d2487fd54f4 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -866,6 +866,21 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n static DEFINE_MUTEX(node_private_lock);\n static bool node_private_initialized;\n \n+/**\n+ * node_private_allows_longterm_pin - Check if a private node allows longterm pinning\n+ * @nid: Node identifier\n+ *\n+ * Out-of-line helper for folio_is_longterm_pinnable() since mm.h cannot\n+ * include node_private.h (circular dependency).\n+ *\n+ * Returns true if the node has NP_OPS_LONGTERM_PIN set.\n+ */\n+bool node_private_allows_longterm_pin(int nid)\n+{\n+\treturn node_private_has_flag(nid, NP_OPS_LONGTERM_PIN);\n+}\n+EXPORT_SYMBOL_GPL(node_private_allows_longterm_pin);\n+\n /**\n  * node_private_register - Register a private node\n  * @nid: Node identifier\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex fb1819ad42c3..9088fd08aeb9 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -2192,6 +2192,13 @@ static inline bool is_zero_folio(const struct folio *folio)\n \n /* MIGRATE_CMA and ZONE_MOVABLE do not allow pin folios */\n #ifdef CONFIG_MIGRATION\n+\n+#ifdef CONFIG_NUMA\n+bool node_private_allows_longterm_pin(int nid);\n+#else\n+static inline bool node_private_allows_longterm_pin(int nid) { return false; }\n+#endif\n+\n static inline bool folio_is_longterm_pinnable(struct folio *folio)\n {\n #ifdef CONFIG_CMA\n@@ -2215,6 +2222,21 @@ static inline bool folio_is_longterm_pinnable(struct folio *folio)\n \tif (folio_is_fsdax(folio))\n \t\treturn false;\n \n+\t/*\n+\t * Private node folios are not longterm pinnable by default.\n+\t * Services that support pinning opt in via NP_OPS_LONGTERM_PIN.\n+\t * node_private_allows_longterm_pin() is out-of-line because\n+\t * node_private.h includes mm.h (circular dependency).\n+\t *\n+\t * Guarded by CONFIG_NUMA because on !CONFIG_NUMA the single-node\n+\t * node_state() stub returns true for node 0, which would make\n+\t * all folios non-pinnable via the false-returning stub.\n+\t */\n+#ifdef CONFIG_NUMA\n+\tif (node_state(folio_nid(folio), N_MEMORY_PRIVATE))\n+\t\treturn node_private_allows_longterm_pin(folio_nid(folio));\n+#endif\n+\n \t/* Otherwise, non-movable zone folios can be pinned. */\n \treturn !folio_is_zone_movable(folio);\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex fe0336773ddb..7a7438fb9eda 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -144,6 +144,8 @@ struct node_private_ops {\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n /* Allow compaction to run on the node.  Service must start kcompactd. */\n #define NP_OPS_COMPACTION\t\tBIT(6)\n+/* Allow longterm DMA pinning (RDMA, VFIO, etc.) of folios on this node */\n+#define NP_OPS_LONGTERM_PIN\t\tBIT(7)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about notifying private-node services of hardware errors on their nodes by adding a memory_failure callback to struct node_private_ops, which will be called after TestSetPageHWPoison succeeds and before get_hwpoison_page. The kernel always proceeds with standard hwpoison handling for online pages.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "added callback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a void memory_failure notification callback to struct\nnode_private_ops so services managing N_MEMORY_PRIVATE nodes notified\nwhen a page on their node experiences a hardware error.\n\nThe callback is notification only -- the kernel always proceeds with\nstandard hwpoison handling for online pages.\n\nThe notification hook fires after TestSetPageHWPoison succeeds and\nbefore get_hwpoison_page giving the service a chance to clean up.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 16 ++++++++++++++++\n mm/memory-failure.c          | 15 +++++++++++++++\n 3 files changed, 37 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7a7438fb9eda..d2669f68ac20 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -113,6 +113,10 @@ struct node_reclaim_policy {\n  *   watermark_boost lifecycle (kswapd will not clear it).\n  *   If NULL, normal boost policy applies.\n  *\n+ * @memory_failure: Notification of hardware error on a page on this node.\n+ *   [folio-referenced callback]\n+ *   Notification only, kernel always handles the failure.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -127,6 +131,8 @@ struct node_private_ops {\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n \tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n+\tvoid (*memory_failure)(struct folio *folio, unsigned long pfn,\n+\t\t\t       int mf_flags);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex db32cb2d7a29..64467ca774f1 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1608,6 +1608,22 @@ static inline void node_private_reclaim_policy(int nid,\n }\n #endif\n \n+static inline void folio_managed_memory_failure(struct folio *folio,\n+\t\t\t\t\t\tunsigned long pfn,\n+\t\t\t\t\t\tint mf_flags)\n+{\n+\t/* Zone device pages handle memory failure via dev_pagemap_ops */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn;\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->memory_failure)\n+\t\t\tops->memory_failure(folio, pfn, mf_flags);\n+\t}\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/memory-failure.c b/mm/memory-failure.c\nindex c80c2907da33..79c91d44ec1e 100644\n--- a/mm/memory-failure.c\n+++ b/mm/memory-failure.c\n@@ -2379,6 +2379,15 @@ int memory_failure(unsigned long pfn, int flags)\n \t\tgoto unlock_mutex;\n \t}\n \n+\t/*\n+\t * Notify private-node services about the hardware error so they\n+\t * can update internal tracking (e.g., CXL poison lists, stop\n+\t * demoting to failing DIMMs).  This is notification only -- the\n+\t * kernel proceeds with standard hwpoison handling regardless.\n+\t */\n+\tif (unlikely(page_is_private_managed(p)))\n+\t\tfolio_managed_memory_failure(page_folio(p), pfn, flags);\n+\n \t/*\n \t * We need/can do nothing about count=0 pages.\n \t * 1) it's a free page, and therefore in safe hand:\n@@ -2825,6 +2834,12 @@ static int soft_offline_in_use_page(struct page *page)\n \t\treturn 0;\n \t}\n \n+\tif (!folio_managed_allows_migrate(folio)) {\n+\t\tpr_info(\"%#lx: cannot migrate private node folio\\n\", pfn);\n+\t\tfolio_put(folio);\n+\t\treturn -EBUSY;\n+\t}\n+\n \tisolated = isolate_folio_to_list(folio, &pagelist);\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the ordering of registering private regions and hotplugging memory, explaining that their new function combines these two steps to ensure proper ordering. The function first registers the private region, then hotplugs the memory, and on failure, unregisters the private region. They also added checks for migration support and online status when removing the last of memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a new function for drivers to hotplug memory as N_MEMORY_PRIVATE.\n\nThis function combines node_private_region_register() with\n__add_memory_driver_managed() to ensure proper ordering:\n\n1. Register the private region first (sets private node context)\n2. Then hotplug the memory (sets N_MEMORY_PRIVATE)\n3. On failure, unregister the private region to avoid leaving the\n   node in an inconsistent state.\n\nWhen the last of memory is removed, hotplug also removes the private\nnode context. If migration is not supported and the node is still\nonline, fire a warning (likely bug in the driver).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory_hotplug.h |  11 +++\n include/linux/mmzone.h         |  12 ++++\n mm/memory_hotplug.c            | 122 ++++++++++++++++++++++++++++++---\n 3 files changed, 135 insertions(+), 10 deletions(-)\n\ndiff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h\nindex 1f19f08552ea..e5abade9450a 100644\n--- a/include/linux/memory_hotplug.h\n+++ b/include/linux/memory_hotplug.h\n@@ -293,6 +293,7 @@ extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n extern int remove_memory(u64 start, u64 size);\n extern void __remove_memory(u64 start, u64 size);\n extern int offline_and_remove_memory(u64 start, u64 size);\n+extern int offline_and_remove_private_memory(int nid, u64 start, u64 size);\n \n #else\n static inline void try_offline_node(int nid) {}\n@@ -309,6 +310,12 @@ static inline int remove_memory(u64 start, u64 size)\n }\n \n static inline void __remove_memory(u64 start, u64 size) {}\n+\n+static inline int offline_and_remove_private_memory(int nid, u64 start,\n+\t\t\t\t\t\t    u64 size)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n@@ -326,6 +333,10 @@ int __add_memory_driver_managed(int nid, u64 start, u64 size,\n extern int add_memory_driver_managed(int nid, u64 start, u64 size,\n \t\t\t\t     const char *resource_name,\n \t\t\t\t     mhp_t mhp_flags);\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np);\n extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,\n \t\t\t\t   unsigned long nr_pages,\n \t\t\t\t   struct vmem_altmap *altmap, int migratetype,\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 992eb1c5a2c6..cc532b67ad3f 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1524,6 +1524,18 @@ typedef struct pglist_data {\n #endif\n } pg_data_t;\n \n+#ifdef CONFIG_NUMA\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn pgdat->private;\n+}\n+#else\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn false;\n+}\n+#endif\n+\n #define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n #define node_spanned_pages(nid)\t(NODE_DATA(nid)->node_spanned_pages)\n \ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex d2dc527bd5b0..9d72f44a30dc 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1173,8 +1174,7 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tmove_pfn_range_to_zone(zone, pfn, nr_pages, NULL, MIGRATE_MOVABLE,\n \t\t\t       true);\n \n-\tif (!node_state(nid, N_MEMORY)) {\n-\t\t/* Adding memory to the node for the first time */\n+\tif (!node_state(nid, N_MEMORY) && !node_state(nid, N_MEMORY_PRIVATE)) {\n \t\tnode_arg.nid = nid;\n \t\tret = node_notify(NODE_ADDING_FIRST_MEMORY, &node_arg);\n \t\tret = notifier_to_errno(ret);\n@@ -1208,8 +1208,12 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tonline_pages_range(pfn, nr_pages);\n \tadjust_present_page_count(pfn_to_page(pfn), group, nr_pages);\n \n-\tif (node_arg.nid >= 0)\n-\t\tnode_set_state(nid, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (pgdat_is_private(NODE_DATA(nid)))\n+\t\t\tnode_set_state(nid, N_MEMORY_PRIVATE);\n+\t\telse\n+\t\t\tnode_set_state(nid, N_MEMORY);\n+\t}\n \tif (need_zonelists_rebuild)\n \t\tbuild_all_zonelists(NULL);\n \n@@ -1227,8 +1231,14 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t/* reinitialise watermarks and update pcp limits */\n \tinit_per_zone_wmark_min();\n \n-\tkswapd_run(nid);\n-\tkcompactd_run(nid);\n+\t/*\n+\t * Don't start reclaim/compaction daemons for private nodes.\n+\t * Private node services will decide whether to start these services.\n+\t */\n+\tif (!pgdat_is_private(NODE_DATA(nid))) {\n+\t\tkswapd_run(nid);\n+\t\tkcompactd_run(nid);\n+\t}\n \n \tif (node_arg.nid >= 0)\n \t\t/* First memory added successfully. Notify consumers. */\n@@ -1722,6 +1732,54 @@ int add_memory_driver_managed(int nid, u64 start, u64 size,\n }\n EXPORT_SYMBOL_GPL(add_memory_driver_managed);\n \n+/**\n+ * add_private_memory_driver_managed - add driver-managed N_MEMORY_PRIVATE memory\n+ * @nid: NUMA node ID (or memory group ID when MHP_NID_IS_MGID is set)\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ * @resource_name: \"System RAM ($DRIVER)\" format\n+ * @mhp_flags: Memory hotplug flags\n+ * @online_type: MMOP_* online type\n+ * @np: Driver-owned node_private structure (owner, refcount)\n+ *\n+ * Registers node_private first, then hotplugs the memory.\n+ *\n+ * On failure, unregisters the node_private.\n+ */\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np)\n+{\n+\tstruct memory_group *group;\n+\tint real_nid = nid;\n+\tint rc;\n+\n+\tif (!np)\n+\t\treturn -EINVAL;\n+\n+\tif (mhp_flags & MHP_NID_IS_MGID) {\n+\t\tgroup = memory_group_find_by_id(nid);\n+\t\tif (!group)\n+\t\t\treturn -EINVAL;\n+\t\treal_nid = group->nid;\n+\t}\n+\n+\trc = node_private_register(real_nid, np);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\trc = __add_memory_driver_managed(nid, start, size, resource_name,\n+\t\t\t\t\t mhp_flags, online_type);\n+\tif (rc) {\n+\t\tnode_private_unregister(real_nid);\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(add_private_memory_driver_managed);\n+\n /*\n  * Platforms should define arch_get_mappable_range() that provides\n  * maximum possible addressable physical memory range for which the\n@@ -1872,6 +1930,15 @@ static void do_migrate_range(unsigned long start_pfn, unsigned long end_pfn)\n \t\t\tgoto put_folio;\n \t\t}\n \n+\t\t/* Private nodes w/o migration must ensure folios are offline */\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION)) {\n+\t\t\tWARN_ONCE(1, \"hot-unplug on non-migratable node %d pfn %lx\\n\",\n+\t\t\t\t  folio_nid(folio), pfn);\n+\t\t\tpfn = folio_pfn(folio) + folio_nr_pages(folio) - 1;\n+\t\t\tgoto put_folio;\n+\t\t}\n+\n \t\tif (!isolate_folio_to_list(folio, &source)) {\n \t\t\tif (__ratelimit(&migrate_rs)) {\n \t\t\t\tpr_warn(\"failed to isolate pfn %lx\\n\",\n@@ -2014,8 +2081,8 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \n \t/*\n \t * Check whether the node will have no present pages after we offline\n-\t * 'nr_pages' more. If so, we know that the node will become empty, and\n-\t * so we will clear N_MEMORY for it.\n+\t * 'nr_pages' more. If so, send pre-notification for last memory removal.\n+\t * We will clear N_MEMORY(_PRIVATE) if this is the case.\n \t */\n \tif (nr_pages >= pgdat->node_present_pages) {\n \t\tnode_arg.nid = node;\n@@ -2108,8 +2175,12 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * Make sure to mark the node as memory-less before rebuilding the zone\n \t * list. Otherwise this node would still appear in the fallback lists.\n \t */\n-\tif (node_arg.nid >= 0)\n-\t\tnode_clear_state(node, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (node_state(node, N_MEMORY))\n+\t\t\tnode_clear_state(node, N_MEMORY);\n+\t\telse if (node_state(node, N_MEMORY_PRIVATE))\n+\t\t\tnode_clear_state(node, N_MEMORY_PRIVATE);\n+\t}\n \tif (!populated_zone(zone)) {\n \t\tzone_pcp_reset(zone);\n \t\tbuild_all_zonelists(NULL);\n@@ -2461,4 +2532,35 @@ int offline_and_remove_memory(u64 start, u64 size)\n \treturn rc;\n }\n EXPORT_SYMBOL_GPL(offline_and_remove_memory);\n+\n+/**\n+ * offline_and_remove_private_memory - offline, remove, and unregister private memory\n+ * @nid: NUMA node ID of the private memory\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ *\n+ * Counterpart to add_private_memory_driver_managed().  Offlines and removes\n+ * the memory range, then attempts to unregister the node_private.\n+ *\n+ * offline_and_remove_memory() clears N_MEMORY_PRIVATE when the last block\n+ * is offlined, which allows node_private_unregister() to clear the\n+ * pgdat->node_private pointer.  If other private memory ranges remain on\n+ * the node, node_private_unregister() returns -EBUSY (N_MEMORY_PRIVATE\n+ * is still set) and the node_private remains registered.\n+ *\n+ * Return: 0 on full success (memory removed and node_private unregistered),\n+ *         -EBUSY if memory was removed but node still has other private memory,\n+ *         other negative error code if offline/remove failed.\n+ */\n+int offline_and_remove_private_memory(int nid, u64 start, u64 size)\n+{\n+\tint rc;\n+\n+\trc = offline_and_remove_memory(start, size);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\treturn node_private_unregister(nid);\n+}\n+EXPORT_SYMBOL_GPL(offline_and_remove_private_memory);\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the lack of locking in the swapoff path, acknowledged that the per-vswap spinlock needs to be dropped before calling try_to_unmap(), and agreed to restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CRAM (Compressed RAM) subsystem that manages folios demoted\nto N_MEMORY_PRIVATE nodes via the standard kernel LRU.\n\nWe limit entry into CRAM by demotion in to provide devices a way for\ndrivers to close access - which allows the system to stabiliz under\nmemory pressure (the device can run out of real memory when compression\nratios drop too far).\n\nWe utilize write-protect to prevent unbounded writes to compressed\nmemory pages, which may cause run-away compression ratio loss without\na reliable way to prevent the degenerate case (cascading poisons).\n\nCRAM provides the bridge between the mm/ private node infrastructure\nand compressed memory hardware.  Folios are aged by kswapd on the\nprivate node and reclaimed to swap when the device signals pressure.\n\nWrite faults trigger promotion back to regular DRAM via the\nops->handle_fault callback.\n\nDevice pressure is communicated via watermark_boost on the private\nnode's zone.\n\nCRAM registers node_private_ops with:\n  - handle_fault:   promotes folio back to DRAM on write\n  - migrate_to:     custom demotion to the CRAM node\n  - folio_migrate:  (no-op)\n  - free_folio:     zeroes pages on free to scrub stale data\n  - reclaim_policy: provides mayswap/writeback/boost overrides\n  - flags: NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n\t   NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE\n           NP_OPS_RECLAIM\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cram.h |  66 ++++++\n mm/Kconfig           |  10 +\n mm/Makefile          |   1 +\n mm/cram.c            | 508 +++++++++++++++++++++++++++++++++++++++++++\n 4 files changed, 585 insertions(+)\n create mode 100644 include/linux/cram.h\n create mode 100644 mm/cram.c\n\ndiff --git a/include/linux/cram.h b/include/linux/cram.h\nnew file mode 100644\nindex 000000000000..a3c10362fd4f\n--- /dev/null\n+++ b/include/linux/cram.h\n@@ -0,0 +1,66 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_CRAM_H\n+#define _LINUX_CRAM_H\n+\n+#include <linux/mm_types.h>\n+\n+struct folio;\n+struct list_head;\n+struct vm_fault;\n+\n+#define CRAM_PRESSURE_MAX\t1000\n+\n+/**\n+ * cram_flush_cb_t - Driver callback invoked when a folio on a private node\n+ *                   is freed (refcount reaches zero).\n+ * @folio: the folio being freed\n+ * @private: opaque driver data passed at registration\n+ *\n+ * Return:\n+ *   0: Flush resolved -- page should return to buddy allocator (e.g., flush\n+ *      record bit was set, meaning this free is from our own flush resolution)\n+ *   1: Page deferred -- driver took a reference, page will be flushed later.\n+ *      Do NOT return to buddy allocator.\n+ *   2: Buffer full -- caller should zero the page and return to buddy.\n+ */\n+typedef int (*cram_flush_cb_t)(struct folio *folio, void *private);\n+\n+#ifdef CONFIG_CRAM\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data);\n+int cram_unregister_private_node(int nid);\n+int cram_unpurge(int nid);\n+void cram_set_pressure(int nid, unsigned int pressure);\n+void cram_clear_pressure(int nid);\n+\n+#else /* !CONFIG_CRAM */\n+\n+static inline int cram_register_private_node(int nid, void *owner,\n+\t\t\t\t\t     cram_flush_cb_t flush_cb,\n+\t\t\t\t\t     void *flush_data)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unregister_private_node(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unpurge(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+}\n+\n+static inline void cram_clear_pressure(int nid)\n+{\n+}\n+\n+#endif /* CONFIG_CRAM */\n+\n+#endif /* _LINUX_CRAM_H */\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex bd0ea5454af8..054462b954d8 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -662,6 +662,16 @@ config MIGRATION\n config DEVICE_MIGRATION\n \tdef_bool MIGRATION && ZONE_DEVICE\n \n+config CRAM\n+\tbool \"Compressed RAM - private node memory management\"\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\tdepends on MEMORY_HOTPLUG\n+\thelp\n+\t  Enables management of N_MEMORY_PRIVATE nodes for compressed RAM\n+\t  and similar use cases. Provides demotion, promotion, and lifecycle\n+\t  management for private memory nodes.\n+\n config ARCH_ENABLE_HUGEPAGE_MIGRATION\n \tbool\n \ndiff --git a/mm/Makefile b/mm/Makefile\nindex 2d0570a16e5b..0e1421512643 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -98,6 +98,7 @@ obj-$(CONFIG_MEMTEST)\t\t+= memtest.o\n obj-$(CONFIG_MIGRATION) += migrate.o\n obj-$(CONFIG_NUMA) += memory-tiers.o\n obj-$(CONFIG_DEVICE_MIGRATION) += migrate_device.o\n+obj-$(CONFIG_CRAM) += cram.o\n obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o\n obj-$(CONFIG_PAGE_COUNTER) += page_counter.o\n obj-$(CONFIG_LIVEUPDATE) += memfd_luo.o\ndiff --git a/mm/cram.c b/mm/cram.c\nnew file mode 100644\nindex 000000000000..6709e61f5b9d\n--- /dev/null\n+++ b/mm/cram.c\n@@ -0,0 +1,508 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * mm/cram.c - Compressed RAM / private node memory management\n+ *\n+ * Copyright 2026 Meta Technologies Inc.\n+ *   Author: Gregory Price <gourry@gourry.net>\n+ *\n+ * Manages folios demoted to N_MEMORY_PRIVATE nodes via the standard kernel\n+ * LRU.  Folios are aged by kswapd on the private node and reclaimed to swap\n+ * (demotion is suppressed for private nodes).  Write faults trigger promotion\n+ * back to regular DRAM via the ops->handle_fault callback.\n+ *\n+ * All reclaim/demotion uses the standard vmscan infrastructure. Device pressure\n+ * is communicated via watermark_boost on the private node's zone.\n+ */\n+\n+#include <linux/atomic.h>\n+#include <linux/cpuset.h>\n+#include <linux/cram.h>\n+#include <linux/errno.h>\n+#include <linux/gfp.h>\n+#include <linux/jiffies.h>\n+#include <linux/highmem.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/list.h>\n+#include <linux/migrate.h>\n+#include <linux/mm.h>\n+#include <linux/huge_mm.h>\n+#include <linux/mmzone.h>\n+#include <linux/mutex.h>\n+#include <linux/nodemask.h>\n+#include <linux/node_private.h>\n+#include <linux/pagemap.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+#include <linux/swap.h>\n+\n+#include \"internal.h\"\n+\n+struct cram_node {\n+\tvoid\t\t*owner;\n+\tbool\t\tpurged;\t\t/* node is being torn down */\n+\tunsigned int\tpressure;\n+\trefcount_t\trefcount;\n+\tcram_flush_cb_t\tflush_cb;\t/* optional driver flush callback */\n+\tvoid\t\t*flush_data;\t/* opaque data for flush_cb */\n+};\n+\n+static struct cram_node *cram_nodes[MAX_NUMNODES];\n+static DEFINE_MUTEX(cram_mutex);\n+\n+static inline bool cram_valid_nid(int nid)\n+{\n+\treturn nid >= 0 && nid < MAX_NUMNODES;\n+}\n+\n+static inline struct cram_node *get_cram_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn NULL;\n+\n+\trcu_read_lock();\n+\tcn = rcu_dereference(cram_nodes[nid]);\n+\tif (cn && !refcount_inc_not_zero(&cn->refcount))\n+\t\tcn = NULL;\n+\trcu_read_unlock();\n+\n+\treturn cn;\n+}\n+\n+static inline void put_cram_node(struct cram_node *cn)\n+{\n+\tif (cn)\n+\t\trefcount_dec(&cn->refcount);\n+}\n+\n+static void cram_zero_folio(struct folio *folio)\n+{\n+\tunsigned int i, nr = folio_nr_pages(folio);\n+\n+\tif (want_init_on_free())\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr; i++)\n+\t\tclear_highpage(folio_page(folio, i));\n+}\n+\n+static bool cram_free_folio_cb(struct folio *folio)\n+{\n+\tint nid = folio_nid(folio);\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\tgoto zero_and_free;\n+\n+\tif (!cn->flush_cb)\n+\t\tgoto zero_and_free_put;\n+\n+\tret = cn->flush_cb(folio, cn->flush_data);\n+\tput_cram_node(cn);\n+\n+\tswitch (ret) {\n+\tcase 0:\n+\t\t/* Flush resolved: return to buddy (already zeroed by device) */\n+\t\treturn false;\n+\tcase 1:\n+\t\t/* Deferred: driver holds a ref, do not free to buddy */\n+\t\treturn true;\n+\tcase 2:\n+\tdefault:\n+\t\t/* Buffer full or unknown: zero locally, return to buddy */\n+\t\tgoto zero_and_free;\n+\t}\n+\n+zero_and_free_put:\n+\tput_cram_node(cn);\n+zero_and_free:\n+\tcram_zero_folio(folio);\n+\treturn false;\n+}\n+\n+static struct folio *alloc_cram_folio(struct folio *src, unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_PRIVATE | __GFP_KSWAPD_RECLAIM |\n+\t\t     __GFP_HIGHMEM | __GFP_MOVABLE |\n+\t\t     __GFP_NOWARN | __GFP_NORETRY;\n+\n+\t/* Stop allocating if backpressure fired mid-batch */\n+\tif (node_private_migration_blocked(nid))\n+\t\treturn NULL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc_node(gfp, order, nid);\n+}\n+\n+static void cram_put_new_folio(struct folio *folio, unsigned long private)\n+{\n+\tcram_zero_folio(folio);\n+\tfolio_put(folio);\n+}\n+\n+/*\n+ * Allocate a DRAM folio for promotion out of a private node.\n+ *\n+ * Unlike alloc_migration_target(), this does NOT strip __GFP_RECLAIM for\n+ * large folios, the generic helper does that because THP allocations are\n+ * opportunistic, but promotion from a private node is mandatory: the page\n+ * MUST move to DRAM or the process cannot make forward progress.\n+ *\n+ * __GFP_RETRY_MAYFAIL tells the allocator to try hard (multiple reclaim\n+ * rounds, wait for writeback) before giving up.\n+ */\n+static struct folio *alloc_cram_promote_folio(struct folio *src,\n+\t\t\t\t\t      unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc(gfp, order, nid, NULL);\n+}\n+\n+static int cram_migrate_to(struct list_head *demote_folios, int to_nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason,\n+\t\t\t   unsigned int *nr_succeeded)\n+{\n+\tstruct cram_node *cn;\n+\tunsigned int nr_success = 0;\n+\tint ret = 0;\n+\n+\tcn = get_cram_node(to_nid);\n+\tif (!cn)\n+\t\treturn -ENODEV;\n+\n+\tif (cn->purged) {\n+\t\tret = -ENODEV;\n+\t\tgoto out;\n+\t}\n+\n+\t/* Block new demotions at maximum pressure */\n+\tif (READ_ONCE(cn->pressure) >= CRAM_PRESSURE_MAX) {\n+\t\tret = -ENOSPC;\n+\t\tgoto out;\n+\t}\n+\n+\tret = migrate_pages(demote_folios, alloc_cram_folio, cram_put_new_folio,\n+\t\t\t    (unsigned long)to_nid, mode, reason,\n+\t\t\t    &nr_success);\n+\n+\t/*\n+\t * migrate_folio_move() calls folio_add_lru() for each migrated\n+\t * folio, but that only adds the folio to a per-CPU batch, \n+\t * PG_lru is not set until the batch is drained.  Drain now so\n+\t * that cram_fault() can isolate these folios immediately.\n+\t *\n+\t * Use lru_add_drain_all() because migrate_pages() may process\n+\t * folios across CPUs, and the local drain might miss batches\n+\t * filled on other CPUs.\n+\t */\n+\tif (nr_success)\n+\t\tlru_add_drain_all();\n+out:\n+\tput_cram_node(cn);\n+\tif (nr_succeeded)\n+\t\t*nr_succeeded = nr_success;\n+\treturn ret;\n+}\n+\n+static void cram_release_ptl(struct vm_fault *vmf, enum pgtable_level level)\n+{\n+\tif (level == PGTABLE_LEVEL_PTE)\n+\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n+\telse\n+\t\tspin_unlock(vmf->ptl);\n+}\n+\n+static vm_fault_t cram_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t     enum pgtable_level level)\n+{\n+\tstruct folio *f, *f2;\n+\tstruct cram_node *cn;\n+\tunsigned int nr_succeeded = 0;\n+\tint nid;\n+\tLIST_HEAD(folios);\n+\n+\tnid = folio_nid(folio);\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn) {\n+\t\tcram_release_ptl(vmf, level);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Isolate from LRU while holding PTL.  This serializes against\n+\t * other CPUs faulting on the same folio: only one CPU can clear\n+\t * PG_lru under the PTL, and it proceeds to migration.  Other\n+\t * CPUs find the folio already isolated and bail out, preventing\n+\t * the refcount pile-up that causes migrate_pages() to fail with\n+\t * -EAGAIN.\n+\t *\n+\t * No explicit folio_get() is needed: the page table entry holds\n+\t * a reference (we still hold PTL), and folio_isolate_lru() takes\n+\t * its own reference.  This matches do_numa_page()'s pattern.\n+\t *\n+\t * PG_lru should already be set: cram_migrate_to() drains per-CPU\n+\t * LRU batches after migration, and the failure path below\n+\t * drains after putback.\n+\t */\n+\tif (!folio_isolate_lru(folio)) {\n+\t\tput_cram_node(cn);\n+\t\tcram_release_ptl(vmf, level);\n+\t\tcond_resched();\n+\t\treturn 0;\n+\t}\n+\n+\t/* Folio isolated, release PTL, proceed to migration */\n+\tcram_release_ptl(vmf, level);\n+\n+\tnode_stat_mod_folio(folio,\n+\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(folio),\n+\t\t\t    folio_nr_pages(folio));\n+\tlist_add(&folio->lru, &folios);\n+\n+\tmigrate_pages(&folios, alloc_cram_promote_folio, NULL,\n+\t\t      (unsigned long)numa_node_id(),\n+\t\t      MIGRATE_SYNC, MR_NUMA_MISPLACED, &nr_succeeded);\n+\n+\t/* Put failed folios back on LRU; retry on next fault */\n+\tlist_for_each_entry_safe(f, f2, &folios, lru) {\n+\t\tlist_del(&f->lru);\n+\t\tnode_stat_mod_folio(f,\n+\t\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(f),\n+\t\t\t\t    -folio_nr_pages(f));\n+\t\tfolio_putback_lru(f);\n+\t}\n+\n+\t/*\n+\t * If migration failed, folio_putback_lru() batched the folio\n+\t * into this CPU's per-CPU LRU cache (PG_lru not yet set).\n+\t * Drain now so the folio is immediately visible on the LRU,\n+\t * the next fault can then isolate it without an IPI storm\n+\t * via lru_add_drain_all().\n+\t *\n+\t * Return VM_FAULT_RETRY after releasing the fault lock so the\n+\t * arch handler retries from scratch.  Without this, returning 0\n+\t * causes a tight livelock: the process immediately re-faults on\n+\t * the same write-protected entry, alloc fails again, and\n+\t * VM_FAULT_OOM eventually leaks out through a stale path.\n+\t * VM_FAULT_RETRY gives the system breathing room to reclaim.\n+\t */\n+\tif (!nr_succeeded) {\n+\t\tlru_add_drain();\n+\t\tcond_resched();\n+\t\tput_cram_node(cn);\n+\t\trelease_fault_lock(vmf);\n+\t\treturn VM_FAULT_RETRY;\n+\t}\n+\n+\tcond_resched();\n+\tput_cram_node(cn);\n+\treturn 0;\n+}\n+\n+static void cram_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static void cram_reclaim_policy(int nid, struct node_reclaim_policy *policy)\n+{\n+\tpolicy->may_swap = true;\n+\tpolicy->may_writepage = true;\n+\tpolicy->managed_watermarks = true;\n+}\n+\n+static vm_fault_t cram_handle_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t    enum pgtable_level level)\n+{\n+\treturn cram_fault(folio, vmf, level);\n+}\n+\n+static const struct node_private_ops cram_ops = {\n+\t.handle_fault\t\t= cram_handle_fault,\n+\t.migrate_to\t\t= cram_migrate_to,\n+\t.folio_migrate\t\t= cram_folio_migrate,\n+\t.free_folio\t\t= cram_free_folio_cb,\n+\t.reclaim_policy\t\t= cram_reclaim_policy,\n+\t.flags\t\t\t= NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n+\t\t\t\t  NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE |\n+\t\t\t\t  NP_OPS_RECLAIM,\n+};\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data)\n+{\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (cn) {\n+\t\tif (cn->owner != owner) {\n+\t\t\tmutex_unlock(&cram_mutex);\n+\t\t\treturn -EBUSY;\n+\t\t}\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn 0;\n+\t}\n+\n+\tcn = kzalloc(sizeof(*cn), GFP_KERNEL);\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENOMEM;\n+\t}\n+\n+\tcn->owner = owner;\n+\tcn->pressure = 0;\n+\tcn->flush_cb = flush_cb;\n+\tcn->flush_data = flush_data;\n+\trefcount_set(&cn->refcount, 1);\n+\n+\tret = node_private_set_ops(nid, &cram_ops);\n+\tif (ret) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\tkfree(cn);\n+\t\treturn ret;\n+\t}\n+\n+\trcu_assign_pointer(cram_nodes[nid], cn);\n+\n+\t/* Start kswapd on the private node for LRU aging and reclaim */\n+\tkswapd_run(nid);\n+\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* Now that ops->migrate_to is set, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_register_private_node);\n+\n+int cram_unregister_private_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tkswapd_stop(nid);\n+\n+\tWARN_ON(node_private_clear_ops(nid, &cram_ops));\n+\trcu_assign_pointer(cram_nodes[nid], NULL);\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* ops->migrate_to cleared, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\n+\tsynchronize_rcu();\n+\twhile (!refcount_dec_if_one(&cn->refcount))\n+\t\tcond_resched();\n+\tkfree(cn);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unregister_private_node);\n+\n+int cram_unpurge(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tcn->purged = false;\n+\n+\tmutex_unlock(&cram_mutex);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unpurge);\n+\n+void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+\tstruct cram_node *cn;\n+\tstruct node_private *np;\n+\tstruct zone *zone;\n+\tunsigned long managed, boost;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\treturn;\n+\n+\tif (pressure > CRAM_PRESSURE_MAX)\n+\t\tpressure = CRAM_PRESSURE_MAX;\n+\n+\tWRITE_ONCE(cn->pressure, pressure);\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\t/* Block demotions only at maximum pressure */\n+\tif (np)\n+\t\tWRITE_ONCE(np->migration_blocked,\n+\t\t\t   pressure >= CRAM_PRESSURE_MAX);\n+\trcu_read_unlock();\n+\n+\tzone = NULL;\n+\tfor (int i = 0; i < MAX_NR_ZONES; i++) {\n+\t\tstruct zone *z = &NODE_DATA(nid)->node_zones[i];\n+\n+\t\tif (zone_managed_pages(z) > 0) {\n+\t\t\tzone = z;\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+\tif (!zone) {\n+\t\tput_cram_node(cn);\n+\t\treturn;\n+\t}\n+\tmanaged = zone_managed_pages(zone);\n+\n+\t/* Boost proportional to pressure. 0:no boost, 1000:full managed */\n+\tboost = (managed * (unsigned long)pressure) / CRAM_PRESSURE_MAX;\n+\tWRITE_ONCE(zone->watermark_boost, boost);\n+\n+\tif (boost) {\n+\t\tset_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);\n+\t\twakeup_kswapd(zone, GFP_KERNEL, 0, ZONE_MOVABLE);\n+\t}\n+\n+\tput_cram_node(cn);\n+}\n+EXPORT_SYMBOL_GPL(cram_set_pressure);\n+\n+void cram_clear_pressure(int nid)\n+{\n+\tcram_set_pressure(nid, 0);\n+}\n+EXPORT_SYMBOL_GPL(cram_clear_pressure);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the need for a sysram region to directly perform memory hotplug operations, which would eliminate the intermediate dax_region/dax device layer. The author agrees that this feature is necessary and explains how it will work, including its key features such as supporting memory tier integration and automatically hotplugging memory on probe if online type is configured.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed to implement a new feature",
                "explained the benefits of the feature"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CXL sysram region for direct memory hotplug of CXL RAM regions.\n\nThis region eliminates the intermediate dax_region/dax device layer by\ndirectly performing memory hotplug operations.\n\nKey features:\n- Supports memory tier integration for proper NUMA placement\n- Uses the CXL_SYSRAM_ONLINE_* Kconfig options for default online type\n- Automatically hotplugs memory on probe if online type is configured\n- Will be extended to support private memory nodes in the future\n\nThe driver registers a sysram_regionN device as a child of the CXL\nregion, managing the memory hotplug lifecycle through device add/remove.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/Makefile        |   1 +\n drivers/cxl/core/core.h          |   4 +\n drivers/cxl/core/port.c          |   2 +\n drivers/cxl/core/region_sysram.c | 351 +++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h                |  48 +++++\n 5 files changed, 406 insertions(+)\n create mode 100644 drivers/cxl/core/region_sysram.c\n\ndiff --git a/drivers/cxl/core/Makefile b/drivers/cxl/core/Makefile\nindex d3ec8aea64c5..d7ce52c50810 100644\n--- a/drivers/cxl/core/Makefile\n+++ b/drivers/cxl/core/Makefile\n@@ -18,6 +18,7 @@ cxl_core-$(CONFIG_TRACING) += trace.o\n cxl_core-$(CONFIG_CXL_REGION) += region.o\n cxl_core-$(CONFIG_CXL_REGION) += region_dax.o\n cxl_core-$(CONFIG_CXL_REGION) += region_pmem.o\n+cxl_core-$(CONFIG_CXL_REGION) += region_sysram.o\n cxl_core-$(CONFIG_CXL_MCE) += mce.o\n cxl_core-$(CONFIG_CXL_FEATURES) += features.o\n cxl_core-$(CONFIG_CXL_EDAC_MEM_FEATURES) += edac.o\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 6e1f695fd155..973bbcae43f7 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -35,6 +35,7 @@ extern struct device_attribute dev_attr_delete_region;\n extern struct device_attribute dev_attr_region;\n extern const struct device_type cxl_pmem_region_type;\n extern const struct device_type cxl_dax_region_type;\n+extern const struct device_type cxl_sysram_type;\n extern const struct device_type cxl_region_type;\n \n int cxl_decoder_detach(struct cxl_region *cxlr,\n@@ -46,6 +47,7 @@ int cxl_decoder_detach(struct cxl_region *cxlr,\n #define SET_CXL_REGION_ATTR(x) (&dev_attr_##x.attr),\n #define CXL_PMEM_REGION_TYPE(x) (&cxl_pmem_region_type)\n #define CXL_DAX_REGION_TYPE(x) (&cxl_dax_region_type)\n+#define CXL_SYSRAM_TYPE(x) (&cxl_sysram_type)\n int cxl_region_init(void);\n void cxl_region_exit(void);\n int cxl_get_poison_by_endpoint(struct cxl_port *port);\n@@ -54,6 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\n@@ -88,6 +91,7 @@ static inline void cxl_region_exit(void)\n #define SET_CXL_REGION_ATTR(x)\n #define CXL_PMEM_REGION_TYPE(x) NULL\n #define CXL_DAX_REGION_TYPE(x) NULL\n+#define CXL_SYSRAM_TYPE(x) NULL\n #endif\n \n struct cxl_send_command;\ndiff --git a/drivers/cxl/core/port.c b/drivers/cxl/core/port.c\nindex 5c82e6f32572..d6e82b3c2b64 100644\n--- a/drivers/cxl/core/port.c\n+++ b/drivers/cxl/core/port.c\n@@ -66,6 +66,8 @@ static int cxl_device_id(const struct device *dev)\n \t\treturn CXL_DEVICE_PMEM_REGION;\n \tif (dev->type == CXL_DAX_REGION_TYPE())\n \t\treturn CXL_DEVICE_DAX_REGION;\n+\tif (dev->type == CXL_SYSRAM_TYPE())\n+\t\treturn CXL_DEVICE_SYSRAM;\n \tif (is_cxl_port(dev)) {\n \t\tif (is_cxl_root(to_cxl_port(dev)))\n \t\t\treturn CXL_DEVICE_ROOT;\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nnew file mode 100644\nindex 000000000000..47a415deb352\n--- /dev/null\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -0,0 +1,351 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Sysram Region - Direct memory hotplug for CXL RAM regions\n+ *\n+ * This interface directly performs memory hotplug for CXL RAM regions,\n+ * eliminating the indirection through DAX.\n+ */\n+\n+#include <linux/memory_hotplug.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/memory.h>\n+#include <linux/device.h>\n+#include <linux/slab.h>\n+#include <linux/mm.h>\n+#include <cxlmem.h>\n+#include <cxl.h>\n+#include \"core.h\"\n+\n+static const char *sysram_res_name = \"System RAM (CXL)\";\n+\n+/**\n+ * cxl_region_find_sysram - Find the sysram device associated with a region\n+ * @cxlr: The CXL region\n+ *\n+ * Finds and returns the sysram child device of a CXL region.\n+ * The caller must release the device reference with put_device()\n+ * when done with the returned pointer.\n+ *\n+ * Return: Pointer to cxl_sysram, or NULL if not found\n+ */\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram;\n+\tstruct device *sdev;\n+\tchar sname[32];\n+\n+\tsnprintf(sname, sizeof(sname), \"sysram_region%d\", cxlr->id);\n+\tsdev = device_find_child_by_name(&cxlr->dev, sname);\n+\tif (!sdev)\n+\t\treturn NULL;\n+\n+\tsysram = to_cxl_sysram(sdev);\n+\treturn sysram;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_region_find_sysram, \"CXL\");\n+\n+static int sysram_get_numa_node(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_region_params *p = &cxlr->params;\n+\tint nid;\n+\n+\tnid = phys_to_target_node(p->res->start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(p->res->start);\n+\n+\treturn nid;\n+}\n+\n+static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n+{\n+\tstruct resource *res;\n+\tmhp_t mhp_flags;\n+\tint rc;\n+\n+\tif (sysram->res)\n+\t\treturn -EBUSY;\n+\n+\tres = request_mem_region(sysram->hpa_range.start,\n+\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t sysram->res_name);\n+\tif (!res)\n+\t\treturn -EBUSY;\n+\n+\tsysram->res = res;\n+\n+\t/*\n+\t * Set flags appropriate for System RAM. Leave ..._BUSY clear\n+\t * so that add_memory() can add a child resource.\n+\t */\n+\tres->flags = IORESOURCE_SYSTEM_RAM;\n+\n+\tmhp_flags = MHP_NID_IS_MGID;\n+\n+\t/*\n+\t * Ensure that future kexec'd kernels will not treat\n+\t * this as RAM automatically.\n+\t */\n+\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t online_type);\n+\tif (rc) {\n+\t\tremove_resource(res);\n+\t\tkfree(res);\n+\t\tsysram->res = NULL;\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n+{\n+\tint rc;\n+\n+\tif (!sysram->res)\n+\t\treturn 0;\n+\n+\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t       range_len(&sysram->hpa_range));\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tif (sysram->res) {\n+\t\tremove_resource(sysram->res);\n+\t\tkfree(sysram->res);\n+\t\tsysram->res = NULL;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn sysram_hotplug_remove(sysram);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_offline_and_remove, \"CXL\");\n+\n+static void cxl_sysram_release(struct device *dev)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\n+\tif (sysram->res)\n+\t\tsysram_hotplug_remove(sysram);\n+\n+\tkfree(sysram->res_name);\n+\n+\tif (sysram->mgid >= 0)\n+\t\tmemory_group_unregister(sysram->mgid);\n+\n+\tif (sysram->mtype)\n+\t\tclear_node_memory_type(sysram->numa_node, sysram->mtype);\n+\n+\tkfree(sysram);\n+}\n+\n+static ssize_t hotplug_store(struct device *dev,\n+\t\t\t     struct device_attribute *attr,\n+\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\tint online_type, rc;\n+\n+\tonline_type = mhp_online_type_from_str(buf);\n+\tif (online_type < 0)\n+\t\treturn online_type;\n+\n+\tif (online_type == MMOP_OFFLINE)\n+\t\trc = sysram_hotplug_remove(sysram);\n+\telse\n+\t\trc = sysram_hotplug_add(sysram, online_type);\n+\n+\tif (rc)\n+\t\tdev_warn(dev, \"hotplug %s failed: %d\\n\",\n+\t\t\t online_type == MMOP_OFFLINE ? \"offline\" : \"online\", rc);\n+\n+\treturn rc ? rc : len;\n+}\n+static DEVICE_ATTR_WO(hotplug);\n+\n+static struct attribute *cxl_sysram_attrs[] = {\n+\t&dev_attr_hotplug.attr,\n+\tNULL\n+};\n+\n+static const struct attribute_group cxl_sysram_attribute_group = {\n+\t.attrs = cxl_sysram_attrs,\n+};\n+\n+static const struct attribute_group *cxl_sysram_attribute_groups[] = {\n+\t&cxl_base_attribute_group,\n+\t&cxl_sysram_attribute_group,\n+\tNULL\n+};\n+\n+const struct device_type cxl_sysram_type = {\n+\t.name = \"cxl_sysram\",\n+\t.release = cxl_sysram_release,\n+\t.groups = cxl_sysram_attribute_groups,\n+};\n+\n+static bool is_cxl_sysram(struct device *dev)\n+{\n+\treturn dev->type == &cxl_sysram_type;\n+}\n+\n+struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\tif (dev_WARN_ONCE(dev, !is_cxl_sysram(dev),\n+\t\t\t  \"not a cxl_sysram device\\n\"))\n+\t\treturn NULL;\n+\treturn container_of(dev, struct cxl_sysram, dev);\n+}\n+EXPORT_SYMBOL_NS_GPL(to_cxl_sysram, \"CXL\");\n+\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram)\n+{\n+\treturn &sysram->dev;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_dev, \"CXL\");\n+\n+static struct lock_class_key cxl_sysram_key;\n+\n+static enum mmop cxl_sysram_get_default_online_type(void)\n+{\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_SYSTEM_DEFAULT))\n+\t\treturn mhp_get_default_online_type();\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_MOVABLE))\n+\t\treturn MMOP_ONLINE_MOVABLE;\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_NORMAL))\n+\t\treturn MMOP_ONLINE;\n+\treturn MMOP_OFFLINE;\n+}\n+\n+static struct cxl_sysram *cxl_sysram_alloc(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram __free(kfree) = NULL;\n+\tstruct device *dev;\n+\n+\tsysram = kzalloc(sizeof(*sysram), GFP_KERNEL);\n+\tif (!sysram)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tsysram->online_type = cxl_sysram_get_default_online_type();\n+\tsysram->last_hotplug_cmd = MMOP_OFFLINE;\n+\tsysram->numa_node = -1;\n+\tsysram->mgid = -1;\n+\n+\tdev = &sysram->dev;\n+\tsysram->cxlr = cxlr;\n+\tdevice_initialize(dev);\n+\tlockdep_set_class(&dev->mutex, &cxl_sysram_key);\n+\tdevice_set_pm_not_required(dev);\n+\tdev->parent = &cxlr->dev;\n+\tdev->bus = &cxl_bus_type;\n+\tdev->type = &cxl_sysram_type;\n+\n+\treturn_ptr(sysram);\n+}\n+\n+static void sysram_unregister(void *_sysram)\n+{\n+\tstruct cxl_sysram *sysram = _sysram;\n+\n+\tdevice_unregister(&sysram->dev);\n+}\n+\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+{\n+\tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n+\tstruct memory_dev_type *mtype;\n+\tstruct range hpa_range;\n+\tstruct device *dev;\n+\tint adist = MEMTIER_DEFAULT_LOWTIER_ADISTANCE;\n+\tint numa_node;\n+\tint rc;\n+\n+\trc = cxl_region_get_hpa_range(cxlr, &hpa_range);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\thpa_range = memory_block_align_range(&hpa_range);\n+\tif (hpa_range.start >= hpa_range.end) {\n+\t\tdev_warn(&cxlr->dev, \"region too small after alignment\\n\");\n+\t\treturn -ENOSPC;\n+\t}\n+\n+\tsysram = cxl_sysram_alloc(cxlr);\n+\tif (IS_ERR(sysram))\n+\t\treturn PTR_ERR(sysram);\n+\n+\tsysram->hpa_range = hpa_range;\n+\n+\tsysram->res_name = kasprintf(GFP_KERNEL, \"cxl_sysram%d\", cxlr->id);\n+\tif (!sysram->res_name)\n+\t\treturn -ENOMEM;\n+\n+\t/* Override default online type if caller specified one */\n+\tif (online_type >= 0)\n+\t\tsysram->online_type = online_type;\n+\n+\tdev = &sysram->dev;\n+\n+\trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Setup memory tier before adding device */\n+\tnuma_node = sysram_get_numa_node(cxlr);\n+\tif (numa_node < 0) {\n+\t\tdev_warn(&cxlr->dev, \"rejecting region with invalid node: %d\\n\",\n+\t\t\t numa_node);\n+\t\treturn -EINVAL;\n+\t}\n+\tsysram->numa_node = numa_node;\n+\n+\tmt_calc_adistance(numa_node, &adist);\n+\tmtype = mt_get_memory_type(adist);\n+\tif (IS_ERR(mtype))\n+\t\treturn PTR_ERR(mtype);\n+\tsysram->mtype = mtype;\n+\n+\tinit_node_memory_type(numa_node, mtype);\n+\n+\t/* Register memory group for this region */\n+\trc = memory_group_register_static(numa_node,\n+\t\t\t\t\t  PFN_UP(range_len(&hpa_range)));\n+\tif (rc < 0)\n+\t\treturn rc;\n+\tsysram->mgid = rc;\n+\n+\trc = device_add(dev);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n+\t\tdev_name(dev));\n+\n+\t/*\n+\t * Dynamic capacity regions (DCD) will have memory added later.\n+\t * For static RAM regions, hotplug the entire range now.\n+\t */\n+\tif (cxlr->mode != CXL_PARTMODE_RAM)\n+\t\tgoto out;\n+\n+\t/* If default online_type is a valid online mode, immediately hotplug */\n+\tif (sysram->online_type > MMOP_OFFLINE) {\n+\t\trc = sysram_hotplug_add(sysram, sysram->online_type);\n+\t\tif (rc)\n+\t\t\tdev_warn(dev, \"hotplug failed: %d\\n\", rc);\n+\t\telse\n+\t\t\tsysram->last_hotplug_cmd = sysram->online_type;\n+\t}\n+\n+out:\n+\treturn devm_add_action_or_reset(&cxlr->dev, sysram_unregister,\n+\t\t\t\t\tno_free_ptr(sysram));\n+}\n+EXPORT_SYMBOL_NS_GPL(devm_cxl_add_sysram, \"CXL\");\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex f899f240f229..8e8342fd4fde 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -607,6 +607,34 @@ struct cxl_dax_region {\n \tenum dax_driver_type dax_driver;\n };\n \n+/**\n+ * struct cxl_sysram - CXL SysRAM region for system memory hotplug\n+ * @dev: device for this sysram\n+ * @cxlr: parent cxl_region\n+ * @online_type: Default memory online type for new hotplug ops (MMOP_* value)\n+ * @last_hotplug_cmd: Last hotplug command submitted (MMOP_* value)\n+ * @hpa_range: Host physical address range for the region\n+ * @res_name: Resource name for the memory region\n+ * @res: Memory resource (set when hotplugged)\n+ * @mgid: Memory group id\n+ * @mtype: Memory tier type\n+ * @numa_node: NUMA node for this memory\n+ *\n+ * Device that directly performs memory hotplug for CXL RAM regions.\n+ */\n+struct cxl_sysram {\n+\tstruct device dev;\n+\tstruct cxl_region *cxlr;\n+\tenum mmop online_type;\n+\tint last_hotplug_cmd;\n+\tstruct range hpa_range;\n+\tconst char *res_name;\n+\tstruct resource *res;\n+\tint mgid;\n+\tstruct memory_dev_type *mtype;\n+\tint numa_node;\n+};\n+\n /**\n  * struct cxl_port - logical collection of upstream port devices and\n  *\t\t     downstream port devices to construct a CXL memory\n@@ -807,6 +835,7 @@ DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device\n DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n DEFINE_FREE(put_cxl_dax_region, struct cxl_dax_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n+DEFINE_FREE(put_cxl_sysram, struct cxl_sysram *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n \n int devm_cxl_enumerate_ports(struct cxl_memdev *cxlmd);\n void cxl_bus_rescan(void);\n@@ -889,6 +918,7 @@ void cxl_destroy_region(struct cxl_region *cxlr);\n struct device *cxl_region_dev(struct cxl_region *cxlr);\n enum cxl_partition_mode cxl_region_mode(struct cxl_region *cxlr);\n int cxl_get_region_range(struct cxl_region *cxlr, struct range *range);\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr);\n int cxl_get_committed_regions(struct cxl_memdev *cxlmd,\n \t\t\t      struct cxl_region **regions, int max_regions);\n struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n@@ -936,6 +966,7 @@ void cxl_driver_unregister(struct cxl_driver *cxl_drv);\n #define CXL_DEVICE_PMEM_REGION\t\t7\n #define CXL_DEVICE_DAX_REGION\t\t8\n #define CXL_DEVICE_PMU\t\t\t9\n+#define CXL_DEVICE_SYSRAM\t\t10\n \n #define MODULE_ALIAS_CXL(type) MODULE_ALIAS(\"cxl:t\" __stringify(type) \"*\")\n #define CXL_MODALIAS_FMT \"cxl:t%d\"\n@@ -954,6 +985,10 @@ bool is_cxl_pmem_region(struct device *dev);\n struct cxl_pmem_region *to_cxl_pmem_region(struct device *dev);\n int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n+struct cxl_sysram *to_cxl_sysram(struct device *dev);\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n static inline bool is_cxl_pmem_region(struct device *dev)\n@@ -972,6 +1007,19 @@ static inline struct cxl_dax_region *to_cxl_dax_region(struct device *dev)\n {\n \treturn NULL;\n }\n+static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\treturn NULL;\n+}\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+\t\t\t\t      enum mmop online_type)\n+{\n+\treturn -ENXIO;\n+}\n+static inline int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn -ENXIO;\n+}\n static inline u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint,\n \t\t\t\t\t       u64 spa)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private memory regions being isolated from normal allocations and reclaim by adding support for N_MEMORY_PRIVATE hotplug via add_private_memory_driver_managed(). They modified the cxl_sysram region to register as a private node when private=true is passed to devm_cxl_add_sysram(), allowing callers to isolate their memory. A fix is planned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "planned a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Extend the cxl_sysram region to support N_MEMORY_PRIVATE hotplug\nvia add_private_memory_driver_managed(). When a caller passes\nprivate=true to devm_cxl_add_sysram(), the memory is registered\nas a private node, isolating it from normal allocations and reclaim.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/core.h          |  2 +-\n drivers/cxl/core/region_sysram.c | 50 +++++++++++++++++++++++++-------\n drivers/cxl/cxl.h                |  9 ++++--\n 3 files changed, 48 insertions(+), 13 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 973bbcae43f7..8ca3d6d41fe4 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -56,7 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nindex 47a415deb352..77aaa52e7332 100644\n--- a/drivers/cxl/core/region_sysram.c\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -85,12 +85,23 @@ static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n \t/*\n \t * Ensure that future kexec'd kernels will not treat\n \t * this as RAM automatically.\n+\t *\n+\t * For private regions, use add_private_memory_driver_managed()\n+\t * to register as N_MEMORY_PRIVATE which isolates the memory from\n+\t * normal allocations and reclaim.\n \t */\n-\trc = __add_memory_driver_managed(sysram->mgid,\n-\t\t\t\t\t sysram->hpa_range.start,\n-\t\t\t\t\t range_len(&sysram->hpa_range),\n-\t\t\t\t\t sysram_res_name, mhp_flags,\n-\t\t\t\t\t online_type);\n+\tif (sysram->private)\n+\t\trc = add_private_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t       sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t       online_type, &sysram->np);\n+\telse\n+\t\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t online_type);\n \tif (rc) {\n \t\tremove_resource(res);\n \t\tkfree(res);\n@@ -108,10 +119,23 @@ static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n \tif (!sysram->res)\n \t\treturn 0;\n \n-\trc = offline_and_remove_memory(sysram->hpa_range.start,\n-\t\t\t\t       range_len(&sysram->hpa_range));\n-\tif (rc)\n-\t\treturn rc;\n+\tif (sysram->private) {\n+\t\trc = offline_and_remove_private_memory(sysram->numa_node,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\t/*\n+\t\t * -EBUSY means memory was removed but node_private_unregister()\n+\t\t * could not complete because other regions share the node.\n+\t\t * Continue to resource cleanup since the memory is gone.\n+\t\t */\n+\t\tif (rc && rc != -EBUSY)\n+\t\t\treturn rc;\n+\t} else {\n+\t\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\t}\n \n \tif (sysram->res) {\n \t\tremove_resource(sysram->res);\n@@ -257,7 +281,8 @@ static void sysram_unregister(void *_sysram)\n \tdevice_unregister(&sysram->dev);\n }\n \n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n+\t\t\tenum mmop online_type)\n {\n \tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n \tstruct memory_dev_type *mtype;\n@@ -291,6 +316,11 @@ int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n \tif (online_type >= 0)\n \t\tsysram->online_type = online_type;\n \n+\t/* Set up private node registration if requested */\n+\tsysram->private = private;\n+\tif (private)\n+\t\tsysram->np.owner = sysram;\n+\n \tdev = &sysram->dev;\n \n \trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 8e8342fd4fde..54e5f9ac59dc 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -10,6 +10,7 @@\n #include <linux/bitops.h>\n #include <linux/log2.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n #include <linux/io.h>\n #include <linux/range.h>\n #include <linux/dax.h>\n@@ -619,6 +620,8 @@ struct cxl_dax_region {\n  * @mgid: Memory group id\n  * @mtype: Memory tier type\n  * @numa_node: NUMA node for this memory\n+ * @private: true if this region uses N_MEMORY_PRIVATE hotplug\n+ * @np: private node registration state (valid when @private is true)\n  *\n  * Device that directly performs memory hotplug for CXL RAM regions.\n  */\n@@ -633,6 +636,8 @@ struct cxl_sysram {\n \tint mgid;\n \tstruct memory_dev_type *mtype;\n \tint numa_node;\n+\tbool private;\n+\tstruct node_private np;\n };\n \n /**\n@@ -987,7 +992,7 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n struct cxl_sysram *to_cxl_sysram(struct device *dev);\n struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n@@ -1011,7 +1016,7 @@ static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n {\n \treturn NULL;\n }\n-static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n \t\t\t\t      enum mmop online_type)\n {\n \treturn -ENXIO;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the driver's interaction with the migration target control, explaining that they moved struct migration_target_control to include/linux/migrate.h so the driver can use alloc_migration_target() without depending on mm-internal headers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a sample CXL type-3 driver that registers device memory as\nprivate-node NUMA memory reachable only via explicit mempolicy\n(set_mempolicy / mbind).\n\nProbe flow:\n  1. Call cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Look for pre-committed RAM regions; if none exist, create one\n     using cxl_get_hpa_freespace() + cxl_request_dpa() +\n     cxl_create_region()\n  3. Convert the region to sysram via devm_cxl_add_sysram() with\n     private=true and MMOP_ONLINE_MOVABLE\n  4. Register node_private_ops with NP_OPS_MIGRATION | NP_OPS_MEMPOLICY\n     so the node is excluded from default allocations\n\nThe migrate_to callback uses alloc_migration_target() with\n__GFP_THISNODE | __GFP_PRIVATE to keep pages on the target node.\n\nMove struct migration_target_control from mm/internal.h to\ninclude/linux/migrate.h so the driver can use alloc_migration_target()\nwithout depending on mm-internal headers.\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/Kconfig                           |   2 +\n drivers/cxl/Makefile                          |   2 +\n drivers/cxl/type3_drivers/Kconfig             |   2 +\n drivers/cxl/type3_drivers/Makefile            |   2 +\n .../cxl/type3_drivers/cxl_mempolicy/Kconfig   |  16 +\n .../cxl/type3_drivers/cxl_mempolicy/Makefile  |   4 +\n .../type3_drivers/cxl_mempolicy/mempolicy.c   | 297 ++++++++++++++++++\n include/linux/migrate.h                       |   7 +-\n mm/internal.h                                 |   7 -\n 9 files changed, 331 insertions(+), 8 deletions(-)\n create mode 100644 drivers/cxl/type3_drivers/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n\ndiff --git a/drivers/cxl/Kconfig b/drivers/cxl/Kconfig\nindex f99aa7274d12..1648cdeaa0c9 100644\n--- a/drivers/cxl/Kconfig\n+++ b/drivers/cxl/Kconfig\n@@ -278,4 +278,6 @@ config CXL_ATL\n \tdepends on CXL_REGION\n \tdepends on ACPI_PRMT && AMD_NB\n \n+source \"drivers/cxl/type3_drivers/Kconfig\"\n+\n endif\ndiff --git a/drivers/cxl/Makefile b/drivers/cxl/Makefile\nindex 2caa90fa4bf2..94d2b2233bf8 100644\n--- a/drivers/cxl/Makefile\n+++ b/drivers/cxl/Makefile\n@@ -19,3 +19,5 @@ cxl_acpi-y := acpi.o\n cxl_pmem-y := pmem.o security.o\n cxl_mem-y := mem.o\n cxl_pci-y := pci.o\n+\n+obj-y += type3_drivers/\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nnew file mode 100644\nindex 000000000000..369b21763856\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nnew file mode 100644\nindex 000000000000..2b82265ff118\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\nnew file mode 100644\nindex 000000000000..3c45da237b9f\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n@@ -0,0 +1,16 @@\n+config CXL_MEMPOLICY\n+\ttristate \"CXL Private Memory with Mempolicy Support\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\thelp\n+\t  Minimal driver for CXL memory devices that registers memory as\n+\t  N_MEMORY_PRIVATE with mempolicy support.  The memory is isolated\n+\t  from default allocations and can only be reached via explicit\n+\t  mempolicy (set_mempolicy or mbind).\n+\n+\t  No compression, no PTE controls, the memory behaves like normal\n+\t  DRAM but is excluded from fallback allocations.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\nnew file mode 100644\nindex 000000000000..dfb58fc88ad9\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy.o\n+cxl_mempolicy-y := mempolicy.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\nnew file mode 100644\nindex 000000000000..1c19818eb268\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n@@ -0,0 +1,297 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Mempolicy Driver\n+ *\n+ * Minimal driver for CXL memory devices that registers memory as\n+ * N_MEMORY_PRIVATE with mempolicy support but no PTE controls.  The\n+ * memory behaves like normal DRAM but is isolated from default allocations,\n+ * it can only be reached via explicit mempolicy (set_mempolicy/mbind).\n+ *\n+ * Usage:\n+ *   1. Unbind device from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   2. Bind to cxl_mempolicy:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n+ */\n+\n+#include <linux/module.h>\n+#include <linux/pci.h>\n+#include <linux/xarray.h>\n+#include <linux/node_private.h>\n+#include <linux/migrate.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+struct cxl_mempolicy_ctx {\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint nid;\n+};\n+\n+static DEFINE_XARRAY(ctx_xa);\n+\n+static struct cxl_mempolicy_ctx *memdev_to_ctx(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\n+\treturn xa_load(&ctx_xa, (unsigned long)pdev);\n+}\n+\n+static int cxl_mempolicy_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason,\n+\t\t\t\t    unsigned int *nr_succeeded)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE |\n+\t\t\t    __GFP_PRIVATE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, nr_succeeded);\n+}\n+\n+static void cxl_mempolicy_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static const struct node_private_ops cxl_mempolicy_ops = {\n+\t.migrate_to\t= cxl_mempolicy_migrate_to,\n+\t.folio_migrate\t= cxl_mempolicy_folio_migrate,\n+\t.flags = NP_OPS_MIGRATION | NP_OPS_MEMPOLICY,\n+};\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tctx->cxled = cxled;\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\treturn cxlr;\n+}\n+\n+static int setup_private_node(struct cxl_memdev *cxlmd,\n+\t\t\t      struct cxl_region *cxlr)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct range hpa_range;\n+\tint rc;\n+\n+\tdevice_release_driver(cxl_region_dev(cxlr));\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to add sysram: %d\\n\", rc);\n+\t\tif (device_attach(cxl_region_dev(cxlr)) < 0)\n+\t\t\tdev_warn(cxl_region_dev(cxlr),\n+\t\t\t\t \"failed to re-attach driver\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tctx->nid = phys_to_target_node(hpa_range.start);\n+\tif (ctx->nid == NUMA_NO_NODE)\n+\t\tctx->nid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\trc = node_private_set_ops(ctx->nid, &cxl_mempolicy_ops);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to set ops on node %d: %d\\n\", ctx->nid, rc);\n+\t\tctx->nid = NUMA_NO_NODE;\n+\t\treturn rc;\n+\t}\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"node %d registered as private mempolicy memory\\n\", ctx->nid);\n+\treturn 0;\n+}\n+\n+static int cxl_mempolicy_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i;\n+\tint rc;\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"cxl_mempolicy attach: looking for regions\\n\");\n+\n+\t/* Phase 1: look for pre-committed RAM regions */\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) != CXL_PARTMODE_RAM) {\n+\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tcxlr = regions[i];\n+\t\trc = setup_private_node(cxlmd, cxlr);\n+\t\tput_device(cxl_region_dev(cxlr));\n+\t\tif (rc == 0) {\n+\t\t\t/* Release remaining region references */\n+\t\t\tfor (i++; i < nr; i++)\n+\t\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\treturn 0;\n+\t\t}\n+\t}\n+\n+\t/* Phase 2: no committed regions, create one */\n+\tdev_info(&cxlmd->dev,\n+\t\t \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"no RAM capacity: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = setup_private_node(cxlmd, cxlr);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to setup private node: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\t/* Only take ownership of regions we created (Phase 2) */\n+\tmemdev_to_ctx(cxlmd)->cxlr = cxlr;\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_mempolicy_attach = {\n+\t.probe = cxl_mempolicy_attach_probe,\n+};\n+\n+static int cxl_mempolicy_probe(struct pci_dev *pdev,\n+\t\t\t       const struct pci_device_id *id)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probing device\\n\");\n+\n+\tctx = devm_kzalloc(&pdev->dev, sizeof(*ctx), GFP_KERNEL);\n+\tif (!ctx)\n+\t\treturn -ENOMEM;\n+\tctx->nid = NUMA_NO_NODE;\n+\n+\trc = xa_insert(&ctx_xa, (unsigned long)pdev, ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_mempolicy_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_mempolicy_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = xa_erase(&ctx_xa, (unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: removing device\\n\");\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\tif (ctx->nid != NUMA_NO_NODE)\n+\t\tWARN_ON(node_private_clear_ops(ctx->nid, &cxl_mempolicy_ops));\n+\n+\tif (ctx->cxlr) {\n+\t\tcxl_destroy_region(ctx->cxlr);\n+\t\tctx->cxlr = NULL;\n+\t}\n+\n+\tif (ctx->cxled) {\n+\t\tcxl_dpa_free(ctx->cxled);\n+\t\tctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_mempolicy_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_mempolicy_pci_tbl);\n+\n+static struct pci_driver cxl_mempolicy_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_mempolicy_pci_tbl,\n+\t.probe\t\t= cxl_mempolicy_probe,\n+\t.remove\t\t= cxl_mempolicy_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_mempolicy_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Private Memory with Mempolicy Support\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 7b2da3875ff2..1f9fb61f3932 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -10,7 +10,12 @@\n typedef struct folio *new_folio_t(struct folio *folio, unsigned long private);\n typedef void free_folio_t(struct folio *folio, unsigned long private);\n \n-struct migration_target_control;\n+struct migration_target_control {\n+\tint nid;\t\t/* preferred node id */\n+\tnodemask_t *nmask;\n+\tgfp_t gfp_mask;\n+\tenum migrate_reason reason;\n+};\n \n /**\n  * struct movable_operations - Driver page migration\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 64467ca774f1..85cd11189854 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1352,13 +1352,6 @@ extern const struct trace_print_flags gfpflag_names[];\n \n void setup_zone_pageset(struct zone *zone);\n \n-struct migration_target_control {\n-\tint nid;\t\t/* preferred node id */\n-\tnodemask_t *nmask;\n-\tgfp_t gfp_mask;\n-\tenum migrate_reason reason;\n-};\n-\n /*\n  * mm/filemap.c\n  */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the cxl_compression driver's page reclamation using the CXL Media Operations Zero command (opcode 0x4402). The author explains that if the device does not support this command, the driver falls back to inline CPU zeroing.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a generic CXL type-3 driver for compressed memory controllers.\n\nThe driver provides an alternative PCI binding that converts CXL\nRAM regions to private-node sysram and registers them with the\nCRAM subsystem for transparent demotion/promotion.\n\nProbe flow:\n  1. cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Discover/convert auto-RAM regions or create a RAM region\n  3. Convert to private-node sysram via devm_cxl_add_sysram()\n  4. Register with CRAM via cram_register_private_node()\n\nPage flush pipeline:\n  When a CRAM folio is freed, the CRAM free_folio   callback buffers\n  it into a per-CPU RCU-protected flush buffer to offload the operation.\n\n  A periodic kthread swaps the per-CPU buffers under RCU, then sends\n  batched Sanitize-Zero commands so the device can zero pages.\n\n  A flush_record bitmap tracks in-flight pages to avoid re-buffering on\n  the second free_folio entry after folio_put().\n\n  Overflow from full buffers is handled by a per-CPU workqueue fallback.\n\nWatermark interrupts:\n  MSI-X vector 12 - delivers \"Low\" watermark interrupts\n  MSI-X vector 13 - delivers \"High\" watermark interrupts\n  This adjusts CRAM pressure:\n\tLow  - increases pressure.\n  \tHigh - reduces pressure.\n\n  A dynamic watermark mode cycles through four phases with\n  progressively tighter thresholds.\n\n  Static watermark mode sets pressure 0 or MAX respectively.\n\nTeardown ordering:\n  pre_teardown  - cram_unregister + retry-loop memory offline\n  post_teardown - kthread stop, drain all flush buffers via CCI\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/type3_drivers/Kconfig             |    1 +\n drivers/cxl/type3_drivers/Makefile            |    1 +\n .../cxl/type3_drivers/cxl_compression/Kconfig |   20 +\n .../type3_drivers/cxl_compression/Makefile    |    4 +\n .../cxl_compression/compression.c             | 1025 +++++++++++++++++\n 5 files changed, 1051 insertions(+)\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/compression.c\n\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nindex 369b21763856..98f73e46730e 100644\n--- a/drivers/cxl/type3_drivers/Kconfig\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\n+source \"drivers/cxl/type3_drivers/cxl_compression/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nindex 2b82265ff118..f5b0766d92af 100644\n--- a/drivers/cxl/type3_drivers/Makefile\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression/\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Kconfig b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\nnew file mode 100644\nindex 000000000000..8c891a48b000\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\n@@ -0,0 +1,20 @@\n+config CXL_COMPRESSION\n+\ttristate \"CXL Compression Memory Driver\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on CRAM\n+\thelp\n+\t  This driver provides an alternative PCI binding for CXL memory\n+\t  devices with compressed memory support. It converts CXL RAM\n+\t  regions to sysram for direct memory hotplug and registers with\n+\t  the CRAM subsystem for transparent compression.\n+\n+\t  Page reclamation uses the standard CXL Media Operations Zero\n+\t  command (opcode 0x4402). If the device does not support it,\n+\t  the driver falls back to inline CPU zeroing.\n+\n+\t  Usage: First unbind the device from cxl_pci, then bind to\n+\t  cxl_compression. The driver will initialize the CXL device and\n+\t  convert any RAM regions to use direct memory hotplug via sysram.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Makefile b/drivers/cxl/type3_drivers/cxl_compression/Makefile\nnew file mode 100644\nindex 000000000000..46f34809bf74\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression.o\n+cxl_compression-y := compression.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/compression.c b/drivers/cxl/type3_drivers/cxl_compression/compression.c\nnew file mode 100644\nindex 000000000000..e4c8b62227e2\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/compression.c\n@@ -0,0 +1,1025 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Compression Driver\n+ *\n+ * This driver provides an alternative binding for CXL memory devices that\n+ * converts all associated RAM regions to sysram_regions for direct memory\n+ * hotplug, bypassing the standard dax region path.\n+ *\n+ * Page reclamation uses the standard CXL Media Operations Zero command\n+ * (opcode 0x4402, class 0x01, subclass 0x01).  Watermark interrupts\n+ * are delivered via separate MSI-X vectors (12 for lthresh, 13 for\n+ * hthresh), injected externally via QMP.\n+ *\n+ * Usage:\n+ *   1. Device initially binds to cxl_pci at boot\n+ *   2. Unbind from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   3. Bind to cxl_compression:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n+ */\n+\n+#include <linux/unaligned.h>\n+#include <linux/io-64-nonatomic-lo-hi.h>\n+#include <linux/module.h>\n+#include <linux/delay.h>\n+#include <linux/sizes.h>\n+#include <linux/mutex.h>\n+#include <linux/list.h>\n+#include <linux/pci.h>\n+#include <linux/io.h>\n+#include <linux/interrupt.h>\n+#include <linux/bitmap.h>\n+#include <linux/highmem.h>\n+#include <linux/workqueue.h>\n+#include <linux/kthread.h>\n+#include <linux/rcupdate.h>\n+#include <linux/percpu.h>\n+#include <linux/sched.h>\n+#include <linux/cram.h>\n+#include <linux/memory_hotplug.h>\n+#include <linux/xarray.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+/*\n+ * Per-device compression context lookup.\n+ *\n+ * pci_set_drvdata() MUST store cxlds because mbox_to_cxlds() uses\n+ * dev_get_drvdata() to recover the cxl_dev_state from the mailbox host\n+ * device.  Storing anything else in pci drvdata breaks every CXL mailbox\n+ * command.  Use an xarray keyed by pci_dev pointer so that multiple\n+ * devices can bind concurrently without colliding.\n+ */\n+static DEFINE_XARRAY(comp_ctx_xa);\n+\n+static struct cxl_compression_ctx *pdev_to_comp_ctx(struct pci_dev *pdev)\n+{\n+\treturn xa_load(&comp_ctx_xa, (unsigned long)pdev);\n+}\n+\n+#define CXL_MEDIA_OP_OPCODE\t\t0x4402\n+#define CXL_MEDIA_OP_CLASS_SANITIZE\t0x01\n+#define CXL_MEDIA_OP_SUBC_ZERO\t\t0x01\n+\n+struct cxl_dpa_range {\n+\t__le64 starting_dpa;\n+\t__le64 length;\n+} __packed;\n+\n+struct cxl_media_op_input {\n+\tu8 media_operation_class;\n+\tu8 media_operation_subclass;\n+\t__le16 reserved;\n+\t__le32 dpa_range_count;\n+\tstruct cxl_dpa_range ranges[];\n+} __packed;\n+\n+#define CXL_CT3_MSIX_LTHRESH\t\t12\n+#define CXL_CT3_MSIX_HTHRESH\t\t13\n+#define CXL_CT3_MSIX_VECTOR_NR\t\t14\n+#define CXL_FLUSH_INTERVAL_DEFAULT_MS\t1000\n+\n+static unsigned int flush_buf_size;\n+module_param(flush_buf_size, uint, 0444);\n+MODULE_PARM_DESC(flush_buf_size,\n+\t\t \"Max DPA ranges per media ops CCI command (0 = use hw max)\");\n+\n+static unsigned int flush_interval_ms = CXL_FLUSH_INTERVAL_DEFAULT_MS;\n+module_param(flush_interval_ms, uint, 0644);\n+MODULE_PARM_DESC(flush_interval_ms,\n+\t\t \"Flush worker interval in ms (default 1000)\");\n+\n+struct cxl_flush_buf {\n+\tunsigned int count;\n+\tunsigned int max;\t\t\t/* max ranges per command */\n+\tstruct cxl_media_op_input *cmd;\t\t/* pre-allocated CCI payload */\n+\tstruct folio **folios;\t\t\t/* parallel folio tracking */\n+};\n+\n+struct cxl_flush_ctx;\n+\n+struct cxl_pcpu_flush {\n+\tstruct cxl_flush_buf __rcu *active;\t/* callback writes here */\n+\tstruct cxl_flush_buf *overflow_spare;\t/* spare for overflow work */\n+\tstruct work_struct overflow_work;\t/* per-CPU overflow flush */\n+\tstruct cxl_flush_ctx *ctx;\t\t/* backpointer */\n+};\n+\n+/**\n+ * struct cxl_flush_ctx - Per-region flush context\n+ * @flush_record: two-level bitmap, 1 bit per 4KB page, tracks in-flight ops\n+ * @flush_record_pages: number of pages in the flush_record array\n+ * @nr_pages: total number of 4KB pages in the region\n+ * @base_pfn: starting PFN of the region (for DPA offset calculation)\n+ * @buf_max: max DPA ranges per CCI command\n+ * @media_ops_supported: true if device supports media operations zero\n+ * @pcpu: per-CPU flush state\n+ * @kthread_spares: array[nr_cpu_ids] of spare buffers for the kthread\n+ * @flush_thread: round-robin kthread\n+ * @mbox: pointer to CXL mailbox for sending CCI commands\n+ * @dev: device for logging\n+ * @nid: NUMA node of the private region\n+ */\n+struct cxl_flush_ctx {\n+\tunsigned long\t**flush_record;\n+\tunsigned int\t flush_record_pages;\n+\tunsigned long\t nr_pages;\n+\tunsigned long\t base_pfn;\n+\tunsigned int\t buf_max;\n+\tbool\t\t media_ops_supported;\n+\tstruct cxl_pcpu_flush __percpu *pcpu;\n+\tstruct cxl_flush_buf **kthread_spares;\n+\tstruct task_struct *flush_thread;\n+\tstruct cxl_mailbox *mbox;\n+\tstruct device\t*dev;\n+\tint\t\t nid;\n+};\n+\n+/* Bits per page-sized bitmap chunk */\n+#define FLUSH_RECORD_BITS_PER_PAGE\t(PAGE_SIZE * BITS_PER_BYTE)\n+#define FLUSH_RECORD_SHIFT\t\t(PAGE_SHIFT + 3)\n+\n+static unsigned long **flush_record_alloc(unsigned long nr_bits,\n+\t\t\t\t\t  unsigned int *nr_pages_out)\n+{\n+\tunsigned int nr_pages = DIV_ROUND_UP(nr_bits, FLUSH_RECORD_BITS_PER_PAGE);\n+\tunsigned long **pages;\n+\tunsigned int i;\n+\n+\tpages = kcalloc(nr_pages, sizeof(*pages), GFP_KERNEL);\n+\tif (!pages)\n+\t\treturn NULL;\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tpages[i] = (unsigned long *)get_zeroed_page(GFP_KERNEL);\n+\t\tif (!pages[i])\n+\t\t\tgoto err;\n+\t}\n+\n+\t*nr_pages_out = nr_pages;\n+\treturn pages;\n+\n+err:\n+\twhile (i--)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+\treturn NULL;\n+}\n+\n+static void flush_record_free(unsigned long **pages, unsigned int nr_pages)\n+{\n+\tunsigned int i;\n+\n+\tif (!pages)\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr_pages; i++)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+}\n+\n+static inline bool flush_record_test_and_clear(unsigned long **pages,\n+\t\t\t\t\t       unsigned long idx)\n+{\n+\treturn test_and_clear_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\t\t\t  pages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static inline void flush_record_set(unsigned long **pages, unsigned long idx)\n+{\n+\tset_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\tpages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static struct cxl_flush_buf *cxl_flush_buf_alloc(unsigned int max, int nid)\n+{\n+\tstruct cxl_flush_buf *buf;\n+\n+\tbuf = kzalloc_node(sizeof(*buf), GFP_KERNEL, nid);\n+\tif (!buf)\n+\t\treturn NULL;\n+\n+\tbuf->max = max;\n+\tbuf->cmd = kvzalloc_node(struct_size(buf->cmd, ranges, max),\n+\t\t\t\t GFP_KERNEL, nid);\n+\tif (!buf->cmd)\n+\t\tgoto err_cmd;\n+\n+\tbuf->folios = kcalloc_node(max, sizeof(struct folio *),\n+\t\t\t\t   GFP_KERNEL, nid);\n+\tif (!buf->folios)\n+\t\tgoto err_folios;\n+\n+\treturn buf;\n+\n+err_folios:\n+\tkvfree(buf->cmd);\n+err_cmd:\n+\tkfree(buf);\n+\treturn NULL;\n+}\n+\n+static void cxl_flush_buf_free(struct cxl_flush_buf *buf)\n+{\n+\tif (!buf)\n+\t\treturn;\n+\tkvfree(buf->cmd);\n+\tkfree(buf->folios);\n+\tkfree(buf);\n+}\n+\n+static inline void cxl_flush_buf_reset(struct cxl_flush_buf *buf)\n+{\n+\tbuf->count = 0;\n+}\n+\n+static void cxl_flush_buf_send(struct cxl_flush_ctx *ctx,\n+\t\t\t       struct cxl_flush_buf *buf)\n+{\n+\tstruct cxl_mbox_cmd mbox_cmd;\n+\tunsigned int count = buf->count;\n+\tunsigned int i;\n+\tint rc;\n+\n+\tif (count == 0)\n+\t\treturn;\n+\n+\tif (!ctx->media_ops_supported) {\n+\t\t/* No device support, zero all folios inline */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t\tgoto release;\n+\t}\n+\n+\tbuf->cmd->media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE;\n+\tbuf->cmd->media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO;\n+\tbuf->cmd->reserved = 0;\n+\tbuf->cmd->dpa_range_count = cpu_to_le32(count);\n+\n+\tmbox_cmd = (struct cxl_mbox_cmd) {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = buf->cmd,\n+\t\t.size_in = struct_size(buf->cmd, ranges, count),\n+\t\t.poll_interval_ms = 1000,\n+\t\t.poll_count = 30,\n+\t};\n+\n+\trc = cxl_internal_send_cmd(ctx->mbox, &mbox_cmd);\n+\tif (rc) {\n+\t\tdev_warn(ctx->dev,\n+\t\t\t \"media ops zero CCI command failed: %d\\n\", rc);\n+\n+\t\t/* Zero all folios inline on failure */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t}\n+\n+release:\n+\tfor (i = 0; i < count; i++)\n+\t\tfolio_put(buf->folios[i]);\n+\n+\tcxl_flush_buf_reset(buf);\n+}\n+\n+static int cxl_compression_flush_cb(struct folio *folio, void *private)\n+{\n+\tstruct cxl_flush_ctx *ctx = private;\n+\tunsigned long pfn = folio_pfn(folio);\n+\tunsigned long idx = pfn - ctx->base_pfn;\n+\tunsigned long nr = folio_nr_pages(folio);\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tunsigned long flags;\n+\tunsigned int pos;\n+\n+\t/* Case (a): flush record bit set, resolution from our media op */\n+\tif (flush_record_test_and_clear(ctx->flush_record, idx))\n+\t\treturn 0;\n+\n+\tdev_dbg_ratelimited(ctx->dev,\n+\t\t\t     \"flush_cb: folio pfn=%lx order=%u idx=%lu cpu=%d\\n\",\n+\t\t\t     pfn, folio_order(folio), idx,\n+\t\t\t     raw_smp_processor_id());\n+\n+\tlocal_irq_save(flags);\n+\trcu_read_lock();\n+\n+\tpcpu = this_cpu_ptr(ctx->pcpu);\n+\tbuf = rcu_dereference(pcpu->active);\n+\n+\tif (unlikely(!buf || buf->count >= buf->max)) {\n+\t\trcu_read_unlock();\n+\t\tlocal_irq_restore(flags);\n+\t\tif (buf)\n+\t\t\tschedule_work_on(raw_smp_processor_id(),\n+\t\t\t\t\t &pcpu->overflow_work);\n+\t\treturn 2;\n+\t}\n+\n+\t/* Case (b): write DPA range directly into pre-formatted CCI buffer */\n+\tfolio_get(folio);\n+\tflush_record_set(ctx->flush_record, idx);\n+\n+\tpos = buf->count;\n+\tbuf->folios[pos] = folio;\n+\tbuf->cmd->ranges[pos].starting_dpa = cpu_to_le64((u64)idx * PAGE_SIZE);\n+\tbuf->cmd->ranges[pos].length = cpu_to_le64((u64)nr * PAGE_SIZE);\n+\tbuf->count = pos + 1;\n+\n+\trcu_read_unlock();\n+\tlocal_irq_restore(flags);\n+\n+\treturn 1;\n+}\n+\n+static int cxl_flush_kthread_fn(void *data)\n+{\n+\tstruct cxl_flush_ctx *ctx = data;\n+\tstruct cxl_flush_buf *dirty;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tint cpu;\n+\tbool any_dirty;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tany_dirty = false;\n+\n+\t\t/* Phase 1: Swap all per-CPU buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tstruct cxl_flush_buf *spare = ctx->kthread_spares[cpu];\n+\n+\t\t\tif (!spare)\n+\t\t\t\tcontinue;\n+\n+\t\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\t\tcxl_flush_buf_reset(spare);\n+\t\t\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\t\t\tctx->kthread_spares[cpu] = dirty;\n+\n+\t\t\tif (dirty && dirty->count > 0) {\n+\t\t\t\tdev_dbg(ctx->dev,\n+\t\t\t\t\t \"flush_kthread: cpu=%d has %u dirty ranges\\n\",\n+\t\t\t\t\t cpu, dirty->count);\n+\t\t\t\tany_dirty = true;\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (!any_dirty)\n+\t\t\tgoto sleep;\n+\n+\t\t/* Phase 2: Single synchronize_rcu for all swaps */\n+\t\tsynchronize_rcu();\n+\n+\t\t/* Phase 3: Send CCI commands for dirty buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tdirty = ctx->kthread_spares[cpu];\n+\t\t\tif (dirty && dirty->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, dirty);\n+\t\t\t/* dirty is now clean, stays as kthread_spares[cpu] */\n+\t\t}\n+\n+sleep:\n+\t\tschedule_timeout_interruptible(\n+\t\t\tmsecs_to_jiffies(flush_interval_ms));\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static void cxl_flush_overflow_work(struct work_struct *work)\n+{\n+\tstruct cxl_pcpu_flush *pcpu =\n+\t\tcontainer_of(work, struct cxl_pcpu_flush, overflow_work);\n+\tstruct cxl_flush_ctx *ctx = pcpu->ctx;\n+\tstruct cxl_flush_buf *dirty, *spare;\n+\tunsigned long flags;\n+\n+\tdev_dbg(ctx->dev, \"flush_overflow: cpu=%d buffer full, flushing\\n\",\n+\t\t raw_smp_processor_id());\n+\n+\tspare = pcpu->overflow_spare;\n+\tif (!spare)\n+\t\treturn;\n+\n+\tcxl_flush_buf_reset(spare);\n+\n+\tlocal_irq_save(flags);\n+\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\tlocal_irq_restore(flags);\n+\n+\tpcpu->overflow_spare = dirty;\n+\n+\tsynchronize_rcu();\n+\tcxl_flush_buf_send(ctx, dirty);\n+}\n+\n+struct cxl_teardown_ctx {\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+};\n+\n+static void cxl_compression_pre_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\n+\tif (!tctx->flush_ctx)\n+\t\treturn;\n+\n+\t/*\n+\t * Unregister the CRAM node before memory goes offline.\n+\t * node_private_clear_ops requires the node_private to still\n+\t * exist, which is destroyed during memory removal.\n+\t */\n+\tcram_unregister_private_node(tctx->nid);\n+\n+\t/*\n+\t * Offline and remove CXL memory with retry.  CXL compressed\n+\t * memory may have pages pinned by in-flight flush operations;\n+\t * keep retrying until they complete.  Once done, sysram->res\n+\t * is NULL so the devm sysram_unregister action that follows\n+\t * will skip the hotplug removal.\n+\t */\n+\tif (tctx->sysram) {\n+\t\tint rc, retries = 0;\n+\n+\t\twhile (true) {\n+\t\t\trc = cxl_sysram_offline_and_remove(tctx->sysram);\n+\t\t\tif (!rc)\n+\t\t\t\tbreak;\n+\t\t\tif (++retries > 60) {\n+\t\t\t\tpr_err(\"cxl_compression: memory offline failed after %d retries, giving up\\n\",\n+\t\t\t\t       retries);\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tpr_info(\"cxl_compression: memory offline failed (%d), retrying...\\n\",\n+\t\t\t\trc);\n+\t\t\tmsleep(1000);\n+\t\t}\n+\t}\n+}\n+\n+static void cxl_compression_post_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\tstruct cxl_flush_ctx *ctx = tctx->flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tint cpu;\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\t/* cram_unregister_private_node already called in pre_teardown */\n+\n+\tif (ctx->flush_thread) {\n+\t\tkthread_stop(ctx->flush_thread);\n+\t\tctx->flush_thread = NULL;\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\tcancel_work_sync(&pcpu->overflow_work);\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tif (buf && buf->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, buf);\n+\n+\t\tif (pcpu->overflow_spare && pcpu->overflow_spare->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares && ctx->kthread_spares[cpu]) {\n+\t\t\tbuf = ctx->kthread_spares[cpu];\n+\t\t\tif (buf->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, buf);\n+\t\t}\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(ctx->kthread_spares[cpu]);\n+\t}\n+\n+\tkfree(ctx->kthread_spares);\n+\tfree_percpu(ctx->pcpu);\n+\tflush_record_free(ctx->flush_record, ctx->flush_record_pages);\n+}\n+\n+/**\n+ * struct cxl_compression_ctx - Per-device context for compression driver\n+ * @mbox: CXL mailbox for issuing CCI commands\n+ * @pdev: PCI device\n+ * @flush_ctx: Flush context for deferred page reclamation\n+ * @tctx: Teardown context for devm actions\n+ * @sysram: Sysram device for offline+remove in remove path\n+ * @nid: NUMA node ID, NUMA_NO_NODE if unset\n+ * @cxlmd: The memdev associated with this context\n+ * @cxlr: Region created by this driver (NULL if pre-existing)\n+ * @cxled: Endpoint decoder with DPA allocated by this driver\n+ * @regions_converted: Number of regions successfully converted\n+ * @media_ops_supported: Device supports media operations zero (0x4402)\n+ */\n+struct cxl_compression_ctx {\n+\tstruct cxl_mailbox *mbox;\n+\tstruct pci_dev *pdev;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+\tstruct cxl_memdev *cxlmd;\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint regions_converted;\n+\tbool media_ops_supported;\n+};\n+\n+/*\n+ * Probe whether the device supports Media Operations Zero (0x4402).\n+ * Send a zero-count command, a conforming device returns SUCCESS,\n+ * a device that doesn't support it returns UNSUPPORTED (-ENXIO).\n+ */\n+static bool cxl_probe_media_ops_zero(struct cxl_mailbox *mbox,\n+\t\t\t\t     struct device *dev)\n+{\n+\tstruct cxl_media_op_input probe = {\n+\t\t.media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE,\n+\t\t.media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO,\n+\t\t.dpa_range_count = 0,\n+\t};\n+\tstruct cxl_mbox_cmd cmd = {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = &probe,\n+\t\t.size_in = sizeof(probe),\n+\t};\n+\tint rc;\n+\n+\trc = cxl_internal_send_cmd(mbox, &cmd);\n+\tif (rc) {\n+\t\tdev_info(dev,\n+\t\t\t \"media operations zero not supported (rc=%d), using inline zeroing\\n\",\n+\t\t\t rc);\n+\t\treturn false;\n+\t}\n+\n+\tdev_info(dev, \"media operations zero (0x4402) supported\\n\");\n+\treturn true;\n+}\n+\n+struct cxl_compression_wm_ctx {\n+\tstruct device *dev;\n+\tint nid;\n+};\n+\n+static irqreturn_t cxl_compression_lthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"lthresh watermark: pressuring node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, CRAM_PRESSURE_MAX);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static irqreturn_t cxl_compression_hthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"hthresh watermark: resuming node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, 0);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static int convert_region_to_sysram(struct cxl_region *cxlr,\n+\t\t\t\t    struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct device *dev = cxl_region_dev(cxlr);\n+\tstruct cxl_compression_wm_ctx *wm_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tresource_size_t region_start, region_size;\n+\tstruct range hpa_range;\n+\tint nid;\n+\tint irq;\n+\tint cpu;\n+\tint rc;\n+\n+\tif (cxl_region_mode(cxlr) != CXL_PARTMODE_RAM) {\n+\t\tdev_dbg(dev, \"skipping non-RAM region (mode=%d)\\n\",\n+\t\t\tcxl_region_mode(cxlr));\n+\t\treturn 0;\n+\t}\n+\n+\tdev_info(dev, \"converting region to sysram\\n\");\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to add sysram region: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\ttctx = devm_kzalloc(dev, sizeof(*tctx), GFP_KERNEL);\n+\tif (!tctx)\n+\t\treturn -ENOMEM;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_post_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Find the sysram child device for pre_teardown */\n+\tcomp_ctx->sysram = cxl_region_find_sysram(cxlr);\n+\tif (comp_ctx->sysram)\n+\t\ttctx->sysram = comp_ctx->sysram;\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tnid = phys_to_target_node(hpa_range.start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\tregion_start = hpa_range.start;\n+\tregion_size = range_len(&hpa_range);\n+\n+\tflush_ctx = devm_kzalloc(dev, sizeof(*flush_ctx), GFP_KERNEL);\n+\tif (!flush_ctx)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->base_pfn = PHYS_PFN(region_start);\n+\tflush_ctx->nr_pages = region_size >> PAGE_SHIFT;\n+\tflush_ctx->flush_record = flush_record_alloc(flush_ctx->nr_pages,\n+\t\t\t\t\t\t     &flush_ctx->flush_record_pages);\n+\tif (!flush_ctx->flush_record)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->mbox = comp_ctx->mbox;\n+\tflush_ctx->dev = dev;\n+\tflush_ctx->nid = nid;\n+\tflush_ctx->media_ops_supported = comp_ctx->media_ops_supported;\n+\n+\t/*\n+\t * Cap buffer at max DPA ranges that fit in one CCI payload.\n+\t * Header is 8 bytes (struct cxl_media_op_input), each range\n+\t * is 16 bytes (struct cxl_dpa_range).  The module parameter\n+\t * flush_buf_size can further limit this (0 = use hw max).\n+\t */\n+\tflush_ctx->buf_max = (flush_ctx->mbox->payload_size -\n+\t\t\t      sizeof(struct cxl_media_op_input)) /\n+\t\t\t     sizeof(struct cxl_dpa_range);\n+\tif (flush_buf_size && flush_buf_size < flush_ctx->buf_max)\n+\t\tflush_ctx->buf_max = flush_buf_size;\n+\tif (flush_ctx->buf_max == 0)\n+\t\tflush_ctx->buf_max = 1;\n+\n+\tdev_info(dev,\n+\t\t \"flush buffer: %u DPA ranges per command (payload %zu bytes, media_ops %s)\\n\",\n+\t\t flush_ctx->buf_max, flush_ctx->mbox->payload_size,\n+\t\t flush_ctx->media_ops_supported ? \"yes\" : \"no\");\n+\n+\tflush_ctx->pcpu = alloc_percpu(struct cxl_pcpu_flush);\n+\tif (!flush_ctx->pcpu)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->kthread_spares = kcalloc(nr_cpu_ids,\n+\t\t\t\t\t    sizeof(struct cxl_flush_buf *),\n+\t\t\t\t\t    GFP_KERNEL);\n+\tif (!flush_ctx->kthread_spares)\n+\t\tgoto err_pcpu_init;\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *active_buf, *overflow_buf, *spare_buf;\n+\n+\t\tactive_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!active_buf)\n+\t\t\tgoto err_pcpu_init;\n+\n+\t\toverflow_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!overflow_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tspare_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!spare_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tcxl_flush_buf_free(overflow_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\t\tpcpu->ctx = flush_ctx;\n+\t\trcu_assign_pointer(pcpu->active, active_buf);\n+\t\tpcpu->overflow_spare = overflow_buf;\n+\t\tINIT_WORK(&pcpu->overflow_work, cxl_flush_overflow_work);\n+\n+\t\tflush_ctx->kthread_spares[cpu] = spare_buf;\n+\t}\n+\n+\tflush_ctx->flush_thread = kthread_create_on_node(\n+\t\tcxl_flush_kthread_fn, flush_ctx, nid, \"cxl-flush/%d\", nid);\n+\tif (IS_ERR(flush_ctx->flush_thread)) {\n+\t\trc = PTR_ERR(flush_ctx->flush_thread);\n+\t\tflush_ctx->flush_thread = NULL;\n+\t\tgoto err_pcpu_init;\n+\t}\n+\twake_up_process(flush_ctx->flush_thread);\n+\n+\trc = cram_register_private_node(nid, cxlr,\n+\t\t\t\t\tcxl_compression_flush_cb, flush_ctx);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to register cram node %d: %d\\n\", nid, rc);\n+\t\tgoto err_pcpu_init;\n+\t}\n+\n+\ttctx->flush_ctx = flush_ctx;\n+\ttctx->nid = nid;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_pre_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcomp_ctx->flush_ctx = flush_ctx;\n+\tcomp_ctx->tctx = tctx;\n+\tcomp_ctx->nid = nid;\n+\n+\t/*\n+\t * Register watermark IRQ handlers on &pdev->dev for\n+\t * MSI-X vector 12 (lthresh) and vector 13 (hthresh).\n+\t */\n+\twm_ctx = devm_kzalloc(&pdev->dev, sizeof(*wm_ctx), GFP_KERNEL);\n+\tif (!wm_ctx)\n+\t\treturn -ENOMEM;\n+\n+\twm_ctx->dev = &pdev->dev;\n+\twm_ctx->nid = nid;\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_LTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_lthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-lthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register lthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_HTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_hthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-hthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register hthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\treturn 0;\n+\n+err_pcpu_init:\n+\tif (flush_ctx->flush_thread)\n+\t\tkthread_stop(flush_ctx->flush_thread);\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *buf;\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (flush_ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(flush_ctx->kthread_spares[cpu]);\n+\t}\n+\tkfree(flush_ctx->kthread_spares);\n+\tfree_percpu(flush_ctx->pcpu);\n+\tflush_record_free(flush_ctx->flush_record, flush_ctx->flush_record_pages);\n+\treturn rc ? rc : -ENOMEM;\n+}\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\tpdev_to_comp_ctx(to_pci_dev(cxlmd->dev.parent))->cxled = cxled;\n+\treturn cxlr;\n+}\n+\n+static int cxl_compression_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i, converted = 0, errors = 0;\n+\tint rc;\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\t/* Probe device for media operations zero support */\n+\tcomp_ctx->media_ops_supported =\n+\t\tcxl_probe_media_ops_zero(comp_ctx->mbox,\n+\t\t\t\t\t &cxlmd->dev);\n+\n+\tdev_info(&cxlmd->dev, \"compression attach: looking for regions\\n\");\n+\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) == CXL_PARTMODE_RAM) {\n+\t\t\trc = convert_region_to_sysram(regions[i], pdev);\n+\t\t\tif (rc)\n+\t\t\t\terrors++;\n+\t\t\telse\n+\t\t\t\tconverted++;\n+\t\t}\n+\t\tput_device(cxl_region_dev(regions[i]));\n+\t}\n+\n+\tif (converted > 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"converted %d regions to sysram (%d errors)\\n\",\n+\t\t\t converted, errors);\n+\t\treturn errors ? -EIO : 0;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"could not create RAM region: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = convert_region_to_sysram(cxlr, pdev);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to convert region to sysram: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tcomp_ctx->cxlr = cxlr;\n+\n+\tdev_info(&cxlmd->dev, \"created and converted region %s to sysram\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_compression_attach = {\n+\t.probe = cxl_compression_attach_probe,\n+};\n+\n+static int cxl_compression_probe(struct pci_dev *pdev,\n+\t\t\t\t const struct pci_device_id *id)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probing device\\n\");\n+\n+\tcomp_ctx = devm_kzalloc(&pdev->dev, sizeof(*comp_ctx), GFP_KERNEL);\n+\tif (!comp_ctx)\n+\t\treturn -ENOMEM;\n+\tcomp_ctx->nid = NUMA_NO_NODE;\n+\tcomp_ctx->pdev = pdev;\n+\n+\trc = xa_insert(&comp_ctx_xa, (unsigned long)pdev, comp_ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_compression_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&comp_ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_compression_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = xa_erase(&comp_ctx_xa,\n+\t\t\t\t\t\t\t(unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: removing device\\n\");\n+\n+\tif (!comp_ctx || comp_ctx->nid == NUMA_NO_NODE)\n+\t\treturn;\n+\n+\t/*\n+\t * Destroy the region, devm actions on the region device handle teardown\n+\t * in registration-reverse order:\n+\t *   1. pre_teardown:  cram_unregister + retry-forever memory offline\n+\t *   2. sysram_unregister: device_unregister (sysram->res is NULL\n+\t *      after pre_teardown, so cxl_sysram_release skips hotplug)\n+\t *   3. post_teardown: kthread stop, flush cleanup\n+\t *\n+\t * PCI MMIO is still live so CCI commands in post_teardown work.\n+\t */\n+\tif (comp_ctx->cxlr) {\n+\t\tcxl_destroy_region(comp_ctx->cxlr);\n+\t\tcomp_ctx->cxlr = NULL;\n+\t}\n+\n+\tif (comp_ctx->cxled) {\n+\t\tcxl_dpa_free(comp_ctx->cxled);\n+\t\tcomp_ctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_compression_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ /* terminate list */ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_compression_pci_tbl);\n+\n+static struct pci_driver cxl_compression_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_compression_pci_tbl,\n+\t.probe\t\t= cxl_compression_probe,\n+\t.remove\t\t= cxl_compression_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_compression_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Compression Memory Driver with SysRAM regions\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David expressed concern about adding special-casing for private memory nodes, similar to ZONE_DEVICE, and suggested discussing the topic further.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concern",
                "special-casing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm concerned about adding more special-casing (similar to what we \nalready added for ZONE_DEVICE) all over the place.\n\nLike the whole folio_managed_() stuff in mprotect.c\n\nHaving that said, sounds like a reasonable topic to discuss.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged a concern about the semantics of zone_device hooks and proposed two alternative solutions: reusing vma_wants_writenotify() or adding a new vma flag to track protected/device pages.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "proposed alternative solutions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It's a valid concern - and is why I tried to re-use as many of the\nzone_device hooks as possible.  It does not seem zone_device has quite\nthe same semantics for a case like this, so I had to make something new.\n\nDEVICE_COHERENT injects a temporary swap entry to allow the device to do\na large atomic operation - then the page table is restored and the CPU\nis free to change entries as it pleases.\n\nAnother option would be to add the hook to vma_wants_writenotify()\ninstead of the page table code - and mask MM_CP_TRY_CHANGE_WRITABLE.\n\nThis would require adding a vma flag - or maybe a count of protected /\ndevice pages.\n\nint mprotect_fixup() {\n    ...\n    if (vma_wants_manual_pte_write_upgrade(vma))\n        mm_cp_flags |= MM_CP_TRY_CHANGE_WRITABLE;\n}\n\nbool vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)\n{\n    if (vma->managed_wrprotect)\n        return true;\n}\n\nThat would localize the change in folio_managed_fixup_migration_pte() :\n\nstatic inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n                                                      pte_t pte,\n                                                      pte_t old_pte,\n                                                      struct vm_area_struct *vma)\n{\n    ...\n    } else if (folio_managed_wrprotect(page_folio(new))) {\n        pte = pte_wrprotect(pte);\n+       atomic_inc(&vma->managed_wrprotect);\n    }\n    return pte;\n}\n\nThis would cover both the huge_memory.c and mprotect, and maybe that's\njust generally cleaner? I can try that to see if it actually works.\n\n~Gregory",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that existing hooks can be used for write protection and agreed to remove redundant code from page table walks.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged existing solution",
                "agreed to simplify"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "scratch all this - existing hooks exist for exactly this purpose:\n\n\tcan_change_[pte|pmd]_writable()\n\nSurprised I missed this.\n\nI can clean this up to remove it from the page table walks.\n\nStill valid to question whether we want this, but at least the hook\nlives with other write-protect hooks now.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alistair Popple",
              "summary": "Reviewer Alistair Popple expressed concerns that N_MEMORY_PRIVATE may not be the best solution for reusing the existing mm buddy allocator and suggested considering alternative approaches such as adapting DRM's standalone buddy allocator, while also noting that device memory exposure to userspace is an interesting aspect of the series.\n\nThe reviewer agrees that the patch provides a standard interface to userspace for managing device memory and suggests using the existing NUMA APIs as a reasonable approach.\n\nReviewer Alistair Popple noted that the proposed cxl_compression driver is similar to ZONE_DEVICE and questioned why it cannot be extended instead of duplicating code, pointing out a potential lock ordering issue with reclaim paths when using pgmap for ZONE_DEVICE pages. He suggested exploring alternative storage options such as page_ext or considering the future replacement of struct page with folios.\n\nReviewer suggested that the cxl_compression PCI driver is similar to existing ZONE_DEVICE methods and proposed building on those instead of introducing a new feature set.\n\nReviewer Alistair Popple noted that the implementation duplicates a lot of hooks, similar to those provided by ZONE_DEVICE, and requested further discussion on this aspect.\n\nReviewer questioned whether allocation must be handled by the mm allocator, suggesting that a device allocator library could be written or reused from drm_buddy.c\n\nThe reviewer questioned the characterization of ZONE_DEVICE pages as not being real struct pages, suggesting that perspective on this may vary depending on one's role in the mm subsystem, and asked for clarification on what limitations are actually being addressed.\n\nReviewer suggested that ZONE_DEVICE_COHERENT could be extended to support the use case, proposing a couple of extra dev_pagemap_ops and LRU access as a potential solution.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "alternative solutions",
                "agreement",
                "endorsement",
                "suggested alternatives",
                "duplicates hooks",
                "similar to ZONE_DEVICE",
                "clarification requested",
                "questioning characterization"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Having had to re-implement entire portions of mm/ in a driver I agree this isn't\nsomething anyone sane should do :-) However aspects of ZONE_DEVICE were added\nprecisely to help with that so I'm not sure N_MEMORY_PRIVATE is the only or best\nway to do that.\n\nBased on our discussion at LPC I believe one of the primary motivators here was\nto re-use the existing mm buddy allocator rather than writing your own. I remain\nto be convinced that alone is justification enough for doing all this - DRM for\nexample already has quite a nice standalone buddy allocator (drm_buddy.c) that\ncould presumably be used, or adapted for use, by any device driver.\n\nThe interesting part of this series (which I have skimmed but not read in\ndetail) is how device memory gets exposed to userspace - this is something that\nexisting ZONE_DEVICE implementations don't address, instead leaving it up to\ndrivers and associated userspace stacks to deal with allocation, migration, etc.\n\n---\n\nThis is I think is one of the key things that should be enabled - providing a\nstandard interface to userspace for managing device memory. The existing NUMA\nAPIs do seem like a reasonable way to do this.\n\n---\n\nOne does not have to squint too hard to see that the above is not so different\nfrom what ZONE_DEVICE provides today via dev_pagemap_ops(). So I think I think\nit would be worth outlining why the existing ZONE_DEVICE mechanism can't be\nextended to provide these kind of services.\n\nThis seems to add a bunch of code just to use NODE_DATA instead of page->pgmap,\nwithout really explaining why just extending dev_pagemap_ops wouldn't work. The\nobvious reason is that if you want to support things like reclaim, compaction,\netc. these pages need to be on the LRU, which is a little bit hard when that\nfield is also used by the pgmap pointer for ZONE_DEVICE pages.\n\nBut it might be good to explore other options for storing the pgmap - for\nexample page_ext could be used.  Or I hear struct page may go away in place of\nfolios any day now, so maybe that gives us space for both :-)\n\n---\n\nThe above also looks pretty similar to the existing ZONE_DEVICE methods for\ndoing this which is another reason to argue for just building up the feature set\nof the existing boondoggle rather than adding another thingymebob.\n\nIt seems the key thing we are looking for is:\n\n1) A userspace API to allocate/manage device memory (ie. move_pages(), mbind(),\netc.)\n\n2) Allowing reclaim/LRU list processing of device memory.\n\n---\n\ndiscussion (hopefully I can make it to LSFMM). Mostly I'm interested in the\nimplementation as this does on the surface seem to sprinkle around and duplicate\na lot of hooks similar to what ZONE_DEVICE already provides.\n\n---\n\nFor basic allocation I agree this is the case. But there's no reason some device\nallocator library couldn't be written. Or in fact as pointed out above reuse the\nalready existing one in drm_buddy.c.  So would be interested to hear arguments\nfor why allocation has to be done by the mm allocator and/or why an allocation\nlibrary wouldn't work here given DRM already has them.\n\n---\n\nZONE_DEVICE pages are in fact real struct pages, but I will concede that\nperspective probably depends on which bits of the mm you play in. The real\nlimitations you seem to be addressing is more around how we get these pages in\nan LRU, or are there other limitations?\n\n---\n\nWhat I'd like to explore is why ZONE_DEVICE_COHERENT couldn't just be extended\nto support your usecase? It seems a couple of extra dev_pagemap_ops and being\nable to go on the LRU would get you there.\n\n - Alistair",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author acknowledges that using ZONE_DEVICE is insufficient for N_MEMORY_PRIVATE, as it introduces unnecessary complexity. They propose reusing the buddy allocator instead, which would simplify the implementation and eliminate issues related to zones.\n\nThe author explains that the callback similarity between ZONE_DEVICE and private nodes is intentional, as they require the same set of hooks but with different defaults. They argue that extending ZONE_DEVICE into these areas would be cumbersome and inefficient, and that the current implementation is a more straightforward solution.\n\nAuthor addressed a concern about the per-page pgmap and device-to-node mappings, agreeing that NODE_DATA is the right direction regardless of struct page's future or zone it lives in.\n\nThe author acknowledges that implementing mempolicy support for N_MEMORY_PRIVATE is more complex than initially thought, explaining that it requires adding code to vma_alloc_folio_noprof and dealing with ZONE_DEVICE's overloaded nature. They suggest two options: putting pages in the buddy or adding pgmap->device_alloc() callbacks at every allocation site.\n\nAuthor acknowledged reviewer's concern about reusing mm/ services and explained that using the buddy underpins the rest of these services, making it a more efficient approach.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a fix is needed",
                "proposes an alternative solution",
                "acknowledges feedback",
                "provides explanation",
                "agreed with reviewer's suggestion",
                "provided explanation",
                "acknowledges complexity",
                "suggests two options",
                "acknowledged",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree that buddy-access alone is insufficient justification, it\nstarted off that way - but if you want mempolicy/NUMA UAPI access,\nit turns into \"Re-use all of MM\" - and that means using the buddy.\n\nI also expected ZONE_DEVICE vs NODE_DATA to be the primary discussion,\n\nI raise replacing it as a thought experiment, but not the proposal.\n\nThe idea that drm/ is going to switch to private nodes is outside the\nrealm of reality, but part of that is because of years of infrastructure\nbuilt on the assumption that re-using mm/ is infeasible.\n\nBut, lets talk about DEVICE_COHERENT\n\n---\n\nDEVICE_COHERENT is the odd-man out among ZONE_DEVICE modes. The others\nuse softleaf entries and don't allow direct mappings.\n\n(DEVICE_PRIVATE sort of does if you squint, but you can also view that\n a bit like PROT_NONE or read-only controls to force migrations).\n\nIf you take DEVICE_COHERENT and:\n\n- Move pgmap out of the struct page (page_ext, NODE_DATA, etc) to free\n  the LRU list_head\n- Put pages in the buddy (free lists, watermarks, managed_pages) or add\n  pgmap->device_alloc() at every allocation callsite / buddy hook\n- Add LRU support (aging, reclaim, compaction)\n- Add isolated gating (new GFP flag and adjusted zonelist filtering)\n- Add new dev_pagemap_ops callbacks for the various mm/ features\n- Audit evey folio_is_zone_device() to distinguish zone device modes\n\n... you've built N_MEMORY_PRIVATE inside ZONE_DEVICE. Except now\npage_zone(page) returns ZONE_DEVICE - so you inherit the wrong\ndefaults at every existing ZONE_DEVICE check. \n\nSkip-sites become things to opt-out of instead of opting into.\n\nYou just end up with\n\nif (folio_is_zone_device(folio))\n    if (folio_is_my_special_zone_device())\n    else ....\n\nand this just generalizes to\n\nif (folio_is_private_managed(folio))\n    folio_managed_my_hooked_operation()\n\nSo you get the same code, but have added more complexity to ZONE_DEVICE.\n\nI don't think that's needed if we just recognize ZONE is the wrong\nabstraction to be operating on.\n\nHonestly, even ZONE_MOVABLE becomes pointless with N_MEMORY_PRIVATE\nif you disallow longterm pinning - because the managing service handles\nallocations (it has to inject GFP_PRIVATE to get access) or selectively\nenables the mm/ services it knows are safe (mempolicy).\n\nEven if you allow longterm pinning, if your service controls what does\nthe pinning it can still be reclaimable - just manually (killing\nprocesses) instead of letting hotplug do it via migration.\n\nIf your service only allocates movable pages - your ZONE_NORMAL is\neffectively ZONE_MOVABLE.  \n\nIn some cases we use ZONE_MOVABLE to prevent the kernel from allocating\nmemory onto devices (like CXL).  This means struct page is forced to\ntake up DRAM or use memmap_on_memory - meaning you lose high-value\ncapacity or sacrifice contiguity (less huge page support).\n\nThis entire problem can evaporate if you can just use ZONE_NORMAL.\n\nThere are a lot of benefits to just re-using the buddy like this.\n\nZones are the wrong abstraction and cause more problems.\n\n---\n\nYou don't have to squint because it was deliberate :]\n\nThe callback similarity is the feature - they're the same logical\noperations.  The difference is the direction of the defaults.\n\nExtending ZONE_DEVICE into these areas requires the same set of hooks,\nplus distinguishing \"old ZONE_DEVICE\" from \"new ZONE_DEVICE\".\n\nWhere there are new injection sites, it's because ZONE_DEVICE opts\nout of ever touching that code in some other silently implied way.\n\nFor example, reclaim/compaction doesn't run because ZONE_DEVICE doesn't\nadd to managed_pages (among other reasons).\n\nYou'd have to go figure out how to hack those things into ZONE_DEVICE \n*and then* opt every *other* ZONE_DEVICE mode *back out*.\n\nSo you still end up with something like this anyway:\n\nstatic inline bool folio_managed_handle_fault(struct folio *folio,\n                                              struct vm_fault *vmf,\n                                              enum pgtable_level level,\n                                              vm_fault_t *ret)\n{\n        /* Zone device pages use swap entries; handled in do_swap_page */\n        if (folio_is_zone_device(folio))\n                return false;\n\n        if (folio_is_private_node(folio))\n\t\t...\n        return false;\n}\n\n---\n\nIf NUMA is the interface we want, then NODE_DATA is the right direction\nregardless of struct page's future or what zone it lives in.\n\nThere's no reason to keep per-page pgmap w/ device-to-node mappings.\n\nYou can have one driver manage multiple devices with the same numa node\nif it uses the same owner context (PFN already differentiates devices).\n\nThe existing code allows for this.\n\n---\n\nOn (1): ZONE_DEVICE NUMA UAPI is harder than it looks from the surface\n\nMuch of the kernel mm/ infrastructure is written on top of the buddy and\nexpects N_MEMORY to be the sole arbiter of \"Where to Acquire Pages\".\n\nMempolicy depends on:\n   - Buddy support or a new alloc hook around the buddy\n\n   - Migration support (mbind() after allocation migrates)\n     - Migration also deeply assumes buddy and LRU support\n\n   - Changing validations on node states\n     - mempolicy checks N_MEMORY membership, so you have to hack\n       N_MEMORY onto ZONE_DEVICE\n       (or teach it about a new node state... N_MEMORY_PRIVATE)\n\n\nGetting mempolicy to work with N_MEMORY_PRIVATE amounts to adding 2\nlines of code in vma_alloc_folio_noprof:\n\nstruct folio *vma_alloc_folio_noprof(gfp_t gfp, int order,\n                                     struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr)\n{\n        if (pol->flags & MPOL_F_PRIVATE)\n                gfp |= __GFP_PRIVATE;\n\n        folio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n\t/* Woo! I faulted a DEVICE PAGE! */\n}\n\nBut this requires the pages to be managed by the buddy.\n\nThe rest of the mempolicy support is around keeping sane nodemasks when\nthings like cpuset.mems rebinds occur and validating you don't end up\nwith private nodes that don't support mempolicy in your nodemask.\n\nYou have to do all of this anyway, but with the added bonus of fighting\nwith the overloaded nature of ZONE_DEVICE at every step.\n\n==========\n\nOn (2): Assume you solve LRU. \n\nZone Device has no free lists, managed_pages, or watermarks.\n\nkswapd can't run, compaction has no targets, vmscan's pressure model\ndoesn't function.  These all come for free when the pages are\nbuddy-managed on a real zone.  Why re-invent the wheel?\n\n==========\n\nSo you really have two options here:\n\na) Put pages in the buddy, or\n\nb) Add pgmap->device_alloc() callbacks at every allocation site that\n   could target a node:\n     - vma_alloc_folio\n     - alloc_migration_target\n     - alloc_demote_folio\n     - alloc_pages_node\n     - alloc_contig_pages\n     - list goes on\n\nOr more likely - hooking get_page_from_freelist.  Which at that\npoint... just use the buddy?  You're already deep in the hot path.\n\n---\n\nUsing the buddy underpins the rest of mm/ services we want to re-use.\n\nThat's basically it.  Otherwise you have to inject hooks into every\nsurface that touches the buddy...\n\n... or in the buddy (get_page_from_freelist), at which point why not\njust use the buddy?\n\n~Gregory",
              "reply_to": "Alistair Popple",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author considered reviewer's suggestion to simplify patch by removing N_MEMORY_PRIVATE and instead checking NODE_DATA(target_nid)->private, agreeing to explore this alternative.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "considering alternative approach",
                "agrees to look at it more"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This gave me something to chew on\n\nI think this can be done without introducing N_MEMORY_PRIVATE and just\nchecking:   NODE_DATA(target_nid)->private\n\nmeaning these nodes can just be N_MEMORY with the same isolations.\n\nI'll look at this a bit more.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/2] cxl/region: fix region leak when attach_target fails in cxl_add_to_region",
          "message_id": "20260221043013.1420169-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260221043013.1420169-1-gourry@gourry.net/",
          "date": "2026-02-21T04:30:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-21",
          "patch_summary": "This patch fixes a region leak in the CXL driver when attaching a target fails. When `attach_target` returns an error, the auto-discovered region remains registered and consumes HPA resources without ever reaching a committed state. The patch tracks whether the region was created by checking the return value of `cxl_add_to_region`, and if it was not created successfully, it calls `drop_region` to unregister the region and release the HPA resource. This prevents subsequent region creation attempts from failing due to reserved HPA ranges.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "Author addressed a concern about device_attach() being called on auto-discovered regions when a custom attach callback is present. They explained that this can lead to dax memory being left online due to dax_kmem refusing to offline during its remove path. The author agreed to skip device_attach() in such cases, with the custom attach callback responsible for setting up the region afterwards.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a CXL memdev has a custom attach callback, cxl_add_to_region()\nshould not call device_attach() on the auto-discovered region.\n\nThe default device_attach() binds the dax driver, which may online\nmemory via dax_kmem.  The custom attach callback then has to tear down\nthe dax stack to convert the region to sysram, but dax_kmem refuses to\noffline memory during its remove path, leaving regions stuck online.\n\nSkip device_attach() when cxlmd->attach is set.  The attach callback\nis responsible for setting up the region after auto-discovery completes\n(e.g. adding it as sysram directly).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/region.c | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 276046d49f88..e5edeabd9262 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -3971,6 +3971,12 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n \t}\n \n \tif (attach) {\n+\t\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n+\n+\t\t/* Skip device_attach if memdev has is own attach callback */\n+\t\tif (cxlmd->attach)\n+\t\t\treturn 0;\n+\n \t\t/*\n \t\t * If device_attach() fails the range may still be active via\n \t\t * the platform-firmware memory map, otherwise the driver for\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author is pushing back against the review, indicating that the patch should be disregarded because it uses a function introduced by another developer, and instead pointing to an alternative solution.",
              "sentiment": "CONTENTIOUS",
              "sentiment_signals": [
                "pushing back",
                "disregard this patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "BAH - disregard this patch, it uses drop_region which is introduced by\nAlejandro here:\n\nhttps://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/",
              "reply_to": "",
              "message_date": "2026-02-21",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alison Schofield",
              "summary": "Reviewer noted that the patch's current implementation drops the region only when attach_target() fails immediately after creation, but not if it fails later in the process, and suggested revisiting a previous patch that unregisters auto-created regions on assembly failure.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I see you dropping this, perhaps just for the moment, because\nthe drop_region() you wanted to use is not available yet.\n\nThis looks a lot like \n\thttps://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/\n\tcxl/region: Unregister auto-created region when assembly fails\n\tWhen auto-created region assembly fails the region remains registered\n\tbut disabled. The region continues to reserve its memory resource,\n\tpreventing DAX from registering the memory.\n\tUnregister the region on assembly failure to release the resource.\n\nAnd the review comments on that one, or at least on that thread in\ngeneral, was to leave all the broken things in place.\nI didn't agree with that, and hope to see this version move ahead\nwhen you have the drop_region you need.\n\n-- Alison",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that the patch is not currently useful due to lack of usage, but did not commit to revising or removing it.\n\nThe author acknowledged that the patch addresses a specific issue related to auto-regions and manually created regions in a narrow failure scenario where two devices unbind/bind cycle at the same time, but emphasized it's a rare occurrence.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a limitation",
                "did not promise a fix",
                "acknowledged",
                "emphasized"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah it's not a particularly useful cleanup in the current\ninfrastructure because nothing actually uses this pattern (yet).\n\n---\n\nThe important note here is the difference between auto-regions and\nmanually created regions.  For auto-regions, you might have another\nendpoint show up looking for the partially created region - and then\njust go off and create it anyway because it thinks it was first.\n\nBut in my driver, i'm explicitly converting these auto-regions into\nother things, and if that fails it causes *all other* region creation to\nfail - even if it wasn't actually dependent on that original region.\n\nThis is only an issue if you have two devices unbind/bind cycling at\nthe same time - i.e.\n\n   echo 0000:d0:00.00 > cxl_pci/unbind\n   echo 0000:e0:00.00 > cxl_pci/unbind\n   echo 0000:d0:00.00 > mydriver/bind\n   echo 0000:e0:00.00 > mydriver/bind\n\nIf the platform has pre-programmed and locked the decoders, and one of\nthe two devices fails to probe and leaves a hanging partially\ncreated region, the other device will fail too.\n\nIt's a pretty narrow failure scenario.\n\n~Gregory",
              "reply_to": "Alison Schofield",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alison Schofield",
              "summary": "Reviewer noted that the patch's fix will eventually lead to another failure due to a similar issue, and requested clarification on why this specific case is handled differently.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "clarification needed"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That's by design, and that'll eventually fail too.\n\nBut - is see how your case is different. Thanks for the explanation.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer noted that the fix is good but may not provide sufficient debugging/tracing information in case of failure, and suggested further work on the region creation changes",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "further work required",
                "debugging/tracing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Feel free to add it to this series. I have started to send individual \nseries as you know but the part changing the region creation will \nrequire more work than the already sent.\n\nAbout this fix, it looks good to me, although I have to admit I'm a bit \nlost after following the discussion Allison points to. If we want to \nkeep the state of failure for forensics, not sure if the \ndebugging/tracing or default error info in this case will be enough.\n\nIn any case:\n\nReviewed-by: Alejandro Lucero <alucerop@amd.com>",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that keeping objects around causes more issues, expressed uncertainty about the frequency of the problem but did not commit to addressing it",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "lack of commitment"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah i don't quite follow the want to keep the objects around, it seems\nto cause more issues than it solves - but then i also don't think this\nis going to be a particularly common problem\n\n~Gregory",
              "reply_to": "Alejandro Palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH 0/6] mm/memcontrol: Make memcg limits tier-aware",
          "message_id": "aZ3ysV-k1UisnPRG@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3ysV-k1UisnPRG@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T18:49:25Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joshua Hahn (author)",
              "summary": "Author addressed a concern about the impact of relying on per-memcg-lruvec statistics for limit checking, explaining that it can introduce increased latency and proposing an alternative solution by adding new fields to struct page_counter to track tiered memory limits and usage.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On systems with tiered memory, there is currently no tracking of memory\nat the tier-memcg granularity. While per-memcg-lruvec serves at a finer\ngranularity that can be accumulated to give us the desired\nper-tier-memcg accounting, relying on these lruvec stats for limit\nchecking can prove touch too many hot paths too frequently and can\nintroduce increased latency for other memcg users.\n\nInstead, add a new cacheline in struct page_counter to track toptier\nmemcg limits and usage, as well as cached capacity values. This\ncacheline is only used by the mem_cgroup->memory page_counter.\n\nAlso, introduce helpers that use these new fields to calculate\nproportional toptier high and low values, based on the system's\ntoptier:total capacity ratio.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/page_counter.h | 22 +++++++++++++++++++++-\n mm/page_counter.c            | 34 ++++++++++++++++++++++++++++++++++\n 2 files changed, 55 insertions(+), 1 deletion(-)\n\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex d649b6bbbc87..128c1272c88c 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -5,6 +5,7 @@\n #include <linux/atomic.h>\n #include <linux/cache.h>\n #include <linux/limits.h>\n+#include <linux/nodemask.h>\n #include <asm/page.h>\n \n struct page_counter {\n@@ -31,9 +32,23 @@ struct page_counter {\n \t/* Latest cg2 reset watermark */\n \tunsigned long local_watermark;\n \n-\t/* Keep all the read most fields in a separete cacheline. */\n+\t/* Keep all the tiered memory fields in a separate cacheline. */\n \tCACHELINE_PADDING(_pad2_);\n \n+\tatomic_long_t toptier_usage;\n+\n+\t/* effective toptier-proportional low protection */\n+\tunsigned long etoptier_low;\n+\tatomic_long_t toptier_low_usage;\n+\tatomic_long_t children_toptier_low_usage;\n+\n+\t/* Cached toptier capacity for proportional limit calculations */\n+\tunsigned long toptier_capacity;\n+\tunsigned long total_capacity;\n+\n+\t/* Keep all the read most fields in a separate cacheline. */\n+\tCACHELINE_PADDING(_pad3_);\n+\n \tbool protection_support;\n \tbool track_failcnt;\n \tunsigned long min;\n@@ -61,6 +76,9 @@ static inline void page_counter_init(struct page_counter *counter,\n \tcounter->parent = parent;\n \tcounter->protection_support = protection_support;\n \tcounter->track_failcnt = false;\n+\tcounter->toptier_usage = (atomic_long_t)ATOMIC_LONG_INIT(0);\n+\tcounter->toptier_capacity = 0;\n+\tcounter->total_capacity = 0;\n }\n \n static inline unsigned long page_counter_read(struct page_counter *counter)\n@@ -103,6 +121,8 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n \t\t\t\t       bool recursive_protection);\n+unsigned long page_counter_toptier_high(struct page_counter *counter);\n+unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\n static inline void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t\t\t     struct page_counter *counter,\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex 661e0f2a5127..5ec97811c418 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -462,4 +462,38 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\tatomic_long_read(&parent->children_low_usage),\n \t\t\trecursive_protection));\n }\n+\n+unsigned long page_counter_toptier_high(struct page_counter *counter)\n+{\n+\tunsigned long high = READ_ONCE(counter->high);\n+\tunsigned long toptier_cap, total_cap;\n+\n+\tif (high == PAGE_COUNTER_MAX)\n+\t\treturn PAGE_COUNTER_MAX;\n+\n+\ttoptier_cap = counter->toptier_capacity;\n+\ttotal_cap = counter->total_capacity;\n+\n+\tif (!total_cap)\n+\t\treturn PAGE_COUNTER_MAX;\n+\n+\treturn mult_frac(high, toptier_cap, total_cap);\n+}\n+\n+unsigned long page_counter_toptier_low(struct page_counter *counter)\n+{\n+\tunsigned long low = READ_ONCE(counter->low);\n+\tunsigned long toptier_cap, total_cap;\n+\n+\tif (!low)\n+\t\treturn 0;\n+\n+\ttoptier_cap = counter->toptier_capacity;\n+\ttotal_cap = counter->total_capacity;\n+\n+\tif (!total_cap)\n+\t\treturn 0;\n+\n+\treturn mult_frac(low, toptier_cap, total_cap);\n+}\n #endif /* CONFIG_MEMCG || CONFIG_CGROUP_DMEM */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author addressed a concern about updating toptier statistics when charging or uncharging memory control groups (memcgs). They modified the `charge_memcg` function to update the toptier fields after try_charge_memcg succeeds, and also added new functions `memcg_charge_toptier` and `memcg_uncharge_toptier` to handle this. The author did not explicitly state that a fix is planned for v2, but the changes suggest an intention to address the issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "made changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Modify memcg charging and uncharging sites to also update toptier\nstatistics.\n\nUnfortunately, try_charge_memcg is unaware of the physical folio being\ncharged; it only deals with nr_pages. Instead of modifying\ntry_charge_memcg, instead adjust the toptier fields once\ntry_charge_memcg succeeds, inside charge_memcg.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n mm/memcontrol.c | 39 +++++++++++++++++++++++++++++++++++++++\n 1 file changed, 39 insertions(+)\n\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex f3e4a6ce7181..07464f02c529 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -1948,6 +1948,24 @@ static void memcg_uncharge(struct mem_cgroup *memcg, unsigned int nr_pages)\n \t\tpage_counter_uncharge(&memcg->memsw, nr_pages);\n }\n \n+static void memcg_charge_toptier(struct mem_cgroup *memcg,\n+\t\t\t\t unsigned long nr_pages)\n+{\n+\tstruct page_counter *c;\n+\n+\tfor (c = &memcg->memory; c; c = c->parent)\n+\t\tatomic_long_add(nr_pages, &c->toptier_usage);\n+}\n+\n+static void memcg_uncharge_toptier(struct mem_cgroup *memcg,\n+\t\t\t\t   unsigned long nr_pages)\n+{\n+\tstruct page_counter *c;\n+\n+\tfor (c = &memcg->memory; c; c = c->parent)\n+\t\tatomic_long_sub(nr_pages, &c->toptier_usage);\n+}\n+\n /*\n  * Returns stocks cached in percpu and reset cached information.\n  */\n@@ -4830,6 +4848,9 @@ static int charge_memcg(struct folio *folio, struct mem_cgroup *memcg,\n \tif (ret)\n \t\tgoto out;\n \n+\tif (node_is_toptier(folio_nid(folio)))\n+\t\tmemcg_charge_toptier(memcg, folio_nr_pages(folio));\n+\n \tcss_get(&memcg->css);\n \tcommit_charge(folio, memcg);\n \tmemcg1_commit_charge(folio, memcg);\n@@ -4921,6 +4942,7 @@ int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n struct uncharge_gather {\n \tstruct mem_cgroup *memcg;\n \tunsigned long nr_memory;\n+\tunsigned long nr_toptier;\n \tunsigned long pgpgout;\n \tunsigned long nr_kmem;\n \tint nid;\n@@ -4941,6 +4963,8 @@ static void uncharge_batch(const struct uncharge_gather *ug)\n \t\t}\n \t\tmemcg1_oom_recover(ug->memcg);\n \t}\n+\tif (ug->nr_toptier)\n+\t\tmemcg_uncharge_toptier(ug->memcg, ug->nr_toptier);\n \n \tmemcg1_uncharge_batch(ug->memcg, ug->pgpgout, ug->nr_memory, ug->nid);\n \n@@ -4989,6 +5013,9 @@ static void uncharge_folio(struct folio *folio, struct uncharge_gather *ug)\n \n \tnr_pages = folio_nr_pages(folio);\n \n+\tif (node_is_toptier(folio_nid(folio)))\n+\t\tug->nr_toptier += nr_pages;\n+\n \tif (folio_memcg_kmem(folio)) {\n \t\tug->nr_memory += nr_pages;\n \t\tug->nr_kmem += nr_pages;\n@@ -5072,6 +5099,10 @@ void mem_cgroup_replace_folio(struct folio *old, struct folio *new)\n \t\t\tpage_counter_charge(&memcg->memsw, nr_pages);\n \t}\n \n+\t/* The old folio's toptier_usage will be decremented when it is freed */\n+\tif (node_is_toptier(folio_nid(new)))\n+\t\tmemcg_charge_toptier(memcg, nr_pages);\n+\n \tcss_get(&memcg->css);\n \tcommit_charge(new, memcg);\n \tmemcg1_commit_charge(new, memcg);\n@@ -5091,6 +5122,7 @@ void mem_cgroup_replace_folio(struct folio *old, struct folio *new)\n void mem_cgroup_migrate(struct folio *old, struct folio *new)\n {\n \tstruct mem_cgroup *memcg;\n+\tint old_toptier, new_toptier;\n \n \tVM_BUG_ON_FOLIO(!folio_test_locked(old), old);\n \tVM_BUG_ON_FOLIO(!folio_test_locked(new), new);\n@@ -5111,6 +5143,13 @@ void mem_cgroup_migrate(struct folio *old, struct folio *new)\n \tif (!memcg)\n \t\treturn;\n \n+\told_toptier = node_is_toptier(folio_nid(old));\n+\tnew_toptier = node_is_toptier(folio_nid(new));\n+\tif (old_toptier && !new_toptier)\n+\t\tmemcg_uncharge_toptier(memcg, folio_nr_pages(old));\n+\telse if (!old_toptier && new_toptier)\n+\t\tmemcg_charge_toptier(memcg, folio_nr_pages(old));\n+\n \t/* Transfer the charge and the css ref */\n \tcommit_charge(new, memcg);\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author addressed a concern about the dynamic nature of toptier nodes and how they are defined by three criteria: CPUs, online memory, and cpuset.mems. The author explained that only two of these criteria can change dynamically during runtime (online memory and cpuset.mems), and introduced functions to calculate and update toptier capacity accordingly.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a need for dynamic updates",
                "explained reasoning behind implementation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "What a memcg considers to be a valid toptier node is defined by three\ncriteria: (1) The node has CPUs, (2) The node has online memory,\nand (3) The node is within the cgroup's cpuset.mems.\n\nOf the three, the second and third criteria are the only ones that can\nchange dynamically during runtime, via memory hotplug events and\ncpuset.mems changes, respectively.\n\nIntroduce functions to calculate and update toptier capacity, and call\nthem during cpuset.mems changes and memory hotplug events.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/memcontrol.h   |  6 ++++++\n include/linux/memory-tiers.h | 29 +++++++++++++++++++++++++\n include/linux/page_counter.h |  2 ++\n kernel/cgroup/cpuset.c       |  2 +-\n mm/memcontrol.c              | 17 +++++++++++++++\n mm/memory-tiers.c            | 41 ++++++++++++++++++++++++++++++++++++\n mm/page_counter.c            |  8 +++++++\n 7 files changed, 104 insertions(+), 1 deletion(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 5173a9f16721..900a36112b62 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -608,6 +608,8 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \t\t\t\t     struct mem_cgroup *memcg);\n \n+void update_memcg_toptier_capacity(void);\n+\n static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \t\t\t\t\t  struct mem_cgroup *memcg)\n {\n@@ -1116,6 +1118,10 @@ static inline void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n {\n }\n \n+static inline void update_memcg_toptier_capacity(void)\n+{\n+}\n+\n static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \t\t\t\t\t  struct mem_cgroup *memcg)\n {\ndiff --git a/include/linux/memory-tiers.h b/include/linux/memory-tiers.h\nindex 85440473effb..cf616885e0db 100644\n--- a/include/linux/memory-tiers.h\n+++ b/include/linux/memory-tiers.h\n@@ -53,6 +53,9 @@ int mt_perf_to_adistance(struct access_coordinate *perf, int *adist);\n struct memory_dev_type *mt_find_alloc_memory_type(int adist,\n \t\t\t\t\t\t  struct list_head *memory_types);\n void mt_put_memory_types(struct list_head *memory_types);\n+void mt_get_toptier_nodemask(nodemask_t *mask, const nodemask_t *allowed);\n+unsigned long mt_get_toptier_capacity(const nodemask_t *allowed);\n+unsigned long mt_get_total_capacity(const nodemask_t *allowed);\n #ifdef CONFIG_MIGRATION\n int next_demotion_node(int node, const nodemask_t *allowed_mask);\n void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets);\n@@ -152,5 +155,31 @@ static inline struct memory_dev_type *mt_find_alloc_memory_type(int adist,\n static inline void mt_put_memory_types(struct list_head *memory_types)\n {\n }\n+\n+static inline void mt_get_toptier_nodemask(nodemask_t *mask,\n+\t\t\t\t\t   const nodemask_t *allowed)\n+{\n+\t*mask = node_states[N_MEMORY];\n+\tif (allowed)\n+\t\tnodes_and(*mask, *mask, *allowed);\n+}\n+\n+static inline unsigned long mt_get_toptier_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (allowed && !node_isset(nid, *allowed))\n+\t\t\tcontinue;\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\t}\n+\treturn capacity;\n+}\n+\n+static inline unsigned long mt_get_total_capacity(const nodemask_t *allowed)\n+{\n+\treturn mt_get_toptier_capacity(allowed);\n+}\n #endif\t/* CONFIG_NUMA */\n #endif  /* _LINUX_MEMORY_TIERS_H */\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex 128c1272c88c..ada5f1dd75d4 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -121,6 +121,8 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n \t\t\t\t       bool recursive_protection);\n+void page_counter_update_toptier_capacity(struct page_counter *counter,\n+\t\t\t\t\t  const nodemask_t *allowed);\n unsigned long page_counter_toptier_high(struct page_counter *counter);\n unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 7607dfe516e6..e5641dc1af88 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -2620,7 +2620,6 @@ static void update_nodemasks_hier(struct cpuset *cs, nodemask_t *new_mems)\n \trcu_read_lock();\n \tcpuset_for_each_descendant_pre(cp, pos_css, cs) {\n \t\tstruct cpuset *parent = parent_cs(cp);\n-\n \t\tbool has_mems = nodes_and(*new_mems, cp->mems_allowed, parent->effective_mems);\n \n \t\t/*\n@@ -2701,6 +2700,7 @@ static int update_nodemask(struct cpuset *cs, struct cpuset *trialcs,\n \n \t/* use trialcs->mems_allowed as a temp variable */\n \tupdate_nodemasks_hier(cs, &trialcs->mems_allowed);\n+\tupdate_memcg_toptier_capacity();\n \treturn 0;\n }\n \ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 0be1e823d813..f3e4a6ce7181 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -54,6 +54,7 @@\n #include <linux/seq_file.h>\n #include <linux/vmpressure.h>\n #include <linux/memremap.h>\n+#include <linux/memory-tiers.h>\n #include <linux/mm_inline.h>\n #include <linux/swap_cgroup.h>\n #include <linux/cpu.h>\n@@ -3906,6 +3907,7 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \n \t\tpage_counter_init(&memcg->memory, &parent->memory, memcg_on_dfl);\n \t\tpage_counter_init(&memcg->swap, &parent->swap, false);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, NULL);\n #ifdef CONFIG_MEMCG_V1\n \t\tmemcg->memory.track_failcnt = !memcg_on_dfl;\n \t\tWRITE_ONCE(memcg->oom_kill_disable, READ_ONCE(parent->oom_kill_disable));\n@@ -3917,6 +3919,7 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \t\tinit_memcg_events();\n \t\tpage_counter_init(&memcg->memory, NULL, true);\n \t\tpage_counter_init(&memcg->swap, NULL, false);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, NULL);\n #ifdef CONFIG_MEMCG_V1\n \t\tpage_counter_init(&memcg->kmem, NULL, false);\n \t\tpage_counter_init(&memcg->tcpmem, NULL, false);\n@@ -4804,6 +4807,20 @@ void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \tpage_counter_calculate_protection(&root->memory, &memcg->memory, recursive_protection);\n }\n \n+void update_memcg_toptier_capacity(void)\n+{\n+\tstruct mem_cgroup *memcg;\n+\tnodemask_t allowed;\n+\n+\tfor_each_mem_cgroup(memcg) {\n+\t\tif (memcg == root_mem_cgroup)\n+\t\t\tcontinue;\n+\n+\t\tcpuset_nodes_allowed(memcg->css.cgroup, &allowed);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, &allowed);\n+\t}\n+}\n+\n static int charge_memcg(struct folio *folio, struct mem_cgroup *memcg,\n \t\t\tgfp_t gfp)\n {\ndiff --git a/mm/memory-tiers.c b/mm/memory-tiers.c\nindex a88256381519..259caaf4be8f 100644\n--- a/mm/memory-tiers.c\n+++ b/mm/memory-tiers.c\n@@ -889,6 +889,7 @@ static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\tmutex_lock(&memory_tier_lock);\n \t\tif (clear_node_memory_tier(nn->nid))\n \t\t\testablish_demotion_targets();\n+\t\tupdate_memcg_toptier_capacity();\n \t\tmutex_unlock(&memory_tier_lock);\n \t\tbreak;\n \tcase NODE_ADDED_FIRST_MEMORY:\n@@ -896,6 +897,7 @@ static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\tmemtier = set_node_memory_tier(nn->nid);\n \t\tif (!IS_ERR(memtier))\n \t\t\testablish_demotion_targets();\n+\t\tupdate_memcg_toptier_capacity();\n \t\tmutex_unlock(&memory_tier_lock);\n \t\tbreak;\n \t}\n@@ -941,6 +943,45 @@ bool numa_demotion_enabled = false;\n \n bool tier_aware_memcg_limits;\n \n+void mt_get_toptier_nodemask(nodemask_t *mask, const nodemask_t *allowed)\n+{\n+\tint nid;\n+\n+\t*mask = NODE_MASK_NONE;\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (node_is_toptier(nid))\n+\t\t\tnode_set(nid, *mask);\n+\t}\n+\tif (allowed)\n+\t\tnodes_and(*mask, *mask, *allowed);\n+}\n+\n+unsigned long mt_get_toptier_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\tnodemask_t mask;\n+\n+\tmt_get_toptier_nodemask(&mask, allowed);\n+\tfor_each_node_mask(nid, mask)\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\n+\treturn capacity;\n+}\n+\n+unsigned long mt_get_total_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (allowed && !node_isset(nid, *allowed))\n+\t\t\tcontinue;\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\t}\n+\treturn capacity;\n+}\n+\n #ifdef CONFIG_MIGRATION\n #ifdef CONFIG_SYSFS\n static ssize_t demotion_enabled_show(struct kobject *kobj,\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex 5ec97811c418..cf21c72bfd4e 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -11,6 +11,7 @@\n #include <linux/string.h>\n #include <linux/sched.h>\n #include <linux/bug.h>\n+#include <linux/memory-tiers.h>\n #include <asm/page.h>\n \n static bool track_protection(struct page_counter *c)\n@@ -463,6 +464,13 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\trecursive_protection));\n }\n \n+void page_counter_update_toptier_capacity(struct page_counter *counter,\n+\t\t\t\t\t  const nodemask_t *allowed)\n+{\n+\tcounter->toptier_capacity = mt_get_toptier_capacity(allowed);\n+\tcounter->total_capacity = mt_get_total_capacity(allowed);\n+}\n+\n unsigned long page_counter_toptier_high(struct page_counter *counter)\n {\n \tunsigned long high = READ_ONCE(counter->high);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author addressed a concern about fairness in memory distribution among workloads, explaining that current limits are based on total memory footprint rather than where the memory resides. They updated the existing memory.low protection to be tier-aware in charging, enforcement, and protection calculation, providing best-effort attempts at protecting a fair proportion of toptier memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On machines serving multiple workloads whose memory is isolated via\nthe memory cgroup controller, it is currently impossible to enforce a\nfair distribution of toptier memory among the worloads, as the only\nenforcable limits have to do with total memory footprint, but not where\nthat memory resides.\n\nThis makes ensuring a consistent and baseline performance difficult, as\neach workload's performance is heavily impacted by workload-external\nfactors such as which other workloads are co-located in the same host,\nand the order at which different workloads are started.\n\nExtend the existing memory.low protection to be tier-aware in the\ncharging, enforcement, and protection calculation to provide\nbest-effort attempts at protecting a fair proportion of toptier memory.\n\nUpdates to protection and charging are performed in the same path as\nthe standard memcontrol equivalents. Enforcing tier-aware memcg limits\nhowever, are gated behind the sysctl tier_aware_memcg. This is so that\nruntime-enabling of tier aware limits can account for memory already\npresent in the system.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/memcontrol.h   | 15 +++++++++++----\n include/linux/page_counter.h |  7 ++++---\n kernel/cgroup/dmem.c         |  2 +-\n mm/memcontrol.c              | 14 ++++++++++++--\n mm/page_counter.c            | 35 ++++++++++++++++++++++++++++++++++-\n mm/vmscan.c                  | 13 +++++++++----\n 6 files changed, 71 insertions(+), 15 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 900a36112b62..a998a1e3b8b0 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -606,7 +606,9 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n }\n \n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t     struct mem_cgroup *memcg);\n+\t\t\t\t     struct mem_cgroup *memcg, bool toptier);\n+\n+unsigned long mem_cgroup_toptier_usage(struct mem_cgroup *memcg);\n \n void update_memcg_toptier_capacity(void);\n \n@@ -623,11 +625,15 @@ static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n }\n \n static inline bool mem_cgroup_below_low(struct mem_cgroup *target,\n-\t\t\t\t\tstruct mem_cgroup *memcg)\n+\t\t\t\t\tstruct mem_cgroup *memcg, bool toptier)\n {\n \tif (mem_cgroup_unprotected(target, memcg))\n \t\treturn false;\n \n+\tif (toptier)\n+\t\treturn READ_ONCE(memcg->memory.etoptier_low) >=\n+\t\t\t\t mem_cgroup_toptier_usage(memcg);\n+\n \treturn READ_ONCE(memcg->memory.elow) >=\n \t\tpage_counter_read(&memcg->memory);\n }\n@@ -1114,7 +1120,8 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n }\n \n static inline void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t\t\t   struct mem_cgroup *memcg)\n+\t\t\t\t\t\t   struct mem_cgroup *memcg,\n+\t\t\t\t\t\t   bool toptier)\n {\n }\n \n@@ -1128,7 +1135,7 @@ static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \treturn true;\n }\n static inline bool mem_cgroup_below_low(struct mem_cgroup *target,\n-\t\t\t\t\tstruct mem_cgroup *memcg)\n+\t\t\t\t\tstruct mem_cgroup *memcg, bool toptier)\n {\n \treturn false;\n }\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex ada5f1dd75d4..6635ee7b9575 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -120,15 +120,16 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n #if IS_ENABLED(CONFIG_MEMCG) || IS_ENABLED(CONFIG_CGROUP_DMEM)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n-\t\t\t\t       bool recursive_protection);\n+\t\t\t\t       bool recursive_protection, bool toptier);\n void page_counter_update_toptier_capacity(struct page_counter *counter,\n \t\t\t\t\t  const nodemask_t *allowed);\n unsigned long page_counter_toptier_high(struct page_counter *counter);\n unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\n static inline void page_counter_calculate_protection(struct page_counter *root,\n-\t\t\t\t\t\t     struct page_counter *counter,\n-\t\t\t\t\t\t     bool recursive_protection) {}\n+\t\t\t\t\t\tstruct page_counter *counter,\n+\t\t\t\t\t\tbool recursive_protection,\n+\t\t\t\t\t\tbool toptier) {}\n #endif\n \n #endif /* _LINUX_PAGE_COUNTER_H */\ndiff --git a/kernel/cgroup/dmem.c b/kernel/cgroup/dmem.c\nindex 1ea6afffa985..536d43c42de8 100644\n--- a/kernel/cgroup/dmem.c\n+++ b/kernel/cgroup/dmem.c\n@@ -277,7 +277,7 @@ dmem_cgroup_calculate_protection(struct dmem_cgroup_pool_state *limit_pool,\n \t\t\tcontinue;\n \n \t\tpage_counter_calculate_protection(\n-\t\t\tclimit, &found_pool->cnt, true);\n+\t\t\tclimit, &found_pool->cnt, true, false);\n \n \t\tif (found_pool == test_pool)\n \t\t\tbreak;\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 07464f02c529..8aa7ae361a73 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -4806,12 +4806,13 @@ struct cgroup_subsys memory_cgrp_subsys = {\n  * mem_cgroup_calculate_protection - check if memory consumption is in the normal range\n  * @root: the top ancestor of the sub-tree being checked\n  * @memcg: the memory cgroup to check\n+ * @toptier: whether the caller is in a toptier node\n  *\n  * WARNING: This function is not stateless! It can only be used as part\n  *          of a top-down tree iteration, not for isolated queries.\n  */\n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t     struct mem_cgroup *memcg)\n+\t\t\t\t     struct mem_cgroup *memcg, bool toptier)\n {\n \tbool recursive_protection =\n \t\tcgrp_dfl_root.flags & CGRP_ROOT_MEMORY_RECURSIVE_PROT;\n@@ -4822,7 +4823,16 @@ void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \tif (!root)\n \t\troot = root_mem_cgroup;\n \n-\tpage_counter_calculate_protection(&root->memory, &memcg->memory, recursive_protection);\n+\tpage_counter_calculate_protection(&root->memory, &memcg->memory,\n+\t\t\t\t\t  recursive_protection, toptier);\n+}\n+\n+unsigned long mem_cgroup_toptier_usage(struct mem_cgroup *memcg)\n+{\n+\tif (mem_cgroup_disabled() || !memcg)\n+\t\treturn 0;\n+\n+\treturn atomic_long_read(&memcg->memory.toptier_usage);\n }\n \n void update_memcg_toptier_capacity(void)\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex cf21c72bfd4e..79d46a1c4c0c 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -410,12 +410,39 @@ static unsigned long effective_protection(unsigned long usage,\n \treturn ep;\n }\n \n+static void calculate_protection_toptier(struct page_counter *counter,\n+\t\t\t\t\t bool recursive_protection)\n+{\n+\tstruct page_counter *parent = counter->parent;\n+\tunsigned long toptier_low;\n+\tunsigned long toptier_usage, parent_toptier_usage;\n+\tunsigned long toptier_protected, old_toptier_protected;\n+\tlong delta;\n+\n+\ttoptier_low = page_counter_toptier_low(counter);\n+\ttoptier_usage = atomic_long_read(&counter->toptier_usage);\n+\tparent_toptier_usage = atomic_long_read(&parent->toptier_usage);\n+\n+\t/* Propagate toptier low usage to parent for sibling distribution */\n+\ttoptier_protected = min(toptier_usage, toptier_low);\n+\told_toptier_protected = atomic_long_xchg(&counter->toptier_low_usage,\n+\t\t\t\t\t\t toptier_protected);\n+\tdelta = toptier_protected - old_toptier_protected;\n+\tatomic_long_add(delta, &parent->children_toptier_low_usage);\n+\n+\tWRITE_ONCE(counter->etoptier_low,\n+\t\t   effective_protection(toptier_usage, parent_toptier_usage,\n+\t\t   toptier_low, READ_ONCE(parent->etoptier_low),\n+\t\t   atomic_long_read(&parent->children_toptier_low_usage),\n+\t\t   recursive_protection));\n+}\n \n /**\n  * page_counter_calculate_protection - check if memory consumption is in the normal range\n  * @root: the top ancestor of the sub-tree being checked\n  * @counter: the page_counter the counter to update\n  * @recursive_protection: Whether to use memory_recursiveprot behavior.\n+ * @toptier: Whether to calculate toptier-proportional protection\n  *\n  * Calculates elow/emin thresholds for given page_counter.\n  *\n@@ -424,7 +451,7 @@ static unsigned long effective_protection(unsigned long usage,\n  */\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n-\t\t\t\t       bool recursive_protection)\n+\t\t\t\t       bool recursive_protection, bool toptier)\n {\n \tunsigned long usage, parent_usage;\n \tstruct page_counter *parent = counter->parent;\n@@ -446,6 +473,9 @@ void page_counter_calculate_protection(struct page_counter *root,\n \tif (parent == root) {\n \t\tcounter->emin = READ_ONCE(counter->min);\n \t\tcounter->elow = READ_ONCE(counter->low);\n+\t\tif (toptier)\n+\t\t\tWRITE_ONCE(counter->etoptier_low,\n+\t\t\t\t   page_counter_toptier_low(counter));\n \t\treturn;\n \t}\n \n@@ -462,6 +492,9 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\tREAD_ONCE(parent->elow),\n \t\t\tatomic_long_read(&parent->children_low_usage),\n \t\t\trecursive_protection));\n+\n+\tif (toptier)\n+\t\tcalculate_protection_toptier(counter, recursive_protection);\n }\n \n void page_counter_update_toptier_capacity(struct page_counter *counter,\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 6a87ac7be43c..5b4cb030a477 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -4144,6 +4144,7 @@ static void lru_gen_age_node(struct pglist_data *pgdat, struct scan_control *sc)\n \tstruct mem_cgroup *memcg;\n \tunsigned long min_ttl = READ_ONCE(lru_gen_min_ttl);\n \tbool reclaimable = !min_ttl;\n+\tbool toptier = node_is_toptier(pgdat->node_id);\n \n \tVM_WARN_ON_ONCE(!current_is_kswapd());\n \n@@ -4153,7 +4154,7 @@ static void lru_gen_age_node(struct pglist_data *pgdat, struct scan_control *sc)\n \tdo {\n \t\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);\n \n-\t\tmem_cgroup_calculate_protection(NULL, memcg);\n+\t\tmem_cgroup_calculate_protection(NULL, memcg, toptier);\n \n \t\tif (!reclaimable)\n \t\t\treclaimable = lruvec_is_reclaimable(lruvec, sc, min_ttl);\n@@ -4905,12 +4906,14 @@ static int shrink_one(struct lruvec *lruvec, struct scan_control *sc)\n \tunsigned long reclaimed = sc->nr_reclaimed;\n \tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n \tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n+\tbool toptier = tier_aware_memcg_limits &&\n+\t\t       node_is_toptier(pgdat->node_id);\n \n \t/* lru_gen_age_node() called mem_cgroup_calculate_protection() */\n \tif (mem_cgroup_below_min(NULL, memcg))\n \t\treturn MEMCG_LRU_YOUNG;\n \n-\tif (mem_cgroup_below_low(NULL, memcg)) {\n+\tif (mem_cgroup_below_low(NULL, memcg, toptier)) {\n \t\t/* see the comment on MEMCG_NR_GENS */\n \t\tif (READ_ONCE(lruvec->lrugen.seg) != MEMCG_LRU_TAIL)\n \t\t\treturn MEMCG_LRU_TAIL;\n@@ -5960,6 +5963,7 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t};\n \tstruct mem_cgroup_reclaim_cookie *partial = &reclaim;\n \tstruct mem_cgroup *memcg;\n+\tbool toptier = node_is_toptier(pgdat->node_id);\n \n \t/*\n \t * In most cases, direct reclaimers can do partial walks\n@@ -5987,7 +5991,7 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t\t */\n \t\tcond_resched();\n \n-\t\tmem_cgroup_calculate_protection(target_memcg, memcg);\n+\t\tmem_cgroup_calculate_protection(target_memcg, memcg, toptier);\n \n \t\tif (mem_cgroup_below_min(target_memcg, memcg)) {\n \t\t\t/*\n@@ -5995,7 +5999,8 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t\t\t * If there is no reclaimable memory, OOM.\n \t\t\t */\n \t\t\tcontinue;\n-\t\t} else if (mem_cgroup_below_low(target_memcg, memcg)) {\n+\t\t} else if (mem_cgroup_below_low(target_memcg, memcg,\n+\t\t\t\t\ttier_aware_memcg_limits && toptier)) {\n \t\t\t/*\n \t\t\t * Soft protection.\n \t\t\t * Respect the protection only as long as\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author is addressing a concern about the fairness of memory distribution among workloads in isolated cgroups. They explained that the current limits only consider total memory footprint, not where it resides. The author proposed extending the existing memory.high protection to be tier-aware and adding a new nodemask parameter to try_to_free_mem_cgroup_pages for selective reclaim from memory at the memcg-tier intersection of a cgroup.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On machines serving multiple workloads whose memory is isolated via the\nmemory cgroup controller, it is currently impossible to enforce a fair\ndistribution of toptier memory among the workloads, as the only\nenforcable limits have to do with total memory footprint, but not where\nthat memory resides.\n\nThis makes ensuring a consistent and baseline performance difficult, as\neach workload's performance is heavily impacted by workload-external\nfactors wuch as which other workloads are co-located in the same host,\nand the order at which different workloads are started.\n\nExtend the existing memory.high protection to be tier-aware in the\ncharging and enforcement to limit toptier-hogging for workloads.\n\nAlso, add a new nodemask parameter to try_to_free_mem_cgroup_pages,\nwhich can be used to selectively reclaim from memory at the\nmemcg-tier interection of a cgroup.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/swap.h |  3 +-\n mm/memcontrol-v1.c   |  6 ++--\n mm/memcontrol.c      | 85 +++++++++++++++++++++++++++++++++++++-------\n mm/vmscan.c          | 11 +++---\n 4 files changed, 84 insertions(+), 21 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 0effe3cc50f5..c6037ac7bf6e 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -368,7 +368,8 @@ extern unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t\t  unsigned long nr_pages,\n \t\t\t\t\t\t  gfp_t gfp_mask,\n \t\t\t\t\t\t  unsigned int reclaim_options,\n-\t\t\t\t\t\t  int *swappiness);\n+\t\t\t\t\t\t  int *swappiness,\n+\t\t\t\t\t\t  nodemask_t *allowed);\n extern unsigned long mem_cgroup_shrink_node(struct mem_cgroup *mem,\n \t\t\t\t\t\tgfp_t gfp_mask, bool noswap,\n \t\t\t\t\t\tpg_data_t *pgdat,\ndiff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.c\nindex 0b39ba608109..29630c7f3567 100644\n--- a/mm/memcontrol-v1.c\n+++ b/mm/memcontrol-v1.c\n@@ -1497,7 +1497,8 @@ static int mem_cgroup_resize_max(struct mem_cgroup *memcg,\n \t\t}\n \n \t\tif (!try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL,\n-\t\t\t\tmemsw ? 0 : MEMCG_RECLAIM_MAY_SWAP, NULL)) {\n+\t\t\t\tmemsw ? 0 : MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\tNULL, NULL)) {\n \t\t\tret = -EBUSY;\n \t\t\tbreak;\n \t\t}\n@@ -1529,7 +1530,8 @@ static int mem_cgroup_force_empty(struct mem_cgroup *memcg)\n \t\t\treturn -EINTR;\n \n \t\tif (!try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL,\n-\t\t\t\t\t\t  MEMCG_RECLAIM_MAY_SWAP, NULL))\n+\t\t\t\t\t\t  MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\t\t  NULL, NULL))\n \t\t\tnr_retries--;\n \t}\n \ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 8aa7ae361a73..ebd4a1b73c51 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -2184,18 +2184,30 @@ static unsigned long reclaim_high(struct mem_cgroup *memcg,\n \n \tdo {\n \t\tunsigned long pflags;\n-\n-\t\tif (page_counter_read(&memcg->memory) <=\n-\t\t    READ_ONCE(memcg->memory.high))\n+\t\tnodemask_t toptier_nodes, *reclaim_nodes;\n+\t\tbool mem_high_ok, toptier_high_ok;\n+\n+\t\tmt_get_toptier_nodemask(&toptier_nodes, NULL);\n+\t\tmem_high_ok = page_counter_read(&memcg->memory) <=\n+\t\t\t      READ_ONCE(memcg->memory.high);\n+\t\ttoptier_high_ok = !(tier_aware_memcg_limits &&\n+\t\t\t\t    mem_cgroup_toptier_usage(memcg) >\n+\t\t\t\t    page_counter_toptier_high(&memcg->memory));\n+\t\tif (mem_high_ok && toptier_high_ok)\n \t\t\tcontinue;\n \n+\t\tif (mem_high_ok && !toptier_high_ok)\n+\t\t\treclaim_nodes = &toptier_nodes;\n+\t\telse\n+\t\t\treclaim_nodes = NULL;\n+\n \t\tmemcg_memory_event(memcg, MEMCG_HIGH);\n \n \t\tpsi_memstall_enter(&pflags);\n \t\tnr_reclaimed += try_to_free_mem_cgroup_pages(memcg, nr_pages,\n \t\t\t\t\t\t\tgfp_mask,\n \t\t\t\t\t\t\tMEMCG_RECLAIM_MAY_SWAP,\n-\t\t\t\t\t\t\tNULL);\n+\t\t\t\t\t\t\tNULL, reclaim_nodes);\n \t\tpsi_memstall_leave(&pflags);\n \t} while ((memcg = parent_mem_cgroup(memcg)) &&\n \t\t !mem_cgroup_is_root(memcg));\n@@ -2296,6 +2308,24 @@ static u64 mem_find_max_overage(struct mem_cgroup *memcg)\n \treturn max_overage;\n }\n \n+static u64 toptier_find_max_overage(struct mem_cgroup *memcg)\n+{\n+\tu64 overage, max_overage = 0;\n+\n+\tif (!tier_aware_memcg_limits)\n+\t\treturn 0;\n+\n+\tdo {\n+\t\tunsigned long usage = mem_cgroup_toptier_usage(memcg);\n+\t\tunsigned long high = page_counter_toptier_high(&memcg->memory);\n+\n+\t\toverage = calculate_overage(usage, high);\n+\t\tmax_overage = max(overage, max_overage);\n+\t} while ((memcg = parent_mem_cgroup(memcg)) &&\n+\t\t  !mem_cgroup_is_root(memcg));\n+\n+\treturn max_overage;\n+}\n static u64 swap_find_max_overage(struct mem_cgroup *memcg)\n {\n \tu64 overage, max_overage = 0;\n@@ -2401,6 +2431,14 @@ void __mem_cgroup_handle_over_high(gfp_t gfp_mask)\n \tpenalty_jiffies += calculate_high_delay(memcg, nr_pages,\n \t\t\t\t\t\tswap_find_max_overage(memcg));\n \n+\t/*\n+\t * Don't double-penalize for toptier high overage if system-wide\n+\t * memory.high has already been breached.\n+\t */\n+\tif (!penalty_jiffies)\n+\t\tpenalty_jiffies += calculate_high_delay(memcg, nr_pages,\n+\t\t\t\t\ttoptier_find_max_overage(memcg));\n+\n \t/*\n \t * Clamp the max delay per usermode return so as to still keep the\n \t * application moving forwards and also permit diagnostics, albeit\n@@ -2503,7 +2541,8 @@ static int try_charge_memcg(struct mem_cgroup *memcg, gfp_t gfp_mask,\n \n \tpsi_memstall_enter(&pflags);\n \tnr_reclaimed = try_to_free_mem_cgroup_pages(mem_over_limit, nr_pages,\n-\t\t\t\t\t\t    gfp_mask, reclaim_options, NULL);\n+\t\t\t\t\t\t    gfp_mask, reclaim_options,\n+\t\t\t\t\t\t    NULL, NULL);\n \tpsi_memstall_leave(&pflags);\n \n \tif (mem_cgroup_margin(mem_over_limit) >= nr_pages)\n@@ -2592,23 +2631,26 @@ static int try_charge_memcg(struct mem_cgroup *memcg, gfp_t gfp_mask,\n \t * reclaim, the cost of mismatch is negligible.\n \t */\n \tdo {\n-\t\tbool mem_high, swap_high;\n+\t\tbool mem_high, swap_high, toptier_high = false;\n \n \t\tmem_high = page_counter_read(&memcg->memory) >\n \t\t\tREAD_ONCE(memcg->memory.high);\n \t\tswap_high = page_counter_read(&memcg->swap) >\n \t\t\tREAD_ONCE(memcg->swap.high);\n+\t\ttoptier_high = tier_aware_memcg_limits &&\n+\t\t\t       (mem_cgroup_toptier_usage(memcg) >\n+\t\t\t\tpage_counter_toptier_high(&memcg->memory));\n \n \t\t/* Don't bother a random interrupted task */\n \t\tif (!in_task()) {\n-\t\t\tif (mem_high) {\n+\t\t\tif (mem_high || toptier_high) {\n \t\t\t\tschedule_work(&memcg->high_work);\n \t\t\t\tbreak;\n \t\t\t}\n \t\t\tcontinue;\n \t\t}\n \n-\t\tif (mem_high || swap_high) {\n+\t\tif (mem_high || swap_high || toptier_high) {\n \t\t\t/*\n \t\t\t * The allocating tasks in this cgroup will need to do\n \t\t\t * reclaim or be throttled to prevent further growth\n@@ -4476,7 +4518,7 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n \tunsigned int nr_retries = MAX_RECLAIM_RETRIES;\n \tbool drained = false;\n-\tunsigned long high;\n+\tunsigned long high, toptier_high;\n \tint err;\n \n \tbuf = strstrip(buf);\n@@ -4485,15 +4527,22 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \t\treturn err;\n \n \tpage_counter_set_high(&memcg->memory, high);\n+\ttoptier_high = page_counter_toptier_high(&memcg->memory);\n \n \tif (of->file->f_flags & O_NONBLOCK)\n \t\tgoto out;\n \n \tfor (;;) {\n \t\tunsigned long nr_pages = page_counter_read(&memcg->memory);\n+\t\tunsigned long toptier_pages = mem_cgroup_toptier_usage(memcg);\n \t\tunsigned long reclaimed;\n+\t\tunsigned long to_free;\n+\t\tnodemask_t toptier_nodes, *reclaim_nodes;\n+\t\tbool mem_high_ok = nr_pages <= high;\n+\t\tbool toptier_high_ok = !(tier_aware_memcg_limits &&\n+\t\t\t\t\t toptier_pages > toptier_high);\n \n-\t\tif (nr_pages <= high)\n+\t\tif (mem_high_ok && toptier_high_ok)\n \t\t\tbreak;\n \n \t\tif (signal_pending(current))\n@@ -4505,8 +4554,17 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \t\t\tcontinue;\n \t\t}\n \n-\t\treclaimed = try_to_free_mem_cgroup_pages(memcg, nr_pages - high,\n-\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP, NULL);\n+\t\tmt_get_toptier_nodemask(&toptier_nodes, NULL);\n+\t\tif (mem_high_ok && !toptier_high_ok) {\n+\t\t\treclaim_nodes = &toptier_nodes;\n+\t\t\tto_free = toptier_pages - toptier_high;\n+\t\t} else {\n+\t\t\treclaim_nodes = NULL;\n+\t\t\tto_free = nr_pages - high;\n+\t\t}\n+\t\treclaimed = try_to_free_mem_cgroup_pages(memcg, to_free,\n+\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\tNULL, reclaim_nodes);\n \n \t\tif (!reclaimed && !nr_retries--)\n \t\t\tbreak;\n@@ -4558,7 +4616,8 @@ static ssize_t memory_max_write(struct kernfs_open_file *of,\n \n \t\tif (nr_reclaims) {\n \t\t\tif (!try_to_free_mem_cgroup_pages(memcg, nr_pages - max,\n-\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP, NULL))\n+\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\tNULL, NULL))\n \t\t\t\tnr_reclaims--;\n \t\t\tcontinue;\n \t\t}\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 5b4cb030a477..94498734b4f5 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -6652,7 +6652,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t   unsigned long nr_pages,\n \t\t\t\t\t   gfp_t gfp_mask,\n \t\t\t\t\t   unsigned int reclaim_options,\n-\t\t\t\t\t   int *swappiness)\n+\t\t\t\t\t   int *swappiness, nodemask_t *allowed)\n {\n \tunsigned long nr_reclaimed;\n \tunsigned int noreclaim_flag;\n@@ -6668,6 +6668,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t.may_unmap = 1,\n \t\t.may_swap = !!(reclaim_options & MEMCG_RECLAIM_MAY_SWAP),\n \t\t.proactive = !!(reclaim_options & MEMCG_RECLAIM_PROACTIVE),\n+\t\t.nodemask = allowed,\n \t};\n \t/*\n \t * Traverse the ZONELIST_FALLBACK zonelist of the current node to put\n@@ -6693,7 +6694,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t   unsigned long nr_pages,\n \t\t\t\t\t   gfp_t gfp_mask,\n \t\t\t\t\t   unsigned int reclaim_options,\n-\t\t\t\t\t   int *swappiness)\n+\t\t\t\t\t   int *swappiness, nodemask_t *allowed)\n {\n \treturn 0;\n }\n@@ -7806,9 +7807,9 @@ int user_proactive_reclaim(char *buf,\n \t\t\treclaim_options = MEMCG_RECLAIM_MAY_SWAP |\n \t\t\t\t\t  MEMCG_RECLAIM_PROACTIVE;\n \t\t\treclaimed = try_to_free_mem_cgroup_pages(memcg,\n-\t\t\t\t\t\t batch_size, gfp_mask,\n-\t\t\t\t\t\t reclaim_options,\n-\t\t\t\t\t\t swappiness == -1 ? NULL : &swappiness);\n+\t\t\t\t\tbatch_size, gfp_mask, reclaim_options,\n+\t\t\t\t\tswappiness == -1 ? NULL : &swappiness,\n+\t\t\t\t\tNULL);\n \t\t} else {\n \t\t\tstruct scan_control sc = {\n \t\t\t\t.gfp_mask = current_gfp_context(gfp_mask),\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author is addressing a concern that the patch does not address the issue of tier-aware memcg limits being less effective on systems with tiered memory, where well-behaved workloads can still hurt other workloads by hogging more toptier memory than their 'fair share'. The author explains that introducing tier-aware memcg limits will scale memory.low/high to reflect the ratio of toptier:total memory a cgroup has access to, and provides an example scenario where this is beneficial. The author also introduces a sysctl to toggle between enforcing and overlooking toptier memcg limit breaches.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Memory cgroups provide an interface that allow multiple workloads on a\nhost to co-exist, and establish both weak and strong memory isolation\nguarantees. For large servers and small embedded systems alike, memcgs\nprovide an effective way to provide a baseline quality of service for\nprotected workloads.\n\nThis works, because for the most part, all memory is equal (except for\nzram / zswap). Restricting a cgroup's memory footprint restricts how\nmuch it can hurt other workloads competing for memory. Likewise, setting\nmemory.low or memory.min limits can provide weak and strong guarantees\nto the performance of a cgroup.\n\nHowever, on systems with tiered memory (e.g. CXL / compressed memory),\nthe quality of service guarantees that memcg limits enforced become less\neffective, as memcg has no awareness of the physical location of its\ncharged memory. In other words, a workload that is well-behaved within\nits memcg limits may still be hurting the performance of other\nwell-behaving workloads on the system by hogging more than its\n\"fair share\" of toptier memory.\n\nIntroduce tier-aware memcg limits, which scale memory.low/high to\nreflect the ratio of toptier:total memory the cgroup has access.\n\nTake the following scenario as an example:\nOn a host with 3:1 toptier:lowtier, say 150G toptier, and 50Glowtier,\nsetting a cgroup's limits to:\n\tmemory.min:  15G\n\tmemory.low:  20G\n\tmemory.high: 40G\n\tmemory.max:  50G\n\nWill be enforced at the toptier as:\n\tmemory.min:          15G\n\tmemory.toptier_low:  15G (20 * 150/200)\n\tmemory.toptier_high: 30G (40 * 150/200)\n\tmemory.max:          50G\n\nLet's say that there are 4 such cgroups on the host. Previously, it would\nbe possible for 3 hosts to completely take over all of DRAM, while one\ncgroup could only access the lowtier memory. In the perspective of a\ntier-agnostic memcg limit enforcement, the three cgroups are all\nwell-behaved, consuming within their memory limits.\n\nThis is not to say that the scenario above is incorrect. In fact, for\nletting the hottest cgroups run in DRAM while pushing out colder cgroups\nto lowtier memory lets the system perform the most aggregate work total.\n\nBut for other scenarios, the target might not be maximizing aggregate\nwork, but maximizing the minimum performance guarantee for each\nindividual workload (think hosts shared across different users, such as\nVM hosting services).\n\nTo reflect these two scenarios, introduce a sysctl tier_aware_memcg,\nwhich allows the host to toggle between enforcing and overlooking\ntoptier memcg limit breaches.\n\nThis work is inspired & based off of Kaiyang Zhao's work from 2024 [1],\nwhere he referred to this concept as \"memory tiering fairness\".\nThe biggest difference in the implementations lie in how toptier memory\nis tracked; in his implementation, an lruvec stat aggregation is done on\neach usage check, while in this implementation, a new cacheline is\nintroduced in page_coutner to keep track of toptier usage (Kaiyang also\nintroduces a new cachline in page_counter, but only uses it to cache\ncapacity and thresholds). This implementation also extends the memory\nlimit enforcement to memory.high as well.\n\n[1] https://lore.kernel.org/linux-mm/20240920221202.1734227-1-kaiyang2@cs.cmu.edu/\n\n---\nJoshua Hahn (6):\n  mm/memory-tiers: Introduce tier-aware memcg limit sysfs\n  mm/page_counter: Introduce tiered memory awareness to page_counter\n  mm/memory-tiers, memcontrol: Introduce toptier capacity updates\n  mm/memcontrol: Charge and uncharge from toptier\n  mm/memcontrol, page_counter: Make memory.low tier-aware\n  mm/memcontrol: Make memory.high tier-aware\n\n include/linux/memcontrol.h   |  21 ++++-\n include/linux/memory-tiers.h |  30 +++++++\n include/linux/page_counter.h |  31 ++++++-\n include/linux/swap.h         |   3 +-\n kernel/cgroup/cpuset.c       |   2 +-\n kernel/cgroup/dmem.c         |   2 +-\n mm/memcontrol-v1.c           |   6 +-\n mm/memcontrol.c              | 155 +++++++++++++++++++++++++++++++----\n mm/memory-tiers.c            |  63 ++++++++++++++\n mm/page_counter.c            |  77 ++++++++++++++++-\n mm/vmscan.c                  |  24 ++++--\n 11 files changed, 376 insertions(+), 38 deletions(-)\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Michal Hocko",
              "summary": "Reviewer Michal Hocko questioned whether the patch's assumption that active workingset sizes of all workloads don't fit into the top tier is typical in real-life configurations, and asked if memory consumption on particular tiers should be limited even without external pressure.\n\nReviewer Michal Hocko questioned whether focusing only on the top tier is a long-term solution, noting that similar issues may arise in other tiers and expressing concern about duplicating limits for each/top tier.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "questioning",
                "request for clarification",
                "requested changes",
                "immediate questions"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This assumes that the active workingset size of all workloads doesn't\nfit into the top tier right? Otherwise promotions would make sure to\nthat we have the most active memory in the top tier. Is this typical in\nreal life configurations?\n\nOr do you intend to limit memory consumption on particular tier even\nwithout an external pressure?\n\n---\n\nLet's spend some more time with the interface first. You seem to be\nfocusing only on the top tier with this interface, right? Is this really the\nright way to go long term? What makes you believe that we do not really\nhit the same issue with other tiers as well? Also do we want/need to\nduplicate all the limits for each/top tier? What is the reasoning for\nthe switch to be runtime sysctl rather than boot-time or cgroup mount\noption?\n\nI will likely have more questions but these are immediate ones after\nreading the cover. Please note I haven't really looked at the\nimplementation yet. I really want to understand usecases and interface\nfirst.\n-- \nMichal Hocko\nSUSE Labs",
              "reply_to": "Joshua Hahn",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "Author is addressing Michal's feedback by mentioning the intention to discuss project scope and use cases in LSFMMBPF, but does not directly address or acknowledge any specific technical concerns raised by Michal.\n\nAuthor acknowledged a concern about the impact of a workload violating its fair share of toptier memory, explaining that it mostly hurts other workloads when the aggregate working set size exceeds toptier memory capacity.\n\nAuthor acknowledged that the current approach may not be suitable for all use cases and proposed a different perspective of thinking about memory allocation in a per-workload context, rather than a per-system one.\n\nAuthor responded to Michal Hocko's concern about the realism of the patch examples, stating they are realistic scenarios for cloud providers and hyperscalers.\n\nAuthor acknowledges a concern about the interface's behavior and proposes two alternative modes: 'fixed' and 'opportunistic' reclaim, asking for feedback on which one aligns better with the reviewer's expectations.\n\nAuthor acknowledges that the patch series was sent out of order, and agrees it would have been better to send the LSFMMBPF topic proposal first, causing some potential confusion.\n\nAuthor acknowledged that the current implementation only addresses two-tiered systems and may not be suitable for future multi-tiered systems, but plans to revisit this issue in a later patchset.\n\nAuthor responded to a question about how tier-aware memcg limits handle cases with multiple nodes or tiers in the toptier, asking for clarification on what specific scenario is being referred to.\n\nAuthor addressed Michal Hocko's concern that allowing cgroups to set their own mount options for tier-aware memcg limits could lead to inconsistent behavior and undermine the purpose of having a performance guarantee, agreeing that this approach is not desirable.\n\nAuthor acknowledged that the reviewer's feedback was good and thanked them for reviewing, indicating no further action or revision planned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "clarifying discussion plans",
                "separating series from proposal",
                "clarification",
                "explanation",
                "acknowledged a potential issue",
                "proposed an alternative approach",
                "acknowledged feedback",
                "provided explanation",
                "acknowledges a concern",
                "proposes alternatives",
                "acknowledged a mistake",
                "apologized for confusion",
                "acknowledged limitations",
                "planned future work",
                "asking_for_clarification",
                "agreed with feedback",
                "acknowledgment",
                "appreciation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hello Michal,\n\nI hope that you are doing well! Thank you for taking the time to review my\nwork and leaving your thoughts.\n\nI wanted to note that I hope to bring this discussion to LSFMMBPF as well,\nto discuss what the scope of the project should be, what usecases there\nare (as I will note below), how to make this scalable and sustainable\nfor the future, etc. I'll send out a topic proposal later today. I had\nseparated the series from the proposal because I imagined that this\nseries would go through many versions, so it would be helpful to have\nthe topic as a unified place for pre-conference discussions.\n\n---\n\nYes, for the scenario above, a workload that is violating its fair share\nof toptier memory mostly hurts other workloads if the aggregate working\nset size of all workloads exceeds the size of toptier memory.\n\n---\n\nThis is true. And for a lot of usecases, this is 100% the right thing to do.\nHowever, with this patch I want to encourage a different perspective,\nwhich is to think about things in a per-workload perspective, and not a\nper-system perspective.\n\nHaving hot memory in high tiers and cold memory in low tiers is only\nlogical, since we increase the system's throughput and make the most\noptimal choices for latency. However, what about systems that care about\nobjectives other than simply maximizing throughput?\n\nIn the original cover letter I offered an example of VM hosting services\nthat care less about maximizing host-wide throughput, but more on ensuring\na bottomline performance guarantee for all workloads running on the system.\nFor the users on these services, they don't care that the host their VM is\nrunning on is maximizing throughput; rather, they care that their VM meets\nthe performance guarantees that their provider promised. If there is no\nway to know or enforce which tier of memory their workload lands on, either\nthe bottomline guarantee becomes very underestimated, or users must deal\nwith a high variance in performance.\n\nHere's another example: Let's say there is a host with multiple workloads,\neach serving queries for a database. The host would like to guarantee the\nlowest maximum latency possible, while maximizing the total throughput\nof the system. Once again in this situation, without tier-aware memcg\nlimits the host can maximize throughput, but can only make severely\nunderestimated promises on the bottom line.\n\n---\n\nI would say so. I think that the two examples above are realistic\nscenarios that cloud providers and hyperscalers might face on tiered systems.\n\n---\n\nThis is a great question, and one that I hope to discuss at LSFMMBPF\nto see how people expect an interface like this to work.\n\nOver the past few weeks, I have been discussing this idea during the\nLinux Memory Hotness and Promotion biweekly calls with Gregory Price [1].\nOne of the proposals that we made there (but did not include in this\nseries) is the idea of \"fixed\" vs. \"opportunistic\" reclaim.\n\nFixed mode is what we have here -- start limiting toptier usage whenever\na workload goes above its fair slice of toptier.\nOpportunistic mode would allow workloads to use more toptier memory than\nits fair share, but only be restricted when toptier is pressured.\n\nWhat do you think about these two options? For the stated goal of this\nseries, which is to help maximize the bottom line for workloads, fair\nshare seemed to make sense. Implementing opportunistic mode changes\non top of this work would most likely just be another sysctl.\n\n---\n\nThat sounds good with me, my goal was to bring this out as an RFC patchset\nso folks could look at the code and understand the motivation, and then send\nout the LSFMMBPF topic proposal. In retrospect I think I should have done\nit in the opposite order. I'm sorry if this caused any confusion.\n\n---\n\nYes, that's right. I'm not sure if this is the right way to go long-term\n(say, past the next 5 years). My thinking was that I can stick with doing\nthis for toptier vs. non-toptier memory for now, and deal with having\n3+ tiers in the future, when we start to have systems with that many tiers.\nAFAICT two-tiered systems are still ~relatively new, and I don't think\nthere are a lot of genuine usecases for enforcing mid-tier memory limits\nas of now. Of course, I would be excited to learn about these usecases\nand work this patchset to support them as well if anybody has them.\n\n---\n\nSorry, I'm not sure that I completely understood this question. Are you\nreferring to the case where we have multiple nodes in the toptier?\nIf so, then all of those nodes are treated the same, and don't have\nunique limits. Or are you referring to the case where we have multiple\ntiers in the toptier? If so, I hope the answer above can answer this too.\n\n---\n\nGood point : -) I don't think cgroup mount options are a good idea,\nsince this would mean that we can have a set of cgroups self-policing\ntheir toptier usage, while another cgroup allocates memory unrestricted.\nThis would punish the self-policing cgroup and we would lose the benefit\nof having a bottomline performance guarantee.\n\n---\n\nThat sounds good to me, thank you again for reviewing this work!\nI hope you have a great day : -)\nJoshua\n\n[1] https://lore.kernel.org/linux-mm/c8bc2dce-d4ec-c16e-8df4-2624c48cfc06@google.com/",
              "reply_to": "Michal Hocko",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer noted that the patch does not handle the case where tier-aware memcg limits are toggled off, and requested a check to ensure that the sysfs attribute is only accessed when the feature is enabled.\n\nReviewer Gregory Price expressed concern that the patch's assumption about always wanting tier-aware memcg limits may reduce the usefulness of secondary memory tiers, as services will prefer not to be deployed on machines with high performance variance.\n\nReviewer Gregory Price noted that lack of tier-awareness is a significant blocker for deploying mixed workloads on large, dense memory systems with multiple tiers (2+), and suggested using the existing knobs (max/high/low/min) to proportionally control coherent memory tiers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "concerns",
                "reduces usefulness",
                "blocker",
                "significant"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Just injecting a few points here\n(disclosure: I have been in the development loop for this feature)\n\n---\n\nYes / No.  This makes the assumption that you always want this.\n\nBarring a minimum Quality of Service mechanism (as Joshua explains)\nthis reduces the usefulness of a secondary tier of memory.\n\nServices will just prefer not to be deployed to these kinds of\nmachines because the performance variance is too high.\n\n---\n\nThe answer is unequivocally yes.\n\nLacking tier-awareness is actually a huge blocker for deploying mixed\nworkloads on large, dense memory systems with multiple tiers (2+).\n\nTechnically we're already at 4-ish tiers: DDR, CXL, ZSWAP, SWAP.\n\nWe have zswap/swap controls in cgroups already, we just lack that same\ncontrol for coherent memory tiers.  This tries to use the existing nobs\n(max/high/low/min) to do what they already do - just proportionally.\n\n~Gregory",
              "reply_to": "Joshua Hahn",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kaiyang Zhao",
              "summary": "Reviewer Kaiyang Zhao noted that current server memory shapes and workload stacking settings cause contention of top-tier memory, leading to significant variations in tail latency and throughput for co-colocated workloads, which this patch set aims to alleviate.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEUTRAL"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hello! I'm the author of the RFC in 2024. Just want to add that we've\nrecently released a preprint paper on arXiv that includes case studies\nwith a few of Meta's production workloads using a prototype version of\nthe patches.\n\nThe results confirmed that co-colocated workloads can have working set\nsizes exceeding the limited top-tier memory capacity given today's\nserver memory shapes and workload stacking settings, causing contention\nof top-tier memory. Workloads see significant variations in tail\nlatency and throughput depending on the share of top-tier tier memory\nthey get, which this patch set will alleviate.\n\nBest,\nKaiyang\n\n[1] https://arxiv.org/pdf/2602.08800",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC][RFC PATCH v4 00/27] Private Memory Nodes (w/ Compressed RAM)",
          "message_id": "aZ3X3Jni0HZXZMVl@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3X3Jni0HZXZMVl@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T16:54:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch introduces a new PCI driver called cxl_compression, which is part of a larger series that adds support for Private Memory Nodes (PMNs) and Compressed RAM. The PMN feature allows for the creation of isolated NUMA nodes, while Compressed RAM enables the compression of memory pages to reduce memory usage. The cxl_compression driver is designed to work with CXL (Compute Express Link) devices, which are used to manage compressed memory. This patch adds the necessary infrastructure to support the cxl_compression driver and allows for the creation of PMNs and Compressed RAM services.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the node_private infrastructure being mutually exclusive with N_MEMORY, explained that it's intended for memory nodes not meant for general consumption, and confirmed that Zonelist construction changes are deferred to a subsequent commit.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY nodes are intended to contain general System RAM. Today, some\ndevice drivers hotplug their memory (marked Specific Purpose or Reserved)\nto get access to mm/ services, but don't intend it for general consumption.\n\nCreate N_MEMORY_PRIVATE for memory nodes whose memory is not intended for\ngeneral consumption. This state is mutually exclusive with N_MEMORY.\n\nAdd the node_private infrastructure for N_MEMORY_PRIVATE nodes:\n\n  - struct node_private: Per-node container stored in NODE_DATA(nid),\n    holding driver callbacks (ops), owner, and refcount.\n\n  - struct node_private_ops: Initial structure with void *reserved\n    placeholder and flags field.  Callbacks will be added by subsequent\n    commits as each consumer is wired up.\n\n  - folio_is_private_node() / page_is_private_node(): check if a\n    folio/page resides on a private node.\n\n  - folio_node_private_ops() / node_private_flags(): retrieve the ops\n    vtable or flags for a folio's node.\n\n  - Registration API: node_private_register()/unregister() for drivers\n    to register callbacks for private nodes. Only one driver callback\n    can be registered per node - attempting to register different ops\n    returns -EBUSY.\n\n  - sysfs attribute exposing N_MEMORY_PRIVATE node state.\n\nZonelist construction changes for private nodes are deferred to a\nsubsequent commit.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 197 ++++++++++++++++++++++++++++++++\n include/linux/mmzone.h       |   4 +\n include/linux/node_private.h | 210 +++++++++++++++++++++++++++++++++++\n include/linux/nodemask.h     |   1 +\n 4 files changed, 412 insertions(+)\n create mode 100644 include/linux/node_private.h\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 00cf4532f121..646dc48a23b5 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -22,6 +22,7 @@\n #include <linux/swap.h>\n #include <linux/slab.h>\n #include <linux/memblock.h>\n+#include <linux/node_private.h>\n \n static const struct bus_type node_subsys = {\n \t.name = \"node\",\n@@ -861,6 +862,198 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n \t\t\t   (void *)&nid, register_mem_block_under_node_hotplug);\n \treturn;\n }\n+\n+static DEFINE_MUTEX(node_private_lock);\n+static bool node_private_initialized;\n+\n+/**\n+ * node_private_register - Register a private node\n+ * @nid: Node identifier\n+ * @np: The node_private structure (driver-allocated, driver-owned)\n+ *\n+ * Register a driver for a private node. Only one driver can register\n+ * per node. If another driver has already registered (with different np),\n+ * -EBUSY is returned. Re-registration with the same np is allowed.\n+ *\n+ * The driver owns the node_private memory and must ensure it remains valid\n+ * until refcount reaches 0 after node_private_unregister().\n+ *\n+ * Returns 0 on success, negative errno on failure.\n+ */\n+int node_private_register(int nid, struct node_private *np)\n+{\n+\tstruct node_private *existing;\n+\tpg_data_t *pgdat;\n+\tint ret = 0;\n+\n+\tif (!np || !node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tif (!node_private_initialized)\n+\t\treturn -ENODEV;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\t/* N_MEMORY_PRIVATE and N_MEMORY are mutually exclusive */\n+\tif (node_state(nid, N_MEMORY)) {\n+\t\tret = -EBUSY;\n+\t\tgoto out;\n+\t}\n+\n+\tpgdat = NODE_DATA(nid);\n+\texisting = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t\t     lockdep_is_held(&node_private_lock));\n+\n+\t/* Only one source my register this node */\n+\tif (existing) {\n+\t\tif (existing != np) {\n+\t\t\tret = -EBUSY;\n+\t\t\tgoto out;\n+\t\t}\n+\t\tgoto out;\n+\t}\n+\n+\trefcount_set(&np->refcount, 1);\n+\tinit_completion(&np->released);\n+\n+\trcu_assign_pointer(pgdat->node_private, np);\n+\tpgdat->private = true;\n+\n+out:\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_register);\n+\n+/**\n+ * node_private_set_ops - Set service callbacks on a registered private node\n+ * @nid: Node identifier\n+ * @ops: Service callbacks and flags (driver-owned, must outlive registration)\n+ *\n+ * Validates flag dependencies and sets the ops on the node's node_private.\n+ * The node must already be registered via node_private_register().\n+ *\n+ * Returns 0 on success, -EINVAL for invalid flag combinations,\n+ * -ENODEV if no node_private is registered on @nid.\n+ */\n+int node_private_set_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!ops)\n+\t\treturn -EINVAL;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse\n+\t\tnp->ops = ops;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_set_ops);\n+\n+/**\n+ * node_private_clear_ops - Clear service callbacks from a private node\n+ * @nid: Node identifier\n+ * @ops: Expected ops pointer (must match current ops)\n+ *\n+ * Clears the ops only if @ops matches the currently registered ops,\n+ * preventing one service from accidentally clearing another's callbacks.\n+ *\n+ * Returns 0 on success, -ENODEV if no node_private is registered,\n+ * -EINVAL if @ops does not match.\n+ */\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse if (np->ops != ops)\n+\t\tret = -EINVAL;\n+\telse\n+\t\tnp->ops = NULL;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_clear_ops);\n+\n+/**\n+ * node_private_unregister - Unregister a private node\n+ * @nid: Node identifier\n+ *\n+ * Unregister the driver from a private node. Only succeeds if all memory\n+ * has been offlined and the node is no longer N_MEMORY_PRIVATE.\n+ * When successful, drops the refcount to 0 indicating the driver can\n+ * free its context.\n+ *\n+ * N_MEMORY_PRIVATE state is cleared by offline_pages() when the last\n+ * memory is offlined, not by this function.\n+ *\n+ * Return: 0 if unregistered, -EBUSY if N_MEMORY_PRIVATE is still set\n+ * (other memory blocks remain on this node).\n+ */\n+int node_private_unregister(int nid)\n+{\n+\tstruct node_private *np;\n+\tpg_data_t *pgdat;\n+\n+\tif (!node_possible(nid))\n+\t\treturn 0;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\tpgdat = NODE_DATA(nid);\n+\tnp = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Only unregister if all memory is offline and N_MEMORY_PRIVATE is\n+\t * cleared. N_MEMORY_PRIVATE is cleared by offline_pages() when the\n+\t * last memory block is offlined.\n+\t */\n+\tif (node_state(nid, N_MEMORY_PRIVATE)) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn -EBUSY;\n+\t}\n+\n+\trcu_assign_pointer(pgdat->node_private, NULL);\n+\tpgdat->private = false;\n+\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\n+\tsynchronize_rcu();\n+\n+\tif (!refcount_dec_and_test(&np->refcount))\n+\t\twait_for_completion(&np->released);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(node_private_unregister);\n+\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n /**\n@@ -959,6 +1152,7 @@ static struct node_attr node_state_attr[] = {\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n \t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\n \t\t\t\t\t   N_GENERIC_INITIATOR),\n@@ -972,6 +1166,7 @@ static struct attribute *node_state_attrs[] = {\n \t&node_state_attr[N_HIGH_MEMORY].attr.attr,\n #endif\n \t&node_state_attr[N_MEMORY].attr.attr,\n+\t&node_state_attr[N_MEMORY_PRIVATE].attr.attr,\n \t&node_state_attr[N_CPU].attr.attr,\n \t&node_state_attr[N_GENERIC_INITIATOR].attr.attr,\n \tNULL\n@@ -1007,5 +1202,7 @@ void __init node_dev_init(void)\n \t\t\tpanic(\"%s() failed to add node: %d\\n\", __func__, ret);\n \t}\n \n+\tnode_private_initialized = true;\n+\n \tregister_memory_blocks_under_nodes();\n }\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex b01cb1e49896..992eb1c5a2c6 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -25,6 +25,8 @@\n #include <linux/zswap.h>\n #include <asm/page.h>\n \n+struct node_private;\n+\n /* Free memory management - zoned buddy allocator.  */\n #ifndef CONFIG_ARCH_FORCE_MAX_ORDER\n #define MAX_PAGE_ORDER 10\n@@ -1514,6 +1516,8 @@ typedef struct pglist_data {\n \tatomic_long_t\t\tvm_stat[NR_VM_NODE_STAT_ITEMS];\n #ifdef CONFIG_NUMA\n \tstruct memory_tier __rcu *memtier;\n+\tstruct node_private __rcu *node_private;\n+\tbool private;\n #endif\n #ifdef CONFIG_MEMORY_FAILURE\n \tstruct memory_failure_stats mf_stats;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nnew file mode 100644\nindex 000000000000..6a70ec39d569\n--- /dev/null\n+++ b/include/linux/node_private.h\n@@ -0,0 +1,210 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_NODE_PRIVATE_H\n+#define _LINUX_NODE_PRIVATE_H\n+\n+#include <linux/completion.h>\n+#include <linux/mm.h>\n+#include <linux/nodemask.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+\n+struct page;\n+struct vm_area_struct;\n+struct vm_fault;\n+\n+/**\n+ * struct node_private_ops - Callbacks for private node services\n+ *\n+ * Services register these callbacks to intercept MM operations that affect\n+ * their private nodes.\n+ *\n+ * Flag bits control which MM subsystems may operate on folios on this node.\n+ *\n+ * The pgdat->node_private pointer is RCU-protected.  Callbacks fall into\n+ * three categories based on their calling context:\n+ *\n+ * Folio-referenced callbacks (RCU released before callback):\n+ *   The caller holds a reference to a folio on the private node, which\n+ *   pins the node's memory online and prevents node_private teardown.\n+ *\n+ * Refcounted callbacks (RCU released before callback):\n+ *   The caller has no folio on the private node (e.g., folios are on a\n+ *   source node being migrated TO this node).  A temporary refcount is\n+ *   taken on node_private under rcu_read_lock to keep the structure (and\n+ *   the service module) alive across the callback.  node_private_unregister\n+ *   waits for all temporary references to drain before returning.\n+ *\n+ * Non-folio callbacks (rcu_read_lock held during callback):\n+ *   No folio reference exists, so rcu_read_lock is held across the\n+ *   callback to prevent node_private from being freed.\n+ *   These callbacks MUST NOT sleep.\n+ *\n+ * @flags: Operation exclusion flags (NP_OPS_* constants).\n+ *\n+ */\n+struct node_private_ops {\n+\tunsigned long flags;\n+};\n+\n+/**\n+ * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n+ *\n+ * This structure is allocated by the driver and passed to node_private_register().\n+ * The driver owns the memory and must ensure it remains valid until after\n+ * node_private_unregister() returns with the reference count dropped to 0.\n+ *\n+ * @owner: Opaque driver identifier\n+ * @refcount: Reference count (1 = registered; temporary refs for non-folio\n+ *\t\tcallbacks that may sleep; 0 = fully released)\n+ * @released: Signaled when refcount drops to 0; unregister waits on this\n+ * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ */\n+struct node_private {\n+\tvoid *owner;\n+\trefcount_t refcount;\n+\tstruct completion released;\n+\tconst struct node_private_ops *ops;\n+};\n+\n+#ifdef CONFIG_NUMA\n+\n+#include <linux/mmzone.h>\n+\n+/**\n+ * folio_is_private_node - Check if folio is on an N_MEMORY_PRIVATE node\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio resides on a private node.\n+ */\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn node_state(folio_nid(folio), N_MEMORY_PRIVATE);\n+}\n+\n+/**\n+ * page_is_private_node - Check if page is on an N_MEMORY_PRIVATE node\n+ * @page: The page to check\n+ *\n+ * Returns true if the page resides on a private node.\n+ */\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\tconst struct node_private_ops *ops;\n+\tstruct node_private *np;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(folio_nid(folio))->node_private);\n+\tops = np ? np->ops : NULL;\n+\trcu_read_unlock();\n+\n+\treturn ops;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\tstruct node_private *np;\n+\tunsigned long flags;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tflags = (np && np->ops) ? np->ops->flags : 0;\n+\trcu_read_unlock();\n+\n+\treturn flags;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn node_private_flags(folio_nid(f)) & flag;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn node_private_flags(nid) & flag;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn node_private_flags(zone_to_nid(z)) & flag;\n+}\n+\n+#else /* !CONFIG_NUMA */\n+\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn false;\n+}\n+\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn false;\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\treturn NULL;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+#endif /* CONFIG_NUMA */\n+\n+#if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\n+\n+int node_private_register(int nid, struct node_private *np);\n+int node_private_unregister(int nid);\n+int node_private_set_ops(int nid, const struct node_private_ops *ops);\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n+\n+#else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n+\n+static inline int node_private_register(int nid, struct node_private *np)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_unregister(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline int node_private_set_ops(int nid,\n+\t\t\t\t       const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_clear_ops(int nid,\n+\t\t\t\t\t const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+#endif /* CONFIG_NUMA && CONFIG_MEMORY_HOTPLUG */\n+\n+#endif /* _LINUX_NODE_PRIVATE_H */\ndiff --git a/include/linux/nodemask.h b/include/linux/nodemask.h\nindex bd38648c998d..c9bcfd5a9a06 100644\n--- a/include/linux/nodemask.h\n+++ b/include/linux/nodemask.h\n@@ -391,6 +391,7 @@ enum node_states {\n \tN_HIGH_MEMORY = N_NORMAL_MEMORY,\n #endif\n \tN_MEMORY,\t\t/* The node has memory(regular, high, movable) */\n+\tN_MEMORY_PRIVATE,\t/* The node's memory is private */\n \tN_CPU,\t\t/* The node has one or more cpus */\n \tN_GENERIC_INITIATOR,\t/* The node has one or more Generic Initiators */\n \tNR_NODE_STATES\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about general allocations landing on private nodes without explicit permission by introducing __GFP_PRIVATE and updating cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE nodes unless this flag is set.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY_PRIVATE nodes hold device-managed memory that should not be\nused for general allocations. Without a gating mechanism, any allocation\ncould land on a private node if it appears in the task's mems_allowed.\n\nIntroduce __GFP_PRIVATE that explicitly opts in to allocation from\nN_MEMORY_PRIVATE nodes.\n\nAdd the GFP_PRIVATE compound mask (__GFP_PRIVATE | __GFP_THISNODE)\nfor callers that explicitly target private nodes to help prevent\nfallback allocations from DRAM.\n\nUpdate cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE\nnodes unless __GFP_PRIVATE is set.\n\nIn interrupt context, only N_MEMORY nodes are valid.\n\nUpdate cpuset_handle_hotplug() to include N_MEMORY_PRIVATE nodes in\nthe effective mems set, allowing cgroup-level control over private\nnode access.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/gfp_types.h      | 15 +++++++++++++--\n include/trace/events/mmflags.h |  4 ++--\n kernel/cgroup/cpuset.c         | 32 ++++++++++++++++++++++++++++----\n 3 files changed, 43 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/gfp_types.h b/include/linux/gfp_types.h\nindex 3de43b12209e..ac375f9a0fc2 100644\n--- a/include/linux/gfp_types.h\n+++ b/include/linux/gfp_types.h\n@@ -33,7 +33,7 @@ enum {\n \t___GFP_IO_BIT,\n \t___GFP_FS_BIT,\n \t___GFP_ZERO_BIT,\n-\t___GFP_UNUSED_BIT,\t/* 0x200u unused */\n+\t___GFP_PRIVATE_BIT,\n \t___GFP_DIRECT_RECLAIM_BIT,\n \t___GFP_KSWAPD_RECLAIM_BIT,\n \t___GFP_WRITE_BIT,\n@@ -69,7 +69,7 @@ enum {\n #define ___GFP_IO\t\tBIT(___GFP_IO_BIT)\n #define ___GFP_FS\t\tBIT(___GFP_FS_BIT)\n #define ___GFP_ZERO\t\tBIT(___GFP_ZERO_BIT)\n-/* 0x200u unused */\n+#define ___GFP_PRIVATE\t\tBIT(___GFP_PRIVATE_BIT)\n #define ___GFP_DIRECT_RECLAIM\tBIT(___GFP_DIRECT_RECLAIM_BIT)\n #define ___GFP_KSWAPD_RECLAIM\tBIT(___GFP_KSWAPD_RECLAIM_BIT)\n #define ___GFP_WRITE\t\tBIT(___GFP_WRITE_BIT)\n@@ -139,6 +139,11 @@ enum {\n  * %__GFP_ACCOUNT causes the allocation to be accounted to kmemcg.\n  *\n  * %__GFP_NO_OBJ_EXT causes slab allocation to have no object extension.\n+ *\n+ * %__GFP_PRIVATE allows allocation from N_MEMORY_PRIVATE nodes (e.g., compressed\n+ * memory, accelerator memory). Without this flag, allocations are restricted\n+ * to N_MEMORY nodes only. Used by migration/demotion paths when explicitly\n+ * targeting private nodes.\n  */\n #define __GFP_RECLAIMABLE ((__force gfp_t)___GFP_RECLAIMABLE)\n #define __GFP_WRITE\t((__force gfp_t)___GFP_WRITE)\n@@ -146,6 +151,7 @@ enum {\n #define __GFP_THISNODE\t((__force gfp_t)___GFP_THISNODE)\n #define __GFP_ACCOUNT\t((__force gfp_t)___GFP_ACCOUNT)\n #define __GFP_NO_OBJ_EXT   ((__force gfp_t)___GFP_NO_OBJ_EXT)\n+#define __GFP_PRIVATE\t((__force gfp_t)___GFP_PRIVATE)\n \n /**\n  * DOC: Watermark modifiers\n@@ -367,6 +373,10 @@ enum {\n  * available and will not wake kswapd/kcompactd on failure. The _LIGHT\n  * version does not attempt reclaim/compaction at all and is by default used\n  * in page fault path, while the non-light is used by khugepaged.\n+ *\n+ * %GFP_PRIVATE adds %__GFP_THISNODE by default to prevent any fallback\n+ * allocations to other nodes, given that the caller was already attempting\n+ * to access driver-managed memory explicitly.\n  */\n #define GFP_ATOMIC\t(__GFP_HIGH|__GFP_KSWAPD_RECLAIM)\n #define GFP_KERNEL\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n@@ -382,5 +392,6 @@ enum {\n #define GFP_TRANSHUGE_LIGHT\t((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \\\n \t\t\t __GFP_NOMEMALLOC | __GFP_NOWARN) & ~__GFP_RECLAIM)\n #define GFP_TRANSHUGE\t(GFP_TRANSHUGE_LIGHT | __GFP_DIRECT_RECLAIM)\n+#define GFP_PRIVATE\t(__GFP_PRIVATE | __GFP_THISNODE)\n \n #endif /* __LINUX_GFP_TYPES_H */\ndiff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h\nindex a6e5a44c9b42..f042cd848451 100644\n--- a/include/trace/events/mmflags.h\n+++ b/include/trace/events/mmflags.h\n@@ -37,7 +37,8 @@\n \tTRACE_GFP_EM(HARDWALL)\t\t\t\\\n \tTRACE_GFP_EM(THISNODE)\t\t\t\\\n \tTRACE_GFP_EM(ACCOUNT)\t\t\t\\\n-\tTRACE_GFP_EM(ZEROTAGS)\n+\tTRACE_GFP_EM(ZEROTAGS)\t\t\t\\\n+\tTRACE_GFP_EM(PRIVATE)\n \n #ifdef CONFIG_KASAN_HW_TAGS\n # define TRACE_GFP_FLAGS_KASAN\t\t\t\\\n@@ -73,7 +74,6 @@\n TRACE_GFP_FLAGS\n \n /* Just in case these are ever used */\n-TRACE_DEFINE_ENUM(___GFP_UNUSED_BIT);\n TRACE_DEFINE_ENUM(___GFP_LAST_BIT);\n \n #define gfpflag_string(flag) {(__force unsigned long)flag, #flag}\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 473aa9261e16..1a597f0c7c6c 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -444,21 +444,32 @@ static void guarantee_active_cpus(struct task_struct *tsk,\n }\n \n /*\n- * Return in *pmask the portion of a cpusets's mems_allowed that\n+ * Return in *pmask the portion of a cpuset's mems_allowed that\n  * are online, with memory.  If none are online with memory, walk\n  * up the cpuset hierarchy until we find one that does have some\n  * online mems.  The top cpuset always has some mems online.\n  *\n  * One way or another, we guarantee to return some non-empty subset\n- * of node_states[N_MEMORY].\n+ * of node_states[N_MEMORY].  N_MEMORY_PRIVATE nodes from the\n+ * original cpuset are preserved, but only N_MEMORY nodes are\n+ * pulled from ancestors.\n  *\n  * Call with callback_lock or cpuset_mutex held.\n  */\n static void guarantee_online_mems(struct cpuset *cs, nodemask_t *pmask)\n {\n+\tstruct cpuset *orig_cs = cs;\n+\tint nid;\n+\n \twhile (!nodes_intersects(cs->effective_mems, node_states[N_MEMORY]))\n \t\tcs = parent_cs(cs);\n+\n \tnodes_and(*pmask, cs->effective_mems, node_states[N_MEMORY]);\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_isset(nid, orig_cs->effective_mems))\n+\t\t\tnode_set(nid, *pmask);\n+\t}\n }\n \n /**\n@@ -4075,7 +4086,9 @@ static void cpuset_handle_hotplug(void)\n \n \t/* fetch the available cpus/mems and find out which changed how */\n \tcpumask_copy(&new_cpus, cpu_active_mask);\n-\tnew_mems = node_states[N_MEMORY];\n+\n+\t/* Include N_MEMORY_PRIVATE so cpuset controls access the same way */\n+\tnodes_or(new_mems, node_states[N_MEMORY], node_states[N_MEMORY_PRIVATE]);\n \n \t/*\n \t * If subpartitions_cpus is populated, it is likely that the check\n@@ -4488,10 +4501,21 @@ bool cpuset_node_allowed(struct cgroup *cgroup, int nid)\n  * __alloc_pages() will include all nodes.  If the slab allocator\n  * is passed an offline node, it will fall back to the local node.\n  * See kmem_cache_alloc_node().\n+ *\n+ *\n+ * Private nodes aren't eligible for these allocations, so skip them.\n+ * guarantee_online_mems guaranttes at least one N_MEMORY node is set.\n  */\n static int cpuset_spread_node(int *rotor)\n {\n-\treturn *rotor = next_node_in(*rotor, current->mems_allowed);\n+\tint node;\n+\n+\tdo {\n+\t\tnode = next_node_in(*rotor, current->mems_allowed);\n+\t\t*rotor = node;\n+\t} while (node_state(node, N_MEMORY_PRIVATE));\n+\n+\treturn node;\n }\n \n /**\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern that the open-coded cpuset filtering in mm/ does not account for N_MEMORY_PRIVATE nodes on systems without cpusets, which can lead to private-node zones leaking into allocation paths. The author added a new helper function numa_zone_allowed() and replaced the open-coded patterns with it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Various locations in mm/ open-code cpuset filtering with:\n\n  cpusets_enabled() && ALLOC_CPUSET && !__cpuset_zone_allowed()\n\nThis pattern does not account for N_MEMORY_PRIVATE nodes on systems\nwithout cpusets, so private-node zones can leak into allocation\npaths that should only see general-purpose memory.\n\nAdd numa_zone_allowed() which consolidates zone filtering. It checks\ncpuset membership when cpusets are enabled, and otherwise gates\nN_MEMORY_PRIVATE zones behind __GFP_PRIVATE globally.\n\nReplace the open-coded patterns in mm/ with the new helper.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/compaction.c |  6 ++----\n mm/hugetlb.c    |  2 +-\n mm/internal.h   |  7 +++++++\n mm/page_alloc.c | 31 ++++++++++++++++++++-----------\n mm/slub.c       |  3 ++-\n 5 files changed, 32 insertions(+), 17 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..6a65145b03d8 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -2829,10 +2829,8 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tenum compact_result status;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 51273baec9e5..f2b914ab5910 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -1353,7 +1353,7 @@ static struct folio *dequeue_hugetlb_folio_nodemask(struct hstate *h, gfp_t gfp_\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {\n \t\tstruct folio *folio;\n \n-\t\tif (!cpuset_zone_allowed(zone, gfp_mask))\n+\t\tif (!numa_zone_alloc_allowed(ALLOC_CPUSET, zone, gfp_mask))\n \t\t\tcontinue;\n \t\t/*\n \t\t * no need to ask again on the same node. Pool is node rather than\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 23ee14790227..97023748e6a9 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t      gfp_t gfp_mask);\n #else\n #define node_reclaim_mode 0\n \n@@ -1218,6 +1220,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t\t     gfp_t gfp_mask)\n+{\n+\treturn true;\n+}\n #endif\n \n static inline bool node_reclaim_enabled(void)\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 2facee0805da..47f2619d3840 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3690,6 +3690,21 @@ static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n \treturn node_distance(zone_to_nid(local_zone), zone_to_nid(zone)) <=\n \t\t\t\tnode_reclaim_distance;\n }\n+\n+/* Returns true if allocation from this zone is permitted */\n+bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone, gfp_t gfp_mask)\n+{\n+\t/* Gate N_MEMORY_PRIVATE zones behind __GFP_PRIVATE */\n+\tif (!(gfp_mask & __GFP_PRIVATE) &&\n+\t    node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn false;\n+\n+\t/* If cpusets is being used, check mems_allowed */\n+\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET))\n+\t\treturn cpuset_zone_allowed(zone, gfp_mask);\n+\n+\treturn true;\n+}\n #else\t/* CONFIG_NUMA */\n static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n {\n@@ -3781,10 +3796,8 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\tstruct page *page;\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \t\t/*\n \t\t * When allocating a page cache page for writing, we\n \t\t * want to get it from a node that is within its dirty\n@@ -4585,10 +4598,8 @@ should_reclaim_retry(gfp_t gfp_mask, unsigned order,\n \t\tunsigned long min_wmark = min_wmark_pages(zone);\n \t\tbool wmark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tavailable = reclaimable = zone_reclaimable_pages(zone);\n \t\tavailable += zone_page_state_snapshot(zone, NR_FREE_PAGES);\n@@ -5084,10 +5095,8 @@ unsigned long alloc_pages_bulk_noprof(gfp_t gfp, int preferred_nid,\n \tfor_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&\n-\t\t    !__cpuset_zone_allowed(zone, gfp)) {\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp))\n \t\t\tcontinue;\n-\t\t}\n \n \t\tif (nr_online_nodes > 1 && zone != zonelist_zone(ac.preferred_zoneref) &&\n \t\t    zone_to_nid(zone) != zonelist_node_idx(ac.preferred_zoneref)) {\ndiff --git a/mm/slub.c b/mm/slub.c\nindex 861592ac5425..e4bd6ede81d1 100644\n--- a/mm/slub.c\n+++ b/mm/slub.c\n@@ -3595,7 +3595,8 @@ static struct slab *get_any_partial(struct kmem_cache *s,\n \n \t\t\tn = get_node(s, zone_to_nid(zone));\n \n-\t\t\tif (n && cpuset_zone_allowed(zone, pc->flags) &&\n+\t\t\tif (n && numa_zone_alloc_allowed(ALLOC_CPUSET, zone,\n+\t\t\t\t\t\t   pc->flags) &&\n \t\t\t\t\tn->nr_partial > s->min_partial) {\n \t\t\t\tslab = get_partial_node(s, n, pc);\n \t\t\t\tif (slab) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about N_MEMORY fallback lists including N_MEMORY_PRIVATE nodes, explaining that this would allow allocations from private nodes in some scenarios and cause unnecessary iterations over ineligible nodes. The author provided a patch to fix the issue by adding private nodes as fallbacks for kernel allocations on behalf of the private node.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY fallback lists should not include N_MEMORY_PRIVATE nodes, at\nworst this would allow allocation from them in some scenarios, and at\nbest it causes iterations over nodes that aren't eligible.\n\nPrivate node primary fallback lists do include N_MEMORY nodes so\nkernel/slab allocations made on behalf of the private node can\nfall back to DRAM when __GFP_PRIVATE is not set.\n\nThe nofallback list contains only the node's own zones, restricting\n__GFP_THISNODE allocations to the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/page_alloc.c | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 47f2619d3840..5a1b35421d78 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5683,6 +5683,26 @@ static void build_zonelists(pg_data_t *pgdat)\n \tlocal_node = pgdat->node_id;\n \tprev_node = local_node;\n \n+\t/*\n+\t * Private nodes need N_MEMORY nodes as fallback for kernel allocations\n+\t * (e.g., slab objects allocated on behalf of this node).\n+\t */\n+\tif (node_state(local_node, N_MEMORY_PRIVATE)) {\n+\t\tnode_order[nr_nodes++] = local_node;\n+\t\tnode_set(local_node, used_mask);\n+\n+\t\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0)\n+\t\t\tnode_order[nr_nodes++] = node;\n+\n+\t\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n+\t\tbuild_thisnode_zonelists(pgdat);\n+\t\tpr_info(\"Fallback order for Node %d (private):\", local_node);\n+\t\tfor (node = 0; node < nr_nodes; node++)\n+\t\t\tpr_cont(\" %d\", node_order[node]);\n+\t\tpr_cont(\"\\n\");\n+\t\treturn;\n+\t}\n+\n \tmemset(node_order, 0, sizeof(node_order));\n \twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author addressed a concern about the need for a unified predicate to exclude both N_MEMORY_PRIVATE and ZONE_DEVICE folios from MM operations, and provided a patch that adds the folio_is_private_managed() function to achieve this.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix is needed",
                "provided a patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Multiple mm/ subsystems already skip operations for ZONE_DEVICE folios,\nand N_MEMORY_PRIVATE folios share the checkpoints for ZONE_DEVICE pages.\n\nAdd folio_is_private_managed() as a unified predicate that returns true\nfor folios on N_MEMORY_PRIVATE nodes or in ZONE_DEVICE.\n\nThis predicate replaces folio_is_zone_device at skip sites where both\nfolio types should be excluded from an MM operation.\n\nAt some locations, explicit zone_device vs private_node checks are more\nappropriate when the operations between the two fundamentally differ.\n\nThe !CONFIG_NUMA stubs fall through to folio_is_zone_device() only,\npreserving existing behavior when NUMA is disabled.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 6a70ec39d569..7687a4cf990c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -92,6 +92,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio) || folio_is_private_node(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n@@ -146,6 +156,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn false;\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about mlocking private node folios, explaining that they should not be locked and citing the existing folio_is_zone_device check as sufficient to handle this case. The author extended this check to include private nodes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "addressed_concern",
                "explained_reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nmlocked.  The existing folio_is_zone_device check is already correctly\nplaced to handle this - simply extend it for private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/mlock.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/mlock.c b/mm/mlock.c\nindex 2f699c3497a5..c56159253e45 100644\n--- a/mm/mlock.c\n+++ b/mm/mlock.c\n@@ -25,6 +25,7 @@\n #include <linux/memcontrol.h>\n #include <linux/mm_inline.h>\n #include <linux/secretmem.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -366,7 +367,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (is_huge_zero_pmd(*pmd))\n \t\t\tgoto out;\n \t\tfolio = pmd_folio(*pmd);\n-\t\tif (folio_is_zone_device(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)))\n \t\t\tgoto out;\n \t\tif (vma->vm_flags & VM_LOCKED)\n \t\t\tmlock_folio(folio);\n@@ -386,7 +387,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (!pte_present(ptent))\n \t\t\tcontinue;\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\tstep = folio_mlock_step(folio, pte, addr, end);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author acknowledged a concern that madvise and pageout operations should not interfere with device driver-managed private node folios, agreed to extend the zone_device check to cover private nodes, and made corresponding changes to mm/madvise.c.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "agreed to make changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nsubjectto madvise cold/pageout/free operations that would interfere\nwith the driver's memory management.\n\nExtend the existing zone_device check to cover private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/madvise.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/madvise.c b/mm/madvise.c\nindex b617b1be0f53..3aac105e840b 100644\n--- a/mm/madvise.c\n+++ b/mm/madvise.c\n@@ -32,6 +32,7 @@\n #include <linux/leafops.h>\n #include <linux/shmem_fs.h>\n #include <linux/mmu_notifier.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlb.h>\n \n@@ -475,7 +476,7 @@ static int madvise_cold_or_pageout_pte_range(pmd_t *pmd,\n \t\t\tcontinue;\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n@@ -704,7 +705,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,\n \t\t}\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private node folios participating in KSM merging by default, agreeing that this can interfere with driver operations. The author extended existing checks to exclude private node folios from KSM merging.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not participate in KSM merging by default.\nThe driver manages the memory lifecycle and KSM's page sharing can\ninterfere with driver operations.\n\nExtend the existing zone_device checks in get_mergeable_page and\nksm_next_page_pmd_entry to cover private node folios as well.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/ksm.c | 9 ++++++---\n 1 file changed, 6 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/ksm.c b/mm/ksm.c\nindex 2d89a7c8b4eb..c48e95a6fff9 100644\n--- a/mm/ksm.c\n+++ b/mm/ksm.c\n@@ -40,6 +40,7 @@\n #include <linux/oom.h>\n #include <linux/numa.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include \"internal.h\"\n@@ -808,7 +809,7 @@ static struct page *get_mergeable_page(struct ksm_rmap_item *rmap_item)\n \n \tfolio = folio_walk_start(&fw, vma, addr, 0);\n \tif (folio) {\n-\t\tif (!folio_is_zone_device(folio) &&\n+\t\tif (!folio_is_private_managed(folio) &&\n \t\t    folio_test_anon(folio)) {\n \t\t\tfolio_get(folio);\n \t\t\tpage = fw.page;\n@@ -2521,7 +2522,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\t\tgoto not_found_unlock;\n \t\t\tfolio = page_folio(page);\n \n-\t\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t\t    !folio_test_anon(folio))\n \t\t\t\tgoto not_found_unlock;\n \n \t\t\tpage += ((addr & (PMD_SIZE - 1)) >> PAGE_SHIFT);\n@@ -2545,7 +2547,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\tcontinue;\n \t\tfolio = page_folio(page);\n \n-\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t    !folio_test_anon(folio))\n \t\t\tcontinue;\n \t\tgoto found_unlock;\n \t}\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about collapse operations on private nodes potentially promoting pages to local nodes and inverting LRU order, agreeing that handling this like zone_device is the best approach for now.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "A collapse operation allocates a new large folio and migrates the\nsmaller folios into it.  This is an issue for private nodes:\n\n  1. The private node service may not support migration\n  2. Collapse may promotes pages from the private node to a local node,\n     which may result in an LRU inversion that defeats memory tiering.\n\nHandle this just like zone_device for now.\n\nIt may be possible to support this later for some private node services\nthat report explicit support for collapse (and migration).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/khugepaged.c | 7 ++++---\n 1 file changed, 4 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/khugepaged.c b/mm/khugepaged.c\nindex 97d1b2824386..36f6bc5da53c 100644\n--- a/mm/khugepaged.c\n+++ b/mm/khugepaged.c\n@@ -21,6 +21,7 @@\n #include <linux/shmem_fs.h>\n #include <linux/dax.h>\n #include <linux/ksm.h>\n+#include <linux/node_private.h>\n #include <linux/pgalloc.h>\n \n #include <asm/tlb.h>\n@@ -571,7 +572,7 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,\n \t\t\tgoto out;\n \t\t}\n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out;\n \t\t}\n@@ -1323,7 +1324,7 @@ static int hpage_collapse_scan_pmd(struct mm_struct *mm,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out_unmap;\n \t\t}\n@@ -1575,7 +1576,7 @@ int collapse_pte_mapped_thp(struct mm_struct *mm, unsigned long addr,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, ptent);\n-\t\tif (WARN_ON_ONCE(page && is_zone_device_page(page)))\n+\t\tif (WARN_ON_ONCE(page && page_is_private_managed(page)))\n \t\t\tpage = NULL;\n \t\t/*\n \t\t * Note that uprobe, debugger, or MAP_PRIVATE may change the\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about cleanup when a folio's refcount drops to zero, explaining that the service may need to perform cleanup before the page returns to the buddy allocator. They added a new function `folio_managed_on_free()` to wrap both zone_device and private node semantics for this operation. The function will return true if the folio is fully handled (zone_device) or false if the callback ran but the folio should continue through the normal free path (private_node).",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "added new function to address concern"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a folio's refcount drops to zero, the service may need to perform\ncleanup before the page returns to the buddy allocator (e.g. zeroing\npages to scrub stale compressed data / release compression ratio).\n\nAdd folio_managed_on_free() to wrap both zone_device and private node\nsemantics for this operation since they are the same.\n\nOne difference between zone_device and private node folios:\n  - private nodes may choose to either take a reference and return true\n    (\"handled\"), or return false to return it back to the buddy.\n\n  - zone_device returns the page to the buddy (always returns true)\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 30 ++++++++++++++++++++++++++++++\n mm/swap.c                    | 21 ++++++++++-----------\n 3 files changed, 46 insertions(+), 11 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7687a4cf990c..09ea7c4cb13c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -39,10 +39,16 @@ struct vm_fault;\n  *   callback to prevent node_private from being freed.\n  *   These callbacks MUST NOT sleep.\n  *\n+ * @free_folio: Called when a folio refcount drops to 0\n+ *   [folio-referenced callback]\n+ *   Returns: true if handled (skip return to buddy)\n+ *            false if no op (return to buddy)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n+\tbool (*free_folio)(struct folio *folio);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 97023748e6a9..658da41cdb8e 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1412,6 +1412,36 @@ int numa_migrate_check(struct folio *folio, struct vm_fault *vmf,\n void free_zone_device_folio(struct folio *folio);\n int migrate_device_coherent_folio(struct folio *folio);\n \n+/**\n+ * folio_managed_on_free - Notify managed-memory service that folio\n+ *                         refcount reached zero.\n+ * @folio: the folio being freed\n+ *\n+ * Returns true if the folio is fully handled (zone_device -- caller\n+ * must return immediately).  Returns false if the callback ran but\n+ * the folio should continue through the normal free path\n+ * (private_node -- pages go back to buddy).\n+ *\n+ * Returns false for normal folios (no-op).\n+ */\n+static inline bool folio_managed_on_free(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio)) {\n+\t\tfree_zone_device_folio(folio);\n+\t\treturn true;\n+\t}\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->free_folio) {\n+\t\t\tif (ops->free_folio(folio))\n+\t\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/swap.c b/mm/swap.c\nindex 2260dcd2775e..dca306e1ae6d 100644\n--- a/mm/swap.c\n+++ b/mm/swap.c\n@@ -37,6 +37,7 @@\n #include <linux/page_idle.h>\n #include <linux/local_lock.h>\n #include <linux/buffer_head.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -96,10 +97,9 @@ static void page_cache_release(struct folio *folio)\n \n void __folio_put(struct folio *folio)\n {\n-\tif (unlikely(folio_is_zone_device(folio))) {\n-\t\tfree_zone_device_folio(folio);\n-\t\treturn;\n-\t}\n+\tif (unlikely(folio_is_private_managed(folio)))\n+\t\tif (folio_managed_on_free(folio))\n+\t\t\treturn;\n \n \tif (folio_test_hugetlb(folio)) {\n \t\tfree_huge_folio(folio);\n@@ -961,19 +961,18 @@ void folios_put_refs(struct folio_batch *folios, unsigned int *refs)\n \t\tif (is_huge_zero_folio(folio))\n \t\t\tcontinue;\n \n-\t\tif (folio_is_zone_device(folio)) {\n+\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n+\t\t\tcontinue;\n+\n+\t\tif (unlikely(folio_is_private_managed(folio))) {\n \t\t\tif (lruvec) {\n \t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n \t\t\t\tlruvec = NULL;\n \t\t\t}\n-\t\t\tif (folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\t\tfree_zone_device_folio(folio);\n-\t\t\tcontinue;\n+\t\t\tif (folio_managed_on_free(folio))\n+\t\t\t\tcontinue;\n \t\t}\n \n-\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\tcontinue;\n-\n \t\t/* hugetlb has its own memcg */\n \t\tif (folio_test_hugetlb(folio)) {\n \t\t\tif (lruvec) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about notifying private node services when a THP folio is split by adding an optional callback to the ops struct and updating the folio_split path in huge_memory.c. The author confirmed that this change will be included in the next version of the patch series.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "confirmed a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private node services may need to update internal metadata when\na THP folio is split.  ZONE_DEVICE already has a split callback via\npgmap->ops; private nodes can provide the same capability.\n\nJust like zone_device, some private node services may want to know\nabout a folio being split.  Add this optional callback to the ops\nstruct and add a wrapper for zone_device and private node callback\ndispatch to be consolidated.\n\nWire this into __folio_split() where the zone_device check was made.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 33 +++++++++++++++++++++++++++++++++\n mm/huge_memory.c             |  6 ++++--\n 2 files changed, 37 insertions(+), 2 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 09ea7c4cb13c..f9dd2d25c8a5 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -3,6 +3,7 @@\n #define _LINUX_NODE_PRIVATE_H\n \n #include <linux/completion.h>\n+#include <linux/memremap.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -44,11 +45,19 @@ struct vm_fault;\n  *   Returns: true if handled (skip return to buddy)\n  *            false if no op (return to buddy)\n  *\n+ * @folio_split: Notification that a folio on this private node is being split.\n+ *    [folio-referenced callback]\n+ *     Called from the folio split path via folio_managed_split_cb().\n+ *     @folio is the original folio; @new_folio is the newly created folio,\n+ *     or NULL when called for the final (original) folio after all sub-folios\n+ *     have been split off.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n+\tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n \tunsigned long flags;\n };\n \n@@ -150,6 +159,24 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn node_private_flags(zone_to_nid(z)) & flag;\n }\n \n+static inline void node_private_split_cb(struct folio *folio,\n+\t\t\t\t\t struct folio *new_folio)\n+{\n+\tconst struct node_private_ops *ops = folio_node_private_ops(folio);\n+\n+\tif (ops && ops->folio_split)\n+\t\tops->folio_split(folio, new_folio);\n+}\n+\n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+\telse if (folio_is_private_node(original_folio))\n+\t\tnode_private_split_cb(original_folio, new_folio);\n+}\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -198,6 +225,12 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn false;\n }\n \n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+}\n #endif /* CONFIG_NUMA */\n \n #if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 40cf59301c21..2ecae494291a 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -24,6 +24,7 @@\n #include <linux/freezer.h>\n #include <linux/mman.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/debugfs.h>\n #include <linux/migrate.h>\n@@ -3850,7 +3851,7 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \n \t\t\tnext = folio_next(new_folio);\n \n-\t\t\tzone_device_private_split_cb(folio, new_folio);\n+\t\t\tfolio_managed_split_cb(folio, new_folio);\n \n \t\t\tfolio_ref_unfreeze(new_folio,\n \t\t\t\t\t   folio_cache_ref_count(new_folio) + 1);\n@@ -3889,7 +3890,8 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \t\t\tfolio_put_refs(new_folio, nr_pages);\n \t\t}\n \n-\t\tzone_device_private_split_cb(folio, NULL);\n+\t\tfolio_managed_split_cb(folio, NULL);\n+\n \t\t/*\n \t\t * Unfreeze @folio only after all page cache entries, which\n \t\t * used to point to it, have been updated with new folios.\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about user-driven migration to private nodes, explaining that ZONE_DEVICE always rejects it but private nodes should be able to opt in. They added the NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper to dispatch migration requests, allowing migrate_pages syscall to target private nodes.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "added new functionality",
                "acknowledged reviewer feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services may want to support user-driven migration\n(migrate_pages syscall, mbind) to allow data movement between regular\nand private nodes.\n\nZONE_DEVICE always rejects user migration, but private nodes should\nbe able to opt in.\n\nAdd NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper that\ndispatches migration requests.  Private nodes can either set the flag\nand provide a custom migrate_to callback for driver-managed migration.\n\nIn migrate_to_node(), allows GFP_PRIVATE when the destination node\nsupports NP_OPS_MIGRATION, enabling migrate_pages syscall to target\nprivate nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |   4 ++\n include/linux/migrate.h      |  10 +++\n include/linux/node_private.h | 122 +++++++++++++++++++++++++++++++++++\n mm/damon/paddr.c             |   3 +\n mm/internal.h                |  24 +++++++\n mm/mempolicy.c               |  10 +--\n mm/migrate.c                 |  49 ++++++++++----\n mm/rmap.c                    |   4 +-\n 8 files changed, 206 insertions(+), 20 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 646dc48a23b5..e587f5781135 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -949,6 +949,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \tif (!node_possible(nid))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MIGRATION) &&\n+\t    (!ops->migrate_to || !ops->folio_migrate))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 26ca00c325d9..7b2da3875ff2 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -71,6 +71,9 @@ void folio_migrate_flags(struct folio *newfolio, struct folio *folio);\n int folio_migrate_mapping(struct address_space *mapping,\n \t\tstruct folio *newfolio, struct folio *folio, int extra_count);\n int set_movable_ops(const struct movable_operations *ops, enum pagetype type);\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason);\n \n #else\n \n@@ -96,6 +99,13 @@ static inline int set_movable_ops(const struct movable_operations *ops, enum pag\n {\n \treturn -ENOSYS;\n }\n+static inline int migrate_folios_to_node(struct list_head *folios,\n+\t\t\t\t\t\t  int nid,\n+\t\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t\t  enum migrate_reason reason)\n+{\n+\treturn -ENOSYS;\n+}\n \n #endif /* CONFIG_MIGRATION */\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex f9dd2d25c8a5..0c5be1ee6e60 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -4,6 +4,7 @@\n \n #include <linux/completion.h>\n #include <linux/memremap.h>\n+#include <linux/migrate_mode.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -52,15 +53,40 @@ struct vm_fault;\n  *     or NULL when called for the final (original) folio after all sub-folios\n  *     have been split off.\n  *\n+ * @migrate_to: Migrate folios TO this node.\n+ *\t[refcounted callback]\n+ *\tReturns: 0 on full success, >0 = number of folios that failed to\n+ *\t\t migrate, <0 = error.  Matches migrate_pages() semantics.\n+ *\t\t @nr_succeeded is set to the number of successfully migrated\n+ *\t\t folios (may be NULL if caller doesn't need it).\n+ *\n+ * @folio_migrate: Post-migration notification that a folio on this private node\n+ *    changed physical location (on the same node or a different node).\n+ *    [folio-referenced callback]\n+ *     Called from migrate_folio_move() after data has been copied but before\n+ *     migration entries are replaced with real PTEs.  Both @src and @dst are\n+ *     locked.  Faults block in migration_entry_wait() until\n+ *     remove_migration_ptes() runs, so the service can safely update\n+ *     PFN-based metadata (compression tables, device page tables, DMA\n+ *     mappings, etc.) before any access through the page tables.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n \tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n+\tint (*migrate_to)(struct list_head *folios, int nid,\n+\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t  unsigned int *nr_succeeded);\n+\tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tunsigned long flags;\n };\n \n+/* Allow user/kernel migration; requires migrate_to and folio_migrate */\n+#define NP_OPS_MIGRATION\t\tBIT(0)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\n@@ -177,6 +203,81 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n \t\tnode_private_split_cb(original_folio, new_folio);\n }\n \n+#ifdef CONFIG_MEMORY_HOTPLUG\n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn -ENOENT;\n+\treturn node_private_has_flag(folio_nid(folio), NP_OPS_MIGRATION) ?\n+\t       folio_nid(folio) : -ENOENT;\n+}\n+\n+/**\n+ * folio_managed_allows_migrate - Check if a managed folio supports migration\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio can be migrated.  For zone_device folios, only\n+ * device_private and device_coherent support migration.  For private node\n+ * folios, migration requires NP_OPS_MIGRATION.  Normal folios always\n+ * return true.\n+ */\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\tif (folio_is_private_node(folio))\n+\t\treturn folio_private_flags(folio, NP_OPS_MIGRATION);\n+\treturn true;\n+}\n+\n+/**\n+ * node_private_migrate_to - Attempt service-specific migration to a private node\n+ * @folios: list of folios to migrate (may sleep)\n+ * @nid: target node\n+ * @mode: migration mode (MIGRATE_ASYNC, MIGRATE_SYNC, etc.)\n+ * @reason: migration reason (MR_DEMOTION, MR_SYSCALL, etc.)\n+ * @nr_succeeded: optional output for number of successfully migrated folios\n+ *\n+ * If @nid is an N_MEMORY_PRIVATE node with a migrate_to callback,\n+ * invokes the callback and returns the result with migrate_pages()\n+ * semantics (0 = full success, >0 = failure count, <0 = error).\n+ * Returns -ENODEV if the node is not private or the service is being\n+ * torn down.\n+ *\n+ * The source folios are on other nodes, so they do not pin the target\n+ * node's node_private.  A temporary refcount is taken under rcu_read_lock\n+ * to keep node_private (and the service module) alive across the callback.\n+ */\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\tint (*fn)(struct list_head *, int, enum migrate_mode,\n+\t\t  enum migrate_reason, unsigned int *);\n+\tstruct node_private *np;\n+\tint ret;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (!np || !np->ops || !np->ops->migrate_to ||\n+\t    !refcount_inc_not_zero(&np->refcount)) {\n+\t\trcu_read_unlock();\n+\t\treturn -ENODEV;\n+\t}\n+\tfn = np->ops->migrate_to;\n+\trcu_read_unlock();\n+\n+\tret = fn(folios, nid, mode, reason, nr_succeeded);\n+\n+\tif (refcount_dec_and_test(&np->refcount))\n+\t\tcomplete(&np->released);\n+\n+\treturn ret;\n+}\n+#endif /* CONFIG_MEMORY_HOTPLUG */\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -242,6 +343,27 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\treturn -ENOENT;\n+}\n+\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\treturn true;\n+}\n+\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\treturn -ENODEV;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/damon/paddr.c b/mm/damon/paddr.c\nindex 07a8aead439e..532b8e2c62b0 100644\n--- a/mm/damon/paddr.c\n+++ b/mm/damon/paddr.c\n@@ -277,6 +277,9 @@ static unsigned long damon_pa_migrate(struct damon_region *r,\n \t\telse\n \t\t\t*sz_filter_passed += folio_size(folio) / addr_unit;\n \n+\t\tif (!folio_managed_allows_migrate(folio))\n+\t\t\tgoto put_folio;\n+\n \t\tif (!folio_isolate_lru(folio))\n \t\t\tgoto put_folio;\n \t\tlist_add(&folio->lru, &folio_list);\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 658da41cdb8e..6ab4679fe943 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1442,6 +1442,30 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/**\n+ * folio_managed_migrate_notify - Notify service that a folio changed location\n+ * @src: the old folio (about to be freed)\n+ * @dst: the new folio (data already copied, migration entries still in place)\n+ *\n+ * Called from migrate_folio_move() after data has been copied but before\n+ * remove_migration_ptes() installs real PTEs pointing to @dst.  While\n+ * migration entries are in place, faults block in migration_entry_wait(),\n+ * so the service can safely update PFN-based metadata before any access\n+ * through the page tables.  Both @src and @dst are locked.\n+ */\n+static inline void folio_managed_migrate_notify(struct folio *src,\n+\t\t\t\t\t\tstruct folio *dst)\n+{\n+\tconst struct node_private_ops *ops;\n+\n+\tif (!folio_is_private_node(src))\n+\t\treturn;\n+\n+\tops = folio_node_private_ops(src);\n+\tif (ops && ops->folio_migrate)\n+\t\tops->folio_migrate(src, dst);\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 68a98ba57882..2b0f9762d171 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -111,6 +111,7 @@\n #include <linux/mmu_notifier.h>\n #include <linux/printk.h>\n #include <linux/leafops.h>\n+#include <linux/node_private.h>\n #include <linux/gcd.h>\n \n #include <asm/tlbflush.h>\n@@ -1282,11 +1283,6 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tLIST_HEAD(pagelist);\n \tlong nr_failed;\n \tlong err = 0;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = dest,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n \tnodes_clear(nmask);\n \tnode_set(source, nmask);\n@@ -1311,8 +1307,8 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tmmap_read_unlock(mm);\n \n \tif (!list_empty(&pagelist)) {\n-\t\terr = migrate_pages(&pagelist, alloc_migration_target, NULL,\n-\t\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\t\terr = migrate_folios_to_node(&pagelist, dest, MIGRATE_SYNC,\n+\t\t\t\t\t     MR_SYSCALL);\n \t\tif (err)\n \t\t\tputback_movable_pages(&pagelist);\n \t}\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 5169f9717f60..a54d4af04df3 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -43,6 +43,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/memory-tiers.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1387,6 +1388,8 @@ static int migrate_folio_move(free_folio_t put_new_folio, unsigned long private,\n \tif (old_page_state & PAGE_WAS_MLOCKED)\n \t\tlru_add_drain();\n \n+\tfolio_managed_migrate_notify(src, dst);\n+\n \tif (old_page_state & PAGE_WAS_MAPPED)\n \t\tremove_migration_ptes(src, dst, 0);\n \n@@ -2165,6 +2168,7 @@ int migrate_pages(struct list_head *from, new_folio_t get_new_folio,\n \n \treturn rc_gather;\n }\n+EXPORT_SYMBOL_GPL(migrate_pages);\n \n struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n {\n@@ -2204,6 +2208,31 @@ struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n \n \treturn __folio_alloc(gfp_mask, order, nid, mtc->nmask);\n }\n+EXPORT_SYMBOL_GPL(alloc_migration_target);\n+\n+static int __migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, NULL);\n+}\n+\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason)\n+{\n+\tif (node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_private_migrate_to(folios, nid, mode,\n+\t\t\t\t\t       reason, NULL);\n+\treturn __migrate_folios_to_node(folios, nid, mode, reason);\n+}\n \n #ifdef CONFIG_NUMA\n \n@@ -2221,14 +2250,8 @@ static int store_status(int __user *status, int start, int value, int nr)\n static int do_move_pages_to_node(struct list_head *pagelist, int node)\n {\n \tint err;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = node,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n-\terr = migrate_pages(pagelist, alloc_migration_target, NULL,\n-\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\terr = migrate_folios_to_node(pagelist, node, MIGRATE_SYNC, MR_SYSCALL);\n \tif (err)\n \t\tputback_movable_pages(pagelist);\n \treturn err;\n@@ -2240,7 +2263,7 @@ static int __add_folio_for_migration(struct folio *folio, int node,\n \tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\treturn -EFAULT;\n \n-\tif (folio_is_zone_device(folio))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn -ENOENT;\n \n \tif (folio_nid(folio) == node)\n@@ -2364,7 +2387,8 @@ static int do_pages_move(struct mm_struct *mm, nodemask_t task_nodes,\n \t\terr = -ENODEV;\n \t\tif (node < 0 || node >= MAX_NUMNODES)\n \t\t\tgoto out_flush;\n-\t\tif (!node_state(node, N_MEMORY))\n+\t\tif (!node_state(node, N_MEMORY) &&\n+\t\t    !node_state(node, N_MEMORY_PRIVATE))\n \t\t\tgoto out_flush;\n \n \t\terr = -EACCES;\n@@ -2449,8 +2473,8 @@ static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n \t\tif (folio) {\n \t\t\tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\t\t\terr = -EFAULT;\n-\t\t\telse if (folio_is_zone_device(folio))\n-\t\t\t\terr = -ENOENT;\n+\t\t\telse if (unlikely(folio_is_private_managed(folio)))\n+\t\t\t\terr = folio_managed_allows_user_migrate(folio);\n \t\t\telse\n \t\t\t\terr = folio_nid(folio);\n \t\t\tfolio_walk_end(&fw, vma);\n@@ -2660,6 +2684,9 @@ int migrate_misplaced_folio_prepare(struct folio *folio,\n \tint nr_pages = folio_nr_pages(folio);\n \tpg_data_t *pgdat = NODE_DATA(node);\n \n+\tif (!folio_managed_allows_migrate(folio))\n+\t\treturn -ENOENT;\n+\n \tif (folio_is_file_lru(folio)) {\n \t\t/*\n \t\t * Do not migrate file folios that are mapped in multiple\ndiff --git a/mm/rmap.c b/mm/rmap.c\nindex f955f02d570e..805f9ceb82f3 100644\n--- a/mm/rmap.c\n+++ b/mm/rmap.c\n@@ -72,6 +72,7 @@\n #include <linux/backing-dev.h>\n #include <linux/page_idle.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/userfaultfd_k.h>\n #include <linux/mm_inline.h>\n #include <linux/oom.h>\n@@ -2616,8 +2617,7 @@ void try_to_migrate(struct folio *folio, enum ttu_flags flags)\n \t\t\t\t\tTTU_SYNC | TTU_BATCH_FLUSH)))\n \t\treturn;\n \n-\tif (folio_is_zone_device(folio) &&\n-\t    (!folio_is_device_private(folio) && !folio_is_device_coherent(folio)))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn;\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about allowing userland to directly allocate from private nodes via set_mempolicy() and mbind(), but not wanting those nodes as normal allocable system memory in the fallback lists. The author added a flag NP_OPS_MEMPOLICY requiring NP_OPS_MIGRATION, updated sysfs 'has_memory' attribute, and modified mempolicy migration sites to use __GFP_PRIVATE.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private nodes want userland to directly allocate from the node\nvia set_mempolicy() and mbind() - but don't want that node as normal\nallocable system memory in the fallback lists.\n\nAdd NP_OPS_MEMPOLICY flag requiring NP_OPS_MIGRATION (since mbind can\ndrive migrations).  Only allow private nodes in policy nodemasks if\nall private nodes in the mask support NP_OPS_MEMPOLICY. This prevents\n__GFP_PRIVATE from unlocking nodes without NP_OPS_MEMPOLICY support.\n\nAdd __GFP_PRIVATE to mempolicy migration sites so moves to opted-in\nprivate nodes succeed.\n\nUpdate the sysfs \"has_memory\" attribute to include N_MEMORY_PRIVATE\nnodes with NP_OPS_MEMPOLICY set, allowing existing numactl userland\ntools to work without modification.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c            | 22 +++++++++++++-\n include/linux/node_private.h   | 40 +++++++++++++++++++++++++\n include/uapi/linux/mempolicy.h |  1 +\n mm/mempolicy.c                 | 54 ++++++++++++++++++++++++++++++----\n mm/page_alloc.c                |  5 ++++\n 5 files changed, 116 insertions(+), 6 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex e587f5781135..c08b5a948779 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -953,6 +953,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (!ops->migrate_to || !ops->folio_migrate))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\n@@ -1145,6 +1149,21 @@ static ssize_t show_node_state(struct device *dev,\n \t\t\t  nodemask_pr_args(&node_states[na->state]));\n }\n \n+/* has_memory includes N_MEMORY + N_MEMORY_PRIVATE that support mempolicy. */\n+static ssize_t show_has_memory(struct device *dev,\n+\t\t\t       struct device_attribute *attr, char *buf)\n+{\n+\tnodemask_t mask = node_states[N_MEMORY];\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_set(nid, mask);\n+\t}\n+\n+\treturn sysfs_emit(buf, \"%*pbl\\n\", nodemask_pr_args(&mask));\n+}\n+\n #define _NODE_ATTR(name, state) \\\n \t{ __ATTR(name, 0444, show_node_state, NULL), state }\n \n@@ -1155,7 +1174,8 @@ static struct node_attr node_state_attr[] = {\n #ifdef CONFIG_HIGHMEM\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n-\t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY] = { __ATTR(has_memory, 0444, show_has_memory, NULL),\n+\t\t       N_MEMORY },\n \t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 0c5be1ee6e60..e9b58afa366b 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -86,6 +86,8 @@ struct node_private_ops {\n \n /* Allow user/kernel migration; requires migrate_to and folio_migrate */\n #define NP_OPS_MIGRATION\t\tBIT(0)\n+/* Allow mempolicy-directed allocation and mbind migration to this node */\n+#define NP_OPS_MEMPOLICY\t\tBIT(1)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -276,6 +278,34 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \n \treturn ret;\n }\n+\n+static inline bool node_mpol_eligible(int nid)\n+{\n+\tbool ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_state(nid, N_MEMORY);\n+\n+\trcu_read_lock();\n+\tret = node_private_has_flag(nid, NP_OPS_MEMPOLICY);\n+\trcu_read_unlock();\n+\treturn ret;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\tint nid;\n+\tbool eligible = false;\n+\n+\tfor_each_node_mask(nid, *nodes) {\n+\t\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\t\tcontinue;\n+\t\tif (!node_mpol_eligible(nid))\n+\t\t\treturn false;\n+\t\teligible = true;\n+\t}\n+\treturn eligible;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -364,6 +394,16 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \treturn -ENODEV;\n }\n \n+static inline bool node_mpol_eligible(int nid)\n+{\n+\treturn false;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/include/uapi/linux/mempolicy.h b/include/uapi/linux/mempolicy.h\nindex 8fbbe613611a..b606eae983c8 100644\n--- a/include/uapi/linux/mempolicy.h\n+++ b/include/uapi/linux/mempolicy.h\n@@ -64,6 +64,7 @@ enum {\n #define MPOL_F_SHARED  (1 << 0)\t/* identify shared policies */\n #define MPOL_F_MOF\t(1 << 3) /* this policy wants migrate on fault */\n #define MPOL_F_MORON\t(1 << 4) /* Migrate On protnone Reference On Node */\n+#define MPOL_F_PRIVATE\t(1 << 5) /* policy targets private node; use __GFP_PRIVATE */\n \n /*\n  * Enabling zone reclaim means the page allocator will attempt to fulfill\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 2b0f9762d171..8ac014950e88 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -406,8 +406,6 @@ static int mpol_new_preferred(struct mempolicy *pol, const nodemask_t *nodes)\n static int mpol_set_nodemask(struct mempolicy *pol,\n \t\t     const nodemask_t *nodes, struct nodemask_scratch *nsc)\n {\n-\tint ret;\n-\n \t/*\n \t * Default (pol==NULL) resp. local memory policies are not a\n \t * subject of any remapping. They also do not need any special\n@@ -416,9 +414,12 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \tif (!pol || pol->mode == MPOL_LOCAL)\n \t\treturn 0;\n \n-\t/* Check N_MEMORY */\n+\t/* Check N_MEMORY and N_MEMORY_PRIVATE*/\n \tnodes_and(nsc->mask1,\n \t\t  cpuset_current_mems_allowed, node_states[N_MEMORY]);\n+\tnodes_and(nsc->mask2, cpuset_current_mems_allowed,\n+\t\t  node_states[N_MEMORY_PRIVATE]);\n+\tnodes_or(nsc->mask1, nsc->mask1, nsc->mask2);\n \n \tVM_BUG_ON(!nodes);\n \n@@ -432,8 +433,13 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \telse\n \t\tpol->w.cpuset_mems_allowed = cpuset_current_mems_allowed;\n \n-\tret = mpol_ops[pol->mode].create(pol, &nsc->mask2);\n-\treturn ret;\n+\t/* All private nodes in the mask must have NP_OPS_MEMPOLICY. */\n+\tif (nodes_private_mpol_allowed(&nsc->mask2))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse if (nodes_intersects(nsc->mask2, node_states[N_MEMORY_PRIVATE]))\n+\t\treturn -EINVAL;\n+\n+\treturn mpol_ops[pol->mode].create(pol, &nsc->mask2);\n }\n \n /*\n@@ -500,6 +506,7 @@ static void mpol_rebind_default(struct mempolicy *pol, const nodemask_t *nodes)\n static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n {\n \tnodemask_t tmp;\n+\tint nid;\n \n \tif (pol->flags & MPOL_F_STATIC_NODES)\n \t\tnodes_and(tmp, pol->w.user_nodemask, *nodes);\n@@ -514,6 +521,21 @@ static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n \tif (nodes_empty(tmp))\n \t\ttmp = *nodes;\n \n+\t/*\n+\t * Drop private nodes that don't have mempolicy support.\n+\t * cpusets guarantees at least one N_MEMORY node in effective_mems\n+\t * and mems_allowed, so dropping private nodes here is safe.\n+\t */\n+\tfor_each_node_mask(nid, tmp) {\n+\t\tif (node_state(nid, N_MEMORY_PRIVATE) &&\n+\t\t    !node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_clear(nid, tmp);\n+\t}\n+\tif (nodes_intersects(tmp, node_states[N_MEMORY_PRIVATE]))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse\n+\t\tpol->flags &= ~MPOL_F_PRIVATE;\n+\n \tpol->nodes = tmp;\n }\n \n@@ -661,6 +683,9 @@ static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)\n \t}\n \tif (!queue_folio_required(folio, qp))\n \t\treturn;\n+\tif (folio_is_private_node(folio) &&\n+\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\treturn;\n \tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||\n \t    !vma_migratable(walk->vma) ||\n \t    !migrate_folio_add(folio, qp->pagelist, qp->flags))\n@@ -717,6 +742,9 @@ static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n \t\tif (!folio || folio_is_zone_device(folio))\n \t\t\tcontinue;\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\t\tcontinue;\n \t\tif (folio_test_large(folio) && max_nr != 1)\n \t\t\tnr = folio_pte_batch(folio, pte, ptent, max_nr);\n \t\t/*\n@@ -1451,6 +1479,9 @@ static struct folio *alloc_migration_target_by_mpol(struct folio *src,\n \telse\n \t\tgfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL | __GFP_COMP;\n \n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \treturn folio_alloc_mpol(gfp, order, pol, ilx, nid);\n }\n #else\n@@ -2280,6 +2311,15 @@ static nodemask_t *policy_nodemask(gfp_t gfp, struct mempolicy *pol,\n \t\t\tnodemask = &pol->nodes;\n \t\tif (pol->home_node != NUMA_NO_NODE)\n \t\t\t*nid = pol->home_node;\n+\t\telse if ((pol->flags & MPOL_F_PRIVATE) &&\n+\t\t\t !node_isset(*nid, pol->nodes)) {\n+\t\t\t/*\n+\t\t\t * Private nodes are not in N_MEMORY nodes' zonelists.\n+\t\t\t * When the preferred nid (usually numa_node_id()) can't\n+\t\t\t * reach the policy nodes, start from a policy node.\n+\t\t\t */\n+\t\t\t*nid = first_node(pol->nodes);\n+\t\t}\n \t\t/*\n \t\t * __GFP_THISNODE shouldn't even be used with the bind policy\n \t\t * because we might easily break the expectation to stay on the\n@@ -2533,6 +2573,10 @@ struct folio *vma_alloc_folio_noprof(gfp_t gfp, int order, struct vm_area_struct\n \t\tgfp |= __GFP_NOWARN;\n \n \tpol = get_vma_policy(vma, addr, order, &ilx);\n+\n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \tfolio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n \tmpol_cond_put(pol);\n \treturn folio;\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 5a1b35421d78..ec6c1f8e85d8 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3849,8 +3849,13 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\t * if another process has NUMA bindings and is causing\n \t\t * kswapd wakeups on only some nodes. Avoid accidental\n \t\t * \"node_reclaim_mode\"-like behavior in this case.\n+\t\t *\n+\t\t * Nodes without kswapd (some private nodes) are never\n+\t\t * skipped - this causes some mempolicies to silently\n+\t\t * fall back to DRAM even if the node is eligible.\n \t\t */\n \t\tif (skip_kswapd_nodes &&\n+\t\t    zone->zone_pgdat->kswapd &&\n \t\t    !waitqueue_active(&zone->zone_pgdat->kswapd_wait)) {\n \t\t\tskipped_kswapd_nodes = true;\n \t\t\tcontinue;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private nodes being used as demotion targets in the memory-tiers subsystem, agreeing that they should be added to the demotion target mask and implementing backpressure support to allow vmscan to fall back to swap. The author also acknowledged that the current demotion logic induces LRU inversions and suggested re-doing it to allow less fallback and kick kswapd instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The memory-tier subsystem needs to know which private nodes should\nappear as demotion targets.\n\nAdd NP_OPS_DEMOTION (BIT(2)):\n   Node can be added as a demotion target by memory-tiers.\n\nAdd demotion backpressure support so private nodes can reject\nnew demotions cleanly, allowing vmscan to fall back to swap.\n\nIn the demotion path, try demotion to private nodes invididually,\nthen clear private nodes from the demotion target mask until a\nnon-private node is found, then fall back to the remaining mask.\nThis prevents LRU inversion while still allowing forward progress.\n\nThis is the closest match to the current behavior without making\nprivate nodes inaccessible or preventing forward progress. We\nshould probably completely re-do the demotion logic to allow less\nfallback and kick kswapd instead - right now we induce LRU\ninversions by simply falling back to any node in the demotion list.\n\nAdd memory_tier_refresh_demotion() export for services to trigger\nre-evaluation of demotion targets after changing their flags.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory-tiers.h |  9 +++++++\n include/linux/node_private.h | 22 +++++++++++++++++\n mm/internal.h                |  7 ++++++\n mm/memory-tiers.c            | 46 ++++++++++++++++++++++++++++++++----\n mm/page_alloc.c              | 12 +++++++---\n mm/vmscan.c                  | 30 ++++++++++++++++++++++-\n 6 files changed, 117 insertions(+), 9 deletions(-)\n\ndiff --git a/include/linux/memory-tiers.h b/include/linux/memory-tiers.h\nindex 3e1159f6762c..e1476432e359 100644\n--- a/include/linux/memory-tiers.h\n+++ b/include/linux/memory-tiers.h\n@@ -58,6 +58,7 @@ struct memory_dev_type *mt_get_memory_type(int adist);\n int next_demotion_node(int node);\n void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets);\n bool node_is_toptier(int node);\n+void memory_tier_refresh_demotion(void);\n #else\n static inline int next_demotion_node(int node)\n {\n@@ -73,6 +74,10 @@ static inline bool node_is_toptier(int node)\n {\n \treturn true;\n }\n+\n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n #endif\n \n #else\n@@ -106,6 +111,10 @@ static inline bool node_is_toptier(int node)\n \treturn true;\n }\n \n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n+\n static inline int register_mt_adistance_algorithm(struct notifier_block *nb)\n {\n \treturn 0;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e9b58afa366b..e254e36056cd 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -88,6 +88,8 @@ struct node_private_ops {\n #define NP_OPS_MIGRATION\t\tBIT(0)\n /* Allow mempolicy-directed allocation and mbind migration to this node */\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n+/* Node participates as a demotion target in memory-tiers */\n+#define NP_OPS_DEMOTION\t\t\tBIT(2)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -101,12 +103,14 @@ struct node_private_ops {\n  *\t\tcallbacks that may sleep; 0 = fully released)\n  * @released: Signaled when refcount drops to 0; unregister waits on this\n  * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ * @migration_blocked: Service signals migrations should pause\n  */\n struct node_private {\n \tvoid *owner;\n \trefcount_t refcount;\n \tstruct completion released;\n \tconst struct node_private_ops *ops;\n+\tbool migration_blocked;\n };\n \n #ifdef CONFIG_NUMA\n@@ -306,6 +310,19 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \t}\n \treturn eligible;\n }\n+\n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\tstruct node_private *np;\n+\tbool blocked;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tblocked = np && READ_ONCE(np->migration_blocked);\n+\trcu_read_unlock();\n+\n+\treturn blocked;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -404,6 +421,11 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \treturn false;\n }\n \n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 6ab4679fe943..5950e20d4023 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t  const nodemask_t *candidates);\n extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t      gfp_t gfp_mask);\n #else\n@@ -1220,6 +1222,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t\t const nodemask_t *candidates)\n+{\n+\treturn NUMA_NO_NODE;\n+}\n static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t\t     gfp_t gfp_mask)\n {\ndiff --git a/mm/memory-tiers.c b/mm/memory-tiers.c\nindex 9c742e18e48f..434190fdc078 100644\n--- a/mm/memory-tiers.c\n+++ b/mm/memory-tiers.c\n@@ -3,6 +3,7 @@\n #include <linux/lockdep.h>\n #include <linux/sysfs.h>\n #include <linux/kobject.h>\n+#include <linux/node_private.h>\n #include <linux/memory.h>\n #include <linux/memory-tiers.h>\n #include <linux/notifier.h>\n@@ -380,6 +381,8 @@ static void disable_all_demotion_targets(void)\n \t\tif (memtier)\n \t\t\tmemtier->lower_tier_mask = NODE_MASK_NONE;\n \t}\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE)\n+\t\tnode_demotion[node].preferred = NODE_MASK_NONE;\n \t/*\n \t * Ensure that the \"disable\" is visible across the system.\n \t * Readers will see either a combination of before+disable\n@@ -421,6 +424,7 @@ static void establish_demotion_targets(void)\n \tint target = NUMA_NO_NODE, node;\n \tint distance, best_distance;\n \tnodemask_t tier_nodes, lower_tier;\n+\tnodemask_t all_memory;\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n@@ -429,6 +433,13 @@ static void establish_demotion_targets(void)\n \n \tdisable_all_demotion_targets();\n \n+\t/* Include private nodes that have opted in to demotion. */\n+\tall_memory = node_states[N_MEMORY];\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(node, NP_OPS_DEMOTION))\n+\t\t\tnode_set(node, all_memory);\n+\t}\n+\n \tfor_each_node_state(node, N_MEMORY) {\n \t\tbest_distance = -1;\n \t\tnd = &node_demotion[node];\n@@ -442,12 +453,12 @@ static void establish_demotion_targets(void)\n \t\tmemtier = list_next_entry(memtier, list);\n \t\ttier_nodes = get_memtier_nodemask(memtier);\n \t\t/*\n-\t\t * find_next_best_node, use 'used' nodemask as a skip list.\n+\t\t * find_next_best_node_in, use 'used' nodemask as a skip list.\n \t\t * Add all memory nodes except the selected memory tier\n \t\t * nodelist to skip list so that we find the best node from the\n \t\t * memtier nodelist.\n \t\t */\n-\t\tnodes_andnot(tier_nodes, node_states[N_MEMORY], tier_nodes);\n+\t\tnodes_andnot(tier_nodes, all_memory, tier_nodes);\n \n \t\t/*\n \t\t * Find all the nodes in the memory tier node list of same best distance.\n@@ -455,7 +466,8 @@ static void establish_demotion_targets(void)\n \t\t * in the preferred mask when allocating pages during demotion.\n \t\t */\n \t\tdo {\n-\t\t\ttarget = find_next_best_node(node, &tier_nodes);\n+\t\t\ttarget = find_next_best_node_in(node, &tier_nodes,\n+\t\t\t\t\t\t\t&all_memory);\n \t\t\tif (target == NUMA_NO_NODE)\n \t\t\t\tbreak;\n \n@@ -495,7 +507,7 @@ static void establish_demotion_targets(void)\n \t * allocation to a set of nodes that is closer the above selected\n \t * preferred node.\n \t */\n-\tlower_tier = node_states[N_MEMORY];\n+\tlower_tier = all_memory;\n \tlist_for_each_entry(memtier, &memory_tiers, list) {\n \t\t/*\n \t\t * Keep removing current tier from lower_tier nodes,\n@@ -542,7 +554,7 @@ static struct memory_tier *set_node_memory_tier(int node)\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n-\tif (!node_state(node, N_MEMORY))\n+\tif (!node_state(node, N_MEMORY) && !node_state(node, N_MEMORY_PRIVATE))\n \t\treturn ERR_PTR(-EINVAL);\n \n \tmt_calc_adistance(node, &adist);\n@@ -865,6 +877,30 @@ int mt_calc_adistance(int node, int *adist)\n }\n EXPORT_SYMBOL_GPL(mt_calc_adistance);\n \n+/**\n+ * memory_tier_refresh_demotion() - Re-establish demotion targets\n+ *\n+ * Called by services after registering or unregistering ops->migrate_to on\n+ * a private node, so that establish_demotion_targets() picks up the change.\n+ */\n+void memory_tier_refresh_demotion(void)\n+{\n+\tint nid;\n+\n+\tmutex_lock(&memory_tier_lock);\n+\t/*\n+\t * Ensure private nodes are registered with a tier, otherwise\n+\t * they won't show up in any node's demotion targets nodemask.\n+\t */\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (!__node_get_memory_tier(nid))\n+\t\t\tset_node_memory_tier(nid);\n+\t}\n+\testablish_demotion_targets();\n+\tmutex_unlock(&memory_tier_lock);\n+}\n+EXPORT_SYMBOL_GPL(memory_tier_refresh_demotion);\n+\n static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\t\t\t\t      unsigned long action, void *_arg)\n {\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex ec6c1f8e85d8..e272dfdc6b00 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5589,7 +5589,8 @@ static int node_load[MAX_NUMNODES];\n  *\n  * Return: node id of the found node or %NUMA_NO_NODE if no node is found.\n  */\n-int find_next_best_node(int node, nodemask_t *used_node_mask)\n+int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t   const nodemask_t *candidates)\n {\n \tint n, val;\n \tint min_val = INT_MAX;\n@@ -5599,12 +5600,12 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \t * Use the local node if we haven't already, but for memoryless local\n \t * node, we should skip it and fall back to other nodes.\n \t */\n-\tif (!node_isset(node, *used_node_mask) && node_state(node, N_MEMORY)) {\n+\tif (!node_isset(node, *used_node_mask) && node_isset(node, *candidates)) {\n \t\tnode_set(node, *used_node_mask);\n \t\treturn node;\n \t}\n \n-\tfor_each_node_state(n, N_MEMORY) {\n+\tfor_each_node_mask(n, *candidates) {\n \n \t\t/* Don't want a node to appear more than once */\n \t\tif (node_isset(n, *used_node_mask))\n@@ -5636,6 +5637,11 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \treturn best_node;\n }\n \n+int find_next_best_node(int node, nodemask_t *used_node_mask)\n+{\n+\treturn find_next_best_node_in(node, used_node_mask,\n+\t\t\t\t      &node_states[N_MEMORY]);\n+}\n \n /*\n  * Build zonelists ordered by node and zones within node.\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 6113be4d3519..0f534428ea88 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -355,6 +356,10 @@ static bool can_demote(int nid, struct scan_control *sc,\n \tif (demotion_nid == NUMA_NO_NODE)\n \t\treturn false;\n \n+\t/* Don't demote when the target's service signals backpressure */\n+\tif (node_private_migration_blocked(demotion_nid))\n+\t\treturn false;\n+\n \t/* If demotion node isn't in the cgroup's mems_allowed, fall back */\n \treturn mem_cgroup_node_allowed(memcg, demotion_nid);\n }\n@@ -1022,8 +1027,10 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \t\t\t\t     struct pglist_data *pgdat)\n {\n \tint target_nid = next_demotion_node(pgdat->node_id);\n-\tunsigned int nr_succeeded;\n+\tint first_nid = target_nid;\n+\tunsigned int nr_succeeded = 0;\n \tnodemask_t allowed_mask;\n+\tint ret;\n \n \tstruct migration_target_control mtc = {\n \t\t/*\n@@ -1046,6 +1053,27 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \n \tnode_get_allowed_targets(pgdat, &allowed_mask);\n \n+\t/* Try private node targets until we find non-private node */\n+\twhile (node_state(target_nid, N_MEMORY_PRIVATE)) {\n+\t\tunsigned int nr = 0;\n+\n+\t\tret = node_private_migrate_to(demote_folios, target_nid,\n+\t\t\t\t\t      MIGRATE_ASYNC, MR_DEMOTION,\n+\t\t\t\t\t      &nr);\n+\t\tnr_succeeded += nr;\n+\t\tif (ret == 0 || list_empty(demote_folios))\n+\t\t\treturn nr_succeeded;\n+\n+\t\ttarget_nid = next_node_in(target_nid, allowed_mask);\n+\t\tif (target_nid == first_nid)\n+\t\t\treturn nr_succeeded;\n+\t\tif (!node_state(target_nid, N_MEMORY_PRIVATE))\n+\t\t\tbreak;\n+\t}\n+\n+\t/* target_nid is a non-private node; use standard migration */\n+\tmtc.nid = target_nid;\n+\n \t/* Demotion ignores all cpuset and mempolicy settings */\n \tmigrate_pages(demote_folios, alloc_demote_folio, NULL,\n \t\t      (unsigned long)&mtc, MIGRATE_ASYNC, MR_DEMOTION,\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about write faults on private nodes by adding a new operation flag NP_OPS_PROTECT_WRITE and modifying several functions to prevent PTEs from being upgraded to writable when the node is write-protected.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Services that intercept write faults (e.g., for promotion tracking)\nneed PTEs to stay read-only. This requires preventing mprotect\nfrom silently upgrade the PTE, bypassing the service's handle_fault\ncallback.\n\nAdd NP_OPS_PROTECT_WRITE and folio_managed_wrprotect().\n\nIn change_pte_range() and change_huge_pmd(), suppress PTE write-upgrade\nwhen MM_CP_TRY_CHANGE_WRITABLE is sees the folio is write-protected.\n\nIn handle_pte_fault() and do_huge_pmd_wp_page(), dispatch to the node's\nops->handle_fault callback when set, allowing the service to handle write\nfaults with promotion or other custom logic.\n\nNP_OPS_MEMPOLICY is incompatible with NP_OPS_PROTECT_WRITE to avoid the\nfootgun of binding a writable VMA to a write-protected node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++\n include/linux/node_private.h | 22 ++++++++\n mm/huge_memory.c             | 17 ++++++-\n mm/internal.h                | 99 ++++++++++++++++++++++++++++++++++++\n mm/memory.c                  | 15 ++++++\n mm/migrate.c                 | 14 +----\n mm/mprotect.c                |  4 +-\n 7 files changed, 159 insertions(+), 16 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex c08b5a948779..a4955b9b5b93 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -957,6 +957,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    (ops->flags & NP_OPS_PROTECT_WRITE))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e254e36056cd..27d6e5d84e61 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -70,6 +70,24 @@ struct vm_fault;\n  *     PFN-based metadata (compression tables, device page tables, DMA\n  *     mappings, etc.) before any access through the page tables.\n  *\n+ * @handle_fault: Handle fault on folio on this private node.\n+ *   [folio-referenced callback, PTL held on entry]\n+ *\n+ *   Called from handle_pte_fault() (PTE level) or do_huge_pmd_wp_page()\n+ *   (PMD level) after lock acquisition and entry verification.\n+ *   @folio is the faulting folio, @level indicates the page table level.\n+ *\n+ *   For PGTABLE_LEVEL_PTE: vmf->pte is mapped and vmf->ptl is the\n+ *   PTE lock.  Release via pte_unmap_unlock(vmf->pte, vmf->ptl).\n+ *\n+ *   For PGTABLE_LEVEL_PMD: vmf->pte is NULL and vmf->ptl is the\n+ *   PMD lock.  Release via spin_unlock(vmf->ptl).\n+ *\n+ *   The callback MUST release PTL on ALL paths.\n+ *   The caller will NOT touch the page table entry after this returns.\n+ *\n+ *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -81,6 +99,8 @@ struct node_private_ops {\n \t\t\t\t  enum migrate_reason reason,\n \t\t\t\t  unsigned int *nr_succeeded);\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n+\tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t   enum pgtable_level level);\n \tunsigned long flags;\n };\n \n@@ -90,6 +110,8 @@ struct node_private_ops {\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n /* Node participates as a demotion target in memory-tiers */\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n+/* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n+#define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 2ecae494291a..d9ba6593244d 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -2063,12 +2063,14 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tstruct page *page;\n \tunsigned long haddr = vmf->address & HPAGE_PMD_MASK;\n \tpmd_t orig_pmd = vmf->orig_pmd;\n+\tvm_fault_t ret;\n+\n \n \tvmf->ptl = pmd_lockptr(vma->vm_mm, vmf->pmd);\n \tVM_BUG_ON_VMA(!vma->anon_vma, vma);\n \n \tif (is_huge_zero_pmd(orig_pmd)) {\n-\t\tvm_fault_t ret = do_huge_zero_wp_pmd(vmf);\n+\t\tret = do_huge_zero_wp_pmd(vmf);\n \n \t\tif (!(ret & VM_FAULT_FALLBACK))\n \t\t\treturn ret;\n@@ -2088,6 +2090,13 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tfolio = page_folio(page);\n \tVM_BUG_ON_PAGE(!PageHead(page), page);\n \n+\t/* Private-managed write-protect: let the service handle the fault */\n+\tif (unlikely(folio_is_private_managed(folio))) {\n+\t\tif (folio_managed_handle_fault(folio, vmf,\n+\t\t\t\t\t      PGTABLE_LEVEL_PMD, &ret))\n+\t\t\treturn ret;\n+\t}\n+\n \t/* Early check when only holding the PT lock. */\n \tif (PageAnonExclusive(page))\n \t\tgoto reuse;\n@@ -2633,7 +2642,8 @@ int change_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,\n \n \t/* See change_pte_range(). */\n \tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) && !pmd_write(entry) &&\n-\t    can_change_pmd_writable(vma, addr, entry))\n+\t    can_change_pmd_writable(vma, addr, entry) &&\n+\t    !folio_managed_wrprotect(pmd_folio(entry)))\n \t\tentry = pmd_mkwrite(entry, vma);\n \n \tret = HPAGE_PMD_NR;\n@@ -4943,6 +4953,9 @@ void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n \tif (folio_test_dirty(folio) && softleaf_is_migration_dirty(entry))\n \t\tpmde = pmd_mkdirty(pmde);\n \n+\tif (folio_managed_wrprotect(folio))\n+\t\tpmde = pmd_wrprotect(pmde);\n+\n \tif (folio_is_device_private(folio)) {\n \t\tswp_entry_t entry;\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 5950e20d4023..ae4ff86e8dc6 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -11,6 +11,7 @@\n #include <linux/khugepaged.h>\n #include <linux/mm.h>\n #include <linux/mm_inline.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/pagewalk.h>\n #include <linux/rmap.h>\n@@ -18,6 +19,7 @@\n #include <linux/leafops.h>\n #include <linux/swap_cgroup.h>\n #include <linux/tracepoint-defs.h>\n+#include <linux/node_private.h>\n \n /* Internal core VMA manipulation functions. */\n #include \"vma.h\"\n@@ -1449,6 +1451,103 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/*\n+ * folio_managed_handle_fault - Dispatch fault on managed-memory folio\n+ * @folio: the faulting folio (must not be NULL)\n+ * @vmf: the vm_fault descriptor (PTL held: vmf->ptl locked)\n+ * @level: page table level (PGTABLE_LEVEL_PTE or PGTABLE_LEVEL_PMD)\n+ * @ret: output fault result if handled\n+ *\n+ * Called with PTL held.  If a handle_fault callback exists, it is invoked\n+ * with PTL still held.  The callback is responsible for releasing PTL on\n+ * all paths.\n+ *\n+ * Returns true if the service handled the fault (PTL released by callback,\n+ * caller returns *ret).  Returns false if no handler exists (PTL still held,\n+ * caller continues with normal fault handling).\n+ */\n+static inline bool folio_managed_handle_fault(struct folio *folio,\n+\t\t\t\t\t      struct vm_fault *vmf,\n+\t\t\t\t\t      enum pgtable_level level,\n+\t\t\t\t\t      vm_fault_t *ret)\n+{\n+\t/* Zone device pages use swap entries; handled in do_swap_page */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->handle_fault) {\n+\t\t\t*ret = ops->handle_fault(folio, vmf, level);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n+/**\n+ * folio_managed_wrprotect - Should this folio's mappings stay write-protected?\n+ * @folio: the folio to check\n+ *\n+ * Returns true if the folio is on a private node with NP_OPS_PROTECT_WRITE,\n+ * meaning page table entries (PTE or PMD) should not be made writable.\n+ * Write faults are intercepted by the service's handle_fault callback\n+ * to promote the folio to DRAM.\n+ *\n+ * Used by:\n+ *   - change_pte_range() / change_huge_pmd(): prevent mprotect write-upgrade\n+ *   - remove_migration_pte() / remove_migration_pmd(): strip write after migration\n+ *   - do_huge_pmd_wp_page(): dispatch to fault handler instead of reuse\n+ */\n+static inline bool folio_managed_wrprotect(struct folio *folio)\n+{\n+\treturn unlikely(folio_is_private_node(folio) &&\n+\t\t\tfolio_private_flags(folio, NP_OPS_PROTECT_WRITE));\n+}\n+\n+/**\n+ * folio_managed_fixup_migration_pte - Fixup PTE after migration for\n+ *                                     managed memory pages.\n+ * @new: the destination page\n+ * @pte: the PTE being installed (normal PTE built by caller)\n+ * @old_pte: the original PTE (before migration, for swap entry flags)\n+ * @vma: the VMA\n+ *\n+ * For MEMORY_DEVICE_PRIVATE pages: replaces the PTE with a device-private\n+ * swap entry, preserving soft_dirty and uffd_wp from old_pte.\n+ *\n+ * For N_MEMORY_PRIVATE pages with NP_OPS_PROTECT_WRITE: strips the write\n+ * bit so the next write triggers the fault handler for promotion.\n+ *\n+ * For normal pages: returns pte unmodified.\n+ */\n+static inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n+\t\t\t\t\t\t      pte_t pte,\n+\t\t\t\t\t\t      pte_t old_pte,\n+\t\t\t\t\t\t      struct vm_area_struct *vma)\n+{\n+\tif (unlikely(is_device_private_page(new))) {\n+\t\tsoftleaf_t entry;\n+\n+\t\tif (pte_write(pte))\n+\t\t\tentry = make_writable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\telse\n+\t\t\tentry = make_readable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\tpte = softleaf_to_pte(entry);\n+\t\tif (pte_swp_soft_dirty(old_pte))\n+\t\t\tpte = pte_swp_mksoft_dirty(pte);\n+\t\tif (pte_swp_uffd_wp(old_pte))\n+\t\t\tpte = pte_swp_mkuffd_wp(pte);\n+\t} else if (folio_managed_wrprotect(page_folio(new))) {\n+\t\tpte = pte_wrprotect(pte);\n+\t}\n+\treturn pte;\n+}\n+\n /**\n  * folio_managed_migrate_notify - Notify service that a folio changed location\n  * @src: the old folio (about to be freed)\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 2a55edc48a65..0f78988befef 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -6079,6 +6079,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n \t */\n+\tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n+\t\twritable = false;\n+\t\tignore_writable = true;\n+\t}\n \tif (folio && folio_test_large(folio))\n \t\tnuma_rebuild_large_mapping(vmf, vma, folio, pte, ignore_writable,\n \t\t\t\t\t   pte_write_upgrade);\n@@ -6228,6 +6232,7 @@ static void fix_spurious_fault(struct vm_fault *vmf,\n  */\n static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n {\n+\tstruct folio *folio;\n \tpte_t entry;\n \n \tif (unlikely(pmd_none(*vmf->pmd))) {\n@@ -6284,6 +6289,16 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n \t\tupdate_mmu_tlb(vmf->vma, vmf->address, vmf->pte);\n \t\tgoto unlock;\n \t}\n+\n+\tfolio = vm_normal_folio(vmf->vma, vmf->address, entry);\n+\tif (unlikely(folio && folio_is_private_managed(folio))) {\n+\t\tvm_fault_t fault_ret;\n+\n+\t\tif (folio_managed_handle_fault(folio, vmf, PGTABLE_LEVEL_PTE,\n+\t\t\t\t\t       &fault_ret))\n+\t\t\treturn fault_ret;\n+\t}\n+\n \tif (vmf->flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {\n \t\tif (!pte_write(entry))\n \t\t\treturn do_wp_page(vmf);\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex a54d4af04df3..f632e8b03504 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -398,19 +398,7 @@ static bool remove_migration_pte(struct folio *folio,\n \t\tif (folio_test_anon(folio) && !softleaf_is_migration_read(entry))\n \t\t\trmap_flags |= RMAP_EXCLUSIVE;\n \n-\t\tif (unlikely(is_device_private_page(new))) {\n-\t\t\tif (pte_write(pte))\n-\t\t\t\tentry = make_writable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\telse\n-\t\t\t\tentry = make_readable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\tpte = softleaf_to_pte(entry);\n-\t\t\tif (pte_swp_soft_dirty(old_pte))\n-\t\t\t\tpte = pte_swp_mksoft_dirty(pte);\n-\t\t\tif (pte_swp_uffd_wp(old_pte))\n-\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n-\t\t}\n+\t\tpte = folio_managed_fixup_migration_pte(new, pte, old_pte, vma);\n \n #ifdef CONFIG_HUGETLB_PAGE\n \t\tif (folio_test_hugetlb(folio)) {\ndiff --git a/mm/mprotect.c b/mm/mprotect.c\nindex 283889e4f1ce..830be609bc24 100644\n--- a/mm/mprotect.c\n+++ b/mm/mprotect.c\n@@ -30,6 +30,7 @@\n #include <linux/mm_inline.h>\n #include <linux/pgtable.h>\n #include <linux/userfaultfd_k.h>\n+#include <linux/node_private.h>\n #include <uapi/linux/mman.h>\n #include <asm/cacheflush.h>\n #include <asm/mmu_context.h>\n@@ -290,7 +291,8 @@ static long change_pte_range(struct mmu_gather *tlb,\n \t\t\t * COW or special handling is required.\n \t\t\t */\n \t\t\tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) &&\n-\t\t\t     !pte_write(ptent))\n+\t\t\t     !pte_write(ptent) &&\n+\t\t\t     !(folio && folio_managed_wrprotect(folio)))\n \t\t\t\tset_write_prot_commit_flush_ptes(vma, folio, page,\n \t\t\t\taddr, pte, oldpte, ptent, nr_ptes, tlb);\n \t\t\telse\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the reclaim policy for private nodes in boosted reclaim mode, explaining that it needs to allow swap and writepage operations. They proposed adding a reclaim_policy callback to struct node_private_ops and a struct node_reclaim_policy to configure these policies. The author also added zone_reclaim_allowed() to filter private nodes that have not opted into reclaim.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix is needed",
                "proposed changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services that drive kswapd via watermark_boost need\ncontrol over the reclaim policy.  There are three problems:\n\n1) Boosted reclaim suppresses may_swap and may_writepage.  When\n   demotion is not possible, swap is the only evict path, so kswapd\n   cannot make progress and pages are stranded.\n\n2) __setup_per_zone_wmarks() unconditionally zeros watermark_boost,\n   killing the service's pressure signal.\n\n3) Not all private nodes want reclaim to touch their pages.\n\nAdd a reclaim_policy callback to struct node_private_ops and a\nstruct node_reclaim_policy with:\n\n  - active:             set by the helper when a callback was invoked\n  - may_swap:           allow swap writeback during boosted reclaim\n  - may_writepage:      allow writepage during boosted reclaim\n  - managed_watermarks: service owns watermark_boost lifecycle\n\nWe do not allow disabling swap/writepage, as core MM may have\nexplicitly enabled them on a non-boosted pass.\n\nWe only allow enablign swap/writepage, so that the supression during\na boost can be overridden.  This allows a device to force evictions\neven when the system otherwise would not percieve pressure.\n\nThis is important for a service like compressed RAM, as device capacity\nmay differ from reported capacity, and device may want to relieve real\npressure (poor compression ratio) as opposed to percieved pressure\n(i.e. how many pages are in use).\n\nAdd zone_reclaim_allowed() to filter private nodes that have not\nopted into reclaim.\n\nRegular nodes fall through to cpuset_zone_allowed() unchanged.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 28 ++++++++++++++++++++++++++++\n mm/internal.h                | 36 ++++++++++++++++++++++++++++++++++++\n mm/page_alloc.c              | 11 ++++++++++-\n mm/vmscan.c                  | 25 +++++++++++++++++++++++--\n 4 files changed, 97 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 27d6e5d84e61..34be52383255 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -14,6 +14,24 @@ struct page;\n struct vm_area_struct;\n struct vm_fault;\n \n+/**\n+ * struct node_reclaim_policy - Reclaim policy overrides for private nodes\n+ * @active: set by node_private_reclaim_policy() when a callback was invoked\n+ * @may_swap: allow swap writeback during boosted reclaim\n+ * @may_writepage: allow writepage during boosted reclaim\n+ * @managed_watermarks: service owns watermark_boost lifecycle; kswapd must\n+ *                      not clear it after boosted reclaim\n+ *\n+ * Passed to the reclaim_policy callback so each private node service can\n+ * inject its own reclaim policy before kswapd runs boosted reclaim.\n+ */\n+struct node_reclaim_policy {\n+\tbool active;\n+\tbool may_swap;\n+\tbool may_writepage;\n+\tbool managed_watermarks;\n+};\n+\n /**\n  * struct node_private_ops - Callbacks for private node services\n  *\n@@ -88,6 +106,13 @@ struct vm_fault;\n  *\n  *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n  *\n+ * @reclaim_policy: Configure reclaim policy for boosted reclaim.\n+ *   [called hodling rcu_read_lock, MUST NOT sleep]\n+ *   Called by kswapd before boosted reclaim to let the service override\n+ *   may_swap / may_writepage.  If provided, the service also owns the\n+ *   watermark_boost lifecycle (kswapd will not clear it).\n+ *   If NULL, normal boost policy applies.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -101,6 +126,7 @@ struct node_private_ops {\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n+\tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n \tunsigned long flags;\n };\n \n@@ -112,6 +138,8 @@ struct node_private_ops {\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n /* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n+/* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n+#define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/internal.h b/mm/internal.h\nindex ae4ff86e8dc6..db32cb2d7a29 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1572,6 +1572,42 @@ static inline void folio_managed_migrate_notify(struct folio *src,\n \t\tops->folio_migrate(src, dst);\n }\n \n+/**\n+ * node_private_reclaim_policy - invoke the service's reclaim policy callback\n+ * @nid: NUMA node id\n+ * @policy: reclaim policy struct to fill in\n+ *\n+ * Called by kswapd before boosted reclaim.  Zeroes @policy, then if the\n+ * private node service provides a reclaim_policy callback, invokes it\n+ * and sets policy->active to true.\n+ */\n+#ifdef CONFIG_NUMA\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tstruct node_private *np;\n+\n+\tmemset(policy, 0, sizeof(*policy));\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (np && np->ops && np->ops->reclaim_policy) {\n+\t\tnp->ops->reclaim_policy(nid, policy);\n+\t\tpolicy->active = true;\n+\t}\n+\trcu_read_unlock();\n+}\n+#else\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tmemset(policy, 0, sizeof(*policy));\n+}\n+#endif\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e272dfdc6b00..9692048ab5fb 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -55,6 +55,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/node_private.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -6437,6 +6438,8 @@ static void __setup_per_zone_wmarks(void)\n \tunsigned long lowmem_pages = 0;\n \tstruct zone *zone;\n \tunsigned long flags;\n+\tstruct node_reclaim_policy rp;\n+\tint prev_nid = NUMA_NO_NODE;\n \n \t/* Calculate total number of !ZONE_HIGHMEM and !ZONE_MOVABLE pages */\n \tfor_each_zone(zone) {\n@@ -6446,6 +6449,7 @@ static void __setup_per_zone_wmarks(void)\n \n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n+\t\tint nid = zone_to_nid(zone);\n \n \t\tspin_lock_irqsave(&zone->lock, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n@@ -6482,7 +6486,12 @@ static void __setup_per_zone_wmarks(void)\n \t\t\t    mult_frac(zone_managed_pages(zone),\n \t\t\t\t      watermark_scale_factor, 10000));\n \n-\t\tzone->watermark_boost = 0;\n+\t\tif (nid != prev_nid) {\n+\t\t\tnode_private_reclaim_policy(nid, &rp);\n+\t\t\tprev_nid = nid;\n+\t\t}\n+\t\tif (!rp.managed_watermarks)\n+\t\t\tzone->watermark_boost = 0;\n \t\tzone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_HIGH] = low_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 0f534428ea88..07de666c1276 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -73,6 +73,13 @@\n #define CREATE_TRACE_POINTS\n #include <trace/events/vmscan.h>\n \n+static inline bool zone_reclaim_allowed(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn zone_private_flags(zone, NP_OPS_RECLAIM);\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n struct scan_control {\n \t/* How many pages shrink_list() should reclaim */\n \tunsigned long nr_to_reclaim;\n@@ -6274,7 +6281,7 @@ static void shrink_zones(struct zonelist *zonelist, struct scan_control *sc)\n \t\t * to global LRU.\n \t\t */\n \t\tif (!cgroup_reclaim(sc)) {\n-\t\t\tif (!cpuset_zone_allowed(zone,\n+\t\t\tif (!zone_reclaim_allowed(zone,\n \t\t\t\t\t\t GFP_KERNEL | __GFP_HARDWALL))\n \t\t\t\tcontinue;\n \n@@ -6992,6 +6999,7 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \tunsigned long zone_boosts[MAX_NR_ZONES] = { 0, };\n \tbool boosted;\n \tstruct zone *zone;\n+\tstruct node_reclaim_policy policy;\n \tstruct scan_control sc = {\n \t\t.gfp_mask = GFP_KERNEL,\n \t\t.order = order,\n@@ -7016,6 +7024,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t}\n \tboosted = nr_boost_reclaim;\n \n+\t/* Query/cache private node reclaim policy once per balance() */\n+\tnode_private_reclaim_policy(pgdat->node_id, &policy);\n+\n restart:\n \tset_reclaim_active(pgdat, highest_zoneidx);\n \tsc.priority = DEF_PRIORITY;\n@@ -7083,6 +7094,12 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\tsc.may_writepage = !laptop_mode && !nr_boost_reclaim;\n \t\tsc.may_swap = !nr_boost_reclaim;\n \n+\t\t/* Private nodes may enable swap/writepage when using boost */\n+\t\tif (policy.active) {\n+\t\t\tsc.may_swap |= policy.may_swap;\n+\t\t\tsc.may_writepage |= policy.may_writepage;\n+\t\t}\n+\n \t\t/*\n \t\t * Do some background aging, to give pages a chance to be\n \t\t * referenced before reclaiming. All pages are rotated\n@@ -7176,6 +7193,10 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\t\tif (!zone_boosts[i])\n \t\t\t\tcontinue;\n \n+\t\t\t/* Some private nodes may own the\\ boost lifecycle */\n+\t\t\tif (policy.managed_watermarks)\n+\t\t\t\tcontinue;\n+\n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n \t\t\tspin_lock_irqsave(&zone->lock, flags);\n@@ -7406,7 +7427,7 @@ void wakeup_kswapd(struct zone *zone, gfp_t gfp_flags, int order,\n \tif (!managed_zone(zone))\n \t\treturn;\n \n-\tif (!cpuset_zone_allowed(zone, gfp_flags))\n+\tif (!zone_reclaim_allowed(zone, gfp_flags))\n \t\treturn;\n \n \tpgdat = zone->zone_pgdat;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed the concern that the OOM killer may select an undeserving victim if it doesn't know whether killing a task can actually free memory on a private node. The author introduced NP_OPS_OOM_ELIGIBLE and helpers to check if a private node is OOM-eligible, and updated constrained_alloc() to use these checks.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The OOM killer must know whether killing a task can actually free\nmemory such that pressure is reduced.\n\nA private node only contributes to relieving pressure if it participates\nin both reclaim and demotion. Without this check, the check, the OOM\nkiller may select an undeserving victim.\n\nIntroduce NP_OPS_OOM_ELIGIBLE and helpers node_oom_eligible() and\nzone_oom_eligible().\n\nReplace cpuset_mems_allowed_intersects() in oom_cpuset_eligible()\nwith oom_mems_intersect() that iterates N_MEMORY nodes and skips\nineligible private nodes.\n\nUpdate constrained_alloc() to use zone_oom_eligible() for constraint\ndetection and node_oom_eligible() to exclude ineligible nodes from\ntotalpages accounting.\n\nRemove cpuset_mems_allowed_intersects() as it has no remaining callers.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cpuset.h       |  9 -------\n include/linux/node_private.h |  3 +++\n kernel/cgroup/cpuset.c       | 17 ------------\n mm/oom_kill.c                | 52 ++++++++++++++++++++++++++++++++----\n 4 files changed, 50 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/cpuset.h b/include/linux/cpuset.h\nindex 7b2f3f6b68a9..53ccfb00b277 100644\n--- a/include/linux/cpuset.h\n+++ b/include/linux/cpuset.h\n@@ -97,9 +97,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-extern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t  const struct task_struct *tsk2);\n-\n #ifdef CONFIG_CPUSETS_V1\n #define cpuset_memory_pressure_bump() \t\t\t\t\\\n \tdo {\t\t\t\t\t\t\t\\\n@@ -241,12 +238,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-static inline int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t\t const struct task_struct *tsk2)\n-{\n-\treturn 1;\n-}\n-\n static inline void cpuset_memory_pressure_bump(void) {}\n \n static inline void cpuset_task_status_allowed(struct seq_file *m,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34be52383255..34d862f09e24 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -141,6 +141,9 @@ struct node_private_ops {\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n+/* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n+#define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 1a597f0c7c6c..29789d544fd5 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -4530,23 +4530,6 @@ int cpuset_mem_spread_node(void)\n \treturn cpuset_spread_node(&current->cpuset_mem_spread_rotor);\n }\n \n-/**\n- * cpuset_mems_allowed_intersects - Does @tsk1's mems_allowed intersect @tsk2's?\n- * @tsk1: pointer to task_struct of some task.\n- * @tsk2: pointer to task_struct of some other task.\n- *\n- * Description: Return true if @tsk1's mems_allowed intersects the\n- * mems_allowed of @tsk2.  Used by the OOM killer to determine if\n- * one of the task's memory usage might impact the memory available\n- * to the other.\n- **/\n-\n-int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t   const struct task_struct *tsk2)\n-{\n-\treturn nodes_intersects(tsk1->mems_allowed, tsk2->mems_allowed);\n-}\n-\n /**\n  * cpuset_print_current_mems_allowed - prints current's cpuset and mems_allowed\n  *\ndiff --git a/mm/oom_kill.c b/mm/oom_kill.c\nindex 5eb11fbba704..cd0d65ccd1e8 100644\n--- a/mm/oom_kill.c\n+++ b/mm/oom_kill.c\n@@ -74,7 +74,45 @@ static inline bool is_memcg_oom(struct oom_control *oc)\n \treturn oc->memcg != NULL;\n }\n \n+/* Private nodes are only eligible if they support both reclaim and demotion */\n+static inline bool node_oom_eligible(int nid)\n+{\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn (node_private_flags(nid) & NP_OPS_OOM_ELIGIBLE) ==\n+\t\tNP_OPS_OOM_ELIGIBLE;\n+}\n+\n+static inline bool zone_oom_eligible(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (!node_oom_eligible(zone_to_nid(zone)))\n+\t\treturn false;\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n #ifdef CONFIG_NUMA\n+/*\n+ * Killing a task can only relieve system pressure if freed memory can be\n+ * demoted there and reclaim can operate on the node's pages, so we\n+ * omit private nodes that aren't eligible.\n+ */\n+static bool oom_mems_intersect(const struct task_struct *tsk1,\n+\t\t\t       const struct task_struct *tsk2)\n+{\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (!node_isset(nid, tsk1->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_isset(nid, tsk2->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_oom_eligible(nid))\n+\t\t\tcontinue;\n+\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n /**\n  * oom_cpuset_eligible() - check task eligibility for kill\n  * @start: task struct of which task to consider\n@@ -107,9 +145,10 @@ static bool oom_cpuset_eligible(struct task_struct *start,\n \t\t} else {\n \t\t\t/*\n \t\t\t * This is not a mempolicy constrained oom, so only\n-\t\t\t * check the mems of tsk's cpuset.\n+\t\t\t * check the mems of tsk's cpuset, excluding private\n+\t\t\t * nodes that do not participate in kernel reclaim.\n \t\t\t */\n-\t\t\tret = cpuset_mems_allowed_intersects(current, tsk);\n+\t\t\tret = oom_mems_intersect(current, tsk);\n \t\t}\n \t\tif (ret)\n \t\t\tbreak;\n@@ -291,16 +330,19 @@ static enum oom_constraint constrained_alloc(struct oom_control *oc)\n \t\treturn CONSTRAINT_MEMORY_POLICY;\n \t}\n \n-\t/* Check this allocation failure is caused by cpuset's wall function */\n+\t/* Check this allocation failure is caused by cpuset or private node constraints */\n \tfor_each_zone_zonelist_nodemask(zone, z, oc->zonelist,\n \t\t\thighest_zoneidx, oc->nodemask)\n-\t\tif (!cpuset_zone_allowed(zone, oc->gfp_mask))\n+\t\tif (!zone_oom_eligible(zone, oc->gfp_mask))\n \t\t\tcpuset_limited = true;\n \n \tif (cpuset_limited) {\n \t\toc->totalpages = total_swap_pages;\n-\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed)\n+\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed) {\n+\t\t\tif (!node_oom_eligible(nid))\n+\t\t\t\tcontinue;\n \t\t\toc->totalpages += node_present_pages(nid);\n+\t\t}\n \t\treturn CONSTRAINT_CPUSET;\n \t}\n \treturn CONSTRAINT_NONE;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private nodes engaging in NUMA balancing faults by introducing an opt-in method (NP_OPS_NUMA_BALANCING) and adding a helper function to filter for private nodes that have opted in. The author also added code to enforce write-protection on failed or skipped migrations.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "added new functionality"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Not all private nodes may wish to engage in NUMA balancing faults.\n\nAdd the NP_OPS_NUMA_BALANCING flag (BIT(5)) as an opt-in method.\n\nIntroduce folio_managed_allows_numa() helper:\n   ZONE_DEVICE folios always return false (never NUMA-scanned)\n   NP_OPS_NUMA_BALANCING filters for private nodes\n\nIn do_numa_page(), if a private-node folio with NP_OPS_PROTECT_WRITE\nis still on its node after a failed/skipped migration, enforce\nwrite-protection so the next write triggers handle_fault.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h | 16 ++++++++++++++++\n mm/memory.c                  | 11 +++++++++++\n mm/mempolicy.c               |  5 ++++-\n 4 files changed, 35 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex a4955b9b5b93..88aaac45e814 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -961,6 +961,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (ops->flags & NP_OPS_PROTECT_WRITE))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_NUMA_BALANCING) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34d862f09e24..5ac60db1f044 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -140,6 +140,8 @@ struct node_private_ops {\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n+/* Allow NUMA balancing to scan and migrate folios on this node */\n+#define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n@@ -263,6 +265,15 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n }\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\tif (!folio_is_private_managed(folio))\n+\t\treturn true;\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\treturn folio_private_flags(folio, NP_OPS_NUMA_BALANCING);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \tif (folio_is_zone_device(folio))\n@@ -443,6 +454,11 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\treturn !folio_is_zone_device(folio);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \treturn -ENOENT;\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 0f78988befef..88a581baae40 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -78,6 +78,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/pgalloc.h>\n #include <linux/uaccess.h>\n+#include <linux/node_private.h>\n \n #include <trace/events/kmem.h>\n \n@@ -6041,6 +6042,12 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \tif (!folio || folio_is_zone_device(folio))\n \t\tgoto out_map;\n \n+\t/*\n+\t * We do not need to check private-node folios here because the private\n+\t * memory service either never opted in to NUMA balancing, or it did\n+\t * and we need to restore private PTE controls on the failure path.\n+\t */\n+\n \tnid = folio_nid(folio);\n \tnr_pages = folio_nr_pages(folio);\n \n@@ -6078,6 +6085,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t/*\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n+\t *\n+\t * If the folio is still on a private node with NP_OPS_PROTECT_WRITE,\n+\t * enforce write-protection so the next write triggers handle_fault.\n+\t * This covers migration-failed and migration-skipped paths.\n \t */\n \tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n \t\twritable = false;\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 8ac014950e88..8a3a9916ab59 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -861,7 +861,10 @@ bool folio_can_map_prot_numa(struct folio *folio, struct vm_area_struct *vma,\n {\n \tint nid;\n \n-\tif (!folio || folio_is_zone_device(folio) || folio_test_ksm(folio))\n+\tif (!folio || folio_test_ksm(folio))\n+\t\treturn false;\n+\n+\tif (unlikely(!folio_managed_allows_numa(folio)))\n \t\treturn false;\n \n \t/* Also skip shared copy-on-write folios */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about compaction on private nodes, explaining that it requires migration and services may have PFN-based metadata to update. They added a folio_migrate callback, zone_supports_compaction function, and filtered three direct compaction zone loops. The service is responsible for starting kcompactd on its node.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node zones should not be compacted unless the service explicitly\nopts in - as compaction requires migration and services may have\nPFN-based metadata that needs updating.\n\nAdd a folio_migrate callback which fires from migrate_folio_move() for\neach relocated folio before faults are unblocked.\n\nAdd zone_supports_compaction() which returns true for normal zones and\nchecks NP_OPS_COMPACTION for N_MEMORY_PRIVATE zones.\n\nFilter three direct compaction zone loops:\n  - compaction_zonelist_suitable() (reclaimer eligibility)\n  - try_to_compact_pages()         (direct compaction)\n  - compact_node()                 (proactive/manual compaction)\n\nkcompactd paths are intentionally unfiltered -- the service is\nresponsible for starting kcompactd on its node.\n\nNP_OPS_COMPACTION requires NP_OPS_MIGRATION.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h |  2 ++\n mm/compaction.c              | 26 ++++++++++++++++++++++++++\n 3 files changed, 32 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 88aaac45e814..da523aca18fa 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -965,6 +965,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_COMPACTION) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 5ac60db1f044..fe0336773ddb 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -142,6 +142,8 @@ struct node_private_ops {\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n /* Allow NUMA balancing to scan and migrate folios on this node */\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n+/* Allow compaction to run on the node.  Service must start kcompactd. */\n+#define NP_OPS_COMPACTION\t\tBIT(6)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 6a65145b03d8..d8532b957ec6 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,9 +24,26 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/node_private.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n+\n+/*\n+ * Private node zones require NP_OPS_COMPACTION to opt in.  Normal zones\n+ * always support compaction.\n+ */\n+static inline bool zone_supports_compaction(struct zone *zone)\n+{\n+#ifdef CONFIG_NUMA\n+\tif (!node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn zone_private_flags(zone, NP_OPS_COMPACTION);\n+#else\n+\treturn true;\n+#endif\n+}\n+\n /*\n  * Fragmentation score check interval for proactive compaction purposes.\n  */\n@@ -2443,6 +2460,9 @@ bool compaction_zonelist_suitable(struct alloc_context *ac, int order,\n \t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tunsigned long available;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\t/*\n \t\t * Do not consider all the reclaimable memory because we do not\n \t\t * want to trash just for a single high order allocation which\n@@ -2832,6 +2852,9 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\n \t\t\trc = max_t(enum compact_result, COMPACT_DEFERRED, rc);\n@@ -2906,6 +2929,9 @@ static int compact_node(pg_data_t *pgdat, bool proactive)\n \t\tif (!populated_zone(zone))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (fatal_signal_pending(current))\n \t\t\treturn -EINTR;\n \n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private node folios being longterm-pinnable by default, explaining that this would prevent services from controlling the memory during pinning. They added an NP_OPS_LONGTERM_PIN flag for services to opt-in and modified the folio_is_longterm_pinnable() function in mm.h to check for this flag.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not be longterm-pinnable by default.\nA pinned folio is frozen in place, no migration, compaction, or\nreclaim, so the service loses control for the duration of the pin.\n\nSome services may depend on hot-unplugability and must disallow\nlongterm pinning.  Others (accelerators with shared CPU-device state)\nneed pinning to work.\n\nAdd NP_OPS_LONGTERM_PIN flag for services to opt in with. Hook into\nfolio_is_longterm_pinnable() in mm.h, which all GUP callers\nout-of-line helper, node_private_allows_longterm_pin(),  called\nonly for N_MEMORY_PRIVATE nodes.\n\nWithout the flag: folio_is_longterm_pinnable() returns false, migration\nfails (no __GFP_PRIVATE in GFP mask) and pin_user_pages(FOLL_LONGTERM)\nreturns -ENOMEM.\n\nWith the flag: pin succeeds and the folio stays on the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 15 +++++++++++++++\n include/linux/mm.h           | 22 ++++++++++++++++++++++\n include/linux/node_private.h |  2 ++\n 3 files changed, 39 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex da523aca18fa..5d2487fd54f4 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -866,6 +866,21 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n static DEFINE_MUTEX(node_private_lock);\n static bool node_private_initialized;\n \n+/**\n+ * node_private_allows_longterm_pin - Check if a private node allows longterm pinning\n+ * @nid: Node identifier\n+ *\n+ * Out-of-line helper for folio_is_longterm_pinnable() since mm.h cannot\n+ * include node_private.h (circular dependency).\n+ *\n+ * Returns true if the node has NP_OPS_LONGTERM_PIN set.\n+ */\n+bool node_private_allows_longterm_pin(int nid)\n+{\n+\treturn node_private_has_flag(nid, NP_OPS_LONGTERM_PIN);\n+}\n+EXPORT_SYMBOL_GPL(node_private_allows_longterm_pin);\n+\n /**\n  * node_private_register - Register a private node\n  * @nid: Node identifier\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex fb1819ad42c3..9088fd08aeb9 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -2192,6 +2192,13 @@ static inline bool is_zero_folio(const struct folio *folio)\n \n /* MIGRATE_CMA and ZONE_MOVABLE do not allow pin folios */\n #ifdef CONFIG_MIGRATION\n+\n+#ifdef CONFIG_NUMA\n+bool node_private_allows_longterm_pin(int nid);\n+#else\n+static inline bool node_private_allows_longterm_pin(int nid) { return false; }\n+#endif\n+\n static inline bool folio_is_longterm_pinnable(struct folio *folio)\n {\n #ifdef CONFIG_CMA\n@@ -2215,6 +2222,21 @@ static inline bool folio_is_longterm_pinnable(struct folio *folio)\n \tif (folio_is_fsdax(folio))\n \t\treturn false;\n \n+\t/*\n+\t * Private node folios are not longterm pinnable by default.\n+\t * Services that support pinning opt in via NP_OPS_LONGTERM_PIN.\n+\t * node_private_allows_longterm_pin() is out-of-line because\n+\t * node_private.h includes mm.h (circular dependency).\n+\t *\n+\t * Guarded by CONFIG_NUMA because on !CONFIG_NUMA the single-node\n+\t * node_state() stub returns true for node 0, which would make\n+\t * all folios non-pinnable via the false-returning stub.\n+\t */\n+#ifdef CONFIG_NUMA\n+\tif (node_state(folio_nid(folio), N_MEMORY_PRIVATE))\n+\t\treturn node_private_allows_longterm_pin(folio_nid(folio));\n+#endif\n+\n \t/* Otherwise, non-movable zone folios can be pinned. */\n \treturn !folio_is_zone_movable(folio);\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex fe0336773ddb..7a7438fb9eda 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -144,6 +144,8 @@ struct node_private_ops {\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n /* Allow compaction to run on the node.  Service must start kcompactd. */\n #define NP_OPS_COMPACTION\t\tBIT(6)\n+/* Allow longterm DMA pinning (RDMA, VFIO, etc.) of folios on this node */\n+#define NP_OPS_LONGTERM_PIN\t\tBIT(7)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about notifying private-node services of hardware errors on their nodes by adding a memory_failure callback to struct node_private_ops, which will be called after TestSetPageHWPoison succeeds and before get_hwpoison_page. The kernel always proceeds with standard hwpoison handling for online pages.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "added callback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a void memory_failure notification callback to struct\nnode_private_ops so services managing N_MEMORY_PRIVATE nodes notified\nwhen a page on their node experiences a hardware error.\n\nThe callback is notification only -- the kernel always proceeds with\nstandard hwpoison handling for online pages.\n\nThe notification hook fires after TestSetPageHWPoison succeeds and\nbefore get_hwpoison_page giving the service a chance to clean up.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 16 ++++++++++++++++\n mm/memory-failure.c          | 15 +++++++++++++++\n 3 files changed, 37 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7a7438fb9eda..d2669f68ac20 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -113,6 +113,10 @@ struct node_reclaim_policy {\n  *   watermark_boost lifecycle (kswapd will not clear it).\n  *   If NULL, normal boost policy applies.\n  *\n+ * @memory_failure: Notification of hardware error on a page on this node.\n+ *   [folio-referenced callback]\n+ *   Notification only, kernel always handles the failure.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -127,6 +131,8 @@ struct node_private_ops {\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n \tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n+\tvoid (*memory_failure)(struct folio *folio, unsigned long pfn,\n+\t\t\t       int mf_flags);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex db32cb2d7a29..64467ca774f1 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1608,6 +1608,22 @@ static inline void node_private_reclaim_policy(int nid,\n }\n #endif\n \n+static inline void folio_managed_memory_failure(struct folio *folio,\n+\t\t\t\t\t\tunsigned long pfn,\n+\t\t\t\t\t\tint mf_flags)\n+{\n+\t/* Zone device pages handle memory failure via dev_pagemap_ops */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn;\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->memory_failure)\n+\t\t\tops->memory_failure(folio, pfn, mf_flags);\n+\t}\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/memory-failure.c b/mm/memory-failure.c\nindex c80c2907da33..79c91d44ec1e 100644\n--- a/mm/memory-failure.c\n+++ b/mm/memory-failure.c\n@@ -2379,6 +2379,15 @@ int memory_failure(unsigned long pfn, int flags)\n \t\tgoto unlock_mutex;\n \t}\n \n+\t/*\n+\t * Notify private-node services about the hardware error so they\n+\t * can update internal tracking (e.g., CXL poison lists, stop\n+\t * demoting to failing DIMMs).  This is notification only -- the\n+\t * kernel proceeds with standard hwpoison handling regardless.\n+\t */\n+\tif (unlikely(page_is_private_managed(p)))\n+\t\tfolio_managed_memory_failure(page_folio(p), pfn, flags);\n+\n \t/*\n \t * We need/can do nothing about count=0 pages.\n \t * 1) it's a free page, and therefore in safe hand:\n@@ -2825,6 +2834,12 @@ static int soft_offline_in_use_page(struct page *page)\n \t\treturn 0;\n \t}\n \n+\tif (!folio_managed_allows_migrate(folio)) {\n+\t\tpr_info(\"%#lx: cannot migrate private node folio\\n\", pfn);\n+\t\tfolio_put(folio);\n+\t\treturn -EBUSY;\n+\t}\n+\n \tisolated = isolate_folio_to_list(folio, &pagelist);\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the ordering of registering private regions and hotplugging memory, explaining that their new function combines these two steps to ensure proper ordering. The function first registers the private region, then hotplugs the memory, and on failure, unregisters the private region. They also added checks for migration support and online status when removing the last of memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a new function for drivers to hotplug memory as N_MEMORY_PRIVATE.\n\nThis function combines node_private_region_register() with\n__add_memory_driver_managed() to ensure proper ordering:\n\n1. Register the private region first (sets private node context)\n2. Then hotplug the memory (sets N_MEMORY_PRIVATE)\n3. On failure, unregister the private region to avoid leaving the\n   node in an inconsistent state.\n\nWhen the last of memory is removed, hotplug also removes the private\nnode context. If migration is not supported and the node is still\nonline, fire a warning (likely bug in the driver).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory_hotplug.h |  11 +++\n include/linux/mmzone.h         |  12 ++++\n mm/memory_hotplug.c            | 122 ++++++++++++++++++++++++++++++---\n 3 files changed, 135 insertions(+), 10 deletions(-)\n\ndiff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h\nindex 1f19f08552ea..e5abade9450a 100644\n--- a/include/linux/memory_hotplug.h\n+++ b/include/linux/memory_hotplug.h\n@@ -293,6 +293,7 @@ extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n extern int remove_memory(u64 start, u64 size);\n extern void __remove_memory(u64 start, u64 size);\n extern int offline_and_remove_memory(u64 start, u64 size);\n+extern int offline_and_remove_private_memory(int nid, u64 start, u64 size);\n \n #else\n static inline void try_offline_node(int nid) {}\n@@ -309,6 +310,12 @@ static inline int remove_memory(u64 start, u64 size)\n }\n \n static inline void __remove_memory(u64 start, u64 size) {}\n+\n+static inline int offline_and_remove_private_memory(int nid, u64 start,\n+\t\t\t\t\t\t    u64 size)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n@@ -326,6 +333,10 @@ int __add_memory_driver_managed(int nid, u64 start, u64 size,\n extern int add_memory_driver_managed(int nid, u64 start, u64 size,\n \t\t\t\t     const char *resource_name,\n \t\t\t\t     mhp_t mhp_flags);\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np);\n extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,\n \t\t\t\t   unsigned long nr_pages,\n \t\t\t\t   struct vmem_altmap *altmap, int migratetype,\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 992eb1c5a2c6..cc532b67ad3f 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1524,6 +1524,18 @@ typedef struct pglist_data {\n #endif\n } pg_data_t;\n \n+#ifdef CONFIG_NUMA\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn pgdat->private;\n+}\n+#else\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn false;\n+}\n+#endif\n+\n #define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n #define node_spanned_pages(nid)\t(NODE_DATA(nid)->node_spanned_pages)\n \ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex d2dc527bd5b0..9d72f44a30dc 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1173,8 +1174,7 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tmove_pfn_range_to_zone(zone, pfn, nr_pages, NULL, MIGRATE_MOVABLE,\n \t\t\t       true);\n \n-\tif (!node_state(nid, N_MEMORY)) {\n-\t\t/* Adding memory to the node for the first time */\n+\tif (!node_state(nid, N_MEMORY) && !node_state(nid, N_MEMORY_PRIVATE)) {\n \t\tnode_arg.nid = nid;\n \t\tret = node_notify(NODE_ADDING_FIRST_MEMORY, &node_arg);\n \t\tret = notifier_to_errno(ret);\n@@ -1208,8 +1208,12 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tonline_pages_range(pfn, nr_pages);\n \tadjust_present_page_count(pfn_to_page(pfn), group, nr_pages);\n \n-\tif (node_arg.nid >= 0)\n-\t\tnode_set_state(nid, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (pgdat_is_private(NODE_DATA(nid)))\n+\t\t\tnode_set_state(nid, N_MEMORY_PRIVATE);\n+\t\telse\n+\t\t\tnode_set_state(nid, N_MEMORY);\n+\t}\n \tif (need_zonelists_rebuild)\n \t\tbuild_all_zonelists(NULL);\n \n@@ -1227,8 +1231,14 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t/* reinitialise watermarks and update pcp limits */\n \tinit_per_zone_wmark_min();\n \n-\tkswapd_run(nid);\n-\tkcompactd_run(nid);\n+\t/*\n+\t * Don't start reclaim/compaction daemons for private nodes.\n+\t * Private node services will decide whether to start these services.\n+\t */\n+\tif (!pgdat_is_private(NODE_DATA(nid))) {\n+\t\tkswapd_run(nid);\n+\t\tkcompactd_run(nid);\n+\t}\n \n \tif (node_arg.nid >= 0)\n \t\t/* First memory added successfully. Notify consumers. */\n@@ -1722,6 +1732,54 @@ int add_memory_driver_managed(int nid, u64 start, u64 size,\n }\n EXPORT_SYMBOL_GPL(add_memory_driver_managed);\n \n+/**\n+ * add_private_memory_driver_managed - add driver-managed N_MEMORY_PRIVATE memory\n+ * @nid: NUMA node ID (or memory group ID when MHP_NID_IS_MGID is set)\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ * @resource_name: \"System RAM ($DRIVER)\" format\n+ * @mhp_flags: Memory hotplug flags\n+ * @online_type: MMOP_* online type\n+ * @np: Driver-owned node_private structure (owner, refcount)\n+ *\n+ * Registers node_private first, then hotplugs the memory.\n+ *\n+ * On failure, unregisters the node_private.\n+ */\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np)\n+{\n+\tstruct memory_group *group;\n+\tint real_nid = nid;\n+\tint rc;\n+\n+\tif (!np)\n+\t\treturn -EINVAL;\n+\n+\tif (mhp_flags & MHP_NID_IS_MGID) {\n+\t\tgroup = memory_group_find_by_id(nid);\n+\t\tif (!group)\n+\t\t\treturn -EINVAL;\n+\t\treal_nid = group->nid;\n+\t}\n+\n+\trc = node_private_register(real_nid, np);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\trc = __add_memory_driver_managed(nid, start, size, resource_name,\n+\t\t\t\t\t mhp_flags, online_type);\n+\tif (rc) {\n+\t\tnode_private_unregister(real_nid);\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(add_private_memory_driver_managed);\n+\n /*\n  * Platforms should define arch_get_mappable_range() that provides\n  * maximum possible addressable physical memory range for which the\n@@ -1872,6 +1930,15 @@ static void do_migrate_range(unsigned long start_pfn, unsigned long end_pfn)\n \t\t\tgoto put_folio;\n \t\t}\n \n+\t\t/* Private nodes w/o migration must ensure folios are offline */\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION)) {\n+\t\t\tWARN_ONCE(1, \"hot-unplug on non-migratable node %d pfn %lx\\n\",\n+\t\t\t\t  folio_nid(folio), pfn);\n+\t\t\tpfn = folio_pfn(folio) + folio_nr_pages(folio) - 1;\n+\t\t\tgoto put_folio;\n+\t\t}\n+\n \t\tif (!isolate_folio_to_list(folio, &source)) {\n \t\t\tif (__ratelimit(&migrate_rs)) {\n \t\t\t\tpr_warn(\"failed to isolate pfn %lx\\n\",\n@@ -2014,8 +2081,8 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \n \t/*\n \t * Check whether the node will have no present pages after we offline\n-\t * 'nr_pages' more. If so, we know that the node will become empty, and\n-\t * so we will clear N_MEMORY for it.\n+\t * 'nr_pages' more. If so, send pre-notification for last memory removal.\n+\t * We will clear N_MEMORY(_PRIVATE) if this is the case.\n \t */\n \tif (nr_pages >= pgdat->node_present_pages) {\n \t\tnode_arg.nid = node;\n@@ -2108,8 +2175,12 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * Make sure to mark the node as memory-less before rebuilding the zone\n \t * list. Otherwise this node would still appear in the fallback lists.\n \t */\n-\tif (node_arg.nid >= 0)\n-\t\tnode_clear_state(node, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (node_state(node, N_MEMORY))\n+\t\t\tnode_clear_state(node, N_MEMORY);\n+\t\telse if (node_state(node, N_MEMORY_PRIVATE))\n+\t\t\tnode_clear_state(node, N_MEMORY_PRIVATE);\n+\t}\n \tif (!populated_zone(zone)) {\n \t\tzone_pcp_reset(zone);\n \t\tbuild_all_zonelists(NULL);\n@@ -2461,4 +2532,35 @@ int offline_and_remove_memory(u64 start, u64 size)\n \treturn rc;\n }\n EXPORT_SYMBOL_GPL(offline_and_remove_memory);\n+\n+/**\n+ * offline_and_remove_private_memory - offline, remove, and unregister private memory\n+ * @nid: NUMA node ID of the private memory\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ *\n+ * Counterpart to add_private_memory_driver_managed().  Offlines and removes\n+ * the memory range, then attempts to unregister the node_private.\n+ *\n+ * offline_and_remove_memory() clears N_MEMORY_PRIVATE when the last block\n+ * is offlined, which allows node_private_unregister() to clear the\n+ * pgdat->node_private pointer.  If other private memory ranges remain on\n+ * the node, node_private_unregister() returns -EBUSY (N_MEMORY_PRIVATE\n+ * is still set) and the node_private remains registered.\n+ *\n+ * Return: 0 on full success (memory removed and node_private unregistered),\n+ *         -EBUSY if memory was removed but node still has other private memory,\n+ *         other negative error code if offline/remove failed.\n+ */\n+int offline_and_remove_private_memory(int nid, u64 start, u64 size)\n+{\n+\tint rc;\n+\n+\trc = offline_and_remove_memory(start, size);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\treturn node_private_unregister(nid);\n+}\n+EXPORT_SYMBOL_GPL(offline_and_remove_private_memory);\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the lack of locking in the swapoff path, acknowledged that the per-vswap spinlock needs to be dropped before calling try_to_unmap(), and agreed to restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CRAM (Compressed RAM) subsystem that manages folios demoted\nto N_MEMORY_PRIVATE nodes via the standard kernel LRU.\n\nWe limit entry into CRAM by demotion in to provide devices a way for\ndrivers to close access - which allows the system to stabiliz under\nmemory pressure (the device can run out of real memory when compression\nratios drop too far).\n\nWe utilize write-protect to prevent unbounded writes to compressed\nmemory pages, which may cause run-away compression ratio loss without\na reliable way to prevent the degenerate case (cascading poisons).\n\nCRAM provides the bridge between the mm/ private node infrastructure\nand compressed memory hardware.  Folios are aged by kswapd on the\nprivate node and reclaimed to swap when the device signals pressure.\n\nWrite faults trigger promotion back to regular DRAM via the\nops->handle_fault callback.\n\nDevice pressure is communicated via watermark_boost on the private\nnode's zone.\n\nCRAM registers node_private_ops with:\n  - handle_fault:   promotes folio back to DRAM on write\n  - migrate_to:     custom demotion to the CRAM node\n  - folio_migrate:  (no-op)\n  - free_folio:     zeroes pages on free to scrub stale data\n  - reclaim_policy: provides mayswap/writeback/boost overrides\n  - flags: NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n\t   NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE\n           NP_OPS_RECLAIM\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cram.h |  66 ++++++\n mm/Kconfig           |  10 +\n mm/Makefile          |   1 +\n mm/cram.c            | 508 +++++++++++++++++++++++++++++++++++++++++++\n 4 files changed, 585 insertions(+)\n create mode 100644 include/linux/cram.h\n create mode 100644 mm/cram.c\n\ndiff --git a/include/linux/cram.h b/include/linux/cram.h\nnew file mode 100644\nindex 000000000000..a3c10362fd4f\n--- /dev/null\n+++ b/include/linux/cram.h\n@@ -0,0 +1,66 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_CRAM_H\n+#define _LINUX_CRAM_H\n+\n+#include <linux/mm_types.h>\n+\n+struct folio;\n+struct list_head;\n+struct vm_fault;\n+\n+#define CRAM_PRESSURE_MAX\t1000\n+\n+/**\n+ * cram_flush_cb_t - Driver callback invoked when a folio on a private node\n+ *                   is freed (refcount reaches zero).\n+ * @folio: the folio being freed\n+ * @private: opaque driver data passed at registration\n+ *\n+ * Return:\n+ *   0: Flush resolved -- page should return to buddy allocator (e.g., flush\n+ *      record bit was set, meaning this free is from our own flush resolution)\n+ *   1: Page deferred -- driver took a reference, page will be flushed later.\n+ *      Do NOT return to buddy allocator.\n+ *   2: Buffer full -- caller should zero the page and return to buddy.\n+ */\n+typedef int (*cram_flush_cb_t)(struct folio *folio, void *private);\n+\n+#ifdef CONFIG_CRAM\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data);\n+int cram_unregister_private_node(int nid);\n+int cram_unpurge(int nid);\n+void cram_set_pressure(int nid, unsigned int pressure);\n+void cram_clear_pressure(int nid);\n+\n+#else /* !CONFIG_CRAM */\n+\n+static inline int cram_register_private_node(int nid, void *owner,\n+\t\t\t\t\t     cram_flush_cb_t flush_cb,\n+\t\t\t\t\t     void *flush_data)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unregister_private_node(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unpurge(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+}\n+\n+static inline void cram_clear_pressure(int nid)\n+{\n+}\n+\n+#endif /* CONFIG_CRAM */\n+\n+#endif /* _LINUX_CRAM_H */\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex bd0ea5454af8..054462b954d8 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -662,6 +662,16 @@ config MIGRATION\n config DEVICE_MIGRATION\n \tdef_bool MIGRATION && ZONE_DEVICE\n \n+config CRAM\n+\tbool \"Compressed RAM - private node memory management\"\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\tdepends on MEMORY_HOTPLUG\n+\thelp\n+\t  Enables management of N_MEMORY_PRIVATE nodes for compressed RAM\n+\t  and similar use cases. Provides demotion, promotion, and lifecycle\n+\t  management for private memory nodes.\n+\n config ARCH_ENABLE_HUGEPAGE_MIGRATION\n \tbool\n \ndiff --git a/mm/Makefile b/mm/Makefile\nindex 2d0570a16e5b..0e1421512643 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -98,6 +98,7 @@ obj-$(CONFIG_MEMTEST)\t\t+= memtest.o\n obj-$(CONFIG_MIGRATION) += migrate.o\n obj-$(CONFIG_NUMA) += memory-tiers.o\n obj-$(CONFIG_DEVICE_MIGRATION) += migrate_device.o\n+obj-$(CONFIG_CRAM) += cram.o\n obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o\n obj-$(CONFIG_PAGE_COUNTER) += page_counter.o\n obj-$(CONFIG_LIVEUPDATE) += memfd_luo.o\ndiff --git a/mm/cram.c b/mm/cram.c\nnew file mode 100644\nindex 000000000000..6709e61f5b9d\n--- /dev/null\n+++ b/mm/cram.c\n@@ -0,0 +1,508 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * mm/cram.c - Compressed RAM / private node memory management\n+ *\n+ * Copyright 2026 Meta Technologies Inc.\n+ *   Author: Gregory Price <gourry@gourry.net>\n+ *\n+ * Manages folios demoted to N_MEMORY_PRIVATE nodes via the standard kernel\n+ * LRU.  Folios are aged by kswapd on the private node and reclaimed to swap\n+ * (demotion is suppressed for private nodes).  Write faults trigger promotion\n+ * back to regular DRAM via the ops->handle_fault callback.\n+ *\n+ * All reclaim/demotion uses the standard vmscan infrastructure. Device pressure\n+ * is communicated via watermark_boost on the private node's zone.\n+ */\n+\n+#include <linux/atomic.h>\n+#include <linux/cpuset.h>\n+#include <linux/cram.h>\n+#include <linux/errno.h>\n+#include <linux/gfp.h>\n+#include <linux/jiffies.h>\n+#include <linux/highmem.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/list.h>\n+#include <linux/migrate.h>\n+#include <linux/mm.h>\n+#include <linux/huge_mm.h>\n+#include <linux/mmzone.h>\n+#include <linux/mutex.h>\n+#include <linux/nodemask.h>\n+#include <linux/node_private.h>\n+#include <linux/pagemap.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+#include <linux/swap.h>\n+\n+#include \"internal.h\"\n+\n+struct cram_node {\n+\tvoid\t\t*owner;\n+\tbool\t\tpurged;\t\t/* node is being torn down */\n+\tunsigned int\tpressure;\n+\trefcount_t\trefcount;\n+\tcram_flush_cb_t\tflush_cb;\t/* optional driver flush callback */\n+\tvoid\t\t*flush_data;\t/* opaque data for flush_cb */\n+};\n+\n+static struct cram_node *cram_nodes[MAX_NUMNODES];\n+static DEFINE_MUTEX(cram_mutex);\n+\n+static inline bool cram_valid_nid(int nid)\n+{\n+\treturn nid >= 0 && nid < MAX_NUMNODES;\n+}\n+\n+static inline struct cram_node *get_cram_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn NULL;\n+\n+\trcu_read_lock();\n+\tcn = rcu_dereference(cram_nodes[nid]);\n+\tif (cn && !refcount_inc_not_zero(&cn->refcount))\n+\t\tcn = NULL;\n+\trcu_read_unlock();\n+\n+\treturn cn;\n+}\n+\n+static inline void put_cram_node(struct cram_node *cn)\n+{\n+\tif (cn)\n+\t\trefcount_dec(&cn->refcount);\n+}\n+\n+static void cram_zero_folio(struct folio *folio)\n+{\n+\tunsigned int i, nr = folio_nr_pages(folio);\n+\n+\tif (want_init_on_free())\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr; i++)\n+\t\tclear_highpage(folio_page(folio, i));\n+}\n+\n+static bool cram_free_folio_cb(struct folio *folio)\n+{\n+\tint nid = folio_nid(folio);\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\tgoto zero_and_free;\n+\n+\tif (!cn->flush_cb)\n+\t\tgoto zero_and_free_put;\n+\n+\tret = cn->flush_cb(folio, cn->flush_data);\n+\tput_cram_node(cn);\n+\n+\tswitch (ret) {\n+\tcase 0:\n+\t\t/* Flush resolved: return to buddy (already zeroed by device) */\n+\t\treturn false;\n+\tcase 1:\n+\t\t/* Deferred: driver holds a ref, do not free to buddy */\n+\t\treturn true;\n+\tcase 2:\n+\tdefault:\n+\t\t/* Buffer full or unknown: zero locally, return to buddy */\n+\t\tgoto zero_and_free;\n+\t}\n+\n+zero_and_free_put:\n+\tput_cram_node(cn);\n+zero_and_free:\n+\tcram_zero_folio(folio);\n+\treturn false;\n+}\n+\n+static struct folio *alloc_cram_folio(struct folio *src, unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_PRIVATE | __GFP_KSWAPD_RECLAIM |\n+\t\t     __GFP_HIGHMEM | __GFP_MOVABLE |\n+\t\t     __GFP_NOWARN | __GFP_NORETRY;\n+\n+\t/* Stop allocating if backpressure fired mid-batch */\n+\tif (node_private_migration_blocked(nid))\n+\t\treturn NULL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc_node(gfp, order, nid);\n+}\n+\n+static void cram_put_new_folio(struct folio *folio, unsigned long private)\n+{\n+\tcram_zero_folio(folio);\n+\tfolio_put(folio);\n+}\n+\n+/*\n+ * Allocate a DRAM folio for promotion out of a private node.\n+ *\n+ * Unlike alloc_migration_target(), this does NOT strip __GFP_RECLAIM for\n+ * large folios, the generic helper does that because THP allocations are\n+ * opportunistic, but promotion from a private node is mandatory: the page\n+ * MUST move to DRAM or the process cannot make forward progress.\n+ *\n+ * __GFP_RETRY_MAYFAIL tells the allocator to try hard (multiple reclaim\n+ * rounds, wait for writeback) before giving up.\n+ */\n+static struct folio *alloc_cram_promote_folio(struct folio *src,\n+\t\t\t\t\t      unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc(gfp, order, nid, NULL);\n+}\n+\n+static int cram_migrate_to(struct list_head *demote_folios, int to_nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason,\n+\t\t\t   unsigned int *nr_succeeded)\n+{\n+\tstruct cram_node *cn;\n+\tunsigned int nr_success = 0;\n+\tint ret = 0;\n+\n+\tcn = get_cram_node(to_nid);\n+\tif (!cn)\n+\t\treturn -ENODEV;\n+\n+\tif (cn->purged) {\n+\t\tret = -ENODEV;\n+\t\tgoto out;\n+\t}\n+\n+\t/* Block new demotions at maximum pressure */\n+\tif (READ_ONCE(cn->pressure) >= CRAM_PRESSURE_MAX) {\n+\t\tret = -ENOSPC;\n+\t\tgoto out;\n+\t}\n+\n+\tret = migrate_pages(demote_folios, alloc_cram_folio, cram_put_new_folio,\n+\t\t\t    (unsigned long)to_nid, mode, reason,\n+\t\t\t    &nr_success);\n+\n+\t/*\n+\t * migrate_folio_move() calls folio_add_lru() for each migrated\n+\t * folio, but that only adds the folio to a per-CPU batch, \n+\t * PG_lru is not set until the batch is drained.  Drain now so\n+\t * that cram_fault() can isolate these folios immediately.\n+\t *\n+\t * Use lru_add_drain_all() because migrate_pages() may process\n+\t * folios across CPUs, and the local drain might miss batches\n+\t * filled on other CPUs.\n+\t */\n+\tif (nr_success)\n+\t\tlru_add_drain_all();\n+out:\n+\tput_cram_node(cn);\n+\tif (nr_succeeded)\n+\t\t*nr_succeeded = nr_success;\n+\treturn ret;\n+}\n+\n+static void cram_release_ptl(struct vm_fault *vmf, enum pgtable_level level)\n+{\n+\tif (level == PGTABLE_LEVEL_PTE)\n+\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n+\telse\n+\t\tspin_unlock(vmf->ptl);\n+}\n+\n+static vm_fault_t cram_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t     enum pgtable_level level)\n+{\n+\tstruct folio *f, *f2;\n+\tstruct cram_node *cn;\n+\tunsigned int nr_succeeded = 0;\n+\tint nid;\n+\tLIST_HEAD(folios);\n+\n+\tnid = folio_nid(folio);\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn) {\n+\t\tcram_release_ptl(vmf, level);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Isolate from LRU while holding PTL.  This serializes against\n+\t * other CPUs faulting on the same folio: only one CPU can clear\n+\t * PG_lru under the PTL, and it proceeds to migration.  Other\n+\t * CPUs find the folio already isolated and bail out, preventing\n+\t * the refcount pile-up that causes migrate_pages() to fail with\n+\t * -EAGAIN.\n+\t *\n+\t * No explicit folio_get() is needed: the page table entry holds\n+\t * a reference (we still hold PTL), and folio_isolate_lru() takes\n+\t * its own reference.  This matches do_numa_page()'s pattern.\n+\t *\n+\t * PG_lru should already be set: cram_migrate_to() drains per-CPU\n+\t * LRU batches after migration, and the failure path below\n+\t * drains after putback.\n+\t */\n+\tif (!folio_isolate_lru(folio)) {\n+\t\tput_cram_node(cn);\n+\t\tcram_release_ptl(vmf, level);\n+\t\tcond_resched();\n+\t\treturn 0;\n+\t}\n+\n+\t/* Folio isolated, release PTL, proceed to migration */\n+\tcram_release_ptl(vmf, level);\n+\n+\tnode_stat_mod_folio(folio,\n+\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(folio),\n+\t\t\t    folio_nr_pages(folio));\n+\tlist_add(&folio->lru, &folios);\n+\n+\tmigrate_pages(&folios, alloc_cram_promote_folio, NULL,\n+\t\t      (unsigned long)numa_node_id(),\n+\t\t      MIGRATE_SYNC, MR_NUMA_MISPLACED, &nr_succeeded);\n+\n+\t/* Put failed folios back on LRU; retry on next fault */\n+\tlist_for_each_entry_safe(f, f2, &folios, lru) {\n+\t\tlist_del(&f->lru);\n+\t\tnode_stat_mod_folio(f,\n+\t\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(f),\n+\t\t\t\t    -folio_nr_pages(f));\n+\t\tfolio_putback_lru(f);\n+\t}\n+\n+\t/*\n+\t * If migration failed, folio_putback_lru() batched the folio\n+\t * into this CPU's per-CPU LRU cache (PG_lru not yet set).\n+\t * Drain now so the folio is immediately visible on the LRU,\n+\t * the next fault can then isolate it without an IPI storm\n+\t * via lru_add_drain_all().\n+\t *\n+\t * Return VM_FAULT_RETRY after releasing the fault lock so the\n+\t * arch handler retries from scratch.  Without this, returning 0\n+\t * causes a tight livelock: the process immediately re-faults on\n+\t * the same write-protected entry, alloc fails again, and\n+\t * VM_FAULT_OOM eventually leaks out through a stale path.\n+\t * VM_FAULT_RETRY gives the system breathing room to reclaim.\n+\t */\n+\tif (!nr_succeeded) {\n+\t\tlru_add_drain();\n+\t\tcond_resched();\n+\t\tput_cram_node(cn);\n+\t\trelease_fault_lock(vmf);\n+\t\treturn VM_FAULT_RETRY;\n+\t}\n+\n+\tcond_resched();\n+\tput_cram_node(cn);\n+\treturn 0;\n+}\n+\n+static void cram_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static void cram_reclaim_policy(int nid, struct node_reclaim_policy *policy)\n+{\n+\tpolicy->may_swap = true;\n+\tpolicy->may_writepage = true;\n+\tpolicy->managed_watermarks = true;\n+}\n+\n+static vm_fault_t cram_handle_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t    enum pgtable_level level)\n+{\n+\treturn cram_fault(folio, vmf, level);\n+}\n+\n+static const struct node_private_ops cram_ops = {\n+\t.handle_fault\t\t= cram_handle_fault,\n+\t.migrate_to\t\t= cram_migrate_to,\n+\t.folio_migrate\t\t= cram_folio_migrate,\n+\t.free_folio\t\t= cram_free_folio_cb,\n+\t.reclaim_policy\t\t= cram_reclaim_policy,\n+\t.flags\t\t\t= NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n+\t\t\t\t  NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE |\n+\t\t\t\t  NP_OPS_RECLAIM,\n+};\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data)\n+{\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (cn) {\n+\t\tif (cn->owner != owner) {\n+\t\t\tmutex_unlock(&cram_mutex);\n+\t\t\treturn -EBUSY;\n+\t\t}\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn 0;\n+\t}\n+\n+\tcn = kzalloc(sizeof(*cn), GFP_KERNEL);\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENOMEM;\n+\t}\n+\n+\tcn->owner = owner;\n+\tcn->pressure = 0;\n+\tcn->flush_cb = flush_cb;\n+\tcn->flush_data = flush_data;\n+\trefcount_set(&cn->refcount, 1);\n+\n+\tret = node_private_set_ops(nid, &cram_ops);\n+\tif (ret) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\tkfree(cn);\n+\t\treturn ret;\n+\t}\n+\n+\trcu_assign_pointer(cram_nodes[nid], cn);\n+\n+\t/* Start kswapd on the private node for LRU aging and reclaim */\n+\tkswapd_run(nid);\n+\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* Now that ops->migrate_to is set, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_register_private_node);\n+\n+int cram_unregister_private_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tkswapd_stop(nid);\n+\n+\tWARN_ON(node_private_clear_ops(nid, &cram_ops));\n+\trcu_assign_pointer(cram_nodes[nid], NULL);\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* ops->migrate_to cleared, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\n+\tsynchronize_rcu();\n+\twhile (!refcount_dec_if_one(&cn->refcount))\n+\t\tcond_resched();\n+\tkfree(cn);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unregister_private_node);\n+\n+int cram_unpurge(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tcn->purged = false;\n+\n+\tmutex_unlock(&cram_mutex);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unpurge);\n+\n+void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+\tstruct cram_node *cn;\n+\tstruct node_private *np;\n+\tstruct zone *zone;\n+\tunsigned long managed, boost;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\treturn;\n+\n+\tif (pressure > CRAM_PRESSURE_MAX)\n+\t\tpressure = CRAM_PRESSURE_MAX;\n+\n+\tWRITE_ONCE(cn->pressure, pressure);\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\t/* Block demotions only at maximum pressure */\n+\tif (np)\n+\t\tWRITE_ONCE(np->migration_blocked,\n+\t\t\t   pressure >= CRAM_PRESSURE_MAX);\n+\trcu_read_unlock();\n+\n+\tzone = NULL;\n+\tfor (int i = 0; i < MAX_NR_ZONES; i++) {\n+\t\tstruct zone *z = &NODE_DATA(nid)->node_zones[i];\n+\n+\t\tif (zone_managed_pages(z) > 0) {\n+\t\t\tzone = z;\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+\tif (!zone) {\n+\t\tput_cram_node(cn);\n+\t\treturn;\n+\t}\n+\tmanaged = zone_managed_pages(zone);\n+\n+\t/* Boost proportional to pressure. 0:no boost, 1000:full managed */\n+\tboost = (managed * (unsigned long)pressure) / CRAM_PRESSURE_MAX;\n+\tWRITE_ONCE(zone->watermark_boost, boost);\n+\n+\tif (boost) {\n+\t\tset_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);\n+\t\twakeup_kswapd(zone, GFP_KERNEL, 0, ZONE_MOVABLE);\n+\t}\n+\n+\tput_cram_node(cn);\n+}\n+EXPORT_SYMBOL_GPL(cram_set_pressure);\n+\n+void cram_clear_pressure(int nid)\n+{\n+\tcram_set_pressure(nid, 0);\n+}\n+EXPORT_SYMBOL_GPL(cram_clear_pressure);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the need for a sysram region to directly perform memory hotplug operations, which would eliminate the intermediate dax_region/dax device layer. The author agrees that this feature is necessary and explains how it will work, including its key features such as supporting memory tier integration and automatically hotplugging memory on probe if online type is configured.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed to implement a new feature",
                "explained the benefits of the feature"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CXL sysram region for direct memory hotplug of CXL RAM regions.\n\nThis region eliminates the intermediate dax_region/dax device layer by\ndirectly performing memory hotplug operations.\n\nKey features:\n- Supports memory tier integration for proper NUMA placement\n- Uses the CXL_SYSRAM_ONLINE_* Kconfig options for default online type\n- Automatically hotplugs memory on probe if online type is configured\n- Will be extended to support private memory nodes in the future\n\nThe driver registers a sysram_regionN device as a child of the CXL\nregion, managing the memory hotplug lifecycle through device add/remove.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/Makefile        |   1 +\n drivers/cxl/core/core.h          |   4 +\n drivers/cxl/core/port.c          |   2 +\n drivers/cxl/core/region_sysram.c | 351 +++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h                |  48 +++++\n 5 files changed, 406 insertions(+)\n create mode 100644 drivers/cxl/core/region_sysram.c\n\ndiff --git a/drivers/cxl/core/Makefile b/drivers/cxl/core/Makefile\nindex d3ec8aea64c5..d7ce52c50810 100644\n--- a/drivers/cxl/core/Makefile\n+++ b/drivers/cxl/core/Makefile\n@@ -18,6 +18,7 @@ cxl_core-$(CONFIG_TRACING) += trace.o\n cxl_core-$(CONFIG_CXL_REGION) += region.o\n cxl_core-$(CONFIG_CXL_REGION) += region_dax.o\n cxl_core-$(CONFIG_CXL_REGION) += region_pmem.o\n+cxl_core-$(CONFIG_CXL_REGION) += region_sysram.o\n cxl_core-$(CONFIG_CXL_MCE) += mce.o\n cxl_core-$(CONFIG_CXL_FEATURES) += features.o\n cxl_core-$(CONFIG_CXL_EDAC_MEM_FEATURES) += edac.o\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 6e1f695fd155..973bbcae43f7 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -35,6 +35,7 @@ extern struct device_attribute dev_attr_delete_region;\n extern struct device_attribute dev_attr_region;\n extern const struct device_type cxl_pmem_region_type;\n extern const struct device_type cxl_dax_region_type;\n+extern const struct device_type cxl_sysram_type;\n extern const struct device_type cxl_region_type;\n \n int cxl_decoder_detach(struct cxl_region *cxlr,\n@@ -46,6 +47,7 @@ int cxl_decoder_detach(struct cxl_region *cxlr,\n #define SET_CXL_REGION_ATTR(x) (&dev_attr_##x.attr),\n #define CXL_PMEM_REGION_TYPE(x) (&cxl_pmem_region_type)\n #define CXL_DAX_REGION_TYPE(x) (&cxl_dax_region_type)\n+#define CXL_SYSRAM_TYPE(x) (&cxl_sysram_type)\n int cxl_region_init(void);\n void cxl_region_exit(void);\n int cxl_get_poison_by_endpoint(struct cxl_port *port);\n@@ -54,6 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\n@@ -88,6 +91,7 @@ static inline void cxl_region_exit(void)\n #define SET_CXL_REGION_ATTR(x)\n #define CXL_PMEM_REGION_TYPE(x) NULL\n #define CXL_DAX_REGION_TYPE(x) NULL\n+#define CXL_SYSRAM_TYPE(x) NULL\n #endif\n \n struct cxl_send_command;\ndiff --git a/drivers/cxl/core/port.c b/drivers/cxl/core/port.c\nindex 5c82e6f32572..d6e82b3c2b64 100644\n--- a/drivers/cxl/core/port.c\n+++ b/drivers/cxl/core/port.c\n@@ -66,6 +66,8 @@ static int cxl_device_id(const struct device *dev)\n \t\treturn CXL_DEVICE_PMEM_REGION;\n \tif (dev->type == CXL_DAX_REGION_TYPE())\n \t\treturn CXL_DEVICE_DAX_REGION;\n+\tif (dev->type == CXL_SYSRAM_TYPE())\n+\t\treturn CXL_DEVICE_SYSRAM;\n \tif (is_cxl_port(dev)) {\n \t\tif (is_cxl_root(to_cxl_port(dev)))\n \t\t\treturn CXL_DEVICE_ROOT;\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nnew file mode 100644\nindex 000000000000..47a415deb352\n--- /dev/null\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -0,0 +1,351 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Sysram Region - Direct memory hotplug for CXL RAM regions\n+ *\n+ * This interface directly performs memory hotplug for CXL RAM regions,\n+ * eliminating the indirection through DAX.\n+ */\n+\n+#include <linux/memory_hotplug.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/memory.h>\n+#include <linux/device.h>\n+#include <linux/slab.h>\n+#include <linux/mm.h>\n+#include <cxlmem.h>\n+#include <cxl.h>\n+#include \"core.h\"\n+\n+static const char *sysram_res_name = \"System RAM (CXL)\";\n+\n+/**\n+ * cxl_region_find_sysram - Find the sysram device associated with a region\n+ * @cxlr: The CXL region\n+ *\n+ * Finds and returns the sysram child device of a CXL region.\n+ * The caller must release the device reference with put_device()\n+ * when done with the returned pointer.\n+ *\n+ * Return: Pointer to cxl_sysram, or NULL if not found\n+ */\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram;\n+\tstruct device *sdev;\n+\tchar sname[32];\n+\n+\tsnprintf(sname, sizeof(sname), \"sysram_region%d\", cxlr->id);\n+\tsdev = device_find_child_by_name(&cxlr->dev, sname);\n+\tif (!sdev)\n+\t\treturn NULL;\n+\n+\tsysram = to_cxl_sysram(sdev);\n+\treturn sysram;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_region_find_sysram, \"CXL\");\n+\n+static int sysram_get_numa_node(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_region_params *p = &cxlr->params;\n+\tint nid;\n+\n+\tnid = phys_to_target_node(p->res->start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(p->res->start);\n+\n+\treturn nid;\n+}\n+\n+static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n+{\n+\tstruct resource *res;\n+\tmhp_t mhp_flags;\n+\tint rc;\n+\n+\tif (sysram->res)\n+\t\treturn -EBUSY;\n+\n+\tres = request_mem_region(sysram->hpa_range.start,\n+\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t sysram->res_name);\n+\tif (!res)\n+\t\treturn -EBUSY;\n+\n+\tsysram->res = res;\n+\n+\t/*\n+\t * Set flags appropriate for System RAM. Leave ..._BUSY clear\n+\t * so that add_memory() can add a child resource.\n+\t */\n+\tres->flags = IORESOURCE_SYSTEM_RAM;\n+\n+\tmhp_flags = MHP_NID_IS_MGID;\n+\n+\t/*\n+\t * Ensure that future kexec'd kernels will not treat\n+\t * this as RAM automatically.\n+\t */\n+\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t online_type);\n+\tif (rc) {\n+\t\tremove_resource(res);\n+\t\tkfree(res);\n+\t\tsysram->res = NULL;\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n+{\n+\tint rc;\n+\n+\tif (!sysram->res)\n+\t\treturn 0;\n+\n+\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t       range_len(&sysram->hpa_range));\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tif (sysram->res) {\n+\t\tremove_resource(sysram->res);\n+\t\tkfree(sysram->res);\n+\t\tsysram->res = NULL;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn sysram_hotplug_remove(sysram);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_offline_and_remove, \"CXL\");\n+\n+static void cxl_sysram_release(struct device *dev)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\n+\tif (sysram->res)\n+\t\tsysram_hotplug_remove(sysram);\n+\n+\tkfree(sysram->res_name);\n+\n+\tif (sysram->mgid >= 0)\n+\t\tmemory_group_unregister(sysram->mgid);\n+\n+\tif (sysram->mtype)\n+\t\tclear_node_memory_type(sysram->numa_node, sysram->mtype);\n+\n+\tkfree(sysram);\n+}\n+\n+static ssize_t hotplug_store(struct device *dev,\n+\t\t\t     struct device_attribute *attr,\n+\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\tint online_type, rc;\n+\n+\tonline_type = mhp_online_type_from_str(buf);\n+\tif (online_type < 0)\n+\t\treturn online_type;\n+\n+\tif (online_type == MMOP_OFFLINE)\n+\t\trc = sysram_hotplug_remove(sysram);\n+\telse\n+\t\trc = sysram_hotplug_add(sysram, online_type);\n+\n+\tif (rc)\n+\t\tdev_warn(dev, \"hotplug %s failed: %d\\n\",\n+\t\t\t online_type == MMOP_OFFLINE ? \"offline\" : \"online\", rc);\n+\n+\treturn rc ? rc : len;\n+}\n+static DEVICE_ATTR_WO(hotplug);\n+\n+static struct attribute *cxl_sysram_attrs[] = {\n+\t&dev_attr_hotplug.attr,\n+\tNULL\n+};\n+\n+static const struct attribute_group cxl_sysram_attribute_group = {\n+\t.attrs = cxl_sysram_attrs,\n+};\n+\n+static const struct attribute_group *cxl_sysram_attribute_groups[] = {\n+\t&cxl_base_attribute_group,\n+\t&cxl_sysram_attribute_group,\n+\tNULL\n+};\n+\n+const struct device_type cxl_sysram_type = {\n+\t.name = \"cxl_sysram\",\n+\t.release = cxl_sysram_release,\n+\t.groups = cxl_sysram_attribute_groups,\n+};\n+\n+static bool is_cxl_sysram(struct device *dev)\n+{\n+\treturn dev->type == &cxl_sysram_type;\n+}\n+\n+struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\tif (dev_WARN_ONCE(dev, !is_cxl_sysram(dev),\n+\t\t\t  \"not a cxl_sysram device\\n\"))\n+\t\treturn NULL;\n+\treturn container_of(dev, struct cxl_sysram, dev);\n+}\n+EXPORT_SYMBOL_NS_GPL(to_cxl_sysram, \"CXL\");\n+\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram)\n+{\n+\treturn &sysram->dev;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_dev, \"CXL\");\n+\n+static struct lock_class_key cxl_sysram_key;\n+\n+static enum mmop cxl_sysram_get_default_online_type(void)\n+{\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_SYSTEM_DEFAULT))\n+\t\treturn mhp_get_default_online_type();\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_MOVABLE))\n+\t\treturn MMOP_ONLINE_MOVABLE;\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_NORMAL))\n+\t\treturn MMOP_ONLINE;\n+\treturn MMOP_OFFLINE;\n+}\n+\n+static struct cxl_sysram *cxl_sysram_alloc(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram __free(kfree) = NULL;\n+\tstruct device *dev;\n+\n+\tsysram = kzalloc(sizeof(*sysram), GFP_KERNEL);\n+\tif (!sysram)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tsysram->online_type = cxl_sysram_get_default_online_type();\n+\tsysram->last_hotplug_cmd = MMOP_OFFLINE;\n+\tsysram->numa_node = -1;\n+\tsysram->mgid = -1;\n+\n+\tdev = &sysram->dev;\n+\tsysram->cxlr = cxlr;\n+\tdevice_initialize(dev);\n+\tlockdep_set_class(&dev->mutex, &cxl_sysram_key);\n+\tdevice_set_pm_not_required(dev);\n+\tdev->parent = &cxlr->dev;\n+\tdev->bus = &cxl_bus_type;\n+\tdev->type = &cxl_sysram_type;\n+\n+\treturn_ptr(sysram);\n+}\n+\n+static void sysram_unregister(void *_sysram)\n+{\n+\tstruct cxl_sysram *sysram = _sysram;\n+\n+\tdevice_unregister(&sysram->dev);\n+}\n+\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+{\n+\tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n+\tstruct memory_dev_type *mtype;\n+\tstruct range hpa_range;\n+\tstruct device *dev;\n+\tint adist = MEMTIER_DEFAULT_LOWTIER_ADISTANCE;\n+\tint numa_node;\n+\tint rc;\n+\n+\trc = cxl_region_get_hpa_range(cxlr, &hpa_range);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\thpa_range = memory_block_align_range(&hpa_range);\n+\tif (hpa_range.start >= hpa_range.end) {\n+\t\tdev_warn(&cxlr->dev, \"region too small after alignment\\n\");\n+\t\treturn -ENOSPC;\n+\t}\n+\n+\tsysram = cxl_sysram_alloc(cxlr);\n+\tif (IS_ERR(sysram))\n+\t\treturn PTR_ERR(sysram);\n+\n+\tsysram->hpa_range = hpa_range;\n+\n+\tsysram->res_name = kasprintf(GFP_KERNEL, \"cxl_sysram%d\", cxlr->id);\n+\tif (!sysram->res_name)\n+\t\treturn -ENOMEM;\n+\n+\t/* Override default online type if caller specified one */\n+\tif (online_type >= 0)\n+\t\tsysram->online_type = online_type;\n+\n+\tdev = &sysram->dev;\n+\n+\trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Setup memory tier before adding device */\n+\tnuma_node = sysram_get_numa_node(cxlr);\n+\tif (numa_node < 0) {\n+\t\tdev_warn(&cxlr->dev, \"rejecting region with invalid node: %d\\n\",\n+\t\t\t numa_node);\n+\t\treturn -EINVAL;\n+\t}\n+\tsysram->numa_node = numa_node;\n+\n+\tmt_calc_adistance(numa_node, &adist);\n+\tmtype = mt_get_memory_type(adist);\n+\tif (IS_ERR(mtype))\n+\t\treturn PTR_ERR(mtype);\n+\tsysram->mtype = mtype;\n+\n+\tinit_node_memory_type(numa_node, mtype);\n+\n+\t/* Register memory group for this region */\n+\trc = memory_group_register_static(numa_node,\n+\t\t\t\t\t  PFN_UP(range_len(&hpa_range)));\n+\tif (rc < 0)\n+\t\treturn rc;\n+\tsysram->mgid = rc;\n+\n+\trc = device_add(dev);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n+\t\tdev_name(dev));\n+\n+\t/*\n+\t * Dynamic capacity regions (DCD) will have memory added later.\n+\t * For static RAM regions, hotplug the entire range now.\n+\t */\n+\tif (cxlr->mode != CXL_PARTMODE_RAM)\n+\t\tgoto out;\n+\n+\t/* If default online_type is a valid online mode, immediately hotplug */\n+\tif (sysram->online_type > MMOP_OFFLINE) {\n+\t\trc = sysram_hotplug_add(sysram, sysram->online_type);\n+\t\tif (rc)\n+\t\t\tdev_warn(dev, \"hotplug failed: %d\\n\", rc);\n+\t\telse\n+\t\t\tsysram->last_hotplug_cmd = sysram->online_type;\n+\t}\n+\n+out:\n+\treturn devm_add_action_or_reset(&cxlr->dev, sysram_unregister,\n+\t\t\t\t\tno_free_ptr(sysram));\n+}\n+EXPORT_SYMBOL_NS_GPL(devm_cxl_add_sysram, \"CXL\");\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex f899f240f229..8e8342fd4fde 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -607,6 +607,34 @@ struct cxl_dax_region {\n \tenum dax_driver_type dax_driver;\n };\n \n+/**\n+ * struct cxl_sysram - CXL SysRAM region for system memory hotplug\n+ * @dev: device for this sysram\n+ * @cxlr: parent cxl_region\n+ * @online_type: Default memory online type for new hotplug ops (MMOP_* value)\n+ * @last_hotplug_cmd: Last hotplug command submitted (MMOP_* value)\n+ * @hpa_range: Host physical address range for the region\n+ * @res_name: Resource name for the memory region\n+ * @res: Memory resource (set when hotplugged)\n+ * @mgid: Memory group id\n+ * @mtype: Memory tier type\n+ * @numa_node: NUMA node for this memory\n+ *\n+ * Device that directly performs memory hotplug for CXL RAM regions.\n+ */\n+struct cxl_sysram {\n+\tstruct device dev;\n+\tstruct cxl_region *cxlr;\n+\tenum mmop online_type;\n+\tint last_hotplug_cmd;\n+\tstruct range hpa_range;\n+\tconst char *res_name;\n+\tstruct resource *res;\n+\tint mgid;\n+\tstruct memory_dev_type *mtype;\n+\tint numa_node;\n+};\n+\n /**\n  * struct cxl_port - logical collection of upstream port devices and\n  *\t\t     downstream port devices to construct a CXL memory\n@@ -807,6 +835,7 @@ DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device\n DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n DEFINE_FREE(put_cxl_dax_region, struct cxl_dax_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n+DEFINE_FREE(put_cxl_sysram, struct cxl_sysram *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n \n int devm_cxl_enumerate_ports(struct cxl_memdev *cxlmd);\n void cxl_bus_rescan(void);\n@@ -889,6 +918,7 @@ void cxl_destroy_region(struct cxl_region *cxlr);\n struct device *cxl_region_dev(struct cxl_region *cxlr);\n enum cxl_partition_mode cxl_region_mode(struct cxl_region *cxlr);\n int cxl_get_region_range(struct cxl_region *cxlr, struct range *range);\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr);\n int cxl_get_committed_regions(struct cxl_memdev *cxlmd,\n \t\t\t      struct cxl_region **regions, int max_regions);\n struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n@@ -936,6 +966,7 @@ void cxl_driver_unregister(struct cxl_driver *cxl_drv);\n #define CXL_DEVICE_PMEM_REGION\t\t7\n #define CXL_DEVICE_DAX_REGION\t\t8\n #define CXL_DEVICE_PMU\t\t\t9\n+#define CXL_DEVICE_SYSRAM\t\t10\n \n #define MODULE_ALIAS_CXL(type) MODULE_ALIAS(\"cxl:t\" __stringify(type) \"*\")\n #define CXL_MODALIAS_FMT \"cxl:t%d\"\n@@ -954,6 +985,10 @@ bool is_cxl_pmem_region(struct device *dev);\n struct cxl_pmem_region *to_cxl_pmem_region(struct device *dev);\n int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n+struct cxl_sysram *to_cxl_sysram(struct device *dev);\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n static inline bool is_cxl_pmem_region(struct device *dev)\n@@ -972,6 +1007,19 @@ static inline struct cxl_dax_region *to_cxl_dax_region(struct device *dev)\n {\n \treturn NULL;\n }\n+static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\treturn NULL;\n+}\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+\t\t\t\t      enum mmop online_type)\n+{\n+\treturn -ENXIO;\n+}\n+static inline int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn -ENXIO;\n+}\n static inline u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint,\n \t\t\t\t\t       u64 spa)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private memory regions being isolated from normal allocations and reclaim by adding support for N_MEMORY_PRIVATE hotplug via add_private_memory_driver_managed(). They modified the cxl_sysram region to register as a private node when private=true is passed to devm_cxl_add_sysram(), allowing callers to isolate their memory. A fix is planned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "planned a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Extend the cxl_sysram region to support N_MEMORY_PRIVATE hotplug\nvia add_private_memory_driver_managed(). When a caller passes\nprivate=true to devm_cxl_add_sysram(), the memory is registered\nas a private node, isolating it from normal allocations and reclaim.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/core.h          |  2 +-\n drivers/cxl/core/region_sysram.c | 50 +++++++++++++++++++++++++-------\n drivers/cxl/cxl.h                |  9 ++++--\n 3 files changed, 48 insertions(+), 13 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 973bbcae43f7..8ca3d6d41fe4 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -56,7 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nindex 47a415deb352..77aaa52e7332 100644\n--- a/drivers/cxl/core/region_sysram.c\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -85,12 +85,23 @@ static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n \t/*\n \t * Ensure that future kexec'd kernels will not treat\n \t * this as RAM automatically.\n+\t *\n+\t * For private regions, use add_private_memory_driver_managed()\n+\t * to register as N_MEMORY_PRIVATE which isolates the memory from\n+\t * normal allocations and reclaim.\n \t */\n-\trc = __add_memory_driver_managed(sysram->mgid,\n-\t\t\t\t\t sysram->hpa_range.start,\n-\t\t\t\t\t range_len(&sysram->hpa_range),\n-\t\t\t\t\t sysram_res_name, mhp_flags,\n-\t\t\t\t\t online_type);\n+\tif (sysram->private)\n+\t\trc = add_private_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t       sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t       online_type, &sysram->np);\n+\telse\n+\t\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t online_type);\n \tif (rc) {\n \t\tremove_resource(res);\n \t\tkfree(res);\n@@ -108,10 +119,23 @@ static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n \tif (!sysram->res)\n \t\treturn 0;\n \n-\trc = offline_and_remove_memory(sysram->hpa_range.start,\n-\t\t\t\t       range_len(&sysram->hpa_range));\n-\tif (rc)\n-\t\treturn rc;\n+\tif (sysram->private) {\n+\t\trc = offline_and_remove_private_memory(sysram->numa_node,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\t/*\n+\t\t * -EBUSY means memory was removed but node_private_unregister()\n+\t\t * could not complete because other regions share the node.\n+\t\t * Continue to resource cleanup since the memory is gone.\n+\t\t */\n+\t\tif (rc && rc != -EBUSY)\n+\t\t\treturn rc;\n+\t} else {\n+\t\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\t}\n \n \tif (sysram->res) {\n \t\tremove_resource(sysram->res);\n@@ -257,7 +281,8 @@ static void sysram_unregister(void *_sysram)\n \tdevice_unregister(&sysram->dev);\n }\n \n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n+\t\t\tenum mmop online_type)\n {\n \tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n \tstruct memory_dev_type *mtype;\n@@ -291,6 +316,11 @@ int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n \tif (online_type >= 0)\n \t\tsysram->online_type = online_type;\n \n+\t/* Set up private node registration if requested */\n+\tsysram->private = private;\n+\tif (private)\n+\t\tsysram->np.owner = sysram;\n+\n \tdev = &sysram->dev;\n \n \trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 8e8342fd4fde..54e5f9ac59dc 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -10,6 +10,7 @@\n #include <linux/bitops.h>\n #include <linux/log2.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n #include <linux/io.h>\n #include <linux/range.h>\n #include <linux/dax.h>\n@@ -619,6 +620,8 @@ struct cxl_dax_region {\n  * @mgid: Memory group id\n  * @mtype: Memory tier type\n  * @numa_node: NUMA node for this memory\n+ * @private: true if this region uses N_MEMORY_PRIVATE hotplug\n+ * @np: private node registration state (valid when @private is true)\n  *\n  * Device that directly performs memory hotplug for CXL RAM regions.\n  */\n@@ -633,6 +636,8 @@ struct cxl_sysram {\n \tint mgid;\n \tstruct memory_dev_type *mtype;\n \tint numa_node;\n+\tbool private;\n+\tstruct node_private np;\n };\n \n /**\n@@ -987,7 +992,7 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n struct cxl_sysram *to_cxl_sysram(struct device *dev);\n struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n@@ -1011,7 +1016,7 @@ static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n {\n \treturn NULL;\n }\n-static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n \t\t\t\t      enum mmop online_type)\n {\n \treturn -ENXIO;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the driver's interaction with the migration target control, explaining that they moved struct migration_target_control to include/linux/migrate.h so the driver can use alloc_migration_target() without depending on mm-internal headers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a sample CXL type-3 driver that registers device memory as\nprivate-node NUMA memory reachable only via explicit mempolicy\n(set_mempolicy / mbind).\n\nProbe flow:\n  1. Call cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Look for pre-committed RAM regions; if none exist, create one\n     using cxl_get_hpa_freespace() + cxl_request_dpa() +\n     cxl_create_region()\n  3. Convert the region to sysram via devm_cxl_add_sysram() with\n     private=true and MMOP_ONLINE_MOVABLE\n  4. Register node_private_ops with NP_OPS_MIGRATION | NP_OPS_MEMPOLICY\n     so the node is excluded from default allocations\n\nThe migrate_to callback uses alloc_migration_target() with\n__GFP_THISNODE | __GFP_PRIVATE to keep pages on the target node.\n\nMove struct migration_target_control from mm/internal.h to\ninclude/linux/migrate.h so the driver can use alloc_migration_target()\nwithout depending on mm-internal headers.\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/Kconfig                           |   2 +\n drivers/cxl/Makefile                          |   2 +\n drivers/cxl/type3_drivers/Kconfig             |   2 +\n drivers/cxl/type3_drivers/Makefile            |   2 +\n .../cxl/type3_drivers/cxl_mempolicy/Kconfig   |  16 +\n .../cxl/type3_drivers/cxl_mempolicy/Makefile  |   4 +\n .../type3_drivers/cxl_mempolicy/mempolicy.c   | 297 ++++++++++++++++++\n include/linux/migrate.h                       |   7 +-\n mm/internal.h                                 |   7 -\n 9 files changed, 331 insertions(+), 8 deletions(-)\n create mode 100644 drivers/cxl/type3_drivers/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n\ndiff --git a/drivers/cxl/Kconfig b/drivers/cxl/Kconfig\nindex f99aa7274d12..1648cdeaa0c9 100644\n--- a/drivers/cxl/Kconfig\n+++ b/drivers/cxl/Kconfig\n@@ -278,4 +278,6 @@ config CXL_ATL\n \tdepends on CXL_REGION\n \tdepends on ACPI_PRMT && AMD_NB\n \n+source \"drivers/cxl/type3_drivers/Kconfig\"\n+\n endif\ndiff --git a/drivers/cxl/Makefile b/drivers/cxl/Makefile\nindex 2caa90fa4bf2..94d2b2233bf8 100644\n--- a/drivers/cxl/Makefile\n+++ b/drivers/cxl/Makefile\n@@ -19,3 +19,5 @@ cxl_acpi-y := acpi.o\n cxl_pmem-y := pmem.o security.o\n cxl_mem-y := mem.o\n cxl_pci-y := pci.o\n+\n+obj-y += type3_drivers/\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nnew file mode 100644\nindex 000000000000..369b21763856\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nnew file mode 100644\nindex 000000000000..2b82265ff118\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\nnew file mode 100644\nindex 000000000000..3c45da237b9f\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n@@ -0,0 +1,16 @@\n+config CXL_MEMPOLICY\n+\ttristate \"CXL Private Memory with Mempolicy Support\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\thelp\n+\t  Minimal driver for CXL memory devices that registers memory as\n+\t  N_MEMORY_PRIVATE with mempolicy support.  The memory is isolated\n+\t  from default allocations and can only be reached via explicit\n+\t  mempolicy (set_mempolicy or mbind).\n+\n+\t  No compression, no PTE controls, the memory behaves like normal\n+\t  DRAM but is excluded from fallback allocations.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\nnew file mode 100644\nindex 000000000000..dfb58fc88ad9\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy.o\n+cxl_mempolicy-y := mempolicy.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\nnew file mode 100644\nindex 000000000000..1c19818eb268\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n@@ -0,0 +1,297 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Mempolicy Driver\n+ *\n+ * Minimal driver for CXL memory devices that registers memory as\n+ * N_MEMORY_PRIVATE with mempolicy support but no PTE controls.  The\n+ * memory behaves like normal DRAM but is isolated from default allocations,\n+ * it can only be reached via explicit mempolicy (set_mempolicy/mbind).\n+ *\n+ * Usage:\n+ *   1. Unbind device from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   2. Bind to cxl_mempolicy:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n+ */\n+\n+#include <linux/module.h>\n+#include <linux/pci.h>\n+#include <linux/xarray.h>\n+#include <linux/node_private.h>\n+#include <linux/migrate.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+struct cxl_mempolicy_ctx {\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint nid;\n+};\n+\n+static DEFINE_XARRAY(ctx_xa);\n+\n+static struct cxl_mempolicy_ctx *memdev_to_ctx(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\n+\treturn xa_load(&ctx_xa, (unsigned long)pdev);\n+}\n+\n+static int cxl_mempolicy_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason,\n+\t\t\t\t    unsigned int *nr_succeeded)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE |\n+\t\t\t    __GFP_PRIVATE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, nr_succeeded);\n+}\n+\n+static void cxl_mempolicy_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static const struct node_private_ops cxl_mempolicy_ops = {\n+\t.migrate_to\t= cxl_mempolicy_migrate_to,\n+\t.folio_migrate\t= cxl_mempolicy_folio_migrate,\n+\t.flags = NP_OPS_MIGRATION | NP_OPS_MEMPOLICY,\n+};\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tctx->cxled = cxled;\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\treturn cxlr;\n+}\n+\n+static int setup_private_node(struct cxl_memdev *cxlmd,\n+\t\t\t      struct cxl_region *cxlr)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct range hpa_range;\n+\tint rc;\n+\n+\tdevice_release_driver(cxl_region_dev(cxlr));\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to add sysram: %d\\n\", rc);\n+\t\tif (device_attach(cxl_region_dev(cxlr)) < 0)\n+\t\t\tdev_warn(cxl_region_dev(cxlr),\n+\t\t\t\t \"failed to re-attach driver\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tctx->nid = phys_to_target_node(hpa_range.start);\n+\tif (ctx->nid == NUMA_NO_NODE)\n+\t\tctx->nid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\trc = node_private_set_ops(ctx->nid, &cxl_mempolicy_ops);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to set ops on node %d: %d\\n\", ctx->nid, rc);\n+\t\tctx->nid = NUMA_NO_NODE;\n+\t\treturn rc;\n+\t}\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"node %d registered as private mempolicy memory\\n\", ctx->nid);\n+\treturn 0;\n+}\n+\n+static int cxl_mempolicy_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i;\n+\tint rc;\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"cxl_mempolicy attach: looking for regions\\n\");\n+\n+\t/* Phase 1: look for pre-committed RAM regions */\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) != CXL_PARTMODE_RAM) {\n+\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tcxlr = regions[i];\n+\t\trc = setup_private_node(cxlmd, cxlr);\n+\t\tput_device(cxl_region_dev(cxlr));\n+\t\tif (rc == 0) {\n+\t\t\t/* Release remaining region references */\n+\t\t\tfor (i++; i < nr; i++)\n+\t\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\treturn 0;\n+\t\t}\n+\t}\n+\n+\t/* Phase 2: no committed regions, create one */\n+\tdev_info(&cxlmd->dev,\n+\t\t \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"no RAM capacity: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = setup_private_node(cxlmd, cxlr);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to setup private node: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\t/* Only take ownership of regions we created (Phase 2) */\n+\tmemdev_to_ctx(cxlmd)->cxlr = cxlr;\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_mempolicy_attach = {\n+\t.probe = cxl_mempolicy_attach_probe,\n+};\n+\n+static int cxl_mempolicy_probe(struct pci_dev *pdev,\n+\t\t\t       const struct pci_device_id *id)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probing device\\n\");\n+\n+\tctx = devm_kzalloc(&pdev->dev, sizeof(*ctx), GFP_KERNEL);\n+\tif (!ctx)\n+\t\treturn -ENOMEM;\n+\tctx->nid = NUMA_NO_NODE;\n+\n+\trc = xa_insert(&ctx_xa, (unsigned long)pdev, ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_mempolicy_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_mempolicy_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = xa_erase(&ctx_xa, (unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: removing device\\n\");\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\tif (ctx->nid != NUMA_NO_NODE)\n+\t\tWARN_ON(node_private_clear_ops(ctx->nid, &cxl_mempolicy_ops));\n+\n+\tif (ctx->cxlr) {\n+\t\tcxl_destroy_region(ctx->cxlr);\n+\t\tctx->cxlr = NULL;\n+\t}\n+\n+\tif (ctx->cxled) {\n+\t\tcxl_dpa_free(ctx->cxled);\n+\t\tctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_mempolicy_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_mempolicy_pci_tbl);\n+\n+static struct pci_driver cxl_mempolicy_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_mempolicy_pci_tbl,\n+\t.probe\t\t= cxl_mempolicy_probe,\n+\t.remove\t\t= cxl_mempolicy_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_mempolicy_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Private Memory with Mempolicy Support\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 7b2da3875ff2..1f9fb61f3932 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -10,7 +10,12 @@\n typedef struct folio *new_folio_t(struct folio *folio, unsigned long private);\n typedef void free_folio_t(struct folio *folio, unsigned long private);\n \n-struct migration_target_control;\n+struct migration_target_control {\n+\tint nid;\t\t/* preferred node id */\n+\tnodemask_t *nmask;\n+\tgfp_t gfp_mask;\n+\tenum migrate_reason reason;\n+};\n \n /**\n  * struct movable_operations - Driver page migration\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 64467ca774f1..85cd11189854 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1352,13 +1352,6 @@ extern const struct trace_print_flags gfpflag_names[];\n \n void setup_zone_pageset(struct zone *zone);\n \n-struct migration_target_control {\n-\tint nid;\t\t/* preferred node id */\n-\tnodemask_t *nmask;\n-\tgfp_t gfp_mask;\n-\tenum migrate_reason reason;\n-};\n-\n /*\n  * mm/filemap.c\n  */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the cxl_compression driver's page reclamation using the CXL Media Operations Zero command (opcode 0x4402). The author explains that if the device does not support this command, the driver falls back to inline CPU zeroing.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a generic CXL type-3 driver for compressed memory controllers.\n\nThe driver provides an alternative PCI binding that converts CXL\nRAM regions to private-node sysram and registers them with the\nCRAM subsystem for transparent demotion/promotion.\n\nProbe flow:\n  1. cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Discover/convert auto-RAM regions or create a RAM region\n  3. Convert to private-node sysram via devm_cxl_add_sysram()\n  4. Register with CRAM via cram_register_private_node()\n\nPage flush pipeline:\n  When a CRAM folio is freed, the CRAM free_folio   callback buffers\n  it into a per-CPU RCU-protected flush buffer to offload the operation.\n\n  A periodic kthread swaps the per-CPU buffers under RCU, then sends\n  batched Sanitize-Zero commands so the device can zero pages.\n\n  A flush_record bitmap tracks in-flight pages to avoid re-buffering on\n  the second free_folio entry after folio_put().\n\n  Overflow from full buffers is handled by a per-CPU workqueue fallback.\n\nWatermark interrupts:\n  MSI-X vector 12 - delivers \"Low\" watermark interrupts\n  MSI-X vector 13 - delivers \"High\" watermark interrupts\n  This adjusts CRAM pressure:\n\tLow  - increases pressure.\n  \tHigh - reduces pressure.\n\n  A dynamic watermark mode cycles through four phases with\n  progressively tighter thresholds.\n\n  Static watermark mode sets pressure 0 or MAX respectively.\n\nTeardown ordering:\n  pre_teardown  - cram_unregister + retry-loop memory offline\n  post_teardown - kthread stop, drain all flush buffers via CCI\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/type3_drivers/Kconfig             |    1 +\n drivers/cxl/type3_drivers/Makefile            |    1 +\n .../cxl/type3_drivers/cxl_compression/Kconfig |   20 +\n .../type3_drivers/cxl_compression/Makefile    |    4 +\n .../cxl_compression/compression.c             | 1025 +++++++++++++++++\n 5 files changed, 1051 insertions(+)\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/compression.c\n\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nindex 369b21763856..98f73e46730e 100644\n--- a/drivers/cxl/type3_drivers/Kconfig\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\n+source \"drivers/cxl/type3_drivers/cxl_compression/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nindex 2b82265ff118..f5b0766d92af 100644\n--- a/drivers/cxl/type3_drivers/Makefile\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression/\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Kconfig b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\nnew file mode 100644\nindex 000000000000..8c891a48b000\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\n@@ -0,0 +1,20 @@\n+config CXL_COMPRESSION\n+\ttristate \"CXL Compression Memory Driver\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on CRAM\n+\thelp\n+\t  This driver provides an alternative PCI binding for CXL memory\n+\t  devices with compressed memory support. It converts CXL RAM\n+\t  regions to sysram for direct memory hotplug and registers with\n+\t  the CRAM subsystem for transparent compression.\n+\n+\t  Page reclamation uses the standard CXL Media Operations Zero\n+\t  command (opcode 0x4402). If the device does not support it,\n+\t  the driver falls back to inline CPU zeroing.\n+\n+\t  Usage: First unbind the device from cxl_pci, then bind to\n+\t  cxl_compression. The driver will initialize the CXL device and\n+\t  convert any RAM regions to use direct memory hotplug via sysram.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Makefile b/drivers/cxl/type3_drivers/cxl_compression/Makefile\nnew file mode 100644\nindex 000000000000..46f34809bf74\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression.o\n+cxl_compression-y := compression.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/compression.c b/drivers/cxl/type3_drivers/cxl_compression/compression.c\nnew file mode 100644\nindex 000000000000..e4c8b62227e2\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/compression.c\n@@ -0,0 +1,1025 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Compression Driver\n+ *\n+ * This driver provides an alternative binding for CXL memory devices that\n+ * converts all associated RAM regions to sysram_regions for direct memory\n+ * hotplug, bypassing the standard dax region path.\n+ *\n+ * Page reclamation uses the standard CXL Media Operations Zero command\n+ * (opcode 0x4402, class 0x01, subclass 0x01).  Watermark interrupts\n+ * are delivered via separate MSI-X vectors (12 for lthresh, 13 for\n+ * hthresh), injected externally via QMP.\n+ *\n+ * Usage:\n+ *   1. Device initially binds to cxl_pci at boot\n+ *   2. Unbind from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   3. Bind to cxl_compression:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n+ */\n+\n+#include <linux/unaligned.h>\n+#include <linux/io-64-nonatomic-lo-hi.h>\n+#include <linux/module.h>\n+#include <linux/delay.h>\n+#include <linux/sizes.h>\n+#include <linux/mutex.h>\n+#include <linux/list.h>\n+#include <linux/pci.h>\n+#include <linux/io.h>\n+#include <linux/interrupt.h>\n+#include <linux/bitmap.h>\n+#include <linux/highmem.h>\n+#include <linux/workqueue.h>\n+#include <linux/kthread.h>\n+#include <linux/rcupdate.h>\n+#include <linux/percpu.h>\n+#include <linux/sched.h>\n+#include <linux/cram.h>\n+#include <linux/memory_hotplug.h>\n+#include <linux/xarray.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+/*\n+ * Per-device compression context lookup.\n+ *\n+ * pci_set_drvdata() MUST store cxlds because mbox_to_cxlds() uses\n+ * dev_get_drvdata() to recover the cxl_dev_state from the mailbox host\n+ * device.  Storing anything else in pci drvdata breaks every CXL mailbox\n+ * command.  Use an xarray keyed by pci_dev pointer so that multiple\n+ * devices can bind concurrently without colliding.\n+ */\n+static DEFINE_XARRAY(comp_ctx_xa);\n+\n+static struct cxl_compression_ctx *pdev_to_comp_ctx(struct pci_dev *pdev)\n+{\n+\treturn xa_load(&comp_ctx_xa, (unsigned long)pdev);\n+}\n+\n+#define CXL_MEDIA_OP_OPCODE\t\t0x4402\n+#define CXL_MEDIA_OP_CLASS_SANITIZE\t0x01\n+#define CXL_MEDIA_OP_SUBC_ZERO\t\t0x01\n+\n+struct cxl_dpa_range {\n+\t__le64 starting_dpa;\n+\t__le64 length;\n+} __packed;\n+\n+struct cxl_media_op_input {\n+\tu8 media_operation_class;\n+\tu8 media_operation_subclass;\n+\t__le16 reserved;\n+\t__le32 dpa_range_count;\n+\tstruct cxl_dpa_range ranges[];\n+} __packed;\n+\n+#define CXL_CT3_MSIX_LTHRESH\t\t12\n+#define CXL_CT3_MSIX_HTHRESH\t\t13\n+#define CXL_CT3_MSIX_VECTOR_NR\t\t14\n+#define CXL_FLUSH_INTERVAL_DEFAULT_MS\t1000\n+\n+static unsigned int flush_buf_size;\n+module_param(flush_buf_size, uint, 0444);\n+MODULE_PARM_DESC(flush_buf_size,\n+\t\t \"Max DPA ranges per media ops CCI command (0 = use hw max)\");\n+\n+static unsigned int flush_interval_ms = CXL_FLUSH_INTERVAL_DEFAULT_MS;\n+module_param(flush_interval_ms, uint, 0644);\n+MODULE_PARM_DESC(flush_interval_ms,\n+\t\t \"Flush worker interval in ms (default 1000)\");\n+\n+struct cxl_flush_buf {\n+\tunsigned int count;\n+\tunsigned int max;\t\t\t/* max ranges per command */\n+\tstruct cxl_media_op_input *cmd;\t\t/* pre-allocated CCI payload */\n+\tstruct folio **folios;\t\t\t/* parallel folio tracking */\n+};\n+\n+struct cxl_flush_ctx;\n+\n+struct cxl_pcpu_flush {\n+\tstruct cxl_flush_buf __rcu *active;\t/* callback writes here */\n+\tstruct cxl_flush_buf *overflow_spare;\t/* spare for overflow work */\n+\tstruct work_struct overflow_work;\t/* per-CPU overflow flush */\n+\tstruct cxl_flush_ctx *ctx;\t\t/* backpointer */\n+};\n+\n+/**\n+ * struct cxl_flush_ctx - Per-region flush context\n+ * @flush_record: two-level bitmap, 1 bit per 4KB page, tracks in-flight ops\n+ * @flush_record_pages: number of pages in the flush_record array\n+ * @nr_pages: total number of 4KB pages in the region\n+ * @base_pfn: starting PFN of the region (for DPA offset calculation)\n+ * @buf_max: max DPA ranges per CCI command\n+ * @media_ops_supported: true if device supports media operations zero\n+ * @pcpu: per-CPU flush state\n+ * @kthread_spares: array[nr_cpu_ids] of spare buffers for the kthread\n+ * @flush_thread: round-robin kthread\n+ * @mbox: pointer to CXL mailbox for sending CCI commands\n+ * @dev: device for logging\n+ * @nid: NUMA node of the private region\n+ */\n+struct cxl_flush_ctx {\n+\tunsigned long\t**flush_record;\n+\tunsigned int\t flush_record_pages;\n+\tunsigned long\t nr_pages;\n+\tunsigned long\t base_pfn;\n+\tunsigned int\t buf_max;\n+\tbool\t\t media_ops_supported;\n+\tstruct cxl_pcpu_flush __percpu *pcpu;\n+\tstruct cxl_flush_buf **kthread_spares;\n+\tstruct task_struct *flush_thread;\n+\tstruct cxl_mailbox *mbox;\n+\tstruct device\t*dev;\n+\tint\t\t nid;\n+};\n+\n+/* Bits per page-sized bitmap chunk */\n+#define FLUSH_RECORD_BITS_PER_PAGE\t(PAGE_SIZE * BITS_PER_BYTE)\n+#define FLUSH_RECORD_SHIFT\t\t(PAGE_SHIFT + 3)\n+\n+static unsigned long **flush_record_alloc(unsigned long nr_bits,\n+\t\t\t\t\t  unsigned int *nr_pages_out)\n+{\n+\tunsigned int nr_pages = DIV_ROUND_UP(nr_bits, FLUSH_RECORD_BITS_PER_PAGE);\n+\tunsigned long **pages;\n+\tunsigned int i;\n+\n+\tpages = kcalloc(nr_pages, sizeof(*pages), GFP_KERNEL);\n+\tif (!pages)\n+\t\treturn NULL;\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tpages[i] = (unsigned long *)get_zeroed_page(GFP_KERNEL);\n+\t\tif (!pages[i])\n+\t\t\tgoto err;\n+\t}\n+\n+\t*nr_pages_out = nr_pages;\n+\treturn pages;\n+\n+err:\n+\twhile (i--)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+\treturn NULL;\n+}\n+\n+static void flush_record_free(unsigned long **pages, unsigned int nr_pages)\n+{\n+\tunsigned int i;\n+\n+\tif (!pages)\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr_pages; i++)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+}\n+\n+static inline bool flush_record_test_and_clear(unsigned long **pages,\n+\t\t\t\t\t       unsigned long idx)\n+{\n+\treturn test_and_clear_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\t\t\t  pages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static inline void flush_record_set(unsigned long **pages, unsigned long idx)\n+{\n+\tset_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\tpages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static struct cxl_flush_buf *cxl_flush_buf_alloc(unsigned int max, int nid)\n+{\n+\tstruct cxl_flush_buf *buf;\n+\n+\tbuf = kzalloc_node(sizeof(*buf), GFP_KERNEL, nid);\n+\tif (!buf)\n+\t\treturn NULL;\n+\n+\tbuf->max = max;\n+\tbuf->cmd = kvzalloc_node(struct_size(buf->cmd, ranges, max),\n+\t\t\t\t GFP_KERNEL, nid);\n+\tif (!buf->cmd)\n+\t\tgoto err_cmd;\n+\n+\tbuf->folios = kcalloc_node(max, sizeof(struct folio *),\n+\t\t\t\t   GFP_KERNEL, nid);\n+\tif (!buf->folios)\n+\t\tgoto err_folios;\n+\n+\treturn buf;\n+\n+err_folios:\n+\tkvfree(buf->cmd);\n+err_cmd:\n+\tkfree(buf);\n+\treturn NULL;\n+}\n+\n+static void cxl_flush_buf_free(struct cxl_flush_buf *buf)\n+{\n+\tif (!buf)\n+\t\treturn;\n+\tkvfree(buf->cmd);\n+\tkfree(buf->folios);\n+\tkfree(buf);\n+}\n+\n+static inline void cxl_flush_buf_reset(struct cxl_flush_buf *buf)\n+{\n+\tbuf->count = 0;\n+}\n+\n+static void cxl_flush_buf_send(struct cxl_flush_ctx *ctx,\n+\t\t\t       struct cxl_flush_buf *buf)\n+{\n+\tstruct cxl_mbox_cmd mbox_cmd;\n+\tunsigned int count = buf->count;\n+\tunsigned int i;\n+\tint rc;\n+\n+\tif (count == 0)\n+\t\treturn;\n+\n+\tif (!ctx->media_ops_supported) {\n+\t\t/* No device support, zero all folios inline */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t\tgoto release;\n+\t}\n+\n+\tbuf->cmd->media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE;\n+\tbuf->cmd->media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO;\n+\tbuf->cmd->reserved = 0;\n+\tbuf->cmd->dpa_range_count = cpu_to_le32(count);\n+\n+\tmbox_cmd = (struct cxl_mbox_cmd) {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = buf->cmd,\n+\t\t.size_in = struct_size(buf->cmd, ranges, count),\n+\t\t.poll_interval_ms = 1000,\n+\t\t.poll_count = 30,\n+\t};\n+\n+\trc = cxl_internal_send_cmd(ctx->mbox, &mbox_cmd);\n+\tif (rc) {\n+\t\tdev_warn(ctx->dev,\n+\t\t\t \"media ops zero CCI command failed: %d\\n\", rc);\n+\n+\t\t/* Zero all folios inline on failure */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t}\n+\n+release:\n+\tfor (i = 0; i < count; i++)\n+\t\tfolio_put(buf->folios[i]);\n+\n+\tcxl_flush_buf_reset(buf);\n+}\n+\n+static int cxl_compression_flush_cb(struct folio *folio, void *private)\n+{\n+\tstruct cxl_flush_ctx *ctx = private;\n+\tunsigned long pfn = folio_pfn(folio);\n+\tunsigned long idx = pfn - ctx->base_pfn;\n+\tunsigned long nr = folio_nr_pages(folio);\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tunsigned long flags;\n+\tunsigned int pos;\n+\n+\t/* Case (a): flush record bit set, resolution from our media op */\n+\tif (flush_record_test_and_clear(ctx->flush_record, idx))\n+\t\treturn 0;\n+\n+\tdev_dbg_ratelimited(ctx->dev,\n+\t\t\t     \"flush_cb: folio pfn=%lx order=%u idx=%lu cpu=%d\\n\",\n+\t\t\t     pfn, folio_order(folio), idx,\n+\t\t\t     raw_smp_processor_id());\n+\n+\tlocal_irq_save(flags);\n+\trcu_read_lock();\n+\n+\tpcpu = this_cpu_ptr(ctx->pcpu);\n+\tbuf = rcu_dereference(pcpu->active);\n+\n+\tif (unlikely(!buf || buf->count >= buf->max)) {\n+\t\trcu_read_unlock();\n+\t\tlocal_irq_restore(flags);\n+\t\tif (buf)\n+\t\t\tschedule_work_on(raw_smp_processor_id(),\n+\t\t\t\t\t &pcpu->overflow_work);\n+\t\treturn 2;\n+\t}\n+\n+\t/* Case (b): write DPA range directly into pre-formatted CCI buffer */\n+\tfolio_get(folio);\n+\tflush_record_set(ctx->flush_record, idx);\n+\n+\tpos = buf->count;\n+\tbuf->folios[pos] = folio;\n+\tbuf->cmd->ranges[pos].starting_dpa = cpu_to_le64((u64)idx * PAGE_SIZE);\n+\tbuf->cmd->ranges[pos].length = cpu_to_le64((u64)nr * PAGE_SIZE);\n+\tbuf->count = pos + 1;\n+\n+\trcu_read_unlock();\n+\tlocal_irq_restore(flags);\n+\n+\treturn 1;\n+}\n+\n+static int cxl_flush_kthread_fn(void *data)\n+{\n+\tstruct cxl_flush_ctx *ctx = data;\n+\tstruct cxl_flush_buf *dirty;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tint cpu;\n+\tbool any_dirty;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tany_dirty = false;\n+\n+\t\t/* Phase 1: Swap all per-CPU buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tstruct cxl_flush_buf *spare = ctx->kthread_spares[cpu];\n+\n+\t\t\tif (!spare)\n+\t\t\t\tcontinue;\n+\n+\t\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\t\tcxl_flush_buf_reset(spare);\n+\t\t\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\t\t\tctx->kthread_spares[cpu] = dirty;\n+\n+\t\t\tif (dirty && dirty->count > 0) {\n+\t\t\t\tdev_dbg(ctx->dev,\n+\t\t\t\t\t \"flush_kthread: cpu=%d has %u dirty ranges\\n\",\n+\t\t\t\t\t cpu, dirty->count);\n+\t\t\t\tany_dirty = true;\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (!any_dirty)\n+\t\t\tgoto sleep;\n+\n+\t\t/* Phase 2: Single synchronize_rcu for all swaps */\n+\t\tsynchronize_rcu();\n+\n+\t\t/* Phase 3: Send CCI commands for dirty buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tdirty = ctx->kthread_spares[cpu];\n+\t\t\tif (dirty && dirty->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, dirty);\n+\t\t\t/* dirty is now clean, stays as kthread_spares[cpu] */\n+\t\t}\n+\n+sleep:\n+\t\tschedule_timeout_interruptible(\n+\t\t\tmsecs_to_jiffies(flush_interval_ms));\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static void cxl_flush_overflow_work(struct work_struct *work)\n+{\n+\tstruct cxl_pcpu_flush *pcpu =\n+\t\tcontainer_of(work, struct cxl_pcpu_flush, overflow_work);\n+\tstruct cxl_flush_ctx *ctx = pcpu->ctx;\n+\tstruct cxl_flush_buf *dirty, *spare;\n+\tunsigned long flags;\n+\n+\tdev_dbg(ctx->dev, \"flush_overflow: cpu=%d buffer full, flushing\\n\",\n+\t\t raw_smp_processor_id());\n+\n+\tspare = pcpu->overflow_spare;\n+\tif (!spare)\n+\t\treturn;\n+\n+\tcxl_flush_buf_reset(spare);\n+\n+\tlocal_irq_save(flags);\n+\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\tlocal_irq_restore(flags);\n+\n+\tpcpu->overflow_spare = dirty;\n+\n+\tsynchronize_rcu();\n+\tcxl_flush_buf_send(ctx, dirty);\n+}\n+\n+struct cxl_teardown_ctx {\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+};\n+\n+static void cxl_compression_pre_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\n+\tif (!tctx->flush_ctx)\n+\t\treturn;\n+\n+\t/*\n+\t * Unregister the CRAM node before memory goes offline.\n+\t * node_private_clear_ops requires the node_private to still\n+\t * exist, which is destroyed during memory removal.\n+\t */\n+\tcram_unregister_private_node(tctx->nid);\n+\n+\t/*\n+\t * Offline and remove CXL memory with retry.  CXL compressed\n+\t * memory may have pages pinned by in-flight flush operations;\n+\t * keep retrying until they complete.  Once done, sysram->res\n+\t * is NULL so the devm sysram_unregister action that follows\n+\t * will skip the hotplug removal.\n+\t */\n+\tif (tctx->sysram) {\n+\t\tint rc, retries = 0;\n+\n+\t\twhile (true) {\n+\t\t\trc = cxl_sysram_offline_and_remove(tctx->sysram);\n+\t\t\tif (!rc)\n+\t\t\t\tbreak;\n+\t\t\tif (++retries > 60) {\n+\t\t\t\tpr_err(\"cxl_compression: memory offline failed after %d retries, giving up\\n\",\n+\t\t\t\t       retries);\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tpr_info(\"cxl_compression: memory offline failed (%d), retrying...\\n\",\n+\t\t\t\trc);\n+\t\t\tmsleep(1000);\n+\t\t}\n+\t}\n+}\n+\n+static void cxl_compression_post_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\tstruct cxl_flush_ctx *ctx = tctx->flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tint cpu;\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\t/* cram_unregister_private_node already called in pre_teardown */\n+\n+\tif (ctx->flush_thread) {\n+\t\tkthread_stop(ctx->flush_thread);\n+\t\tctx->flush_thread = NULL;\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\tcancel_work_sync(&pcpu->overflow_work);\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tif (buf && buf->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, buf);\n+\n+\t\tif (pcpu->overflow_spare && pcpu->overflow_spare->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares && ctx->kthread_spares[cpu]) {\n+\t\t\tbuf = ctx->kthread_spares[cpu];\n+\t\t\tif (buf->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, buf);\n+\t\t}\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(ctx->kthread_spares[cpu]);\n+\t}\n+\n+\tkfree(ctx->kthread_spares);\n+\tfree_percpu(ctx->pcpu);\n+\tflush_record_free(ctx->flush_record, ctx->flush_record_pages);\n+}\n+\n+/**\n+ * struct cxl_compression_ctx - Per-device context for compression driver\n+ * @mbox: CXL mailbox for issuing CCI commands\n+ * @pdev: PCI device\n+ * @flush_ctx: Flush context for deferred page reclamation\n+ * @tctx: Teardown context for devm actions\n+ * @sysram: Sysram device for offline+remove in remove path\n+ * @nid: NUMA node ID, NUMA_NO_NODE if unset\n+ * @cxlmd: The memdev associated with this context\n+ * @cxlr: Region created by this driver (NULL if pre-existing)\n+ * @cxled: Endpoint decoder with DPA allocated by this driver\n+ * @regions_converted: Number of regions successfully converted\n+ * @media_ops_supported: Device supports media operations zero (0x4402)\n+ */\n+struct cxl_compression_ctx {\n+\tstruct cxl_mailbox *mbox;\n+\tstruct pci_dev *pdev;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+\tstruct cxl_memdev *cxlmd;\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint regions_converted;\n+\tbool media_ops_supported;\n+};\n+\n+/*\n+ * Probe whether the device supports Media Operations Zero (0x4402).\n+ * Send a zero-count command, a conforming device returns SUCCESS,\n+ * a device that doesn't support it returns UNSUPPORTED (-ENXIO).\n+ */\n+static bool cxl_probe_media_ops_zero(struct cxl_mailbox *mbox,\n+\t\t\t\t     struct device *dev)\n+{\n+\tstruct cxl_media_op_input probe = {\n+\t\t.media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE,\n+\t\t.media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO,\n+\t\t.dpa_range_count = 0,\n+\t};\n+\tstruct cxl_mbox_cmd cmd = {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = &probe,\n+\t\t.size_in = sizeof(probe),\n+\t};\n+\tint rc;\n+\n+\trc = cxl_internal_send_cmd(mbox, &cmd);\n+\tif (rc) {\n+\t\tdev_info(dev,\n+\t\t\t \"media operations zero not supported (rc=%d), using inline zeroing\\n\",\n+\t\t\t rc);\n+\t\treturn false;\n+\t}\n+\n+\tdev_info(dev, \"media operations zero (0x4402) supported\\n\");\n+\treturn true;\n+}\n+\n+struct cxl_compression_wm_ctx {\n+\tstruct device *dev;\n+\tint nid;\n+};\n+\n+static irqreturn_t cxl_compression_lthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"lthresh watermark: pressuring node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, CRAM_PRESSURE_MAX);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static irqreturn_t cxl_compression_hthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"hthresh watermark: resuming node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, 0);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static int convert_region_to_sysram(struct cxl_region *cxlr,\n+\t\t\t\t    struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct device *dev = cxl_region_dev(cxlr);\n+\tstruct cxl_compression_wm_ctx *wm_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tresource_size_t region_start, region_size;\n+\tstruct range hpa_range;\n+\tint nid;\n+\tint irq;\n+\tint cpu;\n+\tint rc;\n+\n+\tif (cxl_region_mode(cxlr) != CXL_PARTMODE_RAM) {\n+\t\tdev_dbg(dev, \"skipping non-RAM region (mode=%d)\\n\",\n+\t\t\tcxl_region_mode(cxlr));\n+\t\treturn 0;\n+\t}\n+\n+\tdev_info(dev, \"converting region to sysram\\n\");\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to add sysram region: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\ttctx = devm_kzalloc(dev, sizeof(*tctx), GFP_KERNEL);\n+\tif (!tctx)\n+\t\treturn -ENOMEM;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_post_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Find the sysram child device for pre_teardown */\n+\tcomp_ctx->sysram = cxl_region_find_sysram(cxlr);\n+\tif (comp_ctx->sysram)\n+\t\ttctx->sysram = comp_ctx->sysram;\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tnid = phys_to_target_node(hpa_range.start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\tregion_start = hpa_range.start;\n+\tregion_size = range_len(&hpa_range);\n+\n+\tflush_ctx = devm_kzalloc(dev, sizeof(*flush_ctx), GFP_KERNEL);\n+\tif (!flush_ctx)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->base_pfn = PHYS_PFN(region_start);\n+\tflush_ctx->nr_pages = region_size >> PAGE_SHIFT;\n+\tflush_ctx->flush_record = flush_record_alloc(flush_ctx->nr_pages,\n+\t\t\t\t\t\t     &flush_ctx->flush_record_pages);\n+\tif (!flush_ctx->flush_record)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->mbox = comp_ctx->mbox;\n+\tflush_ctx->dev = dev;\n+\tflush_ctx->nid = nid;\n+\tflush_ctx->media_ops_supported = comp_ctx->media_ops_supported;\n+\n+\t/*\n+\t * Cap buffer at max DPA ranges that fit in one CCI payload.\n+\t * Header is 8 bytes (struct cxl_media_op_input), each range\n+\t * is 16 bytes (struct cxl_dpa_range).  The module parameter\n+\t * flush_buf_size can further limit this (0 = use hw max).\n+\t */\n+\tflush_ctx->buf_max = (flush_ctx->mbox->payload_size -\n+\t\t\t      sizeof(struct cxl_media_op_input)) /\n+\t\t\t     sizeof(struct cxl_dpa_range);\n+\tif (flush_buf_size && flush_buf_size < flush_ctx->buf_max)\n+\t\tflush_ctx->buf_max = flush_buf_size;\n+\tif (flush_ctx->buf_max == 0)\n+\t\tflush_ctx->buf_max = 1;\n+\n+\tdev_info(dev,\n+\t\t \"flush buffer: %u DPA ranges per command (payload %zu bytes, media_ops %s)\\n\",\n+\t\t flush_ctx->buf_max, flush_ctx->mbox->payload_size,\n+\t\t flush_ctx->media_ops_supported ? \"yes\" : \"no\");\n+\n+\tflush_ctx->pcpu = alloc_percpu(struct cxl_pcpu_flush);\n+\tif (!flush_ctx->pcpu)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->kthread_spares = kcalloc(nr_cpu_ids,\n+\t\t\t\t\t    sizeof(struct cxl_flush_buf *),\n+\t\t\t\t\t    GFP_KERNEL);\n+\tif (!flush_ctx->kthread_spares)\n+\t\tgoto err_pcpu_init;\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *active_buf, *overflow_buf, *spare_buf;\n+\n+\t\tactive_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!active_buf)\n+\t\t\tgoto err_pcpu_init;\n+\n+\t\toverflow_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!overflow_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tspare_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!spare_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tcxl_flush_buf_free(overflow_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\t\tpcpu->ctx = flush_ctx;\n+\t\trcu_assign_pointer(pcpu->active, active_buf);\n+\t\tpcpu->overflow_spare = overflow_buf;\n+\t\tINIT_WORK(&pcpu->overflow_work, cxl_flush_overflow_work);\n+\n+\t\tflush_ctx->kthread_spares[cpu] = spare_buf;\n+\t}\n+\n+\tflush_ctx->flush_thread = kthread_create_on_node(\n+\t\tcxl_flush_kthread_fn, flush_ctx, nid, \"cxl-flush/%d\", nid);\n+\tif (IS_ERR(flush_ctx->flush_thread)) {\n+\t\trc = PTR_ERR(flush_ctx->flush_thread);\n+\t\tflush_ctx->flush_thread = NULL;\n+\t\tgoto err_pcpu_init;\n+\t}\n+\twake_up_process(flush_ctx->flush_thread);\n+\n+\trc = cram_register_private_node(nid, cxlr,\n+\t\t\t\t\tcxl_compression_flush_cb, flush_ctx);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to register cram node %d: %d\\n\", nid, rc);\n+\t\tgoto err_pcpu_init;\n+\t}\n+\n+\ttctx->flush_ctx = flush_ctx;\n+\ttctx->nid = nid;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_pre_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcomp_ctx->flush_ctx = flush_ctx;\n+\tcomp_ctx->tctx = tctx;\n+\tcomp_ctx->nid = nid;\n+\n+\t/*\n+\t * Register watermark IRQ handlers on &pdev->dev for\n+\t * MSI-X vector 12 (lthresh) and vector 13 (hthresh).\n+\t */\n+\twm_ctx = devm_kzalloc(&pdev->dev, sizeof(*wm_ctx), GFP_KERNEL);\n+\tif (!wm_ctx)\n+\t\treturn -ENOMEM;\n+\n+\twm_ctx->dev = &pdev->dev;\n+\twm_ctx->nid = nid;\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_LTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_lthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-lthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register lthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_HTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_hthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-hthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register hthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\treturn 0;\n+\n+err_pcpu_init:\n+\tif (flush_ctx->flush_thread)\n+\t\tkthread_stop(flush_ctx->flush_thread);\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *buf;\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (flush_ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(flush_ctx->kthread_spares[cpu]);\n+\t}\n+\tkfree(flush_ctx->kthread_spares);\n+\tfree_percpu(flush_ctx->pcpu);\n+\tflush_record_free(flush_ctx->flush_record, flush_ctx->flush_record_pages);\n+\treturn rc ? rc : -ENOMEM;\n+}\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\tpdev_to_comp_ctx(to_pci_dev(cxlmd->dev.parent))->cxled = cxled;\n+\treturn cxlr;\n+}\n+\n+static int cxl_compression_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i, converted = 0, errors = 0;\n+\tint rc;\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\t/* Probe device for media operations zero support */\n+\tcomp_ctx->media_ops_supported =\n+\t\tcxl_probe_media_ops_zero(comp_ctx->mbox,\n+\t\t\t\t\t &cxlmd->dev);\n+\n+\tdev_info(&cxlmd->dev, \"compression attach: looking for regions\\n\");\n+\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) == CXL_PARTMODE_RAM) {\n+\t\t\trc = convert_region_to_sysram(regions[i], pdev);\n+\t\t\tif (rc)\n+\t\t\t\terrors++;\n+\t\t\telse\n+\t\t\t\tconverted++;\n+\t\t}\n+\t\tput_device(cxl_region_dev(regions[i]));\n+\t}\n+\n+\tif (converted > 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"converted %d regions to sysram (%d errors)\\n\",\n+\t\t\t converted, errors);\n+\t\treturn errors ? -EIO : 0;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"could not create RAM region: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = convert_region_to_sysram(cxlr, pdev);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to convert region to sysram: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tcomp_ctx->cxlr = cxlr;\n+\n+\tdev_info(&cxlmd->dev, \"created and converted region %s to sysram\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_compression_attach = {\n+\t.probe = cxl_compression_attach_probe,\n+};\n+\n+static int cxl_compression_probe(struct pci_dev *pdev,\n+\t\t\t\t const struct pci_device_id *id)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probing device\\n\");\n+\n+\tcomp_ctx = devm_kzalloc(&pdev->dev, sizeof(*comp_ctx), GFP_KERNEL);\n+\tif (!comp_ctx)\n+\t\treturn -ENOMEM;\n+\tcomp_ctx->nid = NUMA_NO_NODE;\n+\tcomp_ctx->pdev = pdev;\n+\n+\trc = xa_insert(&comp_ctx_xa, (unsigned long)pdev, comp_ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_compression_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&comp_ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_compression_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = xa_erase(&comp_ctx_xa,\n+\t\t\t\t\t\t\t(unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: removing device\\n\");\n+\n+\tif (!comp_ctx || comp_ctx->nid == NUMA_NO_NODE)\n+\t\treturn;\n+\n+\t/*\n+\t * Destroy the region, devm actions on the region device handle teardown\n+\t * in registration-reverse order:\n+\t *   1. pre_teardown:  cram_unregister + retry-forever memory offline\n+\t *   2. sysram_unregister: device_unregister (sysram->res is NULL\n+\t *      after pre_teardown, so cxl_sysram_release skips hotplug)\n+\t *   3. post_teardown: kthread stop, flush cleanup\n+\t *\n+\t * PCI MMIO is still live so CCI commands in post_teardown work.\n+\t */\n+\tif (comp_ctx->cxlr) {\n+\t\tcxl_destroy_region(comp_ctx->cxlr);\n+\t\tcomp_ctx->cxlr = NULL;\n+\t}\n+\n+\tif (comp_ctx->cxled) {\n+\t\tcxl_dpa_free(comp_ctx->cxled);\n+\t\tcomp_ctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_compression_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ /* terminate list */ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_compression_pci_tbl);\n+\n+static struct pci_driver cxl_compression_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_compression_pci_tbl,\n+\t.probe\t\t= cxl_compression_probe,\n+\t.remove\t\t= cxl_compression_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_compression_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Compression Memory Driver with SysRAM regions\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David expressed concern about adding special-casing for private memory nodes, similar to ZONE_DEVICE, and suggested discussing the topic further.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concern",
                "special-casing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm concerned about adding more special-casing (similar to what we \nalready added for ZONE_DEVICE) all over the place.\n\nLike the whole folio_managed_() stuff in mprotect.c\n\nHaving that said, sounds like a reasonable topic to discuss.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged a concern about the semantics of zone_device hooks and proposed two alternative solutions: reusing vma_wants_writenotify() or adding a new vma flag to track protected/device pages.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "proposed alternative solutions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It's a valid concern - and is why I tried to re-use as many of the\nzone_device hooks as possible.  It does not seem zone_device has quite\nthe same semantics for a case like this, so I had to make something new.\n\nDEVICE_COHERENT injects a temporary swap entry to allow the device to do\na large atomic operation - then the page table is restored and the CPU\nis free to change entries as it pleases.\n\nAnother option would be to add the hook to vma_wants_writenotify()\ninstead of the page table code - and mask MM_CP_TRY_CHANGE_WRITABLE.\n\nThis would require adding a vma flag - or maybe a count of protected /\ndevice pages.\n\nint mprotect_fixup() {\n    ...\n    if (vma_wants_manual_pte_write_upgrade(vma))\n        mm_cp_flags |= MM_CP_TRY_CHANGE_WRITABLE;\n}\n\nbool vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)\n{\n    if (vma->managed_wrprotect)\n        return true;\n}\n\nThat would localize the change in folio_managed_fixup_migration_pte() :\n\nstatic inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n                                                      pte_t pte,\n                                                      pte_t old_pte,\n                                                      struct vm_area_struct *vma)\n{\n    ...\n    } else if (folio_managed_wrprotect(page_folio(new))) {\n        pte = pte_wrprotect(pte);\n+       atomic_inc(&vma->managed_wrprotect);\n    }\n    return pte;\n}\n\nThis would cover both the huge_memory.c and mprotect, and maybe that's\njust generally cleaner? I can try that to see if it actually works.\n\n~Gregory",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that existing hooks can be used for write protection and agreed to remove redundant code from page table walks.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged existing solution",
                "agreed to simplify"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "scratch all this - existing hooks exist for exactly this purpose:\n\n\tcan_change_[pte|pmd]_writable()\n\nSurprised I missed this.\n\nI can clean this up to remove it from the page table walks.\n\nStill valid to question whether we want this, but at least the hook\nlives with other write-protect hooks now.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alistair Popple",
              "summary": "Reviewer Alistair Popple expressed concerns that N_MEMORY_PRIVATE may not be the best solution for reusing the existing mm buddy allocator and suggested considering alternative approaches such as adapting DRM's standalone buddy allocator, while also noting that device memory exposure to userspace is an interesting aspect of the series.\n\nThe reviewer agrees that the patch provides a standard interface to userspace for managing device memory and suggests using the existing NUMA APIs as a reasonable approach.\n\nReviewer Alistair Popple noted that the proposed cxl_compression driver is similar to ZONE_DEVICE and questioned why it cannot be extended instead of duplicating code, pointing out a potential lock ordering issue with reclaim paths when using pgmap for ZONE_DEVICE pages. He suggested exploring alternative storage options such as page_ext or considering the future replacement of struct page with folios.\n\nReviewer suggested that the cxl_compression PCI driver is similar to existing ZONE_DEVICE methods and proposed building on those instead of introducing a new feature set.\n\nReviewer Alistair Popple noted that the implementation duplicates a lot of hooks, similar to those provided by ZONE_DEVICE, and requested further discussion on this aspect.\n\nReviewer questioned whether allocation must be handled by the mm allocator, suggesting that a device allocator library could be written or reused from drm_buddy.c\n\nThe reviewer questioned the characterization of ZONE_DEVICE pages as not being real struct pages, suggesting that perspective on this may vary depending on one's role in the mm subsystem, and asked for clarification on what limitations are actually being addressed.\n\nReviewer suggested that ZONE_DEVICE_COHERENT could be extended to support the use case, proposing a couple of extra dev_pagemap_ops and LRU access as a potential solution.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "alternative solutions",
                "agreement",
                "endorsement",
                "suggested alternatives",
                "duplicates hooks",
                "similar to ZONE_DEVICE",
                "clarification requested",
                "questioning characterization"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Having had to re-implement entire portions of mm/ in a driver I agree this isn't\nsomething anyone sane should do :-) However aspects of ZONE_DEVICE were added\nprecisely to help with that so I'm not sure N_MEMORY_PRIVATE is the only or best\nway to do that.\n\nBased on our discussion at LPC I believe one of the primary motivators here was\nto re-use the existing mm buddy allocator rather than writing your own. I remain\nto be convinced that alone is justification enough for doing all this - DRM for\nexample already has quite a nice standalone buddy allocator (drm_buddy.c) that\ncould presumably be used, or adapted for use, by any device driver.\n\nThe interesting part of this series (which I have skimmed but not read in\ndetail) is how device memory gets exposed to userspace - this is something that\nexisting ZONE_DEVICE implementations don't address, instead leaving it up to\ndrivers and associated userspace stacks to deal with allocation, migration, etc.\n\n---\n\nThis is I think is one of the key things that should be enabled - providing a\nstandard interface to userspace for managing device memory. The existing NUMA\nAPIs do seem like a reasonable way to do this.\n\n---\n\nOne does not have to squint too hard to see that the above is not so different\nfrom what ZONE_DEVICE provides today via dev_pagemap_ops(). So I think I think\nit would be worth outlining why the existing ZONE_DEVICE mechanism can't be\nextended to provide these kind of services.\n\nThis seems to add a bunch of code just to use NODE_DATA instead of page->pgmap,\nwithout really explaining why just extending dev_pagemap_ops wouldn't work. The\nobvious reason is that if you want to support things like reclaim, compaction,\netc. these pages need to be on the LRU, which is a little bit hard when that\nfield is also used by the pgmap pointer for ZONE_DEVICE pages.\n\nBut it might be good to explore other options for storing the pgmap - for\nexample page_ext could be used.  Or I hear struct page may go away in place of\nfolios any day now, so maybe that gives us space for both :-)\n\n---\n\nThe above also looks pretty similar to the existing ZONE_DEVICE methods for\ndoing this which is another reason to argue for just building up the feature set\nof the existing boondoggle rather than adding another thingymebob.\n\nIt seems the key thing we are looking for is:\n\n1) A userspace API to allocate/manage device memory (ie. move_pages(), mbind(),\netc.)\n\n2) Allowing reclaim/LRU list processing of device memory.\n\n---\n\ndiscussion (hopefully I can make it to LSFMM). Mostly I'm interested in the\nimplementation as this does on the surface seem to sprinkle around and duplicate\na lot of hooks similar to what ZONE_DEVICE already provides.\n\n---\n\nFor basic allocation I agree this is the case. But there's no reason some device\nallocator library couldn't be written. Or in fact as pointed out above reuse the\nalready existing one in drm_buddy.c.  So would be interested to hear arguments\nfor why allocation has to be done by the mm allocator and/or why an allocation\nlibrary wouldn't work here given DRM already has them.\n\n---\n\nZONE_DEVICE pages are in fact real struct pages, but I will concede that\nperspective probably depends on which bits of the mm you play in. The real\nlimitations you seem to be addressing is more around how we get these pages in\nan LRU, or are there other limitations?\n\n---\n\nWhat I'd like to explore is why ZONE_DEVICE_COHERENT couldn't just be extended\nto support your usecase? It seems a couple of extra dev_pagemap_ops and being\nable to go on the LRU would get you there.\n\n - Alistair",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author acknowledges that using ZONE_DEVICE is insufficient for N_MEMORY_PRIVATE, as it introduces unnecessary complexity. They propose reusing the buddy allocator instead, which would simplify the implementation and eliminate issues related to zones.\n\nThe author explains that the callback similarity between ZONE_DEVICE and private nodes is intentional, as they require the same set of hooks but with different defaults. They argue that extending ZONE_DEVICE into these areas would be cumbersome and inefficient, and that the current implementation is a more straightforward solution.\n\nAuthor addressed a concern about the per-page pgmap and device-to-node mappings, agreeing that NODE_DATA is the right direction regardless of struct page's future or zone it lives in.\n\nThe author acknowledges that implementing mempolicy support for N_MEMORY_PRIVATE is more complex than initially thought, explaining that it requires adding code to vma_alloc_folio_noprof and dealing with ZONE_DEVICE's overloaded nature. They suggest two options: putting pages in the buddy or adding pgmap->device_alloc() callbacks at every allocation site.\n\nAuthor acknowledged reviewer's concern about reusing mm/ services and explained that using the buddy underpins the rest of these services, making it a more efficient approach.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a fix is needed",
                "proposes an alternative solution",
                "acknowledges feedback",
                "provides explanation",
                "agreed with reviewer's suggestion",
                "provided explanation",
                "acknowledges complexity",
                "suggests two options",
                "acknowledged",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree that buddy-access alone is insufficient justification, it\nstarted off that way - but if you want mempolicy/NUMA UAPI access,\nit turns into \"Re-use all of MM\" - and that means using the buddy.\n\nI also expected ZONE_DEVICE vs NODE_DATA to be the primary discussion,\n\nI raise replacing it as a thought experiment, but not the proposal.\n\nThe idea that drm/ is going to switch to private nodes is outside the\nrealm of reality, but part of that is because of years of infrastructure\nbuilt on the assumption that re-using mm/ is infeasible.\n\nBut, lets talk about DEVICE_COHERENT\n\n---\n\nDEVICE_COHERENT is the odd-man out among ZONE_DEVICE modes. The others\nuse softleaf entries and don't allow direct mappings.\n\n(DEVICE_PRIVATE sort of does if you squint, but you can also view that\n a bit like PROT_NONE or read-only controls to force migrations).\n\nIf you take DEVICE_COHERENT and:\n\n- Move pgmap out of the struct page (page_ext, NODE_DATA, etc) to free\n  the LRU list_head\n- Put pages in the buddy (free lists, watermarks, managed_pages) or add\n  pgmap->device_alloc() at every allocation callsite / buddy hook\n- Add LRU support (aging, reclaim, compaction)\n- Add isolated gating (new GFP flag and adjusted zonelist filtering)\n- Add new dev_pagemap_ops callbacks for the various mm/ features\n- Audit evey folio_is_zone_device() to distinguish zone device modes\n\n... you've built N_MEMORY_PRIVATE inside ZONE_DEVICE. Except now\npage_zone(page) returns ZONE_DEVICE - so you inherit the wrong\ndefaults at every existing ZONE_DEVICE check. \n\nSkip-sites become things to opt-out of instead of opting into.\n\nYou just end up with\n\nif (folio_is_zone_device(folio))\n    if (folio_is_my_special_zone_device())\n    else ....\n\nand this just generalizes to\n\nif (folio_is_private_managed(folio))\n    folio_managed_my_hooked_operation()\n\nSo you get the same code, but have added more complexity to ZONE_DEVICE.\n\nI don't think that's needed if we just recognize ZONE is the wrong\nabstraction to be operating on.\n\nHonestly, even ZONE_MOVABLE becomes pointless with N_MEMORY_PRIVATE\nif you disallow longterm pinning - because the managing service handles\nallocations (it has to inject GFP_PRIVATE to get access) or selectively\nenables the mm/ services it knows are safe (mempolicy).\n\nEven if you allow longterm pinning, if your service controls what does\nthe pinning it can still be reclaimable - just manually (killing\nprocesses) instead of letting hotplug do it via migration.\n\nIf your service only allocates movable pages - your ZONE_NORMAL is\neffectively ZONE_MOVABLE.  \n\nIn some cases we use ZONE_MOVABLE to prevent the kernel from allocating\nmemory onto devices (like CXL).  This means struct page is forced to\ntake up DRAM or use memmap_on_memory - meaning you lose high-value\ncapacity or sacrifice contiguity (less huge page support).\n\nThis entire problem can evaporate if you can just use ZONE_NORMAL.\n\nThere are a lot of benefits to just re-using the buddy like this.\n\nZones are the wrong abstraction and cause more problems.\n\n---\n\nYou don't have to squint because it was deliberate :]\n\nThe callback similarity is the feature - they're the same logical\noperations.  The difference is the direction of the defaults.\n\nExtending ZONE_DEVICE into these areas requires the same set of hooks,\nplus distinguishing \"old ZONE_DEVICE\" from \"new ZONE_DEVICE\".\n\nWhere there are new injection sites, it's because ZONE_DEVICE opts\nout of ever touching that code in some other silently implied way.\n\nFor example, reclaim/compaction doesn't run because ZONE_DEVICE doesn't\nadd to managed_pages (among other reasons).\n\nYou'd have to go figure out how to hack those things into ZONE_DEVICE \n*and then* opt every *other* ZONE_DEVICE mode *back out*.\n\nSo you still end up with something like this anyway:\n\nstatic inline bool folio_managed_handle_fault(struct folio *folio,\n                                              struct vm_fault *vmf,\n                                              enum pgtable_level level,\n                                              vm_fault_t *ret)\n{\n        /* Zone device pages use swap entries; handled in do_swap_page */\n        if (folio_is_zone_device(folio))\n                return false;\n\n        if (folio_is_private_node(folio))\n\t\t...\n        return false;\n}\n\n---\n\nIf NUMA is the interface we want, then NODE_DATA is the right direction\nregardless of struct page's future or what zone it lives in.\n\nThere's no reason to keep per-page pgmap w/ device-to-node mappings.\n\nYou can have one driver manage multiple devices with the same numa node\nif it uses the same owner context (PFN already differentiates devices).\n\nThe existing code allows for this.\n\n---\n\nOn (1): ZONE_DEVICE NUMA UAPI is harder than it looks from the surface\n\nMuch of the kernel mm/ infrastructure is written on top of the buddy and\nexpects N_MEMORY to be the sole arbiter of \"Where to Acquire Pages\".\n\nMempolicy depends on:\n   - Buddy support or a new alloc hook around the buddy\n\n   - Migration support (mbind() after allocation migrates)\n     - Migration also deeply assumes buddy and LRU support\n\n   - Changing validations on node states\n     - mempolicy checks N_MEMORY membership, so you have to hack\n       N_MEMORY onto ZONE_DEVICE\n       (or teach it about a new node state... N_MEMORY_PRIVATE)\n\n\nGetting mempolicy to work with N_MEMORY_PRIVATE amounts to adding 2\nlines of code in vma_alloc_folio_noprof:\n\nstruct folio *vma_alloc_folio_noprof(gfp_t gfp, int order,\n                                     struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr)\n{\n        if (pol->flags & MPOL_F_PRIVATE)\n                gfp |= __GFP_PRIVATE;\n\n        folio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n\t/* Woo! I faulted a DEVICE PAGE! */\n}\n\nBut this requires the pages to be managed by the buddy.\n\nThe rest of the mempolicy support is around keeping sane nodemasks when\nthings like cpuset.mems rebinds occur and validating you don't end up\nwith private nodes that don't support mempolicy in your nodemask.\n\nYou have to do all of this anyway, but with the added bonus of fighting\nwith the overloaded nature of ZONE_DEVICE at every step.\n\n==========\n\nOn (2): Assume you solve LRU. \n\nZone Device has no free lists, managed_pages, or watermarks.\n\nkswapd can't run, compaction has no targets, vmscan's pressure model\ndoesn't function.  These all come for free when the pages are\nbuddy-managed on a real zone.  Why re-invent the wheel?\n\n==========\n\nSo you really have two options here:\n\na) Put pages in the buddy, or\n\nb) Add pgmap->device_alloc() callbacks at every allocation site that\n   could target a node:\n     - vma_alloc_folio\n     - alloc_migration_target\n     - alloc_demote_folio\n     - alloc_pages_node\n     - alloc_contig_pages\n     - list goes on\n\nOr more likely - hooking get_page_from_freelist.  Which at that\npoint... just use the buddy?  You're already deep in the hot path.\n\n---\n\nUsing the buddy underpins the rest of mm/ services we want to re-use.\n\nThat's basically it.  Otherwise you have to inject hooks into every\nsurface that touches the buddy...\n\n... or in the buddy (get_page_from_freelist), at which point why not\njust use the buddy?\n\n~Gregory",
              "reply_to": "Alistair Popple",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author considered reviewer's suggestion to simplify patch by removing N_MEMORY_PRIVATE and instead checking NODE_DATA(target_nid)->private, agreeing to explore this alternative.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "considering alternative approach",
                "agrees to look at it more"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This gave me something to chew on\n\nI think this can be done without introducing N_MEMORY_PRIVATE and just\nchecking:   NODE_DATA(target_nid)->private\n\nmeaning these nodes can just be N_MEMORY with the same isolations.\n\nI'll look at this a bit more.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH v5 00/10] mm: Hot page tracking and promotion infrastructure",
          "message_id": "aZ3D_8GJit3FYhQc@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3D_8GJit3FYhQc@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T15:30:11Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed a concern about the migrate_misplaced_folio_prepare() function requiring a non-NULL VMA to hold the PTL lock, and explained that when called with a NULL VMA, it does not require the caller to hold the PTL. The author made changes to the mm/migrate.c file to allow for this.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "We want isolation of misplaced folios to work in contexts\nwhere VMA isn't available, typically when performing migrations\nfrom a kernel thread context. In order to prepare for that,\nallow migrate_misplaced_folio_prepare() to be called with\na NULL VMA.\n\nWhen migrate_misplaced_folio_prepare() is called with non-NULL\nVMA, it will check if the folio is mapped shared and that requires\nholding PTL lock. This path isn't taken when the function is\ninvoked with NULL VMA (migration outside of process context).\nTherefore, when VMA == NULL, migrate_misplaced_folio_prepare()\ndoes not require the caller to hold the PTL.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n mm/migrate.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 5169f9717f60..70f8f3ad4fd8 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -2652,7 +2652,8 @@ static struct folio *alloc_misplaced_dst_folio(struct folio *src,\n \n /*\n  * Prepare for calling migrate_misplaced_folio() by isolating the folio if\n- * permitted. Must be called with the PTL still held.\n+ * permitted. Must be called with the PTL still held if called with a non-NULL\n+ * vma.\n  */\n int migrate_misplaced_folio_prepare(struct folio *folio,\n \t\tstruct vm_area_struct *vma, int node)\n@@ -2669,7 +2670,7 @@ int migrate_misplaced_folio_prepare(struct folio *folio,\n \t\t * See folio_maybe_mapped_shared() on possible imprecision\n \t\t * when we cannot easily detect if a folio is shared.\n \t\t */\n-\t\tif ((vma->vm_flags & VM_EXEC) && folio_maybe_mapped_shared(folio))\n+\t\tif (vma && (vma->vm_flags & VM_EXEC) && folio_maybe_mapped_shared(folio))\n \t\t\treturn -EACCES;\n \n \t\t/*\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author is addressing a concern about the efficiency of migrating multiple folios at once, which is inefficient for batch operations. The author introduces migrate_misplaced_folios_batch(), a batch variant that leverages migrate_pages() internally for improved performance.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "introduces new functionality",
                "acknowledges existing inefficiency"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Gregory Price <gourry@gourry.net>\n\nTiered memory systems often require migrating multiple folios at once.\nCurrently, migrate_misplaced_folio() handles only one folio per call,\nwhich is inefficient for batch operations. This patch introduces\nmigrate_misplaced_folios_batch(), a batch variant that leverages\nmigrate_pages() internally for improved performance.\n\nThe caller must isolate folios beforehand using\nmigrate_misplaced_folio_prepare(). On return, the folio list will be\nempty regardless of success or failure.\n\nThis function will be used by pghot kmigrated thread.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n[Rewrote commit description]\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n include/linux/migrate.h |  6 ++++++\n mm/migrate.c            | 36 ++++++++++++++++++++++++++++++++++++\n 2 files changed, 42 insertions(+)\n\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 26ca00c325d9..f28326b88592 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -103,6 +103,7 @@ static inline int set_movable_ops(const struct movable_operations *ops, enum pag\n int migrate_misplaced_folio_prepare(struct folio *folio,\n \t\tstruct vm_area_struct *vma, int node);\n int migrate_misplaced_folio(struct folio *folio, int node);\n+int migrate_misplaced_folios_batch(struct list_head *folio_list, int node);\n #else\n static inline int migrate_misplaced_folio_prepare(struct folio *folio,\n \t\tstruct vm_area_struct *vma, int node)\n@@ -113,6 +114,11 @@ static inline int migrate_misplaced_folio(struct folio *folio, int node)\n {\n \treturn -EAGAIN; /* can't migrate now */\n }\n+static inline int migrate_misplaced_folios_batch(struct list_head *folio_list,\n+\t\t\t\t\t\t int node)\n+{\n+\treturn -EAGAIN; /* can't migrate now */\n+}\n #endif /* CONFIG_NUMA_BALANCING */\n \n #ifdef CONFIG_MIGRATION\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 70f8f3ad4fd8..4a3a9a4ff435 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -2747,5 +2747,41 @@ int migrate_misplaced_folio(struct folio *folio, int node)\n \tBUG_ON(!list_empty(&migratepages));\n \treturn nr_remaining ? -EAGAIN : 0;\n }\n+\n+/**\n+ * migrate_misplaced_folios_batch() - Batch variant of migrate_misplaced_folio.\n+ * Attempts to migrate a folio list to the specified destination.\n+ * @folio_list: Isolated list of folios to be batch-migrated.\n+ * @node: The NUMA node ID to where the folios should be migrated.\n+ *\n+ * Caller is expected to have isolated the folios by calling\n+ * migrate_misplaced_folio_prepare(), which will result in an\n+ * elevated reference count on the folio.\n+ *\n+ * This function will un-isolate the folios, drop the elevated reference\n+ * and remove them from the list before returning.\n+ *\n+ * Return: 0 on success and -EAGAIN on failure or partial migration.\n+ *         On return, @folio_list will be empty regardless of success/failure.\n+ */\n+int migrate_misplaced_folios_batch(struct list_head *folio_list, int node)\n+{\n+\tpg_data_t *pgdat = NODE_DATA(node);\n+\tunsigned int nr_succeeded = 0;\n+\tint nr_remaining;\n+\n+\tnr_remaining = migrate_pages(folio_list, alloc_misplaced_dst_folio,\n+\t\t\t\t     NULL, node, MIGRATE_ASYNC,\n+\t\t\t\t     MR_NUMA_MISPLACED, &nr_succeeded);\n+\tif (nr_remaining)\n+\t\tputback_movable_pages(folio_list);\n+\n+\tif (nr_succeeded) {\n+\t\tcount_vm_numa_events(NUMA_PAGE_MIGRATE, nr_succeeded);\n+\t\tmod_node_page_state(pgdat, PGPROMOTE_SUCCESS, nr_succeeded);\n+\t}\n+\tWARN_ON(!list_empty(folio_list));\n+\treturn nr_remaining ? -EAGAIN : 0;\n+}\n #endif /* CONFIG_NUMA_BALANCING */\n #endif /* CONFIG_NUMA */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed a concern about the handling of page table scanning for PTE Accessed bit, explaining that some temperature sources may not provide the nid from which the page was accessed and that a configurable/default toptier node is used as promotion target. The author provided details on how this works in their subsystem.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This introduces a subsystem for collecting memory access\ninformation from different sources. It maintains the hotness\ninformation based on the access history and time of access.\n\nAdditionally, it provides per-lower-tier-node kernel threads\n(named kmigrated) that periodically promote the pages that\nare eligible for promotion.\n\nSub-systems that generate hot page access info can report that\nusing this API:\n\nint pghot_record_access(unsigned long pfn, int nid, int src,\n                        unsigned long time)\n\n@pfn: The PFN of the memory accessed\n@nid: The accessing NUMA node ID\n@src: The temperature source (subsystem) that generated the\n      access info\n@time: The access time in jiffies\n\nSome temperature sources may not provide the nid from which\nthe page was accessed. This is true for sources that use\npage table scanning for PTE Accessed bit. For such sources,\na configurable/default toptier node is used as promotion\ntarget.\n\nThe hotness information is stored for every page of lower\ntier memory in a u8 variable (1 byte) that is part of\nmem_section data structure.\n\nkmigrated is a per-lower-tier-node kernel thread that migrates\nthe folios marked for migration in batches. Each kmigrated\nthread walks the PFN range spanning its node and checks\nfor potential migration candidates.\n\nA bunch of tunables for enabling different hotness sources,\nsetting target_nid, frequency threshold are provided in debugfs.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n Documentation/admin-guide/mm/pghot.txt |  84 ++++++\n include/linux/mmzone.h                 |  21 ++\n include/linux/pghot.h                  |  94 +++++++\n include/linux/vm_event_item.h          |   6 +\n mm/Kconfig                             |  14 +\n mm/Makefile                            |   1 +\n mm/mm_init.c                           |  10 +\n mm/pghot-default.c                     |  73 +++++\n mm/pghot-tunables.c                    | 189 +++++++++++++\n mm/pghot.c                             | 370 +++++++++++++++++++++++++\n mm/vmstat.c                            |   6 +\n 11 files changed, 868 insertions(+)\n create mode 100644 Documentation/admin-guide/mm/pghot.txt\n create mode 100644 include/linux/pghot.h\n create mode 100644 mm/pghot-default.c\n create mode 100644 mm/pghot-tunables.c\n create mode 100644 mm/pghot.c\n\ndiff --git a/Documentation/admin-guide/mm/pghot.txt b/Documentation/admin-guide/mm/pghot.txt\nnew file mode 100644\nindex 000000000000..01291b72e7ab\n--- /dev/null\n+++ b/Documentation/admin-guide/mm/pghot.txt\n@@ -0,0 +1,84 @@\n+.. SPDX-License-Identifier: GPL-2.0\n+\n+=================================\n+PGHOT: Hot Page Tracking Tunables\n+=================================\n+\n+Overview\n+========\n+The PGHOT subsystem tracks frequently accessed pages in lower-tier memory and\n+promotes them to faster tiers. It uses per-PFN hotness metadata and asynchronous\n+migration via per-node kernel threads (kmigrated).\n+\n+This document describes tunables available via **debugfs** and **sysctl** for\n+PGHOT.\n+\n+Debugfs Interface\n+=================\n+Path: /sys/kernel/debug/pghot/\n+\n+1. **enabled_sources**\n+   - Bitmask to enable/disable hotness sources.\n+   - Bits:\n+     - 0: Hardware hints (value 0x1)\n+     - 1: Page table scan (value 0x2)\n+     - 2: Hint faults (value 0x4)\n+   - Default: 0 (disabled)\n+   - Example:\n+     # echo 0x7 > /sys/kernel/debug/pghot/enabled_sources\n+     Enables all sources.\n+\n+2. **target_nid**\n+   - Toptier NUMA node ID to which hot pages should be promoted when source\n+     does not provide nid. Used when hotness source can't provide accessing\n+     NID or when the tracking mode is default.\n+   - Default: 0\n+   - Example:\n+     # echo 1 > /sys/kernel/debug/pghot/target_nid\n+\n+3. **freq_threshold**\n+   - Minimum access frequency before a page is marked ready for promotion.\n+   - Range: 1 to 3\n+   - Default: 2\n+   - Example:\n+     # echo 3 > /sys/kernel/debug/pghot/freq_threshold\n+\n+4. **kmigrated_sleep_ms**\n+   - Sleep interval (ms) for kmigrated thread between scans.\n+   - Default: 100\n+\n+5. **kmigrated_batch_nr**\n+   - Maximum number of folios migrated in one batch.\n+   - Default: 512\n+\n+Sysctl Interface\n+================\n+1. pghot_promote_freq_window_ms\n+\n+Path: /proc/sys/vm/pghot_promote_freq_window_ms\n+\n+- Controls the time window (in ms) for counting access frequency. A page is\n+  considered hot only when **freq_threshold** number of accesses occur with\n+  this time period.\n+- Default: 4000 (4 seconds)\n+- Example:\n+  # sysctl vm.pghot_promote_freq_window_ms=3000\n+\n+Vmstat Counters\n+===============\n+Following vmstat counters provide some stats about pghot subsystem.\n+\n+Path: /proc/vmstat\n+\n+1. **pghot_recorded_accesses**\n+   - Number of total hot page accesses recorded by pghot.\n+\n+2. **pghot_recorded_hwhints**\n+   - Number of recorded accesses reported by hwhints source.\n+\n+3. **pghot_recorded_pgtscans**\n+   - Number of recorded accesses reported by PTE A-bit based source.\n+\n+4. **pghot_recorded_hintfaults**\n+   - Number of recorded accesses reported by NUMA Balancing based\n+     hotness source.\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 75ef7c9f9307..22e08befb096 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1064,6 +1064,7 @@ enum pgdat_flags {\n \t\t\t\t\t * many pages under writeback\n \t\t\t\t\t */\n \tPGDAT_RECLAIM_LOCKED,\t\t/* prevents concurrent reclaim */\n+\tPGDAT_KMIGRATED_ACTIVATE,\t/* activates kmigrated */\n };\n \n enum zone_flags {\n@@ -1518,6 +1519,10 @@ typedef struct pglist_data {\n #ifdef CONFIG_MEMORY_FAILURE\n \tstruct memory_failure_stats mf_stats;\n #endif\n+#ifdef CONFIG_PGHOT\n+\tstruct task_struct *kmigrated;\n+\twait_queue_head_t kmigrated_wait;\n+#endif\n } pg_data_t;\n \n #define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n@@ -1916,12 +1921,28 @@ struct mem_section {\n \tunsigned long section_mem_map;\n \n \tstruct mem_section_usage *usage;\n+#ifdef CONFIG_PGHOT\n+\t/*\n+\t * Per-PFN hotness data for this section.\n+\t * Array of phi_t (u8 in default mode).\n+\t * LSB is used as PGHOT_SECTION_HOT_BIT flag.\n+\t */\n+\tvoid *hot_map;\n+#endif\n #ifdef CONFIG_PAGE_EXTENSION\n \t/*\n \t * If SPARSEMEM, pgdat doesn't have page_ext pointer. We use\n \t * section. (see page_ext.h about this.)\n \t */\n \tstruct page_ext *page_ext;\n+#endif\n+\t/*\n+\t * Padding to maintain consistent mem_section size when exactly\n+\t * one of PGHOT or PAGE_EXTENSION is enabled. This ensures\n+\t * optimal alignment regardless of configuration.\n+\t */\n+#if (defined(CONFIG_PGHOT) && !defined(CONFIG_PAGE_EXTENSION)) || \\\n+\t\t(!defined(CONFIG_PGHOT) && defined(CONFIG_PAGE_EXTENSION))\n \tunsigned long pad;\n #endif\n \t/*\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nnew file mode 100644\nindex 000000000000..88e57aab697b\n--- /dev/null\n+++ b/include/linux/pghot.h\n@@ -0,0 +1,94 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_PGHOT_H\n+#define _LINUX_PGHOT_H\n+\n+/* Page hotness temperature sources */\n+enum pghot_src {\n+\tPGHOT_HW_HINTS,\n+\tPGHOT_PGTABLE_SCAN,\n+\tPGHOT_HINT_FAULT,\n+};\n+\n+#ifdef CONFIG_PGHOT\n+#include <linux/static_key.h>\n+\n+extern unsigned int pghot_target_nid;\n+extern unsigned int pghot_src_enabled;\n+extern unsigned int pghot_freq_threshold;\n+extern unsigned int kmigrated_sleep_ms;\n+extern unsigned int kmigrated_batch_nr;\n+extern unsigned int sysctl_pghot_freq_window;\n+\n+void pghot_debug_init(void);\n+\n+DECLARE_STATIC_KEY_FALSE(pghot_src_hwhints);\n+DECLARE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n+DECLARE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n+\n+/*\n+ * Bit positions to enable individual sources in pghot/records_enabled\n+ * of debugfs.\n+ */\n+enum pghot_src_enabled {\n+\tPGHOT_HWHINTS_BIT = 0,\n+\tPGHOT_PGTSCAN_BIT,\n+\tPGHOT_HINTFAULT_BIT,\n+\tPGHOT_MAX_BIT\n+};\n+\n+#define PGHOT_HWHINTS_ENABLED\t\tBIT(PGHOT_HWHINTS_BIT)\n+#define PGHOT_PGTSCAN_ENABLED\t\tBIT(PGHOT_PGTSCAN_BIT)\n+#define PGHOT_HINTFAULT_ENABLED\t\tBIT(PGHOT_HINTFAULT_BIT)\n+#define PGHOT_SRC_ENABLED_MASK\t\tGENMASK(PGHOT_MAX_BIT - 1, 0)\n+\n+#define PGHOT_DEFAULT_FREQ_THRESHOLD\t2\n+\n+#define KMIGRATED_DEFAULT_SLEEP_MS\t100\n+#define KMIGRATED_DEFAULT_BATCH_NR\t512\n+\n+#define PGHOT_DEFAULT_NODE\t\t0\n+\n+#define PGHOT_DEFAULT_FREQ_WINDOW\t(4 * MSEC_PER_SEC)\n+\n+/*\n+ * Bits 0-6 are used to store frequency and time.\n+ * Bit 7 is used to indicate the page is ready for migration.\n+ */\n+#define PGHOT_MIGRATE_READY\t\t7\n+\n+#define PGHOT_FREQ_WIDTH\t\t2\n+/* Bucketed time is stored in 5 bits which can represent up to 4s with HZ=1000 */\n+#define PGHOT_TIME_BUCKETS_WIDTH\t7\n+#define PGHOT_TIME_WIDTH\t\t5\n+#define PGHOT_NID_WIDTH\t\t\t10\n+\n+#define PGHOT_FREQ_SHIFT\t\t0\n+#define PGHOT_TIME_SHIFT\t\t(PGHOT_FREQ_SHIFT + PGHOT_FREQ_WIDTH)\n+\n+#define PGHOT_FREQ_MASK\t\t\tGENMASK(PGHOT_FREQ_WIDTH - 1, 0)\n+#define PGHOT_TIME_MASK\t\t\tGENMASK(PGHOT_TIME_WIDTH - 1, 0)\n+#define PGHOT_TIME_BUCKETS_MASK\t\t(PGHOT_TIME_MASK << PGHOT_TIME_BUCKETS_WIDTH)\n+\n+#define PGHOT_NID_MAX\t\t\t((1 << PGHOT_NID_WIDTH) - 1)\n+#define PGHOT_FREQ_MAX\t\t\t((1 << PGHOT_FREQ_WIDTH) - 1)\n+#define PGHOT_TIME_MAX\t\t\t((1 << PGHOT_TIME_WIDTH) - 1)\n+\n+typedef u8 phi_t;\n+\n+#define PGHOT_RECORD_SIZE\t\tsizeof(phi_t)\n+\n+#define PGHOT_SECTION_HOT_BIT\t\t0\n+#define PGHOT_SECTION_HOT_MASK\t\tBIT(PGHOT_SECTION_HOT_BIT)\n+\n+unsigned long pghot_access_latency(unsigned long old_time, unsigned long time);\n+bool pghot_update_record(phi_t *phi, int nid, unsigned long now);\n+int pghot_get_record(phi_t *phi, int *nid, int *freq, unsigned long *time);\n+\n+int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now);\n+#else\n+static inline int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now)\n+{\n+\treturn 0;\n+}\n+#endif /* CONFIG_PGHOT */\n+#endif /* _LINUX_PGHOT_H */\ndiff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\nindex 92f80b4d69a6..5b8fd93b55fd 100644\n--- a/include/linux/vm_event_item.h\n+++ b/include/linux/vm_event_item.h\n@@ -188,6 +188,12 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n \t\tKSTACK_REST,\n #endif\n #endif /* CONFIG_DEBUG_STACK_USAGE */\n+#ifdef CONFIG_PGHOT\n+\t\tPGHOT_RECORDED_ACCESSES,\n+\t\tPGHOT_RECORD_HWHINTS,\n+\t\tPGHOT_RECORD_PGTSCANS,\n+\t\tPGHOT_RECORD_HINTFAULTS,\n+#endif /* CONFIG_PGHOT */\n \t\tNR_VM_EVENT_ITEMS\n };\n \ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex bd0ea5454af8..f4f0147faac5 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -1464,6 +1464,20 @@ config PT_RECLAIM\n config FIND_NORMAL_PAGE\n \tdef_bool n\n \n+config PGHOT\n+\tbool \"Hot page tracking and promotion\"\n+\tdef_bool n\n+\tdepends on NUMA && MIGRATION && SPARSEMEM && MMU\n+\thelp\n+\t  A sub-system to track page accesses in lower tier memory and\n+\t  maintain hot page information. Promotes hot pages from lower\n+\t  tiers to top tier by using the memory access information provided\n+\t  by various sources. Asynchronous promotion is done by per-node\n+\t  kernel threads.\n+\n+\t  This adds 1 byte of metadata overhead per page in lower-tier\n+\t  memory nodes.\n+\n source \"mm/damon/Kconfig\"\n \n endmenu\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 2d0570a16e5b..655a27f3a215 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -147,3 +147,4 @@ obj-$(CONFIG_SHRINKER_DEBUG) += shrinker_debug.o\n obj-$(CONFIG_EXECMEM) += execmem.o\n obj-$(CONFIG_TMPFS_QUOTA) += shmem_quota.o\n obj-$(CONFIG_PT_RECLAIM) += pt_reclaim.o\n+obj-$(CONFIG_PGHOT) += pghot.o pghot-tunables.o pghot-default.o\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex fc2a6f1e518f..64109feaa1c3 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -1401,6 +1401,15 @@ static void pgdat_init_kcompactd(struct pglist_data *pgdat)\n static void pgdat_init_kcompactd(struct pglist_data *pgdat) {}\n #endif\n \n+#ifdef CONFIG_PGHOT\n+static void pgdat_init_kmigrated(struct pglist_data *pgdat)\n+{\n+\tinit_waitqueue_head(&pgdat->kmigrated_wait);\n+}\n+#else\n+static inline void pgdat_init_kmigrated(struct pglist_data *pgdat) {}\n+#endif\n+\n static void __meminit pgdat_init_internals(struct pglist_data *pgdat)\n {\n \tint i;\n@@ -1410,6 +1419,7 @@ static void __meminit pgdat_init_internals(struct pglist_data *pgdat)\n \n \tpgdat_init_split_queue(pgdat);\n \tpgdat_init_kcompactd(pgdat);\n+\tpgdat_init_kmigrated(pgdat);\n \n \tinit_waitqueue_head(&pgdat->kswapd_wait);\n \tinit_waitqueue_head(&pgdat->pfmemalloc_wait);\ndiff --git a/mm/pghot-default.c b/mm/pghot-default.c\nnew file mode 100644\nindex 000000000000..e0a3b2ed2592\n--- /dev/null\n+++ b/mm/pghot-default.c\n@@ -0,0 +1,73 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * pghot: Default mode\n+ *\n+ * 1 byte hotness record per PFN.\n+ * Bucketed time and frequency tracked as part of the record.\n+ * Promotion to @pghot_target_nid by default.\n+ */\n+\n+#include <linux/pghot.h>\n+#include <linux/jiffies.h>\n+\n+/*\n+ * @time is regular time, @old_time is bucketed time.\n+ */\n+unsigned long pghot_access_latency(unsigned long old_time, unsigned long time)\n+{\n+\ttime &= PGHOT_TIME_BUCKETS_MASK;\n+\told_time <<= PGHOT_TIME_BUCKETS_WIDTH;\n+\n+\treturn jiffies_to_msecs((time - old_time) & PGHOT_TIME_BUCKETS_MASK);\n+}\n+\n+bool pghot_update_record(phi_t *phi, int nid, unsigned long now)\n+{\n+\tphi_t freq, old_freq, hotness, old_hotness, old_time;\n+\tphi_t time = now >> PGHOT_TIME_BUCKETS_WIDTH;\n+\n+\told_hotness = READ_ONCE(*phi);\n+\tdo {\n+\t\tbool new_window = false;\n+\n+\t\thotness = old_hotness;\n+\t\told_freq = (hotness >> PGHOT_FREQ_SHIFT) & PGHOT_FREQ_MASK;\n+\t\told_time = (hotness >> PGHOT_TIME_SHIFT) & PGHOT_TIME_MASK;\n+\n+\t\tif (pghot_access_latency(old_time, now) > sysctl_pghot_freq_window)\n+\t\t\tnew_window = true;\n+\n+\t\tif (new_window)\n+\t\t\tfreq = 1;\n+\t\telse if (old_freq < PGHOT_FREQ_MAX)\n+\t\t\tfreq = old_freq + 1;\n+\t\telse\n+\t\t\tfreq = old_freq;\n+\n+\t\thotness &= ~(PGHOT_FREQ_MASK << PGHOT_FREQ_SHIFT);\n+\t\thotness &= ~(PGHOT_TIME_MASK << PGHOT_TIME_SHIFT);\n+\n+\t\thotness |= (freq & PGHOT_FREQ_MASK) << PGHOT_FREQ_SHIFT;\n+\t\thotness |= (time & PGHOT_TIME_MASK) << PGHOT_TIME_SHIFT;\n+\n+\t\tif (freq >= pghot_freq_threshold)\n+\t\t\thotness |= BIT(PGHOT_MIGRATE_READY);\n+\t} while (unlikely(!try_cmpxchg(phi, &old_hotness, hotness)));\n+\treturn !!(hotness & BIT(PGHOT_MIGRATE_READY));\n+}\n+\n+int pghot_get_record(phi_t *phi, int *nid, int *freq, unsigned long *time)\n+{\n+\tphi_t old_hotness, hotness = 0;\n+\n+\told_hotness = READ_ONCE(*phi);\n+\tdo {\n+\t\tif (!(old_hotness & BIT(PGHOT_MIGRATE_READY)))\n+\t\t\treturn -EINVAL;\n+\t} while (unlikely(!try_cmpxchg(phi, &old_hotness, hotness)));\n+\n+\t*nid = pghot_target_nid;\n+\t*freq = (old_hotness >> PGHOT_FREQ_SHIFT) & PGHOT_FREQ_MASK;\n+\t*time = (old_hotness >> PGHOT_TIME_SHIFT) & PGHOT_TIME_MASK;\n+\treturn 0;\n+}\ndiff --git a/mm/pghot-tunables.c b/mm/pghot-tunables.c\nnew file mode 100644\nindex 000000000000..79afbcb1e4f0\n--- /dev/null\n+++ b/mm/pghot-tunables.c\n@@ -0,0 +1,189 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * pghot tunables in debugfs\n+ */\n+#include <linux/pghot.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/debugfs.h>\n+\n+static struct dentry *debugfs_pghot;\n+static DEFINE_MUTEX(pghot_tunables_lock);\n+\n+static ssize_t pghot_freq_th_write(struct file *filp, const char __user *ubuf,\n+\t\t\t\t   size_t cnt, loff_t *ppos)\n+{\n+\tchar buf[16];\n+\tunsigned int freq;\n+\n+\tif (cnt > 15)\n+\t\tcnt = 15;\n+\n+\tif (copy_from_user(&buf, ubuf, cnt))\n+\t\treturn -EFAULT;\n+\tbuf[cnt] = '\\0';\n+\n+\tif (kstrtouint(buf, 10, &freq))\n+\t\treturn -EINVAL;\n+\n+\tif (!freq || freq > PGHOT_FREQ_MAX)\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&pghot_tunables_lock);\n+\tpghot_freq_threshold = freq;\n+\tmutex_unlock(&pghot_tunables_lock);\n+\n+\t*ppos += cnt;\n+\treturn cnt;\n+}\n+\n+static int pghot_freq_th_show(struct seq_file *m, void *v)\n+{\n+\tseq_printf(m, \"%d\\n\", pghot_freq_threshold);\n+\treturn 0;\n+}\n+\n+static int pghot_freq_th_open(struct inode *inode, struct file *filp)\n+{\n+\treturn single_open(filp, pghot_freq_th_show, NULL);\n+}\n+\n+static const struct file_operations pghot_freq_th_fops = {\n+\t.open\t\t= pghot_freq_th_open,\n+\t.write\t\t= pghot_freq_th_write,\n+\t.read\t\t= seq_read,\n+\t.llseek\t\t= seq_lseek,\n+\t.release\t= seq_release,\n+};\n+\n+static ssize_t pghot_target_nid_write(struct file *filp, const char __user *ubuf,\n+\t\t\t\t      size_t cnt, loff_t *ppos)\n+{\n+\tchar buf[16];\n+\tunsigned int nid;\n+\n+\tif (cnt > 15)\n+\t\tcnt = 15;\n+\n+\tif (copy_from_user(&buf, ubuf, cnt))\n+\t\treturn -EFAULT;\n+\tbuf[cnt] = '\\0';\n+\n+\tif (kstrtouint(buf, 10, &nid))\n+\t\treturn -EINVAL;\n+\n+\tif (nid > PGHOT_NID_MAX || !node_online(nid) || !node_is_toptier(nid))\n+\t\treturn -EINVAL;\n+\tmutex_lock(&pghot_tunables_lock);\n+\tpghot_target_nid = nid;\n+\tmutex_unlock(&pghot_tunables_lock);\n+\n+\t*ppos += cnt;\n+\treturn cnt;\n+}\n+\n+static int pghot_target_nid_show(struct seq_file *m, void *v)\n+{\n+\tseq_printf(m, \"%d\\n\", pghot_target_nid);\n+\treturn 0;\n+}\n+\n+static int pghot_target_nid_open(struct inode *inode, struct file *filp)\n+{\n+\treturn single_open(filp, pghot_target_nid_show, NULL);\n+}\n+\n+static const struct file_operations pghot_target_nid_fops = {\n+\t.open\t\t= pghot_target_nid_open,\n+\t.write\t\t= pghot_target_nid_write,\n+\t.read\t\t= seq_read,\n+\t.llseek\t\t= seq_lseek,\n+\t.release\t= seq_release,\n+};\n+\n+static void pghot_src_enabled_update(unsigned int enabled)\n+{\n+\tunsigned int changed = pghot_src_enabled ^ enabled;\n+\n+\tif (changed & PGHOT_HWHINTS_ENABLED) {\n+\t\tif (enabled & PGHOT_HWHINTS_ENABLED)\n+\t\t\tstatic_branch_enable(&pghot_src_hwhints);\n+\t\telse\n+\t\t\tstatic_branch_disable(&pghot_src_hwhints);\n+\t}\n+\n+\tif (changed & PGHOT_PGTSCAN_ENABLED) {\n+\t\tif (enabled & PGHOT_PGTSCAN_ENABLED)\n+\t\t\tstatic_branch_enable(&pghot_src_pgtscans);\n+\t\telse\n+\t\t\tstatic_branch_disable(&pghot_src_pgtscans);\n+\t}\n+\n+\tif (changed & PGHOT_HINTFAULT_ENABLED) {\n+\t\tif (enabled & PGHOT_HINTFAULT_ENABLED)\n+\t\t\tstatic_branch_enable(&pghot_src_hintfaults);\n+\t\telse\n+\t\t\tstatic_branch_disable(&pghot_src_hintfaults);\n+\t}\n+}\n+\n+static ssize_t pghot_src_enabled_write(struct file *filp, const char __user *ubuf,\n+\t\t\t\t\t   size_t cnt, loff_t *ppos)\n+{\n+\tchar buf[16];\n+\tunsigned int enabled;\n+\n+\tif (cnt > 15)\n+\t\tcnt = 15;\n+\n+\tif (copy_from_user(&buf, ubuf, cnt))\n+\t\treturn -EFAULT;\n+\tbuf[cnt] = '\\0';\n+\n+\tif (kstrtouint(buf, 0, &enabled))\n+\t\treturn -EINVAL;\n+\n+\tif (enabled & ~PGHOT_SRC_ENABLED_MASK)\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&pghot_tunables_lock);\n+\tpghot_src_enabled_update(enabled);\n+\tpghot_src_enabled = enabled;\n+\tmutex_unlock(&pghot_tunables_lock);\n+\n+\t*ppos += cnt;\n+\treturn cnt;\n+}\n+\n+static int pghot_src_enabled_show(struct seq_file *m, void *v)\n+{\n+\tseq_printf(m, \"%d\\n\", pghot_src_enabled);\n+\treturn 0;\n+}\n+\n+static int pghot_src_enabled_open(struct inode *inode, struct file *filp)\n+{\n+\treturn single_open(filp, pghot_src_enabled_show, NULL);\n+}\n+\n+static const struct file_operations pghot_src_enabled_fops = {\n+\t.open\t\t= pghot_src_enabled_open,\n+\t.write\t\t= pghot_src_enabled_write,\n+\t.read\t\t= seq_read,\n+\t.llseek\t\t= seq_lseek,\n+\t.release\t= seq_release,\n+};\n+\n+void pghot_debug_init(void)\n+{\n+\tdebugfs_pghot = debugfs_create_dir(\"pghot\", NULL);\n+\tdebugfs_create_file(\"enabled_sources\", 0644, debugfs_pghot, NULL,\n+\t\t\t    &pghot_src_enabled_fops);\n+\tdebugfs_create_file(\"target_nid\", 0644, debugfs_pghot, NULL,\n+\t\t\t    &pghot_target_nid_fops);\n+\tdebugfs_create_file(\"freq_threshold\", 0644, debugfs_pghot, NULL,\n+\t\t\t    &pghot_freq_th_fops);\n+\tdebugfs_create_u32(\"kmigrated_sleep_ms\", 0644, debugfs_pghot,\n+\t\t\t    &kmigrated_sleep_ms);\n+\tdebugfs_create_u32(\"kmigrated_batch_nr\", 0644, debugfs_pghot,\n+\t\t\t    &kmigrated_batch_nr);\n+}\ndiff --git a/mm/pghot.c b/mm/pghot.c\nnew file mode 100644\nindex 000000000000..95b5012d5b99\n--- /dev/null\n+++ b/mm/pghot.c\n@@ -0,0 +1,370 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * Maintains information about hot pages from slower tier nodes and\n+ * promotes them.\n+ *\n+ * Per-PFN hotness information is stored for lower tier nodes in\n+ * mem_section.\n+ *\n+ * In the default mode, a single byte (u8) is used to store\n+ * the frequency of access and last access time. Promotions are done\n+ * to a default toptier NID.\n+ *\n+ * A kernel thread named kmigrated is provided to migrate or promote\n+ * the hot pages. kmigrated runs for each lower tier node. It iterates\n+ * over the node's PFNs and  migrates pages marked for migration into\n+ * their targeted nodes.\n+ */\n+#include <linux/mm.h>\n+#include <linux/migrate.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/pghot.h>\n+\n+unsigned int pghot_target_nid = PGHOT_DEFAULT_NODE;\n+unsigned int pghot_src_enabled;\n+unsigned int pghot_freq_threshold = PGHOT_DEFAULT_FREQ_THRESHOLD;\n+unsigned int kmigrated_sleep_ms = KMIGRATED_DEFAULT_SLEEP_MS;\n+unsigned int kmigrated_batch_nr = KMIGRATED_DEFAULT_BATCH_NR;\n+\n+unsigned int sysctl_pghot_freq_window = PGHOT_DEFAULT_FREQ_WINDOW;\n+\n+DEFINE_STATIC_KEY_FALSE(pghot_src_hwhints);\n+DEFINE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n+DEFINE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n+\n+#ifdef CONFIG_SYSCTL\n+static const struct ctl_table pghot_sysctls[] = {\n+\t{\n+\t\t.procname       = \"pghot_promote_freq_window_ms\",\n+\t\t.data           = &sysctl_pghot_freq_window,\n+\t\t.maxlen         = sizeof(unsigned int),\n+\t\t.mode           = 0644,\n+\t\t.proc_handler   = proc_dointvec_minmax,\n+\t\t.extra1         = SYSCTL_ZERO,\n+\t},\n+};\n+#endif\n+\n+static bool kmigrated_started __ro_after_init;\n+\n+/**\n+ * pghot_record_access() - Record page accesses from lower tier memory\n+ * for the purpose of tracking page hotness and subsequent promotion.\n+ *\n+ * @pfn: PFN of the page\n+ * @nid: Unused\n+ * @src: The identifier of the sub-system that reports the access\n+ * @now: Access time in jiffies\n+ *\n+ * Updates the frequency and time of access and marks the page as\n+ * ready for migration if the frequency crosses a threshold. The pages\n+ * marked for migration are migrated by kmigrated kernel thread.\n+ *\n+ * Return: 0 on success and -EINVAL on failure to record the access.\n+ */\n+int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now)\n+{\n+\tstruct mem_section *ms;\n+\tstruct folio *folio;\n+\tphi_t *phi, *hot_map;\n+\tstruct page *page;\n+\n+\tif (!kmigrated_started)\n+\t\treturn -EINVAL;\n+\n+\tif (nid >= PGHOT_NID_MAX)\n+\t\treturn -EINVAL;\n+\n+\tswitch (src) {\n+\tcase PGHOT_HW_HINTS:\n+\t\tif (!static_branch_likely(&pghot_src_hwhints))\n+\t\t\treturn -EINVAL;\n+\t\tcount_vm_event(PGHOT_RECORD_HWHINTS);\n+\t\tbreak;\n+\tcase PGHOT_PGTABLE_SCAN:\n+\t\tif (!static_branch_likely(&pghot_src_pgtscans))\n+\t\t\treturn -EINVAL;\n+\t\tcount_vm_event(PGHOT_RECORD_PGTSCANS);\n+\t\tbreak;\n+\tcase PGHOT_HINT_FAULT:\n+\t\tif (!static_branch_likely(&pghot_src_hintfaults))\n+\t\t\treturn -EINVAL;\n+\t\tcount_vm_event(PGHOT_RECORD_HINTFAULTS);\n+\t\tbreak;\n+\tdefault:\n+\t\treturn -EINVAL;\n+\t}\n+\n+\t/*\n+\t * Record only accesses from lower tiers.\n+\t */\n+\tif (node_is_toptier(pfn_to_nid(pfn)))\n+\t\treturn 0;\n+\n+\t/*\n+\t * Reject the non-migratable pages right away.\n+\t */\n+\tpage = pfn_to_online_page(pfn);\n+\tif (!page || is_zone_device_page(page))\n+\t\treturn 0;\n+\n+\tfolio = page_folio(page);\n+\tif (!folio_test_lru(folio))\n+\t\treturn 0;\n+\n+\t/* Get the hotness slot corresponding to the 1st PFN of the folio */\n+\tpfn = folio_pfn(folio);\n+\tms = __pfn_to_section(pfn);\n+\tif (!ms || !ms->hot_map)\n+\t\treturn -EINVAL;\n+\n+\thot_map = (phi_t *)(((unsigned long)(ms->hot_map)) & ~PGHOT_SECTION_HOT_MASK);\n+\tphi = &hot_map[pfn % PAGES_PER_SECTION];\n+\n+\tcount_vm_event(PGHOT_RECORDED_ACCESSES);\n+\n+\t/*\n+\t * Update the hotness parameters.\n+\t */\n+\tif (pghot_update_record(phi, nid, now)) {\n+\t\tset_bit(PGHOT_SECTION_HOT_BIT, (unsigned long *)&ms->hot_map);\n+\t\tset_bit(PGDAT_KMIGRATED_ACTIVATE, &page_pgdat(page)->flags);\n+\t}\n+\treturn 0;\n+}\n+\n+static int pghot_get_hotness(unsigned long pfn, int *nid, int *freq,\n+\t\t\t     unsigned long *time)\n+{\n+\tphi_t *phi, *hot_map;\n+\tstruct mem_section *ms;\n+\n+\tms = __pfn_to_section(pfn);\n+\tif (!ms || !ms->hot_map)\n+\t\treturn -EINVAL;\n+\n+\thot_map = (phi_t *)(((unsigned long)(ms->hot_map)) & ~PGHOT_SECTION_HOT_MASK);\n+\tphi = &hot_map[pfn % PAGES_PER_SECTION];\n+\n+\treturn pghot_get_record(phi, nid, freq, time);\n+}\n+\n+/*\n+ * Walks the PFNs of the zone, isolates and migrates them in batches.\n+ */\n+static void kmigrated_walk_zone(unsigned long start_pfn, unsigned long end_pfn,\n+\t\t\t\tint src_nid)\n+{\n+\tint cur_nid = NUMA_NO_NODE;\n+\tLIST_HEAD(migrate_list);\n+\tint batch_count = 0;\n+\tstruct folio *folio;\n+\tstruct page *page;\n+\tunsigned long pfn;\n+\n+\tpfn = start_pfn;\n+\tdo {\n+\t\tint nid = NUMA_NO_NODE, nr = 1;\n+\t\tint freq = 0;\n+\t\tunsigned long time = 0;\n+\n+\t\tif (!pfn_valid(pfn))\n+\t\t\tgoto out_next;\n+\n+\t\tpage = pfn_to_online_page(pfn);\n+\t\tif (!page)\n+\t\t\tgoto out_next;\n+\n+\t\tfolio = page_folio(page);\n+\t\tnr = folio_nr_pages(folio);\n+\t\tif (folio_nid(folio) != src_nid)\n+\t\t\tgoto out_next;\n+\n+\t\tif (!folio_test_lru(folio))\n+\t\t\tgoto out_next;\n+\n+\t\tif (pghot_get_hotness(pfn, &nid, &freq, &time))\n+\t\t\tgoto out_next;\n+\n+\t\tif (nid == NUMA_NO_NODE)\n+\t\t\tnid = pghot_target_nid;\n+\n+\t\tif (folio_nid(folio) == nid)\n+\t\t\tgoto out_next;\n+\n+\t\tif (migrate_misplaced_folio_prepare(folio, NULL, nid))\n+\t\t\tgoto out_next;\n+\n+\t\tif (cur_nid == NUMA_NO_NODE)\n+\t\t\tcur_nid = nid;\n+\n+\t\t/* If NID changed, flush the previous batch first */\n+\t\tif (cur_nid != nid) {\n+\t\t\tif (!list_empty(&migrate_list))\n+\t\t\t\tmigrate_misplaced_folios_batch(&migrate_list, cur_nid);\n+\t\t\tcur_nid = nid;\n+\t\t\tbatch_count = 0;\n+\t\t\tcond_resched();\n+\t\t}\n+\n+\t\tlist_add(&folio->lru, &migrate_list);\n+\n+\t\tif (++batch_count > kmigrated_batch_nr) {\n+\t\t\tmigrate_misplaced_folios_batch(&migrate_list, cur_nid);\n+\t\t\tbatch_count = 0;\n+\t\t\tcond_resched();\n+\t\t}\n+out_next:\n+\t\tpfn += nr;\n+\t} while (pfn < end_pfn);\n+\tif (!list_empty(&migrate_list))\n+\t\tmigrate_misplaced_folios_batch(&migrate_list, cur_nid);\n+}\n+\n+static void kmigrated_do_work(pg_data_t *pgdat)\n+{\n+\tunsigned long section_nr, s_begin, start_pfn;\n+\tstruct mem_section *ms;\n+\tint nid;\n+\n+\tclear_bit(PGDAT_KMIGRATED_ACTIVATE, &pgdat->flags);\n+\t/* s_begin = first_present_section_nr(); */\n+\ts_begin = next_present_section_nr(-1);\n+\tfor_each_present_section_nr(s_begin, section_nr) {\n+\t\tstart_pfn = section_nr_to_pfn(section_nr);\n+\t\tms = __nr_to_section(section_nr);\n+\n+\t\tif (!pfn_valid(start_pfn))\n+\t\t\tcontinue;\n+\n+\t\tnid = pfn_to_nid(start_pfn);\n+\t\tif (node_is_toptier(nid) || nid != pgdat->node_id)\n+\t\t\tcontinue;\n+\n+\t\tif (!test_and_clear_bit(PGHOT_SECTION_HOT_BIT, (unsigned long *)&ms->hot_map))\n+\t\t\tcontinue;\n+\n+\t\tkmigrated_walk_zone(start_pfn, start_pfn + PAGES_PER_SECTION,\n+\t\t\t\t    pgdat->node_id);\n+\t}\n+}\n+\n+static inline bool kmigrated_work_requested(pg_data_t *pgdat)\n+{\n+\treturn test_bit(PGDAT_KMIGRATED_ACTIVATE, &pgdat->flags);\n+}\n+\n+/*\n+ * Per-node kthread that iterates over its PFNs and migrates the\n+ * pages that have been marked for migration.\n+ */\n+static int kmigrated(void *p)\n+{\n+\tlong timeout = msecs_to_jiffies(kmigrated_sleep_ms);\n+\tpg_data_t *pgdat = p;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tif (wait_event_timeout(pgdat->kmigrated_wait, kmigrated_work_requested(pgdat),\n+\t\t\t\t       timeout))\n+\t\t\tkmigrated_do_work(pgdat);\n+\t}\n+\treturn 0;\n+}\n+\n+static int kmigrated_run(int nid)\n+{\n+\tpg_data_t *pgdat = NODE_DATA(nid);\n+\tint ret;\n+\n+\tif (node_is_toptier(nid))\n+\t\treturn 0;\n+\n+\tif (!pgdat->kmigrated) {\n+\t\tpgdat->kmigrated = kthread_create_on_node(kmigrated, pgdat, nid,\n+\t\t\t\t\t\t\t  \"kmigrated%d\", nid);\n+\t\tif (IS_ERR(pgdat->kmigrated)) {\n+\t\t\tret = PTR_ERR(pgdat->kmigrated);\n+\t\t\tpgdat->kmigrated = NULL;\n+\t\t\tpr_err(\"Failed to start kmigrated%d, ret %d\\n\", nid, ret);\n+\t\t\treturn ret;\n+\t\t}\n+\t\tpr_info(\"pghot: Started kmigrated thread for node %d\\n\", nid);\n+\t}\n+\twake_up_process(pgdat->kmigrated);\n+\treturn 0;\n+}\n+\n+static void pghot_free_hot_map(void)\n+{\n+\tunsigned long section_nr, s_begin;\n+\tstruct mem_section *ms;\n+\n+\t/* s_begin = first_present_section_nr(); */\n+\ts_begin = next_present_section_nr(-1);\n+\tfor_each_present_section_nr(s_begin, section_nr) {\n+\t\tms = __nr_to_section(section_nr);\n+\t\tkfree(ms->hot_map);\n+\t}\n+}\n+\n+static int pghot_alloc_hot_map(void)\n+{\n+\tunsigned long section_nr, s_begin, start_pfn;\n+\tstruct mem_section *ms;\n+\tint nid;\n+\n+\t/* s_begin = first_present_section_nr(); */\n+\ts_begin = next_present_section_nr(-1);\n+\tfor_each_present_section_nr(s_begin, section_nr) {\n+\t\tms = __nr_to_section(section_nr);\n+\t\tstart_pfn = section_nr_to_pfn(section_nr);\n+\t\tnid = pfn_to_nid(start_pfn);\n+\n+\t\tif (node_is_toptier(nid) || !pfn_valid(start_pfn))\n+\t\t\tcontinue;\n+\n+\t\tms->hot_map = kcalloc_node(PAGES_PER_SECTION, PGHOT_RECORD_SIZE, GFP_KERNEL,\n+\t\t\t\t\t   nid);\n+\t\tif (!ms->hot_map)\n+\t\t\tgoto out_free_hot_map;\n+\t}\n+\treturn 0;\n+\n+out_free_hot_map:\n+\tpghot_free_hot_map();\n+\treturn -ENOMEM;\n+}\n+\n+static int __init pghot_init(void)\n+{\n+\tpg_data_t *pgdat;\n+\tint nid, ret;\n+\n+\tret = pghot_alloc_hot_map();\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tret = kmigrated_run(nid);\n+\t\tif (ret)\n+\t\t\tgoto out_stop_kthread;\n+\t}\n+\tregister_sysctl_init(\"vm\", pghot_sysctls);\n+\tpghot_debug_init();\n+\n+\tkmigrated_started = true;\n+\treturn 0;\n+\n+out_stop_kthread:\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tpgdat = NODE_DATA(nid);\n+\t\tif (pgdat->kmigrated) {\n+\t\t\tkthread_stop(pgdat->kmigrated);\n+\t\t\tpgdat->kmigrated = NULL;\n+\t\t}\n+\t}\n+\tpghot_free_hot_map();\n+\treturn ret;\n+}\n+\n+late_initcall_sync(pghot_init)\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 65de88cdf40e..f6f91b9dd887 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -1501,6 +1501,12 @@ const char * const vmstat_text[] = {\n \t[I(KSTACK_REST)]\t\t\t= \"kstack_rest\",\n #endif\n #endif\n+#ifdef CONFIG_PGHOT\n+\t[I(PGHOT_RECORDED_ACCESSES)]\t\t= \"pghot_recorded_accesses\",\n+\t[I(PGHOT_RECORD_HWHINTS)]\t\t= \"pghot_recorded_hwhints\",\n+\t[I(PGHOT_RECORD_PGTSCANS)]\t\t= \"pghot_recorded_pgtscans\",\n+\t[I(PGHOT_RECORD_HINTFAULTS)]\t\t= \"pghot_recorded_hintfaults\",\n+#endif /* CONFIG_PGHOT */\n #undef I\n #endif /* CONFIG_VM_EVENT_COUNTERS */\n };\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author acknowledged that the default mode of pghot has limitations in tracking access time and toptier NID, but explained that precision mode addresses these issues by storing hotness information in 4 bytes per PFN, allowing for more fine-grained access time tracking and explicit toptier target NID tracking. The author noted that this precise mode is typically useful when the toptier consists of multiple nodes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged limitations",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By default, one byte per PFN is used to store hotness information.\nLimited number of bits are used to store the access time leading\nto coarse-grained time tracking. Also there aren't enough bits\nto track the toptier NID explicitly and hence the default target_nid\nis used for promotion.\n\nThis precise mode relaxes the above situation by storing the\nhotness information in 4 bytes per PFN. More fine-grained\naccess time tracking and toptier NID tracking becomes possible\nin this mode.\n\nTypically useful when toptier consists of more than one node.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n Documentation/admin-guide/mm/pghot.txt |  4 +-\n include/linux/mmzone.h                 |  2 +-\n include/linux/pghot.h                  | 31 ++++++++++++\n mm/Kconfig                             | 11 ++++\n mm/Makefile                            |  7 ++-\n mm/pghot-precise.c                     | 70 ++++++++++++++++++++++++++\n mm/pghot.c                             | 13 +++--\n 7 files changed, 130 insertions(+), 8 deletions(-)\n create mode 100644 mm/pghot-precise.c\n\ndiff --git a/Documentation/admin-guide/mm/pghot.txt b/Documentation/admin-guide/mm/pghot.txt\nindex 01291b72e7ab..b329e692ef89 100644\n--- a/Documentation/admin-guide/mm/pghot.txt\n+++ b/Documentation/admin-guide/mm/pghot.txt\n@@ -38,7 +38,7 @@ Path: /sys/kernel/debug/pghot/\n \n 3. **freq_threshold**\n    - Minimum access frequency before a page is marked ready for promotion.\n-   - Range: 1 to 3\n+   - Range: 1 to 3 in default mode, 1 to 7 in precision mode.\n    - Default: 2\n    - Example:\n      # echo 3 > /sys/kernel/debug/pghot/freq_threshold\n@@ -60,7 +60,7 @@ Path: /proc/sys/vm/pghot_promote_freq_window_ms\n - Controls the time window (in ms) for counting access frequency. A page is\n   considered hot only when **freq_threshold** number of accesses occur with\n   this time period.\n-- Default: 4000 (4 seconds)\n+- Default: 4000 (4 seconds) in default mode and 5000 (5s) in precision mode.\n - Example:\n   # sysctl vm.pghot_promote_freq_window_ms=3000\n \ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 22e08befb096..49c374064fc2 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1924,7 +1924,7 @@ struct mem_section {\n #ifdef CONFIG_PGHOT\n \t/*\n \t * Per-PFN hotness data for this section.\n-\t * Array of phi_t (u8 in default mode).\n+\t * Array of phi_t (u8 in default mode, u32 in precision mode).\n \t * LSB is used as PGHOT_SECTION_HOT_BIT flag.\n \t */\n \tvoid *hot_map;\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nindex 88e57aab697b..d3d59b0c0cf6 100644\n--- a/include/linux/pghot.h\n+++ b/include/linux/pghot.h\n@@ -48,6 +48,36 @@ enum pghot_src_enabled {\n \n #define PGHOT_DEFAULT_NODE\t\t0\n \n+#if defined(CONFIG_PGHOT_PRECISE)\n+#define PGHOT_DEFAULT_FREQ_WINDOW\t(5 * MSEC_PER_SEC)\n+\n+/*\n+ * Bits 0-26 are used to store nid, frequency and time.\n+ * Bits 27-30 are unused now.\n+ * Bit 31 is used to indicate the page is ready for migration.\n+ */\n+#define PGHOT_MIGRATE_READY\t\t31\n+\n+#define PGHOT_NID_WIDTH\t\t\t10\n+#define PGHOT_FREQ_WIDTH\t\t3\n+/* time is stored in 14 bits which can represent up to 16s with HZ=1000 */\n+#define PGHOT_TIME_WIDTH\t\t14\n+\n+#define PGHOT_NID_SHIFT\t\t\t0\n+#define PGHOT_FREQ_SHIFT\t\t(PGHOT_NID_SHIFT + PGHOT_NID_WIDTH)\n+#define PGHOT_TIME_SHIFT\t\t(PGHOT_FREQ_SHIFT + PGHOT_FREQ_WIDTH)\n+\n+#define PGHOT_NID_MASK\t\t\tGENMASK(PGHOT_NID_WIDTH - 1, 0)\n+#define PGHOT_FREQ_MASK\t\t\tGENMASK(PGHOT_FREQ_WIDTH - 1, 0)\n+#define PGHOT_TIME_MASK\t\t\tGENMASK(PGHOT_TIME_WIDTH - 1, 0)\n+\n+#define PGHOT_NID_MAX\t\t\t((1 << PGHOT_NID_WIDTH) - 1)\n+#define PGHOT_FREQ_MAX\t\t\t((1 << PGHOT_FREQ_WIDTH) - 1)\n+#define PGHOT_TIME_MAX\t\t\t((1 << PGHOT_TIME_WIDTH) - 1)\n+\n+typedef u32 phi_t;\n+\n+#else\t/* !CONFIG_PGHOT_PRECISE */\n #define PGHOT_DEFAULT_FREQ_WINDOW\t(4 * MSEC_PER_SEC)\n \n /*\n@@ -74,6 +104,7 @@ enum pghot_src_enabled {\n #define PGHOT_TIME_MAX\t\t\t((1 << PGHOT_TIME_WIDTH) - 1)\n \n typedef u8 phi_t;\n+#endif /* CONFIG_PGHOT_PRECISE */\n \n #define PGHOT_RECORD_SIZE\t\tsizeof(phi_t)\n \ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex f4f0147faac5..fde5aee3e16f 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -1478,6 +1478,17 @@ config PGHOT\n \t  This adds 1 byte of metadata overhead per page in lower-tier\n \t  memory nodes.\n \n+config PGHOT_PRECISE\n+\tbool \"Hot page tracking precision mode\"\n+\tdef_bool n\n+\tdepends on PGHOT\n+\thelp\n+\t  Enables precision mode for tracking hot pages with pghot sub-system.\n+\t  Adds fine-grained access time tracking and explicit toptier target\n+\t  NID tracking. Precise hot page tracking comes at the cost of using\n+\t  4 bytes per page against the default one byte per page. Preferable\n+\t  to enable this on systems with multiple nodes in toptier.\n+\n source \"mm/damon/Kconfig\"\n \n endmenu\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 655a27f3a215..89f999647752 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -147,4 +147,9 @@ obj-$(CONFIG_SHRINKER_DEBUG) += shrinker_debug.o\n obj-$(CONFIG_EXECMEM) += execmem.o\n obj-$(CONFIG_TMPFS_QUOTA) += shmem_quota.o\n obj-$(CONFIG_PT_RECLAIM) += pt_reclaim.o\n-obj-$(CONFIG_PGHOT) += pghot.o pghot-tunables.o pghot-default.o\n+obj-$(CONFIG_PGHOT) += pghot.o pghot-tunables.o\n+ifdef CONFIG_PGHOT_PRECISE\n+obj-$(CONFIG_PGHOT) += pghot-precise.o\n+else\n+obj-$(CONFIG_PGHOT) += pghot-default.o\n+endif\ndiff --git a/mm/pghot-precise.c b/mm/pghot-precise.c\nnew file mode 100644\nindex 000000000000..d8d4f15b3f9f\n--- /dev/null\n+++ b/mm/pghot-precise.c\n@@ -0,0 +1,70 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * pghot: Precision mode\n+ *\n+ * 4 byte hotness record per PFN (u32)\n+ * NID, time and frequency tracked as part of the record.\n+ */\n+\n+#include <linux/pghot.h>\n+#include <linux/jiffies.h>\n+\n+unsigned long pghot_access_latency(unsigned long old_time, unsigned long time)\n+{\n+\treturn jiffies_to_msecs((time - old_time) & PGHOT_TIME_MASK);\n+}\n+\n+bool pghot_update_record(phi_t *phi, int nid, unsigned long now)\n+{\n+\tphi_t freq, old_freq, hotness, old_hotness, old_time, old_nid;\n+\tphi_t time = now & PGHOT_TIME_MASK;\n+\n+\told_hotness = READ_ONCE(*phi);\n+\tdo {\n+\t\tbool new_window = false;\n+\n+\t\thotness = old_hotness;\n+\t\told_nid = (hotness >> PGHOT_NID_SHIFT) & PGHOT_NID_MASK;\n+\t\told_freq = (hotness >> PGHOT_FREQ_SHIFT) & PGHOT_FREQ_MASK;\n+\t\told_time = (hotness >> PGHOT_TIME_SHIFT) & PGHOT_TIME_MASK;\n+\n+\t\tif (pghot_access_latency(old_time, time) > sysctl_pghot_freq_window)\n+\t\t\tnew_window = true;\n+\n+\t\tif (new_window)\n+\t\t\tfreq = 1;\n+\t\telse if (old_freq < PGHOT_FREQ_MAX)\n+\t\t\tfreq = old_freq + 1;\n+\t\telse\n+\t\t\tfreq = old_freq;\n+\t\tnid = (nid == NUMA_NO_NODE) ? pghot_target_nid : nid;\n+\n+\t\thotness &= ~(PGHOT_NID_MASK << PGHOT_NID_SHIFT);\n+\t\thotness &= ~(PGHOT_FREQ_MASK << PGHOT_FREQ_SHIFT);\n+\t\thotness &= ~(PGHOT_TIME_MASK << PGHOT_TIME_SHIFT);\n+\n+\t\thotness |= (nid & PGHOT_NID_MASK) << PGHOT_NID_SHIFT;\n+\t\thotness |= (freq & PGHOT_FREQ_MASK) << PGHOT_FREQ_SHIFT;\n+\t\thotness |= (time & PGHOT_TIME_MASK) << PGHOT_TIME_SHIFT;\n+\n+\t\tif (freq >= pghot_freq_threshold)\n+\t\t\thotness |= BIT(PGHOT_MIGRATE_READY);\n+\t} while (unlikely(!try_cmpxchg(phi, &old_hotness, hotness)));\n+\treturn !!(hotness & BIT(PGHOT_MIGRATE_READY));\n+}\n+\n+int pghot_get_record(phi_t *phi, int *nid, int *freq, unsigned long *time)\n+{\n+\tphi_t old_hotness, hotness = 0;\n+\n+\told_hotness = READ_ONCE(*phi);\n+\tdo {\n+\t\tif (!(old_hotness & BIT(PGHOT_MIGRATE_READY)))\n+\t\t\treturn -EINVAL;\n+\t} while (unlikely(!try_cmpxchg(phi, &old_hotness, hotness)));\n+\n+\t*nid = (old_hotness >> PGHOT_NID_SHIFT) & PGHOT_NID_MASK;\n+\t*freq = (old_hotness >> PGHOT_FREQ_SHIFT) & PGHOT_FREQ_MASK;\n+\t*time = (old_hotness >> PGHOT_TIME_SHIFT) & PGHOT_TIME_MASK;\n+\treturn 0;\n+}\ndiff --git a/mm/pghot.c b/mm/pghot.c\nindex 95b5012d5b99..bf1d9029cbaa 100644\n--- a/mm/pghot.c\n+++ b/mm/pghot.c\n@@ -10,6 +10,9 @@\n  * the frequency of access and last access time. Promotions are done\n  * to a default toptier NID.\n  *\n+ * In the precision mode, 4 bytes are used to store the frequency\n+ * of access, last access time and the accessing NID.\n+ *\n  * A kernel thread named kmigrated is provided to migrate or promote\n  * the hot pages. kmigrated runs for each lower tier node. It iterates\n  * over the node's PFNs and  migrates pages marked for migration into\n@@ -52,13 +55,15 @@ static bool kmigrated_started __ro_after_init;\n  * for the purpose of tracking page hotness and subsequent promotion.\n  *\n  * @pfn: PFN of the page\n- * @nid: Unused\n+ * @nid: Target NID to where the page needs to be migrated in precision\n+ *       mode but unused in default mode\n  * @src: The identifier of the sub-system that reports the access\n  * @now: Access time in jiffies\n  *\n- * Updates the frequency and time of access and marks the page as\n- * ready for migration if the frequency crosses a threshold. The pages\n- * marked for migration are migrated by kmigrated kernel thread.\n+ * Updates the NID (in precision mode only), frequency and time of access\n+ * and marks the page as ready for migration if the frequency crosses a\n+ * threshold. The pages marked for migration are migrated by kmigrated\n+ * kernel thread.\n  *\n  * Return: 0 on success and -EINVAL on failure to record the access.\n  */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author acknowledged that the NUMA Balancing mode of NUMA Balancing currently handles hot page detection, classification, and promotion independently within the scheduler. They agreed to offload this functionality to the common hot page tracking system provided by pghot, which will handle hot page detection via hint faults and offload the rest of the functionality to kmigrated.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "agreed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Currently hot page promotion (NUMA_BALANCING_MEMORY_TIERING\nmode of NUMA Balancing) does hot page detection (via hint faults),\nhot page classification and eventual promotion, all by itself and\nsits within the scheduler.\n\nWith pghot, the new hot page tracking and promotion mechanism\nbeing available, NUMA Balancing can limit itself to detection\nof hot pages (via hint faults) and off-load rest of the\nfunctionality to the common hot page tracking system.\n\npghot_record_access(PGHOT_HINT_FAULT) API is used to feed the\nhot page info to pghot. In addition, the migration rate limiting\nand dynamic threshold logic are moved to kmigrated so that the\nsame can be used for hot pages reported by other sources too.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n kernel/sched/debug.c |   1 -\n kernel/sched/fair.c  | 152 ++-----------------------------------------\n mm/huge_memory.c     |  26 ++------\n mm/memory.c          |  31 ++-------\n mm/pghot.c           | 124 +++++++++++++++++++++++++++++++++++\n 5 files changed, 141 insertions(+), 193 deletions(-)\n\ndiff --git a/kernel/sched/debug.c b/kernel/sched/debug.c\nindex 41caa22e0680..02931902a9c6 100644\n--- a/kernel/sched/debug.c\n+++ b/kernel/sched/debug.c\n@@ -520,7 +520,6 @@ static __init int sched_init_debug(void)\n \tdebugfs_create_u32(\"scan_period_min_ms\", 0644, numa, &sysctl_numa_balancing_scan_period_min);\n \tdebugfs_create_u32(\"scan_period_max_ms\", 0644, numa, &sysctl_numa_balancing_scan_period_max);\n \tdebugfs_create_u32(\"scan_size_mb\", 0644, numa, &sysctl_numa_balancing_scan_size);\n-\tdebugfs_create_u32(\"hot_threshold_ms\", 0644, numa, &sysctl_numa_balancing_hot_threshold);\n #endif /* CONFIG_NUMA_BALANCING */\n \n \tdebugfs_create_file(\"debug\", 0444, debugfs_sched, NULL, &sched_debug_fops);\ndiff --git a/kernel/sched/fair.c b/kernel/sched/fair.c\nindex da46c3164537..4e70f58fbbfa 100644\n--- a/kernel/sched/fair.c\n+++ b/kernel/sched/fair.c\n@@ -125,11 +125,6 @@ int __weak arch_asym_cpu_priority(int cpu)\n static unsigned int sysctl_sched_cfs_bandwidth_slice\t\t= 5000UL;\n #endif\n \n-#ifdef CONFIG_NUMA_BALANCING\n-/* Restrict the NUMA promotion throughput (MB/s) for each target node. */\n-static unsigned int sysctl_numa_balancing_promote_rate_limit = 65536;\n-#endif\n-\n #ifdef CONFIG_SYSCTL\n static const struct ctl_table sched_fair_sysctls[] = {\n #ifdef CONFIG_CFS_BANDWIDTH\n@@ -142,16 +137,6 @@ static const struct ctl_table sched_fair_sysctls[] = {\n \t\t.extra1         = SYSCTL_ONE,\n \t},\n #endif\n-#ifdef CONFIG_NUMA_BALANCING\n-\t{\n-\t\t.procname\t= \"numa_balancing_promote_rate_limit_MBps\",\n-\t\t.data\t\t= &sysctl_numa_balancing_promote_rate_limit,\n-\t\t.maxlen\t\t= sizeof(unsigned int),\n-\t\t.mode\t\t= 0644,\n-\t\t.proc_handler\t= proc_dointvec_minmax,\n-\t\t.extra1\t\t= SYSCTL_ZERO,\n-\t},\n-#endif /* CONFIG_NUMA_BALANCING */\n };\n \n static int __init sched_fair_sysctl_init(void)\n@@ -1427,9 +1412,6 @@ unsigned int sysctl_numa_balancing_scan_size = 256;\n /* Scan @scan_size MB every @scan_period after an initial @scan_delay in ms */\n unsigned int sysctl_numa_balancing_scan_delay = 1000;\n \n-/* The page with hint page fault latency < threshold in ms is considered hot */\n-unsigned int sysctl_numa_balancing_hot_threshold = MSEC_PER_SEC;\n-\n struct numa_group {\n \trefcount_t refcount;\n \n@@ -1784,108 +1766,6 @@ static inline bool cpupid_valid(int cpupid)\n \treturn cpupid_to_cpu(cpupid) < nr_cpu_ids;\n }\n \n-/*\n- * For memory tiering mode, if there are enough free pages (more than\n- * enough watermark defined here) in fast memory node, to take full\n- * advantage of fast memory capacity, all recently accessed slow\n- * memory pages will be migrated to fast memory node without\n- * considering hot threshold.\n- */\n-static bool pgdat_free_space_enough(struct pglist_data *pgdat)\n-{\n-\tint z;\n-\tunsigned long enough_wmark;\n-\n-\tenough_wmark = max(1UL * 1024 * 1024 * 1024 >> PAGE_SHIFT,\n-\t\t\t   pgdat->node_present_pages >> 4);\n-\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n-\t\tstruct zone *zone = pgdat->node_zones + z;\n-\n-\t\tif (!populated_zone(zone))\n-\t\t\tcontinue;\n-\n-\t\tif (zone_watermark_ok(zone, 0,\n-\t\t\t\t      promo_wmark_pages(zone) + enough_wmark,\n-\t\t\t\t      ZONE_MOVABLE, 0))\n-\t\t\treturn true;\n-\t}\n-\treturn false;\n-}\n-\n-/*\n- * For memory tiering mode, when page tables are scanned, the scan\n- * time will be recorded in struct page in addition to make page\n- * PROT_NONE for slow memory page.  So when the page is accessed, in\n- * hint page fault handler, the hint page fault latency is calculated\n- * via,\n- *\n- *\thint page fault latency = hint page fault time - scan time\n- *\n- * The smaller the hint page fault latency, the higher the possibility\n- * for the page to be hot.\n- */\n-static int numa_hint_fault_latency(struct folio *folio)\n-{\n-\tint last_time, time;\n-\n-\ttime = jiffies_to_msecs(jiffies);\n-\tlast_time = folio_xchg_access_time(folio, time);\n-\n-\treturn (time - last_time) & PAGE_ACCESS_TIME_MASK;\n-}\n-\n-/*\n- * For memory tiering mode, too high promotion/demotion throughput may\n- * hurt application latency.  So we provide a mechanism to rate limit\n- * the number of pages that are tried to be promoted.\n- */\n-static bool numa_promotion_rate_limit(struct pglist_data *pgdat,\n-\t\t\t\t      unsigned long rate_limit, int nr)\n-{\n-\tunsigned long nr_cand;\n-\tunsigned int now, start;\n-\n-\tnow = jiffies_to_msecs(jiffies);\n-\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE, nr);\n-\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n-\tstart = pgdat->nbp_rl_start;\n-\tif (now - start > MSEC_PER_SEC &&\n-\t    cmpxchg(&pgdat->nbp_rl_start, start, now) == start)\n-\t\tpgdat->nbp_rl_nr_cand = nr_cand;\n-\tif (nr_cand - pgdat->nbp_rl_nr_cand >= rate_limit)\n-\t\treturn true;\n-\treturn false;\n-}\n-\n-#define NUMA_MIGRATION_ADJUST_STEPS\t16\n-\n-static void numa_promotion_adjust_threshold(struct pglist_data *pgdat,\n-\t\t\t\t\t    unsigned long rate_limit,\n-\t\t\t\t\t    unsigned int ref_th)\n-{\n-\tunsigned int now, start, th_period, unit_th, th;\n-\tunsigned long nr_cand, ref_cand, diff_cand;\n-\n-\tnow = jiffies_to_msecs(jiffies);\n-\tth_period = sysctl_numa_balancing_scan_period_max;\n-\tstart = pgdat->nbp_th_start;\n-\tif (now - start > th_period &&\n-\t    cmpxchg(&pgdat->nbp_th_start, start, now) == start) {\n-\t\tref_cand = rate_limit *\n-\t\t\tsysctl_numa_balancing_scan_period_max / MSEC_PER_SEC;\n-\t\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n-\t\tdiff_cand = nr_cand - pgdat->nbp_th_nr_cand;\n-\t\tunit_th = ref_th * 2 / NUMA_MIGRATION_ADJUST_STEPS;\n-\t\tth = pgdat->nbp_threshold ? : ref_th;\n-\t\tif (diff_cand > ref_cand * 11 / 10)\n-\t\t\tth = max(th - unit_th, unit_th);\n-\t\telse if (diff_cand < ref_cand * 9 / 10)\n-\t\t\tth = min(th + unit_th, ref_th * 2);\n-\t\tpgdat->nbp_th_nr_cand = nr_cand;\n-\t\tpgdat->nbp_threshold = th;\n-\t}\n-}\n-\n bool should_numa_migrate_memory(struct task_struct *p, struct folio *folio,\n \t\t\t\tint src_nid, int dst_cpu)\n {\n@@ -1901,33 +1781,11 @@ bool should_numa_migrate_memory(struct task_struct *p, struct folio *folio,\n \n \t/*\n \t * The pages in slow memory node should be migrated according\n-\t * to hot/cold instead of private/shared.\n-\t */\n-\tif (folio_use_access_time(folio)) {\n-\t\tstruct pglist_data *pgdat;\n-\t\tunsigned long rate_limit;\n-\t\tunsigned int latency, th, def_th;\n-\t\tlong nr = folio_nr_pages(folio);\n-\n-\t\tpgdat = NODE_DATA(dst_nid);\n-\t\tif (pgdat_free_space_enough(pgdat)) {\n-\t\t\t/* workload changed, reset hot threshold */\n-\t\t\tpgdat->nbp_threshold = 0;\n-\t\t\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE_NRL, nr);\n-\t\t\treturn true;\n-\t\t}\n-\n-\t\tdef_th = sysctl_numa_balancing_hot_threshold;\n-\t\trate_limit = MB_TO_PAGES(sysctl_numa_balancing_promote_rate_limit);\n-\t\tnuma_promotion_adjust_threshold(pgdat, rate_limit, def_th);\n-\n-\t\tth = pgdat->nbp_threshold ? : def_th;\n-\t\tlatency = numa_hint_fault_latency(folio);\n-\t\tif (latency >= th)\n-\t\t\treturn false;\n-\n-\t\treturn !numa_promotion_rate_limit(pgdat, rate_limit, nr);\n-\t}\n+\t * to hot/cold instead of private/shared. Also the migration\n+\t * of such pages are handled by kmigrated.\n+\t */\n+\tif (folio_use_access_time(folio))\n+\t\treturn true;\n \n \tthis_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);\n \tlast_cpupid = folio_xchg_last_cpupid(folio, this_cpupid);\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 40cf59301c21..f52587e70b3c 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -40,6 +40,7 @@\n #include <linux/pgalloc.h>\n #include <linux/pgalloc_tag.h>\n #include <linux/pagewalk.h>\n+#include <linux/pghot.h>\n \n #include <asm/tlb.h>\n #include \"internal.h\"\n@@ -2217,29 +2218,12 @@ vm_fault_t do_huge_pmd_numa_page(struct vm_fault *vmf)\n \n \ttarget_nid = numa_migrate_check(folio, vmf, haddr, &flags, writable,\n \t\t\t\t\t&last_cpupid);\n+\tnid = target_nid;\n \tif (target_nid == NUMA_NO_NODE)\n \t\tgoto out_map;\n-\tif (migrate_misplaced_folio_prepare(folio, vma, target_nid)) {\n-\t\tflags |= TNF_MIGRATE_FAIL;\n-\t\tgoto out_map;\n-\t}\n-\t/* The folio is isolated and isolation code holds a folio reference. */\n-\tspin_unlock(vmf->ptl);\n-\twritable = false;\n \n-\tif (!migrate_misplaced_folio(folio, target_nid)) {\n-\t\tflags |= TNF_MIGRATED;\n-\t\tnid = target_nid;\n-\t\ttask_numa_fault(last_cpupid, nid, HPAGE_PMD_NR, flags);\n-\t\treturn 0;\n-\t}\n+\twritable = false;\n \n-\tflags |= TNF_MIGRATE_FAIL;\n-\tvmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);\n-\tif (unlikely(!pmd_same(pmdp_get(vmf->pmd), vmf->orig_pmd))) {\n-\t\tspin_unlock(vmf->ptl);\n-\t\treturn 0;\n-\t}\n out_map:\n \t/* Restore the PMD */\n \tpmd = pmd_modify(pmdp_get(vmf->pmd), vma->vm_page_prot);\n@@ -2250,8 +2234,10 @@ vm_fault_t do_huge_pmd_numa_page(struct vm_fault *vmf)\n \tupdate_mmu_cache_pmd(vma, vmf->address, vmf->pmd);\n \tspin_unlock(vmf->ptl);\n \n-\tif (nid != NUMA_NO_NODE)\n+\tif (nid != NUMA_NO_NODE) {\n+\t\tpghot_record_access(folio_pfn(folio), nid, PGHOT_HINT_FAULT, jiffies);\n \t\ttask_numa_fault(last_cpupid, nid, HPAGE_PMD_NR, flags);\n+\t}\n \treturn 0;\n }\n \ndiff --git a/mm/memory.c b/mm/memory.c\nindex 2a55edc48a65..98a9a3b675a0 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -75,6 +75,7 @@\n #include <linux/perf_event.h>\n #include <linux/ptrace.h>\n #include <linux/vmalloc.h>\n+#include <linux/pghot.h>\n #include <linux/sched/sysctl.h>\n #include <linux/pgalloc.h>\n #include <linux/uaccess.h>\n@@ -6046,34 +6047,12 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \n \ttarget_nid = numa_migrate_check(folio, vmf, vmf->address, &flags,\n \t\t\t\t\twritable, &last_cpupid);\n+\tnid = target_nid;\n \tif (target_nid == NUMA_NO_NODE)\n \t\tgoto out_map;\n-\tif (migrate_misplaced_folio_prepare(folio, vma, target_nid)) {\n-\t\tflags |= TNF_MIGRATE_FAIL;\n-\t\tgoto out_map;\n-\t}\n-\t/* The folio is isolated and isolation code holds a folio reference. */\n-\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n+\n \twritable = false;\n \tignore_writable = true;\n-\n-\t/* Migrate to the requested node */\n-\tif (!migrate_misplaced_folio(folio, target_nid)) {\n-\t\tnid = target_nid;\n-\t\tflags |= TNF_MIGRATED;\n-\t\ttask_numa_fault(last_cpupid, nid, nr_pages, flags);\n-\t\treturn 0;\n-\t}\n-\n-\tflags |= TNF_MIGRATE_FAIL;\n-\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n-\t\t\t\t       vmf->address, &vmf->ptl);\n-\tif (unlikely(!vmf->pte))\n-\t\treturn 0;\n-\tif (unlikely(!pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {\n-\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n-\t\treturn 0;\n-\t}\n out_map:\n \t/*\n \t * Make it present again, depending on how arch implements\n@@ -6087,8 +6066,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t\t\t\t\t    writable);\n \tpte_unmap_unlock(vmf->pte, vmf->ptl);\n \n-\tif (nid != NUMA_NO_NODE)\n+\tif (nid != NUMA_NO_NODE) {\n+\t\tpghot_record_access(folio_pfn(folio), nid, PGHOT_HINT_FAULT, jiffies);\n \t\ttask_numa_fault(last_cpupid, nid, nr_pages, flags);\n+\t}\n \treturn 0;\n }\n \ndiff --git a/mm/pghot.c b/mm/pghot.c\nindex bf1d9029cbaa..6fc76c1eaff8 100644\n--- a/mm/pghot.c\n+++ b/mm/pghot.c\n@@ -17,6 +17,9 @@\n  * the hot pages. kmigrated runs for each lower tier node. It iterates\n  * over the node's PFNs and  migrates pages marked for migration into\n  * their targeted nodes.\n+ *\n+ * Migration rate-limiting and dynamic threshold logic implementations\n+ * were moved from NUMA Balancing mode 2.\n  */\n #include <linux/mm.h>\n #include <linux/migrate.h>\n@@ -31,6 +34,12 @@ unsigned int kmigrated_batch_nr = KMIGRATED_DEFAULT_BATCH_NR;\n \n unsigned int sysctl_pghot_freq_window = PGHOT_DEFAULT_FREQ_WINDOW;\n \n+/* Restrict the NUMA promotion throughput (MB/s) for each target node. */\n+static unsigned int sysctl_pghot_promote_rate_limit = 65536;\n+\n+#define KMIGRATED_MIGRATION_ADJUST_STEPS\t16\n+#define KMIGRATED_PROMOTION_THRESHOLD_WINDOW\t60000\n+\n DEFINE_STATIC_KEY_FALSE(pghot_src_hwhints);\n DEFINE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n DEFINE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n@@ -45,6 +54,14 @@ static const struct ctl_table pghot_sysctls[] = {\n \t\t.proc_handler   = proc_dointvec_minmax,\n \t\t.extra1         = SYSCTL_ZERO,\n \t},\n+\t{\n+\t\t.procname\t= \"pghot_promote_rate_limit_MBps\",\n+\t\t.data\t\t= &sysctl_pghot_promote_rate_limit,\n+\t\t.maxlen\t\t= sizeof(unsigned int),\n+\t\t.mode\t\t= 0644,\n+\t\t.proc_handler\t= proc_dointvec_minmax,\n+\t\t.extra1\t\t= SYSCTL_ZERO,\n+\t},\n };\n #endif\n \n@@ -138,6 +155,110 @@ int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now)\n \treturn 0;\n }\n \n+/*\n+ * For memory tiering mode, if there are enough free pages (more than\n+ * enough watermark defined here) in fast memory node, to take full\n+ * advantage of fast memory capacity, all recently accessed slow\n+ * memory pages will be migrated to fast memory node without\n+ * considering hot threshold.\n+ */\n+static bool pgdat_free_space_enough(struct pglist_data *pgdat)\n+{\n+\tint z;\n+\tunsigned long enough_wmark;\n+\n+\tenough_wmark = max(1UL * 1024 * 1024 * 1024 >> PAGE_SHIFT,\n+\t\t\t   pgdat->node_present_pages >> 4);\n+\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n+\t\tstruct zone *zone = pgdat->node_zones + z;\n+\n+\t\tif (!populated_zone(zone))\n+\t\t\tcontinue;\n+\n+\t\tif (zone_watermark_ok(zone, 0,\n+\t\t\t\t      promo_wmark_pages(zone) + enough_wmark,\n+\t\t\t\t      ZONE_MOVABLE, 0))\n+\t\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n+/*\n+ * For memory tiering mode, too high promotion/demotion throughput may\n+ * hurt application latency.  So we provide a mechanism to rate limit\n+ * the number of pages that are tried to be promoted.\n+ */\n+static bool kmigrated_promotion_rate_limit(struct pglist_data *pgdat, unsigned long rate_limit,\n+\t\t\t\t\t   int nr, unsigned long now_ms)\n+{\n+\tunsigned long nr_cand;\n+\tunsigned int start;\n+\n+\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE, nr);\n+\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n+\tstart = pgdat->nbp_rl_start;\n+\tif (now_ms - start > MSEC_PER_SEC &&\n+\t    cmpxchg(&pgdat->nbp_rl_start, start, now_ms) == start)\n+\t\tpgdat->nbp_rl_nr_cand = nr_cand;\n+\tif (nr_cand - pgdat->nbp_rl_nr_cand >= rate_limit)\n+\t\treturn true;\n+\treturn false;\n+}\n+\n+static void kmigrated_promotion_adjust_threshold(struct pglist_data *pgdat,\n+\t\t\t\t\t\t unsigned long rate_limit, unsigned int ref_th,\n+\t\t\t\t\t\t unsigned long now_ms)\n+{\n+\tunsigned int start, th_period, unit_th, th;\n+\tunsigned long nr_cand, ref_cand, diff_cand;\n+\n+\tth_period = KMIGRATED_PROMOTION_THRESHOLD_WINDOW;\n+\tstart = pgdat->nbp_th_start;\n+\tif (now_ms - start > th_period &&\n+\t    cmpxchg(&pgdat->nbp_th_start, start, now_ms) == start) {\n+\t\tref_cand = rate_limit *\n+\t\t\tKMIGRATED_PROMOTION_THRESHOLD_WINDOW / MSEC_PER_SEC;\n+\t\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n+\t\tdiff_cand = nr_cand - pgdat->nbp_th_nr_cand;\n+\t\tunit_th = ref_th * 2 / KMIGRATED_MIGRATION_ADJUST_STEPS;\n+\t\tth = pgdat->nbp_threshold ? : ref_th;\n+\t\tif (diff_cand > ref_cand * 11 / 10)\n+\t\t\tth = max(th - unit_th, unit_th);\n+\t\telse if (diff_cand < ref_cand * 9 / 10)\n+\t\t\tth = min(th + unit_th, ref_th * 2);\n+\t\tpgdat->nbp_th_nr_cand = nr_cand;\n+\t\tpgdat->nbp_threshold = th;\n+\t}\n+}\n+\n+static bool kmigrated_should_migrate_memory(unsigned long nr_pages, int nid,\n+\t\t\t\t\t    unsigned long time)\n+{\n+\tstruct pglist_data *pgdat;\n+\tunsigned long rate_limit;\n+\tunsigned int th, def_th;\n+\tunsigned long now_ms = jiffies_to_msecs(jiffies); /* Based on full-width jiffies */\n+\tunsigned long now = jiffies;\n+\n+\tpgdat = NODE_DATA(nid);\n+\tif (pgdat_free_space_enough(pgdat)) {\n+\t\t/* workload changed, reset hot threshold */\n+\t\tpgdat->nbp_threshold = 0;\n+\t\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE_NRL, nr_pages);\n+\t\treturn true;\n+\t}\n+\n+\tdef_th = sysctl_pghot_freq_window;\n+\trate_limit = MB_TO_PAGES(sysctl_pghot_promote_rate_limit);\n+\tkmigrated_promotion_adjust_threshold(pgdat, rate_limit, def_th, now_ms);\n+\n+\tth = pgdat->nbp_threshold ? : def_th;\n+\tif (pghot_access_latency(time, now) >= th)\n+\t\treturn false;\n+\n+\treturn !kmigrated_promotion_rate_limit(pgdat, rate_limit, nr_pages, now_ms);\n+}\n+\n static int pghot_get_hotness(unsigned long pfn, int *nid, int *freq,\n \t\t\t     unsigned long *time)\n {\n@@ -197,6 +318,9 @@ static void kmigrated_walk_zone(unsigned long start_pfn, unsigned long end_pfn,\n \t\tif (folio_nid(folio) == nid)\n \t\t\tgoto out_next;\n \n+\t\tif (!kmigrated_should_migrate_memory(nr, nid, time))\n+\t\t\tgoto out_next;\n+\n \t\tif (migrate_misplaced_folio_prepare(folio, NULL, nid))\n \t\t\tgoto out_next;\n \n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed a concern about the use of IBS (Instruction Based Sampling) for memory access tracking, explaining that it provides physical and virtual address information and indicates if the access came from a slower tier. The author also acknowledged the need to find a clean way to disable perf IBS so that IBS can be used for memory access profiling.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged concern",
                "need to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Use IBS (Instruction Based Sampling) feature present\nin AMD processors for memory access tracking. The access\ninformation obtained from IBS via NMI is fed to pghot\nsub-system for futher action.\n\nIn addition to many other information related to the memory\naccess, IBS provides physical (and virtual) address of the access\nand indicates if the access came from slower tier. Only memory\naccesses originating from slower tiers are further acted upon\nby this driver.\n\nThe samples are initially accumulated in percpu buffers which\nare flushed to pghot hot page tracking mechanism using irq_work.\n\nTODO: Many counters are added to vmstat just as debugging aid\nfor now.\n\nAbout IBS\n---------\nIBS can be programmed to provide data about instruction\nexecution periodically. This is done by programming a desired\nsample count (number of ops) in a control register. When the\nprogrammed number of ops are dispatched, a micro-op gets tagged,\nvarious information about the tagged micro-op's execution is\npopulated in IBS execution MSRs and an interrupt is raised.\nWhile IBS provides a lot of data for each sample, for the\npurpose of  memory access profiling, we are interested in\nlinear and physical address of the memory access that reached\nDRAM. Recent AMD processors provide further filtering where\nit is possible to limit the sampling to those ops that had\nan L3 miss which greately reduces the non-useful samples.\n\nWhile IBS provides capability to sample instruction fetch\nand execution, only IBS execution sampling is used here\nto collect data about memory accesses that occur during\nthe instruction execution.\n\nMore information about IBS is available in Sec 13.3 of\nAMD64 Architecture Programmer's Manual, Volume 2:System\nProgramming which is present at:\nhttps://bugzilla.kernel.org/attachment.cgi?id=288923\n\nInformation about MSRs used for programming IBS can be\nfound in Sec 2.1.14.4 of PPR Vol 1 for AMD Family 19h\nModel 11h B1 which is currently present at:\nhttps://www.amd.com/system/files/TechDocs/55901_0.25.zip\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n arch/x86/events/amd/ibs.c        |  10 +\n arch/x86/include/asm/msr-index.h |  16 ++\n arch/x86/mm/Makefile             |   1 +\n arch/x86/mm/ibs.c                | 317 +++++++++++++++++++++++++++++++\n include/linux/pghot.h            |   8 +\n include/linux/vm_event_item.h    |  19 ++\n mm/Kconfig                       |  13 ++\n mm/vmstat.c                      |  19 ++\n 8 files changed, 403 insertions(+)\n create mode 100644 arch/x86/mm/ibs.c\n\ndiff --git a/arch/x86/events/amd/ibs.c b/arch/x86/events/amd/ibs.c\nindex aca89f23d2e0..dc544d084c17 100644\n--- a/arch/x86/events/amd/ibs.c\n+++ b/arch/x86/events/amd/ibs.c\n@@ -13,6 +13,7 @@\n #include <linux/ptrace.h>\n #include <linux/syscore_ops.h>\n #include <linux/sched/clock.h>\n+#include <linux/pghot.h>\n \n #include <asm/apic.h>\n #include <asm/msr.h>\n@@ -1760,6 +1761,15 @@ static __init int amd_ibs_init(void)\n {\n \tu32 caps;\n \n+\t/*\n+\t * TODO: Find a clean way to disable perf IBS so that IBS\n+\t * can be used for memory access profiling.\n+\t */\n+\tif (hwmem_access_profiler_inuse()) {\n+\t\tpr_info(\"IBS isn't available for perf use\\n\");\n+\t\treturn 0;\n+\t}\n+\n \tcaps = __get_ibs_caps();\n \tif (!caps)\n \t\treturn -ENODEV;\t/* ibs not supported by the cpu */\ndiff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h\nindex 3d0a0950d20a..3c5d69ec83a2 100644\n--- a/arch/x86/include/asm/msr-index.h\n+++ b/arch/x86/include/asm/msr-index.h\n@@ -784,6 +784,22 @@\n /* AMD Last Branch Record MSRs */\n #define MSR_AMD64_LBR_SELECT\t\t\t0xc000010e\n \n+/* AMD IBS MSR bits */\n+#define MSR_AMD64_IBSOPDATA2_DATASRC\t\t\t0x7\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_LCL_CACHE\t\t0x1\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_PEER_CACHE_NEAR\t0x2\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_DRAM\t\t0x3\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_FAR_CCX_CACHE\t0x5\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_EXT_MEM\t\t0x8\n+#define\tMSR_AMD64_IBSOPDATA2_RMTNODE\t\t\t0x10\n+\n+#define MSR_AMD64_IBSOPDATA3_LDOP\t\tBIT_ULL(0)\n+#define MSR_AMD64_IBSOPDATA3_STOP\t\tBIT_ULL(1)\n+#define MSR_AMD64_IBSOPDATA3_DCMISS\t\tBIT_ULL(7)\n+#define MSR_AMD64_IBSOPDATA3_LADDR_VALID\tBIT_ULL(17)\n+#define MSR_AMD64_IBSOPDATA3_PADDR_VALID\tBIT_ULL(18)\n+#define MSR_AMD64_IBSOPDATA3_L2MISS\t\tBIT_ULL(20)\n+\n /* Zen4 */\n #define MSR_ZEN4_BP_CFG                 0xc001102e\n #define MSR_ZEN4_BP_CFG_BP_SPEC_REDUCE_BIT 4\ndiff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile\nindex 5b9908f13dcf..361a456582e9 100644\n--- a/arch/x86/mm/Makefile\n+++ b/arch/x86/mm/Makefile\n@@ -57,3 +57,4 @@ obj-$(CONFIG_X86_MEM_ENCRYPT)\t+= mem_encrypt.o\n obj-$(CONFIG_AMD_MEM_ENCRYPT)\t+= mem_encrypt_amd.o\n \n obj-$(CONFIG_AMD_MEM_ENCRYPT)\t+= mem_encrypt_boot.o\n+obj-$(CONFIG_HWMEM_PROFILER)\t+= ibs.o\ndiff --git a/arch/x86/mm/ibs.c b/arch/x86/mm/ibs.c\nnew file mode 100644\nindex 000000000000..752f688375f9\n--- /dev/null\n+++ b/arch/x86/mm/ibs.c\n@@ -0,0 +1,317 @@\n+// SPDX-License-Identifier: GPL-2.0\n+\n+#include <linux/init.h>\n+#include <linux/pghot.h>\n+#include <linux/percpu.h>\n+#include <linux/workqueue.h>\n+#include <linux/irq_work.h>\n+\n+#include <asm/nmi.h>\n+#include <asm/perf_event.h> /* TODO: Move defns like IBS_OP_ENABLE into non-perf header */\n+#include <asm/apic.h>\n+\n+bool hwmem_access_profiling;\n+\n+static u64 ibs_config __read_mostly;\n+static u32 ibs_caps;\n+\n+#define IBS_NR_SAMPLES\t150\n+\n+/*\n+ * Basic access info captured for each memory access.\n+ */\n+struct ibs_sample {\n+\tunsigned long pfn;\n+\tunsigned long time;\t/* jiffies when accessed */\n+\tint nid;\t\t/* Accessing node ID, if known */\n+};\n+\n+/*\n+ * Percpu buffer of access samples. Samples are accumulated here\n+ * before pushing them to pghot sub-system for further action.\n+ */\n+struct ibs_sample_pcpu {\n+\tstruct ibs_sample samples[IBS_NR_SAMPLES];\n+\tint head, tail;\n+};\n+\n+struct ibs_sample_pcpu __percpu *ibs_s;\n+\n+/*\n+ * The workqueue for pushing the percpu access samples to pghot sub-system.\n+ */\n+static struct work_struct ibs_work;\n+static struct irq_work ibs_irq_work;\n+\n+bool hwmem_access_profiler_inuse(void)\n+{\n+\treturn hwmem_access_profiling;\n+}\n+\n+/*\n+ * Record the IBS-reported access sample in percpu buffer.\n+ * Called from IBS NMI handler.\n+ */\n+static int ibs_push_sample(unsigned long pfn, int nid, unsigned long time)\n+{\n+\tstruct ibs_sample_pcpu *ibs_pcpu = raw_cpu_ptr(ibs_s);\n+\tint next = ibs_pcpu->head + 1;\n+\n+\tif (next >= IBS_NR_SAMPLES)\n+\t\tnext = 0;\n+\n+\tif (next == ibs_pcpu->tail)\n+\t\treturn 0;\n+\n+\tibs_pcpu->samples[ibs_pcpu->head].pfn = pfn;\n+\tibs_pcpu->samples[ibs_pcpu->head].time = time;\n+\tibs_pcpu->samples[ibs_pcpu->head].nid = nid;\n+\tibs_pcpu->head = next;\n+\treturn 1;\n+}\n+\n+static int ibs_pop_sample(struct ibs_sample *s)\n+{\n+\tstruct ibs_sample_pcpu *ibs_pcpu = raw_cpu_ptr(ibs_s);\n+\n+\tint next = ibs_pcpu->tail + 1;\n+\n+\tif (ibs_pcpu->head == ibs_pcpu->tail)\n+\t\treturn 0;\n+\n+\tif (next >= IBS_NR_SAMPLES)\n+\t\tnext = 0;\n+\n+\t*s = ibs_pcpu->samples[ibs_pcpu->tail];\n+\tibs_pcpu->tail = next;\n+\treturn 1;\n+}\n+\n+/*\n+ * Remove access samples from percpu buffer and send them\n+ * to pghot sub-system for further action.\n+ */\n+static void ibs_work_handler(struct work_struct *work)\n+{\n+\tstruct ibs_sample s;\n+\n+\twhile (ibs_pop_sample(&s))\n+\t\tpghot_record_access(s.pfn, s.nid, PGHOT_HW_HINTS, s.time);\n+}\n+\n+static void ibs_irq_handler(struct irq_work *i)\n+{\n+\tschedule_work_on(smp_processor_id(), &ibs_work);\n+}\n+\n+/*\n+ * IBS NMI handler: Process the memory access info reported by IBS.\n+ *\n+ * Reads the MSRs to collect all the information about the reported\n+ * memory access, validates the access, stores the valid sample and\n+ * schedules the work on this CPU to further process the sample.\n+ */\n+static int ibs_overflow_handler(unsigned int cmd, struct pt_regs *regs)\n+{\n+\tstruct mm_struct *mm = current->mm;\n+\tu64 ops_ctl, ops_data3, ops_data2;\n+\tu64 laddr = -1, paddr = -1;\n+\tu64 data_src, rmt_node;\n+\tstruct page *page;\n+\tunsigned long pfn;\n+\n+\trdmsrl(MSR_AMD64_IBSOPCTL, ops_ctl);\n+\n+\t/*\n+\t * When IBS sampling period is reprogrammed via read-modify-update\n+\t * of MSR_AMD64_IBSOPCTL, overflow NMIs could be generated with\n+\t * IBS_OP_ENABLE not set. For such cases, return as HANDLED.\n+\t *\n+\t * With this, the handler will say \"handled\" for all NMIs that\n+\t * aren't related to this NMI.  This stems from the limitation of\n+\t * having both status and control bits in one MSR.\n+\t */\n+\tif (!(ops_ctl & IBS_OP_VAL))\n+\t\tgoto handled;\n+\n+\twrmsrl(MSR_AMD64_IBSOPCTL, ops_ctl & ~IBS_OP_VAL);\n+\n+\tcount_vm_event(HWHINT_NR_EVENTS);\n+\n+\tif (!user_mode(regs)) {\n+\t\tcount_vm_event(HWHINT_KERNEL);\n+\t\tgoto handled;\n+\t}\n+\n+\tif (!mm) {\n+\t\tcount_vm_event(HWHINT_KTHREAD);\n+\t\tgoto handled;\n+\t}\n+\n+\trdmsrl(MSR_AMD64_IBSOPDATA3, ops_data3);\n+\n+\t/* Load/Store ops only */\n+\t/* TODO: DataSrc isn't valid for stores, so filter out stores? */\n+\tif (!(ops_data3 & (MSR_AMD64_IBSOPDATA3_LDOP |\n+\t\t\t   MSR_AMD64_IBSOPDATA3_STOP))) {\n+\t\tcount_vm_event(HWHINT_NON_LOAD_STORES);\n+\t\tgoto handled;\n+\t}\n+\n+\t/* Discard the sample if it was L1 or L2 hit */\n+\tif (!(ops_data3 & (MSR_AMD64_IBSOPDATA3_DCMISS |\n+\t\t\t   MSR_AMD64_IBSOPDATA3_L2MISS))) {\n+\t\tcount_vm_event(HWHINT_DC_L2_HITS);\n+\t\tgoto handled;\n+\t}\n+\n+\trdmsrl(MSR_AMD64_IBSOPDATA2, ops_data2);\n+\tdata_src = ops_data2 & MSR_AMD64_IBSOPDATA2_DATASRC;\n+\tif (ibs_caps & IBS_CAPS_ZEN4)\n+\t\tdata_src |= ((ops_data2 & 0xC0) >> 3);\n+\n+\tswitch (data_src) {\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_LCL_CACHE:\n+\t\tcount_vm_event(HWHINT_LOCAL_L3L1L2);\n+\t\tbreak;\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_PEER_CACHE_NEAR:\n+\t\tcount_vm_event(HWHINT_LOCAL_PEER_CACHE_NEAR);\n+\t\tbreak;\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_DRAM:\n+\t\tcount_vm_event(HWHINT_DRAM_ACCESSES);\n+\t\tbreak;\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_EXT_MEM:\n+\t\tcount_vm_event(HWHINT_CXL_ACCESSES);\n+\t\tbreak;\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_FAR_CCX_CACHE:\n+\t\tcount_vm_event(HWHINT_FAR_CACHE_HITS);\n+\t\tbreak;\n+\t}\n+\n+\trmt_node = ops_data2 & MSR_AMD64_IBSOPDATA2_RMTNODE;\n+\tif (rmt_node)\n+\t\tcount_vm_event(HWHINT_REMOTE_NODE);\n+\n+\t/* Is linear addr valid? */\n+\tif (ops_data3 & MSR_AMD64_IBSOPDATA3_LADDR_VALID)\n+\t\trdmsrl(MSR_AMD64_IBSDCLINAD, laddr);\n+\telse {\n+\t\tcount_vm_event(HWHINT_LADDR_INVALID);\n+\t\tgoto handled;\n+\t}\n+\n+\t/* Discard kernel address accesses */\n+\tif (laddr & (1UL << 63)) {\n+\t\tcount_vm_event(HWHINT_KERNEL_ADDR);\n+\t\tgoto handled;\n+\t}\n+\n+\t/* Is phys addr valid? */\n+\tif (ops_data3 & MSR_AMD64_IBSOPDATA3_PADDR_VALID)\n+\t\trdmsrl(MSR_AMD64_IBSDCPHYSAD, paddr);\n+\telse {\n+\t\tcount_vm_event(HWHINT_PADDR_INVALID);\n+\t\tgoto handled;\n+\t}\n+\n+\tpfn = PHYS_PFN(paddr);\n+\tpage = pfn_to_online_page(pfn);\n+\tif (!page)\n+\t\tgoto handled;\n+\n+\tif (!PageLRU(page)) {\n+\t\tcount_vm_event(HWHINT_NON_LRU);\n+\t\tgoto handled;\n+\t}\n+\n+\tif (!ibs_push_sample(pfn, numa_node_id(), jiffies)) {\n+\t\tcount_vm_event(HWHINT_BUFFER_FULL);\n+\t\tgoto handled;\n+\t}\n+\n+\tirq_work_queue(&ibs_irq_work);\n+\tcount_vm_event(HWHINT_USEFUL_SAMPLES);\n+\n+handled:\n+\treturn NMI_HANDLED;\n+}\n+\n+static inline int get_ibs_lvt_offset(void)\n+{\n+\tu64 val;\n+\n+\trdmsrl(MSR_AMD64_IBSCTL, val);\n+\tif (!(val & IBSCTL_LVT_OFFSET_VALID))\n+\t\treturn -EINVAL;\n+\n+\treturn val & IBSCTL_LVT_OFFSET_MASK;\n+}\n+\n+static void setup_APIC_ibs(void)\n+{\n+\tint offset;\n+\n+\toffset = get_ibs_lvt_offset();\n+\tif (offset < 0)\n+\t\tgoto failed;\n+\n+\tif (!setup_APIC_eilvt(offset, 0, APIC_EILVT_MSG_NMI, 0))\n+\t\treturn;\n+failed:\n+\tpr_warn(\"IBS APIC setup failed on cpu #%d\\n\",\n+\t\tsmp_processor_id());\n+}\n+\n+static void clear_APIC_ibs(void)\n+{\n+\tint offset;\n+\n+\toffset = get_ibs_lvt_offset();\n+\tif (offset >= 0)\n+\t\tsetup_APIC_eilvt(offset, 0, APIC_EILVT_MSG_FIX, 1);\n+}\n+\n+static int x86_amd_ibs_access_profile_startup(unsigned int cpu)\n+{\n+\tsetup_APIC_ibs();\n+\treturn 0;\n+}\n+\n+static int x86_amd_ibs_access_profile_teardown(unsigned int cpu)\n+{\n+\tclear_APIC_ibs();\n+\treturn 0;\n+}\n+\n+static int __init ibs_access_profiling_init(void)\n+{\n+\tif (!boot_cpu_has(X86_FEATURE_IBS)) {\n+\t\tpr_info(\"IBS capability is unavailable for access profiling\\n\");\n+\t\treturn 0;\n+\t}\n+\n+\tibs_s = alloc_percpu_gfp(struct ibs_sample_pcpu, GFP_KERNEL | __GFP_ZERO);\n+\tif (!ibs_s)\n+\t\treturn 0;\n+\n+\tINIT_WORK(&ibs_work, ibs_work_handler);\n+\tinit_irq_work(&ibs_irq_work, ibs_irq_handler);\n+\n+\t/* Uses IBS Op sampling */\n+\tibs_config = IBS_OP_CNT_CTL | IBS_OP_ENABLE;\n+\tibs_caps = cpuid_eax(IBS_CPUID_FEATURES);\n+\tif (ibs_caps & IBS_CAPS_ZEN4)\n+\t\tibs_config |= IBS_OP_L3MISSONLY;\n+\n+\tregister_nmi_handler(NMI_LOCAL, ibs_overflow_handler, 0, \"ibs\");\n+\n+\tcpuhp_setup_state(CPUHP_AP_PERF_X86_AMD_IBS_STARTING,\n+\t\t\t  \"x86/amd/ibs_access_profile:starting\",\n+\t\t\t  x86_amd_ibs_access_profile_startup,\n+\t\t\t  x86_amd_ibs_access_profile_teardown);\n+\n+\tpr_info(\"IBS setup for memory access profiling\\n\");\n+\treturn 0;\n+}\n+\n+arch_initcall(ibs_access_profiling_init);\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nindex d3d59b0c0cf6..20ea9767dbdd 100644\n--- a/include/linux/pghot.h\n+++ b/include/linux/pghot.h\n@@ -2,6 +2,14 @@\n #ifndef _LINUX_PGHOT_H\n #define _LINUX_PGHOT_H\n \n+#include <linux/types.h>\n+\n+#ifdef CONFIG_HWMEM_PROFILER\n+bool hwmem_access_profiler_inuse(void);\n+#else\n+static inline bool hwmem_access_profiler_inuse(void) { return false; }\n+#endif\n+\n /* Page hotness temperature sources */\n enum pghot_src {\n \tPGHOT_HW_HINTS,\ndiff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\nindex 5b8fd93b55fd..67efbca9051c 100644\n--- a/include/linux/vm_event_item.h\n+++ b/include/linux/vm_event_item.h\n@@ -193,6 +193,25 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n \t\tPGHOT_RECORD_HWHINTS,\n \t\tPGHOT_RECORD_PGTSCANS,\n \t\tPGHOT_RECORD_HINTFAULTS,\n+#ifdef CONFIG_HWMEM_PROFILER\n+\t\tHWHINT_NR_EVENTS,\n+\t\tHWHINT_KERNEL,\n+\t\tHWHINT_KTHREAD,\n+\t\tHWHINT_NON_LOAD_STORES,\n+\t\tHWHINT_DC_L2_HITS,\n+\t\tHWHINT_LOCAL_L3L1L2,\n+\t\tHWHINT_LOCAL_PEER_CACHE_NEAR,\n+\t\tHWHINT_FAR_CACHE_HITS,\n+\t\tHWHINT_DRAM_ACCESSES,\n+\t\tHWHINT_CXL_ACCESSES,\n+\t\tHWHINT_REMOTE_NODE,\n+\t\tHWHINT_LADDR_INVALID,\n+\t\tHWHINT_KERNEL_ADDR,\n+\t\tHWHINT_PADDR_INVALID,\n+\t\tHWHINT_NON_LRU,\n+\t\tHWHINT_BUFFER_FULL,\n+\t\tHWHINT_USEFUL_SAMPLES,\n+#endif /* CONFIG_HWMEM_PROFILER */\n #endif /* CONFIG_PGHOT */\n \t\tNR_VM_EVENT_ITEMS\n };\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex fde5aee3e16f..07b16aece877 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -1489,6 +1489,19 @@ config PGHOT_PRECISE\n \t  4 bytes per page against the default one byte per page. Preferable\n \t  to enable this on systems with multiple nodes in toptier.\n \n+config HWMEM_PROFILER\n+\tbool \"HW based memory access profiling\"\n+\tdef_bool n\n+\tdepends on PGHOT\n+\tdepends on X86_64\n+\thelp\n+\t  Some hardware platforms are capable of providing memory access\n+\t  information in direct and actionable manner. Instruction Based\n+\t  Sampling (IBS) present on AMD Zen CPUs in one such example.\n+\t  Memory accesses obtained via such HW based mechanisms are\n+\t  rolled up to PGHOT sub-system for further action like hot page\n+\t  promotion or NUMA Balancing\n+\n source \"mm/damon/Kconfig\"\n \n endmenu\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex f6f91b9dd887..62c47f44edf0 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -1506,6 +1506,25 @@ const char * const vmstat_text[] = {\n \t[I(PGHOT_RECORD_HWHINTS)]\t\t= \"pghot_recorded_hwhints\",\n \t[I(PGHOT_RECORD_PGTSCANS)]\t\t= \"pghot_recorded_pgtscans\",\n \t[I(PGHOT_RECORD_HINTFAULTS)]\t\t= \"pghot_recorded_hintfaults\",\n+#ifdef CONFIG_HWMEM_PROFILER\n+\t[I(HWHINT_NR_EVENTS)]\t\t\t= \"hwhint_nr_events\",\n+\t[I(HWHINT_KERNEL)]\t\t\t= \"hwhint_kernel\",\n+\t[I(HWHINT_KTHREAD)]\t\t\t= \"hwhint_kthread\",\n+\t[I(HWHINT_NON_LOAD_STORES)]\t\t= \"hwhint_non_load_stores\",\n+\t[I(HWHINT_DC_L2_HITS)]\t\t\t= \"hwhint_dc_l2_hits\",\n+\t[I(HWHINT_LOCAL_L3L1L2)]\t\t= \"hwhint_local_l3l1l2\",\n+\t[I(HWHINT_LOCAL_PEER_CACHE_NEAR)]\t= \"hwhint_local_peer_cache_near\",\n+\t[I(HWHINT_FAR_CACHE_HITS)]\t\t= \"hwhint_far_cache_hits\",\n+\t[I(HWHINT_DRAM_ACCESSES)]\t\t= \"hwhint_dram_accesses\",\n+\t[I(HWHINT_CXL_ACCESSES)]\t\t= \"hwhint_cxl_accesses\",\n+\t[I(HWHINT_REMOTE_NODE)]\t\t\t= \"hwhint_remote_node\",\n+\t[I(HWHINT_LADDR_INVALID)]\t\t= \"hwhint_invalid_laddr\",\n+\t[I(HWHINT_KERNEL_ADDR)]\t\t\t= \"hwhint_kernel_addr\",\n+\t[I(HWHINT_PADDR_INVALID)]\t\t= \"hwhint_invalid_paddr\",\n+\t[I(HWHINT_NON_LRU)]\t\t\t= \"hwhint_non_lru\",\n+\t[I(HWHINT_BUFFER_FULL)]\t\t\t= \"hwhint_buffer_full\",\n+\t[I(HWHINT_USEFUL_SAMPLES)]\t\t= \"hwhint_useful_samples\",\n+#endif /* CONFIG_HWMEM_PROFILER */\n #endif /* CONFIG_PGHOT */\n #undef I\n #endif /* CONFIG_VM_EVENT_COUNTERS */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed a concern about the IBS (Integrated Bottleneck Detection) memory access data collection being turned on for kernel mode execution, which is not desired. The author explicitly disabled profiling in NMI handlers and stated that they will re-examine the code to separate architecture-specific logic.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "re-examination"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Enable IBS memory access data collection for user memory\naccesses by programming the required MSRs. The profiling\nis turned ON only for user mode execution and turned OFF\nfor kernel mode execution. Profiling is explicitly disabled\nfor NMI handler too.\n\nTODOs:\n\n- IBS sampling rate is kept fixed for now.\n- Arch/vendor separation/isolation of the code needs relook.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n arch/x86/include/asm/entry-common.h |  3 +++\n arch/x86/include/asm/hardirq.h      |  2 ++\n arch/x86/mm/ibs.c                   | 32 +++++++++++++++++++++++++++++\n include/linux/pghot.h               |  4 ++++\n 4 files changed, 41 insertions(+)\n\ndiff --git a/arch/x86/include/asm/entry-common.h b/arch/x86/include/asm/entry-common.h\nindex ce3eb6d5fdf9..0f381a63669e 100644\n--- a/arch/x86/include/asm/entry-common.h\n+++ b/arch/x86/include/asm/entry-common.h\n@@ -4,6 +4,7 @@\n \n #include <linux/randomize_kstack.h>\n #include <linux/user-return-notifier.h>\n+#include <linux/pghot.h>\n \n #include <asm/nospec-branch.h>\n #include <asm/io_bitmap.h>\n@@ -13,6 +14,7 @@\n /* Check that the stack and regs on entry from user mode are sane. */\n static __always_inline void arch_enter_from_user_mode(struct pt_regs *regs)\n {\n+\thwmem_access_profiling_stop();\n \tif (IS_ENABLED(CONFIG_DEBUG_ENTRY)) {\n \t\t/*\n \t\t * Make sure that the entry code gave us a sensible EFLAGS\n@@ -106,6 +108,7 @@ static inline void arch_exit_to_user_mode_prepare(struct pt_regs *regs,\n static __always_inline void arch_exit_to_user_mode(void)\n {\n \tamd_clear_divider();\n+\thwmem_access_profiling_start();\n }\n #define arch_exit_to_user_mode arch_exit_to_user_mode\n \ndiff --git a/arch/x86/include/asm/hardirq.h b/arch/x86/include/asm/hardirq.h\nindex 6b6d472baa0b..e80c305c17d1 100644\n--- a/arch/x86/include/asm/hardirq.h\n+++ b/arch/x86/include/asm/hardirq.h\n@@ -91,4 +91,6 @@ static __always_inline bool kvm_get_cpu_l1tf_flush_l1d(void)\n static __always_inline void kvm_set_cpu_l1tf_flush_l1d(void) { }\n #endif /* IS_ENABLED(CONFIG_KVM_INTEL) */\n \n+#define arch_nmi_enter()\thwmem_access_profiling_stop()\n+#define arch_nmi_exit()\t\thwmem_access_profiling_start()\n #endif /* _ASM_X86_HARDIRQ_H */\ndiff --git a/arch/x86/mm/ibs.c b/arch/x86/mm/ibs.c\nindex 752f688375f9..d0d93f09432d 100644\n--- a/arch/x86/mm/ibs.c\n+++ b/arch/x86/mm/ibs.c\n@@ -16,6 +16,7 @@ static u64 ibs_config __read_mostly;\n static u32 ibs_caps;\n \n #define IBS_NR_SAMPLES\t150\n+#define IBS_SAMPLE_PERIOD      10000\n \n /*\n  * Basic access info captured for each memory access.\n@@ -43,6 +44,36 @@ struct ibs_sample_pcpu __percpu *ibs_s;\n static struct work_struct ibs_work;\n static struct irq_work ibs_irq_work;\n \n+void hwmem_access_profiling_stop(void)\n+{\n+\tu64 ops_ctl;\n+\n+\tif (!hwmem_access_profiling)\n+\t\treturn;\n+\n+\trdmsrl(MSR_AMD64_IBSOPCTL, ops_ctl);\n+\twrmsrl(MSR_AMD64_IBSOPCTL, ops_ctl & ~IBS_OP_ENABLE);\n+}\n+\n+void hwmem_access_profiling_start(void)\n+{\n+\tu64 config = 0;\n+\tunsigned int period = IBS_SAMPLE_PERIOD;\n+\n+\tif (!hwmem_access_profiling)\n+\t\treturn;\n+\n+\t/* Disable IBS for kernel thread */\n+\tif (!current->mm)\n+\t\tgoto out;\n+\n+\tconfig = (period >> 4) & IBS_OP_MAX_CNT;\n+\tconfig |= (period & IBS_OP_MAX_CNT_EXT_MASK);\n+\tconfig |= ibs_config;\n+out:\n+\twrmsrl(MSR_AMD64_IBSOPCTL, config);\n+}\n+\n bool hwmem_access_profiler_inuse(void)\n {\n \treturn hwmem_access_profiling;\n@@ -310,6 +341,7 @@ static int __init ibs_access_profiling_init(void)\n \t\t\t  x86_amd_ibs_access_profile_startup,\n \t\t\t  x86_amd_ibs_access_profile_teardown);\n \n+\thwmem_access_profiling = true;\n \tpr_info(\"IBS setup for memory access profiling\\n\");\n \treturn 0;\n }\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nindex 20ea9767dbdd..603791183102 100644\n--- a/include/linux/pghot.h\n+++ b/include/linux/pghot.h\n@@ -6,8 +6,12 @@\n \n #ifdef CONFIG_HWMEM_PROFILER\n bool hwmem_access_profiler_inuse(void);\n+void hwmem_access_profiling_start(void);\n+void hwmem_access_profiling_stop(void);\n #else\n static inline bool hwmem_access_profiler_inuse(void) { return false; }\n+static inline void hwmem_access_profiling_start(void) {}\n+static inline void hwmem_access_profiling_stop(void) {}\n #endif\n \n /* Page hotness temperature sources */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "Author acknowledged the need to refactor MGLRU page table walking logic to make it resumable and introduced two new hooks: accessed callback and flush callback, which will be used in a new scan function that repeatedly scans on the same young generation without adding a new one. The author did not mention any plans for fixing this issue in the current patchset.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for refactor",
                "introduced new hooks"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Kinsey Ho <kinseyho@google.com>\n\nRefactor the existing MGLRU page table walking logic to make it\nresumable.\n\nAdditionally, introduce two hooks into the MGLRU page table walk:\naccessed callback and flush callback. The accessed callback is called\nfor each accessed page detected via the scanned accessed bit. The flush\ncallback is called when the accessed callback reports that a flush is\nrequired. This allows for processing pages in batches for efficiency.\n\nWith a generalised page table walk, introduce a new scan function which\nrepeatedly scans on the same young generation and does not add a new\nyoung generation.\n\nSigned-off-by: Kinsey Ho <kinseyho@google.com>\nSigned-off-by: Yuanchu Xie <yuanchu@google.com>\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n include/linux/mmzone.h |   5 ++\n mm/internal.h          |   4 +\n mm/vmscan.c            | 181 +++++++++++++++++++++++++++++++----------\n 3 files changed, 145 insertions(+), 45 deletions(-)\n\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 49c374064fc2..26350a4951ff 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -548,6 +548,8 @@ struct lru_gen_mm_walk {\n \tunsigned long seq;\n \t/* the next address within an mm to scan */\n \tunsigned long next_addr;\n+\t/* called for each accessed pte/pmd */\n+\tbool (*accessed_cb)(unsigned long pfn);\n \t/* to batch promoted pages */\n \tint nr_pages[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES];\n \t/* to batch the mm stats */\n@@ -555,6 +557,9 @@ struct lru_gen_mm_walk {\n \t/* total batched items */\n \tint batched;\n \tint swappiness;\n+\t/* for the pmd under scanning */\n+\tint nr_young_pte;\n+\tint nr_total_pte;\n \tbool force_scan;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex e430da900430..426db1ae286f 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -538,6 +538,10 @@ extern unsigned long highest_memmap_pfn;\n bool folio_isolate_lru(struct folio *folio);\n void folio_putback_lru(struct folio *folio);\n extern void reclaim_throttle(pg_data_t *pgdat, enum vmscan_throttle_state reason);\n+void set_task_reclaim_state(struct task_struct *task,\n+\t\t\t\t   struct reclaim_state *rs);\n+void lru_gen_scan_lruvec(struct lruvec *lruvec, unsigned long seq,\n+\t\t\t bool (*accessed_cb)(unsigned long), void (*flush_cb)(void));\n #ifdef CONFIG_NUMA\n int user_proactive_reclaim(char *buf,\n \t\t\t   struct mem_cgroup *memcg, pg_data_t *pgdat);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 670fe9fae5ba..02f3dd128638 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -289,7 +289,7 @@ static int sc_swappiness(struct scan_control *sc, struct mem_cgroup *memcg)\n \t\t\tcontinue;\t\t\t\t\\\n \t\telse\n \n-static void set_task_reclaim_state(struct task_struct *task,\n+void set_task_reclaim_state(struct task_struct *task,\n \t\t\t\t   struct reclaim_state *rs)\n {\n \t/* Check for an overwrite */\n@@ -3058,7 +3058,7 @@ static bool iterate_mm_list(struct lru_gen_mm_walk *walk, struct mm_struct **ite\n \n \tVM_WARN_ON_ONCE(mm_state->seq + 1 < walk->seq);\n \n-\tif (walk->seq <= mm_state->seq)\n+\tif (!walk->accessed_cb && walk->seq <= mm_state->seq)\n \t\tgoto done;\n \n \tif (!mm_state->head)\n@@ -3484,16 +3484,14 @@ static void walk_update_folio(struct lru_gen_mm_walk *walk, struct folio *folio,\n \t}\n }\n \n-static bool walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n-\t\t\t   struct mm_walk *args)\n+static int walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n+\t\t\t   struct mm_walk *args, bool *suitable)\n {\n \tint i;\n \tbool dirty;\n \tpte_t *pte;\n \tspinlock_t *ptl;\n \tunsigned long addr;\n-\tint total = 0;\n-\tint young = 0;\n \tstruct folio *last = NULL;\n \tstruct lru_gen_mm_walk *walk = args->private;\n \tstruct mem_cgroup *memcg = lruvec_memcg(walk->lruvec);\n@@ -3501,19 +3499,24 @@ static bool walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n \tDEFINE_MAX_SEQ(walk->lruvec);\n \tint gen = lru_gen_from_seq(max_seq);\n \tpmd_t pmdval;\n+\tint err = 0;\n \n \tpte = pte_offset_map_rw_nolock(args->mm, pmd, start & PMD_MASK, &pmdval, &ptl);\n-\tif (!pte)\n-\t\treturn false;\n+\tif (!pte) {\n+\t\t*suitable = false;\n+\t\treturn err;\n+\t}\n \n \tif (!spin_trylock(ptl)) {\n \t\tpte_unmap(pte);\n-\t\treturn true;\n+\t\t*suitable = true;\n+\t\treturn err;\n \t}\n \n \tif (unlikely(!pmd_same(pmdval, pmdp_get_lockless(pmd)))) {\n \t\tpte_unmap_unlock(pte, ptl);\n-\t\treturn false;\n+\t\t*suitable = false;\n+\t\treturn err;\n \t}\n \n \tarch_enter_lazy_mmu_mode();\n@@ -3522,8 +3525,9 @@ static bool walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n \t\tunsigned long pfn;\n \t\tstruct folio *folio;\n \t\tpte_t ptent = ptep_get(pte + i);\n+\t\tbool do_flush;\n \n-\t\ttotal++;\n+\t\twalk->nr_total_pte++;\n \t\twalk->mm_stats[MM_LEAF_TOTAL]++;\n \n \t\tpfn = get_pte_pfn(ptent, args->vma, addr, pgdat);\n@@ -3547,23 +3551,36 @@ static bool walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n \t\tif (pte_dirty(ptent))\n \t\t\tdirty = true;\n \n-\t\tyoung++;\n+\t\twalk->nr_young_pte++;\n \t\twalk->mm_stats[MM_LEAF_YOUNG]++;\n+\n+\t\tif (!walk->accessed_cb)\n+\t\t\tcontinue;\n+\n+\t\tdo_flush = walk->accessed_cb(pfn);\n+\t\tif (do_flush) {\n+\t\t\twalk->next_addr = addr + PAGE_SIZE;\n+\n+\t\t\terr = -EAGAIN;\n+\t\t\tbreak;\n+\t\t}\n \t}\n \n \twalk_update_folio(walk, last, gen, dirty);\n \tlast = NULL;\n \n-\tif (i < PTRS_PER_PTE && get_next_vma(PMD_MASK, PAGE_SIZE, args, &start, &end))\n+\tif (!err && i < PTRS_PER_PTE &&\n+\t    get_next_vma(PMD_MASK, PAGE_SIZE, args, &start, &end))\n \t\tgoto restart;\n \n \tarch_leave_lazy_mmu_mode();\n \tpte_unmap_unlock(pte, ptl);\n \n-\treturn suitable_to_scan(total, young);\n+\t*suitable = suitable_to_scan(walk->nr_total_pte, walk->nr_young_pte);\n+\treturn err;\n }\n \n-static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area_struct *vma,\n+static int walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area_struct *vma,\n \t\t\t\t  struct mm_walk *args, unsigned long *bitmap, unsigned long *first)\n {\n \tint i;\n@@ -3576,6 +3593,7 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \tstruct pglist_data *pgdat = lruvec_pgdat(walk->lruvec);\n \tDEFINE_MAX_SEQ(walk->lruvec);\n \tint gen = lru_gen_from_seq(max_seq);\n+\tint err = 0;\n \n \tVM_WARN_ON_ONCE(pud_leaf(*pud));\n \n@@ -3583,13 +3601,13 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \tif (*first == -1) {\n \t\t*first = addr;\n \t\tbitmap_zero(bitmap, MIN_LRU_BATCH);\n-\t\treturn;\n+\t\treturn err;\n \t}\n \n \ti = addr == -1 ? 0 : pmd_index(addr) - pmd_index(*first);\n \tif (i && i <= MIN_LRU_BATCH) {\n \t\t__set_bit(i - 1, bitmap);\n-\t\treturn;\n+\t\treturn err;\n \t}\n \n \tpmd = pmd_offset(pud, *first);\n@@ -3603,6 +3621,7 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \tdo {\n \t\tunsigned long pfn;\n \t\tstruct folio *folio;\n+\t\tbool do_flush;\n \n \t\t/* don't round down the first address */\n \t\taddr = i ? (*first & PMD_MASK) + i * PMD_SIZE : *first;\n@@ -3639,6 +3658,17 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \t\t\tdirty = true;\n \n \t\twalk->mm_stats[MM_LEAF_YOUNG]++;\n+\t\tif (!walk->accessed_cb)\n+\t\t\tgoto next;\n+\n+\t\tdo_flush = walk->accessed_cb(pfn);\n+\t\tif (do_flush) {\n+\t\t\ti = find_next_bit(bitmap, MIN_LRU_BATCH, i) + 1;\n+\n+\t\t\twalk->next_addr = (*first & PMD_MASK) + i * PMD_SIZE;\n+\t\t\terr = -EAGAIN;\n+\t\t\tbreak;\n+\t\t}\n next:\n \t\ti = i > MIN_LRU_BATCH ? 0 : find_next_bit(bitmap, MIN_LRU_BATCH, i) + 1;\n \t} while (i <= MIN_LRU_BATCH);\n@@ -3649,9 +3679,10 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \tspin_unlock(ptl);\n done:\n \t*first = -1;\n+\treturn err;\n }\n \n-static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n+static int walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \t\t\t   struct mm_walk *args)\n {\n \tint i;\n@@ -3663,6 +3694,7 @@ static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \tunsigned long first = -1;\n \tstruct lru_gen_mm_walk *walk = args->private;\n \tstruct lru_gen_mm_state *mm_state = get_mm_state(walk->lruvec);\n+\tint err = 0;\n \n \tVM_WARN_ON_ONCE(pud_leaf(*pud));\n \n@@ -3676,6 +3708,7 @@ static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \t/* walk_pte_range() may call get_next_vma() */\n \tvma = args->vma;\n \tfor (i = pmd_index(start), addr = start; addr != end; i++, addr = next) {\n+\t\tbool suitable;\n \t\tpmd_t val = pmdp_get_lockless(pmd + i);\n \n \t\tnext = pmd_addr_end(addr, end);\n@@ -3692,7 +3725,10 @@ static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \t\t\twalk->mm_stats[MM_LEAF_TOTAL]++;\n \n \t\t\tif (pfn != -1)\n-\t\t\t\twalk_pmd_range_locked(pud, addr, vma, args, bitmap, &first);\n+\t\t\t\terr = walk_pmd_range_locked(pud, addr, vma, args,\n+\t\t\t\t\t\tbitmap, &first);\n+\t\t\tif (err)\n+\t\t\t\treturn err;\n \t\t\tcontinue;\n \t\t}\n \n@@ -3701,33 +3737,51 @@ static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \t\t\tif (!pmd_young(val))\n \t\t\t\tcontinue;\n \n-\t\t\twalk_pmd_range_locked(pud, addr, vma, args, bitmap, &first);\n+\t\t\terr = walk_pmd_range_locked(pud, addr, vma, args,\n+\t\t\t\t\t\tbitmap, &first);\n+\t\t\tif (err)\n+\t\t\t\treturn err;\n \t\t}\n \n \t\tif (!walk->force_scan && !test_bloom_filter(mm_state, walk->seq, pmd + i))\n \t\t\tcontinue;\n \n+\t\terr = walk_pte_range(&val, addr, next, args, &suitable);\n+\t\tif (err && walk->next_addr < next && first == -1)\n+\t\t\treturn err;\n+\n+\t\twalk->nr_total_pte = 0;\n+\t\twalk->nr_young_pte = 0;\n+\n \t\twalk->mm_stats[MM_NONLEAF_FOUND]++;\n \n-\t\tif (!walk_pte_range(&val, addr, next, args))\n-\t\t\tcontinue;\n+\t\tif (!suitable)\n+\t\t\tgoto next;\n \n \t\twalk->mm_stats[MM_NONLEAF_ADDED]++;\n \n \t\t/* carry over to the next generation */\n \t\tupdate_bloom_filter(mm_state, walk->seq + 1, pmd + i);\n+next:\n+\t\tif (err) {\n+\t\t\twalk->next_addr = first;\n+\t\t\treturn err;\n+\t\t}\n \t}\n \n-\twalk_pmd_range_locked(pud, -1, vma, args, bitmap, &first);\n+\terr = walk_pmd_range_locked(pud, -1, vma, args, bitmap, &first);\n \n-\tif (i < PTRS_PER_PMD && get_next_vma(PUD_MASK, PMD_SIZE, args, &start, &end))\n+\tif (!err && i < PTRS_PER_PMD &&\n+\t    get_next_vma(PUD_MASK, PMD_SIZE, args, &start, &end))\n \t\tgoto restart;\n+\n+\treturn err;\n }\n \n static int walk_pud_range(p4d_t *p4d, unsigned long start, unsigned long end,\n \t\t\t  struct mm_walk *args)\n {\n-\tint i;\n+\tint i, err;\n \tpud_t *pud;\n \tunsigned long addr;\n \tunsigned long next;\n@@ -3745,7 +3799,9 @@ static int walk_pud_range(p4d_t *p4d, unsigned long start, unsigned long end,\n \t\tif (!pud_present(val) || WARN_ON_ONCE(pud_leaf(val)))\n \t\t\tcontinue;\n \n-\t\twalk_pmd_range(&val, addr, next, args);\n+\t\terr = walk_pmd_range(&val, addr, next, args);\n+\t\tif (err)\n+\t\t\treturn err;\n \n \t\tif (need_resched() || walk->batched >= MAX_LRU_BATCH) {\n \t\t\tend = (addr | ~PUD_MASK) + 1;\n@@ -3766,40 +3822,48 @@ static int walk_pud_range(p4d_t *p4d, unsigned long start, unsigned long end,\n \treturn -EAGAIN;\n }\n \n-static void walk_mm(struct mm_struct *mm, struct lru_gen_mm_walk *walk)\n+static int try_walk_mm(struct mm_struct *mm, struct lru_gen_mm_walk *walk)\n {\n+\tint err;\n \tstatic const struct mm_walk_ops mm_walk_ops = {\n \t\t.test_walk = should_skip_vma,\n \t\t.p4d_entry = walk_pud_range,\n \t\t.walk_lock = PGWALK_RDLOCK,\n \t};\n-\tint err;\n \tstruct lruvec *lruvec = walk->lruvec;\n \n-\twalk->next_addr = FIRST_USER_ADDRESS;\n+\tDEFINE_MAX_SEQ(lruvec);\n \n-\tdo {\n-\t\tDEFINE_MAX_SEQ(lruvec);\n+\terr = -EBUSY;\n \n-\t\terr = -EBUSY;\n+\t/* another thread might have called inc_max_seq() */\n+\tif (walk->seq != max_seq)\n+\t\treturn err;\n \n-\t\t/* another thread might have called inc_max_seq() */\n-\t\tif (walk->seq != max_seq)\n-\t\t\tbreak;\n+\t/* the caller might be holding the lock for write */\n+\tif (mmap_read_trylock(mm)) {\n+\t\terr = walk_page_range(mm, walk->next_addr, ULONG_MAX,\n+\t\t\t\t      &mm_walk_ops, walk);\n \n-\t\t/* the caller might be holding the lock for write */\n-\t\tif (mmap_read_trylock(mm)) {\n-\t\t\terr = walk_page_range(mm, walk->next_addr, ULONG_MAX, &mm_walk_ops, walk);\n+\t\tmmap_read_unlock(mm);\n+\t}\n \n-\t\t\tmmap_read_unlock(mm);\n-\t\t}\n+\tif (walk->batched) {\n+\t\tspin_lock_irq(&lruvec->lru_lock);\n+\t\treset_batch_size(walk);\n+\t\tspin_unlock_irq(&lruvec->lru_lock);\n+\t}\n \n-\t\tif (walk->batched) {\n-\t\t\tspin_lock_irq(&lruvec->lru_lock);\n-\t\t\treset_batch_size(walk);\n-\t\t\tspin_unlock_irq(&lruvec->lru_lock);\n-\t\t}\n+\treturn err;\n+}\n+\n+static void walk_mm(struct mm_struct *mm, struct lru_gen_mm_walk *walk)\n+{\n+\tint err;\n \n+\twalk->next_addr = FIRST_USER_ADDRESS;\n+\tdo {\n+\t\terr = try_walk_mm(mm, walk);\n \t\tcond_resched();\n \t} while (err == -EAGAIN);\n }\n@@ -4011,6 +4075,33 @@ static bool inc_max_seq(struct lruvec *lruvec, unsigned long seq, int swappiness\n \treturn success;\n }\n \n+void lru_gen_scan_lruvec(struct lruvec *lruvec, unsigned long seq,\n+\t\t\t bool (*accessed_cb)(unsigned long), void (*flush_cb)(void))\n+{\n+\tstruct lru_gen_mm_walk *walk = current->reclaim_state->mm_walk;\n+\tstruct mm_struct *mm = NULL;\n+\n+\twalk->lruvec = lruvec;\n+\twalk->seq = seq;\n+\twalk->accessed_cb = accessed_cb;\n+\twalk->swappiness = MAX_SWAPPINESS;\n+\n+\tdo {\n+\t\tint err = -EBUSY;\n+\n+\t\titerate_mm_list(walk, &mm);\n+\t\tif (!mm)\n+\t\t\tbreak;\n+\n+\t\twalk->next_addr = FIRST_USER_ADDRESS;\n+\t\tdo {\n+\t\t\terr = try_walk_mm(mm, walk);\n+\t\t\tcond_resched();\n+\t\t\tflush_cb();\n+\t\t} while (err == -EAGAIN);\n+\t} while (mm);\n+}\n+\n static bool try_to_inc_max_seq(struct lruvec *lruvec, unsigned long seq,\n \t\t\t       int swappiness, bool force_scan)\n {\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author is addressing a concern about the overhead of the new subsystem, specifically the memory usage in precision mode. They are introducing a new kernel daemon, klruscand, that periodically invokes the MGLRU page table walk to gather access information and forward it to pghot for promotion decisions. This benefits from reusing existing infrastructure optimized with features like hierarchical scanning and bloom filters. The author has not explicitly stated whether this addresses the original concern about memory usage.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "addressing_concern",
                "new_infrastructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Kinsey Ho <kinseyho@google.com>\n\nIntroduce a new kernel daemon, klruscand, that periodically invokes the\nMGLRU page table walk. It leverages the new callbacks to gather access\ninformation and forwards it to pghot sub-system for promotion decisions.\n\nThis benefits from reusing the existing MGLRU page table walk\ninfrastructure, which is optimized with features such as hierarchical\nscanning and bloom filters to reduce CPU overhead.\n\nAs an additional optimization to be added in the future, we can tune\nthe scan intervals for each memcg.\n\nSigned-off-by: Kinsey Ho <kinseyho@google.com>\nSigned-off-by: Yuanchu Xie <yuanchu@google.com>\n[Reduced the scan interval to 500ms, KLRUSCAND to default n in config]\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n mm/Kconfig     |   8 ++++\n mm/Makefile    |   1 +\n mm/klruscand.c | 110 +++++++++++++++++++++++++++++++++++++++++++++++++\n 3 files changed, 119 insertions(+)\n create mode 100644 mm/klruscand.c\n\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex 07b16aece877..9e9eca8db8bf 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -1502,6 +1502,14 @@ config HWMEM_PROFILER\n \t  rolled up to PGHOT sub-system for further action like hot page\n \t  promotion or NUMA Balancing\n \n+config KLRUSCAND\n+\tbool \"Kernel lower tier access scan daemon\"\n+\tdefault n\n+\tdepends on PGHOT && LRU_GEN_WALKS_MMU\n+\thelp\n+\t  Scan for accesses from lower tiers by invoking MGLRU to perform\n+\t  page table walks.\n+\n source \"mm/damon/Kconfig\"\n \n endmenu\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 89f999647752..c68df497a063 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -153,3 +153,4 @@ obj-$(CONFIG_PGHOT) += pghot-precise.o\n else\n obj-$(CONFIG_PGHOT) += pghot-default.o\n endif\n+obj-$(CONFIG_KLRUSCAND) += klruscand.o\ndiff --git a/mm/klruscand.c b/mm/klruscand.c\nnew file mode 100644\nindex 000000000000..13a41b38d67d\n--- /dev/null\n+++ b/mm/klruscand.c\n@@ -0,0 +1,110 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+#include <linux/memcontrol.h>\n+#include <linux/kthread.h>\n+#include <linux/module.h>\n+#include <linux/vmalloc.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/pghot.h>\n+\n+#include \"internal.h\"\n+\n+#define KLRUSCAND_INTERVAL 500\n+#define BATCH_SIZE (2 << 16)\n+\n+static struct task_struct *scan_thread;\n+static unsigned long pfn_batch[BATCH_SIZE];\n+static int batch_index;\n+\n+static void flush_cb(void)\n+{\n+\tint i;\n+\n+\tfor (i = 0; i < batch_index; i++) {\n+\t\tunsigned long pfn = pfn_batch[i];\n+\n+\t\tpghot_record_access(pfn, NUMA_NO_NODE, PGHOT_PGTABLE_SCAN, jiffies);\n+\n+\t\tif (i % 16 == 0)\n+\t\t\tcond_resched();\n+\t}\n+\tbatch_index = 0;\n+}\n+\n+static bool accessed_cb(unsigned long pfn)\n+{\n+\tWARN_ON_ONCE(batch_index == BATCH_SIZE);\n+\n+\tif (batch_index < BATCH_SIZE)\n+\t\tpfn_batch[batch_index++] = pfn;\n+\n+\treturn batch_index == BATCH_SIZE;\n+}\n+\n+static int klruscand_run(void *unused)\n+{\n+\tstruct lru_gen_mm_walk *walk;\n+\n+\twalk = kzalloc(sizeof(*walk),\n+\t\t       __GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN);\n+\tif (!walk)\n+\t\treturn -ENOMEM;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tunsigned long next_wake_time;\n+\t\tlong sleep_time;\n+\t\tstruct mem_cgroup *memcg;\n+\t\tint flags;\n+\t\tint nid;\n+\n+\t\tnext_wake_time = jiffies + msecs_to_jiffies(KLRUSCAND_INTERVAL);\n+\n+\t\tfor_each_node_state(nid, N_MEMORY) {\n+\t\t\tpg_data_t *pgdat = NODE_DATA(nid);\n+\t\t\tstruct reclaim_state rs = { 0 };\n+\n+\t\t\tif (node_is_toptier(nid))\n+\t\t\t\tcontinue;\n+\n+\t\t\trs.mm_walk = walk;\n+\t\t\tset_task_reclaim_state(current, &rs);\n+\t\t\tflags = memalloc_noreclaim_save();\n+\n+\t\t\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);\n+\t\t\tdo {\n+\t\t\t\tstruct lruvec *lruvec =\n+\t\t\t\t\tmem_cgroup_lruvec(memcg, pgdat);\n+\t\t\t\tunsigned long max_seq =\n+\t\t\t\t\tREAD_ONCE((lruvec)->lrugen.max_seq);\n+\n+\t\t\t\tlru_gen_scan_lruvec(lruvec, max_seq, accessed_cb, flush_cb);\n+\t\t\t\tcond_resched();\n+\t\t\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)));\n+\n+\t\t\tmemalloc_noreclaim_restore(flags);\n+\t\t\tset_task_reclaim_state(current, NULL);\n+\t\t\tmemset(walk, 0, sizeof(*walk));\n+\t\t}\n+\n+\t\tsleep_time = next_wake_time - jiffies;\n+\t\tif (sleep_time > 0 && sleep_time != MAX_SCHEDULE_TIMEOUT)\n+\t\t\tschedule_timeout_idle(sleep_time);\n+\t}\n+\tkfree(walk);\n+\treturn 0;\n+}\n+\n+static int __init klruscand_init(void)\n+{\n+\tstruct task_struct *task;\n+\n+\ttask = kthread_run(klruscand_run, NULL, \"klruscand\");\n+\n+\tif (IS_ERR(task)) {\n+\t\tpr_err(\"Failed to create klruscand kthread\\n\");\n+\t\treturn PTR_ERR(task);\n+\t}\n+\n+\tscan_thread = task;\n+\treturn 0;\n+}\n+module_init(klruscand_init);\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author acknowledged that unmapped page cache pages in lower tiers don't get promoted easily and explained how the patch addresses this issue by using folio_mark_accessed() as a source of hotness, but noted that there is still room for improvement (TODO: better naming and evaluating overhead).",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "improvement"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Unmapped page cache pages that end up in lower tiers don't get\npromoted easily. There were attempts to identify such pages and\nget them promoted as part of NUMA Balancing earlier [1]. The\nsame idea is taken forward here by using folio_mark_accessed()\nas a source of hotness.\n\nLower tier accesses from folio_mark_accessed() are reported to\npghot sub-system for hotness tracking and subsequent promotion.\n\nTODO: Need a better naming for this hotness source. Need to\nbetter understand/evaluate the overhead of hotness info\ncollection from this path.\n\n[1] https://lore.kernel.org/linux-mm/20250411221111.493193-1-gourry@gourry.net/\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n Documentation/admin-guide/mm/pghot.txt | 7 ++++++-\n include/linux/pghot.h                  | 5 +++++\n include/linux/vm_event_item.h          | 1 +\n mm/pghot-tunables.c                    | 7 +++++++\n mm/pghot.c                             | 6 ++++++\n mm/swap.c                              | 8 ++++++++\n mm/vmstat.c                            | 1 +\n 7 files changed, 34 insertions(+), 1 deletion(-)\n\ndiff --git a/Documentation/admin-guide/mm/pghot.txt b/Documentation/admin-guide/mm/pghot.txt\nindex b329e692ef89..c8eb61064247 100644\n--- a/Documentation/admin-guide/mm/pghot.txt\n+++ b/Documentation/admin-guide/mm/pghot.txt\n@@ -23,9 +23,10 @@ Path: /sys/kernel/debug/pghot/\n      - 0: Hardware hints (value 0x1)\n      - 1: Page table scan (value 0x2)\n      - 2: Hint faults (value 0x4)\n+     - 3: folio_mark_accessed (value 0x8)\n    - Default: 0 (disabled)\n    - Example:\n-     # echo 0x7 > /sys/kernel/debug/pghot/enabled_sources\n+     # echo 0xf > /sys/kernel/debug/pghot/enabled_sources\n      Enables all sources.\n \n 2. **target_nid**\n@@ -82,3 +83,7 @@ Path: /proc/vmstat\n 4. **pghot_recorded_hintfaults**\n    - Number of recorded accesses reported by NUMA Balancing based\n      hotness source.\n+\n+5. **pghot_recorded_fma**\n+   - Number of recorded accesses reported by folio_mark_accessed()\n+     hotness source.\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nindex 603791183102..8cf9dfb5365a 100644\n--- a/include/linux/pghot.h\n+++ b/include/linux/pghot.h\n@@ -19,6 +19,7 @@ enum pghot_src {\n \tPGHOT_HW_HINTS,\n \tPGHOT_PGTABLE_SCAN,\n \tPGHOT_HINT_FAULT,\n+\tPGHOT_FMA,\n };\n \n #ifdef CONFIG_PGHOT\n@@ -36,6 +37,7 @@ void pghot_debug_init(void);\n DECLARE_STATIC_KEY_FALSE(pghot_src_hwhints);\n DECLARE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n DECLARE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n+DECLARE_STATIC_KEY_FALSE(pghot_src_fma);\n \n /*\n  * Bit positions to enable individual sources in pghot/records_enabled\n@@ -45,6 +47,7 @@ enum pghot_src_enabled {\n \tPGHOT_HWHINTS_BIT = 0,\n \tPGHOT_PGTSCAN_BIT,\n \tPGHOT_HINTFAULT_BIT,\n+\tPGHOT_FMA_BIT,\n \tPGHOT_MAX_BIT\n };\n \n@@ -52,6 +55,8 @@ enum pghot_src_enabled {\n #define PGHOT_PGTSCAN_ENABLED\t\tBIT(PGHOT_PGTSCAN_BIT)\n #define PGHOT_HINTFAULT_ENABLED\t\tBIT(PGHOT_HINTFAULT_BIT)\n #define PGHOT_SRC_ENABLED_MASK\t\tGENMASK(PGHOT_MAX_BIT - 1, 0)\n+#define PGHOT_FMA_ENABLED\t\tBIT(PGHOT_FMA_BIT)\n+#define PGHOT_SRC_ENABLED_MASK\t\tGENMASK(PGHOT_MAX_BIT - 1, 0)\n \n #define PGHOT_DEFAULT_FREQ_THRESHOLD\t2\n \ndiff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\nindex 67efbca9051c..ac1f28646b9c 100644\n--- a/include/linux/vm_event_item.h\n+++ b/include/linux/vm_event_item.h\n@@ -193,6 +193,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n \t\tPGHOT_RECORD_HWHINTS,\n \t\tPGHOT_RECORD_PGTSCANS,\n \t\tPGHOT_RECORD_HINTFAULTS,\n+\t\tPGHOT_RECORD_FMA,\n #ifdef CONFIG_HWMEM_PROFILER\n \t\tHWHINT_NR_EVENTS,\n \t\tHWHINT_KERNEL,\ndiff --git a/mm/pghot-tunables.c b/mm/pghot-tunables.c\nindex 79afbcb1e4f0..11c7f742a1be 100644\n--- a/mm/pghot-tunables.c\n+++ b/mm/pghot-tunables.c\n@@ -124,6 +124,13 @@ static void pghot_src_enabled_update(unsigned int enabled)\n \t\telse\n \t\t\tstatic_branch_disable(&pghot_src_hintfaults);\n \t}\n+\n+\tif (changed & PGHOT_FMA_ENABLED) {\n+\t\tif (enabled & PGHOT_FMA_ENABLED)\n+\t\t\tstatic_branch_enable(&pghot_src_fma);\n+\t\telse\n+\t\t\tstatic_branch_disable(&pghot_src_fma);\n+\t}\n }\n \n static ssize_t pghot_src_enabled_write(struct file *filp, const char __user *ubuf,\ndiff --git a/mm/pghot.c b/mm/pghot.c\nindex 6fc76c1eaff8..537f4af816ff 100644\n--- a/mm/pghot.c\n+++ b/mm/pghot.c\n@@ -43,6 +43,7 @@ static unsigned int sysctl_pghot_promote_rate_limit = 65536;\n DEFINE_STATIC_KEY_FALSE(pghot_src_hwhints);\n DEFINE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n DEFINE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n+DEFINE_STATIC_KEY_FALSE(pghot_src_fma);\n \n #ifdef CONFIG_SYSCTL\n static const struct ctl_table pghot_sysctls[] = {\n@@ -113,6 +114,11 @@ int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now)\n \t\t\treturn -EINVAL;\n \t\tcount_vm_event(PGHOT_RECORD_HINTFAULTS);\n \t\tbreak;\n+\tcase PGHOT_FMA:\n+\t\tif (!static_branch_likely(&pghot_src_fma))\n+\t\t\treturn -EINVAL;\n+\t\tcount_vm_event(PGHOT_RECORD_FMA);\n+\t\tbreak;\n \tdefault:\n \t\treturn -EINVAL;\n \t}\ndiff --git a/mm/swap.c b/mm/swap.c\nindex 2260dcd2775e..31a654b19844 100644\n--- a/mm/swap.c\n+++ b/mm/swap.c\n@@ -37,6 +37,8 @@\n #include <linux/page_idle.h>\n #include <linux/local_lock.h>\n #include <linux/buffer_head.h>\n+#include <linux/pghot.h>\n+#include <linux/memory-tiers.h>\n \n #include \"internal.h\"\n \n@@ -454,8 +456,14 @@ static bool lru_gen_clear_refs(struct folio *folio)\n  */\n void folio_mark_accessed(struct folio *folio)\n {\n+\tunsigned long pfn = folio_pfn(folio);\n+\n \tif (folio_test_dropbehind(folio))\n \t\treturn;\n+\n+\tif (!node_is_toptier(pfn_to_nid(pfn)))\n+\t\tpghot_record_access(pfn, NUMA_NO_NODE, PGHOT_FMA, jiffies);\n+\n \tif (lru_gen_enabled()) {\n \t\tlru_gen_inc_refs(folio);\n \t\treturn;\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 62c47f44edf0..c4d90baf440b 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -1506,6 +1506,7 @@ const char * const vmstat_text[] = {\n \t[I(PGHOT_RECORD_HWHINTS)]\t\t= \"pghot_recorded_hwhints\",\n \t[I(PGHOT_RECORD_PGTSCANS)]\t\t= \"pghot_recorded_pgtscans\",\n \t[I(PGHOT_RECORD_HINTFAULTS)]\t\t= \"pghot_recorded_hintfaults\",\n+\t[I(PGHOT_RECORD_FMA)]\t\t\t= \"pghot_recorded_fma\",\n #ifdef CONFIG_HWMEM_PROFILER\n \t[I(HWHINT_NR_EVENTS)]\t\t\t= \"hwhint_nr_events\",\n \t[I(HWHINT_KERNEL)]\t\t\t= \"hwhint_kernel\",\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-29",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed a concern about the performance of the pghot subsystem in certain scenarios, specifically the NUMAB2 benchmark and the hwhints source. The author provided results from microbenchmarks that show the patched case performing similarly to the base case in most scenarios, but noted that the pgtscan source needs tuning. The author did not commit to making any changes or fixes, instead presenting the results as evidence of the subsystem's performance.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "author presents results without committing to change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Here is the first set of results from a microbenchmark:\n\nTest system details\n-------------------\n3 node AMD Zen5 system with 2 regular NUMA nodes (0, 1) and a CXL node (2)\n\n$ numactl -H\navailable: 3 nodes (0-2)\nnode 0 cpus: 0-95,192-287\nnode 0 size: 128460 MB\nnode 1 cpus: 96-191,288-383\nnode 1 size: 128893 MB\nnode 2 cpus:\nnode 2 size: 257993 MB\nnode distances:\nnode   0   1   2\n  0:  10  32  50\n  1:  32  10  60\n  2:  255  255  10\n\nHotness sources\n---------------\nNUMAB0 - Without NUMA Balancing in base case and with no source enabled\n         in the patched case. No migrations occur.\nNUMAB2 - Existing hot page promotion for the base case and\n         use of hint faults as source in the patched case.\npgtscan - Klruscand (MGLRU based PTE A bit scanning) source\nhwhints - IBS as source\n\nPghot by default promotes after two accesses but for NUMAB2 source,\npromotion is done after one access to match the base behaviour.\n(/sys/kernel/debug/pghot/freq_threshold=1)\n\n==============================================================\nScenario 1 - Enough memory in toptier and hence only promotion\n==============================================================\nMulti-threaded application with 64 threads that access memory at 4K granularity\nrepetitively and randomly. The number of accesses per thread and the randomness\npattern for each thread are fixed beforehand. The accesses are divided into\nstores and loads in the ratio of 50:50.\n\nBenchmark threads run on Node 0, while memory is initially provisioned on\nCXL node 2 before the accesses start.\n\nRepetitive accesses results in lowertier pages becoming hot and kmigrated\ndetecting and migrating them. The benchmark score is the time taken to finish\nthe accesses in microseconds. The sooner it finishes the better it is. All the\nnumbers shown below are average of 3 runs.\n\nDefault mode - Time taken (microseconds, lower is better)\n---------------------------------------------------------\nSource          Base            Pghot\n---------------------------------------------------------\nNUMAB0          117,069,417     115,802,776\nNUMAB2          102,918,471     103,378,828\npgtscan         NA              110,203,286\nhwhints         NA              92,880,388\n---------------------------------------------------------\n\nDefault mode - Pages migrated (pgpromote_success)\n---------------------------------------------------------\nSource          Base            Pghot\n---------------------------------------------------------\nNUMAB0          0               0\nNUMAB2          2097147         2097131\npgtscan         NA              2097130\nhwhints         NA              1706556\n---------------------------------------------------------\n\nPrecision mode - Time taken (microseconds, lower is better)\n-----------------------------------------------------------\nSource          Base            Pghot\n-----------------------------------------------------------\nNUMAB0          117,069,417     115,078,527\nNUMAB2          102,918,471     101,742,985\npgtscan         NA              110,024,513     NA\nhwhints         NA              101,163,603     NA\n-----------------------------------------------------------\n\nPrecision mode - Pages migrated (pgpromote_success)\n---------------------------------------------------\nSource          Base            Pghot\n---------------------------------------------------\nNUMAB0          0               0\nNUMAB2          2097147         2097144\npgtscan         NA              2097129\nhwhints         NA              1144304\n---------------------------------------------------\n\n- The NUMAB2 benchmark numbers and pgpromote_success numbers more\n  or less match in base and patched case.\n- Though the pgtscan case promotes all possible pages, the\n  benchmark number suffers. This source needs tuning.\n- Hwhints case is able to provide benchmark numbers similar to\n  base NUMAB2 even with less number of migrations.\n- With both default and precision modes of pghot the benchmark\n  behaves more or less similarly.\n\n==============================================================\nScenario 2 - Toptier memory overcommited, promotion + demotion\n==============================================================\nSingle threaded application that allocates memory on both DRAM and CXL nodes\nusing mmap(MAP_POPULATE). Every 1G region of allocated memory on CXL node is\naccessed at 4K granularity randomly and repetitively to build up the notion\nof hotness in the 1GB region that is under access. This should drive promotion.\nFor promotion to work successfully, the DRAM memory that has been provisioned\n(and not being accessed) should be demoted first. There is enough free memory\nin the CXL node to for demotions.\n\nIn summary, this benchmark creates a memory pressure on DRAM node and does\nCXL memory accesses to drive both demotion and promotion.\n\nThe number of accesses are fixed and hence, the quicker the accessed pages\nget promoted to DRAM, the sooner the benchmark is expected to finish.\nAll the numbers shown below are average of 3 runs.\n\nDRAM-node                       = 1\nCXL-node                        = 2\nInitial DRAM alloc ratio        = 75%\nAllocation-size                 = 171798691840\nInitial DRAM Alloc-size         = 128849018880\nInitial CXL Alloc-size          = 42949672960\nHot-region-size                 = 1073741824\nNr-regions                      = 160\nNr-regions DRAM                 = 120 (provisioned but not accessed)\nNr-hot-regions CXL              = 40\nAccess pattern                  = random\nAccess granularity              = 4096\nDelay b/n accesses              = 0\nLoad/store ratio                = 50l50s\nTHP used                        = no\nNr accesses                     = 42949672960\nNr repetitions                  = 1024\n\nDefault mode - Time taken (microseconds, lower is better)\n------------------------------------------------------\nSource          Base            Pghot\n------------------------------------------------------\nNUMAB0          63,809,267      60,794,786\nNUMAB2          67,541,601      62,376,991\npgtscan         NA              67,902,126\nhwhints         NA              59,872,525\n------------------------------------------------------\n\nDefault mode - Pages migrated (pgpromote_success)\n-------------------------------------------------\nSource          Base            Pghot\n-------------------------------------------------\nNUMAB0          0               0\nNUMAB2          179635          932693  (High R2R variation in base)\npgtscan         NA              27487\nhwhints         NA              274\n---------------------------------------\n\nPrecision mode - Time taken (microseconds, lower is better)\n------------------------------------------------------\nSource          Base            Pghot\n------------------------------------------------------\nNUMAB0          63,809,267      64,553,914\nNUMAB2          67,541,601      62,148,082\npgtscan         NA              65,073,396\nhwhints         NA              59,958,655\n------------------------------------------------------\n\nPrecision mode - Pages migrated (pgpromote_success)\n---------------------------------------------------\nSource          Base            Pghot\n---------------------------------------------------\nNUMAB0          0               0\nNUMAB2          179635          988360  (High R2R variaion in base)\npgtscan         NA              21418   (High R2R variation in patched)\nhwhints         NA              174     (High R2R variation in patched)\n---------------------------------------------------\n\n- The base case itself doesn't show any improvement in benchmark numbers due\n  to hot page promotion. The same pattern is seen in pghot case with all\n  the sources except hwhints. The benchmark itself may need tuning so that\n  promotion helps.\n- There is a high run to run variation in the number of pages promoted in\n  base case.\n- Most promotion attempts in base case fail because the NUMA hint fault\n  latency is found to exceed the threshold value (default threshold\n  is 1000ms) in majority of the promotion attempts.\n- Unlike base NUMAB2 where the hint fault latency is the difference between the\n  PTE update time (during scanning) and the access time (hint fault), pghot uses\n  a single latency threshold (4000ms in pghot-default and 5000ms in\n  pghot-precise) for two purposes.\n        1. If the time difference between successive accesses are within the\n           threshold, the page is marked as hot.\n        2. Later when kmigrated picks up the page for migration, it will migrate\n           only if the difference between the current time and the time when the\n          page was marked hot is with the threshold.\n  Because of the above difference in behaviour, more number of pages get\n  qualified for promotion compared to base NUMAB2.",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed a concern about the performance benefits of hot page promotion, specifically in scenarios where toptier memory is overcommitted. They provided benchmark results from redis-memtier and explained that while there's no clear benefit seen in one scenario, the number of pages promoted remains similar across all cases.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear benefit",
                "similar performance"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Numbers from redis-memtier benchmark:\n\nTest system details\n-------------------\n3 node AMD Zen5 system with 2 regular NUMA nodes (0, 1) and a CXL node (2)\n\n$ numactl -H\navailable: 3 nodes (0-2)\nnode 0 cpus: 0-95,192-287\nnode 0 size: 128460 MB\nnode 1 cpus: 96-191,288-383\nnode 1 size: 128893 MB\nnode 2 cpus:\nnode 2 size: 257993 MB\nnode distances:\nnode   0   1   2\n  0:  10  32  50\n  1:  32  10  60\n  2:  255  255  10\n\nHotness sources\n---------------\nNUMAB0 - Without NUMA Balancing in base case and with no source enabled\n         in the patched case. No migrations occur.\nNUMAB2 - Existing hot page promotion for the base case and\n         use of hint faults as source in the patched case.\n\nPghot by default promotes after two accesses but for NUMAB2 source,\npromotion is done after one access to match the base behaviour.\n(/sys/kernel/debug/pghot/freq_threshold=1)\n\n==============================================================\nScenario 1 - Enough memory in toptier and hence only promotion\n==============================================================\nIn the setup phase, 64GB database is provisioned and explicitly moved\nto Node 2 by migrating redis-server's memory to Node 2.\nMemtier is run on Node 1.\n\nParallel distribution, 50% of the keys accessed, each 4 times.\n16        Threads\n100       Connections per thread\n77808     Requests per client\n\n==================================================================================================\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9\nLatency       KB/sec\n--------------------------------------------------------------------------------------------------\nBase, NUMAB0\nTotals     225827.75       226.49746       225.27900       425.98300\n454.65500    513106.09\n--------------------------------------------------------------------------------------------------\nBase, NUMAB2\nTotals     254869.29       205.61759       216.06300       399.35900\n454.65500    579091.74\n--------------------------------------------------------------------------------------------------\npghot-default, NUMAB2\nTotals     264229.35       202.81411       215.03900       393.21500\n446.46300    600358.86\n--------------------------------------------------------------------------------------------------\npghot-precise, NUMAB2\nTotals     261136.17       203.32692       215.03900       391.16700\n446.46300    593330.81\n==================================================================================================\n\npgpromote_success\n==================================\nBase, NUMAB0            0\nBase, NUMAB2            10,435,178\npghot-default, NUMAB2   10,435,031\npghot-precise, NUMAB2   10,435,245\n==================================\n\n- There is a clear benefit of hot page promotion seen. Both\n  base and pghot show similar benefits.\n- The number of pages promoted in both cases are more or less\n  same.\n\n==============================================================\nScenario 2 - Toptier memory overcommited, promotion + demotion\n==============================================================\nIn the setup phase, 192GB database is provisioned. The database occupies\nNode 1 entirely(~128GB) and spills over to Node 2 (~64GB).\nMemtier is run on Node 1.\n\nParallel distribution, 50% of the keys accessed, each 4 times.\n16        Threads\n100       Connections per thread\n233424    Requests per client\n\n==================================================================================================\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9\nLatency       KB/sec\n--------------------------------------------------------------------------------------------------\nBase, NUMAB0\nTotals     246474.55       211.90623       192.51100       370.68700\n448.51100    560235.63\n--------------------------------------------------------------------------------------------------\nBase, NUMAB2\nTotals     232790.88       221.18604       214.01500       419.83900\n509.95100    529132.72\n--------------------------------------------------------------------------------------------------\npghot-default, NUMAB2\nTotals     241615.60       216.12761       210.94300       391.16700\n475.13500    549191.27\n--------------------------------------------------------------------------------------------------\npghot-precise, NUMAB2\nTotals     238557.37       217.57630       207.87100       395.26300\n471.03900    542239.92\n==================================================================================================\n                        pgpromote_success       pgdemote_kswapd\n===============================================================\nBase, NUMAB0            0                       832,494\nBase, NUMAB2            352,075                 720,409\npghot-default, NUMAB2   25,865,321              26,154,984\npghot-precise, NUMAB2   25,525,429              25,838,095\n===============================================================\n\n- No clear benefit is seen with hot page promotion both in base and pghot case.\n- Most promotion attempts in base case fail because the NUMA hint fault latency\n  is found to exceed the threshold value (default threshold of 1000ms) in\n  majority of the promotion attempts.\n- Unlike base NUMAB2 where the hint fault latency is the difference between the\n  PTE update time (during scanning) and the access time (hint fault), pghot uses\n  a single latency threshold (4000ms in pghot-default and 5000ms in\n  pghot-precise) for two purposes.\n        1. If the time difference between successive accesses are within the\n           threshold, the page is marked as hot.\n        2. Later when kmigrated picks up the page for migration, it will migrate\n           only if the difference between the current time and the time when the\n          page was marked hot is with the threshold.\n  Because of the above difference in behaviour, more number of pages get\n  qualified for promotion compared to base NUMAB2.",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author is addressing a concern about the performance of pghot-default mode, specifically why it doesn't show benefits despite achieving similar page promotion numbers as NUMAB2. The author explains that this is because pghot-default promotes to NID=0 by default, which may not be beneficial since processes are running on both Node 0 and Node 1.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "author provides explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Here are Graph500 numbers for the hint fault source:\n\nTest system details\n-------------------\n3 node AMD Zen5 system with 2 regular NUMA nodes (0, 1) and a CXL node (2)\n\n$ numactl -H\navailable: 3 nodes (0-2)\nnode 0 cpus: 0-95,192-287\nnode 0 size: 128460 MB\nnode 1 cpus: 96-191,288-383\nnode 1 size: 128893 MB\nnode 2 cpus:\nnode 2 size: 257993 MB\nnode distances:\nnode   0   1   2\n  0:  10  32  50\n  1:  32  10  60\n  2:  255  255  10\n\nHotness sources\n---------------\nNUMAB0 - Without NUMA Balancing in base case and with no source enabled\n         in the pghot case. No migrations occur.\nNUMAB2 - Existing hot page promotion for the base case and\n         use of hint faults as source in the pghot case.\n\nPghot by default promotes after two accesses but for NUMAB2 source,\npromotion is done after one access to match the base behaviour.\n(/sys/kernel/debug/pghot/freq_threshold=1)\n\nGraph500 details\n----------------\nCommand: mpirun -n 128 --bind-to core --map-by core\ngraph500/src/graph500_reference_bfs 28 16\n\nAfter the graph creation, the processes are stopped and data is migrated\nto CXL node 2 before continuing so that BFS phase starts accessing lower\ntier memory.\n\nTotal memory usage is slightly over 100GB and will fit within Node 0 and 1.\nHence there is no memory pressure to induce demotions.\n\n=====================================================================================\n                        Base            Base            pghot-default\npghot-precise\n                        NUMAB0          NUMAB2          NUMAB2          NUMAB2\n=====================================================================================\nharmonic_mean_TEPS      5.10676e+08     7.56804e+08     5.92473e+08     7.47091e+08\nmean_time               8.41027         5.67508         7.24915         5.74886\nmedian_TEPS             5.11535e+08     7.24252e+08     5.63155e+08     7.71638e+08\nmax_TEPS                5.1785e+08      1.06051e+09     7.88018e+08     1.0504e+09\n\npgpromote_success       0               13557718        13737730        13734469\nnuma_pte_updates        0               26491591        26848847        26726856\nnuma_hint_faults        0               13558077        13882743        13798024\n=====================================================================================\n\n\n- The base case shows a good improvement with NUMAB2(48%) in harmonic_mean_TEPS.\n- The same improvement gets maintained with pghot-precise too (46%).\n- pghot-default mode doesn't show benefit even when achieving similar page promotion\n  numbers. This mode doesn't track accessing NID and by default promotes to NID=0\n  which probably isn't all that beneficial as processes are running on both Node 0\n  and Node 1.",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "Author acknowledged a bug in the folio isolation code, specifically that it should hold a folio reference before calling folio_isolate_lru(), which would prevent VM_BUG_ON_FOLIO() from being triggered; they have already fixed this issue on their GitHub branch and the numbers for Graph500 benchmark are with this fix.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a bug",
                "has already fixed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "We should hold a folio reference before the above call which will isolate the\nfolio from LRU. Otherwise we may hit\n\nVM_BUG_ON_FOLIO(!folio_ref_count(folio), folio)\n\nin folio_isolate_lru().\n\nI hit this only when running Graph500 benchmark and have fixed it in\nthe github at: https://github.com/AMDESE/linux-mm/tree/bharata/pghot-rfcv6-pre\n\nThe numbers that I have posted for micro-benchmarks and redis-memtier are\nwithout this fix while Graph500 numbers are with this fix.\n\nRegards,\nBharata.",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price requested clarification on the meaning of TEPS, a benchmark used in the patchset, and asked whether higher values are better or worse.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "lack of understanding",
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Can you contextualize TEPS?  Higher better? Higher worse? etc.\nUnfamiliar with this benchmark.\n\n~Gregory",
              "reply_to": "Bharata Rao",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer suggested selecting a random or round-robin node from the upper tier to improve promotion accuracy, as the current implementation lacks access-nid data.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Lacking access-nid data, maybe it's better to select a random (or\nround-robin) node in the upper tier?  That would at least approach 1/N\naccuracy in promotion for most access patterns.\n\n~Gregory",
              "reply_to": "Bharata Rao",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price noted that zone-device folios should not be tracked by pghot, suggesting a fast-out for these cases to avoid unnecessary tracking and potentially generalizing this check to private-node memory as well.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Also relevant note from other work I'm doing, we may want a fast-out for\nzone-device folios here.  We should not bother tracking those at all.\n\n(this may also become relevant for private-node memory as well, but I\nmay try to generalize zone_device & private-node checks as the\nconditions are very similar).\n\n~Gregory",
              "reply_to": "Bharata Rao",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author acknowledged that zone device folios are not tracked by pghot, explaining they are discarded by the pghot_record_access() function.\n\nAuthor acknowledges that the patch needs further work, but does not provide a clear plan for addressing the issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledgment",
                "clarification",
                "acknowledges need for further work",
                "does not provide a clear plan"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, zone device folios aren't not tracked by pghot. They get discarded\nby pghot_record_access() itself.\n\n---\n\nGood.\n\nRegards,\nBharata.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "Author responded to feedback about the Graph500 benchmark results, stating that higher TEPS values are better.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "In the Graph500 benchmark, higher TEPS (Traversed Edges Per Second) values are\nbetter.\n\nRegards,\nBharata.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author is addressing concerns about performance issues in the hot page tracking and promotion subsystem, specifically in scenarios where demotion is present. They provided benchmark results showing that both default and precise modes of pghot exhibit similar behavior to the base case when promotion and demotion are enabled (NUMAB2 case). The author notes that while overall benchmark numbers remain consistent, there is a spike in PTE updates and hint faults during some runs, but they have yet to understand the exact reason for this. They do not indicate any plans to fix these issues or revise the patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "acknowledges performance issue"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "These numbers are from scenario where demotion is present:\n\n=============================================\nOver-committed scenario, promotion + demotion\n=============================================\nCommand: mpirun -n 128 --bind-to core --map-by core\n/home/bharata/benchmarks/graph500/src/graph500_reference_bfs 30 16\n\nThe scale factor of 30 results in around 400GB of memory being\nprovisioned resulting in the data spilling over to CXL node.\nNo explicit migration of data is done in this case unlike the\nprevious case.\n\n=====================================================================================\n                        Base            Base            pghot-default\npghot-precise\n                        NUMAB0          NUMAB2          NUMAB2          NUMAB2\n=====================================================================================\nharmonic_mean_TEPS      9.28713e+08     7.90431e+08     7.32193e+08     7.81051e+08\nmean_time               18.4984         21.7346         23.4634         21.9956\nmedian_TEPS             9.25707e+08     7.86684e+08     7.27053e+08     7.82823e+08\nmax_TEPS                9.57632e+08     8.4758e+08      8.22172e+08     7.9889e+08\n\npgpromote_success       0               22846743        22807167        25994988\npgpromote_candidate     0               24628924        29436044        27029173\npgpromote_candidate_nrl 0               140921          220             38387\npgdemote_kswapd         0               41523110        45121134        50042594\nnuma_pte_updates        0               121904763       71503891        68779424\nnuma_hint_faults        0               81708126        29583391        27176332\n=====================================================================================\n\n- In the base case, the benchmark suffers when promotion and demotion are\n  enabled (NUMAB2 case).\n- Same behaviour is seen with both modes of pghot.\n- Though the overall benchmark numbers remain more or less same with base and\n  pghot NUMAB2 cases, the number of pte updates and hint faults are seen\n  to spike up during some runs. Yet to understand the exact reason for this.",
              "reply_to": "",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price requested that the patch series include a base-commit hash, which would facilitate automated testing and backporting.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "In the future can you add a \n\nbase-commit:\n\nfor the series?  Make's it easier to automate pulling it in for testing\nand backports etc.\n\n~Gregory",
              "reply_to": "Bharata Rao",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "Author acknowledged a concern about the patch series' application and agreed to make changes, providing a link to the latest GitHub branch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "agreed to make changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Good suggestion, will do thanks.\n\nBTW this series applies on f0b9d8eb98df.\nLatest github branch:\nhttps://github.com/AMDESE/linux-mm/tree/bharata/pghot-rfcv6-pre\n\nRegards,\nBharata.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-16",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author is addressing concerns about the performance of their hot page tracking and promotion subsystem in comparison to existing NUMA balancing-based promotion. They provided benchmark results from NAS Parallel Benchmark (NPB) showing that their subsystem, especially in precision mode, can match or even outperform the base case numbers. The author attributes the poor performance in the default mode to promotion being limited to a single NID.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledges concerns",
                "provides evidence"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Here are some numbers from NAS Parallel Benchmark (NPB) with BT application:\n\nTest system details\n-------------------\n3 node AMD Zen5 system with 2 regular NUMA nodes (0, 1) and a CXL node (2)\n\n$ numactl -H\navailable: 3 nodes (0-2)\nnode 0 cpus: 0-95,192-287\nnode 0 size: 128460 MB\nnode 1 cpus: 96-191,288-383\nnode 1 size: 128893 MB\nnode 2 cpus:\nnode 2 size: 257993 MB\nnode distances:\nnode   0   1   2\n  0:  10  32  50\n  1:  32  10  60\n  2:  255  255  10\n\nHotness sources\n---------------\nNUMAB0 - Without NUMA Balancing in base case and with no source enabled\n         in the pghot case. No migrations occur.\nNUMAB2 - Existing hot page promotion for the base case and\n         use of hint faults as source in the pghot case.\n         Both promotion and demotion are enabled in this case.\n\nPghot by default promotes after two accesses but for NUMAB2 source,\npromotion is done after one access to match the base behaviour.\n(/sys/kernel/debug/pghot/freq_threshold=1)\n\n\nNAS-BT details\n--------------\nCommand: mpirun -np 16 /usr/bin/numactl --cpunodebind=0,1\nNPB3.4.4/NPB3.4-MPI/bin/bt.F.x\n\nWhile class D uses around 24G of memory (which is too less to show the benefit\nof promition), class E results in around 368G of memory which overflows my\ntoptier. Hence I wanted something in between these classes. So I have  modified\nclass F to the problem size of 768 which results in around 160GB of memory.\n\nAfter the memory consumption stabilizes, all the rank PIDs are paused and\ntheir memory is moved to CXL node using migratepages command. This simulates\nthe situation of memory residing on lower tier node and access by BT processes\nleading to promotion.\n\nTime in seconds - Lower is better\nMop/s total - Higher is better\n=====================================================================================\n                        Base            Base            pghot-default\npghot-precise\n                        NUMAB0          NUMAB2          NUMAB2          NUMAB2\n=====================================================================================\nTime in seconds         7349.86         4422.50         6219.71         4113.56\nMop/s total             53247.66        88493.630       62923.030       95139.810\n\npgpromote_success       0               42181834        248503390       41955718\npgpromote_candidate     0               0               577086192       0\npgpromote_candidate_nrl 0               42181834        29410329        41956171\npgdemote_kswapd         0               0               216489010       0\nnuma_pte_updates        0               42252749        607470975       42037882\nnuma_hint_faults        0               42183772        606540729       41968150\n=====================================================================================\n\n- In the base case, the benchmark numbers improve significantly due to hot page\n  promotion.\n- Though the benchmark runs for hundreds of minutes, the pages get promoted\n  within the first few mins.\n- pghot-precise is able to match the base case numbers.\n- The benchmark suffers in pghot-default case due to promotion being limited\n  to the default NID (0) only. This leads to excessive PTE updates, hint faults,\n  demotion and promotion churn.",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer suggested modifying pghot-default to randomly select a top-tier node instead of always using NID(0), which would improve its correctness and allow for comparison with NUMAB2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Wow, this really seems to justify the extra memory usage.\n\nIs it possible for you to change pghot-default to move the page to a\nrandom (or round-robin) node on the top tier instead of NID(0) by default?\n\nAt least then pghot-default would be correct 1/N % of the time (in theory).\nI'd be curious to see how close it gets to NUMAB2 with that.\n\n~Gregory",
              "reply_to": "Bharata Rao",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author is addressing a concern about the performance of pghot-default compared to pghot-precise and base NUMAB2 case, providing data that shows numbers catch up after some time.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "For pghot-default, with target_nid alternating between the available\ntoptier nodes 0 and 1, the numbers catch up with pghot-precise and base\nNUMAB2 case as seen below:\n================================\nTime in seconds         4337.98\nMop/s total             90217.86\n\npgpromote_success       42170085\npgpromote_candidate     0\npgpromote_candidate_nrl 42171963\npgdemote_kswapd         0\nnuma_pte_updates        42338538\nnuma_hint_faults        42185662\n================================\n\nRegards,\nBharata.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer expressed skepticism about the patch's performance, suggesting that its success might be due to chance rather than actual optimization.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "skepticism",
                "questioning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Fascinating! Thank you for the quick follow up.\n\nI wonder if this was a lucky run, it almost seems *too* perfect.\n\n~Gregory",
              "reply_to": "Bharata Rao",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bharata Rao (author)",
              "summary": "The author addressed a concern about the accuracy of pgpromote_success metrics, providing additional numbers from another run and explaining why the round-robin behavior affects the nrl metric for node 0 only.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It consistently performs that way. Here are the numbers from another\nrun:\n\n================================\nTime in seconds         4329.22\nMop/s total             90400.27\n\npgpromote_success       41967282\npgpromote_candidate     0\npgpromote_candidate_nrl 41968339\npgdemote_kswapd         0\nnuma_pte_updates        42253854\nnuma_hint_faults        42019449\n================================\n\ngrep -E \"pgpromote|pgdemote\" /sys/devices/system/node/node0/vmstat\npgpromote_success 20996597\npgpromote_candidate 0\npgpromote_candidate_nrl 41968339 (*)\npgdemote_kswapd 0\npgdemote_direct 0\npgdemote_khugepaged 0\npgdemote_proactive 0\n\ngrep -E \"pgpromote|pgdemote\" /sys/devices/system/node/node1/vmstat\npgpromote_success 20970685\npgpromote_candidate 0\npgpromote_candidate_nrl 0\npgdemote_kswapd 0\npgdemote_direct 0\npgdemote_khugepaged 0\npgdemote_proactive 0\n\n\n(*) The round-robin b/n nodes 0 and 1 happens after this metric is\nattributed to the original default target_nid. Hence nrl metric\ngets populated for node 0 only.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC][RFC PATCH v4 00/27] Private Memory Nodes (w/ Compressed RAM)",
          "message_id": "aZ3BEn_73Rk8Fn7L@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3BEn_73Rk8Fn7L@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T15:17:43Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch introduces a new PCI driver called cxl_compression, which is part of a larger series that adds support for Private Memory Nodes (PMNs) and Compressed RAM. The PMN feature allows for the creation of isolated NUMA nodes, while Compressed RAM enables the compression of memory pages to reduce memory usage. The cxl_compression driver is designed to work with CXL (Compute Express Link) devices, which are used to manage compressed memory. This patch adds the necessary infrastructure to support the cxl_compression driver and allows for the creation of PMNs and Compressed RAM services.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the node_private infrastructure being mutually exclusive with N_MEMORY, explained that it's intended for memory nodes not meant for general consumption, and confirmed that Zonelist construction changes are deferred to a subsequent commit.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY nodes are intended to contain general System RAM. Today, some\ndevice drivers hotplug their memory (marked Specific Purpose or Reserved)\nto get access to mm/ services, but don't intend it for general consumption.\n\nCreate N_MEMORY_PRIVATE for memory nodes whose memory is not intended for\ngeneral consumption. This state is mutually exclusive with N_MEMORY.\n\nAdd the node_private infrastructure for N_MEMORY_PRIVATE nodes:\n\n  - struct node_private: Per-node container stored in NODE_DATA(nid),\n    holding driver callbacks (ops), owner, and refcount.\n\n  - struct node_private_ops: Initial structure with void *reserved\n    placeholder and flags field.  Callbacks will be added by subsequent\n    commits as each consumer is wired up.\n\n  - folio_is_private_node() / page_is_private_node(): check if a\n    folio/page resides on a private node.\n\n  - folio_node_private_ops() / node_private_flags(): retrieve the ops\n    vtable or flags for a folio's node.\n\n  - Registration API: node_private_register()/unregister() for drivers\n    to register callbacks for private nodes. Only one driver callback\n    can be registered per node - attempting to register different ops\n    returns -EBUSY.\n\n  - sysfs attribute exposing N_MEMORY_PRIVATE node state.\n\nZonelist construction changes for private nodes are deferred to a\nsubsequent commit.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 197 ++++++++++++++++++++++++++++++++\n include/linux/mmzone.h       |   4 +\n include/linux/node_private.h | 210 +++++++++++++++++++++++++++++++++++\n include/linux/nodemask.h     |   1 +\n 4 files changed, 412 insertions(+)\n create mode 100644 include/linux/node_private.h\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 00cf4532f121..646dc48a23b5 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -22,6 +22,7 @@\n #include <linux/swap.h>\n #include <linux/slab.h>\n #include <linux/memblock.h>\n+#include <linux/node_private.h>\n \n static const struct bus_type node_subsys = {\n \t.name = \"node\",\n@@ -861,6 +862,198 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n \t\t\t   (void *)&nid, register_mem_block_under_node_hotplug);\n \treturn;\n }\n+\n+static DEFINE_MUTEX(node_private_lock);\n+static bool node_private_initialized;\n+\n+/**\n+ * node_private_register - Register a private node\n+ * @nid: Node identifier\n+ * @np: The node_private structure (driver-allocated, driver-owned)\n+ *\n+ * Register a driver for a private node. Only one driver can register\n+ * per node. If another driver has already registered (with different np),\n+ * -EBUSY is returned. Re-registration with the same np is allowed.\n+ *\n+ * The driver owns the node_private memory and must ensure it remains valid\n+ * until refcount reaches 0 after node_private_unregister().\n+ *\n+ * Returns 0 on success, negative errno on failure.\n+ */\n+int node_private_register(int nid, struct node_private *np)\n+{\n+\tstruct node_private *existing;\n+\tpg_data_t *pgdat;\n+\tint ret = 0;\n+\n+\tif (!np || !node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tif (!node_private_initialized)\n+\t\treturn -ENODEV;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\t/* N_MEMORY_PRIVATE and N_MEMORY are mutually exclusive */\n+\tif (node_state(nid, N_MEMORY)) {\n+\t\tret = -EBUSY;\n+\t\tgoto out;\n+\t}\n+\n+\tpgdat = NODE_DATA(nid);\n+\texisting = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t\t     lockdep_is_held(&node_private_lock));\n+\n+\t/* Only one source my register this node */\n+\tif (existing) {\n+\t\tif (existing != np) {\n+\t\t\tret = -EBUSY;\n+\t\t\tgoto out;\n+\t\t}\n+\t\tgoto out;\n+\t}\n+\n+\trefcount_set(&np->refcount, 1);\n+\tinit_completion(&np->released);\n+\n+\trcu_assign_pointer(pgdat->node_private, np);\n+\tpgdat->private = true;\n+\n+out:\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_register);\n+\n+/**\n+ * node_private_set_ops - Set service callbacks on a registered private node\n+ * @nid: Node identifier\n+ * @ops: Service callbacks and flags (driver-owned, must outlive registration)\n+ *\n+ * Validates flag dependencies and sets the ops on the node's node_private.\n+ * The node must already be registered via node_private_register().\n+ *\n+ * Returns 0 on success, -EINVAL for invalid flag combinations,\n+ * -ENODEV if no node_private is registered on @nid.\n+ */\n+int node_private_set_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!ops)\n+\t\treturn -EINVAL;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse\n+\t\tnp->ops = ops;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_set_ops);\n+\n+/**\n+ * node_private_clear_ops - Clear service callbacks from a private node\n+ * @nid: Node identifier\n+ * @ops: Expected ops pointer (must match current ops)\n+ *\n+ * Clears the ops only if @ops matches the currently registered ops,\n+ * preventing one service from accidentally clearing another's callbacks.\n+ *\n+ * Returns 0 on success, -ENODEV if no node_private is registered,\n+ * -EINVAL if @ops does not match.\n+ */\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse if (np->ops != ops)\n+\t\tret = -EINVAL;\n+\telse\n+\t\tnp->ops = NULL;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_clear_ops);\n+\n+/**\n+ * node_private_unregister - Unregister a private node\n+ * @nid: Node identifier\n+ *\n+ * Unregister the driver from a private node. Only succeeds if all memory\n+ * has been offlined and the node is no longer N_MEMORY_PRIVATE.\n+ * When successful, drops the refcount to 0 indicating the driver can\n+ * free its context.\n+ *\n+ * N_MEMORY_PRIVATE state is cleared by offline_pages() when the last\n+ * memory is offlined, not by this function.\n+ *\n+ * Return: 0 if unregistered, -EBUSY if N_MEMORY_PRIVATE is still set\n+ * (other memory blocks remain on this node).\n+ */\n+int node_private_unregister(int nid)\n+{\n+\tstruct node_private *np;\n+\tpg_data_t *pgdat;\n+\n+\tif (!node_possible(nid))\n+\t\treturn 0;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\tpgdat = NODE_DATA(nid);\n+\tnp = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Only unregister if all memory is offline and N_MEMORY_PRIVATE is\n+\t * cleared. N_MEMORY_PRIVATE is cleared by offline_pages() when the\n+\t * last memory block is offlined.\n+\t */\n+\tif (node_state(nid, N_MEMORY_PRIVATE)) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn -EBUSY;\n+\t}\n+\n+\trcu_assign_pointer(pgdat->node_private, NULL);\n+\tpgdat->private = false;\n+\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\n+\tsynchronize_rcu();\n+\n+\tif (!refcount_dec_and_test(&np->refcount))\n+\t\twait_for_completion(&np->released);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(node_private_unregister);\n+\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n /**\n@@ -959,6 +1152,7 @@ static struct node_attr node_state_attr[] = {\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n \t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\n \t\t\t\t\t   N_GENERIC_INITIATOR),\n@@ -972,6 +1166,7 @@ static struct attribute *node_state_attrs[] = {\n \t&node_state_attr[N_HIGH_MEMORY].attr.attr,\n #endif\n \t&node_state_attr[N_MEMORY].attr.attr,\n+\t&node_state_attr[N_MEMORY_PRIVATE].attr.attr,\n \t&node_state_attr[N_CPU].attr.attr,\n \t&node_state_attr[N_GENERIC_INITIATOR].attr.attr,\n \tNULL\n@@ -1007,5 +1202,7 @@ void __init node_dev_init(void)\n \t\t\tpanic(\"%s() failed to add node: %d\\n\", __func__, ret);\n \t}\n \n+\tnode_private_initialized = true;\n+\n \tregister_memory_blocks_under_nodes();\n }\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex b01cb1e49896..992eb1c5a2c6 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -25,6 +25,8 @@\n #include <linux/zswap.h>\n #include <asm/page.h>\n \n+struct node_private;\n+\n /* Free memory management - zoned buddy allocator.  */\n #ifndef CONFIG_ARCH_FORCE_MAX_ORDER\n #define MAX_PAGE_ORDER 10\n@@ -1514,6 +1516,8 @@ typedef struct pglist_data {\n \tatomic_long_t\t\tvm_stat[NR_VM_NODE_STAT_ITEMS];\n #ifdef CONFIG_NUMA\n \tstruct memory_tier __rcu *memtier;\n+\tstruct node_private __rcu *node_private;\n+\tbool private;\n #endif\n #ifdef CONFIG_MEMORY_FAILURE\n \tstruct memory_failure_stats mf_stats;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nnew file mode 100644\nindex 000000000000..6a70ec39d569\n--- /dev/null\n+++ b/include/linux/node_private.h\n@@ -0,0 +1,210 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_NODE_PRIVATE_H\n+#define _LINUX_NODE_PRIVATE_H\n+\n+#include <linux/completion.h>\n+#include <linux/mm.h>\n+#include <linux/nodemask.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+\n+struct page;\n+struct vm_area_struct;\n+struct vm_fault;\n+\n+/**\n+ * struct node_private_ops - Callbacks for private node services\n+ *\n+ * Services register these callbacks to intercept MM operations that affect\n+ * their private nodes.\n+ *\n+ * Flag bits control which MM subsystems may operate on folios on this node.\n+ *\n+ * The pgdat->node_private pointer is RCU-protected.  Callbacks fall into\n+ * three categories based on their calling context:\n+ *\n+ * Folio-referenced callbacks (RCU released before callback):\n+ *   The caller holds a reference to a folio on the private node, which\n+ *   pins the node's memory online and prevents node_private teardown.\n+ *\n+ * Refcounted callbacks (RCU released before callback):\n+ *   The caller has no folio on the private node (e.g., folios are on a\n+ *   source node being migrated TO this node).  A temporary refcount is\n+ *   taken on node_private under rcu_read_lock to keep the structure (and\n+ *   the service module) alive across the callback.  node_private_unregister\n+ *   waits for all temporary references to drain before returning.\n+ *\n+ * Non-folio callbacks (rcu_read_lock held during callback):\n+ *   No folio reference exists, so rcu_read_lock is held across the\n+ *   callback to prevent node_private from being freed.\n+ *   These callbacks MUST NOT sleep.\n+ *\n+ * @flags: Operation exclusion flags (NP_OPS_* constants).\n+ *\n+ */\n+struct node_private_ops {\n+\tunsigned long flags;\n+};\n+\n+/**\n+ * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n+ *\n+ * This structure is allocated by the driver and passed to node_private_register().\n+ * The driver owns the memory and must ensure it remains valid until after\n+ * node_private_unregister() returns with the reference count dropped to 0.\n+ *\n+ * @owner: Opaque driver identifier\n+ * @refcount: Reference count (1 = registered; temporary refs for non-folio\n+ *\t\tcallbacks that may sleep; 0 = fully released)\n+ * @released: Signaled when refcount drops to 0; unregister waits on this\n+ * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ */\n+struct node_private {\n+\tvoid *owner;\n+\trefcount_t refcount;\n+\tstruct completion released;\n+\tconst struct node_private_ops *ops;\n+};\n+\n+#ifdef CONFIG_NUMA\n+\n+#include <linux/mmzone.h>\n+\n+/**\n+ * folio_is_private_node - Check if folio is on an N_MEMORY_PRIVATE node\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio resides on a private node.\n+ */\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn node_state(folio_nid(folio), N_MEMORY_PRIVATE);\n+}\n+\n+/**\n+ * page_is_private_node - Check if page is on an N_MEMORY_PRIVATE node\n+ * @page: The page to check\n+ *\n+ * Returns true if the page resides on a private node.\n+ */\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\tconst struct node_private_ops *ops;\n+\tstruct node_private *np;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(folio_nid(folio))->node_private);\n+\tops = np ? np->ops : NULL;\n+\trcu_read_unlock();\n+\n+\treturn ops;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\tstruct node_private *np;\n+\tunsigned long flags;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tflags = (np && np->ops) ? np->ops->flags : 0;\n+\trcu_read_unlock();\n+\n+\treturn flags;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn node_private_flags(folio_nid(f)) & flag;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn node_private_flags(nid) & flag;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn node_private_flags(zone_to_nid(z)) & flag;\n+}\n+\n+#else /* !CONFIG_NUMA */\n+\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn false;\n+}\n+\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn false;\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\treturn NULL;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+#endif /* CONFIG_NUMA */\n+\n+#if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\n+\n+int node_private_register(int nid, struct node_private *np);\n+int node_private_unregister(int nid);\n+int node_private_set_ops(int nid, const struct node_private_ops *ops);\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n+\n+#else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n+\n+static inline int node_private_register(int nid, struct node_private *np)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_unregister(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline int node_private_set_ops(int nid,\n+\t\t\t\t       const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_clear_ops(int nid,\n+\t\t\t\t\t const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+#endif /* CONFIG_NUMA && CONFIG_MEMORY_HOTPLUG */\n+\n+#endif /* _LINUX_NODE_PRIVATE_H */\ndiff --git a/include/linux/nodemask.h b/include/linux/nodemask.h\nindex bd38648c998d..c9bcfd5a9a06 100644\n--- a/include/linux/nodemask.h\n+++ b/include/linux/nodemask.h\n@@ -391,6 +391,7 @@ enum node_states {\n \tN_HIGH_MEMORY = N_NORMAL_MEMORY,\n #endif\n \tN_MEMORY,\t\t/* The node has memory(regular, high, movable) */\n+\tN_MEMORY_PRIVATE,\t/* The node's memory is private */\n \tN_CPU,\t\t/* The node has one or more cpus */\n \tN_GENERIC_INITIATOR,\t/* The node has one or more Generic Initiators */\n \tNR_NODE_STATES\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about general allocations landing on private nodes without explicit permission by introducing __GFP_PRIVATE and updating cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE nodes unless this flag is set.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY_PRIVATE nodes hold device-managed memory that should not be\nused for general allocations. Without a gating mechanism, any allocation\ncould land on a private node if it appears in the task's mems_allowed.\n\nIntroduce __GFP_PRIVATE that explicitly opts in to allocation from\nN_MEMORY_PRIVATE nodes.\n\nAdd the GFP_PRIVATE compound mask (__GFP_PRIVATE | __GFP_THISNODE)\nfor callers that explicitly target private nodes to help prevent\nfallback allocations from DRAM.\n\nUpdate cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE\nnodes unless __GFP_PRIVATE is set.\n\nIn interrupt context, only N_MEMORY nodes are valid.\n\nUpdate cpuset_handle_hotplug() to include N_MEMORY_PRIVATE nodes in\nthe effective mems set, allowing cgroup-level control over private\nnode access.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/gfp_types.h      | 15 +++++++++++++--\n include/trace/events/mmflags.h |  4 ++--\n kernel/cgroup/cpuset.c         | 32 ++++++++++++++++++++++++++++----\n 3 files changed, 43 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/gfp_types.h b/include/linux/gfp_types.h\nindex 3de43b12209e..ac375f9a0fc2 100644\n--- a/include/linux/gfp_types.h\n+++ b/include/linux/gfp_types.h\n@@ -33,7 +33,7 @@ enum {\n \t___GFP_IO_BIT,\n \t___GFP_FS_BIT,\n \t___GFP_ZERO_BIT,\n-\t___GFP_UNUSED_BIT,\t/* 0x200u unused */\n+\t___GFP_PRIVATE_BIT,\n \t___GFP_DIRECT_RECLAIM_BIT,\n \t___GFP_KSWAPD_RECLAIM_BIT,\n \t___GFP_WRITE_BIT,\n@@ -69,7 +69,7 @@ enum {\n #define ___GFP_IO\t\tBIT(___GFP_IO_BIT)\n #define ___GFP_FS\t\tBIT(___GFP_FS_BIT)\n #define ___GFP_ZERO\t\tBIT(___GFP_ZERO_BIT)\n-/* 0x200u unused */\n+#define ___GFP_PRIVATE\t\tBIT(___GFP_PRIVATE_BIT)\n #define ___GFP_DIRECT_RECLAIM\tBIT(___GFP_DIRECT_RECLAIM_BIT)\n #define ___GFP_KSWAPD_RECLAIM\tBIT(___GFP_KSWAPD_RECLAIM_BIT)\n #define ___GFP_WRITE\t\tBIT(___GFP_WRITE_BIT)\n@@ -139,6 +139,11 @@ enum {\n  * %__GFP_ACCOUNT causes the allocation to be accounted to kmemcg.\n  *\n  * %__GFP_NO_OBJ_EXT causes slab allocation to have no object extension.\n+ *\n+ * %__GFP_PRIVATE allows allocation from N_MEMORY_PRIVATE nodes (e.g., compressed\n+ * memory, accelerator memory). Without this flag, allocations are restricted\n+ * to N_MEMORY nodes only. Used by migration/demotion paths when explicitly\n+ * targeting private nodes.\n  */\n #define __GFP_RECLAIMABLE ((__force gfp_t)___GFP_RECLAIMABLE)\n #define __GFP_WRITE\t((__force gfp_t)___GFP_WRITE)\n@@ -146,6 +151,7 @@ enum {\n #define __GFP_THISNODE\t((__force gfp_t)___GFP_THISNODE)\n #define __GFP_ACCOUNT\t((__force gfp_t)___GFP_ACCOUNT)\n #define __GFP_NO_OBJ_EXT   ((__force gfp_t)___GFP_NO_OBJ_EXT)\n+#define __GFP_PRIVATE\t((__force gfp_t)___GFP_PRIVATE)\n \n /**\n  * DOC: Watermark modifiers\n@@ -367,6 +373,10 @@ enum {\n  * available and will not wake kswapd/kcompactd on failure. The _LIGHT\n  * version does not attempt reclaim/compaction at all and is by default used\n  * in page fault path, while the non-light is used by khugepaged.\n+ *\n+ * %GFP_PRIVATE adds %__GFP_THISNODE by default to prevent any fallback\n+ * allocations to other nodes, given that the caller was already attempting\n+ * to access driver-managed memory explicitly.\n  */\n #define GFP_ATOMIC\t(__GFP_HIGH|__GFP_KSWAPD_RECLAIM)\n #define GFP_KERNEL\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n@@ -382,5 +392,6 @@ enum {\n #define GFP_TRANSHUGE_LIGHT\t((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \\\n \t\t\t __GFP_NOMEMALLOC | __GFP_NOWARN) & ~__GFP_RECLAIM)\n #define GFP_TRANSHUGE\t(GFP_TRANSHUGE_LIGHT | __GFP_DIRECT_RECLAIM)\n+#define GFP_PRIVATE\t(__GFP_PRIVATE | __GFP_THISNODE)\n \n #endif /* __LINUX_GFP_TYPES_H */\ndiff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h\nindex a6e5a44c9b42..f042cd848451 100644\n--- a/include/trace/events/mmflags.h\n+++ b/include/trace/events/mmflags.h\n@@ -37,7 +37,8 @@\n \tTRACE_GFP_EM(HARDWALL)\t\t\t\\\n \tTRACE_GFP_EM(THISNODE)\t\t\t\\\n \tTRACE_GFP_EM(ACCOUNT)\t\t\t\\\n-\tTRACE_GFP_EM(ZEROTAGS)\n+\tTRACE_GFP_EM(ZEROTAGS)\t\t\t\\\n+\tTRACE_GFP_EM(PRIVATE)\n \n #ifdef CONFIG_KASAN_HW_TAGS\n # define TRACE_GFP_FLAGS_KASAN\t\t\t\\\n@@ -73,7 +74,6 @@\n TRACE_GFP_FLAGS\n \n /* Just in case these are ever used */\n-TRACE_DEFINE_ENUM(___GFP_UNUSED_BIT);\n TRACE_DEFINE_ENUM(___GFP_LAST_BIT);\n \n #define gfpflag_string(flag) {(__force unsigned long)flag, #flag}\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 473aa9261e16..1a597f0c7c6c 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -444,21 +444,32 @@ static void guarantee_active_cpus(struct task_struct *tsk,\n }\n \n /*\n- * Return in *pmask the portion of a cpusets's mems_allowed that\n+ * Return in *pmask the portion of a cpuset's mems_allowed that\n  * are online, with memory.  If none are online with memory, walk\n  * up the cpuset hierarchy until we find one that does have some\n  * online mems.  The top cpuset always has some mems online.\n  *\n  * One way or another, we guarantee to return some non-empty subset\n- * of node_states[N_MEMORY].\n+ * of node_states[N_MEMORY].  N_MEMORY_PRIVATE nodes from the\n+ * original cpuset are preserved, but only N_MEMORY nodes are\n+ * pulled from ancestors.\n  *\n  * Call with callback_lock or cpuset_mutex held.\n  */\n static void guarantee_online_mems(struct cpuset *cs, nodemask_t *pmask)\n {\n+\tstruct cpuset *orig_cs = cs;\n+\tint nid;\n+\n \twhile (!nodes_intersects(cs->effective_mems, node_states[N_MEMORY]))\n \t\tcs = parent_cs(cs);\n+\n \tnodes_and(*pmask, cs->effective_mems, node_states[N_MEMORY]);\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_isset(nid, orig_cs->effective_mems))\n+\t\t\tnode_set(nid, *pmask);\n+\t}\n }\n \n /**\n@@ -4075,7 +4086,9 @@ static void cpuset_handle_hotplug(void)\n \n \t/* fetch the available cpus/mems and find out which changed how */\n \tcpumask_copy(&new_cpus, cpu_active_mask);\n-\tnew_mems = node_states[N_MEMORY];\n+\n+\t/* Include N_MEMORY_PRIVATE so cpuset controls access the same way */\n+\tnodes_or(new_mems, node_states[N_MEMORY], node_states[N_MEMORY_PRIVATE]);\n \n \t/*\n \t * If subpartitions_cpus is populated, it is likely that the check\n@@ -4488,10 +4501,21 @@ bool cpuset_node_allowed(struct cgroup *cgroup, int nid)\n  * __alloc_pages() will include all nodes.  If the slab allocator\n  * is passed an offline node, it will fall back to the local node.\n  * See kmem_cache_alloc_node().\n+ *\n+ *\n+ * Private nodes aren't eligible for these allocations, so skip them.\n+ * guarantee_online_mems guaranttes at least one N_MEMORY node is set.\n  */\n static int cpuset_spread_node(int *rotor)\n {\n-\treturn *rotor = next_node_in(*rotor, current->mems_allowed);\n+\tint node;\n+\n+\tdo {\n+\t\tnode = next_node_in(*rotor, current->mems_allowed);\n+\t\t*rotor = node;\n+\t} while (node_state(node, N_MEMORY_PRIVATE));\n+\n+\treturn node;\n }\n \n /**\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern that the open-coded cpuset filtering in mm/ does not account for N_MEMORY_PRIVATE nodes on systems without cpusets, which can lead to private-node zones leaking into allocation paths. The author added a new helper function numa_zone_allowed() and replaced the open-coded patterns with it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Various locations in mm/ open-code cpuset filtering with:\n\n  cpusets_enabled() && ALLOC_CPUSET && !__cpuset_zone_allowed()\n\nThis pattern does not account for N_MEMORY_PRIVATE nodes on systems\nwithout cpusets, so private-node zones can leak into allocation\npaths that should only see general-purpose memory.\n\nAdd numa_zone_allowed() which consolidates zone filtering. It checks\ncpuset membership when cpusets are enabled, and otherwise gates\nN_MEMORY_PRIVATE zones behind __GFP_PRIVATE globally.\n\nReplace the open-coded patterns in mm/ with the new helper.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/compaction.c |  6 ++----\n mm/hugetlb.c    |  2 +-\n mm/internal.h   |  7 +++++++\n mm/page_alloc.c | 31 ++++++++++++++++++++-----------\n mm/slub.c       |  3 ++-\n 5 files changed, 32 insertions(+), 17 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..6a65145b03d8 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -2829,10 +2829,8 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tenum compact_result status;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 51273baec9e5..f2b914ab5910 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -1353,7 +1353,7 @@ static struct folio *dequeue_hugetlb_folio_nodemask(struct hstate *h, gfp_t gfp_\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {\n \t\tstruct folio *folio;\n \n-\t\tif (!cpuset_zone_allowed(zone, gfp_mask))\n+\t\tif (!numa_zone_alloc_allowed(ALLOC_CPUSET, zone, gfp_mask))\n \t\t\tcontinue;\n \t\t/*\n \t\t * no need to ask again on the same node. Pool is node rather than\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 23ee14790227..97023748e6a9 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t      gfp_t gfp_mask);\n #else\n #define node_reclaim_mode 0\n \n@@ -1218,6 +1220,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t\t     gfp_t gfp_mask)\n+{\n+\treturn true;\n+}\n #endif\n \n static inline bool node_reclaim_enabled(void)\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 2facee0805da..47f2619d3840 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3690,6 +3690,21 @@ static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n \treturn node_distance(zone_to_nid(local_zone), zone_to_nid(zone)) <=\n \t\t\t\tnode_reclaim_distance;\n }\n+\n+/* Returns true if allocation from this zone is permitted */\n+bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone, gfp_t gfp_mask)\n+{\n+\t/* Gate N_MEMORY_PRIVATE zones behind __GFP_PRIVATE */\n+\tif (!(gfp_mask & __GFP_PRIVATE) &&\n+\t    node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn false;\n+\n+\t/* If cpusets is being used, check mems_allowed */\n+\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET))\n+\t\treturn cpuset_zone_allowed(zone, gfp_mask);\n+\n+\treturn true;\n+}\n #else\t/* CONFIG_NUMA */\n static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n {\n@@ -3781,10 +3796,8 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\tstruct page *page;\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \t\t/*\n \t\t * When allocating a page cache page for writing, we\n \t\t * want to get it from a node that is within its dirty\n@@ -4585,10 +4598,8 @@ should_reclaim_retry(gfp_t gfp_mask, unsigned order,\n \t\tunsigned long min_wmark = min_wmark_pages(zone);\n \t\tbool wmark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tavailable = reclaimable = zone_reclaimable_pages(zone);\n \t\tavailable += zone_page_state_snapshot(zone, NR_FREE_PAGES);\n@@ -5084,10 +5095,8 @@ unsigned long alloc_pages_bulk_noprof(gfp_t gfp, int preferred_nid,\n \tfor_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&\n-\t\t    !__cpuset_zone_allowed(zone, gfp)) {\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp))\n \t\t\tcontinue;\n-\t\t}\n \n \t\tif (nr_online_nodes > 1 && zone != zonelist_zone(ac.preferred_zoneref) &&\n \t\t    zone_to_nid(zone) != zonelist_node_idx(ac.preferred_zoneref)) {\ndiff --git a/mm/slub.c b/mm/slub.c\nindex 861592ac5425..e4bd6ede81d1 100644\n--- a/mm/slub.c\n+++ b/mm/slub.c\n@@ -3595,7 +3595,8 @@ static struct slab *get_any_partial(struct kmem_cache *s,\n \n \t\t\tn = get_node(s, zone_to_nid(zone));\n \n-\t\t\tif (n && cpuset_zone_allowed(zone, pc->flags) &&\n+\t\t\tif (n && numa_zone_alloc_allowed(ALLOC_CPUSET, zone,\n+\t\t\t\t\t\t   pc->flags) &&\n \t\t\t\t\tn->nr_partial > s->min_partial) {\n \t\t\t\tslab = get_partial_node(s, n, pc);\n \t\t\t\tif (slab) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about N_MEMORY fallback lists including N_MEMORY_PRIVATE nodes, explaining that this would allow allocations from private nodes in some scenarios and cause unnecessary iterations over ineligible nodes. The author provided a patch to fix the issue by adding private nodes as fallbacks for kernel allocations on behalf of the private node.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY fallback lists should not include N_MEMORY_PRIVATE nodes, at\nworst this would allow allocation from them in some scenarios, and at\nbest it causes iterations over nodes that aren't eligible.\n\nPrivate node primary fallback lists do include N_MEMORY nodes so\nkernel/slab allocations made on behalf of the private node can\nfall back to DRAM when __GFP_PRIVATE is not set.\n\nThe nofallback list contains only the node's own zones, restricting\n__GFP_THISNODE allocations to the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/page_alloc.c | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 47f2619d3840..5a1b35421d78 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5683,6 +5683,26 @@ static void build_zonelists(pg_data_t *pgdat)\n \tlocal_node = pgdat->node_id;\n \tprev_node = local_node;\n \n+\t/*\n+\t * Private nodes need N_MEMORY nodes as fallback for kernel allocations\n+\t * (e.g., slab objects allocated on behalf of this node).\n+\t */\n+\tif (node_state(local_node, N_MEMORY_PRIVATE)) {\n+\t\tnode_order[nr_nodes++] = local_node;\n+\t\tnode_set(local_node, used_mask);\n+\n+\t\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0)\n+\t\t\tnode_order[nr_nodes++] = node;\n+\n+\t\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n+\t\tbuild_thisnode_zonelists(pgdat);\n+\t\tpr_info(\"Fallback order for Node %d (private):\", local_node);\n+\t\tfor (node = 0; node < nr_nodes; node++)\n+\t\t\tpr_cont(\" %d\", node_order[node]);\n+\t\tpr_cont(\"\\n\");\n+\t\treturn;\n+\t}\n+\n \tmemset(node_order, 0, sizeof(node_order));\n \twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author addressed a concern about the need for a unified predicate to exclude both N_MEMORY_PRIVATE and ZONE_DEVICE folios from MM operations, and provided a patch that adds the folio_is_private_managed() function to achieve this.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix is needed",
                "provided a patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Multiple mm/ subsystems already skip operations for ZONE_DEVICE folios,\nand N_MEMORY_PRIVATE folios share the checkpoints for ZONE_DEVICE pages.\n\nAdd folio_is_private_managed() as a unified predicate that returns true\nfor folios on N_MEMORY_PRIVATE nodes or in ZONE_DEVICE.\n\nThis predicate replaces folio_is_zone_device at skip sites where both\nfolio types should be excluded from an MM operation.\n\nAt some locations, explicit zone_device vs private_node checks are more\nappropriate when the operations between the two fundamentally differ.\n\nThe !CONFIG_NUMA stubs fall through to folio_is_zone_device() only,\npreserving existing behavior when NUMA is disabled.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 6a70ec39d569..7687a4cf990c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -92,6 +92,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio) || folio_is_private_node(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n@@ -146,6 +156,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn false;\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about mlocking private node folios, explaining that they should not be locked and citing the existing folio_is_zone_device check as sufficient to handle this case. The author extended this check to include private nodes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "addressed_concern",
                "explained_reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nmlocked.  The existing folio_is_zone_device check is already correctly\nplaced to handle this - simply extend it for private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/mlock.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/mlock.c b/mm/mlock.c\nindex 2f699c3497a5..c56159253e45 100644\n--- a/mm/mlock.c\n+++ b/mm/mlock.c\n@@ -25,6 +25,7 @@\n #include <linux/memcontrol.h>\n #include <linux/mm_inline.h>\n #include <linux/secretmem.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -366,7 +367,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (is_huge_zero_pmd(*pmd))\n \t\t\tgoto out;\n \t\tfolio = pmd_folio(*pmd);\n-\t\tif (folio_is_zone_device(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)))\n \t\t\tgoto out;\n \t\tif (vma->vm_flags & VM_LOCKED)\n \t\t\tmlock_folio(folio);\n@@ -386,7 +387,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (!pte_present(ptent))\n \t\t\tcontinue;\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\tstep = folio_mlock_step(folio, pte, addr, end);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author acknowledged a concern that madvise and pageout operations should not interfere with device driver-managed private node folios, agreed to extend the zone_device check to cover private nodes, and made corresponding changes to mm/madvise.c.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "agreed to make changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nsubjectto madvise cold/pageout/free operations that would interfere\nwith the driver's memory management.\n\nExtend the existing zone_device check to cover private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/madvise.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/madvise.c b/mm/madvise.c\nindex b617b1be0f53..3aac105e840b 100644\n--- a/mm/madvise.c\n+++ b/mm/madvise.c\n@@ -32,6 +32,7 @@\n #include <linux/leafops.h>\n #include <linux/shmem_fs.h>\n #include <linux/mmu_notifier.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlb.h>\n \n@@ -475,7 +476,7 @@ static int madvise_cold_or_pageout_pte_range(pmd_t *pmd,\n \t\t\tcontinue;\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n@@ -704,7 +705,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,\n \t\t}\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private node folios participating in KSM merging by default, agreeing that this can interfere with driver operations. The author extended existing checks to exclude private node folios from KSM merging.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not participate in KSM merging by default.\nThe driver manages the memory lifecycle and KSM's page sharing can\ninterfere with driver operations.\n\nExtend the existing zone_device checks in get_mergeable_page and\nksm_next_page_pmd_entry to cover private node folios as well.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/ksm.c | 9 ++++++---\n 1 file changed, 6 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/ksm.c b/mm/ksm.c\nindex 2d89a7c8b4eb..c48e95a6fff9 100644\n--- a/mm/ksm.c\n+++ b/mm/ksm.c\n@@ -40,6 +40,7 @@\n #include <linux/oom.h>\n #include <linux/numa.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include \"internal.h\"\n@@ -808,7 +809,7 @@ static struct page *get_mergeable_page(struct ksm_rmap_item *rmap_item)\n \n \tfolio = folio_walk_start(&fw, vma, addr, 0);\n \tif (folio) {\n-\t\tif (!folio_is_zone_device(folio) &&\n+\t\tif (!folio_is_private_managed(folio) &&\n \t\t    folio_test_anon(folio)) {\n \t\t\tfolio_get(folio);\n \t\t\tpage = fw.page;\n@@ -2521,7 +2522,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\t\tgoto not_found_unlock;\n \t\t\tfolio = page_folio(page);\n \n-\t\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t\t    !folio_test_anon(folio))\n \t\t\t\tgoto not_found_unlock;\n \n \t\t\tpage += ((addr & (PMD_SIZE - 1)) >> PAGE_SHIFT);\n@@ -2545,7 +2547,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\tcontinue;\n \t\tfolio = page_folio(page);\n \n-\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t    !folio_test_anon(folio))\n \t\t\tcontinue;\n \t\tgoto found_unlock;\n \t}\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about collapse operations on private nodes potentially promoting pages to local nodes and inverting LRU order, agreeing that handling this like zone_device is the best approach for now.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "A collapse operation allocates a new large folio and migrates the\nsmaller folios into it.  This is an issue for private nodes:\n\n  1. The private node service may not support migration\n  2. Collapse may promotes pages from the private node to a local node,\n     which may result in an LRU inversion that defeats memory tiering.\n\nHandle this just like zone_device for now.\n\nIt may be possible to support this later for some private node services\nthat report explicit support for collapse (and migration).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/khugepaged.c | 7 ++++---\n 1 file changed, 4 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/khugepaged.c b/mm/khugepaged.c\nindex 97d1b2824386..36f6bc5da53c 100644\n--- a/mm/khugepaged.c\n+++ b/mm/khugepaged.c\n@@ -21,6 +21,7 @@\n #include <linux/shmem_fs.h>\n #include <linux/dax.h>\n #include <linux/ksm.h>\n+#include <linux/node_private.h>\n #include <linux/pgalloc.h>\n \n #include <asm/tlb.h>\n@@ -571,7 +572,7 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,\n \t\t\tgoto out;\n \t\t}\n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out;\n \t\t}\n@@ -1323,7 +1324,7 @@ static int hpage_collapse_scan_pmd(struct mm_struct *mm,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out_unmap;\n \t\t}\n@@ -1575,7 +1576,7 @@ int collapse_pte_mapped_thp(struct mm_struct *mm, unsigned long addr,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, ptent);\n-\t\tif (WARN_ON_ONCE(page && is_zone_device_page(page)))\n+\t\tif (WARN_ON_ONCE(page && page_is_private_managed(page)))\n \t\t\tpage = NULL;\n \t\t/*\n \t\t * Note that uprobe, debugger, or MAP_PRIVATE may change the\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about cleanup when a folio's refcount drops to zero, explaining that the service may need to perform cleanup before the page returns to the buddy allocator. They added a new function `folio_managed_on_free()` to wrap both zone_device and private node semantics for this operation. The function will return true if the folio is fully handled (zone_device) or false if the callback ran but the folio should continue through the normal free path (private_node).",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "added new function to address concern"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a folio's refcount drops to zero, the service may need to perform\ncleanup before the page returns to the buddy allocator (e.g. zeroing\npages to scrub stale compressed data / release compression ratio).\n\nAdd folio_managed_on_free() to wrap both zone_device and private node\nsemantics for this operation since they are the same.\n\nOne difference between zone_device and private node folios:\n  - private nodes may choose to either take a reference and return true\n    (\"handled\"), or return false to return it back to the buddy.\n\n  - zone_device returns the page to the buddy (always returns true)\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 30 ++++++++++++++++++++++++++++++\n mm/swap.c                    | 21 ++++++++++-----------\n 3 files changed, 46 insertions(+), 11 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7687a4cf990c..09ea7c4cb13c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -39,10 +39,16 @@ struct vm_fault;\n  *   callback to prevent node_private from being freed.\n  *   These callbacks MUST NOT sleep.\n  *\n+ * @free_folio: Called when a folio refcount drops to 0\n+ *   [folio-referenced callback]\n+ *   Returns: true if handled (skip return to buddy)\n+ *            false if no op (return to buddy)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n+\tbool (*free_folio)(struct folio *folio);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 97023748e6a9..658da41cdb8e 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1412,6 +1412,36 @@ int numa_migrate_check(struct folio *folio, struct vm_fault *vmf,\n void free_zone_device_folio(struct folio *folio);\n int migrate_device_coherent_folio(struct folio *folio);\n \n+/**\n+ * folio_managed_on_free - Notify managed-memory service that folio\n+ *                         refcount reached zero.\n+ * @folio: the folio being freed\n+ *\n+ * Returns true if the folio is fully handled (zone_device -- caller\n+ * must return immediately).  Returns false if the callback ran but\n+ * the folio should continue through the normal free path\n+ * (private_node -- pages go back to buddy).\n+ *\n+ * Returns false for normal folios (no-op).\n+ */\n+static inline bool folio_managed_on_free(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio)) {\n+\t\tfree_zone_device_folio(folio);\n+\t\treturn true;\n+\t}\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->free_folio) {\n+\t\t\tif (ops->free_folio(folio))\n+\t\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/swap.c b/mm/swap.c\nindex 2260dcd2775e..dca306e1ae6d 100644\n--- a/mm/swap.c\n+++ b/mm/swap.c\n@@ -37,6 +37,7 @@\n #include <linux/page_idle.h>\n #include <linux/local_lock.h>\n #include <linux/buffer_head.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -96,10 +97,9 @@ static void page_cache_release(struct folio *folio)\n \n void __folio_put(struct folio *folio)\n {\n-\tif (unlikely(folio_is_zone_device(folio))) {\n-\t\tfree_zone_device_folio(folio);\n-\t\treturn;\n-\t}\n+\tif (unlikely(folio_is_private_managed(folio)))\n+\t\tif (folio_managed_on_free(folio))\n+\t\t\treturn;\n \n \tif (folio_test_hugetlb(folio)) {\n \t\tfree_huge_folio(folio);\n@@ -961,19 +961,18 @@ void folios_put_refs(struct folio_batch *folios, unsigned int *refs)\n \t\tif (is_huge_zero_folio(folio))\n \t\t\tcontinue;\n \n-\t\tif (folio_is_zone_device(folio)) {\n+\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n+\t\t\tcontinue;\n+\n+\t\tif (unlikely(folio_is_private_managed(folio))) {\n \t\t\tif (lruvec) {\n \t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n \t\t\t\tlruvec = NULL;\n \t\t\t}\n-\t\t\tif (folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\t\tfree_zone_device_folio(folio);\n-\t\t\tcontinue;\n+\t\t\tif (folio_managed_on_free(folio))\n+\t\t\t\tcontinue;\n \t\t}\n \n-\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\tcontinue;\n-\n \t\t/* hugetlb has its own memcg */\n \t\tif (folio_test_hugetlb(folio)) {\n \t\t\tif (lruvec) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about notifying private node services when a THP folio is split by adding an optional callback to the ops struct and updating the folio_split path in huge_memory.c. The author confirmed that this change will be included in the next version of the patch series.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "confirmed a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private node services may need to update internal metadata when\na THP folio is split.  ZONE_DEVICE already has a split callback via\npgmap->ops; private nodes can provide the same capability.\n\nJust like zone_device, some private node services may want to know\nabout a folio being split.  Add this optional callback to the ops\nstruct and add a wrapper for zone_device and private node callback\ndispatch to be consolidated.\n\nWire this into __folio_split() where the zone_device check was made.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 33 +++++++++++++++++++++++++++++++++\n mm/huge_memory.c             |  6 ++++--\n 2 files changed, 37 insertions(+), 2 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 09ea7c4cb13c..f9dd2d25c8a5 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -3,6 +3,7 @@\n #define _LINUX_NODE_PRIVATE_H\n \n #include <linux/completion.h>\n+#include <linux/memremap.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -44,11 +45,19 @@ struct vm_fault;\n  *   Returns: true if handled (skip return to buddy)\n  *            false if no op (return to buddy)\n  *\n+ * @folio_split: Notification that a folio on this private node is being split.\n+ *    [folio-referenced callback]\n+ *     Called from the folio split path via folio_managed_split_cb().\n+ *     @folio is the original folio; @new_folio is the newly created folio,\n+ *     or NULL when called for the final (original) folio after all sub-folios\n+ *     have been split off.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n+\tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n \tunsigned long flags;\n };\n \n@@ -150,6 +159,24 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn node_private_flags(zone_to_nid(z)) & flag;\n }\n \n+static inline void node_private_split_cb(struct folio *folio,\n+\t\t\t\t\t struct folio *new_folio)\n+{\n+\tconst struct node_private_ops *ops = folio_node_private_ops(folio);\n+\n+\tif (ops && ops->folio_split)\n+\t\tops->folio_split(folio, new_folio);\n+}\n+\n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+\telse if (folio_is_private_node(original_folio))\n+\t\tnode_private_split_cb(original_folio, new_folio);\n+}\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -198,6 +225,12 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn false;\n }\n \n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+}\n #endif /* CONFIG_NUMA */\n \n #if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 40cf59301c21..2ecae494291a 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -24,6 +24,7 @@\n #include <linux/freezer.h>\n #include <linux/mman.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/debugfs.h>\n #include <linux/migrate.h>\n@@ -3850,7 +3851,7 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \n \t\t\tnext = folio_next(new_folio);\n \n-\t\t\tzone_device_private_split_cb(folio, new_folio);\n+\t\t\tfolio_managed_split_cb(folio, new_folio);\n \n \t\t\tfolio_ref_unfreeze(new_folio,\n \t\t\t\t\t   folio_cache_ref_count(new_folio) + 1);\n@@ -3889,7 +3890,8 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \t\t\tfolio_put_refs(new_folio, nr_pages);\n \t\t}\n \n-\t\tzone_device_private_split_cb(folio, NULL);\n+\t\tfolio_managed_split_cb(folio, NULL);\n+\n \t\t/*\n \t\t * Unfreeze @folio only after all page cache entries, which\n \t\t * used to point to it, have been updated with new folios.\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about user-driven migration to private nodes, explaining that ZONE_DEVICE always rejects it but private nodes should be able to opt in. They added the NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper to dispatch migration requests, allowing migrate_pages syscall to target private nodes.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "added new functionality",
                "acknowledged reviewer feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services may want to support user-driven migration\n(migrate_pages syscall, mbind) to allow data movement between regular\nand private nodes.\n\nZONE_DEVICE always rejects user migration, but private nodes should\nbe able to opt in.\n\nAdd NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper that\ndispatches migration requests.  Private nodes can either set the flag\nand provide a custom migrate_to callback for driver-managed migration.\n\nIn migrate_to_node(), allows GFP_PRIVATE when the destination node\nsupports NP_OPS_MIGRATION, enabling migrate_pages syscall to target\nprivate nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |   4 ++\n include/linux/migrate.h      |  10 +++\n include/linux/node_private.h | 122 +++++++++++++++++++++++++++++++++++\n mm/damon/paddr.c             |   3 +\n mm/internal.h                |  24 +++++++\n mm/mempolicy.c               |  10 +--\n mm/migrate.c                 |  49 ++++++++++----\n mm/rmap.c                    |   4 +-\n 8 files changed, 206 insertions(+), 20 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 646dc48a23b5..e587f5781135 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -949,6 +949,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \tif (!node_possible(nid))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MIGRATION) &&\n+\t    (!ops->migrate_to || !ops->folio_migrate))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 26ca00c325d9..7b2da3875ff2 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -71,6 +71,9 @@ void folio_migrate_flags(struct folio *newfolio, struct folio *folio);\n int folio_migrate_mapping(struct address_space *mapping,\n \t\tstruct folio *newfolio, struct folio *folio, int extra_count);\n int set_movable_ops(const struct movable_operations *ops, enum pagetype type);\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason);\n \n #else\n \n@@ -96,6 +99,13 @@ static inline int set_movable_ops(const struct movable_operations *ops, enum pag\n {\n \treturn -ENOSYS;\n }\n+static inline int migrate_folios_to_node(struct list_head *folios,\n+\t\t\t\t\t\t  int nid,\n+\t\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t\t  enum migrate_reason reason)\n+{\n+\treturn -ENOSYS;\n+}\n \n #endif /* CONFIG_MIGRATION */\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex f9dd2d25c8a5..0c5be1ee6e60 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -4,6 +4,7 @@\n \n #include <linux/completion.h>\n #include <linux/memremap.h>\n+#include <linux/migrate_mode.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -52,15 +53,40 @@ struct vm_fault;\n  *     or NULL when called for the final (original) folio after all sub-folios\n  *     have been split off.\n  *\n+ * @migrate_to: Migrate folios TO this node.\n+ *\t[refcounted callback]\n+ *\tReturns: 0 on full success, >0 = number of folios that failed to\n+ *\t\t migrate, <0 = error.  Matches migrate_pages() semantics.\n+ *\t\t @nr_succeeded is set to the number of successfully migrated\n+ *\t\t folios (may be NULL if caller doesn't need it).\n+ *\n+ * @folio_migrate: Post-migration notification that a folio on this private node\n+ *    changed physical location (on the same node or a different node).\n+ *    [folio-referenced callback]\n+ *     Called from migrate_folio_move() after data has been copied but before\n+ *     migration entries are replaced with real PTEs.  Both @src and @dst are\n+ *     locked.  Faults block in migration_entry_wait() until\n+ *     remove_migration_ptes() runs, so the service can safely update\n+ *     PFN-based metadata (compression tables, device page tables, DMA\n+ *     mappings, etc.) before any access through the page tables.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n \tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n+\tint (*migrate_to)(struct list_head *folios, int nid,\n+\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t  unsigned int *nr_succeeded);\n+\tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tunsigned long flags;\n };\n \n+/* Allow user/kernel migration; requires migrate_to and folio_migrate */\n+#define NP_OPS_MIGRATION\t\tBIT(0)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\n@@ -177,6 +203,81 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n \t\tnode_private_split_cb(original_folio, new_folio);\n }\n \n+#ifdef CONFIG_MEMORY_HOTPLUG\n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn -ENOENT;\n+\treturn node_private_has_flag(folio_nid(folio), NP_OPS_MIGRATION) ?\n+\t       folio_nid(folio) : -ENOENT;\n+}\n+\n+/**\n+ * folio_managed_allows_migrate - Check if a managed folio supports migration\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio can be migrated.  For zone_device folios, only\n+ * device_private and device_coherent support migration.  For private node\n+ * folios, migration requires NP_OPS_MIGRATION.  Normal folios always\n+ * return true.\n+ */\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\tif (folio_is_private_node(folio))\n+\t\treturn folio_private_flags(folio, NP_OPS_MIGRATION);\n+\treturn true;\n+}\n+\n+/**\n+ * node_private_migrate_to - Attempt service-specific migration to a private node\n+ * @folios: list of folios to migrate (may sleep)\n+ * @nid: target node\n+ * @mode: migration mode (MIGRATE_ASYNC, MIGRATE_SYNC, etc.)\n+ * @reason: migration reason (MR_DEMOTION, MR_SYSCALL, etc.)\n+ * @nr_succeeded: optional output for number of successfully migrated folios\n+ *\n+ * If @nid is an N_MEMORY_PRIVATE node with a migrate_to callback,\n+ * invokes the callback and returns the result with migrate_pages()\n+ * semantics (0 = full success, >0 = failure count, <0 = error).\n+ * Returns -ENODEV if the node is not private or the service is being\n+ * torn down.\n+ *\n+ * The source folios are on other nodes, so they do not pin the target\n+ * node's node_private.  A temporary refcount is taken under rcu_read_lock\n+ * to keep node_private (and the service module) alive across the callback.\n+ */\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\tint (*fn)(struct list_head *, int, enum migrate_mode,\n+\t\t  enum migrate_reason, unsigned int *);\n+\tstruct node_private *np;\n+\tint ret;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (!np || !np->ops || !np->ops->migrate_to ||\n+\t    !refcount_inc_not_zero(&np->refcount)) {\n+\t\trcu_read_unlock();\n+\t\treturn -ENODEV;\n+\t}\n+\tfn = np->ops->migrate_to;\n+\trcu_read_unlock();\n+\n+\tret = fn(folios, nid, mode, reason, nr_succeeded);\n+\n+\tif (refcount_dec_and_test(&np->refcount))\n+\t\tcomplete(&np->released);\n+\n+\treturn ret;\n+}\n+#endif /* CONFIG_MEMORY_HOTPLUG */\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -242,6 +343,27 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\treturn -ENOENT;\n+}\n+\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\treturn true;\n+}\n+\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\treturn -ENODEV;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/damon/paddr.c b/mm/damon/paddr.c\nindex 07a8aead439e..532b8e2c62b0 100644\n--- a/mm/damon/paddr.c\n+++ b/mm/damon/paddr.c\n@@ -277,6 +277,9 @@ static unsigned long damon_pa_migrate(struct damon_region *r,\n \t\telse\n \t\t\t*sz_filter_passed += folio_size(folio) / addr_unit;\n \n+\t\tif (!folio_managed_allows_migrate(folio))\n+\t\t\tgoto put_folio;\n+\n \t\tif (!folio_isolate_lru(folio))\n \t\t\tgoto put_folio;\n \t\tlist_add(&folio->lru, &folio_list);\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 658da41cdb8e..6ab4679fe943 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1442,6 +1442,30 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/**\n+ * folio_managed_migrate_notify - Notify service that a folio changed location\n+ * @src: the old folio (about to be freed)\n+ * @dst: the new folio (data already copied, migration entries still in place)\n+ *\n+ * Called from migrate_folio_move() after data has been copied but before\n+ * remove_migration_ptes() installs real PTEs pointing to @dst.  While\n+ * migration entries are in place, faults block in migration_entry_wait(),\n+ * so the service can safely update PFN-based metadata before any access\n+ * through the page tables.  Both @src and @dst are locked.\n+ */\n+static inline void folio_managed_migrate_notify(struct folio *src,\n+\t\t\t\t\t\tstruct folio *dst)\n+{\n+\tconst struct node_private_ops *ops;\n+\n+\tif (!folio_is_private_node(src))\n+\t\treturn;\n+\n+\tops = folio_node_private_ops(src);\n+\tif (ops && ops->folio_migrate)\n+\t\tops->folio_migrate(src, dst);\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 68a98ba57882..2b0f9762d171 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -111,6 +111,7 @@\n #include <linux/mmu_notifier.h>\n #include <linux/printk.h>\n #include <linux/leafops.h>\n+#include <linux/node_private.h>\n #include <linux/gcd.h>\n \n #include <asm/tlbflush.h>\n@@ -1282,11 +1283,6 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tLIST_HEAD(pagelist);\n \tlong nr_failed;\n \tlong err = 0;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = dest,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n \tnodes_clear(nmask);\n \tnode_set(source, nmask);\n@@ -1311,8 +1307,8 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tmmap_read_unlock(mm);\n \n \tif (!list_empty(&pagelist)) {\n-\t\terr = migrate_pages(&pagelist, alloc_migration_target, NULL,\n-\t\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\t\terr = migrate_folios_to_node(&pagelist, dest, MIGRATE_SYNC,\n+\t\t\t\t\t     MR_SYSCALL);\n \t\tif (err)\n \t\t\tputback_movable_pages(&pagelist);\n \t}\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 5169f9717f60..a54d4af04df3 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -43,6 +43,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/memory-tiers.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1387,6 +1388,8 @@ static int migrate_folio_move(free_folio_t put_new_folio, unsigned long private,\n \tif (old_page_state & PAGE_WAS_MLOCKED)\n \t\tlru_add_drain();\n \n+\tfolio_managed_migrate_notify(src, dst);\n+\n \tif (old_page_state & PAGE_WAS_MAPPED)\n \t\tremove_migration_ptes(src, dst, 0);\n \n@@ -2165,6 +2168,7 @@ int migrate_pages(struct list_head *from, new_folio_t get_new_folio,\n \n \treturn rc_gather;\n }\n+EXPORT_SYMBOL_GPL(migrate_pages);\n \n struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n {\n@@ -2204,6 +2208,31 @@ struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n \n \treturn __folio_alloc(gfp_mask, order, nid, mtc->nmask);\n }\n+EXPORT_SYMBOL_GPL(alloc_migration_target);\n+\n+static int __migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, NULL);\n+}\n+\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason)\n+{\n+\tif (node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_private_migrate_to(folios, nid, mode,\n+\t\t\t\t\t       reason, NULL);\n+\treturn __migrate_folios_to_node(folios, nid, mode, reason);\n+}\n \n #ifdef CONFIG_NUMA\n \n@@ -2221,14 +2250,8 @@ static int store_status(int __user *status, int start, int value, int nr)\n static int do_move_pages_to_node(struct list_head *pagelist, int node)\n {\n \tint err;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = node,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n-\terr = migrate_pages(pagelist, alloc_migration_target, NULL,\n-\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\terr = migrate_folios_to_node(pagelist, node, MIGRATE_SYNC, MR_SYSCALL);\n \tif (err)\n \t\tputback_movable_pages(pagelist);\n \treturn err;\n@@ -2240,7 +2263,7 @@ static int __add_folio_for_migration(struct folio *folio, int node,\n \tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\treturn -EFAULT;\n \n-\tif (folio_is_zone_device(folio))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn -ENOENT;\n \n \tif (folio_nid(folio) == node)\n@@ -2364,7 +2387,8 @@ static int do_pages_move(struct mm_struct *mm, nodemask_t task_nodes,\n \t\terr = -ENODEV;\n \t\tif (node < 0 || node >= MAX_NUMNODES)\n \t\t\tgoto out_flush;\n-\t\tif (!node_state(node, N_MEMORY))\n+\t\tif (!node_state(node, N_MEMORY) &&\n+\t\t    !node_state(node, N_MEMORY_PRIVATE))\n \t\t\tgoto out_flush;\n \n \t\terr = -EACCES;\n@@ -2449,8 +2473,8 @@ static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n \t\tif (folio) {\n \t\t\tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\t\t\terr = -EFAULT;\n-\t\t\telse if (folio_is_zone_device(folio))\n-\t\t\t\terr = -ENOENT;\n+\t\t\telse if (unlikely(folio_is_private_managed(folio)))\n+\t\t\t\terr = folio_managed_allows_user_migrate(folio);\n \t\t\telse\n \t\t\t\terr = folio_nid(folio);\n \t\t\tfolio_walk_end(&fw, vma);\n@@ -2660,6 +2684,9 @@ int migrate_misplaced_folio_prepare(struct folio *folio,\n \tint nr_pages = folio_nr_pages(folio);\n \tpg_data_t *pgdat = NODE_DATA(node);\n \n+\tif (!folio_managed_allows_migrate(folio))\n+\t\treturn -ENOENT;\n+\n \tif (folio_is_file_lru(folio)) {\n \t\t/*\n \t\t * Do not migrate file folios that are mapped in multiple\ndiff --git a/mm/rmap.c b/mm/rmap.c\nindex f955f02d570e..805f9ceb82f3 100644\n--- a/mm/rmap.c\n+++ b/mm/rmap.c\n@@ -72,6 +72,7 @@\n #include <linux/backing-dev.h>\n #include <linux/page_idle.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/userfaultfd_k.h>\n #include <linux/mm_inline.h>\n #include <linux/oom.h>\n@@ -2616,8 +2617,7 @@ void try_to_migrate(struct folio *folio, enum ttu_flags flags)\n \t\t\t\t\tTTU_SYNC | TTU_BATCH_FLUSH)))\n \t\treturn;\n \n-\tif (folio_is_zone_device(folio) &&\n-\t    (!folio_is_device_private(folio) && !folio_is_device_coherent(folio)))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn;\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about allowing userland to directly allocate from private nodes via set_mempolicy() and mbind(), but not wanting those nodes as normal allocable system memory in the fallback lists. The author added a flag NP_OPS_MEMPOLICY requiring NP_OPS_MIGRATION, updated sysfs 'has_memory' attribute, and modified mempolicy migration sites to use __GFP_PRIVATE.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private nodes want userland to directly allocate from the node\nvia set_mempolicy() and mbind() - but don't want that node as normal\nallocable system memory in the fallback lists.\n\nAdd NP_OPS_MEMPOLICY flag requiring NP_OPS_MIGRATION (since mbind can\ndrive migrations).  Only allow private nodes in policy nodemasks if\nall private nodes in the mask support NP_OPS_MEMPOLICY. This prevents\n__GFP_PRIVATE from unlocking nodes without NP_OPS_MEMPOLICY support.\n\nAdd __GFP_PRIVATE to mempolicy migration sites so moves to opted-in\nprivate nodes succeed.\n\nUpdate the sysfs \"has_memory\" attribute to include N_MEMORY_PRIVATE\nnodes with NP_OPS_MEMPOLICY set, allowing existing numactl userland\ntools to work without modification.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c            | 22 +++++++++++++-\n include/linux/node_private.h   | 40 +++++++++++++++++++++++++\n include/uapi/linux/mempolicy.h |  1 +\n mm/mempolicy.c                 | 54 ++++++++++++++++++++++++++++++----\n mm/page_alloc.c                |  5 ++++\n 5 files changed, 116 insertions(+), 6 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex e587f5781135..c08b5a948779 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -953,6 +953,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (!ops->migrate_to || !ops->folio_migrate))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\n@@ -1145,6 +1149,21 @@ static ssize_t show_node_state(struct device *dev,\n \t\t\t  nodemask_pr_args(&node_states[na->state]));\n }\n \n+/* has_memory includes N_MEMORY + N_MEMORY_PRIVATE that support mempolicy. */\n+static ssize_t show_has_memory(struct device *dev,\n+\t\t\t       struct device_attribute *attr, char *buf)\n+{\n+\tnodemask_t mask = node_states[N_MEMORY];\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_set(nid, mask);\n+\t}\n+\n+\treturn sysfs_emit(buf, \"%*pbl\\n\", nodemask_pr_args(&mask));\n+}\n+\n #define _NODE_ATTR(name, state) \\\n \t{ __ATTR(name, 0444, show_node_state, NULL), state }\n \n@@ -1155,7 +1174,8 @@ static struct node_attr node_state_attr[] = {\n #ifdef CONFIG_HIGHMEM\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n-\t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY] = { __ATTR(has_memory, 0444, show_has_memory, NULL),\n+\t\t       N_MEMORY },\n \t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 0c5be1ee6e60..e9b58afa366b 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -86,6 +86,8 @@ struct node_private_ops {\n \n /* Allow user/kernel migration; requires migrate_to and folio_migrate */\n #define NP_OPS_MIGRATION\t\tBIT(0)\n+/* Allow mempolicy-directed allocation and mbind migration to this node */\n+#define NP_OPS_MEMPOLICY\t\tBIT(1)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -276,6 +278,34 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \n \treturn ret;\n }\n+\n+static inline bool node_mpol_eligible(int nid)\n+{\n+\tbool ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_state(nid, N_MEMORY);\n+\n+\trcu_read_lock();\n+\tret = node_private_has_flag(nid, NP_OPS_MEMPOLICY);\n+\trcu_read_unlock();\n+\treturn ret;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\tint nid;\n+\tbool eligible = false;\n+\n+\tfor_each_node_mask(nid, *nodes) {\n+\t\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\t\tcontinue;\n+\t\tif (!node_mpol_eligible(nid))\n+\t\t\treturn false;\n+\t\teligible = true;\n+\t}\n+\treturn eligible;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -364,6 +394,16 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \treturn -ENODEV;\n }\n \n+static inline bool node_mpol_eligible(int nid)\n+{\n+\treturn false;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/include/uapi/linux/mempolicy.h b/include/uapi/linux/mempolicy.h\nindex 8fbbe613611a..b606eae983c8 100644\n--- a/include/uapi/linux/mempolicy.h\n+++ b/include/uapi/linux/mempolicy.h\n@@ -64,6 +64,7 @@ enum {\n #define MPOL_F_SHARED  (1 << 0)\t/* identify shared policies */\n #define MPOL_F_MOF\t(1 << 3) /* this policy wants migrate on fault */\n #define MPOL_F_MORON\t(1 << 4) /* Migrate On protnone Reference On Node */\n+#define MPOL_F_PRIVATE\t(1 << 5) /* policy targets private node; use __GFP_PRIVATE */\n \n /*\n  * Enabling zone reclaim means the page allocator will attempt to fulfill\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 2b0f9762d171..8ac014950e88 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -406,8 +406,6 @@ static int mpol_new_preferred(struct mempolicy *pol, const nodemask_t *nodes)\n static int mpol_set_nodemask(struct mempolicy *pol,\n \t\t     const nodemask_t *nodes, struct nodemask_scratch *nsc)\n {\n-\tint ret;\n-\n \t/*\n \t * Default (pol==NULL) resp. local memory policies are not a\n \t * subject of any remapping. They also do not need any special\n@@ -416,9 +414,12 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \tif (!pol || pol->mode == MPOL_LOCAL)\n \t\treturn 0;\n \n-\t/* Check N_MEMORY */\n+\t/* Check N_MEMORY and N_MEMORY_PRIVATE*/\n \tnodes_and(nsc->mask1,\n \t\t  cpuset_current_mems_allowed, node_states[N_MEMORY]);\n+\tnodes_and(nsc->mask2, cpuset_current_mems_allowed,\n+\t\t  node_states[N_MEMORY_PRIVATE]);\n+\tnodes_or(nsc->mask1, nsc->mask1, nsc->mask2);\n \n \tVM_BUG_ON(!nodes);\n \n@@ -432,8 +433,13 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \telse\n \t\tpol->w.cpuset_mems_allowed = cpuset_current_mems_allowed;\n \n-\tret = mpol_ops[pol->mode].create(pol, &nsc->mask2);\n-\treturn ret;\n+\t/* All private nodes in the mask must have NP_OPS_MEMPOLICY. */\n+\tif (nodes_private_mpol_allowed(&nsc->mask2))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse if (nodes_intersects(nsc->mask2, node_states[N_MEMORY_PRIVATE]))\n+\t\treturn -EINVAL;\n+\n+\treturn mpol_ops[pol->mode].create(pol, &nsc->mask2);\n }\n \n /*\n@@ -500,6 +506,7 @@ static void mpol_rebind_default(struct mempolicy *pol, const nodemask_t *nodes)\n static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n {\n \tnodemask_t tmp;\n+\tint nid;\n \n \tif (pol->flags & MPOL_F_STATIC_NODES)\n \t\tnodes_and(tmp, pol->w.user_nodemask, *nodes);\n@@ -514,6 +521,21 @@ static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n \tif (nodes_empty(tmp))\n \t\ttmp = *nodes;\n \n+\t/*\n+\t * Drop private nodes that don't have mempolicy support.\n+\t * cpusets guarantees at least one N_MEMORY node in effective_mems\n+\t * and mems_allowed, so dropping private nodes here is safe.\n+\t */\n+\tfor_each_node_mask(nid, tmp) {\n+\t\tif (node_state(nid, N_MEMORY_PRIVATE) &&\n+\t\t    !node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_clear(nid, tmp);\n+\t}\n+\tif (nodes_intersects(tmp, node_states[N_MEMORY_PRIVATE]))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse\n+\t\tpol->flags &= ~MPOL_F_PRIVATE;\n+\n \tpol->nodes = tmp;\n }\n \n@@ -661,6 +683,9 @@ static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)\n \t}\n \tif (!queue_folio_required(folio, qp))\n \t\treturn;\n+\tif (folio_is_private_node(folio) &&\n+\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\treturn;\n \tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||\n \t    !vma_migratable(walk->vma) ||\n \t    !migrate_folio_add(folio, qp->pagelist, qp->flags))\n@@ -717,6 +742,9 @@ static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n \t\tif (!folio || folio_is_zone_device(folio))\n \t\t\tcontinue;\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\t\tcontinue;\n \t\tif (folio_test_large(folio) && max_nr != 1)\n \t\t\tnr = folio_pte_batch(folio, pte, ptent, max_nr);\n \t\t/*\n@@ -1451,6 +1479,9 @@ static struct folio *alloc_migration_target_by_mpol(struct folio *src,\n \telse\n \t\tgfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL | __GFP_COMP;\n \n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \treturn folio_alloc_mpol(gfp, order, pol, ilx, nid);\n }\n #else\n@@ -2280,6 +2311,15 @@ static nodemask_t *policy_nodemask(gfp_t gfp, struct mempolicy *pol,\n \t\t\tnodemask = &pol->nodes;\n \t\tif (pol->home_node != NUMA_NO_NODE)\n \t\t\t*nid = pol->home_node;\n+\t\telse if ((pol->flags & MPOL_F_PRIVATE) &&\n+\t\t\t !node_isset(*nid, pol->nodes)) {\n+\t\t\t/*\n+\t\t\t * Private nodes are not in N_MEMORY nodes' zonelists.\n+\t\t\t * When the preferred nid (usually numa_node_id()) can't\n+\t\t\t * reach the policy nodes, start from a policy node.\n+\t\t\t */\n+\t\t\t*nid = first_node(pol->nodes);\n+\t\t}\n \t\t/*\n \t\t * __GFP_THISNODE shouldn't even be used with the bind policy\n \t\t * because we might easily break the expectation to stay on the\n@@ -2533,6 +2573,10 @@ struct folio *vma_alloc_folio_noprof(gfp_t gfp, int order, struct vm_area_struct\n \t\tgfp |= __GFP_NOWARN;\n \n \tpol = get_vma_policy(vma, addr, order, &ilx);\n+\n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \tfolio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n \tmpol_cond_put(pol);\n \treturn folio;\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 5a1b35421d78..ec6c1f8e85d8 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3849,8 +3849,13 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\t * if another process has NUMA bindings and is causing\n \t\t * kswapd wakeups on only some nodes. Avoid accidental\n \t\t * \"node_reclaim_mode\"-like behavior in this case.\n+\t\t *\n+\t\t * Nodes without kswapd (some private nodes) are never\n+\t\t * skipped - this causes some mempolicies to silently\n+\t\t * fall back to DRAM even if the node is eligible.\n \t\t */\n \t\tif (skip_kswapd_nodes &&\n+\t\t    zone->zone_pgdat->kswapd &&\n \t\t    !waitqueue_active(&zone->zone_pgdat->kswapd_wait)) {\n \t\t\tskipped_kswapd_nodes = true;\n \t\t\tcontinue;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private nodes being used as demotion targets in the memory-tiers subsystem, agreeing that they should be added to the demotion target mask and implementing backpressure support to allow vmscan to fall back to swap. The author also acknowledged that the current demotion logic induces LRU inversions and suggested re-doing it to allow less fallback and kick kswapd instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The memory-tier subsystem needs to know which private nodes should\nappear as demotion targets.\n\nAdd NP_OPS_DEMOTION (BIT(2)):\n   Node can be added as a demotion target by memory-tiers.\n\nAdd demotion backpressure support so private nodes can reject\nnew demotions cleanly, allowing vmscan to fall back to swap.\n\nIn the demotion path, try demotion to private nodes invididually,\nthen clear private nodes from the demotion target mask until a\nnon-private node is found, then fall back to the remaining mask.\nThis prevents LRU inversion while still allowing forward progress.\n\nThis is the closest match to the current behavior without making\nprivate nodes inaccessible or preventing forward progress. We\nshould probably completely re-do the demotion logic to allow less\nfallback and kick kswapd instead - right now we induce LRU\ninversions by simply falling back to any node in the demotion list.\n\nAdd memory_tier_refresh_demotion() export for services to trigger\nre-evaluation of demotion targets after changing their flags.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory-tiers.h |  9 +++++++\n include/linux/node_private.h | 22 +++++++++++++++++\n mm/internal.h                |  7 ++++++\n mm/memory-tiers.c            | 46 ++++++++++++++++++++++++++++++++----\n mm/page_alloc.c              | 12 +++++++---\n mm/vmscan.c                  | 30 ++++++++++++++++++++++-\n 6 files changed, 117 insertions(+), 9 deletions(-)\n\ndiff --git a/include/linux/memory-tiers.h b/include/linux/memory-tiers.h\nindex 3e1159f6762c..e1476432e359 100644\n--- a/include/linux/memory-tiers.h\n+++ b/include/linux/memory-tiers.h\n@@ -58,6 +58,7 @@ struct memory_dev_type *mt_get_memory_type(int adist);\n int next_demotion_node(int node);\n void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets);\n bool node_is_toptier(int node);\n+void memory_tier_refresh_demotion(void);\n #else\n static inline int next_demotion_node(int node)\n {\n@@ -73,6 +74,10 @@ static inline bool node_is_toptier(int node)\n {\n \treturn true;\n }\n+\n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n #endif\n \n #else\n@@ -106,6 +111,10 @@ static inline bool node_is_toptier(int node)\n \treturn true;\n }\n \n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n+\n static inline int register_mt_adistance_algorithm(struct notifier_block *nb)\n {\n \treturn 0;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e9b58afa366b..e254e36056cd 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -88,6 +88,8 @@ struct node_private_ops {\n #define NP_OPS_MIGRATION\t\tBIT(0)\n /* Allow mempolicy-directed allocation and mbind migration to this node */\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n+/* Node participates as a demotion target in memory-tiers */\n+#define NP_OPS_DEMOTION\t\t\tBIT(2)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -101,12 +103,14 @@ struct node_private_ops {\n  *\t\tcallbacks that may sleep; 0 = fully released)\n  * @released: Signaled when refcount drops to 0; unregister waits on this\n  * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ * @migration_blocked: Service signals migrations should pause\n  */\n struct node_private {\n \tvoid *owner;\n \trefcount_t refcount;\n \tstruct completion released;\n \tconst struct node_private_ops *ops;\n+\tbool migration_blocked;\n };\n \n #ifdef CONFIG_NUMA\n@@ -306,6 +310,19 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \t}\n \treturn eligible;\n }\n+\n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\tstruct node_private *np;\n+\tbool blocked;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tblocked = np && READ_ONCE(np->migration_blocked);\n+\trcu_read_unlock();\n+\n+\treturn blocked;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -404,6 +421,11 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \treturn false;\n }\n \n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 6ab4679fe943..5950e20d4023 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t  const nodemask_t *candidates);\n extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t      gfp_t gfp_mask);\n #else\n@@ -1220,6 +1222,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t\t const nodemask_t *candidates)\n+{\n+\treturn NUMA_NO_NODE;\n+}\n static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t\t     gfp_t gfp_mask)\n {\ndiff --git a/mm/memory-tiers.c b/mm/memory-tiers.c\nindex 9c742e18e48f..434190fdc078 100644\n--- a/mm/memory-tiers.c\n+++ b/mm/memory-tiers.c\n@@ -3,6 +3,7 @@\n #include <linux/lockdep.h>\n #include <linux/sysfs.h>\n #include <linux/kobject.h>\n+#include <linux/node_private.h>\n #include <linux/memory.h>\n #include <linux/memory-tiers.h>\n #include <linux/notifier.h>\n@@ -380,6 +381,8 @@ static void disable_all_demotion_targets(void)\n \t\tif (memtier)\n \t\t\tmemtier->lower_tier_mask = NODE_MASK_NONE;\n \t}\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE)\n+\t\tnode_demotion[node].preferred = NODE_MASK_NONE;\n \t/*\n \t * Ensure that the \"disable\" is visible across the system.\n \t * Readers will see either a combination of before+disable\n@@ -421,6 +424,7 @@ static void establish_demotion_targets(void)\n \tint target = NUMA_NO_NODE, node;\n \tint distance, best_distance;\n \tnodemask_t tier_nodes, lower_tier;\n+\tnodemask_t all_memory;\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n@@ -429,6 +433,13 @@ static void establish_demotion_targets(void)\n \n \tdisable_all_demotion_targets();\n \n+\t/* Include private nodes that have opted in to demotion. */\n+\tall_memory = node_states[N_MEMORY];\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(node, NP_OPS_DEMOTION))\n+\t\t\tnode_set(node, all_memory);\n+\t}\n+\n \tfor_each_node_state(node, N_MEMORY) {\n \t\tbest_distance = -1;\n \t\tnd = &node_demotion[node];\n@@ -442,12 +453,12 @@ static void establish_demotion_targets(void)\n \t\tmemtier = list_next_entry(memtier, list);\n \t\ttier_nodes = get_memtier_nodemask(memtier);\n \t\t/*\n-\t\t * find_next_best_node, use 'used' nodemask as a skip list.\n+\t\t * find_next_best_node_in, use 'used' nodemask as a skip list.\n \t\t * Add all memory nodes except the selected memory tier\n \t\t * nodelist to skip list so that we find the best node from the\n \t\t * memtier nodelist.\n \t\t */\n-\t\tnodes_andnot(tier_nodes, node_states[N_MEMORY], tier_nodes);\n+\t\tnodes_andnot(tier_nodes, all_memory, tier_nodes);\n \n \t\t/*\n \t\t * Find all the nodes in the memory tier node list of same best distance.\n@@ -455,7 +466,8 @@ static void establish_demotion_targets(void)\n \t\t * in the preferred mask when allocating pages during demotion.\n \t\t */\n \t\tdo {\n-\t\t\ttarget = find_next_best_node(node, &tier_nodes);\n+\t\t\ttarget = find_next_best_node_in(node, &tier_nodes,\n+\t\t\t\t\t\t\t&all_memory);\n \t\t\tif (target == NUMA_NO_NODE)\n \t\t\t\tbreak;\n \n@@ -495,7 +507,7 @@ static void establish_demotion_targets(void)\n \t * allocation to a set of nodes that is closer the above selected\n \t * preferred node.\n \t */\n-\tlower_tier = node_states[N_MEMORY];\n+\tlower_tier = all_memory;\n \tlist_for_each_entry(memtier, &memory_tiers, list) {\n \t\t/*\n \t\t * Keep removing current tier from lower_tier nodes,\n@@ -542,7 +554,7 @@ static struct memory_tier *set_node_memory_tier(int node)\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n-\tif (!node_state(node, N_MEMORY))\n+\tif (!node_state(node, N_MEMORY) && !node_state(node, N_MEMORY_PRIVATE))\n \t\treturn ERR_PTR(-EINVAL);\n \n \tmt_calc_adistance(node, &adist);\n@@ -865,6 +877,30 @@ int mt_calc_adistance(int node, int *adist)\n }\n EXPORT_SYMBOL_GPL(mt_calc_adistance);\n \n+/**\n+ * memory_tier_refresh_demotion() - Re-establish demotion targets\n+ *\n+ * Called by services after registering or unregistering ops->migrate_to on\n+ * a private node, so that establish_demotion_targets() picks up the change.\n+ */\n+void memory_tier_refresh_demotion(void)\n+{\n+\tint nid;\n+\n+\tmutex_lock(&memory_tier_lock);\n+\t/*\n+\t * Ensure private nodes are registered with a tier, otherwise\n+\t * they won't show up in any node's demotion targets nodemask.\n+\t */\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (!__node_get_memory_tier(nid))\n+\t\t\tset_node_memory_tier(nid);\n+\t}\n+\testablish_demotion_targets();\n+\tmutex_unlock(&memory_tier_lock);\n+}\n+EXPORT_SYMBOL_GPL(memory_tier_refresh_demotion);\n+\n static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\t\t\t\t      unsigned long action, void *_arg)\n {\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex ec6c1f8e85d8..e272dfdc6b00 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5589,7 +5589,8 @@ static int node_load[MAX_NUMNODES];\n  *\n  * Return: node id of the found node or %NUMA_NO_NODE if no node is found.\n  */\n-int find_next_best_node(int node, nodemask_t *used_node_mask)\n+int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t   const nodemask_t *candidates)\n {\n \tint n, val;\n \tint min_val = INT_MAX;\n@@ -5599,12 +5600,12 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \t * Use the local node if we haven't already, but for memoryless local\n \t * node, we should skip it and fall back to other nodes.\n \t */\n-\tif (!node_isset(node, *used_node_mask) && node_state(node, N_MEMORY)) {\n+\tif (!node_isset(node, *used_node_mask) && node_isset(node, *candidates)) {\n \t\tnode_set(node, *used_node_mask);\n \t\treturn node;\n \t}\n \n-\tfor_each_node_state(n, N_MEMORY) {\n+\tfor_each_node_mask(n, *candidates) {\n \n \t\t/* Don't want a node to appear more than once */\n \t\tif (node_isset(n, *used_node_mask))\n@@ -5636,6 +5637,11 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \treturn best_node;\n }\n \n+int find_next_best_node(int node, nodemask_t *used_node_mask)\n+{\n+\treturn find_next_best_node_in(node, used_node_mask,\n+\t\t\t\t      &node_states[N_MEMORY]);\n+}\n \n /*\n  * Build zonelists ordered by node and zones within node.\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 6113be4d3519..0f534428ea88 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -355,6 +356,10 @@ static bool can_demote(int nid, struct scan_control *sc,\n \tif (demotion_nid == NUMA_NO_NODE)\n \t\treturn false;\n \n+\t/* Don't demote when the target's service signals backpressure */\n+\tif (node_private_migration_blocked(demotion_nid))\n+\t\treturn false;\n+\n \t/* If demotion node isn't in the cgroup's mems_allowed, fall back */\n \treturn mem_cgroup_node_allowed(memcg, demotion_nid);\n }\n@@ -1022,8 +1027,10 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \t\t\t\t     struct pglist_data *pgdat)\n {\n \tint target_nid = next_demotion_node(pgdat->node_id);\n-\tunsigned int nr_succeeded;\n+\tint first_nid = target_nid;\n+\tunsigned int nr_succeeded = 0;\n \tnodemask_t allowed_mask;\n+\tint ret;\n \n \tstruct migration_target_control mtc = {\n \t\t/*\n@@ -1046,6 +1053,27 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \n \tnode_get_allowed_targets(pgdat, &allowed_mask);\n \n+\t/* Try private node targets until we find non-private node */\n+\twhile (node_state(target_nid, N_MEMORY_PRIVATE)) {\n+\t\tunsigned int nr = 0;\n+\n+\t\tret = node_private_migrate_to(demote_folios, target_nid,\n+\t\t\t\t\t      MIGRATE_ASYNC, MR_DEMOTION,\n+\t\t\t\t\t      &nr);\n+\t\tnr_succeeded += nr;\n+\t\tif (ret == 0 || list_empty(demote_folios))\n+\t\t\treturn nr_succeeded;\n+\n+\t\ttarget_nid = next_node_in(target_nid, allowed_mask);\n+\t\tif (target_nid == first_nid)\n+\t\t\treturn nr_succeeded;\n+\t\tif (!node_state(target_nid, N_MEMORY_PRIVATE))\n+\t\t\tbreak;\n+\t}\n+\n+\t/* target_nid is a non-private node; use standard migration */\n+\tmtc.nid = target_nid;\n+\n \t/* Demotion ignores all cpuset and mempolicy settings */\n \tmigrate_pages(demote_folios, alloc_demote_folio, NULL,\n \t\t      (unsigned long)&mtc, MIGRATE_ASYNC, MR_DEMOTION,\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about write faults on private nodes by adding a new operation flag NP_OPS_PROTECT_WRITE and modifying several functions to prevent PTEs from being upgraded to writable when the node is write-protected.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Services that intercept write faults (e.g., for promotion tracking)\nneed PTEs to stay read-only. This requires preventing mprotect\nfrom silently upgrade the PTE, bypassing the service's handle_fault\ncallback.\n\nAdd NP_OPS_PROTECT_WRITE and folio_managed_wrprotect().\n\nIn change_pte_range() and change_huge_pmd(), suppress PTE write-upgrade\nwhen MM_CP_TRY_CHANGE_WRITABLE is sees the folio is write-protected.\n\nIn handle_pte_fault() and do_huge_pmd_wp_page(), dispatch to the node's\nops->handle_fault callback when set, allowing the service to handle write\nfaults with promotion or other custom logic.\n\nNP_OPS_MEMPOLICY is incompatible with NP_OPS_PROTECT_WRITE to avoid the\nfootgun of binding a writable VMA to a write-protected node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++\n include/linux/node_private.h | 22 ++++++++\n mm/huge_memory.c             | 17 ++++++-\n mm/internal.h                | 99 ++++++++++++++++++++++++++++++++++++\n mm/memory.c                  | 15 ++++++\n mm/migrate.c                 | 14 +----\n mm/mprotect.c                |  4 +-\n 7 files changed, 159 insertions(+), 16 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex c08b5a948779..a4955b9b5b93 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -957,6 +957,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    (ops->flags & NP_OPS_PROTECT_WRITE))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e254e36056cd..27d6e5d84e61 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -70,6 +70,24 @@ struct vm_fault;\n  *     PFN-based metadata (compression tables, device page tables, DMA\n  *     mappings, etc.) before any access through the page tables.\n  *\n+ * @handle_fault: Handle fault on folio on this private node.\n+ *   [folio-referenced callback, PTL held on entry]\n+ *\n+ *   Called from handle_pte_fault() (PTE level) or do_huge_pmd_wp_page()\n+ *   (PMD level) after lock acquisition and entry verification.\n+ *   @folio is the faulting folio, @level indicates the page table level.\n+ *\n+ *   For PGTABLE_LEVEL_PTE: vmf->pte is mapped and vmf->ptl is the\n+ *   PTE lock.  Release via pte_unmap_unlock(vmf->pte, vmf->ptl).\n+ *\n+ *   For PGTABLE_LEVEL_PMD: vmf->pte is NULL and vmf->ptl is the\n+ *   PMD lock.  Release via spin_unlock(vmf->ptl).\n+ *\n+ *   The callback MUST release PTL on ALL paths.\n+ *   The caller will NOT touch the page table entry after this returns.\n+ *\n+ *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -81,6 +99,8 @@ struct node_private_ops {\n \t\t\t\t  enum migrate_reason reason,\n \t\t\t\t  unsigned int *nr_succeeded);\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n+\tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t   enum pgtable_level level);\n \tunsigned long flags;\n };\n \n@@ -90,6 +110,8 @@ struct node_private_ops {\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n /* Node participates as a demotion target in memory-tiers */\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n+/* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n+#define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 2ecae494291a..d9ba6593244d 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -2063,12 +2063,14 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tstruct page *page;\n \tunsigned long haddr = vmf->address & HPAGE_PMD_MASK;\n \tpmd_t orig_pmd = vmf->orig_pmd;\n+\tvm_fault_t ret;\n+\n \n \tvmf->ptl = pmd_lockptr(vma->vm_mm, vmf->pmd);\n \tVM_BUG_ON_VMA(!vma->anon_vma, vma);\n \n \tif (is_huge_zero_pmd(orig_pmd)) {\n-\t\tvm_fault_t ret = do_huge_zero_wp_pmd(vmf);\n+\t\tret = do_huge_zero_wp_pmd(vmf);\n \n \t\tif (!(ret & VM_FAULT_FALLBACK))\n \t\t\treturn ret;\n@@ -2088,6 +2090,13 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tfolio = page_folio(page);\n \tVM_BUG_ON_PAGE(!PageHead(page), page);\n \n+\t/* Private-managed write-protect: let the service handle the fault */\n+\tif (unlikely(folio_is_private_managed(folio))) {\n+\t\tif (folio_managed_handle_fault(folio, vmf,\n+\t\t\t\t\t      PGTABLE_LEVEL_PMD, &ret))\n+\t\t\treturn ret;\n+\t}\n+\n \t/* Early check when only holding the PT lock. */\n \tif (PageAnonExclusive(page))\n \t\tgoto reuse;\n@@ -2633,7 +2642,8 @@ int change_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,\n \n \t/* See change_pte_range(). */\n \tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) && !pmd_write(entry) &&\n-\t    can_change_pmd_writable(vma, addr, entry))\n+\t    can_change_pmd_writable(vma, addr, entry) &&\n+\t    !folio_managed_wrprotect(pmd_folio(entry)))\n \t\tentry = pmd_mkwrite(entry, vma);\n \n \tret = HPAGE_PMD_NR;\n@@ -4943,6 +4953,9 @@ void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n \tif (folio_test_dirty(folio) && softleaf_is_migration_dirty(entry))\n \t\tpmde = pmd_mkdirty(pmde);\n \n+\tif (folio_managed_wrprotect(folio))\n+\t\tpmde = pmd_wrprotect(pmde);\n+\n \tif (folio_is_device_private(folio)) {\n \t\tswp_entry_t entry;\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 5950e20d4023..ae4ff86e8dc6 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -11,6 +11,7 @@\n #include <linux/khugepaged.h>\n #include <linux/mm.h>\n #include <linux/mm_inline.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/pagewalk.h>\n #include <linux/rmap.h>\n@@ -18,6 +19,7 @@\n #include <linux/leafops.h>\n #include <linux/swap_cgroup.h>\n #include <linux/tracepoint-defs.h>\n+#include <linux/node_private.h>\n \n /* Internal core VMA manipulation functions. */\n #include \"vma.h\"\n@@ -1449,6 +1451,103 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/*\n+ * folio_managed_handle_fault - Dispatch fault on managed-memory folio\n+ * @folio: the faulting folio (must not be NULL)\n+ * @vmf: the vm_fault descriptor (PTL held: vmf->ptl locked)\n+ * @level: page table level (PGTABLE_LEVEL_PTE or PGTABLE_LEVEL_PMD)\n+ * @ret: output fault result if handled\n+ *\n+ * Called with PTL held.  If a handle_fault callback exists, it is invoked\n+ * with PTL still held.  The callback is responsible for releasing PTL on\n+ * all paths.\n+ *\n+ * Returns true if the service handled the fault (PTL released by callback,\n+ * caller returns *ret).  Returns false if no handler exists (PTL still held,\n+ * caller continues with normal fault handling).\n+ */\n+static inline bool folio_managed_handle_fault(struct folio *folio,\n+\t\t\t\t\t      struct vm_fault *vmf,\n+\t\t\t\t\t      enum pgtable_level level,\n+\t\t\t\t\t      vm_fault_t *ret)\n+{\n+\t/* Zone device pages use swap entries; handled in do_swap_page */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->handle_fault) {\n+\t\t\t*ret = ops->handle_fault(folio, vmf, level);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n+/**\n+ * folio_managed_wrprotect - Should this folio's mappings stay write-protected?\n+ * @folio: the folio to check\n+ *\n+ * Returns true if the folio is on a private node with NP_OPS_PROTECT_WRITE,\n+ * meaning page table entries (PTE or PMD) should not be made writable.\n+ * Write faults are intercepted by the service's handle_fault callback\n+ * to promote the folio to DRAM.\n+ *\n+ * Used by:\n+ *   - change_pte_range() / change_huge_pmd(): prevent mprotect write-upgrade\n+ *   - remove_migration_pte() / remove_migration_pmd(): strip write after migration\n+ *   - do_huge_pmd_wp_page(): dispatch to fault handler instead of reuse\n+ */\n+static inline bool folio_managed_wrprotect(struct folio *folio)\n+{\n+\treturn unlikely(folio_is_private_node(folio) &&\n+\t\t\tfolio_private_flags(folio, NP_OPS_PROTECT_WRITE));\n+}\n+\n+/**\n+ * folio_managed_fixup_migration_pte - Fixup PTE after migration for\n+ *                                     managed memory pages.\n+ * @new: the destination page\n+ * @pte: the PTE being installed (normal PTE built by caller)\n+ * @old_pte: the original PTE (before migration, for swap entry flags)\n+ * @vma: the VMA\n+ *\n+ * For MEMORY_DEVICE_PRIVATE pages: replaces the PTE with a device-private\n+ * swap entry, preserving soft_dirty and uffd_wp from old_pte.\n+ *\n+ * For N_MEMORY_PRIVATE pages with NP_OPS_PROTECT_WRITE: strips the write\n+ * bit so the next write triggers the fault handler for promotion.\n+ *\n+ * For normal pages: returns pte unmodified.\n+ */\n+static inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n+\t\t\t\t\t\t      pte_t pte,\n+\t\t\t\t\t\t      pte_t old_pte,\n+\t\t\t\t\t\t      struct vm_area_struct *vma)\n+{\n+\tif (unlikely(is_device_private_page(new))) {\n+\t\tsoftleaf_t entry;\n+\n+\t\tif (pte_write(pte))\n+\t\t\tentry = make_writable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\telse\n+\t\t\tentry = make_readable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\tpte = softleaf_to_pte(entry);\n+\t\tif (pte_swp_soft_dirty(old_pte))\n+\t\t\tpte = pte_swp_mksoft_dirty(pte);\n+\t\tif (pte_swp_uffd_wp(old_pte))\n+\t\t\tpte = pte_swp_mkuffd_wp(pte);\n+\t} else if (folio_managed_wrprotect(page_folio(new))) {\n+\t\tpte = pte_wrprotect(pte);\n+\t}\n+\treturn pte;\n+}\n+\n /**\n  * folio_managed_migrate_notify - Notify service that a folio changed location\n  * @src: the old folio (about to be freed)\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 2a55edc48a65..0f78988befef 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -6079,6 +6079,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n \t */\n+\tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n+\t\twritable = false;\n+\t\tignore_writable = true;\n+\t}\n \tif (folio && folio_test_large(folio))\n \t\tnuma_rebuild_large_mapping(vmf, vma, folio, pte, ignore_writable,\n \t\t\t\t\t   pte_write_upgrade);\n@@ -6228,6 +6232,7 @@ static void fix_spurious_fault(struct vm_fault *vmf,\n  */\n static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n {\n+\tstruct folio *folio;\n \tpte_t entry;\n \n \tif (unlikely(pmd_none(*vmf->pmd))) {\n@@ -6284,6 +6289,16 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n \t\tupdate_mmu_tlb(vmf->vma, vmf->address, vmf->pte);\n \t\tgoto unlock;\n \t}\n+\n+\tfolio = vm_normal_folio(vmf->vma, vmf->address, entry);\n+\tif (unlikely(folio && folio_is_private_managed(folio))) {\n+\t\tvm_fault_t fault_ret;\n+\n+\t\tif (folio_managed_handle_fault(folio, vmf, PGTABLE_LEVEL_PTE,\n+\t\t\t\t\t       &fault_ret))\n+\t\t\treturn fault_ret;\n+\t}\n+\n \tif (vmf->flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {\n \t\tif (!pte_write(entry))\n \t\t\treturn do_wp_page(vmf);\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex a54d4af04df3..f632e8b03504 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -398,19 +398,7 @@ static bool remove_migration_pte(struct folio *folio,\n \t\tif (folio_test_anon(folio) && !softleaf_is_migration_read(entry))\n \t\t\trmap_flags |= RMAP_EXCLUSIVE;\n \n-\t\tif (unlikely(is_device_private_page(new))) {\n-\t\t\tif (pte_write(pte))\n-\t\t\t\tentry = make_writable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\telse\n-\t\t\t\tentry = make_readable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\tpte = softleaf_to_pte(entry);\n-\t\t\tif (pte_swp_soft_dirty(old_pte))\n-\t\t\t\tpte = pte_swp_mksoft_dirty(pte);\n-\t\t\tif (pte_swp_uffd_wp(old_pte))\n-\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n-\t\t}\n+\t\tpte = folio_managed_fixup_migration_pte(new, pte, old_pte, vma);\n \n #ifdef CONFIG_HUGETLB_PAGE\n \t\tif (folio_test_hugetlb(folio)) {\ndiff --git a/mm/mprotect.c b/mm/mprotect.c\nindex 283889e4f1ce..830be609bc24 100644\n--- a/mm/mprotect.c\n+++ b/mm/mprotect.c\n@@ -30,6 +30,7 @@\n #include <linux/mm_inline.h>\n #include <linux/pgtable.h>\n #include <linux/userfaultfd_k.h>\n+#include <linux/node_private.h>\n #include <uapi/linux/mman.h>\n #include <asm/cacheflush.h>\n #include <asm/mmu_context.h>\n@@ -290,7 +291,8 @@ static long change_pte_range(struct mmu_gather *tlb,\n \t\t\t * COW or special handling is required.\n \t\t\t */\n \t\t\tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) &&\n-\t\t\t     !pte_write(ptent))\n+\t\t\t     !pte_write(ptent) &&\n+\t\t\t     !(folio && folio_managed_wrprotect(folio)))\n \t\t\t\tset_write_prot_commit_flush_ptes(vma, folio, page,\n \t\t\t\taddr, pte, oldpte, ptent, nr_ptes, tlb);\n \t\t\telse\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the reclaim policy for private nodes in boosted reclaim mode, explaining that it needs to allow swap and writepage operations. They proposed adding a reclaim_policy callback to struct node_private_ops and a struct node_reclaim_policy to configure these policies. The author also added zone_reclaim_allowed() to filter private nodes that have not opted into reclaim.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix is needed",
                "proposed changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services that drive kswapd via watermark_boost need\ncontrol over the reclaim policy.  There are three problems:\n\n1) Boosted reclaim suppresses may_swap and may_writepage.  When\n   demotion is not possible, swap is the only evict path, so kswapd\n   cannot make progress and pages are stranded.\n\n2) __setup_per_zone_wmarks() unconditionally zeros watermark_boost,\n   killing the service's pressure signal.\n\n3) Not all private nodes want reclaim to touch their pages.\n\nAdd a reclaim_policy callback to struct node_private_ops and a\nstruct node_reclaim_policy with:\n\n  - active:             set by the helper when a callback was invoked\n  - may_swap:           allow swap writeback during boosted reclaim\n  - may_writepage:      allow writepage during boosted reclaim\n  - managed_watermarks: service owns watermark_boost lifecycle\n\nWe do not allow disabling swap/writepage, as core MM may have\nexplicitly enabled them on a non-boosted pass.\n\nWe only allow enablign swap/writepage, so that the supression during\na boost can be overridden.  This allows a device to force evictions\neven when the system otherwise would not percieve pressure.\n\nThis is important for a service like compressed RAM, as device capacity\nmay differ from reported capacity, and device may want to relieve real\npressure (poor compression ratio) as opposed to percieved pressure\n(i.e. how many pages are in use).\n\nAdd zone_reclaim_allowed() to filter private nodes that have not\nopted into reclaim.\n\nRegular nodes fall through to cpuset_zone_allowed() unchanged.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 28 ++++++++++++++++++++++++++++\n mm/internal.h                | 36 ++++++++++++++++++++++++++++++++++++\n mm/page_alloc.c              | 11 ++++++++++-\n mm/vmscan.c                  | 25 +++++++++++++++++++++++--\n 4 files changed, 97 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 27d6e5d84e61..34be52383255 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -14,6 +14,24 @@ struct page;\n struct vm_area_struct;\n struct vm_fault;\n \n+/**\n+ * struct node_reclaim_policy - Reclaim policy overrides for private nodes\n+ * @active: set by node_private_reclaim_policy() when a callback was invoked\n+ * @may_swap: allow swap writeback during boosted reclaim\n+ * @may_writepage: allow writepage during boosted reclaim\n+ * @managed_watermarks: service owns watermark_boost lifecycle; kswapd must\n+ *                      not clear it after boosted reclaim\n+ *\n+ * Passed to the reclaim_policy callback so each private node service can\n+ * inject its own reclaim policy before kswapd runs boosted reclaim.\n+ */\n+struct node_reclaim_policy {\n+\tbool active;\n+\tbool may_swap;\n+\tbool may_writepage;\n+\tbool managed_watermarks;\n+};\n+\n /**\n  * struct node_private_ops - Callbacks for private node services\n  *\n@@ -88,6 +106,13 @@ struct vm_fault;\n  *\n  *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n  *\n+ * @reclaim_policy: Configure reclaim policy for boosted reclaim.\n+ *   [called hodling rcu_read_lock, MUST NOT sleep]\n+ *   Called by kswapd before boosted reclaim to let the service override\n+ *   may_swap / may_writepage.  If provided, the service also owns the\n+ *   watermark_boost lifecycle (kswapd will not clear it).\n+ *   If NULL, normal boost policy applies.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -101,6 +126,7 @@ struct node_private_ops {\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n+\tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n \tunsigned long flags;\n };\n \n@@ -112,6 +138,8 @@ struct node_private_ops {\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n /* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n+/* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n+#define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/internal.h b/mm/internal.h\nindex ae4ff86e8dc6..db32cb2d7a29 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1572,6 +1572,42 @@ static inline void folio_managed_migrate_notify(struct folio *src,\n \t\tops->folio_migrate(src, dst);\n }\n \n+/**\n+ * node_private_reclaim_policy - invoke the service's reclaim policy callback\n+ * @nid: NUMA node id\n+ * @policy: reclaim policy struct to fill in\n+ *\n+ * Called by kswapd before boosted reclaim.  Zeroes @policy, then if the\n+ * private node service provides a reclaim_policy callback, invokes it\n+ * and sets policy->active to true.\n+ */\n+#ifdef CONFIG_NUMA\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tstruct node_private *np;\n+\n+\tmemset(policy, 0, sizeof(*policy));\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (np && np->ops && np->ops->reclaim_policy) {\n+\t\tnp->ops->reclaim_policy(nid, policy);\n+\t\tpolicy->active = true;\n+\t}\n+\trcu_read_unlock();\n+}\n+#else\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tmemset(policy, 0, sizeof(*policy));\n+}\n+#endif\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e272dfdc6b00..9692048ab5fb 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -55,6 +55,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/node_private.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -6437,6 +6438,8 @@ static void __setup_per_zone_wmarks(void)\n \tunsigned long lowmem_pages = 0;\n \tstruct zone *zone;\n \tunsigned long flags;\n+\tstruct node_reclaim_policy rp;\n+\tint prev_nid = NUMA_NO_NODE;\n \n \t/* Calculate total number of !ZONE_HIGHMEM and !ZONE_MOVABLE pages */\n \tfor_each_zone(zone) {\n@@ -6446,6 +6449,7 @@ static void __setup_per_zone_wmarks(void)\n \n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n+\t\tint nid = zone_to_nid(zone);\n \n \t\tspin_lock_irqsave(&zone->lock, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n@@ -6482,7 +6486,12 @@ static void __setup_per_zone_wmarks(void)\n \t\t\t    mult_frac(zone_managed_pages(zone),\n \t\t\t\t      watermark_scale_factor, 10000));\n \n-\t\tzone->watermark_boost = 0;\n+\t\tif (nid != prev_nid) {\n+\t\t\tnode_private_reclaim_policy(nid, &rp);\n+\t\t\tprev_nid = nid;\n+\t\t}\n+\t\tif (!rp.managed_watermarks)\n+\t\t\tzone->watermark_boost = 0;\n \t\tzone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_HIGH] = low_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 0f534428ea88..07de666c1276 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -73,6 +73,13 @@\n #define CREATE_TRACE_POINTS\n #include <trace/events/vmscan.h>\n \n+static inline bool zone_reclaim_allowed(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn zone_private_flags(zone, NP_OPS_RECLAIM);\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n struct scan_control {\n \t/* How many pages shrink_list() should reclaim */\n \tunsigned long nr_to_reclaim;\n@@ -6274,7 +6281,7 @@ static void shrink_zones(struct zonelist *zonelist, struct scan_control *sc)\n \t\t * to global LRU.\n \t\t */\n \t\tif (!cgroup_reclaim(sc)) {\n-\t\t\tif (!cpuset_zone_allowed(zone,\n+\t\t\tif (!zone_reclaim_allowed(zone,\n \t\t\t\t\t\t GFP_KERNEL | __GFP_HARDWALL))\n \t\t\t\tcontinue;\n \n@@ -6992,6 +6999,7 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \tunsigned long zone_boosts[MAX_NR_ZONES] = { 0, };\n \tbool boosted;\n \tstruct zone *zone;\n+\tstruct node_reclaim_policy policy;\n \tstruct scan_control sc = {\n \t\t.gfp_mask = GFP_KERNEL,\n \t\t.order = order,\n@@ -7016,6 +7024,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t}\n \tboosted = nr_boost_reclaim;\n \n+\t/* Query/cache private node reclaim policy once per balance() */\n+\tnode_private_reclaim_policy(pgdat->node_id, &policy);\n+\n restart:\n \tset_reclaim_active(pgdat, highest_zoneidx);\n \tsc.priority = DEF_PRIORITY;\n@@ -7083,6 +7094,12 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\tsc.may_writepage = !laptop_mode && !nr_boost_reclaim;\n \t\tsc.may_swap = !nr_boost_reclaim;\n \n+\t\t/* Private nodes may enable swap/writepage when using boost */\n+\t\tif (policy.active) {\n+\t\t\tsc.may_swap |= policy.may_swap;\n+\t\t\tsc.may_writepage |= policy.may_writepage;\n+\t\t}\n+\n \t\t/*\n \t\t * Do some background aging, to give pages a chance to be\n \t\t * referenced before reclaiming. All pages are rotated\n@@ -7176,6 +7193,10 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\t\tif (!zone_boosts[i])\n \t\t\t\tcontinue;\n \n+\t\t\t/* Some private nodes may own the\\ boost lifecycle */\n+\t\t\tif (policy.managed_watermarks)\n+\t\t\t\tcontinue;\n+\n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n \t\t\tspin_lock_irqsave(&zone->lock, flags);\n@@ -7406,7 +7427,7 @@ void wakeup_kswapd(struct zone *zone, gfp_t gfp_flags, int order,\n \tif (!managed_zone(zone))\n \t\treturn;\n \n-\tif (!cpuset_zone_allowed(zone, gfp_flags))\n+\tif (!zone_reclaim_allowed(zone, gfp_flags))\n \t\treturn;\n \n \tpgdat = zone->zone_pgdat;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed the concern that the OOM killer may select an undeserving victim if it doesn't know whether killing a task can actually free memory on a private node. The author introduced NP_OPS_OOM_ELIGIBLE and helpers to check if a private node is OOM-eligible, and updated constrained_alloc() to use these checks.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The OOM killer must know whether killing a task can actually free\nmemory such that pressure is reduced.\n\nA private node only contributes to relieving pressure if it participates\nin both reclaim and demotion. Without this check, the check, the OOM\nkiller may select an undeserving victim.\n\nIntroduce NP_OPS_OOM_ELIGIBLE and helpers node_oom_eligible() and\nzone_oom_eligible().\n\nReplace cpuset_mems_allowed_intersects() in oom_cpuset_eligible()\nwith oom_mems_intersect() that iterates N_MEMORY nodes and skips\nineligible private nodes.\n\nUpdate constrained_alloc() to use zone_oom_eligible() for constraint\ndetection and node_oom_eligible() to exclude ineligible nodes from\ntotalpages accounting.\n\nRemove cpuset_mems_allowed_intersects() as it has no remaining callers.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cpuset.h       |  9 -------\n include/linux/node_private.h |  3 +++\n kernel/cgroup/cpuset.c       | 17 ------------\n mm/oom_kill.c                | 52 ++++++++++++++++++++++++++++++++----\n 4 files changed, 50 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/cpuset.h b/include/linux/cpuset.h\nindex 7b2f3f6b68a9..53ccfb00b277 100644\n--- a/include/linux/cpuset.h\n+++ b/include/linux/cpuset.h\n@@ -97,9 +97,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-extern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t  const struct task_struct *tsk2);\n-\n #ifdef CONFIG_CPUSETS_V1\n #define cpuset_memory_pressure_bump() \t\t\t\t\\\n \tdo {\t\t\t\t\t\t\t\\\n@@ -241,12 +238,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-static inline int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t\t const struct task_struct *tsk2)\n-{\n-\treturn 1;\n-}\n-\n static inline void cpuset_memory_pressure_bump(void) {}\n \n static inline void cpuset_task_status_allowed(struct seq_file *m,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34be52383255..34d862f09e24 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -141,6 +141,9 @@ struct node_private_ops {\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n+/* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n+#define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 1a597f0c7c6c..29789d544fd5 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -4530,23 +4530,6 @@ int cpuset_mem_spread_node(void)\n \treturn cpuset_spread_node(&current->cpuset_mem_spread_rotor);\n }\n \n-/**\n- * cpuset_mems_allowed_intersects - Does @tsk1's mems_allowed intersect @tsk2's?\n- * @tsk1: pointer to task_struct of some task.\n- * @tsk2: pointer to task_struct of some other task.\n- *\n- * Description: Return true if @tsk1's mems_allowed intersects the\n- * mems_allowed of @tsk2.  Used by the OOM killer to determine if\n- * one of the task's memory usage might impact the memory available\n- * to the other.\n- **/\n-\n-int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t   const struct task_struct *tsk2)\n-{\n-\treturn nodes_intersects(tsk1->mems_allowed, tsk2->mems_allowed);\n-}\n-\n /**\n  * cpuset_print_current_mems_allowed - prints current's cpuset and mems_allowed\n  *\ndiff --git a/mm/oom_kill.c b/mm/oom_kill.c\nindex 5eb11fbba704..cd0d65ccd1e8 100644\n--- a/mm/oom_kill.c\n+++ b/mm/oom_kill.c\n@@ -74,7 +74,45 @@ static inline bool is_memcg_oom(struct oom_control *oc)\n \treturn oc->memcg != NULL;\n }\n \n+/* Private nodes are only eligible if they support both reclaim and demotion */\n+static inline bool node_oom_eligible(int nid)\n+{\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn (node_private_flags(nid) & NP_OPS_OOM_ELIGIBLE) ==\n+\t\tNP_OPS_OOM_ELIGIBLE;\n+}\n+\n+static inline bool zone_oom_eligible(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (!node_oom_eligible(zone_to_nid(zone)))\n+\t\treturn false;\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n #ifdef CONFIG_NUMA\n+/*\n+ * Killing a task can only relieve system pressure if freed memory can be\n+ * demoted there and reclaim can operate on the node's pages, so we\n+ * omit private nodes that aren't eligible.\n+ */\n+static bool oom_mems_intersect(const struct task_struct *tsk1,\n+\t\t\t       const struct task_struct *tsk2)\n+{\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (!node_isset(nid, tsk1->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_isset(nid, tsk2->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_oom_eligible(nid))\n+\t\t\tcontinue;\n+\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n /**\n  * oom_cpuset_eligible() - check task eligibility for kill\n  * @start: task struct of which task to consider\n@@ -107,9 +145,10 @@ static bool oom_cpuset_eligible(struct task_struct *start,\n \t\t} else {\n \t\t\t/*\n \t\t\t * This is not a mempolicy constrained oom, so only\n-\t\t\t * check the mems of tsk's cpuset.\n+\t\t\t * check the mems of tsk's cpuset, excluding private\n+\t\t\t * nodes that do not participate in kernel reclaim.\n \t\t\t */\n-\t\t\tret = cpuset_mems_allowed_intersects(current, tsk);\n+\t\t\tret = oom_mems_intersect(current, tsk);\n \t\t}\n \t\tif (ret)\n \t\t\tbreak;\n@@ -291,16 +330,19 @@ static enum oom_constraint constrained_alloc(struct oom_control *oc)\n \t\treturn CONSTRAINT_MEMORY_POLICY;\n \t}\n \n-\t/* Check this allocation failure is caused by cpuset's wall function */\n+\t/* Check this allocation failure is caused by cpuset or private node constraints */\n \tfor_each_zone_zonelist_nodemask(zone, z, oc->zonelist,\n \t\t\thighest_zoneidx, oc->nodemask)\n-\t\tif (!cpuset_zone_allowed(zone, oc->gfp_mask))\n+\t\tif (!zone_oom_eligible(zone, oc->gfp_mask))\n \t\t\tcpuset_limited = true;\n \n \tif (cpuset_limited) {\n \t\toc->totalpages = total_swap_pages;\n-\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed)\n+\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed) {\n+\t\t\tif (!node_oom_eligible(nid))\n+\t\t\t\tcontinue;\n \t\t\toc->totalpages += node_present_pages(nid);\n+\t\t}\n \t\treturn CONSTRAINT_CPUSET;\n \t}\n \treturn CONSTRAINT_NONE;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private nodes engaging in NUMA balancing faults by introducing an opt-in method (NP_OPS_NUMA_BALANCING) and adding a helper function to filter for private nodes that have opted in. The author also added code to enforce write-protection on failed or skipped migrations.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "added new functionality"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Not all private nodes may wish to engage in NUMA balancing faults.\n\nAdd the NP_OPS_NUMA_BALANCING flag (BIT(5)) as an opt-in method.\n\nIntroduce folio_managed_allows_numa() helper:\n   ZONE_DEVICE folios always return false (never NUMA-scanned)\n   NP_OPS_NUMA_BALANCING filters for private nodes\n\nIn do_numa_page(), if a private-node folio with NP_OPS_PROTECT_WRITE\nis still on its node after a failed/skipped migration, enforce\nwrite-protection so the next write triggers handle_fault.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h | 16 ++++++++++++++++\n mm/memory.c                  | 11 +++++++++++\n mm/mempolicy.c               |  5 ++++-\n 4 files changed, 35 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex a4955b9b5b93..88aaac45e814 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -961,6 +961,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (ops->flags & NP_OPS_PROTECT_WRITE))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_NUMA_BALANCING) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34d862f09e24..5ac60db1f044 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -140,6 +140,8 @@ struct node_private_ops {\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n+/* Allow NUMA balancing to scan and migrate folios on this node */\n+#define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n@@ -263,6 +265,15 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n }\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\tif (!folio_is_private_managed(folio))\n+\t\treturn true;\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\treturn folio_private_flags(folio, NP_OPS_NUMA_BALANCING);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \tif (folio_is_zone_device(folio))\n@@ -443,6 +454,11 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\treturn !folio_is_zone_device(folio);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \treturn -ENOENT;\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 0f78988befef..88a581baae40 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -78,6 +78,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/pgalloc.h>\n #include <linux/uaccess.h>\n+#include <linux/node_private.h>\n \n #include <trace/events/kmem.h>\n \n@@ -6041,6 +6042,12 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \tif (!folio || folio_is_zone_device(folio))\n \t\tgoto out_map;\n \n+\t/*\n+\t * We do not need to check private-node folios here because the private\n+\t * memory service either never opted in to NUMA balancing, or it did\n+\t * and we need to restore private PTE controls on the failure path.\n+\t */\n+\n \tnid = folio_nid(folio);\n \tnr_pages = folio_nr_pages(folio);\n \n@@ -6078,6 +6085,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t/*\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n+\t *\n+\t * If the folio is still on a private node with NP_OPS_PROTECT_WRITE,\n+\t * enforce write-protection so the next write triggers handle_fault.\n+\t * This covers migration-failed and migration-skipped paths.\n \t */\n \tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n \t\twritable = false;\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 8ac014950e88..8a3a9916ab59 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -861,7 +861,10 @@ bool folio_can_map_prot_numa(struct folio *folio, struct vm_area_struct *vma,\n {\n \tint nid;\n \n-\tif (!folio || folio_is_zone_device(folio) || folio_test_ksm(folio))\n+\tif (!folio || folio_test_ksm(folio))\n+\t\treturn false;\n+\n+\tif (unlikely(!folio_managed_allows_numa(folio)))\n \t\treturn false;\n \n \t/* Also skip shared copy-on-write folios */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about compaction on private nodes, explaining that it requires migration and services may have PFN-based metadata to update. They added a folio_migrate callback, zone_supports_compaction function, and filtered three direct compaction zone loops. The service is responsible for starting kcompactd on its node.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node zones should not be compacted unless the service explicitly\nopts in - as compaction requires migration and services may have\nPFN-based metadata that needs updating.\n\nAdd a folio_migrate callback which fires from migrate_folio_move() for\neach relocated folio before faults are unblocked.\n\nAdd zone_supports_compaction() which returns true for normal zones and\nchecks NP_OPS_COMPACTION for N_MEMORY_PRIVATE zones.\n\nFilter three direct compaction zone loops:\n  - compaction_zonelist_suitable() (reclaimer eligibility)\n  - try_to_compact_pages()         (direct compaction)\n  - compact_node()                 (proactive/manual compaction)\n\nkcompactd paths are intentionally unfiltered -- the service is\nresponsible for starting kcompactd on its node.\n\nNP_OPS_COMPACTION requires NP_OPS_MIGRATION.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h |  2 ++\n mm/compaction.c              | 26 ++++++++++++++++++++++++++\n 3 files changed, 32 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 88aaac45e814..da523aca18fa 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -965,6 +965,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_COMPACTION) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 5ac60db1f044..fe0336773ddb 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -142,6 +142,8 @@ struct node_private_ops {\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n /* Allow NUMA balancing to scan and migrate folios on this node */\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n+/* Allow compaction to run on the node.  Service must start kcompactd. */\n+#define NP_OPS_COMPACTION\t\tBIT(6)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 6a65145b03d8..d8532b957ec6 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,9 +24,26 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/node_private.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n+\n+/*\n+ * Private node zones require NP_OPS_COMPACTION to opt in.  Normal zones\n+ * always support compaction.\n+ */\n+static inline bool zone_supports_compaction(struct zone *zone)\n+{\n+#ifdef CONFIG_NUMA\n+\tif (!node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn zone_private_flags(zone, NP_OPS_COMPACTION);\n+#else\n+\treturn true;\n+#endif\n+}\n+\n /*\n  * Fragmentation score check interval for proactive compaction purposes.\n  */\n@@ -2443,6 +2460,9 @@ bool compaction_zonelist_suitable(struct alloc_context *ac, int order,\n \t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tunsigned long available;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\t/*\n \t\t * Do not consider all the reclaimable memory because we do not\n \t\t * want to trash just for a single high order allocation which\n@@ -2832,6 +2852,9 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\n \t\t\trc = max_t(enum compact_result, COMPACT_DEFERRED, rc);\n@@ -2906,6 +2929,9 @@ static int compact_node(pg_data_t *pgdat, bool proactive)\n \t\tif (!populated_zone(zone))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (fatal_signal_pending(current))\n \t\t\treturn -EINTR;\n \n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private node folios being longterm-pinnable by default, explaining that this would prevent services from controlling the memory during pinning. They added an NP_OPS_LONGTERM_PIN flag for services to opt-in and modified the folio_is_longterm_pinnable() function in mm.h to check for this flag.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not be longterm-pinnable by default.\nA pinned folio is frozen in place, no migration, compaction, or\nreclaim, so the service loses control for the duration of the pin.\n\nSome services may depend on hot-unplugability and must disallow\nlongterm pinning.  Others (accelerators with shared CPU-device state)\nneed pinning to work.\n\nAdd NP_OPS_LONGTERM_PIN flag for services to opt in with. Hook into\nfolio_is_longterm_pinnable() in mm.h, which all GUP callers\nout-of-line helper, node_private_allows_longterm_pin(),  called\nonly for N_MEMORY_PRIVATE nodes.\n\nWithout the flag: folio_is_longterm_pinnable() returns false, migration\nfails (no __GFP_PRIVATE in GFP mask) and pin_user_pages(FOLL_LONGTERM)\nreturns -ENOMEM.\n\nWith the flag: pin succeeds and the folio stays on the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 15 +++++++++++++++\n include/linux/mm.h           | 22 ++++++++++++++++++++++\n include/linux/node_private.h |  2 ++\n 3 files changed, 39 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex da523aca18fa..5d2487fd54f4 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -866,6 +866,21 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n static DEFINE_MUTEX(node_private_lock);\n static bool node_private_initialized;\n \n+/**\n+ * node_private_allows_longterm_pin - Check if a private node allows longterm pinning\n+ * @nid: Node identifier\n+ *\n+ * Out-of-line helper for folio_is_longterm_pinnable() since mm.h cannot\n+ * include node_private.h (circular dependency).\n+ *\n+ * Returns true if the node has NP_OPS_LONGTERM_PIN set.\n+ */\n+bool node_private_allows_longterm_pin(int nid)\n+{\n+\treturn node_private_has_flag(nid, NP_OPS_LONGTERM_PIN);\n+}\n+EXPORT_SYMBOL_GPL(node_private_allows_longterm_pin);\n+\n /**\n  * node_private_register - Register a private node\n  * @nid: Node identifier\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex fb1819ad42c3..9088fd08aeb9 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -2192,6 +2192,13 @@ static inline bool is_zero_folio(const struct folio *folio)\n \n /* MIGRATE_CMA and ZONE_MOVABLE do not allow pin folios */\n #ifdef CONFIG_MIGRATION\n+\n+#ifdef CONFIG_NUMA\n+bool node_private_allows_longterm_pin(int nid);\n+#else\n+static inline bool node_private_allows_longterm_pin(int nid) { return false; }\n+#endif\n+\n static inline bool folio_is_longterm_pinnable(struct folio *folio)\n {\n #ifdef CONFIG_CMA\n@@ -2215,6 +2222,21 @@ static inline bool folio_is_longterm_pinnable(struct folio *folio)\n \tif (folio_is_fsdax(folio))\n \t\treturn false;\n \n+\t/*\n+\t * Private node folios are not longterm pinnable by default.\n+\t * Services that support pinning opt in via NP_OPS_LONGTERM_PIN.\n+\t * node_private_allows_longterm_pin() is out-of-line because\n+\t * node_private.h includes mm.h (circular dependency).\n+\t *\n+\t * Guarded by CONFIG_NUMA because on !CONFIG_NUMA the single-node\n+\t * node_state() stub returns true for node 0, which would make\n+\t * all folios non-pinnable via the false-returning stub.\n+\t */\n+#ifdef CONFIG_NUMA\n+\tif (node_state(folio_nid(folio), N_MEMORY_PRIVATE))\n+\t\treturn node_private_allows_longterm_pin(folio_nid(folio));\n+#endif\n+\n \t/* Otherwise, non-movable zone folios can be pinned. */\n \treturn !folio_is_zone_movable(folio);\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex fe0336773ddb..7a7438fb9eda 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -144,6 +144,8 @@ struct node_private_ops {\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n /* Allow compaction to run on the node.  Service must start kcompactd. */\n #define NP_OPS_COMPACTION\t\tBIT(6)\n+/* Allow longterm DMA pinning (RDMA, VFIO, etc.) of folios on this node */\n+#define NP_OPS_LONGTERM_PIN\t\tBIT(7)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about notifying private-node services of hardware errors on their nodes by adding a memory_failure callback to struct node_private_ops, which will be called after TestSetPageHWPoison succeeds and before get_hwpoison_page. The kernel always proceeds with standard hwpoison handling for online pages.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "added callback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a void memory_failure notification callback to struct\nnode_private_ops so services managing N_MEMORY_PRIVATE nodes notified\nwhen a page on their node experiences a hardware error.\n\nThe callback is notification only -- the kernel always proceeds with\nstandard hwpoison handling for online pages.\n\nThe notification hook fires after TestSetPageHWPoison succeeds and\nbefore get_hwpoison_page giving the service a chance to clean up.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 16 ++++++++++++++++\n mm/memory-failure.c          | 15 +++++++++++++++\n 3 files changed, 37 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7a7438fb9eda..d2669f68ac20 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -113,6 +113,10 @@ struct node_reclaim_policy {\n  *   watermark_boost lifecycle (kswapd will not clear it).\n  *   If NULL, normal boost policy applies.\n  *\n+ * @memory_failure: Notification of hardware error on a page on this node.\n+ *   [folio-referenced callback]\n+ *   Notification only, kernel always handles the failure.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -127,6 +131,8 @@ struct node_private_ops {\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n \tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n+\tvoid (*memory_failure)(struct folio *folio, unsigned long pfn,\n+\t\t\t       int mf_flags);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex db32cb2d7a29..64467ca774f1 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1608,6 +1608,22 @@ static inline void node_private_reclaim_policy(int nid,\n }\n #endif\n \n+static inline void folio_managed_memory_failure(struct folio *folio,\n+\t\t\t\t\t\tunsigned long pfn,\n+\t\t\t\t\t\tint mf_flags)\n+{\n+\t/* Zone device pages handle memory failure via dev_pagemap_ops */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn;\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->memory_failure)\n+\t\t\tops->memory_failure(folio, pfn, mf_flags);\n+\t}\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/memory-failure.c b/mm/memory-failure.c\nindex c80c2907da33..79c91d44ec1e 100644\n--- a/mm/memory-failure.c\n+++ b/mm/memory-failure.c\n@@ -2379,6 +2379,15 @@ int memory_failure(unsigned long pfn, int flags)\n \t\tgoto unlock_mutex;\n \t}\n \n+\t/*\n+\t * Notify private-node services about the hardware error so they\n+\t * can update internal tracking (e.g., CXL poison lists, stop\n+\t * demoting to failing DIMMs).  This is notification only -- the\n+\t * kernel proceeds with standard hwpoison handling regardless.\n+\t */\n+\tif (unlikely(page_is_private_managed(p)))\n+\t\tfolio_managed_memory_failure(page_folio(p), pfn, flags);\n+\n \t/*\n \t * We need/can do nothing about count=0 pages.\n \t * 1) it's a free page, and therefore in safe hand:\n@@ -2825,6 +2834,12 @@ static int soft_offline_in_use_page(struct page *page)\n \t\treturn 0;\n \t}\n \n+\tif (!folio_managed_allows_migrate(folio)) {\n+\t\tpr_info(\"%#lx: cannot migrate private node folio\\n\", pfn);\n+\t\tfolio_put(folio);\n+\t\treturn -EBUSY;\n+\t}\n+\n \tisolated = isolate_folio_to_list(folio, &pagelist);\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the ordering of registering private regions and hotplugging memory, explaining that their new function combines these two steps to ensure proper ordering. The function first registers the private region, then hotplugs the memory, and on failure, unregisters the private region. They also added checks for migration support and online status when removing the last of memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a new function for drivers to hotplug memory as N_MEMORY_PRIVATE.\n\nThis function combines node_private_region_register() with\n__add_memory_driver_managed() to ensure proper ordering:\n\n1. Register the private region first (sets private node context)\n2. Then hotplug the memory (sets N_MEMORY_PRIVATE)\n3. On failure, unregister the private region to avoid leaving the\n   node in an inconsistent state.\n\nWhen the last of memory is removed, hotplug also removes the private\nnode context. If migration is not supported and the node is still\nonline, fire a warning (likely bug in the driver).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory_hotplug.h |  11 +++\n include/linux/mmzone.h         |  12 ++++\n mm/memory_hotplug.c            | 122 ++++++++++++++++++++++++++++++---\n 3 files changed, 135 insertions(+), 10 deletions(-)\n\ndiff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h\nindex 1f19f08552ea..e5abade9450a 100644\n--- a/include/linux/memory_hotplug.h\n+++ b/include/linux/memory_hotplug.h\n@@ -293,6 +293,7 @@ extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n extern int remove_memory(u64 start, u64 size);\n extern void __remove_memory(u64 start, u64 size);\n extern int offline_and_remove_memory(u64 start, u64 size);\n+extern int offline_and_remove_private_memory(int nid, u64 start, u64 size);\n \n #else\n static inline void try_offline_node(int nid) {}\n@@ -309,6 +310,12 @@ static inline int remove_memory(u64 start, u64 size)\n }\n \n static inline void __remove_memory(u64 start, u64 size) {}\n+\n+static inline int offline_and_remove_private_memory(int nid, u64 start,\n+\t\t\t\t\t\t    u64 size)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n@@ -326,6 +333,10 @@ int __add_memory_driver_managed(int nid, u64 start, u64 size,\n extern int add_memory_driver_managed(int nid, u64 start, u64 size,\n \t\t\t\t     const char *resource_name,\n \t\t\t\t     mhp_t mhp_flags);\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np);\n extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,\n \t\t\t\t   unsigned long nr_pages,\n \t\t\t\t   struct vmem_altmap *altmap, int migratetype,\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 992eb1c5a2c6..cc532b67ad3f 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1524,6 +1524,18 @@ typedef struct pglist_data {\n #endif\n } pg_data_t;\n \n+#ifdef CONFIG_NUMA\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn pgdat->private;\n+}\n+#else\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn false;\n+}\n+#endif\n+\n #define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n #define node_spanned_pages(nid)\t(NODE_DATA(nid)->node_spanned_pages)\n \ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex d2dc527bd5b0..9d72f44a30dc 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1173,8 +1174,7 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tmove_pfn_range_to_zone(zone, pfn, nr_pages, NULL, MIGRATE_MOVABLE,\n \t\t\t       true);\n \n-\tif (!node_state(nid, N_MEMORY)) {\n-\t\t/* Adding memory to the node for the first time */\n+\tif (!node_state(nid, N_MEMORY) && !node_state(nid, N_MEMORY_PRIVATE)) {\n \t\tnode_arg.nid = nid;\n \t\tret = node_notify(NODE_ADDING_FIRST_MEMORY, &node_arg);\n \t\tret = notifier_to_errno(ret);\n@@ -1208,8 +1208,12 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tonline_pages_range(pfn, nr_pages);\n \tadjust_present_page_count(pfn_to_page(pfn), group, nr_pages);\n \n-\tif (node_arg.nid >= 0)\n-\t\tnode_set_state(nid, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (pgdat_is_private(NODE_DATA(nid)))\n+\t\t\tnode_set_state(nid, N_MEMORY_PRIVATE);\n+\t\telse\n+\t\t\tnode_set_state(nid, N_MEMORY);\n+\t}\n \tif (need_zonelists_rebuild)\n \t\tbuild_all_zonelists(NULL);\n \n@@ -1227,8 +1231,14 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t/* reinitialise watermarks and update pcp limits */\n \tinit_per_zone_wmark_min();\n \n-\tkswapd_run(nid);\n-\tkcompactd_run(nid);\n+\t/*\n+\t * Don't start reclaim/compaction daemons for private nodes.\n+\t * Private node services will decide whether to start these services.\n+\t */\n+\tif (!pgdat_is_private(NODE_DATA(nid))) {\n+\t\tkswapd_run(nid);\n+\t\tkcompactd_run(nid);\n+\t}\n \n \tif (node_arg.nid >= 0)\n \t\t/* First memory added successfully. Notify consumers. */\n@@ -1722,6 +1732,54 @@ int add_memory_driver_managed(int nid, u64 start, u64 size,\n }\n EXPORT_SYMBOL_GPL(add_memory_driver_managed);\n \n+/**\n+ * add_private_memory_driver_managed - add driver-managed N_MEMORY_PRIVATE memory\n+ * @nid: NUMA node ID (or memory group ID when MHP_NID_IS_MGID is set)\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ * @resource_name: \"System RAM ($DRIVER)\" format\n+ * @mhp_flags: Memory hotplug flags\n+ * @online_type: MMOP_* online type\n+ * @np: Driver-owned node_private structure (owner, refcount)\n+ *\n+ * Registers node_private first, then hotplugs the memory.\n+ *\n+ * On failure, unregisters the node_private.\n+ */\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np)\n+{\n+\tstruct memory_group *group;\n+\tint real_nid = nid;\n+\tint rc;\n+\n+\tif (!np)\n+\t\treturn -EINVAL;\n+\n+\tif (mhp_flags & MHP_NID_IS_MGID) {\n+\t\tgroup = memory_group_find_by_id(nid);\n+\t\tif (!group)\n+\t\t\treturn -EINVAL;\n+\t\treal_nid = group->nid;\n+\t}\n+\n+\trc = node_private_register(real_nid, np);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\trc = __add_memory_driver_managed(nid, start, size, resource_name,\n+\t\t\t\t\t mhp_flags, online_type);\n+\tif (rc) {\n+\t\tnode_private_unregister(real_nid);\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(add_private_memory_driver_managed);\n+\n /*\n  * Platforms should define arch_get_mappable_range() that provides\n  * maximum possible addressable physical memory range for which the\n@@ -1872,6 +1930,15 @@ static void do_migrate_range(unsigned long start_pfn, unsigned long end_pfn)\n \t\t\tgoto put_folio;\n \t\t}\n \n+\t\t/* Private nodes w/o migration must ensure folios are offline */\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION)) {\n+\t\t\tWARN_ONCE(1, \"hot-unplug on non-migratable node %d pfn %lx\\n\",\n+\t\t\t\t  folio_nid(folio), pfn);\n+\t\t\tpfn = folio_pfn(folio) + folio_nr_pages(folio) - 1;\n+\t\t\tgoto put_folio;\n+\t\t}\n+\n \t\tif (!isolate_folio_to_list(folio, &source)) {\n \t\t\tif (__ratelimit(&migrate_rs)) {\n \t\t\t\tpr_warn(\"failed to isolate pfn %lx\\n\",\n@@ -2014,8 +2081,8 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \n \t/*\n \t * Check whether the node will have no present pages after we offline\n-\t * 'nr_pages' more. If so, we know that the node will become empty, and\n-\t * so we will clear N_MEMORY for it.\n+\t * 'nr_pages' more. If so, send pre-notification for last memory removal.\n+\t * We will clear N_MEMORY(_PRIVATE) if this is the case.\n \t */\n \tif (nr_pages >= pgdat->node_present_pages) {\n \t\tnode_arg.nid = node;\n@@ -2108,8 +2175,12 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * Make sure to mark the node as memory-less before rebuilding the zone\n \t * list. Otherwise this node would still appear in the fallback lists.\n \t */\n-\tif (node_arg.nid >= 0)\n-\t\tnode_clear_state(node, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (node_state(node, N_MEMORY))\n+\t\t\tnode_clear_state(node, N_MEMORY);\n+\t\telse if (node_state(node, N_MEMORY_PRIVATE))\n+\t\t\tnode_clear_state(node, N_MEMORY_PRIVATE);\n+\t}\n \tif (!populated_zone(zone)) {\n \t\tzone_pcp_reset(zone);\n \t\tbuild_all_zonelists(NULL);\n@@ -2461,4 +2532,35 @@ int offline_and_remove_memory(u64 start, u64 size)\n \treturn rc;\n }\n EXPORT_SYMBOL_GPL(offline_and_remove_memory);\n+\n+/**\n+ * offline_and_remove_private_memory - offline, remove, and unregister private memory\n+ * @nid: NUMA node ID of the private memory\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ *\n+ * Counterpart to add_private_memory_driver_managed().  Offlines and removes\n+ * the memory range, then attempts to unregister the node_private.\n+ *\n+ * offline_and_remove_memory() clears N_MEMORY_PRIVATE when the last block\n+ * is offlined, which allows node_private_unregister() to clear the\n+ * pgdat->node_private pointer.  If other private memory ranges remain on\n+ * the node, node_private_unregister() returns -EBUSY (N_MEMORY_PRIVATE\n+ * is still set) and the node_private remains registered.\n+ *\n+ * Return: 0 on full success (memory removed and node_private unregistered),\n+ *         -EBUSY if memory was removed but node still has other private memory,\n+ *         other negative error code if offline/remove failed.\n+ */\n+int offline_and_remove_private_memory(int nid, u64 start, u64 size)\n+{\n+\tint rc;\n+\n+\trc = offline_and_remove_memory(start, size);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\treturn node_private_unregister(nid);\n+}\n+EXPORT_SYMBOL_GPL(offline_and_remove_private_memory);\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the lack of locking in the swapoff path, acknowledged that the per-vswap spinlock needs to be dropped before calling try_to_unmap(), and agreed to restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CRAM (Compressed RAM) subsystem that manages folios demoted\nto N_MEMORY_PRIVATE nodes via the standard kernel LRU.\n\nWe limit entry into CRAM by demotion in to provide devices a way for\ndrivers to close access - which allows the system to stabiliz under\nmemory pressure (the device can run out of real memory when compression\nratios drop too far).\n\nWe utilize write-protect to prevent unbounded writes to compressed\nmemory pages, which may cause run-away compression ratio loss without\na reliable way to prevent the degenerate case (cascading poisons).\n\nCRAM provides the bridge between the mm/ private node infrastructure\nand compressed memory hardware.  Folios are aged by kswapd on the\nprivate node and reclaimed to swap when the device signals pressure.\n\nWrite faults trigger promotion back to regular DRAM via the\nops->handle_fault callback.\n\nDevice pressure is communicated via watermark_boost on the private\nnode's zone.\n\nCRAM registers node_private_ops with:\n  - handle_fault:   promotes folio back to DRAM on write\n  - migrate_to:     custom demotion to the CRAM node\n  - folio_migrate:  (no-op)\n  - free_folio:     zeroes pages on free to scrub stale data\n  - reclaim_policy: provides mayswap/writeback/boost overrides\n  - flags: NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n\t   NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE\n           NP_OPS_RECLAIM\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cram.h |  66 ++++++\n mm/Kconfig           |  10 +\n mm/Makefile          |   1 +\n mm/cram.c            | 508 +++++++++++++++++++++++++++++++++++++++++++\n 4 files changed, 585 insertions(+)\n create mode 100644 include/linux/cram.h\n create mode 100644 mm/cram.c\n\ndiff --git a/include/linux/cram.h b/include/linux/cram.h\nnew file mode 100644\nindex 000000000000..a3c10362fd4f\n--- /dev/null\n+++ b/include/linux/cram.h\n@@ -0,0 +1,66 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_CRAM_H\n+#define _LINUX_CRAM_H\n+\n+#include <linux/mm_types.h>\n+\n+struct folio;\n+struct list_head;\n+struct vm_fault;\n+\n+#define CRAM_PRESSURE_MAX\t1000\n+\n+/**\n+ * cram_flush_cb_t - Driver callback invoked when a folio on a private node\n+ *                   is freed (refcount reaches zero).\n+ * @folio: the folio being freed\n+ * @private: opaque driver data passed at registration\n+ *\n+ * Return:\n+ *   0: Flush resolved -- page should return to buddy allocator (e.g., flush\n+ *      record bit was set, meaning this free is from our own flush resolution)\n+ *   1: Page deferred -- driver took a reference, page will be flushed later.\n+ *      Do NOT return to buddy allocator.\n+ *   2: Buffer full -- caller should zero the page and return to buddy.\n+ */\n+typedef int (*cram_flush_cb_t)(struct folio *folio, void *private);\n+\n+#ifdef CONFIG_CRAM\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data);\n+int cram_unregister_private_node(int nid);\n+int cram_unpurge(int nid);\n+void cram_set_pressure(int nid, unsigned int pressure);\n+void cram_clear_pressure(int nid);\n+\n+#else /* !CONFIG_CRAM */\n+\n+static inline int cram_register_private_node(int nid, void *owner,\n+\t\t\t\t\t     cram_flush_cb_t flush_cb,\n+\t\t\t\t\t     void *flush_data)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unregister_private_node(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unpurge(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+}\n+\n+static inline void cram_clear_pressure(int nid)\n+{\n+}\n+\n+#endif /* CONFIG_CRAM */\n+\n+#endif /* _LINUX_CRAM_H */\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex bd0ea5454af8..054462b954d8 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -662,6 +662,16 @@ config MIGRATION\n config DEVICE_MIGRATION\n \tdef_bool MIGRATION && ZONE_DEVICE\n \n+config CRAM\n+\tbool \"Compressed RAM - private node memory management\"\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\tdepends on MEMORY_HOTPLUG\n+\thelp\n+\t  Enables management of N_MEMORY_PRIVATE nodes for compressed RAM\n+\t  and similar use cases. Provides demotion, promotion, and lifecycle\n+\t  management for private memory nodes.\n+\n config ARCH_ENABLE_HUGEPAGE_MIGRATION\n \tbool\n \ndiff --git a/mm/Makefile b/mm/Makefile\nindex 2d0570a16e5b..0e1421512643 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -98,6 +98,7 @@ obj-$(CONFIG_MEMTEST)\t\t+= memtest.o\n obj-$(CONFIG_MIGRATION) += migrate.o\n obj-$(CONFIG_NUMA) += memory-tiers.o\n obj-$(CONFIG_DEVICE_MIGRATION) += migrate_device.o\n+obj-$(CONFIG_CRAM) += cram.o\n obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o\n obj-$(CONFIG_PAGE_COUNTER) += page_counter.o\n obj-$(CONFIG_LIVEUPDATE) += memfd_luo.o\ndiff --git a/mm/cram.c b/mm/cram.c\nnew file mode 100644\nindex 000000000000..6709e61f5b9d\n--- /dev/null\n+++ b/mm/cram.c\n@@ -0,0 +1,508 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * mm/cram.c - Compressed RAM / private node memory management\n+ *\n+ * Copyright 2026 Meta Technologies Inc.\n+ *   Author: Gregory Price <gourry@gourry.net>\n+ *\n+ * Manages folios demoted to N_MEMORY_PRIVATE nodes via the standard kernel\n+ * LRU.  Folios are aged by kswapd on the private node and reclaimed to swap\n+ * (demotion is suppressed for private nodes).  Write faults trigger promotion\n+ * back to regular DRAM via the ops->handle_fault callback.\n+ *\n+ * All reclaim/demotion uses the standard vmscan infrastructure. Device pressure\n+ * is communicated via watermark_boost on the private node's zone.\n+ */\n+\n+#include <linux/atomic.h>\n+#include <linux/cpuset.h>\n+#include <linux/cram.h>\n+#include <linux/errno.h>\n+#include <linux/gfp.h>\n+#include <linux/jiffies.h>\n+#include <linux/highmem.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/list.h>\n+#include <linux/migrate.h>\n+#include <linux/mm.h>\n+#include <linux/huge_mm.h>\n+#include <linux/mmzone.h>\n+#include <linux/mutex.h>\n+#include <linux/nodemask.h>\n+#include <linux/node_private.h>\n+#include <linux/pagemap.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+#include <linux/swap.h>\n+\n+#include \"internal.h\"\n+\n+struct cram_node {\n+\tvoid\t\t*owner;\n+\tbool\t\tpurged;\t\t/* node is being torn down */\n+\tunsigned int\tpressure;\n+\trefcount_t\trefcount;\n+\tcram_flush_cb_t\tflush_cb;\t/* optional driver flush callback */\n+\tvoid\t\t*flush_data;\t/* opaque data for flush_cb */\n+};\n+\n+static struct cram_node *cram_nodes[MAX_NUMNODES];\n+static DEFINE_MUTEX(cram_mutex);\n+\n+static inline bool cram_valid_nid(int nid)\n+{\n+\treturn nid >= 0 && nid < MAX_NUMNODES;\n+}\n+\n+static inline struct cram_node *get_cram_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn NULL;\n+\n+\trcu_read_lock();\n+\tcn = rcu_dereference(cram_nodes[nid]);\n+\tif (cn && !refcount_inc_not_zero(&cn->refcount))\n+\t\tcn = NULL;\n+\trcu_read_unlock();\n+\n+\treturn cn;\n+}\n+\n+static inline void put_cram_node(struct cram_node *cn)\n+{\n+\tif (cn)\n+\t\trefcount_dec(&cn->refcount);\n+}\n+\n+static void cram_zero_folio(struct folio *folio)\n+{\n+\tunsigned int i, nr = folio_nr_pages(folio);\n+\n+\tif (want_init_on_free())\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr; i++)\n+\t\tclear_highpage(folio_page(folio, i));\n+}\n+\n+static bool cram_free_folio_cb(struct folio *folio)\n+{\n+\tint nid = folio_nid(folio);\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\tgoto zero_and_free;\n+\n+\tif (!cn->flush_cb)\n+\t\tgoto zero_and_free_put;\n+\n+\tret = cn->flush_cb(folio, cn->flush_data);\n+\tput_cram_node(cn);\n+\n+\tswitch (ret) {\n+\tcase 0:\n+\t\t/* Flush resolved: return to buddy (already zeroed by device) */\n+\t\treturn false;\n+\tcase 1:\n+\t\t/* Deferred: driver holds a ref, do not free to buddy */\n+\t\treturn true;\n+\tcase 2:\n+\tdefault:\n+\t\t/* Buffer full or unknown: zero locally, return to buddy */\n+\t\tgoto zero_and_free;\n+\t}\n+\n+zero_and_free_put:\n+\tput_cram_node(cn);\n+zero_and_free:\n+\tcram_zero_folio(folio);\n+\treturn false;\n+}\n+\n+static struct folio *alloc_cram_folio(struct folio *src, unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_PRIVATE | __GFP_KSWAPD_RECLAIM |\n+\t\t     __GFP_HIGHMEM | __GFP_MOVABLE |\n+\t\t     __GFP_NOWARN | __GFP_NORETRY;\n+\n+\t/* Stop allocating if backpressure fired mid-batch */\n+\tif (node_private_migration_blocked(nid))\n+\t\treturn NULL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc_node(gfp, order, nid);\n+}\n+\n+static void cram_put_new_folio(struct folio *folio, unsigned long private)\n+{\n+\tcram_zero_folio(folio);\n+\tfolio_put(folio);\n+}\n+\n+/*\n+ * Allocate a DRAM folio for promotion out of a private node.\n+ *\n+ * Unlike alloc_migration_target(), this does NOT strip __GFP_RECLAIM for\n+ * large folios, the generic helper does that because THP allocations are\n+ * opportunistic, but promotion from a private node is mandatory: the page\n+ * MUST move to DRAM or the process cannot make forward progress.\n+ *\n+ * __GFP_RETRY_MAYFAIL tells the allocator to try hard (multiple reclaim\n+ * rounds, wait for writeback) before giving up.\n+ */\n+static struct folio *alloc_cram_promote_folio(struct folio *src,\n+\t\t\t\t\t      unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc(gfp, order, nid, NULL);\n+}\n+\n+static int cram_migrate_to(struct list_head *demote_folios, int to_nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason,\n+\t\t\t   unsigned int *nr_succeeded)\n+{\n+\tstruct cram_node *cn;\n+\tunsigned int nr_success = 0;\n+\tint ret = 0;\n+\n+\tcn = get_cram_node(to_nid);\n+\tif (!cn)\n+\t\treturn -ENODEV;\n+\n+\tif (cn->purged) {\n+\t\tret = -ENODEV;\n+\t\tgoto out;\n+\t}\n+\n+\t/* Block new demotions at maximum pressure */\n+\tif (READ_ONCE(cn->pressure) >= CRAM_PRESSURE_MAX) {\n+\t\tret = -ENOSPC;\n+\t\tgoto out;\n+\t}\n+\n+\tret = migrate_pages(demote_folios, alloc_cram_folio, cram_put_new_folio,\n+\t\t\t    (unsigned long)to_nid, mode, reason,\n+\t\t\t    &nr_success);\n+\n+\t/*\n+\t * migrate_folio_move() calls folio_add_lru() for each migrated\n+\t * folio, but that only adds the folio to a per-CPU batch, \n+\t * PG_lru is not set until the batch is drained.  Drain now so\n+\t * that cram_fault() can isolate these folios immediately.\n+\t *\n+\t * Use lru_add_drain_all() because migrate_pages() may process\n+\t * folios across CPUs, and the local drain might miss batches\n+\t * filled on other CPUs.\n+\t */\n+\tif (nr_success)\n+\t\tlru_add_drain_all();\n+out:\n+\tput_cram_node(cn);\n+\tif (nr_succeeded)\n+\t\t*nr_succeeded = nr_success;\n+\treturn ret;\n+}\n+\n+static void cram_release_ptl(struct vm_fault *vmf, enum pgtable_level level)\n+{\n+\tif (level == PGTABLE_LEVEL_PTE)\n+\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n+\telse\n+\t\tspin_unlock(vmf->ptl);\n+}\n+\n+static vm_fault_t cram_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t     enum pgtable_level level)\n+{\n+\tstruct folio *f, *f2;\n+\tstruct cram_node *cn;\n+\tunsigned int nr_succeeded = 0;\n+\tint nid;\n+\tLIST_HEAD(folios);\n+\n+\tnid = folio_nid(folio);\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn) {\n+\t\tcram_release_ptl(vmf, level);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Isolate from LRU while holding PTL.  This serializes against\n+\t * other CPUs faulting on the same folio: only one CPU can clear\n+\t * PG_lru under the PTL, and it proceeds to migration.  Other\n+\t * CPUs find the folio already isolated and bail out, preventing\n+\t * the refcount pile-up that causes migrate_pages() to fail with\n+\t * -EAGAIN.\n+\t *\n+\t * No explicit folio_get() is needed: the page table entry holds\n+\t * a reference (we still hold PTL), and folio_isolate_lru() takes\n+\t * its own reference.  This matches do_numa_page()'s pattern.\n+\t *\n+\t * PG_lru should already be set: cram_migrate_to() drains per-CPU\n+\t * LRU batches after migration, and the failure path below\n+\t * drains after putback.\n+\t */\n+\tif (!folio_isolate_lru(folio)) {\n+\t\tput_cram_node(cn);\n+\t\tcram_release_ptl(vmf, level);\n+\t\tcond_resched();\n+\t\treturn 0;\n+\t}\n+\n+\t/* Folio isolated, release PTL, proceed to migration */\n+\tcram_release_ptl(vmf, level);\n+\n+\tnode_stat_mod_folio(folio,\n+\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(folio),\n+\t\t\t    folio_nr_pages(folio));\n+\tlist_add(&folio->lru, &folios);\n+\n+\tmigrate_pages(&folios, alloc_cram_promote_folio, NULL,\n+\t\t      (unsigned long)numa_node_id(),\n+\t\t      MIGRATE_SYNC, MR_NUMA_MISPLACED, &nr_succeeded);\n+\n+\t/* Put failed folios back on LRU; retry on next fault */\n+\tlist_for_each_entry_safe(f, f2, &folios, lru) {\n+\t\tlist_del(&f->lru);\n+\t\tnode_stat_mod_folio(f,\n+\t\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(f),\n+\t\t\t\t    -folio_nr_pages(f));\n+\t\tfolio_putback_lru(f);\n+\t}\n+\n+\t/*\n+\t * If migration failed, folio_putback_lru() batched the folio\n+\t * into this CPU's per-CPU LRU cache (PG_lru not yet set).\n+\t * Drain now so the folio is immediately visible on the LRU,\n+\t * the next fault can then isolate it without an IPI storm\n+\t * via lru_add_drain_all().\n+\t *\n+\t * Return VM_FAULT_RETRY after releasing the fault lock so the\n+\t * arch handler retries from scratch.  Without this, returning 0\n+\t * causes a tight livelock: the process immediately re-faults on\n+\t * the same write-protected entry, alloc fails again, and\n+\t * VM_FAULT_OOM eventually leaks out through a stale path.\n+\t * VM_FAULT_RETRY gives the system breathing room to reclaim.\n+\t */\n+\tif (!nr_succeeded) {\n+\t\tlru_add_drain();\n+\t\tcond_resched();\n+\t\tput_cram_node(cn);\n+\t\trelease_fault_lock(vmf);\n+\t\treturn VM_FAULT_RETRY;\n+\t}\n+\n+\tcond_resched();\n+\tput_cram_node(cn);\n+\treturn 0;\n+}\n+\n+static void cram_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static void cram_reclaim_policy(int nid, struct node_reclaim_policy *policy)\n+{\n+\tpolicy->may_swap = true;\n+\tpolicy->may_writepage = true;\n+\tpolicy->managed_watermarks = true;\n+}\n+\n+static vm_fault_t cram_handle_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t    enum pgtable_level level)\n+{\n+\treturn cram_fault(folio, vmf, level);\n+}\n+\n+static const struct node_private_ops cram_ops = {\n+\t.handle_fault\t\t= cram_handle_fault,\n+\t.migrate_to\t\t= cram_migrate_to,\n+\t.folio_migrate\t\t= cram_folio_migrate,\n+\t.free_folio\t\t= cram_free_folio_cb,\n+\t.reclaim_policy\t\t= cram_reclaim_policy,\n+\t.flags\t\t\t= NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n+\t\t\t\t  NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE |\n+\t\t\t\t  NP_OPS_RECLAIM,\n+};\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data)\n+{\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (cn) {\n+\t\tif (cn->owner != owner) {\n+\t\t\tmutex_unlock(&cram_mutex);\n+\t\t\treturn -EBUSY;\n+\t\t}\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn 0;\n+\t}\n+\n+\tcn = kzalloc(sizeof(*cn), GFP_KERNEL);\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENOMEM;\n+\t}\n+\n+\tcn->owner = owner;\n+\tcn->pressure = 0;\n+\tcn->flush_cb = flush_cb;\n+\tcn->flush_data = flush_data;\n+\trefcount_set(&cn->refcount, 1);\n+\n+\tret = node_private_set_ops(nid, &cram_ops);\n+\tif (ret) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\tkfree(cn);\n+\t\treturn ret;\n+\t}\n+\n+\trcu_assign_pointer(cram_nodes[nid], cn);\n+\n+\t/* Start kswapd on the private node for LRU aging and reclaim */\n+\tkswapd_run(nid);\n+\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* Now that ops->migrate_to is set, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_register_private_node);\n+\n+int cram_unregister_private_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tkswapd_stop(nid);\n+\n+\tWARN_ON(node_private_clear_ops(nid, &cram_ops));\n+\trcu_assign_pointer(cram_nodes[nid], NULL);\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* ops->migrate_to cleared, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\n+\tsynchronize_rcu();\n+\twhile (!refcount_dec_if_one(&cn->refcount))\n+\t\tcond_resched();\n+\tkfree(cn);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unregister_private_node);\n+\n+int cram_unpurge(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tcn->purged = false;\n+\n+\tmutex_unlock(&cram_mutex);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unpurge);\n+\n+void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+\tstruct cram_node *cn;\n+\tstruct node_private *np;\n+\tstruct zone *zone;\n+\tunsigned long managed, boost;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\treturn;\n+\n+\tif (pressure > CRAM_PRESSURE_MAX)\n+\t\tpressure = CRAM_PRESSURE_MAX;\n+\n+\tWRITE_ONCE(cn->pressure, pressure);\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\t/* Block demotions only at maximum pressure */\n+\tif (np)\n+\t\tWRITE_ONCE(np->migration_blocked,\n+\t\t\t   pressure >= CRAM_PRESSURE_MAX);\n+\trcu_read_unlock();\n+\n+\tzone = NULL;\n+\tfor (int i = 0; i < MAX_NR_ZONES; i++) {\n+\t\tstruct zone *z = &NODE_DATA(nid)->node_zones[i];\n+\n+\t\tif (zone_managed_pages(z) > 0) {\n+\t\t\tzone = z;\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+\tif (!zone) {\n+\t\tput_cram_node(cn);\n+\t\treturn;\n+\t}\n+\tmanaged = zone_managed_pages(zone);\n+\n+\t/* Boost proportional to pressure. 0:no boost, 1000:full managed */\n+\tboost = (managed * (unsigned long)pressure) / CRAM_PRESSURE_MAX;\n+\tWRITE_ONCE(zone->watermark_boost, boost);\n+\n+\tif (boost) {\n+\t\tset_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);\n+\t\twakeup_kswapd(zone, GFP_KERNEL, 0, ZONE_MOVABLE);\n+\t}\n+\n+\tput_cram_node(cn);\n+}\n+EXPORT_SYMBOL_GPL(cram_set_pressure);\n+\n+void cram_clear_pressure(int nid)\n+{\n+\tcram_set_pressure(nid, 0);\n+}\n+EXPORT_SYMBOL_GPL(cram_clear_pressure);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the need for a sysram region to directly perform memory hotplug operations, which would eliminate the intermediate dax_region/dax device layer. The author agrees that this feature is necessary and explains how it will work, including its key features such as supporting memory tier integration and automatically hotplugging memory on probe if online type is configured.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed to implement a new feature",
                "explained the benefits of the feature"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CXL sysram region for direct memory hotplug of CXL RAM regions.\n\nThis region eliminates the intermediate dax_region/dax device layer by\ndirectly performing memory hotplug operations.\n\nKey features:\n- Supports memory tier integration for proper NUMA placement\n- Uses the CXL_SYSRAM_ONLINE_* Kconfig options for default online type\n- Automatically hotplugs memory on probe if online type is configured\n- Will be extended to support private memory nodes in the future\n\nThe driver registers a sysram_regionN device as a child of the CXL\nregion, managing the memory hotplug lifecycle through device add/remove.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/Makefile        |   1 +\n drivers/cxl/core/core.h          |   4 +\n drivers/cxl/core/port.c          |   2 +\n drivers/cxl/core/region_sysram.c | 351 +++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h                |  48 +++++\n 5 files changed, 406 insertions(+)\n create mode 100644 drivers/cxl/core/region_sysram.c\n\ndiff --git a/drivers/cxl/core/Makefile b/drivers/cxl/core/Makefile\nindex d3ec8aea64c5..d7ce52c50810 100644\n--- a/drivers/cxl/core/Makefile\n+++ b/drivers/cxl/core/Makefile\n@@ -18,6 +18,7 @@ cxl_core-$(CONFIG_TRACING) += trace.o\n cxl_core-$(CONFIG_CXL_REGION) += region.o\n cxl_core-$(CONFIG_CXL_REGION) += region_dax.o\n cxl_core-$(CONFIG_CXL_REGION) += region_pmem.o\n+cxl_core-$(CONFIG_CXL_REGION) += region_sysram.o\n cxl_core-$(CONFIG_CXL_MCE) += mce.o\n cxl_core-$(CONFIG_CXL_FEATURES) += features.o\n cxl_core-$(CONFIG_CXL_EDAC_MEM_FEATURES) += edac.o\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 6e1f695fd155..973bbcae43f7 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -35,6 +35,7 @@ extern struct device_attribute dev_attr_delete_region;\n extern struct device_attribute dev_attr_region;\n extern const struct device_type cxl_pmem_region_type;\n extern const struct device_type cxl_dax_region_type;\n+extern const struct device_type cxl_sysram_type;\n extern const struct device_type cxl_region_type;\n \n int cxl_decoder_detach(struct cxl_region *cxlr,\n@@ -46,6 +47,7 @@ int cxl_decoder_detach(struct cxl_region *cxlr,\n #define SET_CXL_REGION_ATTR(x) (&dev_attr_##x.attr),\n #define CXL_PMEM_REGION_TYPE(x) (&cxl_pmem_region_type)\n #define CXL_DAX_REGION_TYPE(x) (&cxl_dax_region_type)\n+#define CXL_SYSRAM_TYPE(x) (&cxl_sysram_type)\n int cxl_region_init(void);\n void cxl_region_exit(void);\n int cxl_get_poison_by_endpoint(struct cxl_port *port);\n@@ -54,6 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\n@@ -88,6 +91,7 @@ static inline void cxl_region_exit(void)\n #define SET_CXL_REGION_ATTR(x)\n #define CXL_PMEM_REGION_TYPE(x) NULL\n #define CXL_DAX_REGION_TYPE(x) NULL\n+#define CXL_SYSRAM_TYPE(x) NULL\n #endif\n \n struct cxl_send_command;\ndiff --git a/drivers/cxl/core/port.c b/drivers/cxl/core/port.c\nindex 5c82e6f32572..d6e82b3c2b64 100644\n--- a/drivers/cxl/core/port.c\n+++ b/drivers/cxl/core/port.c\n@@ -66,6 +66,8 @@ static int cxl_device_id(const struct device *dev)\n \t\treturn CXL_DEVICE_PMEM_REGION;\n \tif (dev->type == CXL_DAX_REGION_TYPE())\n \t\treturn CXL_DEVICE_DAX_REGION;\n+\tif (dev->type == CXL_SYSRAM_TYPE())\n+\t\treturn CXL_DEVICE_SYSRAM;\n \tif (is_cxl_port(dev)) {\n \t\tif (is_cxl_root(to_cxl_port(dev)))\n \t\t\treturn CXL_DEVICE_ROOT;\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nnew file mode 100644\nindex 000000000000..47a415deb352\n--- /dev/null\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -0,0 +1,351 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Sysram Region - Direct memory hotplug for CXL RAM regions\n+ *\n+ * This interface directly performs memory hotplug for CXL RAM regions,\n+ * eliminating the indirection through DAX.\n+ */\n+\n+#include <linux/memory_hotplug.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/memory.h>\n+#include <linux/device.h>\n+#include <linux/slab.h>\n+#include <linux/mm.h>\n+#include <cxlmem.h>\n+#include <cxl.h>\n+#include \"core.h\"\n+\n+static const char *sysram_res_name = \"System RAM (CXL)\";\n+\n+/**\n+ * cxl_region_find_sysram - Find the sysram device associated with a region\n+ * @cxlr: The CXL region\n+ *\n+ * Finds and returns the sysram child device of a CXL region.\n+ * The caller must release the device reference with put_device()\n+ * when done with the returned pointer.\n+ *\n+ * Return: Pointer to cxl_sysram, or NULL if not found\n+ */\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram;\n+\tstruct device *sdev;\n+\tchar sname[32];\n+\n+\tsnprintf(sname, sizeof(sname), \"sysram_region%d\", cxlr->id);\n+\tsdev = device_find_child_by_name(&cxlr->dev, sname);\n+\tif (!sdev)\n+\t\treturn NULL;\n+\n+\tsysram = to_cxl_sysram(sdev);\n+\treturn sysram;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_region_find_sysram, \"CXL\");\n+\n+static int sysram_get_numa_node(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_region_params *p = &cxlr->params;\n+\tint nid;\n+\n+\tnid = phys_to_target_node(p->res->start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(p->res->start);\n+\n+\treturn nid;\n+}\n+\n+static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n+{\n+\tstruct resource *res;\n+\tmhp_t mhp_flags;\n+\tint rc;\n+\n+\tif (sysram->res)\n+\t\treturn -EBUSY;\n+\n+\tres = request_mem_region(sysram->hpa_range.start,\n+\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t sysram->res_name);\n+\tif (!res)\n+\t\treturn -EBUSY;\n+\n+\tsysram->res = res;\n+\n+\t/*\n+\t * Set flags appropriate for System RAM. Leave ..._BUSY clear\n+\t * so that add_memory() can add a child resource.\n+\t */\n+\tres->flags = IORESOURCE_SYSTEM_RAM;\n+\n+\tmhp_flags = MHP_NID_IS_MGID;\n+\n+\t/*\n+\t * Ensure that future kexec'd kernels will not treat\n+\t * this as RAM automatically.\n+\t */\n+\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t online_type);\n+\tif (rc) {\n+\t\tremove_resource(res);\n+\t\tkfree(res);\n+\t\tsysram->res = NULL;\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n+{\n+\tint rc;\n+\n+\tif (!sysram->res)\n+\t\treturn 0;\n+\n+\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t       range_len(&sysram->hpa_range));\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tif (sysram->res) {\n+\t\tremove_resource(sysram->res);\n+\t\tkfree(sysram->res);\n+\t\tsysram->res = NULL;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn sysram_hotplug_remove(sysram);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_offline_and_remove, \"CXL\");\n+\n+static void cxl_sysram_release(struct device *dev)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\n+\tif (sysram->res)\n+\t\tsysram_hotplug_remove(sysram);\n+\n+\tkfree(sysram->res_name);\n+\n+\tif (sysram->mgid >= 0)\n+\t\tmemory_group_unregister(sysram->mgid);\n+\n+\tif (sysram->mtype)\n+\t\tclear_node_memory_type(sysram->numa_node, sysram->mtype);\n+\n+\tkfree(sysram);\n+}\n+\n+static ssize_t hotplug_store(struct device *dev,\n+\t\t\t     struct device_attribute *attr,\n+\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\tint online_type, rc;\n+\n+\tonline_type = mhp_online_type_from_str(buf);\n+\tif (online_type < 0)\n+\t\treturn online_type;\n+\n+\tif (online_type == MMOP_OFFLINE)\n+\t\trc = sysram_hotplug_remove(sysram);\n+\telse\n+\t\trc = sysram_hotplug_add(sysram, online_type);\n+\n+\tif (rc)\n+\t\tdev_warn(dev, \"hotplug %s failed: %d\\n\",\n+\t\t\t online_type == MMOP_OFFLINE ? \"offline\" : \"online\", rc);\n+\n+\treturn rc ? rc : len;\n+}\n+static DEVICE_ATTR_WO(hotplug);\n+\n+static struct attribute *cxl_sysram_attrs[] = {\n+\t&dev_attr_hotplug.attr,\n+\tNULL\n+};\n+\n+static const struct attribute_group cxl_sysram_attribute_group = {\n+\t.attrs = cxl_sysram_attrs,\n+};\n+\n+static const struct attribute_group *cxl_sysram_attribute_groups[] = {\n+\t&cxl_base_attribute_group,\n+\t&cxl_sysram_attribute_group,\n+\tNULL\n+};\n+\n+const struct device_type cxl_sysram_type = {\n+\t.name = \"cxl_sysram\",\n+\t.release = cxl_sysram_release,\n+\t.groups = cxl_sysram_attribute_groups,\n+};\n+\n+static bool is_cxl_sysram(struct device *dev)\n+{\n+\treturn dev->type == &cxl_sysram_type;\n+}\n+\n+struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\tif (dev_WARN_ONCE(dev, !is_cxl_sysram(dev),\n+\t\t\t  \"not a cxl_sysram device\\n\"))\n+\t\treturn NULL;\n+\treturn container_of(dev, struct cxl_sysram, dev);\n+}\n+EXPORT_SYMBOL_NS_GPL(to_cxl_sysram, \"CXL\");\n+\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram)\n+{\n+\treturn &sysram->dev;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_dev, \"CXL\");\n+\n+static struct lock_class_key cxl_sysram_key;\n+\n+static enum mmop cxl_sysram_get_default_online_type(void)\n+{\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_SYSTEM_DEFAULT))\n+\t\treturn mhp_get_default_online_type();\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_MOVABLE))\n+\t\treturn MMOP_ONLINE_MOVABLE;\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_NORMAL))\n+\t\treturn MMOP_ONLINE;\n+\treturn MMOP_OFFLINE;\n+}\n+\n+static struct cxl_sysram *cxl_sysram_alloc(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram __free(kfree) = NULL;\n+\tstruct device *dev;\n+\n+\tsysram = kzalloc(sizeof(*sysram), GFP_KERNEL);\n+\tif (!sysram)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tsysram->online_type = cxl_sysram_get_default_online_type();\n+\tsysram->last_hotplug_cmd = MMOP_OFFLINE;\n+\tsysram->numa_node = -1;\n+\tsysram->mgid = -1;\n+\n+\tdev = &sysram->dev;\n+\tsysram->cxlr = cxlr;\n+\tdevice_initialize(dev);\n+\tlockdep_set_class(&dev->mutex, &cxl_sysram_key);\n+\tdevice_set_pm_not_required(dev);\n+\tdev->parent = &cxlr->dev;\n+\tdev->bus = &cxl_bus_type;\n+\tdev->type = &cxl_sysram_type;\n+\n+\treturn_ptr(sysram);\n+}\n+\n+static void sysram_unregister(void *_sysram)\n+{\n+\tstruct cxl_sysram *sysram = _sysram;\n+\n+\tdevice_unregister(&sysram->dev);\n+}\n+\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+{\n+\tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n+\tstruct memory_dev_type *mtype;\n+\tstruct range hpa_range;\n+\tstruct device *dev;\n+\tint adist = MEMTIER_DEFAULT_LOWTIER_ADISTANCE;\n+\tint numa_node;\n+\tint rc;\n+\n+\trc = cxl_region_get_hpa_range(cxlr, &hpa_range);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\thpa_range = memory_block_align_range(&hpa_range);\n+\tif (hpa_range.start >= hpa_range.end) {\n+\t\tdev_warn(&cxlr->dev, \"region too small after alignment\\n\");\n+\t\treturn -ENOSPC;\n+\t}\n+\n+\tsysram = cxl_sysram_alloc(cxlr);\n+\tif (IS_ERR(sysram))\n+\t\treturn PTR_ERR(sysram);\n+\n+\tsysram->hpa_range = hpa_range;\n+\n+\tsysram->res_name = kasprintf(GFP_KERNEL, \"cxl_sysram%d\", cxlr->id);\n+\tif (!sysram->res_name)\n+\t\treturn -ENOMEM;\n+\n+\t/* Override default online type if caller specified one */\n+\tif (online_type >= 0)\n+\t\tsysram->online_type = online_type;\n+\n+\tdev = &sysram->dev;\n+\n+\trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Setup memory tier before adding device */\n+\tnuma_node = sysram_get_numa_node(cxlr);\n+\tif (numa_node < 0) {\n+\t\tdev_warn(&cxlr->dev, \"rejecting region with invalid node: %d\\n\",\n+\t\t\t numa_node);\n+\t\treturn -EINVAL;\n+\t}\n+\tsysram->numa_node = numa_node;\n+\n+\tmt_calc_adistance(numa_node, &adist);\n+\tmtype = mt_get_memory_type(adist);\n+\tif (IS_ERR(mtype))\n+\t\treturn PTR_ERR(mtype);\n+\tsysram->mtype = mtype;\n+\n+\tinit_node_memory_type(numa_node, mtype);\n+\n+\t/* Register memory group for this region */\n+\trc = memory_group_register_static(numa_node,\n+\t\t\t\t\t  PFN_UP(range_len(&hpa_range)));\n+\tif (rc < 0)\n+\t\treturn rc;\n+\tsysram->mgid = rc;\n+\n+\trc = device_add(dev);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n+\t\tdev_name(dev));\n+\n+\t/*\n+\t * Dynamic capacity regions (DCD) will have memory added later.\n+\t * For static RAM regions, hotplug the entire range now.\n+\t */\n+\tif (cxlr->mode != CXL_PARTMODE_RAM)\n+\t\tgoto out;\n+\n+\t/* If default online_type is a valid online mode, immediately hotplug */\n+\tif (sysram->online_type > MMOP_OFFLINE) {\n+\t\trc = sysram_hotplug_add(sysram, sysram->online_type);\n+\t\tif (rc)\n+\t\t\tdev_warn(dev, \"hotplug failed: %d\\n\", rc);\n+\t\telse\n+\t\t\tsysram->last_hotplug_cmd = sysram->online_type;\n+\t}\n+\n+out:\n+\treturn devm_add_action_or_reset(&cxlr->dev, sysram_unregister,\n+\t\t\t\t\tno_free_ptr(sysram));\n+}\n+EXPORT_SYMBOL_NS_GPL(devm_cxl_add_sysram, \"CXL\");\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex f899f240f229..8e8342fd4fde 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -607,6 +607,34 @@ struct cxl_dax_region {\n \tenum dax_driver_type dax_driver;\n };\n \n+/**\n+ * struct cxl_sysram - CXL SysRAM region for system memory hotplug\n+ * @dev: device for this sysram\n+ * @cxlr: parent cxl_region\n+ * @online_type: Default memory online type for new hotplug ops (MMOP_* value)\n+ * @last_hotplug_cmd: Last hotplug command submitted (MMOP_* value)\n+ * @hpa_range: Host physical address range for the region\n+ * @res_name: Resource name for the memory region\n+ * @res: Memory resource (set when hotplugged)\n+ * @mgid: Memory group id\n+ * @mtype: Memory tier type\n+ * @numa_node: NUMA node for this memory\n+ *\n+ * Device that directly performs memory hotplug for CXL RAM regions.\n+ */\n+struct cxl_sysram {\n+\tstruct device dev;\n+\tstruct cxl_region *cxlr;\n+\tenum mmop online_type;\n+\tint last_hotplug_cmd;\n+\tstruct range hpa_range;\n+\tconst char *res_name;\n+\tstruct resource *res;\n+\tint mgid;\n+\tstruct memory_dev_type *mtype;\n+\tint numa_node;\n+};\n+\n /**\n  * struct cxl_port - logical collection of upstream port devices and\n  *\t\t     downstream port devices to construct a CXL memory\n@@ -807,6 +835,7 @@ DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device\n DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n DEFINE_FREE(put_cxl_dax_region, struct cxl_dax_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n+DEFINE_FREE(put_cxl_sysram, struct cxl_sysram *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n \n int devm_cxl_enumerate_ports(struct cxl_memdev *cxlmd);\n void cxl_bus_rescan(void);\n@@ -889,6 +918,7 @@ void cxl_destroy_region(struct cxl_region *cxlr);\n struct device *cxl_region_dev(struct cxl_region *cxlr);\n enum cxl_partition_mode cxl_region_mode(struct cxl_region *cxlr);\n int cxl_get_region_range(struct cxl_region *cxlr, struct range *range);\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr);\n int cxl_get_committed_regions(struct cxl_memdev *cxlmd,\n \t\t\t      struct cxl_region **regions, int max_regions);\n struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n@@ -936,6 +966,7 @@ void cxl_driver_unregister(struct cxl_driver *cxl_drv);\n #define CXL_DEVICE_PMEM_REGION\t\t7\n #define CXL_DEVICE_DAX_REGION\t\t8\n #define CXL_DEVICE_PMU\t\t\t9\n+#define CXL_DEVICE_SYSRAM\t\t10\n \n #define MODULE_ALIAS_CXL(type) MODULE_ALIAS(\"cxl:t\" __stringify(type) \"*\")\n #define CXL_MODALIAS_FMT \"cxl:t%d\"\n@@ -954,6 +985,10 @@ bool is_cxl_pmem_region(struct device *dev);\n struct cxl_pmem_region *to_cxl_pmem_region(struct device *dev);\n int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n+struct cxl_sysram *to_cxl_sysram(struct device *dev);\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n static inline bool is_cxl_pmem_region(struct device *dev)\n@@ -972,6 +1007,19 @@ static inline struct cxl_dax_region *to_cxl_dax_region(struct device *dev)\n {\n \treturn NULL;\n }\n+static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\treturn NULL;\n+}\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+\t\t\t\t      enum mmop online_type)\n+{\n+\treturn -ENXIO;\n+}\n+static inline int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn -ENXIO;\n+}\n static inline u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint,\n \t\t\t\t\t       u64 spa)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private memory regions being isolated from normal allocations and reclaim by adding support for N_MEMORY_PRIVATE hotplug via add_private_memory_driver_managed(). They modified the cxl_sysram region to register as a private node when private=true is passed to devm_cxl_add_sysram(), allowing callers to isolate their memory. A fix is planned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "planned a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Extend the cxl_sysram region to support N_MEMORY_PRIVATE hotplug\nvia add_private_memory_driver_managed(). When a caller passes\nprivate=true to devm_cxl_add_sysram(), the memory is registered\nas a private node, isolating it from normal allocations and reclaim.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/core.h          |  2 +-\n drivers/cxl/core/region_sysram.c | 50 +++++++++++++++++++++++++-------\n drivers/cxl/cxl.h                |  9 ++++--\n 3 files changed, 48 insertions(+), 13 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 973bbcae43f7..8ca3d6d41fe4 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -56,7 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nindex 47a415deb352..77aaa52e7332 100644\n--- a/drivers/cxl/core/region_sysram.c\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -85,12 +85,23 @@ static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n \t/*\n \t * Ensure that future kexec'd kernels will not treat\n \t * this as RAM automatically.\n+\t *\n+\t * For private regions, use add_private_memory_driver_managed()\n+\t * to register as N_MEMORY_PRIVATE which isolates the memory from\n+\t * normal allocations and reclaim.\n \t */\n-\trc = __add_memory_driver_managed(sysram->mgid,\n-\t\t\t\t\t sysram->hpa_range.start,\n-\t\t\t\t\t range_len(&sysram->hpa_range),\n-\t\t\t\t\t sysram_res_name, mhp_flags,\n-\t\t\t\t\t online_type);\n+\tif (sysram->private)\n+\t\trc = add_private_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t       sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t       online_type, &sysram->np);\n+\telse\n+\t\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t online_type);\n \tif (rc) {\n \t\tremove_resource(res);\n \t\tkfree(res);\n@@ -108,10 +119,23 @@ static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n \tif (!sysram->res)\n \t\treturn 0;\n \n-\trc = offline_and_remove_memory(sysram->hpa_range.start,\n-\t\t\t\t       range_len(&sysram->hpa_range));\n-\tif (rc)\n-\t\treturn rc;\n+\tif (sysram->private) {\n+\t\trc = offline_and_remove_private_memory(sysram->numa_node,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\t/*\n+\t\t * -EBUSY means memory was removed but node_private_unregister()\n+\t\t * could not complete because other regions share the node.\n+\t\t * Continue to resource cleanup since the memory is gone.\n+\t\t */\n+\t\tif (rc && rc != -EBUSY)\n+\t\t\treturn rc;\n+\t} else {\n+\t\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\t}\n \n \tif (sysram->res) {\n \t\tremove_resource(sysram->res);\n@@ -257,7 +281,8 @@ static void sysram_unregister(void *_sysram)\n \tdevice_unregister(&sysram->dev);\n }\n \n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n+\t\t\tenum mmop online_type)\n {\n \tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n \tstruct memory_dev_type *mtype;\n@@ -291,6 +316,11 @@ int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n \tif (online_type >= 0)\n \t\tsysram->online_type = online_type;\n \n+\t/* Set up private node registration if requested */\n+\tsysram->private = private;\n+\tif (private)\n+\t\tsysram->np.owner = sysram;\n+\n \tdev = &sysram->dev;\n \n \trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 8e8342fd4fde..54e5f9ac59dc 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -10,6 +10,7 @@\n #include <linux/bitops.h>\n #include <linux/log2.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n #include <linux/io.h>\n #include <linux/range.h>\n #include <linux/dax.h>\n@@ -619,6 +620,8 @@ struct cxl_dax_region {\n  * @mgid: Memory group id\n  * @mtype: Memory tier type\n  * @numa_node: NUMA node for this memory\n+ * @private: true if this region uses N_MEMORY_PRIVATE hotplug\n+ * @np: private node registration state (valid when @private is true)\n  *\n  * Device that directly performs memory hotplug for CXL RAM regions.\n  */\n@@ -633,6 +636,8 @@ struct cxl_sysram {\n \tint mgid;\n \tstruct memory_dev_type *mtype;\n \tint numa_node;\n+\tbool private;\n+\tstruct node_private np;\n };\n \n /**\n@@ -987,7 +992,7 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n struct cxl_sysram *to_cxl_sysram(struct device *dev);\n struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n@@ -1011,7 +1016,7 @@ static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n {\n \treturn NULL;\n }\n-static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n \t\t\t\t      enum mmop online_type)\n {\n \treturn -ENXIO;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the driver's interaction with the migration target control, explaining that they moved struct migration_target_control to include/linux/migrate.h so the driver can use alloc_migration_target() without depending on mm-internal headers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a sample CXL type-3 driver that registers device memory as\nprivate-node NUMA memory reachable only via explicit mempolicy\n(set_mempolicy / mbind).\n\nProbe flow:\n  1. Call cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Look for pre-committed RAM regions; if none exist, create one\n     using cxl_get_hpa_freespace() + cxl_request_dpa() +\n     cxl_create_region()\n  3. Convert the region to sysram via devm_cxl_add_sysram() with\n     private=true and MMOP_ONLINE_MOVABLE\n  4. Register node_private_ops with NP_OPS_MIGRATION | NP_OPS_MEMPOLICY\n     so the node is excluded from default allocations\n\nThe migrate_to callback uses alloc_migration_target() with\n__GFP_THISNODE | __GFP_PRIVATE to keep pages on the target node.\n\nMove struct migration_target_control from mm/internal.h to\ninclude/linux/migrate.h so the driver can use alloc_migration_target()\nwithout depending on mm-internal headers.\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/Kconfig                           |   2 +\n drivers/cxl/Makefile                          |   2 +\n drivers/cxl/type3_drivers/Kconfig             |   2 +\n drivers/cxl/type3_drivers/Makefile            |   2 +\n .../cxl/type3_drivers/cxl_mempolicy/Kconfig   |  16 +\n .../cxl/type3_drivers/cxl_mempolicy/Makefile  |   4 +\n .../type3_drivers/cxl_mempolicy/mempolicy.c   | 297 ++++++++++++++++++\n include/linux/migrate.h                       |   7 +-\n mm/internal.h                                 |   7 -\n 9 files changed, 331 insertions(+), 8 deletions(-)\n create mode 100644 drivers/cxl/type3_drivers/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n\ndiff --git a/drivers/cxl/Kconfig b/drivers/cxl/Kconfig\nindex f99aa7274d12..1648cdeaa0c9 100644\n--- a/drivers/cxl/Kconfig\n+++ b/drivers/cxl/Kconfig\n@@ -278,4 +278,6 @@ config CXL_ATL\n \tdepends on CXL_REGION\n \tdepends on ACPI_PRMT && AMD_NB\n \n+source \"drivers/cxl/type3_drivers/Kconfig\"\n+\n endif\ndiff --git a/drivers/cxl/Makefile b/drivers/cxl/Makefile\nindex 2caa90fa4bf2..94d2b2233bf8 100644\n--- a/drivers/cxl/Makefile\n+++ b/drivers/cxl/Makefile\n@@ -19,3 +19,5 @@ cxl_acpi-y := acpi.o\n cxl_pmem-y := pmem.o security.o\n cxl_mem-y := mem.o\n cxl_pci-y := pci.o\n+\n+obj-y += type3_drivers/\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nnew file mode 100644\nindex 000000000000..369b21763856\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nnew file mode 100644\nindex 000000000000..2b82265ff118\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\nnew file mode 100644\nindex 000000000000..3c45da237b9f\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n@@ -0,0 +1,16 @@\n+config CXL_MEMPOLICY\n+\ttristate \"CXL Private Memory with Mempolicy Support\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\thelp\n+\t  Minimal driver for CXL memory devices that registers memory as\n+\t  N_MEMORY_PRIVATE with mempolicy support.  The memory is isolated\n+\t  from default allocations and can only be reached via explicit\n+\t  mempolicy (set_mempolicy or mbind).\n+\n+\t  No compression, no PTE controls, the memory behaves like normal\n+\t  DRAM but is excluded from fallback allocations.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\nnew file mode 100644\nindex 000000000000..dfb58fc88ad9\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy.o\n+cxl_mempolicy-y := mempolicy.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\nnew file mode 100644\nindex 000000000000..1c19818eb268\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n@@ -0,0 +1,297 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Mempolicy Driver\n+ *\n+ * Minimal driver for CXL memory devices that registers memory as\n+ * N_MEMORY_PRIVATE with mempolicy support but no PTE controls.  The\n+ * memory behaves like normal DRAM but is isolated from default allocations,\n+ * it can only be reached via explicit mempolicy (set_mempolicy/mbind).\n+ *\n+ * Usage:\n+ *   1. Unbind device from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   2. Bind to cxl_mempolicy:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n+ */\n+\n+#include <linux/module.h>\n+#include <linux/pci.h>\n+#include <linux/xarray.h>\n+#include <linux/node_private.h>\n+#include <linux/migrate.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+struct cxl_mempolicy_ctx {\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint nid;\n+};\n+\n+static DEFINE_XARRAY(ctx_xa);\n+\n+static struct cxl_mempolicy_ctx *memdev_to_ctx(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\n+\treturn xa_load(&ctx_xa, (unsigned long)pdev);\n+}\n+\n+static int cxl_mempolicy_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason,\n+\t\t\t\t    unsigned int *nr_succeeded)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE |\n+\t\t\t    __GFP_PRIVATE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, nr_succeeded);\n+}\n+\n+static void cxl_mempolicy_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static const struct node_private_ops cxl_mempolicy_ops = {\n+\t.migrate_to\t= cxl_mempolicy_migrate_to,\n+\t.folio_migrate\t= cxl_mempolicy_folio_migrate,\n+\t.flags = NP_OPS_MIGRATION | NP_OPS_MEMPOLICY,\n+};\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tctx->cxled = cxled;\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\treturn cxlr;\n+}\n+\n+static int setup_private_node(struct cxl_memdev *cxlmd,\n+\t\t\t      struct cxl_region *cxlr)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct range hpa_range;\n+\tint rc;\n+\n+\tdevice_release_driver(cxl_region_dev(cxlr));\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to add sysram: %d\\n\", rc);\n+\t\tif (device_attach(cxl_region_dev(cxlr)) < 0)\n+\t\t\tdev_warn(cxl_region_dev(cxlr),\n+\t\t\t\t \"failed to re-attach driver\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tctx->nid = phys_to_target_node(hpa_range.start);\n+\tif (ctx->nid == NUMA_NO_NODE)\n+\t\tctx->nid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\trc = node_private_set_ops(ctx->nid, &cxl_mempolicy_ops);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to set ops on node %d: %d\\n\", ctx->nid, rc);\n+\t\tctx->nid = NUMA_NO_NODE;\n+\t\treturn rc;\n+\t}\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"node %d registered as private mempolicy memory\\n\", ctx->nid);\n+\treturn 0;\n+}\n+\n+static int cxl_mempolicy_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i;\n+\tint rc;\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"cxl_mempolicy attach: looking for regions\\n\");\n+\n+\t/* Phase 1: look for pre-committed RAM regions */\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) != CXL_PARTMODE_RAM) {\n+\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tcxlr = regions[i];\n+\t\trc = setup_private_node(cxlmd, cxlr);\n+\t\tput_device(cxl_region_dev(cxlr));\n+\t\tif (rc == 0) {\n+\t\t\t/* Release remaining region references */\n+\t\t\tfor (i++; i < nr; i++)\n+\t\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\treturn 0;\n+\t\t}\n+\t}\n+\n+\t/* Phase 2: no committed regions, create one */\n+\tdev_info(&cxlmd->dev,\n+\t\t \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"no RAM capacity: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = setup_private_node(cxlmd, cxlr);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to setup private node: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\t/* Only take ownership of regions we created (Phase 2) */\n+\tmemdev_to_ctx(cxlmd)->cxlr = cxlr;\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_mempolicy_attach = {\n+\t.probe = cxl_mempolicy_attach_probe,\n+};\n+\n+static int cxl_mempolicy_probe(struct pci_dev *pdev,\n+\t\t\t       const struct pci_device_id *id)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probing device\\n\");\n+\n+\tctx = devm_kzalloc(&pdev->dev, sizeof(*ctx), GFP_KERNEL);\n+\tif (!ctx)\n+\t\treturn -ENOMEM;\n+\tctx->nid = NUMA_NO_NODE;\n+\n+\trc = xa_insert(&ctx_xa, (unsigned long)pdev, ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_mempolicy_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_mempolicy_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = xa_erase(&ctx_xa, (unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: removing device\\n\");\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\tif (ctx->nid != NUMA_NO_NODE)\n+\t\tWARN_ON(node_private_clear_ops(ctx->nid, &cxl_mempolicy_ops));\n+\n+\tif (ctx->cxlr) {\n+\t\tcxl_destroy_region(ctx->cxlr);\n+\t\tctx->cxlr = NULL;\n+\t}\n+\n+\tif (ctx->cxled) {\n+\t\tcxl_dpa_free(ctx->cxled);\n+\t\tctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_mempolicy_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_mempolicy_pci_tbl);\n+\n+static struct pci_driver cxl_mempolicy_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_mempolicy_pci_tbl,\n+\t.probe\t\t= cxl_mempolicy_probe,\n+\t.remove\t\t= cxl_mempolicy_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_mempolicy_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Private Memory with Mempolicy Support\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 7b2da3875ff2..1f9fb61f3932 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -10,7 +10,12 @@\n typedef struct folio *new_folio_t(struct folio *folio, unsigned long private);\n typedef void free_folio_t(struct folio *folio, unsigned long private);\n \n-struct migration_target_control;\n+struct migration_target_control {\n+\tint nid;\t\t/* preferred node id */\n+\tnodemask_t *nmask;\n+\tgfp_t gfp_mask;\n+\tenum migrate_reason reason;\n+};\n \n /**\n  * struct movable_operations - Driver page migration\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 64467ca774f1..85cd11189854 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1352,13 +1352,6 @@ extern const struct trace_print_flags gfpflag_names[];\n \n void setup_zone_pageset(struct zone *zone);\n \n-struct migration_target_control {\n-\tint nid;\t\t/* preferred node id */\n-\tnodemask_t *nmask;\n-\tgfp_t gfp_mask;\n-\tenum migrate_reason reason;\n-};\n-\n /*\n  * mm/filemap.c\n  */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the cxl_compression driver's page reclamation using the CXL Media Operations Zero command (opcode 0x4402). The author explains that if the device does not support this command, the driver falls back to inline CPU zeroing.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a generic CXL type-3 driver for compressed memory controllers.\n\nThe driver provides an alternative PCI binding that converts CXL\nRAM regions to private-node sysram and registers them with the\nCRAM subsystem for transparent demotion/promotion.\n\nProbe flow:\n  1. cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Discover/convert auto-RAM regions or create a RAM region\n  3. Convert to private-node sysram via devm_cxl_add_sysram()\n  4. Register with CRAM via cram_register_private_node()\n\nPage flush pipeline:\n  When a CRAM folio is freed, the CRAM free_folio   callback buffers\n  it into a per-CPU RCU-protected flush buffer to offload the operation.\n\n  A periodic kthread swaps the per-CPU buffers under RCU, then sends\n  batched Sanitize-Zero commands so the device can zero pages.\n\n  A flush_record bitmap tracks in-flight pages to avoid re-buffering on\n  the second free_folio entry after folio_put().\n\n  Overflow from full buffers is handled by a per-CPU workqueue fallback.\n\nWatermark interrupts:\n  MSI-X vector 12 - delivers \"Low\" watermark interrupts\n  MSI-X vector 13 - delivers \"High\" watermark interrupts\n  This adjusts CRAM pressure:\n\tLow  - increases pressure.\n  \tHigh - reduces pressure.\n\n  A dynamic watermark mode cycles through four phases with\n  progressively tighter thresholds.\n\n  Static watermark mode sets pressure 0 or MAX respectively.\n\nTeardown ordering:\n  pre_teardown  - cram_unregister + retry-loop memory offline\n  post_teardown - kthread stop, drain all flush buffers via CCI\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/type3_drivers/Kconfig             |    1 +\n drivers/cxl/type3_drivers/Makefile            |    1 +\n .../cxl/type3_drivers/cxl_compression/Kconfig |   20 +\n .../type3_drivers/cxl_compression/Makefile    |    4 +\n .../cxl_compression/compression.c             | 1025 +++++++++++++++++\n 5 files changed, 1051 insertions(+)\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/compression.c\n\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nindex 369b21763856..98f73e46730e 100644\n--- a/drivers/cxl/type3_drivers/Kconfig\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\n+source \"drivers/cxl/type3_drivers/cxl_compression/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nindex 2b82265ff118..f5b0766d92af 100644\n--- a/drivers/cxl/type3_drivers/Makefile\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression/\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Kconfig b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\nnew file mode 100644\nindex 000000000000..8c891a48b000\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\n@@ -0,0 +1,20 @@\n+config CXL_COMPRESSION\n+\ttristate \"CXL Compression Memory Driver\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on CRAM\n+\thelp\n+\t  This driver provides an alternative PCI binding for CXL memory\n+\t  devices with compressed memory support. It converts CXL RAM\n+\t  regions to sysram for direct memory hotplug and registers with\n+\t  the CRAM subsystem for transparent compression.\n+\n+\t  Page reclamation uses the standard CXL Media Operations Zero\n+\t  command (opcode 0x4402). If the device does not support it,\n+\t  the driver falls back to inline CPU zeroing.\n+\n+\t  Usage: First unbind the device from cxl_pci, then bind to\n+\t  cxl_compression. The driver will initialize the CXL device and\n+\t  convert any RAM regions to use direct memory hotplug via sysram.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Makefile b/drivers/cxl/type3_drivers/cxl_compression/Makefile\nnew file mode 100644\nindex 000000000000..46f34809bf74\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression.o\n+cxl_compression-y := compression.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/compression.c b/drivers/cxl/type3_drivers/cxl_compression/compression.c\nnew file mode 100644\nindex 000000000000..e4c8b62227e2\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/compression.c\n@@ -0,0 +1,1025 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Compression Driver\n+ *\n+ * This driver provides an alternative binding for CXL memory devices that\n+ * converts all associated RAM regions to sysram_regions for direct memory\n+ * hotplug, bypassing the standard dax region path.\n+ *\n+ * Page reclamation uses the standard CXL Media Operations Zero command\n+ * (opcode 0x4402, class 0x01, subclass 0x01).  Watermark interrupts\n+ * are delivered via separate MSI-X vectors (12 for lthresh, 13 for\n+ * hthresh), injected externally via QMP.\n+ *\n+ * Usage:\n+ *   1. Device initially binds to cxl_pci at boot\n+ *   2. Unbind from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   3. Bind to cxl_compression:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n+ */\n+\n+#include <linux/unaligned.h>\n+#include <linux/io-64-nonatomic-lo-hi.h>\n+#include <linux/module.h>\n+#include <linux/delay.h>\n+#include <linux/sizes.h>\n+#include <linux/mutex.h>\n+#include <linux/list.h>\n+#include <linux/pci.h>\n+#include <linux/io.h>\n+#include <linux/interrupt.h>\n+#include <linux/bitmap.h>\n+#include <linux/highmem.h>\n+#include <linux/workqueue.h>\n+#include <linux/kthread.h>\n+#include <linux/rcupdate.h>\n+#include <linux/percpu.h>\n+#include <linux/sched.h>\n+#include <linux/cram.h>\n+#include <linux/memory_hotplug.h>\n+#include <linux/xarray.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+/*\n+ * Per-device compression context lookup.\n+ *\n+ * pci_set_drvdata() MUST store cxlds because mbox_to_cxlds() uses\n+ * dev_get_drvdata() to recover the cxl_dev_state from the mailbox host\n+ * device.  Storing anything else in pci drvdata breaks every CXL mailbox\n+ * command.  Use an xarray keyed by pci_dev pointer so that multiple\n+ * devices can bind concurrently without colliding.\n+ */\n+static DEFINE_XARRAY(comp_ctx_xa);\n+\n+static struct cxl_compression_ctx *pdev_to_comp_ctx(struct pci_dev *pdev)\n+{\n+\treturn xa_load(&comp_ctx_xa, (unsigned long)pdev);\n+}\n+\n+#define CXL_MEDIA_OP_OPCODE\t\t0x4402\n+#define CXL_MEDIA_OP_CLASS_SANITIZE\t0x01\n+#define CXL_MEDIA_OP_SUBC_ZERO\t\t0x01\n+\n+struct cxl_dpa_range {\n+\t__le64 starting_dpa;\n+\t__le64 length;\n+} __packed;\n+\n+struct cxl_media_op_input {\n+\tu8 media_operation_class;\n+\tu8 media_operation_subclass;\n+\t__le16 reserved;\n+\t__le32 dpa_range_count;\n+\tstruct cxl_dpa_range ranges[];\n+} __packed;\n+\n+#define CXL_CT3_MSIX_LTHRESH\t\t12\n+#define CXL_CT3_MSIX_HTHRESH\t\t13\n+#define CXL_CT3_MSIX_VECTOR_NR\t\t14\n+#define CXL_FLUSH_INTERVAL_DEFAULT_MS\t1000\n+\n+static unsigned int flush_buf_size;\n+module_param(flush_buf_size, uint, 0444);\n+MODULE_PARM_DESC(flush_buf_size,\n+\t\t \"Max DPA ranges per media ops CCI command (0 = use hw max)\");\n+\n+static unsigned int flush_interval_ms = CXL_FLUSH_INTERVAL_DEFAULT_MS;\n+module_param(flush_interval_ms, uint, 0644);\n+MODULE_PARM_DESC(flush_interval_ms,\n+\t\t \"Flush worker interval in ms (default 1000)\");\n+\n+struct cxl_flush_buf {\n+\tunsigned int count;\n+\tunsigned int max;\t\t\t/* max ranges per command */\n+\tstruct cxl_media_op_input *cmd;\t\t/* pre-allocated CCI payload */\n+\tstruct folio **folios;\t\t\t/* parallel folio tracking */\n+};\n+\n+struct cxl_flush_ctx;\n+\n+struct cxl_pcpu_flush {\n+\tstruct cxl_flush_buf __rcu *active;\t/* callback writes here */\n+\tstruct cxl_flush_buf *overflow_spare;\t/* spare for overflow work */\n+\tstruct work_struct overflow_work;\t/* per-CPU overflow flush */\n+\tstruct cxl_flush_ctx *ctx;\t\t/* backpointer */\n+};\n+\n+/**\n+ * struct cxl_flush_ctx - Per-region flush context\n+ * @flush_record: two-level bitmap, 1 bit per 4KB page, tracks in-flight ops\n+ * @flush_record_pages: number of pages in the flush_record array\n+ * @nr_pages: total number of 4KB pages in the region\n+ * @base_pfn: starting PFN of the region (for DPA offset calculation)\n+ * @buf_max: max DPA ranges per CCI command\n+ * @media_ops_supported: true if device supports media operations zero\n+ * @pcpu: per-CPU flush state\n+ * @kthread_spares: array[nr_cpu_ids] of spare buffers for the kthread\n+ * @flush_thread: round-robin kthread\n+ * @mbox: pointer to CXL mailbox for sending CCI commands\n+ * @dev: device for logging\n+ * @nid: NUMA node of the private region\n+ */\n+struct cxl_flush_ctx {\n+\tunsigned long\t**flush_record;\n+\tunsigned int\t flush_record_pages;\n+\tunsigned long\t nr_pages;\n+\tunsigned long\t base_pfn;\n+\tunsigned int\t buf_max;\n+\tbool\t\t media_ops_supported;\n+\tstruct cxl_pcpu_flush __percpu *pcpu;\n+\tstruct cxl_flush_buf **kthread_spares;\n+\tstruct task_struct *flush_thread;\n+\tstruct cxl_mailbox *mbox;\n+\tstruct device\t*dev;\n+\tint\t\t nid;\n+};\n+\n+/* Bits per page-sized bitmap chunk */\n+#define FLUSH_RECORD_BITS_PER_PAGE\t(PAGE_SIZE * BITS_PER_BYTE)\n+#define FLUSH_RECORD_SHIFT\t\t(PAGE_SHIFT + 3)\n+\n+static unsigned long **flush_record_alloc(unsigned long nr_bits,\n+\t\t\t\t\t  unsigned int *nr_pages_out)\n+{\n+\tunsigned int nr_pages = DIV_ROUND_UP(nr_bits, FLUSH_RECORD_BITS_PER_PAGE);\n+\tunsigned long **pages;\n+\tunsigned int i;\n+\n+\tpages = kcalloc(nr_pages, sizeof(*pages), GFP_KERNEL);\n+\tif (!pages)\n+\t\treturn NULL;\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tpages[i] = (unsigned long *)get_zeroed_page(GFP_KERNEL);\n+\t\tif (!pages[i])\n+\t\t\tgoto err;\n+\t}\n+\n+\t*nr_pages_out = nr_pages;\n+\treturn pages;\n+\n+err:\n+\twhile (i--)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+\treturn NULL;\n+}\n+\n+static void flush_record_free(unsigned long **pages, unsigned int nr_pages)\n+{\n+\tunsigned int i;\n+\n+\tif (!pages)\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr_pages; i++)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+}\n+\n+static inline bool flush_record_test_and_clear(unsigned long **pages,\n+\t\t\t\t\t       unsigned long idx)\n+{\n+\treturn test_and_clear_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\t\t\t  pages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static inline void flush_record_set(unsigned long **pages, unsigned long idx)\n+{\n+\tset_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\tpages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static struct cxl_flush_buf *cxl_flush_buf_alloc(unsigned int max, int nid)\n+{\n+\tstruct cxl_flush_buf *buf;\n+\n+\tbuf = kzalloc_node(sizeof(*buf), GFP_KERNEL, nid);\n+\tif (!buf)\n+\t\treturn NULL;\n+\n+\tbuf->max = max;\n+\tbuf->cmd = kvzalloc_node(struct_size(buf->cmd, ranges, max),\n+\t\t\t\t GFP_KERNEL, nid);\n+\tif (!buf->cmd)\n+\t\tgoto err_cmd;\n+\n+\tbuf->folios = kcalloc_node(max, sizeof(struct folio *),\n+\t\t\t\t   GFP_KERNEL, nid);\n+\tif (!buf->folios)\n+\t\tgoto err_folios;\n+\n+\treturn buf;\n+\n+err_folios:\n+\tkvfree(buf->cmd);\n+err_cmd:\n+\tkfree(buf);\n+\treturn NULL;\n+}\n+\n+static void cxl_flush_buf_free(struct cxl_flush_buf *buf)\n+{\n+\tif (!buf)\n+\t\treturn;\n+\tkvfree(buf->cmd);\n+\tkfree(buf->folios);\n+\tkfree(buf);\n+}\n+\n+static inline void cxl_flush_buf_reset(struct cxl_flush_buf *buf)\n+{\n+\tbuf->count = 0;\n+}\n+\n+static void cxl_flush_buf_send(struct cxl_flush_ctx *ctx,\n+\t\t\t       struct cxl_flush_buf *buf)\n+{\n+\tstruct cxl_mbox_cmd mbox_cmd;\n+\tunsigned int count = buf->count;\n+\tunsigned int i;\n+\tint rc;\n+\n+\tif (count == 0)\n+\t\treturn;\n+\n+\tif (!ctx->media_ops_supported) {\n+\t\t/* No device support, zero all folios inline */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t\tgoto release;\n+\t}\n+\n+\tbuf->cmd->media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE;\n+\tbuf->cmd->media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO;\n+\tbuf->cmd->reserved = 0;\n+\tbuf->cmd->dpa_range_count = cpu_to_le32(count);\n+\n+\tmbox_cmd = (struct cxl_mbox_cmd) {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = buf->cmd,\n+\t\t.size_in = struct_size(buf->cmd, ranges, count),\n+\t\t.poll_interval_ms = 1000,\n+\t\t.poll_count = 30,\n+\t};\n+\n+\trc = cxl_internal_send_cmd(ctx->mbox, &mbox_cmd);\n+\tif (rc) {\n+\t\tdev_warn(ctx->dev,\n+\t\t\t \"media ops zero CCI command failed: %d\\n\", rc);\n+\n+\t\t/* Zero all folios inline on failure */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t}\n+\n+release:\n+\tfor (i = 0; i < count; i++)\n+\t\tfolio_put(buf->folios[i]);\n+\n+\tcxl_flush_buf_reset(buf);\n+}\n+\n+static int cxl_compression_flush_cb(struct folio *folio, void *private)\n+{\n+\tstruct cxl_flush_ctx *ctx = private;\n+\tunsigned long pfn = folio_pfn(folio);\n+\tunsigned long idx = pfn - ctx->base_pfn;\n+\tunsigned long nr = folio_nr_pages(folio);\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tunsigned long flags;\n+\tunsigned int pos;\n+\n+\t/* Case (a): flush record bit set, resolution from our media op */\n+\tif (flush_record_test_and_clear(ctx->flush_record, idx))\n+\t\treturn 0;\n+\n+\tdev_dbg_ratelimited(ctx->dev,\n+\t\t\t     \"flush_cb: folio pfn=%lx order=%u idx=%lu cpu=%d\\n\",\n+\t\t\t     pfn, folio_order(folio), idx,\n+\t\t\t     raw_smp_processor_id());\n+\n+\tlocal_irq_save(flags);\n+\trcu_read_lock();\n+\n+\tpcpu = this_cpu_ptr(ctx->pcpu);\n+\tbuf = rcu_dereference(pcpu->active);\n+\n+\tif (unlikely(!buf || buf->count >= buf->max)) {\n+\t\trcu_read_unlock();\n+\t\tlocal_irq_restore(flags);\n+\t\tif (buf)\n+\t\t\tschedule_work_on(raw_smp_processor_id(),\n+\t\t\t\t\t &pcpu->overflow_work);\n+\t\treturn 2;\n+\t}\n+\n+\t/* Case (b): write DPA range directly into pre-formatted CCI buffer */\n+\tfolio_get(folio);\n+\tflush_record_set(ctx->flush_record, idx);\n+\n+\tpos = buf->count;\n+\tbuf->folios[pos] = folio;\n+\tbuf->cmd->ranges[pos].starting_dpa = cpu_to_le64((u64)idx * PAGE_SIZE);\n+\tbuf->cmd->ranges[pos].length = cpu_to_le64((u64)nr * PAGE_SIZE);\n+\tbuf->count = pos + 1;\n+\n+\trcu_read_unlock();\n+\tlocal_irq_restore(flags);\n+\n+\treturn 1;\n+}\n+\n+static int cxl_flush_kthread_fn(void *data)\n+{\n+\tstruct cxl_flush_ctx *ctx = data;\n+\tstruct cxl_flush_buf *dirty;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tint cpu;\n+\tbool any_dirty;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tany_dirty = false;\n+\n+\t\t/* Phase 1: Swap all per-CPU buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tstruct cxl_flush_buf *spare = ctx->kthread_spares[cpu];\n+\n+\t\t\tif (!spare)\n+\t\t\t\tcontinue;\n+\n+\t\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\t\tcxl_flush_buf_reset(spare);\n+\t\t\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\t\t\tctx->kthread_spares[cpu] = dirty;\n+\n+\t\t\tif (dirty && dirty->count > 0) {\n+\t\t\t\tdev_dbg(ctx->dev,\n+\t\t\t\t\t \"flush_kthread: cpu=%d has %u dirty ranges\\n\",\n+\t\t\t\t\t cpu, dirty->count);\n+\t\t\t\tany_dirty = true;\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (!any_dirty)\n+\t\t\tgoto sleep;\n+\n+\t\t/* Phase 2: Single synchronize_rcu for all swaps */\n+\t\tsynchronize_rcu();\n+\n+\t\t/* Phase 3: Send CCI commands for dirty buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tdirty = ctx->kthread_spares[cpu];\n+\t\t\tif (dirty && dirty->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, dirty);\n+\t\t\t/* dirty is now clean, stays as kthread_spares[cpu] */\n+\t\t}\n+\n+sleep:\n+\t\tschedule_timeout_interruptible(\n+\t\t\tmsecs_to_jiffies(flush_interval_ms));\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static void cxl_flush_overflow_work(struct work_struct *work)\n+{\n+\tstruct cxl_pcpu_flush *pcpu =\n+\t\tcontainer_of(work, struct cxl_pcpu_flush, overflow_work);\n+\tstruct cxl_flush_ctx *ctx = pcpu->ctx;\n+\tstruct cxl_flush_buf *dirty, *spare;\n+\tunsigned long flags;\n+\n+\tdev_dbg(ctx->dev, \"flush_overflow: cpu=%d buffer full, flushing\\n\",\n+\t\t raw_smp_processor_id());\n+\n+\tspare = pcpu->overflow_spare;\n+\tif (!spare)\n+\t\treturn;\n+\n+\tcxl_flush_buf_reset(spare);\n+\n+\tlocal_irq_save(flags);\n+\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\tlocal_irq_restore(flags);\n+\n+\tpcpu->overflow_spare = dirty;\n+\n+\tsynchronize_rcu();\n+\tcxl_flush_buf_send(ctx, dirty);\n+}\n+\n+struct cxl_teardown_ctx {\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+};\n+\n+static void cxl_compression_pre_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\n+\tif (!tctx->flush_ctx)\n+\t\treturn;\n+\n+\t/*\n+\t * Unregister the CRAM node before memory goes offline.\n+\t * node_private_clear_ops requires the node_private to still\n+\t * exist, which is destroyed during memory removal.\n+\t */\n+\tcram_unregister_private_node(tctx->nid);\n+\n+\t/*\n+\t * Offline and remove CXL memory with retry.  CXL compressed\n+\t * memory may have pages pinned by in-flight flush operations;\n+\t * keep retrying until they complete.  Once done, sysram->res\n+\t * is NULL so the devm sysram_unregister action that follows\n+\t * will skip the hotplug removal.\n+\t */\n+\tif (tctx->sysram) {\n+\t\tint rc, retries = 0;\n+\n+\t\twhile (true) {\n+\t\t\trc = cxl_sysram_offline_and_remove(tctx->sysram);\n+\t\t\tif (!rc)\n+\t\t\t\tbreak;\n+\t\t\tif (++retries > 60) {\n+\t\t\t\tpr_err(\"cxl_compression: memory offline failed after %d retries, giving up\\n\",\n+\t\t\t\t       retries);\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tpr_info(\"cxl_compression: memory offline failed (%d), retrying...\\n\",\n+\t\t\t\trc);\n+\t\t\tmsleep(1000);\n+\t\t}\n+\t}\n+}\n+\n+static void cxl_compression_post_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\tstruct cxl_flush_ctx *ctx = tctx->flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tint cpu;\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\t/* cram_unregister_private_node already called in pre_teardown */\n+\n+\tif (ctx->flush_thread) {\n+\t\tkthread_stop(ctx->flush_thread);\n+\t\tctx->flush_thread = NULL;\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\tcancel_work_sync(&pcpu->overflow_work);\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tif (buf && buf->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, buf);\n+\n+\t\tif (pcpu->overflow_spare && pcpu->overflow_spare->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares && ctx->kthread_spares[cpu]) {\n+\t\t\tbuf = ctx->kthread_spares[cpu];\n+\t\t\tif (buf->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, buf);\n+\t\t}\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(ctx->kthread_spares[cpu]);\n+\t}\n+\n+\tkfree(ctx->kthread_spares);\n+\tfree_percpu(ctx->pcpu);\n+\tflush_record_free(ctx->flush_record, ctx->flush_record_pages);\n+}\n+\n+/**\n+ * struct cxl_compression_ctx - Per-device context for compression driver\n+ * @mbox: CXL mailbox for issuing CCI commands\n+ * @pdev: PCI device\n+ * @flush_ctx: Flush context for deferred page reclamation\n+ * @tctx: Teardown context for devm actions\n+ * @sysram: Sysram device for offline+remove in remove path\n+ * @nid: NUMA node ID, NUMA_NO_NODE if unset\n+ * @cxlmd: The memdev associated with this context\n+ * @cxlr: Region created by this driver (NULL if pre-existing)\n+ * @cxled: Endpoint decoder with DPA allocated by this driver\n+ * @regions_converted: Number of regions successfully converted\n+ * @media_ops_supported: Device supports media operations zero (0x4402)\n+ */\n+struct cxl_compression_ctx {\n+\tstruct cxl_mailbox *mbox;\n+\tstruct pci_dev *pdev;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+\tstruct cxl_memdev *cxlmd;\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint regions_converted;\n+\tbool media_ops_supported;\n+};\n+\n+/*\n+ * Probe whether the device supports Media Operations Zero (0x4402).\n+ * Send a zero-count command, a conforming device returns SUCCESS,\n+ * a device that doesn't support it returns UNSUPPORTED (-ENXIO).\n+ */\n+static bool cxl_probe_media_ops_zero(struct cxl_mailbox *mbox,\n+\t\t\t\t     struct device *dev)\n+{\n+\tstruct cxl_media_op_input probe = {\n+\t\t.media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE,\n+\t\t.media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO,\n+\t\t.dpa_range_count = 0,\n+\t};\n+\tstruct cxl_mbox_cmd cmd = {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = &probe,\n+\t\t.size_in = sizeof(probe),\n+\t};\n+\tint rc;\n+\n+\trc = cxl_internal_send_cmd(mbox, &cmd);\n+\tif (rc) {\n+\t\tdev_info(dev,\n+\t\t\t \"media operations zero not supported (rc=%d), using inline zeroing\\n\",\n+\t\t\t rc);\n+\t\treturn false;\n+\t}\n+\n+\tdev_info(dev, \"media operations zero (0x4402) supported\\n\");\n+\treturn true;\n+}\n+\n+struct cxl_compression_wm_ctx {\n+\tstruct device *dev;\n+\tint nid;\n+};\n+\n+static irqreturn_t cxl_compression_lthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"lthresh watermark: pressuring node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, CRAM_PRESSURE_MAX);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static irqreturn_t cxl_compression_hthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"hthresh watermark: resuming node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, 0);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static int convert_region_to_sysram(struct cxl_region *cxlr,\n+\t\t\t\t    struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct device *dev = cxl_region_dev(cxlr);\n+\tstruct cxl_compression_wm_ctx *wm_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tresource_size_t region_start, region_size;\n+\tstruct range hpa_range;\n+\tint nid;\n+\tint irq;\n+\tint cpu;\n+\tint rc;\n+\n+\tif (cxl_region_mode(cxlr) != CXL_PARTMODE_RAM) {\n+\t\tdev_dbg(dev, \"skipping non-RAM region (mode=%d)\\n\",\n+\t\t\tcxl_region_mode(cxlr));\n+\t\treturn 0;\n+\t}\n+\n+\tdev_info(dev, \"converting region to sysram\\n\");\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to add sysram region: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\ttctx = devm_kzalloc(dev, sizeof(*tctx), GFP_KERNEL);\n+\tif (!tctx)\n+\t\treturn -ENOMEM;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_post_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Find the sysram child device for pre_teardown */\n+\tcomp_ctx->sysram = cxl_region_find_sysram(cxlr);\n+\tif (comp_ctx->sysram)\n+\t\ttctx->sysram = comp_ctx->sysram;\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tnid = phys_to_target_node(hpa_range.start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\tregion_start = hpa_range.start;\n+\tregion_size = range_len(&hpa_range);\n+\n+\tflush_ctx = devm_kzalloc(dev, sizeof(*flush_ctx), GFP_KERNEL);\n+\tif (!flush_ctx)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->base_pfn = PHYS_PFN(region_start);\n+\tflush_ctx->nr_pages = region_size >> PAGE_SHIFT;\n+\tflush_ctx->flush_record = flush_record_alloc(flush_ctx->nr_pages,\n+\t\t\t\t\t\t     &flush_ctx->flush_record_pages);\n+\tif (!flush_ctx->flush_record)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->mbox = comp_ctx->mbox;\n+\tflush_ctx->dev = dev;\n+\tflush_ctx->nid = nid;\n+\tflush_ctx->media_ops_supported = comp_ctx->media_ops_supported;\n+\n+\t/*\n+\t * Cap buffer at max DPA ranges that fit in one CCI payload.\n+\t * Header is 8 bytes (struct cxl_media_op_input), each range\n+\t * is 16 bytes (struct cxl_dpa_range).  The module parameter\n+\t * flush_buf_size can further limit this (0 = use hw max).\n+\t */\n+\tflush_ctx->buf_max = (flush_ctx->mbox->payload_size -\n+\t\t\t      sizeof(struct cxl_media_op_input)) /\n+\t\t\t     sizeof(struct cxl_dpa_range);\n+\tif (flush_buf_size && flush_buf_size < flush_ctx->buf_max)\n+\t\tflush_ctx->buf_max = flush_buf_size;\n+\tif (flush_ctx->buf_max == 0)\n+\t\tflush_ctx->buf_max = 1;\n+\n+\tdev_info(dev,\n+\t\t \"flush buffer: %u DPA ranges per command (payload %zu bytes, media_ops %s)\\n\",\n+\t\t flush_ctx->buf_max, flush_ctx->mbox->payload_size,\n+\t\t flush_ctx->media_ops_supported ? \"yes\" : \"no\");\n+\n+\tflush_ctx->pcpu = alloc_percpu(struct cxl_pcpu_flush);\n+\tif (!flush_ctx->pcpu)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->kthread_spares = kcalloc(nr_cpu_ids,\n+\t\t\t\t\t    sizeof(struct cxl_flush_buf *),\n+\t\t\t\t\t    GFP_KERNEL);\n+\tif (!flush_ctx->kthread_spares)\n+\t\tgoto err_pcpu_init;\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *active_buf, *overflow_buf, *spare_buf;\n+\n+\t\tactive_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!active_buf)\n+\t\t\tgoto err_pcpu_init;\n+\n+\t\toverflow_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!overflow_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tspare_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!spare_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tcxl_flush_buf_free(overflow_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\t\tpcpu->ctx = flush_ctx;\n+\t\trcu_assign_pointer(pcpu->active, active_buf);\n+\t\tpcpu->overflow_spare = overflow_buf;\n+\t\tINIT_WORK(&pcpu->overflow_work, cxl_flush_overflow_work);\n+\n+\t\tflush_ctx->kthread_spares[cpu] = spare_buf;\n+\t}\n+\n+\tflush_ctx->flush_thread = kthread_create_on_node(\n+\t\tcxl_flush_kthread_fn, flush_ctx, nid, \"cxl-flush/%d\", nid);\n+\tif (IS_ERR(flush_ctx->flush_thread)) {\n+\t\trc = PTR_ERR(flush_ctx->flush_thread);\n+\t\tflush_ctx->flush_thread = NULL;\n+\t\tgoto err_pcpu_init;\n+\t}\n+\twake_up_process(flush_ctx->flush_thread);\n+\n+\trc = cram_register_private_node(nid, cxlr,\n+\t\t\t\t\tcxl_compression_flush_cb, flush_ctx);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to register cram node %d: %d\\n\", nid, rc);\n+\t\tgoto err_pcpu_init;\n+\t}\n+\n+\ttctx->flush_ctx = flush_ctx;\n+\ttctx->nid = nid;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_pre_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcomp_ctx->flush_ctx = flush_ctx;\n+\tcomp_ctx->tctx = tctx;\n+\tcomp_ctx->nid = nid;\n+\n+\t/*\n+\t * Register watermark IRQ handlers on &pdev->dev for\n+\t * MSI-X vector 12 (lthresh) and vector 13 (hthresh).\n+\t */\n+\twm_ctx = devm_kzalloc(&pdev->dev, sizeof(*wm_ctx), GFP_KERNEL);\n+\tif (!wm_ctx)\n+\t\treturn -ENOMEM;\n+\n+\twm_ctx->dev = &pdev->dev;\n+\twm_ctx->nid = nid;\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_LTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_lthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-lthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register lthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_HTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_hthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-hthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register hthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\treturn 0;\n+\n+err_pcpu_init:\n+\tif (flush_ctx->flush_thread)\n+\t\tkthread_stop(flush_ctx->flush_thread);\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *buf;\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (flush_ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(flush_ctx->kthread_spares[cpu]);\n+\t}\n+\tkfree(flush_ctx->kthread_spares);\n+\tfree_percpu(flush_ctx->pcpu);\n+\tflush_record_free(flush_ctx->flush_record, flush_ctx->flush_record_pages);\n+\treturn rc ? rc : -ENOMEM;\n+}\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\tpdev_to_comp_ctx(to_pci_dev(cxlmd->dev.parent))->cxled = cxled;\n+\treturn cxlr;\n+}\n+\n+static int cxl_compression_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i, converted = 0, errors = 0;\n+\tint rc;\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\t/* Probe device for media operations zero support */\n+\tcomp_ctx->media_ops_supported =\n+\t\tcxl_probe_media_ops_zero(comp_ctx->mbox,\n+\t\t\t\t\t &cxlmd->dev);\n+\n+\tdev_info(&cxlmd->dev, \"compression attach: looking for regions\\n\");\n+\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) == CXL_PARTMODE_RAM) {\n+\t\t\trc = convert_region_to_sysram(regions[i], pdev);\n+\t\t\tif (rc)\n+\t\t\t\terrors++;\n+\t\t\telse\n+\t\t\t\tconverted++;\n+\t\t}\n+\t\tput_device(cxl_region_dev(regions[i]));\n+\t}\n+\n+\tif (converted > 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"converted %d regions to sysram (%d errors)\\n\",\n+\t\t\t converted, errors);\n+\t\treturn errors ? -EIO : 0;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"could not create RAM region: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = convert_region_to_sysram(cxlr, pdev);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to convert region to sysram: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tcomp_ctx->cxlr = cxlr;\n+\n+\tdev_info(&cxlmd->dev, \"created and converted region %s to sysram\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_compression_attach = {\n+\t.probe = cxl_compression_attach_probe,\n+};\n+\n+static int cxl_compression_probe(struct pci_dev *pdev,\n+\t\t\t\t const struct pci_device_id *id)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probing device\\n\");\n+\n+\tcomp_ctx = devm_kzalloc(&pdev->dev, sizeof(*comp_ctx), GFP_KERNEL);\n+\tif (!comp_ctx)\n+\t\treturn -ENOMEM;\n+\tcomp_ctx->nid = NUMA_NO_NODE;\n+\tcomp_ctx->pdev = pdev;\n+\n+\trc = xa_insert(&comp_ctx_xa, (unsigned long)pdev, comp_ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_compression_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&comp_ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_compression_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = xa_erase(&comp_ctx_xa,\n+\t\t\t\t\t\t\t(unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: removing device\\n\");\n+\n+\tif (!comp_ctx || comp_ctx->nid == NUMA_NO_NODE)\n+\t\treturn;\n+\n+\t/*\n+\t * Destroy the region, devm actions on the region device handle teardown\n+\t * in registration-reverse order:\n+\t *   1. pre_teardown:  cram_unregister + retry-forever memory offline\n+\t *   2. sysram_unregister: device_unregister (sysram->res is NULL\n+\t *      after pre_teardown, so cxl_sysram_release skips hotplug)\n+\t *   3. post_teardown: kthread stop, flush cleanup\n+\t *\n+\t * PCI MMIO is still live so CCI commands in post_teardown work.\n+\t */\n+\tif (comp_ctx->cxlr) {\n+\t\tcxl_destroy_region(comp_ctx->cxlr);\n+\t\tcomp_ctx->cxlr = NULL;\n+\t}\n+\n+\tif (comp_ctx->cxled) {\n+\t\tcxl_dpa_free(comp_ctx->cxled);\n+\t\tcomp_ctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_compression_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ /* terminate list */ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_compression_pci_tbl);\n+\n+static struct pci_driver cxl_compression_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_compression_pci_tbl,\n+\t.probe\t\t= cxl_compression_probe,\n+\t.remove\t\t= cxl_compression_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_compression_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Compression Memory Driver with SysRAM regions\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David expressed concern about adding special-casing for private memory nodes, similar to ZONE_DEVICE, and suggested discussing the topic further.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concern",
                "special-casing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm concerned about adding more special-casing (similar to what we \nalready added for ZONE_DEVICE) all over the place.\n\nLike the whole folio_managed_() stuff in mprotect.c\n\nHaving that said, sounds like a reasonable topic to discuss.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged a concern about the semantics of zone_device hooks and proposed two alternative solutions: reusing vma_wants_writenotify() or adding a new vma flag to track protected/device pages.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "proposed alternative solutions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It's a valid concern - and is why I tried to re-use as many of the\nzone_device hooks as possible.  It does not seem zone_device has quite\nthe same semantics for a case like this, so I had to make something new.\n\nDEVICE_COHERENT injects a temporary swap entry to allow the device to do\na large atomic operation - then the page table is restored and the CPU\nis free to change entries as it pleases.\n\nAnother option would be to add the hook to vma_wants_writenotify()\ninstead of the page table code - and mask MM_CP_TRY_CHANGE_WRITABLE.\n\nThis would require adding a vma flag - or maybe a count of protected /\ndevice pages.\n\nint mprotect_fixup() {\n    ...\n    if (vma_wants_manual_pte_write_upgrade(vma))\n        mm_cp_flags |= MM_CP_TRY_CHANGE_WRITABLE;\n}\n\nbool vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)\n{\n    if (vma->managed_wrprotect)\n        return true;\n}\n\nThat would localize the change in folio_managed_fixup_migration_pte() :\n\nstatic inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n                                                      pte_t pte,\n                                                      pte_t old_pte,\n                                                      struct vm_area_struct *vma)\n{\n    ...\n    } else if (folio_managed_wrprotect(page_folio(new))) {\n        pte = pte_wrprotect(pte);\n+       atomic_inc(&vma->managed_wrprotect);\n    }\n    return pte;\n}\n\nThis would cover both the huge_memory.c and mprotect, and maybe that's\njust generally cleaner? I can try that to see if it actually works.\n\n~Gregory",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that existing hooks can be used for write protection and agreed to remove redundant code from page table walks.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged existing solution",
                "agreed to simplify"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "scratch all this - existing hooks exist for exactly this purpose:\n\n\tcan_change_[pte|pmd]_writable()\n\nSurprised I missed this.\n\nI can clean this up to remove it from the page table walks.\n\nStill valid to question whether we want this, but at least the hook\nlives with other write-protect hooks now.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alistair Popple",
              "summary": "Reviewer Alistair Popple expressed concerns that N_MEMORY_PRIVATE may not be the best solution for reusing the existing mm buddy allocator and suggested considering alternative approaches such as adapting DRM's standalone buddy allocator, while also noting that device memory exposure to userspace is an interesting aspect of the series.\n\nThe reviewer agrees that the patch provides a standard interface to userspace for managing device memory and suggests using the existing NUMA APIs as a reasonable approach.\n\nReviewer Alistair Popple noted that the proposed cxl_compression driver is similar to ZONE_DEVICE and questioned why it cannot be extended instead of duplicating code, pointing out a potential lock ordering issue with reclaim paths when using pgmap for ZONE_DEVICE pages. He suggested exploring alternative storage options such as page_ext or considering the future replacement of struct page with folios.\n\nReviewer suggested that the cxl_compression PCI driver is similar to existing ZONE_DEVICE methods and proposed building on those instead of introducing a new feature set.\n\nReviewer Alistair Popple noted that the implementation duplicates a lot of hooks, similar to those provided by ZONE_DEVICE, and requested further discussion on this aspect.\n\nReviewer questioned whether allocation must be handled by the mm allocator, suggesting that a device allocator library could be written or reused from drm_buddy.c\n\nThe reviewer questioned the characterization of ZONE_DEVICE pages as not being real struct pages, suggesting that perspective on this may vary depending on one's role in the mm subsystem, and asked for clarification on what limitations are actually being addressed.\n\nReviewer suggested that ZONE_DEVICE_COHERENT could be extended to support the use case, proposing a couple of extra dev_pagemap_ops and LRU access as a potential solution.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "alternative solutions",
                "agreement",
                "endorsement",
                "suggested alternatives",
                "duplicates hooks",
                "similar to ZONE_DEVICE",
                "clarification requested",
                "questioning characterization"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Having had to re-implement entire portions of mm/ in a driver I agree this isn't\nsomething anyone sane should do :-) However aspects of ZONE_DEVICE were added\nprecisely to help with that so I'm not sure N_MEMORY_PRIVATE is the only or best\nway to do that.\n\nBased on our discussion at LPC I believe one of the primary motivators here was\nto re-use the existing mm buddy allocator rather than writing your own. I remain\nto be convinced that alone is justification enough for doing all this - DRM for\nexample already has quite a nice standalone buddy allocator (drm_buddy.c) that\ncould presumably be used, or adapted for use, by any device driver.\n\nThe interesting part of this series (which I have skimmed but not read in\ndetail) is how device memory gets exposed to userspace - this is something that\nexisting ZONE_DEVICE implementations don't address, instead leaving it up to\ndrivers and associated userspace stacks to deal with allocation, migration, etc.\n\n---\n\nThis is I think is one of the key things that should be enabled - providing a\nstandard interface to userspace for managing device memory. The existing NUMA\nAPIs do seem like a reasonable way to do this.\n\n---\n\nOne does not have to squint too hard to see that the above is not so different\nfrom what ZONE_DEVICE provides today via dev_pagemap_ops(). So I think I think\nit would be worth outlining why the existing ZONE_DEVICE mechanism can't be\nextended to provide these kind of services.\n\nThis seems to add a bunch of code just to use NODE_DATA instead of page->pgmap,\nwithout really explaining why just extending dev_pagemap_ops wouldn't work. The\nobvious reason is that if you want to support things like reclaim, compaction,\netc. these pages need to be on the LRU, which is a little bit hard when that\nfield is also used by the pgmap pointer for ZONE_DEVICE pages.\n\nBut it might be good to explore other options for storing the pgmap - for\nexample page_ext could be used.  Or I hear struct page may go away in place of\nfolios any day now, so maybe that gives us space for both :-)\n\n---\n\nThe above also looks pretty similar to the existing ZONE_DEVICE methods for\ndoing this which is another reason to argue for just building up the feature set\nof the existing boondoggle rather than adding another thingymebob.\n\nIt seems the key thing we are looking for is:\n\n1) A userspace API to allocate/manage device memory (ie. move_pages(), mbind(),\netc.)\n\n2) Allowing reclaim/LRU list processing of device memory.\n\n---\n\ndiscussion (hopefully I can make it to LSFMM). Mostly I'm interested in the\nimplementation as this does on the surface seem to sprinkle around and duplicate\na lot of hooks similar to what ZONE_DEVICE already provides.\n\n---\n\nFor basic allocation I agree this is the case. But there's no reason some device\nallocator library couldn't be written. Or in fact as pointed out above reuse the\nalready existing one in drm_buddy.c.  So would be interested to hear arguments\nfor why allocation has to be done by the mm allocator and/or why an allocation\nlibrary wouldn't work here given DRM already has them.\n\n---\n\nZONE_DEVICE pages are in fact real struct pages, but I will concede that\nperspective probably depends on which bits of the mm you play in. The real\nlimitations you seem to be addressing is more around how we get these pages in\nan LRU, or are there other limitations?\n\n---\n\nWhat I'd like to explore is why ZONE_DEVICE_COHERENT couldn't just be extended\nto support your usecase? It seems a couple of extra dev_pagemap_ops and being\nable to go on the LRU would get you there.\n\n - Alistair",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author acknowledges that using ZONE_DEVICE is insufficient for N_MEMORY_PRIVATE, as it introduces unnecessary complexity. They propose reusing the buddy allocator instead, which would simplify the implementation and eliminate issues related to zones.\n\nThe author explains that the callback similarity between ZONE_DEVICE and private nodes is intentional, as they require the same set of hooks but with different defaults. They argue that extending ZONE_DEVICE into these areas would be cumbersome and inefficient, and that the current implementation is a more straightforward solution.\n\nAuthor addressed a concern about the per-page pgmap and device-to-node mappings, agreeing that NODE_DATA is the right direction regardless of struct page's future or zone it lives in.\n\nThe author acknowledges that implementing mempolicy support for N_MEMORY_PRIVATE is more complex than initially thought, explaining that it requires adding code to vma_alloc_folio_noprof and dealing with ZONE_DEVICE's overloaded nature. They suggest two options: putting pages in the buddy or adding pgmap->device_alloc() callbacks at every allocation site.\n\nAuthor acknowledged reviewer's concern about reusing mm/ services and explained that using the buddy underpins the rest of these services, making it a more efficient approach.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a fix is needed",
                "proposes an alternative solution",
                "acknowledges feedback",
                "provides explanation",
                "agreed with reviewer's suggestion",
                "provided explanation",
                "acknowledges complexity",
                "suggests two options",
                "acknowledged",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree that buddy-access alone is insufficient justification, it\nstarted off that way - but if you want mempolicy/NUMA UAPI access,\nit turns into \"Re-use all of MM\" - and that means using the buddy.\n\nI also expected ZONE_DEVICE vs NODE_DATA to be the primary discussion,\n\nI raise replacing it as a thought experiment, but not the proposal.\n\nThe idea that drm/ is going to switch to private nodes is outside the\nrealm of reality, but part of that is because of years of infrastructure\nbuilt on the assumption that re-using mm/ is infeasible.\n\nBut, lets talk about DEVICE_COHERENT\n\n---\n\nDEVICE_COHERENT is the odd-man out among ZONE_DEVICE modes. The others\nuse softleaf entries and don't allow direct mappings.\n\n(DEVICE_PRIVATE sort of does if you squint, but you can also view that\n a bit like PROT_NONE or read-only controls to force migrations).\n\nIf you take DEVICE_COHERENT and:\n\n- Move pgmap out of the struct page (page_ext, NODE_DATA, etc) to free\n  the LRU list_head\n- Put pages in the buddy (free lists, watermarks, managed_pages) or add\n  pgmap->device_alloc() at every allocation callsite / buddy hook\n- Add LRU support (aging, reclaim, compaction)\n- Add isolated gating (new GFP flag and adjusted zonelist filtering)\n- Add new dev_pagemap_ops callbacks for the various mm/ features\n- Audit evey folio_is_zone_device() to distinguish zone device modes\n\n... you've built N_MEMORY_PRIVATE inside ZONE_DEVICE. Except now\npage_zone(page) returns ZONE_DEVICE - so you inherit the wrong\ndefaults at every existing ZONE_DEVICE check. \n\nSkip-sites become things to opt-out of instead of opting into.\n\nYou just end up with\n\nif (folio_is_zone_device(folio))\n    if (folio_is_my_special_zone_device())\n    else ....\n\nand this just generalizes to\n\nif (folio_is_private_managed(folio))\n    folio_managed_my_hooked_operation()\n\nSo you get the same code, but have added more complexity to ZONE_DEVICE.\n\nI don't think that's needed if we just recognize ZONE is the wrong\nabstraction to be operating on.\n\nHonestly, even ZONE_MOVABLE becomes pointless with N_MEMORY_PRIVATE\nif you disallow longterm pinning - because the managing service handles\nallocations (it has to inject GFP_PRIVATE to get access) or selectively\nenables the mm/ services it knows are safe (mempolicy).\n\nEven if you allow longterm pinning, if your service controls what does\nthe pinning it can still be reclaimable - just manually (killing\nprocesses) instead of letting hotplug do it via migration.\n\nIf your service only allocates movable pages - your ZONE_NORMAL is\neffectively ZONE_MOVABLE.  \n\nIn some cases we use ZONE_MOVABLE to prevent the kernel from allocating\nmemory onto devices (like CXL).  This means struct page is forced to\ntake up DRAM or use memmap_on_memory - meaning you lose high-value\ncapacity or sacrifice contiguity (less huge page support).\n\nThis entire problem can evaporate if you can just use ZONE_NORMAL.\n\nThere are a lot of benefits to just re-using the buddy like this.\n\nZones are the wrong abstraction and cause more problems.\n\n---\n\nYou don't have to squint because it was deliberate :]\n\nThe callback similarity is the feature - they're the same logical\noperations.  The difference is the direction of the defaults.\n\nExtending ZONE_DEVICE into these areas requires the same set of hooks,\nplus distinguishing \"old ZONE_DEVICE\" from \"new ZONE_DEVICE\".\n\nWhere there are new injection sites, it's because ZONE_DEVICE opts\nout of ever touching that code in some other silently implied way.\n\nFor example, reclaim/compaction doesn't run because ZONE_DEVICE doesn't\nadd to managed_pages (among other reasons).\n\nYou'd have to go figure out how to hack those things into ZONE_DEVICE \n*and then* opt every *other* ZONE_DEVICE mode *back out*.\n\nSo you still end up with something like this anyway:\n\nstatic inline bool folio_managed_handle_fault(struct folio *folio,\n                                              struct vm_fault *vmf,\n                                              enum pgtable_level level,\n                                              vm_fault_t *ret)\n{\n        /* Zone device pages use swap entries; handled in do_swap_page */\n        if (folio_is_zone_device(folio))\n                return false;\n\n        if (folio_is_private_node(folio))\n\t\t...\n        return false;\n}\n\n---\n\nIf NUMA is the interface we want, then NODE_DATA is the right direction\nregardless of struct page's future or what zone it lives in.\n\nThere's no reason to keep per-page pgmap w/ device-to-node mappings.\n\nYou can have one driver manage multiple devices with the same numa node\nif it uses the same owner context (PFN already differentiates devices).\n\nThe existing code allows for this.\n\n---\n\nOn (1): ZONE_DEVICE NUMA UAPI is harder than it looks from the surface\n\nMuch of the kernel mm/ infrastructure is written on top of the buddy and\nexpects N_MEMORY to be the sole arbiter of \"Where to Acquire Pages\".\n\nMempolicy depends on:\n   - Buddy support or a new alloc hook around the buddy\n\n   - Migration support (mbind() after allocation migrates)\n     - Migration also deeply assumes buddy and LRU support\n\n   - Changing validations on node states\n     - mempolicy checks N_MEMORY membership, so you have to hack\n       N_MEMORY onto ZONE_DEVICE\n       (or teach it about a new node state... N_MEMORY_PRIVATE)\n\n\nGetting mempolicy to work with N_MEMORY_PRIVATE amounts to adding 2\nlines of code in vma_alloc_folio_noprof:\n\nstruct folio *vma_alloc_folio_noprof(gfp_t gfp, int order,\n                                     struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr)\n{\n        if (pol->flags & MPOL_F_PRIVATE)\n                gfp |= __GFP_PRIVATE;\n\n        folio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n\t/* Woo! I faulted a DEVICE PAGE! */\n}\n\nBut this requires the pages to be managed by the buddy.\n\nThe rest of the mempolicy support is around keeping sane nodemasks when\nthings like cpuset.mems rebinds occur and validating you don't end up\nwith private nodes that don't support mempolicy in your nodemask.\n\nYou have to do all of this anyway, but with the added bonus of fighting\nwith the overloaded nature of ZONE_DEVICE at every step.\n\n==========\n\nOn (2): Assume you solve LRU. \n\nZone Device has no free lists, managed_pages, or watermarks.\n\nkswapd can't run, compaction has no targets, vmscan's pressure model\ndoesn't function.  These all come for free when the pages are\nbuddy-managed on a real zone.  Why re-invent the wheel?\n\n==========\n\nSo you really have two options here:\n\na) Put pages in the buddy, or\n\nb) Add pgmap->device_alloc() callbacks at every allocation site that\n   could target a node:\n     - vma_alloc_folio\n     - alloc_migration_target\n     - alloc_demote_folio\n     - alloc_pages_node\n     - alloc_contig_pages\n     - list goes on\n\nOr more likely - hooking get_page_from_freelist.  Which at that\npoint... just use the buddy?  You're already deep in the hot path.\n\n---\n\nUsing the buddy underpins the rest of mm/ services we want to re-use.\n\nThat's basically it.  Otherwise you have to inject hooks into every\nsurface that touches the buddy...\n\n... or in the buddy (get_page_from_freelist), at which point why not\njust use the buddy?\n\n~Gregory",
              "reply_to": "Alistair Popple",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author considered reviewer's suggestion to simplify patch by removing N_MEMORY_PRIVATE and instead checking NODE_DATA(target_nid)->private, agreeing to explore this alternative.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "considering alternative approach",
                "agrees to look at it more"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This gave me something to chew on\n\nI think this can be done without introducing N_MEMORY_PRIVATE and just\nchecking:   NODE_DATA(target_nid)->private\n\nmeaning these nodes can just be N_MEMORY with the same isolations.\n\nI'll look at this a bit more.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v1 0/3] cxl region changes for Type2 support",
          "message_id": "aZ3MoKZgs26C2PrZ@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3MoKZgs26C2PrZ@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T16:07:00Z",
          "in_reply_to": null,
          "ack_type": "Tested-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about the need to specify the target_type parameter in the __create_region function, which was previously missing for Type2 devices. The author agrees that this change is necessary and will be included in the next version of the patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nIn preparation for Type2 devices/drivers support, these next patches\nadapt the cxl region code for required Type2 functionality preserving\ncurrent functionality.\n\nAlejandro Lucero (3):\n  cxl: Make region type based on endpoint type\n  cxl/region: Factor out interleave ways setup\n  cxl/region: Factor out interleave granularity setup\n\n drivers/cxl/core/region.c | 87 +++++++++++++++++++++++++--------------\n 1 file changed, 56 insertions(+), 31 deletions(-)\n\n\nbase-commit: 6de23f81a5e08be8fbf5e8d7e9febc72a5b5f27f\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the locking mechanism in the interleave ways store function, explaining that they will factor out a common helper from the user-sysfs region setup for interleave ways to prepare for kernel-driven region creation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup for interleave ways.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/region.c | 41 +++++++++++++++++++++++++--------------\n 1 file changed, 26 insertions(+), 15 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex cac33c99fe6a..3ef4ccf1c92b 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n \n static const struct attribute_group *get_cxl_region_target_group(void);\n \n-static ssize_t interleave_ways_store(struct device *dev,\n-\t\t\t\t     struct device_attribute *attr,\n-\t\t\t\t     const char *buf, size_t len)\n+static int set_interleave_ways(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_root_decoder *cxlrd = cxlr->cxlrd;\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tunsigned int val, save;\n-\tint rc;\n+\tint save, rc;\n \tu8 iw;\n \n-\trc = kstrtouint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = ways_to_eiw(val, &iw);\n \tif (rc)\n \t\treturn rc;\n@@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n \t\treturn -EINVAL;\n \t}\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n@@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n \tsave = p->interleave_ways;\n \tp->interleave_ways = val;\n \trc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n-\tif (rc) {\n+\tif (rc)\n \t\tp->interleave_ways = save;\n+\n+\treturn rc;\n+}\n+\n+static ssize_t interleave_ways_store(struct device *dev,\n+\t\t\t\t     struct device_attribute *attr,\n+\t\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tunsigned int val;\n+\tint rc;\n+\n+\trc = kstrtouint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_ways(cxlr, val);\n+\tif (rc)\n \t\treturn rc;\n-\t}\n \n \treturn len;\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the interleave granularity store function being called without acquiring the region lock, and explained that they will factor out a common helper from the user-sysfs region setup to address this issue.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup forinterleave\ngranularity.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/region.c | 36 ++++++++++++++++++++++++------------\n 1 file changed, 24 insertions(+), 12 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 3ef4ccf1c92b..aed3733490a1 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -559,21 +559,14 @@ static ssize_t interleave_granularity_show(struct device *dev,\n \treturn sysfs_emit(buf, \"%d\\n\", p->interleave_granularity);\n }\n \n-static ssize_t interleave_granularity_store(struct device *dev,\n-\t\t\t\t\t    struct device_attribute *attr,\n-\t\t\t\t\t    const char *buf, size_t len)\n+static int set_interleave_granularity(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_root_decoder *cxlrd = cxlr->cxlrd;\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tint rc, val;\n+\tint rc;\n \tu16 ig;\n \n-\trc = kstrtoint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = granularity_to_eig(val, &ig);\n \tif (rc)\n \t\treturn rc;\n@@ -589,14 +582,33 @@ static ssize_t interleave_granularity_store(struct device *dev,\n \tif (cxld->interleave_ways > 1 && val != cxld->interleave_granularity)\n \t\treturn -EINVAL;\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n \n \tp->interleave_granularity = val;\n+\treturn 0;\n+}\n+\n+static ssize_t interleave_granularity_store(struct device *dev,\n+\t\t\t\t\t    struct device_attribute *attr,\n+\t\t\t\t\t    const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tint rc, val;\n+\n+\trc = kstrtoint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_granularity(cxlr, val);\n+\tif (rc)\n+\t\treturn rc;\n \n \treturn len;\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer Dave Jiang questioned whether the patch set is dependent on a separate preparation set, requesting clarification on their relationship.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "dependency_question"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Does this set depends on the preparation set [1] or is independent?\n\n[1]: https://lore.kernel.org/linux-cxl/20260223142633.2994082-1-alejandro.lucero-palau@amd.com/T/#t",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer Dave Jiang suggested that the patch should account for auto region creation, pointing out that the current logic may not handle this case correctly.\n\nGave Reviewed-by",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "patch needs further review"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "s/is/can be/\n\nSince there's also auto region creation.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price noted that the additional lockdep is a nice addition and has been testing the patches, indicating they are working as expected.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Tested-by"
              ],
              "raw_body": "I've been testing with these for a while - the additional lockdep is a\nnice addition.  So for series\n\nTested-by: Gregory Price <gourry@gourry.net>",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "reviewer confirmed that the patch applies cleanly and is independent from the others, but did not provide any technical feedback or suggestions",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no specific technical concerns raised"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Dave,\n\n\nIt is independent, but it applies cleanly on top of the previous ones as \ncore/region.c is not modified by the other.\n\n\nThank you",
              "reply_to": "Dave Jiang",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v1 3/3] cxl/region: Factor out interleave granularity setup",
          "message_id": "aZ3MdkPQf-5aXZ9j@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3MdkPQf-5aXZ9j@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T16:06:18Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about the need to specify the target_type parameter in the __create_region function, which was previously missing for Type2 devices. The author agrees that this change is necessary and will be included in the next version of the patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nIn preparation for Type2 devices/drivers support, these next patches\nadapt the cxl region code for required Type2 functionality preserving\ncurrent functionality.\n\nAlejandro Lucero (3):\n  cxl: Make region type based on endpoint type\n  cxl/region: Factor out interleave ways setup\n  cxl/region: Factor out interleave granularity setup\n\n drivers/cxl/core/region.c | 87 +++++++++++++++++++++++++--------------\n 1 file changed, 56 insertions(+), 31 deletions(-)\n\n\nbase-commit: 6de23f81a5e08be8fbf5e8d7e9febc72a5b5f27f\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the locking mechanism in the interleave ways store function, explaining that they will factor out a common helper from the user-sysfs region setup for interleave ways to prepare for kernel-driven region creation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup for interleave ways.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/region.c | 41 +++++++++++++++++++++++++--------------\n 1 file changed, 26 insertions(+), 15 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex cac33c99fe6a..3ef4ccf1c92b 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n \n static const struct attribute_group *get_cxl_region_target_group(void);\n \n-static ssize_t interleave_ways_store(struct device *dev,\n-\t\t\t\t     struct device_attribute *attr,\n-\t\t\t\t     const char *buf, size_t len)\n+static int set_interleave_ways(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_root_decoder *cxlrd = cxlr->cxlrd;\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tunsigned int val, save;\n-\tint rc;\n+\tint save, rc;\n \tu8 iw;\n \n-\trc = kstrtouint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = ways_to_eiw(val, &iw);\n \tif (rc)\n \t\treturn rc;\n@@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n \t\treturn -EINVAL;\n \t}\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n@@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n \tsave = p->interleave_ways;\n \tp->interleave_ways = val;\n \trc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n-\tif (rc) {\n+\tif (rc)\n \t\tp->interleave_ways = save;\n+\n+\treturn rc;\n+}\n+\n+static ssize_t interleave_ways_store(struct device *dev,\n+\t\t\t\t     struct device_attribute *attr,\n+\t\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tunsigned int val;\n+\tint rc;\n+\n+\trc = kstrtouint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_ways(cxlr, val);\n+\tif (rc)\n \t\treturn rc;\n-\t}\n \n \treturn len;\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the interleave granularity store function being called without acquiring the region lock, and explained that they will factor out a common helper from the user-sysfs region setup to address this issue.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup forinterleave\ngranularity.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/region.c | 36 ++++++++++++++++++++++++------------\n 1 file changed, 24 insertions(+), 12 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 3ef4ccf1c92b..aed3733490a1 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -559,21 +559,14 @@ static ssize_t interleave_granularity_show(struct device *dev,\n \treturn sysfs_emit(buf, \"%d\\n\", p->interleave_granularity);\n }\n \n-static ssize_t interleave_granularity_store(struct device *dev,\n-\t\t\t\t\t    struct device_attribute *attr,\n-\t\t\t\t\t    const char *buf, size_t len)\n+static int set_interleave_granularity(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_root_decoder *cxlrd = cxlr->cxlrd;\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tint rc, val;\n+\tint rc;\n \tu16 ig;\n \n-\trc = kstrtoint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = granularity_to_eig(val, &ig);\n \tif (rc)\n \t\treturn rc;\n@@ -589,14 +582,33 @@ static ssize_t interleave_granularity_store(struct device *dev,\n \tif (cxld->interleave_ways > 1 && val != cxld->interleave_granularity)\n \t\treturn -EINVAL;\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n \n \tp->interleave_granularity = val;\n+\treturn 0;\n+}\n+\n+static ssize_t interleave_granularity_store(struct device *dev,\n+\t\t\t\t\t    struct device_attribute *attr,\n+\t\t\t\t\t    const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tint rc, val;\n+\n+\trc = kstrtoint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_granularity(cxlr, val);\n+\tif (rc)\n+\t\treturn rc;\n \n \treturn len;\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer Dave Jiang questioned whether the patch set is dependent on a separate preparation set, requesting clarification on their relationship.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "dependency_question"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Does this set depends on the preparation set [1] or is independent?\n\n[1]: https://lore.kernel.org/linux-cxl/20260223142633.2994082-1-alejandro.lucero-palau@amd.com/T/#t",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer Dave Jiang suggested that the patch should account for auto region creation, pointing out that the current logic may not handle this case correctly.\n\nGave Reviewed-by",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "patch needs further review"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "s/is/can be/\n\nSince there's also auto region creation.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price noted that the additional lockdep is a nice addition and has been testing the patches, indicating they are working as expected.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Tested-by"
              ],
              "raw_body": "I've been testing with these for a while - the additional lockdep is a\nnice addition.  So for series\n\nTested-by: Gregory Price <gourry@gourry.net>",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "reviewer confirmed that the patch applies cleanly and is independent from the others, but did not provide any technical feedback or suggestions",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no specific technical concerns raised"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Dave,\n\n\nIt is independent, but it applies cleanly on top of the previous ones as \ncore/region.c is not modified by the other.\n\n\nThank you",
              "reply_to": "Dave Jiang",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v1 2/3] cxl/region: Factor out interleave ways setup",
          "message_id": "aZ3MUyOy2JgavERa@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3MUyOy2JgavERa@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T16:05:43Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about the need to specify the target_type parameter in the __create_region function, which was previously missing for Type2 devices. The author agrees that this change is necessary and will be included in the next version of the patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nIn preparation for Type2 devices/drivers support, these next patches\nadapt the cxl region code for required Type2 functionality preserving\ncurrent functionality.\n\nAlejandro Lucero (3):\n  cxl: Make region type based on endpoint type\n  cxl/region: Factor out interleave ways setup\n  cxl/region: Factor out interleave granularity setup\n\n drivers/cxl/core/region.c | 87 +++++++++++++++++++++++++--------------\n 1 file changed, 56 insertions(+), 31 deletions(-)\n\n\nbase-commit: 6de23f81a5e08be8fbf5e8d7e9febc72a5b5f27f\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the locking mechanism in the interleave ways store function, explaining that they will factor out a common helper from the user-sysfs region setup for interleave ways to prepare for kernel-driven region creation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup for interleave ways.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/region.c | 41 +++++++++++++++++++++++++--------------\n 1 file changed, 26 insertions(+), 15 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex cac33c99fe6a..3ef4ccf1c92b 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n \n static const struct attribute_group *get_cxl_region_target_group(void);\n \n-static ssize_t interleave_ways_store(struct device *dev,\n-\t\t\t\t     struct device_attribute *attr,\n-\t\t\t\t     const char *buf, size_t len)\n+static int set_interleave_ways(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_root_decoder *cxlrd = cxlr->cxlrd;\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tunsigned int val, save;\n-\tint rc;\n+\tint save, rc;\n \tu8 iw;\n \n-\trc = kstrtouint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = ways_to_eiw(val, &iw);\n \tif (rc)\n \t\treturn rc;\n@@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n \t\treturn -EINVAL;\n \t}\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n@@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n \tsave = p->interleave_ways;\n \tp->interleave_ways = val;\n \trc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n-\tif (rc) {\n+\tif (rc)\n \t\tp->interleave_ways = save;\n+\n+\treturn rc;\n+}\n+\n+static ssize_t interleave_ways_store(struct device *dev,\n+\t\t\t\t     struct device_attribute *attr,\n+\t\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tunsigned int val;\n+\tint rc;\n+\n+\trc = kstrtouint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_ways(cxlr, val);\n+\tif (rc)\n \t\treturn rc;\n-\t}\n \n \treturn len;\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the interleave granularity store function being called without acquiring the region lock, and explained that they will factor out a common helper from the user-sysfs region setup to address this issue.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup forinterleave\ngranularity.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/region.c | 36 ++++++++++++++++++++++++------------\n 1 file changed, 24 insertions(+), 12 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 3ef4ccf1c92b..aed3733490a1 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -559,21 +559,14 @@ static ssize_t interleave_granularity_show(struct device *dev,\n \treturn sysfs_emit(buf, \"%d\\n\", p->interleave_granularity);\n }\n \n-static ssize_t interleave_granularity_store(struct device *dev,\n-\t\t\t\t\t    struct device_attribute *attr,\n-\t\t\t\t\t    const char *buf, size_t len)\n+static int set_interleave_granularity(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_root_decoder *cxlrd = cxlr->cxlrd;\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tint rc, val;\n+\tint rc;\n \tu16 ig;\n \n-\trc = kstrtoint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = granularity_to_eig(val, &ig);\n \tif (rc)\n \t\treturn rc;\n@@ -589,14 +582,33 @@ static ssize_t interleave_granularity_store(struct device *dev,\n \tif (cxld->interleave_ways > 1 && val != cxld->interleave_granularity)\n \t\treturn -EINVAL;\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n \n \tp->interleave_granularity = val;\n+\treturn 0;\n+}\n+\n+static ssize_t interleave_granularity_store(struct device *dev,\n+\t\t\t\t\t    struct device_attribute *attr,\n+\t\t\t\t\t    const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tint rc, val;\n+\n+\trc = kstrtoint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_granularity(cxlr, val);\n+\tif (rc)\n+\t\treturn rc;\n \n \treturn len;\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer Dave Jiang questioned whether the patch set is dependent on a separate preparation set, requesting clarification on their relationship.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "dependency_question"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Does this set depends on the preparation set [1] or is independent?\n\n[1]: https://lore.kernel.org/linux-cxl/20260223142633.2994082-1-alejandro.lucero-palau@amd.com/T/#t",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer Dave Jiang suggested that the patch should account for auto region creation, pointing out that the current logic may not handle this case correctly.\n\nGave Reviewed-by",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "patch needs further review"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "s/is/can be/\n\nSince there's also auto region creation.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price noted that the additional lockdep is a nice addition and has been testing the patches, indicating they are working as expected.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Tested-by"
              ],
              "raw_body": "I've been testing with these for a while - the additional lockdep is a\nnice addition.  So for series\n\nTested-by: Gregory Price <gourry@gourry.net>",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "reviewer confirmed that the patch applies cleanly and is independent from the others, but did not provide any technical feedback or suggestions",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no specific technical concerns raised"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Dave,\n\n\nIt is independent, but it applies cleanly on top of the previous ones as \ncore/region.c is not modified by the other.\n\n\nThank you",
              "reply_to": "Dave Jiang",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v1 1/3] cxl: Make region type based on endpoint type",
          "message_id": "aZ3MHVcheVVoooiC@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ3MHVcheVVoooiC@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-24T16:04:48Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about the need to specify the target_type parameter in the __create_region function, which was previously missing for Type2 devices. The author agrees that this change is necessary and will be included in the next version of the patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nIn preparation for Type2 devices/drivers support, these next patches\nadapt the cxl region code for required Type2 functionality preserving\ncurrent functionality.\n\nAlejandro Lucero (3):\n  cxl: Make region type based on endpoint type\n  cxl/region: Factor out interleave ways setup\n  cxl/region: Factor out interleave granularity setup\n\n drivers/cxl/core/region.c | 87 +++++++++++++++++++++++++--------------\n 1 file changed, 56 insertions(+), 31 deletions(-)\n\n\nbase-commit: 6de23f81a5e08be8fbf5e8d7e9febc72a5b5f27f\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the locking mechanism in the interleave ways store function, explaining that they will factor out a common helper from the user-sysfs region setup for interleave ways to prepare for kernel-driven region creation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup for interleave ways.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/region.c | 41 +++++++++++++++++++++++++--------------\n 1 file changed, 26 insertions(+), 15 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex cac33c99fe6a..3ef4ccf1c92b 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n \n static const struct attribute_group *get_cxl_region_target_group(void);\n \n-static ssize_t interleave_ways_store(struct device *dev,\n-\t\t\t\t     struct device_attribute *attr,\n-\t\t\t\t     const char *buf, size_t len)\n+static int set_interleave_ways(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_root_decoder *cxlrd = cxlr->cxlrd;\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tunsigned int val, save;\n-\tint rc;\n+\tint save, rc;\n \tu8 iw;\n \n-\trc = kstrtouint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = ways_to_eiw(val, &iw);\n \tif (rc)\n \t\treturn rc;\n@@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n \t\treturn -EINVAL;\n \t}\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n@@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n \tsave = p->interleave_ways;\n \tp->interleave_ways = val;\n \trc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n-\tif (rc) {\n+\tif (rc)\n \t\tp->interleave_ways = save;\n+\n+\treturn rc;\n+}\n+\n+static ssize_t interleave_ways_store(struct device *dev,\n+\t\t\t\t     struct device_attribute *attr,\n+\t\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tunsigned int val;\n+\tint rc;\n+\n+\trc = kstrtouint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_ways(cxlr, val);\n+\tif (rc)\n \t\treturn rc;\n-\t}\n \n \treturn len;\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the interleave granularity store function being called without acquiring the region lock, and explained that they will factor out a common helper from the user-sysfs region setup to address this issue.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup forinterleave\ngranularity.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/region.c | 36 ++++++++++++++++++++++++------------\n 1 file changed, 24 insertions(+), 12 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 3ef4ccf1c92b..aed3733490a1 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -559,21 +559,14 @@ static ssize_t interleave_granularity_show(struct device *dev,\n \treturn sysfs_emit(buf, \"%d\\n\", p->interleave_granularity);\n }\n \n-static ssize_t interleave_granularity_store(struct device *dev,\n-\t\t\t\t\t    struct device_attribute *attr,\n-\t\t\t\t\t    const char *buf, size_t len)\n+static int set_interleave_granularity(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_root_decoder *cxlrd = cxlr->cxlrd;\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tint rc, val;\n+\tint rc;\n \tu16 ig;\n \n-\trc = kstrtoint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = granularity_to_eig(val, &ig);\n \tif (rc)\n \t\treturn rc;\n@@ -589,14 +582,33 @@ static ssize_t interleave_granularity_store(struct device *dev,\n \tif (cxld->interleave_ways > 1 && val != cxld->interleave_granularity)\n \t\treturn -EINVAL;\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n \n \tp->interleave_granularity = val;\n+\treturn 0;\n+}\n+\n+static ssize_t interleave_granularity_store(struct device *dev,\n+\t\t\t\t\t    struct device_attribute *attr,\n+\t\t\t\t\t    const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tint rc, val;\n+\n+\trc = kstrtoint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_granularity(cxlr, val);\n+\tif (rc)\n+\t\treturn rc;\n \n \treturn len;\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer Dave Jiang questioned whether the patch set is dependent on a separate preparation set, requesting clarification on their relationship.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "dependency_question"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Does this set depends on the preparation set [1] or is independent?\n\n[1]: https://lore.kernel.org/linux-cxl/20260223142633.2994082-1-alejandro.lucero-palau@amd.com/T/#t",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer Dave Jiang suggested that the patch should account for auto region creation, pointing out that the current logic may not handle this case correctly.\n\nGave Reviewed-by",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "patch needs further review"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "s/is/can be/\n\nSince there's also auto region creation.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price noted that the additional lockdep is a nice addition and has been testing the patches, indicating they are working as expected.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Tested-by"
              ],
              "raw_body": "I've been testing with these for a while - the additional lockdep is a\nnice addition.  So for series\n\nTested-by: Gregory Price <gourry@gourry.net>",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "reviewer confirmed that the patch applies cleanly and is independent from the others, but did not provide any technical feedback or suggestions",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no specific technical concerns raised"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Dave,\n\n\nIt is independent, but it applies cleanly on top of the previous ones as \ncore/region.c is not modified by the other.\n\n\nThank you",
              "reply_to": "Dave Jiang",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Jeff Layton",
      "primary_email": "jlayton@kernel.org",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] nfsd: fix heap overflow in NFSv4.0 LOCK replay cache",
          "message_id": "20260224-v4-0-lock-overflow-v1-1-22beeaf5cf6b@kernel.org",
          "url": "https://lore.kernel.org/all/20260224-v4-0-lock-overflow-v1-1-22beeaf5cf6b@kernel.org/",
          "date": "2026-02-24T16:33:44Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch fixes a heap overflow vulnerability in the NFSv4.0 LOCK replay cache by adding bounds checking to prevent oversized responses from being copied into an undersized buffer.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Chuck Lever",
              "summary": "Identified the patch as a fix for a known issue and applied it to nfsd-testing without requesting any changes.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "Applied"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nOn Tue, 24 Feb 2026 11:33:35 -0500, Jeff Layton wrote:\n> The NFSv4.0 replay cache uses a fixed 112-byte inline buffer\n> (rp_ibuf[NFSD4_REPLAY_ISIZE]) to store encoded operation responses.\n> This size was calculated based on OPEN responses and does not account\n> for LOCK denied responses, which include the conflicting lock owner as\n> a variable-length field up to 1024 bytes (NFS4_OPAQUE_LIMIT).\n> \n> When a LOCK operation is denied due to a conflict with an existing lock\n> that has a large owner, nfsd4_encode_operation() copies the full encoded\n> response into the undersized replay buffer via read_bytes_from_xdr_buf()\n> with no bounds check. This results in a slab-out-of-bounds write of up\n> to 944 bytes past the end of the buffer, corrupting adjacent heap memory.\n> \n> [...]\n\nApplied to nfsd-testing, thanks!\n\n[1/1] nfsd: fix heap overflow in NFSv4.0 LOCK replay cache\n      commit: 1e8e9913672a31c6fdd0d237cd3cec88435bd66e\n\n--\nChuck Lever\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] nfsd: convert global state_lock to per-net deleg_lock",
          "message_id": "20260224-nfsd-deleg-lock-v1-1-1df17c1daa47@kernel.org",
          "url": "https://lore.kernel.org/all/20260224-nfsd-deleg-lock-v1-1-1df17c1daa47@kernel.org/",
          "date": "2026-02-24T13:28:24Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch converts the global state_lock spinlock in the NFS daemon to a per-network namespace deleg_lock. This change is motivated by the fact that the state_lock was only used to protect delegation lifecycle operations, which are scoped to a single network namespace. By making the lock per-net, the patch reduces unnecessary contention between containers and improves overall system performance.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Chuck Lever",
              "summary": "Reviewer noted that the patch does not update all lockdep annotations to use the new per-net deleg_lock, which could lead to incorrect locking assumptions and potential deadlocks.\n\nReviewer noted that the patch does not update all lockdep annotations to use the new per-net deleg_lock, which could lead to incorrect locking assumptions and potential deadlocks.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "lockdep",
                "potential deadlock",
                "potential deadlocks"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nOn Tue, 24 Feb 2026 08:28:11 -0500, Jeff Layton wrote:\n\n---\n\nApplied to nfsd-testing with minor changes, thanks!\n\n[1/1] nfsd: convert global state_lock to per-net deleg_lock\n      commit: 87d8659010fe5ba78759ad7b8780656f1c3d350a\n\n--\nChuck Lever",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] nfsd: use dynamic allocation for oversized NFSv4.0 replay cache",
          "message_id": "f16c1806a705e08252b1b39ea44b1de1e6be17d6.camel@kernel.org",
          "url": "https://lore.kernel.org/all/f16c1806a705e08252b1b39ea44b1de1e6be17d6.camel@kernel.org/",
          "date": "2026-02-24T19:59:19Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "Reviewer questioned the necessity of dynamic allocation for oversized replay cache, asking if correctness is a priority when dealing with large lockowners on NFSv4.0",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "questioning correctness",
                "asking about priority"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Certainly a reasonable approach if we care about full correctness when\ndealing with a large lockowner on NFSv4.0. Do we?\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Chuck Lever (author)",
              "summary": "Author acknowledges that the patch may not be suitable for backporting due to its dependency on a newer fix, suggests either backporting the newer fix or squashing both patches together",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledges complexity",
                "suggests alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The idea would be to either:\n\no Backport your fix and not this update, or\no Squash these two together, and backport both\n\nAdmittedly this is a narrow corner case for a minor version that is\ndestined for the scrap heap.\n\n\n-- \nChuck Lever",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton expressed skepticism about the dynamic allocation approach for oversized NFSv4.0 replay cache, stating that he previously considered a similar solution but decided it was not worthwhile and doubted any legitimate user would care about handling long lockowner blobs.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "skepticism",
                "doubt"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right. I ask because I looked at this approach when I was fixing this,\nand decided it wasn't worthwhile. I certainly won't stand in your way\nif you decide you want to handle long lockowner blobs, but I doubt any\nlegitimate user will ever care.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Chuck Lever (author)",
              "summary": "Author acknowledges the need for a more compliant replay cache handling, but suggests an alternative approach without committing to a specific fix.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges issue",
                "suggests alternative"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't disagree at all. My concern is handling replay compliantly.\nMaybe there's another approach.\n\n\n-- \nChuck Lever",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton expressed that dynamic allocation for oversized NFSv4.0 replay cache is preferable to increasing NFSD4_REPLAY_ISIZE, and has no issue with the patch itself.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK",
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think that the only other way is to grow NFSD4_REPLAY_ISIZE, and\ndoing dynamic allocation is preferable to that, IMO.\n\nTo be clear: I don't have a problem with your patch. It just didn't\nseem worthwhile to me. If you think it's worth fixing though, then go\nfor it.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] nfsd: use dynamic allocation for oversized NFSv4.0 replay cache",
          "message_id": "5c6b6e52619caf720912639697af5b388a3ea79a.camel@kernel.org",
          "url": "https://lore.kernel.org/all/5c6b6e52619caf720912639697af5b388a3ea79a.camel@kernel.org/",
          "date": "2026-02-24T19:51:44Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "Reviewer questioned the necessity of dynamic allocation for oversized replay cache, asking if correctness is a priority when dealing with large lockowners on NFSv4.0",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "questioning correctness",
                "asking about priority"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Certainly a reasonable approach if we care about full correctness when\ndealing with a large lockowner on NFSv4.0. Do we?\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Chuck Lever (author)",
              "summary": "Author acknowledges that the patch may not be suitable for backporting due to its dependency on a newer fix, suggests either backporting the newer fix or squashing both patches together",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledges complexity",
                "suggests alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The idea would be to either:\n\no Backport your fix and not this update, or\no Squash these two together, and backport both\n\nAdmittedly this is a narrow corner case for a minor version that is\ndestined for the scrap heap.\n\n\n-- \nChuck Lever",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton expressed skepticism about the dynamic allocation approach for oversized NFSv4.0 replay cache, stating that he previously considered a similar solution but decided it was not worthwhile and doubted any legitimate user would care about handling long lockowner blobs.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "skepticism",
                "doubt"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right. I ask because I looked at this approach when I was fixing this,\nand decided it wasn't worthwhile. I certainly won't stand in your way\nif you decide you want to handle long lockowner blobs, but I doubt any\nlegitimate user will ever care.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Chuck Lever (author)",
              "summary": "Author acknowledges the need for a more compliant replay cache handling, but suggests an alternative approach without committing to a specific fix.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges issue",
                "suggests alternative"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't disagree at all. My concern is handling replay compliantly.\nMaybe there's another approach.\n\n\n-- \nChuck Lever",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton expressed that dynamic allocation for oversized NFSv4.0 replay cache is preferable to increasing NFSD4_REPLAY_ISIZE, and has no issue with the patch itself.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK",
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think that the only other way is to grow NFSD4_REPLAY_ISIZE, and\ndoing dynamic allocation is preferable to that, IMO.\n\nTo be clear: I don't have a problem with your patch. It just didn't\nseem worthwhile to me. If you think it's worth fixing though, then go\nfor it.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] nfsd: use dynamic allocation for oversized NFSv4.0 replay cache",
          "message_id": "887c1ca78b34974160dee3ce7f25d6d077da93ab.camel@kernel.org",
          "url": "https://lore.kernel.org/all/887c1ca78b34974160dee3ce7f25d6d077da93ab.camel@kernel.org/",
          "date": "2026-02-24T19:39:52Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "Reviewer questioned the necessity of dynamic allocation for oversized replay cache, asking if correctness is a priority when dealing with large lockowners on NFSv4.0",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "questioning correctness",
                "asking about priority"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Certainly a reasonable approach if we care about full correctness when\ndealing with a large lockowner on NFSv4.0. Do we?\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Chuck Lever (author)",
              "summary": "Author acknowledges that the patch may not be suitable for backporting due to its dependency on a newer fix, suggests either backporting the newer fix or squashing both patches together",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledges complexity",
                "suggests alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The idea would be to either:\n\no Backport your fix and not this update, or\no Squash these two together, and backport both\n\nAdmittedly this is a narrow corner case for a minor version that is\ndestined for the scrap heap.\n\n\n-- \nChuck Lever",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton expressed skepticism about the dynamic allocation approach for oversized NFSv4.0 replay cache, stating that he previously considered a similar solution but decided it was not worthwhile and doubted any legitimate user would care about handling long lockowner blobs.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "skepticism",
                "doubt"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right. I ask because I looked at this approach when I was fixing this,\nand decided it wasn't worthwhile. I certainly won't stand in your way\nif you decide you want to handle long lockowner blobs, but I doubt any\nlegitimate user will ever care.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Chuck Lever (author)",
              "summary": "Author acknowledges the need for a more compliant replay cache handling, but suggests an alternative approach without committing to a specific fix.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges issue",
                "suggests alternative"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't disagree at all. My concern is handling replay compliantly.\nMaybe there's another approach.\n\n\n-- \nChuck Lever",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton expressed that dynamic allocation for oversized NFSv4.0 replay cache is preferable to increasing NFSD4_REPLAY_ISIZE, and has no issue with the patch itself.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK",
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think that the only other way is to grow NFSD4_REPLAY_ISIZE, and\ndoing dynamic allocation is preferable to that, IMO.\n\nTo be clear: I don't have a problem with your patch. It just didn't\nseem worthwhile to me. If you think it's worth fixing though, then go\nfor it.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Chuck Lever",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joanne Koong",
      "primary_email": "joannelkoong@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v1 0/1] iomap: don't mark folio uptodate if read IO has bytes pending",
          "message_id": "20260219003911.344478-1-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260219003911.344478-1-joannelkoong@gmail.com/",
          "date": "2026-02-19T00:41:04Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-19",
          "patch_summary": "This patch fixes a bug where a folio is marked uptodate even if there are still bytes pending from a read IO operation. The issue arises when the read IO size does not match the file size, causing the folio to be prematurely marked uptodate. The patch prevents this by ensuring that the folio is only marked uptodate after all pending bytes have been accounted for.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the folio uptodate state being cleared when iomap_read_end() calls folio_end_read(). They explained that if a read with bytes pending is in progress, marking the folio as uptodate here would be problematic because the XOR semantics in folio_end_read() would clear it. The author proposed to fix this by not marking the folio as uptodate if the read IO has bytes pending.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If a folio has ifs metadata attached to it and the folio is partially\nread in through an async IO helper with the rest of it then being read\nin through post-EOF zeroing or as inline data, and the helper\nsuccessfully finishes the read first, then post-EOF zeroing / reading\ninline will mark the folio as uptodate in iomap_set_range_uptodate().\n\nThis is a problem because when the read completion path later calls\niomap_read_end(), it will call folio_end_read(), which sets the uptodate\nbit using XOR semantics. Calling folio_end_read() on a folio that was\nalready marked uptodate clears the uptodate bit.\n\nFix this by not marking the folio as uptodate if the read IO has bytes\npending. The folio uptodate state will be set in the read completion\npath through iomap_end_read() -> folio_end_read().\n\nReported-by: Wei Gao <wegao@suse.com>\nSuggested-by: Sasha Levin <sashal@kernel.org>\nTested-by: Wei Gao <wegao@suse.com>\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nFixes: b2f35ac4146d (\"iomap: add caller-provided callbacks for read and readahead\")\n---\n fs/iomap/buffered-io.c | 15 ++++++++++++---\n 1 file changed, 12 insertions(+), 3 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 58887513b894..4fc5ce963feb 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -80,18 +80,27 @@ static void iomap_set_range_uptodate(struct folio *folio, size_t off,\n {\n \tstruct iomap_folio_state *ifs = folio->private;\n \tunsigned long flags;\n-\tbool uptodate = true;\n+\tbool mark_uptodate = true;\n \n \tif (folio_test_uptodate(folio))\n \t\treturn;\n \n \tif (ifs) {\n \t\tspin_lock_irqsave(&ifs->state_lock, flags);\n-\t\tuptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n+\t\t/*\n+\t\t * If a read with bytes pending is in progress, we must not call\n+\t\t * folio_mark_uptodate(). The read completion path\n+\t\t * (iomap_read_end()) will call folio_end_read(), which uses XOR\n+\t\t * semantics to set the uptodate bit. If we set it here, the XOR\n+\t\t * in folio_end_read() will clear it, leaving the folio not\n+\t\t * uptodate.\n+\t\t */\n+\t\tmark_uptodate = ifs_set_range_uptodate(folio, ifs, off, len) &&\n+\t\t\t\t!ifs->read_bytes_pending;\n \t\tspin_unlock_irqrestore(&ifs->state_lock, flags);\n \t}\n \n-\tif (uptodate)\n+\tif (mark_uptodate)\n \t\tfolio_mark_uptodate(folio);\n }\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-18",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong suggested adding a link to a relevant discussion on linux-fsdevel mailing list and CC'ing stable@vger.kernel.org for the v6.19 release, as the original patch was buried in another thread.\n\nReviewer Darrick Wong noted that the patch is a good fix, but asked how difficult it would be to write an fstest to verify its correctness.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested additional information",
                "added context",
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I would add:\n\nLink: https://lore.kernel.org/linux-fsdevel/aYbmy8JdgXwsGaPP@autotest-wegao.qe.prg2.suse.org/\nCc: <stable@vger.kernel.org> # v6.19\n\nsince the recent discussion around this was sort of buried in a\ndifferent thread, and the original patch is now in a released kernel.\n\n---\n\nYeah, that makes sense.  How difficult is this to write up as an fstest?\n\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n\n--D",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Matthew Wilcox",
              "summary": "reviewer expressed frustration that the iomap code has become overly complicated, making it difficult to explain or understand how to fix the issue of marking a folio uptodate if read IO has bytes pending",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "frustration",
                "difficulty understanding"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This isn't \"the xor thing has come back to bite us\".  This is \"the iomap\ncode is now too complicated and I cannot figure out how to explain to\nJoanne that there's really a simple way to do this\".\n\nI'm going to have to set aside my current projects and redo the iomap\nreadahead/read_folio code myself, aren't I?",
              "reply_to": "Darrick Wong",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong expressed confusion about an alternative approach mentioned in a previous thread, and requested clarification on what that simpler way is.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "lack of understanding"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Well you could try explaining to me what that simpler way is?\n\n/me gets the sense he's missing a discussion somewhere...\n\n--D",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-18",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that the issue of marking a folio uptodate after read IO has bytes pending is still present and needs further investigation, but did not commit to revising the patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged the issue persists",
                "did not commit to revising"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This is the link to the prior discussion\nhttps://lore.kernel.org/linux-fsdevel/20251223223018.3295372-1-sashal@kernel.org/T/#mbd61eaa5fd1e8922caa479720232628e39b8c9da\n\nThanks,\nJoanne",
              "reply_to": "Darrick Wong",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer Darrick Wong noted that the iomap read state management has inconsistent behavior across different IO paths, specifically highlighting issues with the read_bytes_pending field and its usage in synchronous and asynchronous reads. He suggested consolidating the read code into a single function to simplify the logic and requested fixing the bug in Linus' tree.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "<willy and I had a chat; this is a clumsy non-AI summary of it>\n\nI started looking at folio read state management in iomap, and made a\nfew observations that (I hope) match what willy's grumpy about.\n\nThere are three ways that iomap can be reading into the pagecache:\na) async ->readahead,\nb) synchronous ->read_folio (page faults), and\nc) synchronous ->read_folio_range (pagecache write).\n\n(Note that (b) can call a different ->read_folio_range than (c), though\nall implementations seem to have the same function)\n\nAll three of these IO paths share the behavior that they try to fill out\nthe folio's contents and set the corresponding folio/ifs uptodate bits\nif that succeeds.  Folio contents can come from anywhere, whether it's:\n\ni) zeroing memory,\nii) copying from an inlinedata buffer, or\niii) asynchronously fetching the contents from somewhere\n\nIn the case of (c) above, if the read fails then we fail the write, and\nif the read succeeds then we start copying to the pagecache.\n\nHowever, (a) and (b) have this additional read_bytes_pending field in\nthe ifs that implements some extra tracking.  AFAICT the purpose of this\nfield is to ensure that we don't call folio_end_read prematurely if\nthere's an async read in progress.  This can happen if iomap_iter\nreturns a negative errno on a partially processed folio, I think?\n\nread_bytes_pending is initialized to the folio_size() at the start of a\nread and subtracted from when parts of the folio are supplied, whether\nthat's synchronous zeroing or asynchronous read ioend completion.  When\nthe field reaches zero, we can then call folio_end_read().\n\nBut then there are twists, like the fact that we only call\niomap_read_init() to set read_bytes_pending if we decide to do an\nasynchronous read.  Or that iomap_read_end and iomap_finish_folio_read\nhave awfully similar code.  I think in the case of (i) and (ii) we also\ndon't touch read_pending_bytes at all, and merely set the uptodate bits?\n\nThis is confusing to me.  It would be more straightforward (I think) if\nwe just did it for all cases instead of adding more conditionals.  IOWs,\nhow hard would it be to consolidate the read code so that there's one\nfunction that iomap calls when it has filled out part of a folio.  Is\nthat possible, even though we shouldn't be calling folio_end_read during\na pagecache write?\n\nAt the end of the day, however, there's a bug in Linus' tree and we need\nto fix it, so Joanne's patch is a sufficient bandaid until we can go\nclean this up.\n\n--D",
              "reply_to": "Matthew Wilcox",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author acknowledged that the synchronous read path also has the same issue as the asynchronous path, which was previously pointed out by Darrick Wong.\n\nAuthor clarified that synchronous zeroing does not update read_bytes_pending, which addresses Darrick Wong's concern about the patch's behavior in a specific scenario.\n\nAuthor Joanne Koong is addressing Darrick Wong's feedback about consolidating synchronous ->read_folio_range() for buffered writes with the async read logic, arguing that it would add unnecessary overhead and complicate handling. She agrees that there are edge cases to consider in the async read path but expresses concerns about introducing additional complexity through zeroing and inline read paths.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a problem",
                "no fix planned",
                "clarification",
                "no clear resolution signal",
                "acknowledges feedback",
                "expresses concern"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "b) is async as well. The code for b) and a) are exactly the same (the\nlogic in iomap_read_folio_iter())\n\n---\n\nSynchronous zeroing does not update read_bytes_pending, only async\nread completions do.\n\n---\n\nimo, I don't think the synchronous ->read_folio_range() for buffered\nwrites should be consolidated with the async read logic. If we have\nthe synchronous write path setting read_bytes_pending, that adds extra\noverhead with having to acquire/release the spinlock for every range\nread in. It also makes the handling more complicated (eg now having to\ndifferentiate whether the folio was read in for a read vs. a write).\nSynchronous ->read_folio_range() for buffered writes is extremely\nsimple and self-contained right now and I think it should be kept that\nway.\n\nFor async reads, I agree that there are a bunch of different edge\ncases that arise from i) ii) and iii), and from the fact that a folio\ncould be composed of a mixture of i) ii) and iii).\n\nThe motivation for adding read_bytes_pending was so we could know\nwhich async read finishes last. eg this example scenario: read a 64k\nfolio where the first and last page are not uptodate but everything in\nbetween is\n* ->read_folio_range() for 0 to 4k\n* ->read_folio_range() for 60k to 64k\nThese two async read calls may be two different I/O requests that\ncomplete at different times but only the last finisher should call\nfolio_end_read().\n\nI don't think having the zeroing and inline read paths also\nmanipulating read_bytes_pending helps here. This was discussed a bit\nin [1] but I think it runs into other edge cases / race conditions [2]\nthat would need to be accounted for and makes some paths more\nsuboptimal (eg unnecessary ifs allocations and spinlock acquires). But\nmaybe I'm missing something here and there is a better approach for\ndoing this?\n\nThanks,\nJoanne\n\n[1] https://lore.kernel.org/linux-fsdevel/CAJnrk1YcuhKwbZLo-11=umcTzH_OJ+bdwZq5=XjeJo8gb9e5ig@mail.gmail.com/T/#md09648082a96122ec1e541993872e0c43da5105f\n[2] https://lore.kernel.org/linux-fsdevel/CAJnrk1YcuhKwbZLo-11=umcTzH_OJ+bdwZq5=XjeJo8gb9e5ig@mail.gmail.com/T/#mdc49b649378798fa9e850c9c6914c8c6af5e2895",
              "reply_to": "Darrick Wong",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that merging the code would be beneficial, but he hasn't found a good way to do it yet, and expressed concern about the range logic in ->read_folio() where writer preparation might only need to read head and tail segments of a folio.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes.  I've been thinking about that on and off, but unfortunately so far\nI've not come up with a good idea how to merge the code.  Doing so would\nbe very useful for many reasons.\n\nThe problem with that isn't really async vs sync; ->read_folio clearly\nshows you you turn underlying asynchronous logic into a synchronous call.\nIt's really about the range logic, where the writer preparation might\nwant to only read the head and the tail segments of a folio.\n\nBut if we can merge that into the main implementation and have a single\ncore implementation we'd be much better off.\n\nAnyone looking for a \"little\" project? :)",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v1 11/11] io_uring/cmd: set selected buffer index in __io_uring_cmd_done()",
          "message_id": "20260210002852.1394504-12-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260210002852.1394504-12-joannelkoong@gmail.com/",
          "date": "2026-02-10T00:31:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-10",
          "patch_summary": "This patch adds the ability to set the selected buffer index in the __io_uring_cmd_done() function, which is part of the io_uring command handling code. This allows the kernel to keep track of the currently selected buffer when issuing commands through the io_uring interface. The change is a part of a larger series that introduces kernel-managed buffer rings, where the kernel allocates and manages buffers on behalf of applications using io_uring. The patch builds upon previous changes in the series, including support for kernel-managed buffer rings, mmap support, and recycling of buffers.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern that the logic in io_register_pbuf_ring() is too complex and difficult to understand, and refactored it into three separate functions: io_copy_and_validate_buf_reg(), io_alloc_new_buffer_list(), and io_setup_pbuf_ring(). The new functions are designed to be more modular and easier to maintain. The author plans to reuse these helpers in upcoming kernel-managed buffer ring support.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "refactored code",
                "preparatory change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Refactor the logic in io_register_pbuf_ring() into generic helpers:\n- io_copy_and_validate_buf_reg(): Copy out user arg and validate user\n  arg and buffer registration parameters\n- io_alloc_new_buffer_list(): Allocate and initialize a new buffer\n  list for the given buffer group ID\n- io_setup_pbuf_ring(): Sets up the physical buffer ring region and\n  handles memory mapping for provided buffer rings\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport which will need to reuse some of these helpers.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c | 129 +++++++++++++++++++++++++++++++-----------------\n 1 file changed, 85 insertions(+), 44 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 67d4fe576473..850b836f32ee 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -596,55 +596,73 @@ int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags)\n \treturn IOU_COMPLETE;\n }\n \n-int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+static int io_copy_and_validate_buf_reg(const void __user *arg,\n+\t\t\t\t\tstruct io_uring_buf_reg *reg,\n+\t\t\t\t\tunsigned int permitted_flags)\n {\n-\tstruct io_uring_buf_reg reg;\n-\tstruct io_buffer_list *bl;\n-\tstruct io_uring_region_desc rd;\n-\tstruct io_uring_buf_ring *br;\n-\tunsigned long mmap_offset;\n-\tunsigned long ring_size;\n-\tint ret;\n-\n-\tlockdep_assert_held(&ctx->uring_lock);\n-\n-\tif (copy_from_user(&reg, arg, sizeof(reg)))\n+\tif (copy_from_user(reg, arg, sizeof(*reg)))\n \t\treturn -EFAULT;\n-\tif (!mem_is_zero(reg.resv, sizeof(reg.resv)))\n+\n+\tif (!mem_is_zero(reg->resv, sizeof(reg->resv)))\n \t\treturn -EINVAL;\n-\tif (reg.flags & ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC))\n+\tif (reg->flags & ~permitted_flags)\n \t\treturn -EINVAL;\n-\tif (!is_power_of_2(reg.ring_entries))\n+\tif (!is_power_of_2(reg->ring_entries))\n \t\treturn -EINVAL;\n \t/* cannot disambiguate full vs empty due to head/tail size */\n-\tif (reg.ring_entries >= 65536)\n+\tif (reg->ring_entries >= 65536)\n \t\treturn -EINVAL;\n+\treturn 0;\n+}\n \n-\tbl = io_buffer_get_list(ctx, reg.bgid);\n-\tif (bl) {\n+static struct io_buffer_list *\n+io_alloc_new_buffer_list(struct io_ring_ctx *ctx,\n+\t\t\t const struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_buffer_list *list;\n+\n+\tlist = io_buffer_get_list(ctx, reg->bgid);\n+\tif (list) {\n \t\t/* if mapped buffer ring OR classic exists, don't allow */\n-\t\tif (bl->flags & IOBL_BUF_RING || !list_empty(&bl->buf_list))\n-\t\t\treturn -EEXIST;\n-\t\tio_destroy_bl(ctx, bl);\n+\t\tif (list->flags & IOBL_BUF_RING || !list_empty(&list->buf_list))\n+\t\t\treturn ERR_PTR(-EEXIST);\n+\t\tio_destroy_bl(ctx, list);\n \t}\n \n-\tbl = kzalloc(sizeof(*bl), GFP_KERNEL_ACCOUNT);\n-\tif (!bl)\n-\t\treturn -ENOMEM;\n+\tlist = kzalloc(sizeof(*list), GFP_KERNEL_ACCOUNT);\n+\tif (!list)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tlist->nr_entries = reg->ring_entries;\n+\tlist->mask = reg->ring_entries - 1;\n+\tlist->flags = IOBL_BUF_RING;\n+\n+\treturn list;\n+}\n+\n+static int io_setup_pbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t      const struct io_uring_buf_reg *reg,\n+\t\t\t      struct io_buffer_list *bl)\n+{\n+\tstruct io_uring_region_desc rd;\n+\tunsigned long mmap_offset;\n+\tunsigned long ring_size;\n+\tint ret;\n \n-\tmmap_offset = (unsigned long)reg.bgid << IORING_OFF_PBUF_SHIFT;\n-\tring_size = flex_array_size(br, bufs, reg.ring_entries);\n+\tmmap_offset = (unsigned long)reg->bgid << IORING_OFF_PBUF_SHIFT;\n+\tring_size = flex_array_size(bl->buf_ring, bufs, reg->ring_entries);\n \n \tmemset(&rd, 0, sizeof(rd));\n \trd.size = PAGE_ALIGN(ring_size);\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP)) {\n-\t\trd.user_addr = reg.ring_addr;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP)) {\n+\t\trd.user_addr = reg->ring_addr;\n \t\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n \t}\n+\n \tret = io_create_region(ctx, &bl->region, &rd, mmap_offset);\n \tif (ret)\n-\t\tgoto fail;\n-\tbr = io_region_get_ptr(&bl->region);\n+\t\treturn ret;\n+\tbl->buf_ring = io_region_get_ptr(&bl->region);\n \n #ifdef SHM_COLOUR\n \t/*\n@@ -656,25 +674,48 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t * should use IOU_PBUF_RING_MMAP instead, and liburing will handle\n \t * this transparently.\n \t */\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP) &&\n-\t    ((reg.ring_addr | (unsigned long)br) & (SHM_COLOUR - 1))) {\n-\t\tret = -EINVAL;\n-\t\tgoto fail;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP) &&\n+\t    ((reg->ring_addr | (unsigned long)bl->buf_ring) &\n+\t     (SHM_COLOUR - 1))) {\n+\t\tio_free_region(ctx->user, &bl->region);\n+\t\treturn -EINVAL;\n \t}\n #endif\n \n-\tbl->nr_entries = reg.ring_entries;\n-\tbl->mask = reg.ring_entries - 1;\n-\tbl->flags |= IOBL_BUF_RING;\n-\tbl->buf_ring = br;\n-\tif (reg.flags & IOU_PBUF_RING_INC)\n+\tif (reg->flags & IOU_PBUF_RING_INC)\n \t\tbl->flags |= IOBL_INC;\n+\n+\treturn 0;\n+}\n+\n+int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tunsigned int permitted_flags;\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tpermitted_flags = IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC;\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, permitted_flags);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_pbuf_ring(ctx, &reg, bl);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n \tret = io_buffer_add_list(ctx, bl, reg.bgid);\n-\tif (!ret)\n-\t\treturn 0;\n-fail:\n-\tio_free_region(ctx->user, &bl->region);\n-\tkfree(bl);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n \treturn ret;\n }\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the naming consistency of functions related to unregistering buffer rings, agreeing that using the more generic name io_unregister_buf_ring() is better and making preparatory changes for upcoming kernel-managed buffer ring support.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "preparatory change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Use the more generic name io_unregister_buf_ring() as this function will\nbe used for unregistering both provided buffer rings and kernel-managed\nbuffer rings.\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c     | 2 +-\n io_uring/kbuf.h     | 2 +-\n io_uring/register.c | 2 +-\n 3 files changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 850b836f32ee..aa9b70b72db4 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -719,7 +719,7 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \treturn ret;\n }\n \n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n {\n \tstruct io_uring_buf_reg reg;\n \tstruct io_buffer_list *bl;\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex bf15e26520d3..40b44f4fdb15 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -74,7 +74,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags);\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 594b1f2ce875..0882cb34f851 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -841,7 +841,7 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-\t\tret = io_unregister_pbuf_ring(ctx, arg);\n+\t\tret = io_unregister_buf_ring(ctx, arg);\n \t\tbreak;\n \tcase IORING_REGISTER_SYNC_CANCEL:\n \t\tret = -EINVAL;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about the implementation of kernel-managed buffer rings, specifically how to handle the allocation and management of buffers for these rings. The author has provided an explanation of their approach, which involves reusing validation and buffer list allocation helpers from earlier refactoring. They have also added new functions to support kernel-managed buffer rings, including io_setup_kmbuf_ring() and io_register_kmbuf_ring().",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "explanation",
                "implementation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for kernel-managed buffer rings (kmbuf rings), which allow\nthe kernel to allocate and manage the backing buffers for a buffer\nring, rather than requiring the application to provide and manage them.\n\nThis introduces two new registration opcodes:\n- IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring\n- IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring\n\nThe existing io_uring_buf_reg structure is extended with a union to\nsupport both application-provided buffer rings (pbuf) and kernel-managed\nbuffer rings (kmbuf):\n- For pbuf rings: ring_addr specifies the user-provided ring address\n- For kmbuf rings: buf_size specifies the size of each buffer. buf_size\n  must be non-zero and page-aligned.\n\nThe implementation follows the same pattern as pbuf ring registration,\nreusing the validation and buffer list allocation helpers introduced in\nearlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as\nkernel-managed for appropriate handling in the I/O path.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  15 ++++-\n io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-\n io_uring/kbuf.h               |   7 ++-\n io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++\n io_uring/memmap.h             |   4 ++\n io_uring/register.c           |   7 +++\n 6 files changed, 219 insertions(+), 6 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex fc473af6feb4..a0889c1744bd 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -715,6 +715,10 @@ enum io_uring_register_op {\n \t/* register bpf filtering programs */\n \tIORING_REGISTER_BPF_FILTER\t\t= 37,\n \n+\t/* register/unregister kernel-managed ring buffer group */\n+\tIORING_REGISTER_KMBUF_RING\t\t= 38,\n+\tIORING_UNREGISTER_KMBUF_RING\t\t= 39,\n+\n \t/* this goes last */\n \tIORING_REGISTER_LAST,\n \n@@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {\n \tIOU_PBUF_RING_INC\t= 2,\n };\n \n-/* argument for IORING_(UN)REGISTER_PBUF_RING */\n+/* argument for IORING_(UN)REGISTER_PBUF_RING and\n+ * IORING_(UN)REGISTER_KMBUF_RING\n+ */\n struct io_uring_buf_reg {\n-\t__u64\tring_addr;\n+\tunion {\n+\t\t/* used for pbuf rings */\n+\t\t__u64\tring_addr;\n+\t\t/* used for kmbuf rings */\n+\t\t__u32   buf_size;\n+\t};\n \t__u32\tring_entries;\n \t__u16\tbgid;\n \t__u16\tflags;\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex aa9b70b72db4..9bc36451d083 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -427,10 +427,13 @@ static int io_remove_buffers_legacy(struct io_ring_ctx *ctx,\n \n static void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)\n {\n-\tif (bl->flags & IOBL_BUF_RING)\n+\tif (bl->flags & IOBL_BUF_RING) {\n \t\tio_free_region(ctx->user, &bl->region);\n-\telse\n+\t\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\t\tkfree(bl->buf_ring);\n+\t} else {\n \t\tio_remove_buffers_legacy(ctx, bl, -1U);\n+\t}\n \n \tkfree(bl);\n }\n@@ -779,3 +782,77 @@ struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n \t\treturn NULL;\n \treturn &bl->region;\n }\n+\n+static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_buffer_list *bl,\n+\t\t\t       struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_uring_buf_ring *ring;\n+\tunsigned long ring_size;\n+\tvoid *buf_region;\n+\tunsigned int i;\n+\tint ret;\n+\n+\t/* allocate pages for the ring structure */\n+\tring_size = flex_array_size(ring, bufs, bl->nr_entries);\n+\tring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);\n+\tif (!ring)\n+\t\treturn -ENOMEM;\n+\n+\tret = io_create_region_multi_buf(ctx, &bl->region, bl->nr_entries,\n+\t\t\t\t\t reg->buf_size);\n+\tif (ret) {\n+\t\tkfree(ring);\n+\t\treturn ret;\n+\t}\n+\n+\t/* initialize ring buf entries to point to the buffers */\n+\tbuf_region = bl->region.ptr;\n+\tfor (i = 0; i < bl->nr_entries; i++) {\n+\t\tstruct io_uring_buf *buf = &ring->bufs[i];\n+\n+\t\tbuf->addr = (u64)(uintptr_t)buf_region;\n+\t\tbuf->len = reg->buf_size;\n+\t\tbuf->bid = i;\n+\n+\t\tbuf_region += reg->buf_size;\n+\t}\n+\tring->tail = bl->nr_entries;\n+\n+\tbl->buf_ring = ring;\n+\tbl->flags |= IOBL_KERNEL_MANAGED;\n+\n+\treturn 0;\n+}\n+\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, 0);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tif (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_kmbuf_ring(ctx, bl, &reg);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n+\tret = io_buffer_add_list(ctx, bl, reg.bgid);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n+\treturn ret;\n+}\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 40b44f4fdb15..62c80a1ebf03 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -7,9 +7,11 @@\n \n enum {\n \t/* ring mapped provided buffers */\n-\tIOBL_BUF_RING\t= 1,\n+\tIOBL_BUF_RING\t\t= 1,\n \t/* buffers are consumed incrementally rather than always fully */\n-\tIOBL_INC\t= 2,\n+\tIOBL_INC\t\t= 2,\n+\t/* buffers are kernel managed */\n+\tIOBL_KERNEL_MANAGED\t= 4,\n };\n \n struct io_buffer_list {\n@@ -74,6 +76,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 89f56609e50a..8d37e93c0433 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -15,6 +15,28 @@\n #include \"rsrc.h\"\n #include \"zcrx.h\"\n \n+static void release_multi_buf_pages(struct page **pages, unsigned long nr_pages)\n+{\n+\tstruct page *page;\n+\tunsigned int nr, i = 0;\n+\n+\twhile (nr_pages) {\n+\t\tpage = pages[i];\n+\n+\t\tif (!page || WARN_ON_ONCE(page != compound_head(page)))\n+\t\t\treturn;\n+\n+\t\tnr = compound_nr(page);\n+\t\tput_page(page);\n+\n+\t\tif (WARN_ON_ONCE(nr > nr_pages))\n+\t\t\treturn;\n+\n+\t\ti += nr;\n+\t\tnr_pages -= nr;\n+\t}\n+}\n+\n static bool io_mem_alloc_compound(struct page **pages, int nr_pages,\n \t\t\t\t  size_t size, gfp_t gfp)\n {\n@@ -86,6 +108,8 @@ enum {\n \tIO_REGION_F_USER_PROVIDED\t\t= 2,\n \t/* only the first page in the array is ref'ed */\n \tIO_REGION_F_SINGLE_REF\t\t\t= 4,\n+\t/* pages in the array belong to multiple discrete allocations */\n+\tIO_REGION_F_MULTI_BUF\t\t\t= 8,\n };\n \n void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n@@ -98,6 +122,8 @@ void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n \n \t\tif (mr->flags & IO_REGION_F_USER_PROVIDED)\n \t\t\tunpin_user_pages(mr->pages, nr_refs);\n+\t\telse if (mr->flags & IO_REGION_F_MULTI_BUF)\n+\t\t\trelease_multi_buf_pages(mr->pages, nr_refs);\n \t\telse\n \t\t\trelease_pages(mr->pages, nr_refs);\n \n@@ -149,6 +175,54 @@ static int io_region_pin_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+static int io_region_allocate_pages_multi_buf(struct io_mapped_region *mr,\n+\t\t\t\t\t      unsigned int nr_bufs,\n+\t\t\t\t\t      unsigned int buf_size)\n+{\n+\tgfp_t gfp = GFP_USER | __GFP_ACCOUNT | __GFP_ZERO | __GFP_NOWARN;\n+\tstruct page **pages, **cur_pages;\n+\tunsigned int nr_allocated;\n+\tunsigned int buf_pages;\n+\tunsigned int i;\n+\n+\tif (!PAGE_ALIGNED(buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbuf_pages = buf_size >> PAGE_SHIFT;\n+\n+\tpages = kvmalloc_array(mr->nr_pages, sizeof(*pages), gfp);\n+\tif (!pages)\n+\t\treturn -ENOMEM;\n+\n+\tcur_pages = pages;\n+\n+\tfor (i = 0; i < nr_bufs; i++) {\n+\t\tif (io_mem_alloc_compound(cur_pages, buf_pages, buf_size,\n+\t\t\t\t\t  gfp)) {\n+\t\t\tcur_pages += buf_pages;\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tnr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,\n+\t\t\t\t\t\t     buf_pages, cur_pages);\n+\t\tif (nr_allocated != buf_pages) {\n+\t\t\tunsigned int total =\n+\t\t\t\t(cur_pages - pages) + nr_allocated;\n+\n+\t\t\trelease_multi_buf_pages(pages, total);\n+\t\t\tkvfree(pages);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tcur_pages += buf_pages;\n+\t}\n+\n+\tmr->flags |= IO_REGION_F_MULTI_BUF;\n+\tmr->pages = pages;\n+\n+\treturn 0;\n+}\n+\n static int io_region_allocate_pages(struct io_mapped_region *mr,\n \t\t\t\t    struct io_uring_region_desc *reg,\n \t\t\t\t    unsigned long mmap_offset)\n@@ -181,6 +255,43 @@ static int io_region_allocate_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size)\n+{\n+\tunsigned int nr_pages;\n+\tint ret;\n+\n+\tif (WARN_ON_ONCE(mr->pages || mr->ptr || mr->nr_pages))\n+\t\treturn -EFAULT;\n+\n+\tif (WARN_ON_ONCE(!nr_bufs || !buf_size || !PAGE_ALIGNED(buf_size)))\n+\t\treturn -EINVAL;\n+\n+\tif (check_mul_overflow(buf_size >> PAGE_SHIFT, nr_bufs, &nr_pages))\n+\t\treturn -EINVAL;\n+\n+\tif (ctx->user) {\n+\t\tret = __io_account_mem(ctx->user, nr_pages);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\t}\n+\tmr->nr_pages = nr_pages;\n+\n+\tret = io_region_allocate_pages_multi_buf(mr, nr_bufs, buf_size);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\tret = io_region_init_ptr(mr);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\treturn 0;\n+out_free:\n+\tio_free_region(ctx->user, mr);\n+\treturn ret;\n+}\n+\n int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset)\ndiff --git a/io_uring/memmap.h b/io_uring/memmap.h\nindex f4cfbb6b9a1f..3aa1167462ae 100644\n--- a/io_uring/memmap.h\n+++ b/io_uring/memmap.h\n@@ -22,6 +22,10 @@ int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset);\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size);\n+\n static inline void *io_region_get_ptr(struct io_mapped_region *mr)\n {\n \treturn mr->ptr;\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 0882cb34f851..2db8daaf8fde 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -837,7 +837,14 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\t\tbreak;\n \t\tret = io_register_pbuf_ring(ctx, arg);\n \t\tbreak;\n+\tcase IORING_REGISTER_KMBUF_RING:\n+\t\tret = -EINVAL;\n+\t\tif (!arg || nr_args != 1)\n+\t\t\tbreak;\n+\t\tret = io_register_kmbuf_ring(ctx, arg);\n+\t\tbreak;\n \tcase IORING_UNREGISTER_PBUF_RING:\n+\tcase IORING_UNREGISTER_KMBUF_RING:\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the selected buffer index in __io_uring_cmd_done() not being set correctly, explained that the issue was due to the lack of a buffer ID in the io_uring_is_kmbuf_ring() check and promised to fix it by adding a new parameter to io_uring_is_kmbuf_ring()",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a technical issue",
                "promised a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for mmapping kernel-managed buffer rings (kmbuf) to\nuserspace, allowing applications to access the kernel-allocated buffers.\n\nSimilar to application-provided buffer rings (pbuf), kmbuf rings use the\nbuffer group ID encoded in the mmap offset to identify which buffer ring\nto map. The implementation follows the same pattern as pbuf rings.\n\nNew mmap offset constants are introduced:\n  - IORING_OFF_KMBUF_RING (0x88000000): Base offset for kmbuf mappings\n  - IORING_OFF_KMBUF_SHIFT (16): Shift value to encode buffer group ID\n\nThe mmap offset encodes the bgid shifted by IORING_OFF_KMBUF_SHIFT.\nThe io_buf_get_region() helper retrieves the appropriate region.\n\nThis allows userspace to mmap the kernel-allocated buffer region and\naccess the buffers directly.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  2 ++\n io_uring/kbuf.c               | 11 +++++++++--\n io_uring/kbuf.h               |  5 +++--\n io_uring/memmap.c             |  5 ++++-\n 4 files changed, 18 insertions(+), 5 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex a0889c1744bd..42a2812c9922 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -545,6 +545,8 @@ struct io_uring_cqe {\n #define IORING_OFF_SQES\t\t\t0x10000000ULL\n #define IORING_OFF_PBUF_RING\t\t0x80000000ULL\n #define IORING_OFF_PBUF_SHIFT\t\t16\n+#define IORING_OFF_KMBUF_RING\t\t0x88000000ULL\n+#define IORING_OFF_KMBUF_SHIFT\t\t16\n #define IORING_OFF_MMAP_MASK\t\t0xf8000000ULL\n \n /*\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9bc36451d083..ccf5b213087b 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -770,16 +770,23 @@ int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg)\n \treturn 0;\n }\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid)\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed)\n {\n \tstruct io_buffer_list *bl;\n+\tbool is_kernel_managed;\n \n \tlockdep_assert_held(&ctx->mmap_lock);\n \n \tbl = xa_load(&ctx->io_bl_xa, bgid);\n \tif (!bl || !(bl->flags & IOBL_BUF_RING))\n \t\treturn NULL;\n+\n+\tis_kernel_managed = !!(bl->flags & IOBL_KERNEL_MANAGED);\n+\tif (is_kernel_managed != kernel_managed)\n+\t\treturn NULL;\n+\n \treturn &bl->region;\n }\n \ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 62c80a1ebf03..11d165888b8e 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -88,8 +88,9 @@ unsigned int __io_put_kbufs(struct io_kiocb *req, struct io_buffer_list *bl,\n bool io_kbuf_commit(struct io_kiocb *req,\n \t\t    struct io_buffer_list *bl, int len, int nr);\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid);\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed);\n \n static inline bool io_kbuf_recycle_ring(struct io_kiocb *req,\n \t\t\t\t\tstruct io_buffer_list *bl)\ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 8d37e93c0433..916315122323 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -356,7 +356,10 @@ static struct io_mapped_region *io_mmap_get_region(struct io_ring_ctx *ctx,\n \t\treturn &ctx->sq_region;\n \tcase IORING_OFF_PBUF_RING:\n \t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_PBUF_SHIFT;\n-\t\treturn io_pbuf_get_region(ctx, id);\n+\t\treturn io_buf_get_region(ctx, id, false);\n+\tcase IORING_OFF_KMBUF_RING:\n+\t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_KMBUF_SHIFT;\n+\t\treturn io_buf_get_region(ctx, id, true);\n \tcase IORING_MAP_OFF_PARAM_REGION:\n \t\treturn &ctx->param_region;\n \tcase IORING_MAP_OFF_ZCRX_REGION:\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about distinguishing between kernel-managed buffer addresses and negative values in error checking, explaining that the io_br_sel struct needs to be modified to separate address and value fields for this purpose.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Allow kernel-managed buffers to be selected. This requires modifying the\nio_br_sel struct to separate the fields for address and val, since a\nkernel address cannot be distinguished from a negative val when error\nchecking.\n\nAuto-commit any selected kernel-managed buffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring_types.h |  8 ++++----\n io_uring/kbuf.c                | 16 ++++++++++++----\n 2 files changed, 16 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 3e4a82a6f817..36cc2e0346d9 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -93,13 +93,13 @@ struct io_mapped_region {\n  */\n struct io_br_sel {\n \tstruct io_buffer_list *buf_list;\n-\t/*\n-\t * Some selection parts return the user address, others return an error.\n-\t */\n \tunion {\n+\t\t/* for classic/ring provided buffers */\n \t\tvoid __user *addr;\n-\t\tssize_t val;\n+\t\t/* for kernel-managed buffers */\n+\t\tvoid *kaddr;\n \t};\n+\tssize_t val;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex ccf5b213087b..1e8395270227 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -155,7 +155,8 @@ static int io_provided_buffers_select(struct io_kiocb *req, size_t *len,\n \treturn 1;\n }\n \n-static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n+static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n+\t\t\t     unsigned int issue_flags)\n {\n \t/*\n \t* If we came in unlocked, we have no choice but to consume the\n@@ -170,7 +171,11 @@ static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n \tif (issue_flags & IO_URING_F_UNLOCKED)\n \t\treturn true;\n \n-\t/* uring_cmd commits kbuf upfront, no need to auto-commit */\n+\t/* kernel-managed buffers are auto-committed */\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\treturn true;\n+\n+\t/* multishot uring_cmd commits kbuf upfront, no need to auto-commit */\n \tif (!io_file_can_poll(req) && req->opcode != IORING_OP_URING_CMD)\n \t\treturn true;\n \treturn false;\n@@ -200,9 +205,12 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n-\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n+\telse\n+\t\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n \n-\tif (io_should_commit(req, issue_flags)) {\n+\tif (io_should_commit(req, bl, issue_flags)) {\n \t\tio_kbuf_commit(req, sel.buf_list, *len, 1);\n \t\tsel.buf_list = NULL;\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about buffer ring pinning, explaining that the new APIs will prevent userspace from unregistering a buffer ring while it is pinned by the kernel. The author added code to implement these APIs and ensure that a pinned buffer ring cannot be unregistered until explicitly unpinned.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "added code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add kernel APIs to pin and unpin buffer rings, preventing userspace from\nunregistering a buffer ring while it is pinned by the kernel.\n\nThis provides a mechanism for kernel subsystems to safely access buffer\nring contents while ensuring the buffer ring remains valid. A pinned\nbuffer ring cannot be unregistered until explicitly unpinned. On the\nuserspace side, trying to unregister a pinned buffer will return -EBUSY.\n\nThis is a preparatory change for upcoming fuse usage of kernel-managed\nbuffer rings. It is necessary for fuse to pin the buffer ring because\nfuse may need to select a buffer in atomic contexts, which it can only\ndo so by using the underlying buffer list pointer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 17 +++++++++++++\n io_uring/kbuf.c              | 48 ++++++++++++++++++++++++++++++++++++\n io_uring/kbuf.h              |  5 ++++\n 3 files changed, 70 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 375fd048c4cb..702b1903e6ee 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -84,6 +84,10 @@ struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n \t\t\t\t struct io_br_sel *sel, unsigned int issue_flags);\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t    unsigned issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -126,6 +130,19 @@ static inline bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n {\n \treturn true;\n }\n+static inline int io_uring_buf_ring_pin(struct io_uring_cmd *cmd,\n+\t\t\t\t\tunsigned buf_group,\n+\t\t\t\t\tunsigned issue_flags,\n+\t\t\t\t\tstruct io_buffer_list **bl)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n+static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned buf_group,\n+\t\t\t\t\t  unsigned issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 1e8395270227..dee1764ed19f 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -9,6 +9,7 @@\n #include <linux/poll.h>\n #include <linux/vmalloc.h>\n #include <linux/io_uring.h>\n+#include <linux/io_uring/cmd.h>\n \n #include <uapi/linux/io_uring.h>\n \n@@ -237,6 +238,51 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \treturn sel;\n }\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *buffer_list;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbuffer_list = io_buffer_get_list(ctx, buf_group);\n+\tif (buffer_list && (buffer_list->flags & IOBL_BUF_RING)) {\n+\t\tif (unlikely(buffer_list->flags & IOBL_PINNED)) {\n+\t\t\tret = -EALREADY;\n+\t\t} else {\n+\t\t\tbuffer_list->flags |= IOBL_PINNED;\n+\t\t\tret = 0;\n+\t\t\t*bl = buffer_list;\n+\t\t}\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_pin);\n+\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t       unsigned issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (bl && (bl->flags & IOBL_BUF_RING) && (bl->flags & IOBL_PINNED)) {\n+\t\tbl->flags &= ~IOBL_PINNED;\n+\t\tret = 0;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_unpin);\n+\n /* cap it at a reasonable 256, will be one page even for 4K */\n #define PEEK_MAX_IMPORT\t\t256\n \n@@ -747,6 +793,8 @@ int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t\treturn -ENOENT;\n \tif (!(bl->flags & IOBL_BUF_RING))\n \t\treturn -EINVAL;\n+\tif (bl->flags & IOBL_PINNED)\n+\t\treturn -EBUSY;\n \n \tscoped_guard(mutex, &ctx->mmap_lock)\n \t\txa_erase(&ctx->io_bl_xa, bl->bgid);\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 11d165888b8e..781630c2cc10 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -12,6 +12,11 @@ enum {\n \tIOBL_INC\t\t= 2,\n \t/* buffers are kernel managed */\n \tIOBL_KERNEL_MANAGED\t= 4,\n+\t/*\n+\t * buffer ring is pinned and cannot be unregistered by userspace until\n+\t * it has been unpinned\n+\t */\n+\tIOBL_PINNED\t\t= 8,\n };\n \n struct io_buffer_list {\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the implementation of buffer recycling in kernel-managed buffer rings, explained that an interface for buffers to be recycled back into a kernel-managed buffer ring is being added, and confirmed that this is a preparatory patch for fuse over io-uring.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "preparatory"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add an interface for buffers to be recycled back into a kernel-managed\nbuffer ring.\n\nThis is a preparatory patch for fuse over io-uring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 11 +++++++++\n io_uring/kbuf.c              | 44 ++++++++++++++++++++++++++++++++++++\n 2 files changed, 55 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 702b1903e6ee..a488e945f883 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -88,6 +88,10 @@ int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t    unsigned issue_flags);\n+\n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -143,6 +147,13 @@ static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n+\t\t\t\t\t unsigned int buf_group, u64 addr,\n+\t\t\t\t\t unsigned int len, unsigned int bid,\n+\t\t\t\t\t unsigned int issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex dee1764ed19f..17b6178be4ce 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -102,6 +102,50 @@ void io_kbuf_drop_legacy(struct io_kiocb *req)\n \treq->kbuf = NULL;\n }\n \n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags)\n+{\n+\tstruct io_kiocb *req = cmd_to_io_kiocb(cmd);\n+\tstruct io_ring_ctx *ctx = req->ctx;\n+\tstruct io_uring_buf_ring *br;\n+\tstruct io_uring_buf *buf;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tif (WARN_ON_ONCE(req->flags & REQ_F_BUFFERS_COMMIT))\n+\t\treturn ret;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\n+\tif (!bl || WARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING)) ||\n+\t    WARN_ON_ONCE(!(bl->flags & IOBL_KERNEL_MANAGED)))\n+\t\tgoto done;\n+\n+\tbr = bl->buf_ring;\n+\n+\tif (WARN_ON_ONCE((br->tail - bl->head) >= bl->nr_entries))\n+\t\tgoto done;\n+\n+\tbuf = &br->bufs[(br->tail) & bl->mask];\n+\n+\tbuf->addr = addr;\n+\tbuf->len = len;\n+\tbuf->bid = bid;\n+\n+\treq->flags &= ~REQ_F_BUFFER_RING;\n+\n+\tbr->tail++;\n+\tret = 0;\n+\n+done:\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_kmbuf_recycle);\n+\n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)\n {\n \tstruct io_ring_ctx *ctx = req->ctx;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring_is_kmbuf_ring() function, explaining that it returns true if there is a kernel-managed buffer ring at the specified buffer group. The author provided code changes to implement this functionality and stated that these changes are preparatory for upcoming fuse kernel-managed buffer support.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "preparatory"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "io_uring_is_kmbuf_ring() returns true if there is a kernel-managed\nbuffer ring at the specified buffer group.\n\nThis is a preparatory patch for upcoming fuse kernel-managed buffer\nsupport, which needs to ensure the buffer ring registered by the server\nis a kernel-managed buffer ring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h |  9 +++++++++\n io_uring/kbuf.c              | 20 ++++++++++++++++++++\n 2 files changed, 29 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex a488e945f883..04a937f6f4d3 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -92,6 +92,9 @@ int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t   u64 addr, unsigned int len, unsigned int bid,\n \t\t\t   unsigned int issue_flags);\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -154,6 +157,12 @@ static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned int buf_group,\n+\t\t\t\t\t  unsigned int issue_flags)\n+{\n+\treturn false;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 17b6178be4ce..797cc2f0a5e9 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -963,3 +963,23 @@ int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \n \treturn ret;\n }\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tbool is_kmbuf_ring = false;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (likely(bl) && (bl->flags & IOBL_KERNEL_MANAGED)) {\n+\t\tWARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING));\n+\t\tis_kmbuf_ring = true;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn is_kmbuf_ring;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_is_kmbuf_ring);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_ring_buffer_select() function being inaccessible to callers who need it without holding the io_uring mutex. The author agreed that exporting this function is necessary for fuse io-uring, which may need to select a buffer from a kernel-managed bufring in atomic contexts.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "export"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Export io_ring_buffer_select() so that it may be used by callers who\npass in a pinned bufring without needing to grab the io_uring mutex.\n\nThis is a preparatory patch that will be needed by fuse io-uring, which\nwill need to select a buffer from a kernel-managed bufring while the\nuring mutex may already be held by in-progress commits, and may need to\nselect a buffer in atomic contexts.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 14 ++++++++++++++\n io_uring/kbuf.c              |  7 ++++---\n 2 files changed, 18 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 04a937f6f4d3..d4b5943bdeb1 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -95,6 +95,10 @@ int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \n bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t    unsigned int issue_flags);\n+\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -163,6 +167,16 @@ static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n {\n \treturn false;\n }\n+static inline struct io_br_sel io_ring_buffer_select(struct io_kiocb *req,\n+\t\t\t\t\t\t     size_t *len,\n+\t\t\t\t\t\t     struct io_buffer_list *bl,\n+\t\t\t\t\t\t     unsigned int issue_flags)\n+{\n+\tstruct io_br_sel sel = {\n+\t\t.val = -EOPNOTSUPP,\n+\t};\n+\treturn sel;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 797cc2f0a5e9..9a93f10d3214 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -226,9 +226,9 @@ static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n \treturn false;\n }\n \n-static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n-\t\t\t\t\t      struct io_buffer_list *bl,\n-\t\t\t\t\t      unsigned int issue_flags)\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags)\n {\n \tstruct io_uring_buf_ring *br = bl->buf_ring;\n \t__u16 tail, head = bl->head;\n@@ -261,6 +261,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \t}\n \treturn sel;\n }\n+EXPORT_SYMBOL_GPL(io_ring_buffer_select);\n \n struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \t\t\t\t  unsigned buf_group, unsigned int issue_flags)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring_cmd_buffer_select() function not returning the selected buffer's id, and responded by modifying the function to return the id in addition to the address and size.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Return the id of the selected buffer in io_buffer_select(). This is\nneeded for kernel-managed buffer rings to later recycle the selected\nbuffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h   | 2 +-\n include/linux/io_uring_types.h | 2 ++\n io_uring/kbuf.c                | 7 +++++--\n 3 files changed, 8 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex d4b5943bdeb1..94df2bdebe77 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);\n \n /*\n  * Select a buffer from the provided buffer group for multishot uring_cmd.\n- * Returns the selected buffer address and size.\n+ * Returns the selected buffer address, size, and id.\n  */\n struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n \t\t\t\t\t    unsigned buf_group, size_t *len,\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 36cc2e0346d9..5a56bb341337 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -100,6 +100,8 @@ struct io_br_sel {\n \t\tvoid *kaddr;\n \t};\n \tssize_t val;\n+\t/* id of the selected buffer */\n+\tunsigned buf_id;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9a93f10d3214..24c1e34ea23e 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -250,6 +250,7 @@ struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n+\tsel.buf_id = req->buf_index;\n \tif (bl->flags & IOBL_KERNEL_MANAGED)\n \t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n \telse\n@@ -274,10 +275,12 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \n \tbl = io_buffer_get_list(ctx, buf_group);\n \tif (likely(bl)) {\n-\t\tif (bl->flags & IOBL_BUF_RING)\n+\t\tif (bl->flags & IOBL_BUF_RING) {\n \t\t\tsel = io_ring_buffer_select(req, len, bl, issue_flags);\n-\t\telse\n+\t\t} else {\n \t\t\tsel.addr = io_provided_buffer_select(req, len, bl);\n+\t\t\tsel.buf_id = req->buf_index;\n+\t\t}\n \t}\n \tio_ring_submit_unlock(req->ctx, issue_flags);\n \treturn sel;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about indicating which buffer was selected in the completion queue entry, explained that this is needed for fuse to relay the information to userspace, and confirmed that the fix will be included in the patch.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a specific technical issue",
                "confirmed a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When uring_cmd operations select a buffer, the completion queue entry\nshould indicate which buffer was selected.\n\nSet IORING_CQE_F_BUFFER on the completed entry and encode the buffer\nindex if a buffer was selected.\n\nThis will be needed for fuse, which needs to relay to userspace which\nselected buffer contains the data.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/uring_cmd.c | 6 +++++-\n 1 file changed, 5 insertions(+), 1 deletion(-)\n\ndiff --git a/io_uring/uring_cmd.c b/io_uring/uring_cmd.c\nindex ee7b49f47cb5..6d38df1a812d 100644\n--- a/io_uring/uring_cmd.c\n+++ b/io_uring/uring_cmd.c\n@@ -151,6 +151,7 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \t\t       unsigned issue_flags, bool is_cqe32)\n {\n \tstruct io_kiocb *req = cmd_to_io_kiocb(ioucmd);\n+\tu32 cflags = 0;\n \n \tif (WARN_ON_ONCE(req->flags & REQ_F_APOLL_MULTISHOT))\n \t\treturn;\n@@ -160,7 +161,10 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \tif (ret < 0)\n \t\treq_set_fail(req);\n \n-\tio_req_set_res(req, ret, 0);\n+\tif (req->flags & (REQ_F_BUFFER_SELECTED | REQ_F_BUFFER_RING))\n+\t\tcflags |= IORING_CQE_F_BUFFER |\n+\t\t\t(req->buf_index << IORING_CQE_BUFFER_SHIFT);\n+\tio_req_set_res(req, ret, cflags);\n \tif (is_cqe32) {\n \t\tif (req->ctx->flags & IORING_SETUP_CQE_MIXED)\n \t\t\treq->cqe.flags |= IORING_CQE_F_32;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested adding a WARN_ON_ONCE() to prevent int promotion from affecting the calculation of (br->tail - bl->head) >= bl->nr_entries, and noted that this is not a critical issue but rather something to be addressed in the future.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you want:\n\n\tif (WARN_ON_ONCE((__u16)(br->tail - bl->head) >= bl->nr_entries))\n\nhere to avoid int promotion from messing this up if tail has wrapped.\n\nIn general, across the patches for the WARN_ON_ONCE(), it's not a huge\nissue to have a litter of them for now. Hopefully we can prune some of\nthese down the line, however.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe questioned the need to set the selected buffer index in __io_uring_cmd_done(), suggesting that the caller could simply use req->buf_index instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm probably missing something here, but why can't the caller just use\nreq->buf_index for this?\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe requested a branch with all patches and users applied, stating that some helpers require an exposed user for proper judgment.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request_for_additional_context"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Generally looks pretty good - for context, do you have a branch with\nthese patches and the users on top too? Makes it a bit easier for cross\nreferencing, as some of these really do need an exposed user to make a\ngood judgement on the helpers.\n\nI know there's the older series, but I'm assuming the latter patches\nchanged somewhat too, and it'd be nicer to look at a current set rather\nthan go back to the older ones.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested refactoring io_pbuf_get_region() to handle kernel-managed buffer rings by adding a new helper function, io_kbuf_get_region(), and checking the bl->flags for IOBL_KERNEL_MANAGED in both functions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor nit",
                "more readable"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "For this, I think just add another helper - leave io_pbuf_get_region()\nand add a bl->flags & IOBL_KERNEL_MANAGED error check in there, and\nadd a io_kbuf_get_region() or similar and have a !(bl->flags &\nIOBL_KERNEL_MANAGED) error check in that one.\n\nThat's easier to read, and there's little reason to avoid duplicating\nthe xa_load() part.\n\nMinor nit, but imho it's more readable that way.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Jens Axboe suggested using a pointer to struct io_buffer_list *bl instead of passing it by value, and recommended returning an ERR_PTR if the function fails or renaming the parameter to **blret\n\nJens Axboe suggested a more efficient way to check if a buffer ring is both pinned and managed by the kernel, recommending a single bitwise AND operation instead of multiple conditional checks.\n\nReviewer Jens Axboe suggested that the patch should be modified to not enforce a character limit on io_uring strings, as this is acceptable for the io_uring implementation.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "suggested improvement",
                "recommended change",
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Probably use the usual struct io_buffer_list *bl here and either use an\nERR_PTR return, or rename the passed on **bl to **blret or something.\n\n---\n\nUsually done as:\n\n\tif ((bl->flags & (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))\n\nand maybe then just have an earlier\n\n\tif (!bl)\n\t\tgoto err;\n\n---\n\nto avoid making it way too long. For io_uring, it's fine to exceed 80\nchars where it makes sense.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer noted that the patch fences itself off from optimizations for huge pages, which can be used when creating a region with user-passed memory\n\nPavel Begunkov noted that io_create_region() should be used instead of a new function in __io_uring_cmd_done(), as it does not introduce any new functionality and violates abstractions; he also suggested stripping buffer allocation from IORING_REGISTER_KMBUF_RING, replacing *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag, or requiring users to register a memory region of appropriate size using IORING_REGISTER_MEM_REGION\n\nReviewer Pavel Begunkov noted that the removal of io_create_region_multi_buf() means that buffer alignment is no longer necessary, and suggested that this could result in wasted memory due to 64KB page sizes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "optimizations",
                "huge pages",
                "requested changes",
                "suggested alternative approaches"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "If you're creating a region, there should be no reason why it\ncan't work with user passed memory. You're fencing yourself off\noptimisations that are already there like huge pages.\n\n---\n\nPlease use io_create_region(), the new function does nothing new\nand only violates abstractions.\n\nProvided buffer rings with kernel addresses could be an interesting\nabstraction, but why is it also responsible for allocating buffers?\nWhat I'd do:\n\n1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.\n2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.\n    Or maybe don't expose it to the user at all and create it from\n    fuse via internal API.\n3. Require the user to register a memory region of appropriate size,\n    see IORING_REGISTER_MEM_REGION, ctx->param_region. Make fuse\n    populating the buffer ring using the memory region.\n\nI wanted to make regions shareable anyway (need it for other purposes),\nI can toss patches for that tomorrow.\n\nA separate question is whether extending buffer rings is the right\napproach as it seems like you're only using it for fuse requests and\nnot for passing buffers to normal requests, but I don't see the\nbig picture here.\n\n---\n\nWith io_create_region_multi_buf() gone, you shouldn't need\nto align every buffer, that could be a lot of wasted memory\n(thinking about 64KB pages).",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Caleb Mateos",
              "summary": "Reviewer Caleb Mateos noted that the patch's optimization in __io_uring_cmd_done() is unnecessary, as modern compilers will automatically perform this optimization and potentially optimize it further.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "optimization",
                "compiler"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, modern compilers will perform this optimization automatically.\nThey'll even optimize it further to !(~bl->flags &\n(IOBL_BUF_RING|IOBL_PINNED)): https://godbolt.org/z/xGoP4TfhP\n\nBest,\nCaleb",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested that the code should follow a common pattern for clarity and readability, citing that the current implementation is easier to read than the original.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes",
                "suggested improvement"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure, it's not about that, it's more about the common way of doing it,\nwhich makes it easier to read for people. FWIW, your example is easier\nto read too than the original.\n\n-- \nJens Axboe",
              "reply_to": "Caleb Mateos",
              "message_date": "2026-02-10",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author is asking a clarifying question about whether there are any optimizations possible with user-allocated buffers that wouldn't be achievable with kernel-allocated buffers, specifically in the context of huge pages.\n\nAuthor Joanne Koong responded to Pavel Begunkov's feedback that __io_uring_cmd_done() should set the selected buffer index, explaining that separate checks are needed between io_create_region() and io_create_region_multi_buf(), and different allocation calls require distinct functions.\n\nAuthor Joanne Koong is responding to feedback about kernel-managed buffer rings, specifically addressing concerns about registering buffers from userspace. She explains that allocating buffers from the kernel-side simplifies interface and lifecycle management, guarantees contiguous page allocation, and avoids complications with user-allocated buffers.\n\nAuthor responded to Pavel Begunkov's feedback by explaining that if kernel-managed buffer rings are squashed into existing pbuf rings, then pbuf rings would need to support pinning, which is necessary for fuse contexts where the uring mutex cannot be grabbed. The author notes they had previously proposed adding pinning to pbuf rings but it was rejected.\n\nAuthor clarified that the term 'normal requests' is specific to fuse's use case and does not apply to io_uring in general.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "question",
                "explanation",
                "asking for clarification",
                "providing explanation",
                "acknowledged a technical requirement",
                "explained reasoning",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Are there any optimizations with user-allocated buffers that wouldn't\nbe possible with kernel-allocated buffers? For huge pages, can't the\nkernel do this as well (eg I see in io_mem_alloc_compound(), it calls\ninto alloc_pages() with order > 0)?\n\n---\n\nThere's separate checks needed between io_create_region() and\nio_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag\nchecking) and different allocation calls (eg\nio_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).\nMaybe I'm misinterpreting your comment (or the code), but I'm not\nseeing how this can just use io_create_region().\n\n---\n\nConceptually, I think it makes the interface and lifecycle management\nsimpler/cleaner. With registering it from userspace, imo there's\nadditional complications with no tangible benefits, eg it's not\nguaranteed that the memory regions registered for the buffers are the\nsame size, with allocating it from the kernel-side we can guarantee\nthat the pages are allocated physically contiguously, userspace setup\nwith user-allocated buffers is less straightforward, etc. In general,\nI'm just not really seeing what advantages there are in allocating the\nbuffers from userspace. Could you elaborate on that part more?\n\n---\n\nIf kmbuf rings are squashed into pbuf rings, then pbuf rings will need\nto support pinning. In fuse, there are some contexts where you can't\ngrab the uring mutex because you're running in atomic context and this\ncan be encountered while recycling the buffer. I originally had a\npatch adding pinning to pbuf rings (to mitigate the overhead of\nregistered buffers lookups) but dropped it when Jens and Caleb didn't\nlike the idea. But for kmbuf rings, pinning will be necessary for\nfuse.\n\n---\n\nWhat are 'normal requests'? For fuse's use case, there are only fuse requests.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that accessing the buffer index from the caller side can be cumbersome and offered alternative solutions, such as introducing a helper function to retrieve the buffer ID.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "offered alternatives"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The caller can, but from the caller side they only have access to the\ncmd so they would need to do something like\n\nstruct io_kiocb *req = cmd_to_iocb_kiocb(ent->cmd);\nbuf_id = req->buf_index;\n\nwhich may be kind of ugly with looking inside io-uring internals.\nMaybe a helper here would be nicer, something like\nio_uring_cmd_buf_id() or io_uring_req_buf_id(). It seemed cleaner to\nme to just return the buf id as part of the io_br_sel struct, but I'm\nhappy to do it another way if you have a preference.\n\nThanks,\nJoanne",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that additional changes are needed, specifically the userside changes on top of the patches, and plans to address them in v2 once a discussion with Pavel is resolved.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for further work",
                "plans to revise in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for reviewing the patches. The branch containing the userside\nchanges on top of these patches is in [1]. I'll make the changes you\npointed out in your other comments as part of v2. Once the discussion\nwith Pavel is resolved / figured out with the changes he wants for v2,\nI'll submit v2.\n\nThanks,\nJoanne\n\n[1] https://github.com/joannekoong/linux/commits/fuse_zero_copy/",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that allocating 1MB in kernel space will not result in a PMD mappable huge page, unlike user space which can allocate 2MB and register the first 1MB for reuse\n\nReviewer Pavel Begunkov suggested that instead of changing io_create_region() to be less strict, the caller should filter arguments to ensure only necessary types are passed.\n\nPavel Begunkov noted that the memmap.c changes in the patch are unnecessary and can be dropped because they only provide contiguous memory within a single buffer, which is already achieved by default io_create_region(). He suggested removing these changes to avoid disabling the usefulness of io_mem_alloc_compound() and to decouple regions from buffer subdivision.\n\nReviewer Pavel Begunkov suggested adding a mechanism to handle user-provided memory for kernel-managed buffer rings, proposing the use of io_create_region() with specific flags and user address information.\n\nThe reviewer suggested separating ring creation from population on the kernel API level, and provided an example of how the fuse kernel module could populate rings without modifying the current layout.\n\nReviewer Pavel Begunkov suggested that instead of introducing new UAPI and internal changes for kernel-managed buffer rings, the existing pbuf implementation could be piggybacked on with a flag to differentiate between them. He proposed setting this flag in __io_uring_cmd_done() if IOU_PBUF_RING_KM is set in flags.\n\nreviewer noted that the patch did not provide buffer rings when pinning the registered buffer table, and suggested an alternative approach where all memory is kept in one larger registered buffer\n\nReviewer Pavel Begunkov expressed concerns that creating many small regions in kernel-managed buffer rings would lead to inefficient memory management, including extra mmap()s, user space overhead, and wasted space for kernel allocations, as well as over-accounting and increased memory footprint for user-provided memory. He also suggested that this approach would limit the ability to free buffers while requests are pending and raised suspicions about ring bound memory lifetimes.\n\nReviewer noted that kernel-managed buffer rings would be particularly useful for operations like read and recv, where the kernel can fill rings without requiring opcode-specific code changes in kbuf.c",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change to allocation size",
                "requested changes",
                "suggested alternative solution",
                "suggested separation",
                "provided alternative implementation",
                "suggested alternative approach",
                "questioning the need for separate UAPI",
                "suggested improvements",
                "no specific request or disagreement mentioned"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, there is handful of differences. To name one, 1MB allocation won't\nget you a PMD mappable huge page, while user space can allocate 2MB,\nregister the first 1MB and reuse the rest for other purposes.\n\n---\n\nIf io_create_region() is too strict, let's discuss that in\nexamples if there are any, but it's likely not a good idea changing\nthat. If it's too lax, filter arguments in the caller. IOW, don't\npass IORING_MEM_REGION_TYPE_USER if it's not used.\n\n---\n\nI saw that and saying that all memmap.c changes can get dropped.\nYou're using it as one big virtually contig kernel memory range then\nchunked into buffers, and that's pretty much what you're getting with\nnormal io_create_region(). I get that you only need it to be\ncontiguous within a single buffer, but that's not what you're doing,\nand it'll be only worse than default io_create_region() e.g.\neffectively disabling any usefulness of io_mem_alloc_compound(),\nand ultimately you don't need to care.\n\nRegions shouldn't know anything about your buffers, how it's\nsubdivided after, etc.\n\n---\n\nstruct io_uring_region_desc rd = {};\ntotal_size = nr_bufs * buf_size;\nrd.size = PAGE_ALIGN(total_size);\nio_create_region(&region, &rd);\n\nAdd something like this for user provided memory:\n\nif (use_user_memory) {\n\trd.user_addr = uaddr;\n\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n}\n\n---\n\nI don't think I follow. I'm saying that it might be interesting\nto separate rings from how and with what they're populated on the\nkernel API level, but the fuse kernel module can do the population\nand get exactly same layout as you currently have:\n\nint fuse_create_ring(size_t region_offset /* user space argument */) {\n\tstruct io_mapped_region *mr = get_mem_region(ctx);\n\t// that can take full control of the ring\n\tring = grab_empty_ring(io_uring_ctx);\n\n\tsize = nr_bufs * buf_size;\n\tif (region_offset + size > get_size(mr)) // + other validation\n\t\treturn error;\n\n\tbuf = mr_get_ptr(mr) + offset;\n\tfor (i = 0; i < nr_bufs; i++) {\n\t\tring_push_buffer(ring, buf, buf_size);\n\t\tbuf += buf_size;\n\t}\n}\n\nfuse might not care, but with empty rings other users will get a\nchannel they can use to do IO (e.g. read requests) using their\nkernel addresses in the future.\n\n---\n\nIt'd change uapi but not internals, you already piggy back it\non pbuf implementation and differentiate with a flag.\n\nIt could basically be:\n\nif (flags & IOU_PBUF_RING_KM)\n\tbl->flags |= IOBL_KERNEL_MANAGED;\n\nPinning can be gated on that flag as well. Pretty likely uapi\nand internals will be a bit cleaner, but that's not a huge deal,\njust don't see why would you roll out a separate set of uapi\n([un]register, offsets, etc.) when essentially it can be treated\nas the same thing.\n\n---\n\nIIRC, you was pinning the registered buffer table and not provided\nbuffer rings? Which would indeed be a bad idea. Thinking about it,\nfwiw, instead of creating multiple registered buffers and trying to\nlock the entire table, you could've kept all memory in one larger\nregistered buffer and pinned only it. It's already refcounted, so\nshouldn't have been much of a problem.\n\n---\n\nTo explain why, I don't think that creating many small regions\nis a good direction going forward. In case of kernel allocation,\nit's extra mmap()s, extra user space management, and wasted space.\nFor user provided memory it's over-accounting and extra memory\nfootprint. It'll also give you better lifecycle guarantees, i.e.\nyou won't be able to free buffers while there are requests for the\ncontext. I'm not so sure about ring bound memory, let's say I have\nmy suspicions, and you'd need to be extra careful about buffer\nlifetimes even after a fuse instance dies.\n\n---\n\nAny kind of read/recv/etc. that can use provided buffers. It's\nwhere kernel memory filled rings would shine, as you'd be able\nto use them together without changing any opcode specific code.\nI.e. not changes in read request implementation, only kbuf.c\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that any pages mapped to userspace can be allocated in the kernel, which would allow for a buffer ring that is only mapped read-only into userspace, enabling zero-copy raids if the device requires stable pages for checksumming or raid.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "appreciation for design",
                "positive comment on future implementation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Any pages mapped to userspace can be allocated in the kernel as well.\n\nAnd I really do like this design, because it means we can have a\nbuffer ring that is only mapped read-only into userspace.  That way\nwe can still do zero-copy raids if the device requires stable pages\nfor checksumming or raid.  I was going to implement this as soon\nas this series lands upstream.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong explained that she originally used io_region_allocate_pages(), but it failed due to excessive memory allocation, so she chose to use io_region_allocate_pages_multi_buf() instead to bypass the issue.\n\nAuthor clarifies her understanding of reviewer's feedback, confirming she initially thought user should own and manage buffers, but now realizes kernel can allocate them through IORING_REGISTER_MEM_REGION interface.\n\nAuthor Joanne Koong addressed Pavel Begunkov's feedback about combining kernel-managed buffer rings (kmbufs) and regular pbufs into a single API, explaining that it would make the pbuf API more complex and harder to understand. She agreed to combine the interfaces in v2 unless someone else objects.\n\nAuthor acknowledged that she previously proposed pinning the registered buffer table, not the pbuf ring, and no further action is implied.\n\nThe author is addressing Pavel Begunkov's suggestion that sparse buffers populated by the kernel should be automatically pinned. The author expresses uncertainty about this idea and notes that if implemented, users would need to unregister buffers individually instead of using IORING_UNREGISTER_BUFFERS.\n\nAuthor is addressing a concern about buffer allocation, specifically whether individual buffers should be allocated separately by the kernel. She acknowledges the memory allocation issue and suggests making a change to allocate the region all at once if it's bypassable, but expresses disagreement with allocating separate buffers due to concerns about extra mmaps and userspace management.\n\nAuthor asked for clarification on reviewer's concerns about over-accounting and extra memory footprint in kernel-managed buffer rings.\n\nAuthor is addressing concerns about the API and kernel buffer allocation in the io_uring series. She plans to make changes in v2, including removing the KMBUF_RING API interface, having kernel buffer allocation go through IORING_REGISTER_MEM_REGION, and adding APIs for subsystems to populate a kernel-managed buffer ring with addresses from the registered memory region.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "clarification",
                "explanation",
                "understanding",
                "acknowledged fix is needed",
                "agreed to restructure",
                "acknowledgment of prior mistake",
                "uncertainty",
                "explaining trade-offs",
                "explaining reasoning",
                "clarifying question",
                "acknowledges fix is needed",
                "plans changes in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When I originally implemented it, I had it use\nio_region_allocate_pages() but this fails because it's allocating way\ntoo much memory at once. For fuse's use case, each buffer is usually\nat least 1 MB if not more. Allocating the memory one buffer a time in\nio_region_allocate_pages_multi_buf() bypasses the allocation errors I\nwas seeing. That's the main reason I don't think this can just use\nio_create_region().\n\n---\n\nOh okay, from your first message I (and I think christoph too) thought\nwhat you were saying is that the user should be responsible for\nallocating the buffers with complete ownership over them, and then\njust pass those allocated to the kernel to use. But what you're saying\nis that just use a different way for getting the kernel to allocate\nthe buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am\nI reading this correctly?\n\n---\n\nimo, it looked cleaner as a separate api because it has different\nexpectations and behaviors and squashing kmbuf into the pbuf api makes\nthe pbuf api needlessly more complex. Though I guess from the\nuserspace pov, liburing could have a wrapper that takes care of\nsetting up the pbuf details for kernel-managed pbufs. But in my head,\nhaving pbufs vs. kmbufs makes it clearer what each one does vs regular\npbufs vs. pbufs that are kernel-managed.\n\nEspecially with now having kmbufs go through the ioring mem region\ninterface, it makes things more confusing imo if they're combined, eg\npbufs that are kernel-managed are created empty and then populated\nfrom the kernel side by whatever subsystem is using them. Right now\nthere's only one mem region supported per ring, but in the future if\nthere's the possibility that multiple mem regions can be registered\n(eg if userspace doesn't know upfront what mem region length they'll\nneed), then we should also probably add in a region id param for the\nregistration arg, which if kmbuf rings go through the pbuf ring\nregistration api, is not possible to do.\n\nBut I'm happy to combine the interfaces and go with your suggestion.\nI'll make this change for v2 unless someone else objects.\n\n---\n\nYeah, you're right I misremembered and the objections / patch I\ndropped was pinning the registered buffer table, not the pbuf ring\n\n---\n\nHmm, I'm not sure this idea would work for sparse buffers populated by\nthe kernel, unless those are automatically pinned too but then from\nthe user POV for unregistration they'd need to unregister buffers\nindividually instead of just calling IORING_UNREGISTER_BUFFERS but it\nmight be annoying for them to now need to know which buffers are\npinned vs not. When i benchmarked the fuse code with vs without pinned\nregistered buffers, it didn't seem to make much of a difference\nperformance-wise thankfully, so I just dropped it.\n\n---\n\nTo clarify, is this in reply to why the individual buffers shouldn't\nbe allocated separately by the kernel?\nI added a comment about this above in the discussion about\nio_region_allocate_pages_multi_buf(), and if the memory allocation\nissue I was seeing is bypassable and the region can be allocated all\nat once, I'm happy to make that change. With having the allocation be\nseparate buffers though, I'm not sure I agree that there are extra\nmmaps / userspace management. All the pages across the buffers are\nvmapped together and the userspace just needs to do 1 mmap call for\nthem. On the userspace side, I don't think there's more management\nsince the mmapped address represents the range across all the buffers.\nI'm not seeing how there's wasted space either since the only\nrequirement is that the buffer size is page aligned. I think also\nthere's a higher chance of the entire buffer region being physically\ncontiguous if each buffer is allocated separately vs. all the buffers\nare allocated as 1 region. I don't feel strongly about this either way\nand I'm happy to allocate the entire region at once if that's\npossible.\n\n---\n\nJust out of curiosity, could you elaborate on the over-accounting and\nextra memory footprint? I was under the impression it would be the\nsame since the accounting gets adjusted by the total bytes allocated?\nFor the extra memory footprint, is the extra footprint from the\nmetadata to describe each buffer region, or are you referring to\nsomething else?\n\n---\n\nThanks for your input on the series. To iterate / sum up, these are\nchanges for v2 I'll be making:\n- api-wise from userspace/liburing: get rid of KMBUF_RING api\ninterface and have users go through PBUF_RING api instead with a flag\nindicating the ring is kernel-managed\n- have kernel buffer allocation go through IORING_REGISTER_MEM_REGION\ninstead, which means when the pbuf ring is created and the\nkernel-managed flag is set, the ring will be empty. The memory region\nwill need to be registered before the mmap call to the ring fd.\n- add apis for subsystems to populate a kernel-managed buffer ring\nwith addresses from the registered mem region\n\nDoes this align with your understanding of the conversation as well or\nis there anything I'm missing?\n\nAnd Christoph, do these changes for v2 work for your use case as well?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig argued against setting the selected buffer index in __io_uring_cmd_done(), citing a need for kernel-controlled allocation and guaranteeing user processes can only read memory, not write to it.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "opinion diverges from original patch"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm arguing exactly against this.  For my use case I need a setup\nwhere the kernel controls the allocation fully and guarantees user\nprocesses can only read the memory but never write to it.  I'd love\nto be able to piggy back than onto your work.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that using power-of-2 round ups for memory allocations will result in wasted memory, as 1MB allocations will not become 2MB huge pages, and also questioned the handling of 1GB huge pages, suggesting users may be able to make better placement decisions.\n\nThe reviewer suggests that the io_uring uapi should include fields for user-provided memory, making it an optional feature for pbuf rings/regions/etc., and notes that fuse can refuse to bind to buffer rings it doesn't like.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "memory_waste",
                "user_control",
                "reviewer's suggestion is neutral as they are not strongly advocating for a change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "pow2 round ups will waste memory. 1MB allocations will never\nbecome 2MB huge pages. And there is a separate question of\n1GB huge pages. The user can be smarter about all placement\ndecisions.\n\n---\n\nThat's an interesting case. To be clear, user provided memory is\nan optional feature for pbuf rings / regions / etc., and I think\nthe io_uring uapi should leave fields for the feature. However, I\nhave nothing against fuse refusing to bind to buffer rings it\ndoesn't like.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested modifying IORING_REGISTER_MEM_REGION to support read-only registrations, and proposed adding a new registration flag or rejecting unsupported setups during init.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal",
                "request for further discussion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IORING_REGISTER_MEM_REGION supports both types of allocations. It can\nhave a new registration flag for read-only, and then you either make\nthe bounce avoidance optional or reject binding fuse to unsupported\nsetups during init. Any arguments against that? I need to go over\nJoanne's reply, but I don't see any contradiction in principal with\nyour use case.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author is clarifying the difference between kernel-managed buffer rings and user-initiated setup of kbuf rings. She explains that if userspace doesn't initiate the setup, IORING_REGISTER_MEM_REGION becomes semantically equivalent to kernel-managed allocation.\n\nAuthor addressed a concern about the complexity and potential over-engineering of the kernel-managed buffer ring interface, suggesting a simpler approach where the user populates the ring through the pbuf interface and adding an optional interface for IORING_REGISTERED_MEM_REGIONS in the future.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation",
                "overkill",
                "over-engineered"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"control the allocation fully\" do you mean for your use case, the\nallocation/setup isn't triggered by userspace but is initiated by the\nkernel (eg user never explicitly registers any kbuf ring, the kernel\njust uses the kbuf ring data structure internally and users can read\nthe buffer contents)? If userspace initiates the setup of the kbuf\nring, going through IORING_REGISTER_MEM_REGION would be semantically\nthe same, except the buffer allocation by the kernel now happens\nbefore the ring is created and then later populated into the ring.\nuserspace would still need to make an mmap call to the region and the\nkernel could enforce that as read-only. But if userspace doesn't\ninitiate the setup, then going through IORING_REGISTER_MEM_REGION gets\nuglier.\n\n---\n\nSo i guess the flow would have to be:\na) user calls io_uring_register_region(&ring, &mem_region_reg) with\nmem_region_reg.region_uptr's size field set to the total buffer size\n(and mem_region_reg.flags read-only bit set if needed)\n     kernel allocates region\nb) user calls mmap() to get the address of the region. If read-only\nbit was set, it gets a read-only address\nc) user calls io_uring_register_buf_ring(&ring, &buf_reg, flags) with\nbuf_reg.flags |= IOU_PBUF_RING_KERNEL_MANAGED\n     kernel creates an empty kernel-managed ring. None of the buffers\nare populated\nd) user tells X subsystem to populate the ring starting from offset Z\nin the registered mem region\ne) on the kernel side, the subsystem populates the ring starting from\noffset Z, filling it up using the buf_size and ring_entries values\nthat the user registered the ring with in c)\n\nTo be completely honest, the more I look at this the more this feels\nlike overkill / over-engineered to me. I get that now the user can do\nthe PMD optimization, but does that actually lead to noticeable\nperformance benefits? It seems especially confusing with them going\nthrough the same pbuf ring interface but having totally different\nexpectations.\n\nWhat about adding a straightforward kmbuf ring that goes through the\npbuf interface (eg the design in this patchset) and then in the future\nadding an interface for pbuf rings (both kernel-managed and\nnon-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if\nusers end up needing/wanting to have their rings populated that way?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "reviewer suggested rounding up buffer allocation to a multiple of PTE levels to mitigate TLB pressure, rather than setting the selected buffer index in __io_uring_cmd_done()\n\nChristoph Hellwig questioned the meaning of 'pbuf' in the patch description, expressing confusion about how it relates to io_uring_register_buffers* and suggesting that web searches have become less useful for understanding io_uring APIs.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested change",
                "alternative solution",
                "confusion",
                "lack of clarity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure.  But if the application cares that much about TLB pressure\nI'd just round up to nice multtiple of PTE levels.\n\n---\n\nCan you clarify what you mean with 'pbuf'?  The only fixed buffer API I\nknow is io_uring_register_buffers* which always takes user provided\nbuffers, so I have a hard time parsing what you're saying there.  But\nthat might just be sign that I'm no expert in io_uring APIs, and that\nweb searches have degraded to the point of not being very useful\nanymore.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer noted that IORING_REGISTER_MEM_REGION's purpose is unclear, as it is described in both the commit message and public documentation as related to cqs (completion queues), but this seems inconsistent.\n\nreviewer noted that the patch does not address their specific use case of block and file system I/O, which is different from the original fuse over io_uring series",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "inconsistency",
                "unclear",
                "use case mismatch"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IORING_REGISTER_MEM_REGION seems to be all about cqs from both your\ncommit message and the public documentation.  I'm confused.\n\n---\n\nMy use case is not about fuse, but good old block and file system\nI/O.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that io_uring_register_buffers() only pins memory, allowing applications or other processes to modify it, which can cause issues for file systems and storage devices that need to verify checksums or rebuild data from parity.\n\nreviewer noted that the patch does not address the issue of buffer selection in __io_uring_cmd_done(), and requested a fix",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The idea is that the application tells the kernel that it wants to use\na fixed buffer pool for reads.  Right now the application does this\nusing io_uring_register_buffers().  The problem with that is that\nio_uring_register_buffers ends up just doing a pin of the memory,\nbut the application or, in case of shared memory, someone else could\nstill modify the memory.  If the underlying file system or storage\ndevice needs verify checksums, or worse rebuild data from parity\n(or uncompress), it needs to ensure that the memory it is operating\non can't be modified by someone else.\n\nSo I've been thinking of a version of io_uring_register_buffers where\nthe buffers are not provided by the application, but instead by the\nkernel and mapped into the application address space read-only for\na while, and I thought I could implement this on top of your series,\nbut I have to admit I haven't really looked into the details all\nthat much.\n\n---\n\nYes.  The PMD mapping also is not that relevant.  Both AMD (implicit)\nand ARM (explicit) have optimizations for contiguous PTEs that are\nalmost as valuable.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the provided buffer rings in this series are not clearly distinguished from registered buffers and questioned the need for io_uring to allocate payload memory, suggesting it is inflexible and may lead to kernel crashes if used with other types of requests. He also suggested making the code cleaner and more flexible for fuse's use case without questioning the I/O path.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "infelicities in design",
                "potential for kernel crashes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Registered, aka fixed, buffers are the ones you pass to\nIORING_OP_[READ,WRITE]_FIXED and some other requests. It's normally\ncreated by io_uring_register_buffers*() / IORING_REGISTER_BUFFERS*\nwith user memory, but there are special cases when it's installed\ninternally by other kernel components, e.g. ublk.\nThis series has nothing to do with them, and relevant parts of\nthe discussion here don't mention them either.\n\nProvided buffer rings, a.k.a pbuf rings, IORING_REGISTER_PBUF_RING\nis a kernel-user shared ring. The entries are user buffers\n{uaddr, size}. The user space adds entries, the kernel (io_uring\nrequests) consumes them and issues I/O using the user addresses.\nE.g. you can issue a IORING_OP_RECV request (+IOSQE_BUFFER_SELECT)\nand it'll grab a buffer from the ring instead of using sqe->addr.\n\npbuf rings, IORING_REGISTER_MEM_REGION, completion/submission\nqueues and all other kernel-user rings/etc. are internally based\non so called regions. All of them support both user allocated\nmemory and kernel allocations + mmap.\n\nThis series essentially creates provided buffer rings, where\n1. the ring now contains kernel addresses\n2. the ring itself is in-kernel only and not shared with user space\n3. it also allocates kernel buffers (as a region), populates the ring\n    with them, and allows mapping the buffers into the user space.\n\nFuse is doing both adding (kernel) buffers to the ring and consuming\nthem. At which point it's not clear:\n\n1. Why it even needs io_uring provided buffer rings, it can be all\n    contained in fuse. Maybe it's trying to reuse pbuf ring code as\n    basically an internal memory allocator, but then why expose buffer\n    rings as an io_uring uapi instead of keeping it internally.\n\n    That's also why I mentioned whether those buffers are supposed to\n    be used with other types of io_uring requests like recv, etc.\n\n2. Why making io_uring to allocate payload memory. The answer to which\n    is probably to reuse the region api with mmap and so on. And why\n    payload buffers are inseparably created together with the ring\n    and via a new io_uring uapi.\n\n    And yes, I believe in the current form it's inflexible, it requires\n    a new io_uring uapi. It requires the number of buffers to match\n    the number of ring entries, which are related but not the same\n    thing. You can't easily add more memory as it's bound to the ring\n    object. The buffer memory won't even have same lifetime as the\n    ring object -- allow using that km buffer ring with recv requests\n    and highly likely I'll most likely give you a way to crash the\n    kernel.\n\nBut hey, I'm tired. I don't have any beef here and am only trying\nto make it a bit cleaner and flexible for fuse in the first place\nwithout even questioning the I/O path. If everyone believes\neverything is right, just ask Jens to merge it.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer expressed concern that the patch introduces using buffer rings for huge payload buffers, which was not their original intention and may lead to memory waste.\n\nreviewer questioned the feasibility of kernel-managed buffer rings without a kernel component returning buffers into the ring, citing that io_uring does not currently support this",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "memory waste",
                "confusion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Think of it as an area of memory for kernel-user communication. Used\nfor syscall parameters passing to avoid copy_from_user, but I added\nit for a bunch of use cases. We'll hopefully get support at some\npoint for passing request arguments like struct iovec. BPF patches\nuse it for communication. I need to respin patches placing SQ/CQ onto\nit (avoid some memory waste).\n\nTbh, I never meant it nor io_uring regions to be used for huge\npayload buffers, but this series already uses regions for that.\n\n---\n\nThen I'm confused. Take a look at the other reply, this series is\nabout buffer rings with kernel memory, it can't work without a kernel\ncomponent returning buffers into the ring, and io_uring doesn't do\nthat. But maybe you're thinking about adding some more elaborate API.\n\nIIUC, Joanne also wants to add support for fuse installing registered\nbuffers, which would allow zero-copy, but those got split out of\nthis series.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the patch does not handle a specific edge case where the selected buffer index is out of range, and suggested working around this issue by wrapping the code in a loop.\n\nReviewer Pavel Begunkov noted that the patch should disentangle memory allocation from ring creation in the io_uring uapi, and instead move ring population into fuse, where it will be populated by the kernel without user space access to the ring.\n\nReviewer Pavel Begunkov noted that the differences between the two buffer allocation paths are minimal and suggested that they could be handled by a single opcode, but did not strongly object to making them separate opcodes.\n\nThe reviewer noted that without patches using the kernel-managed buffer rings functionality, it is inconvenient to test or verify its correctness. They suggested a control path io-uring command (FUSE_CMD_BIND_BUFFER_RING) to bind a fuse buffer ring to an io_uring region and buf_ring, which would allow passing necessary parameters to the bind_queue function.\n\nreviewer questioned the need for a separate buffer region, suggesting use of IORING_REGISTER_MEM_REGION instead\n\nReviewer noted that when allocating huge pages for buffers, the total allocation size may not be a power of two, potentially leading to wasted space due to alignment requirements, and requested consideration for this scenario.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "workaround",
                "disagreement with current approach",
                "suggestion for alternative solution",
                "no strong opinion",
                "open to alternative",
                "inconvenience",
                "suggested change",
                "potential performance issue",
                "alignment requirement"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Let's fix that then. For now, just work it around by wrapping\ninto a loop.\n\nBtw, I thought you're going to use it for metadata like some\nfuse headers and payloads would be zero copied by installing\nit as registered buffers.\n\n...\n\n---\n\nThe main point is disentangling memory allocation from ring\ncreation in the io_uring uapi, and moving ring population\ninto fuse instead of doing it at creation. And it'll still be\npopulated by the kernel (fuse), user space doesn't have access\nto the ring. IORING_REGISTER_MEM_REGION is just the easiest way\nto achieve that without any extra uapi.\n\n...\n\n---\n\nIt appeared to me that they're different because of special\nregion path and embedded buffer allocations, and otherwise\ndifferences would be minimal. But if you think it's still\nbetter to be made as a separate opcode, I'm not opposing it,\ngo for it.\n\n---\n\nNot having patches using the functionality is inconvenient. How\nfuse looks up the buffer ring from io_uring? I could imagine you\nhave some control path io-uring command:\n\ncase FUSE_CMD_BIND_BUFFER_RING:\n\treturn bind_queue(params);\n\nThen you can pass all necessary parameters to it, pseudo code:\n\nstruct fuse_bind_kmbuf_ring_params {\n\tregion_id;\n\tbuf_ring_id;\n\t...\n};\n\nbind_queue(cmd, struct fuse_bind_kmbuf_ring_params *p)\n{\n\tregion = io_uring_get_region(cmd, p->region_id);\n\t// get exclusive access:\n\tbuf_ring = io_uring_get_buf_ring(cmd, p->buf_ring_id);\n\n\tif (!validate_buf_ring(buf_ring))\n\t\treturn NOTSUPPORTED;\n\n\tio_uring_pin(buf_ring);\n\tfuse_populate_buf_ring(buf_ring, region, ...);\n}\n\nDoes that match expectations? I don't think you even need\nthe ring part exposed as an io_uring uapi, tbh, as it\nstays completely in fuse and doesn't meaningfully interact\nwith the rest of io_uring.\n\n...\n\n---\n\nThat was about an argument for using IORING_REGISTER_MEM_REGION\ninstead a separate region. And it's separate from whether\nbuffers should be bound to the ring.\n\n---\n\nI shouldn't affect you much since you have such large buffers,\nbut imagine the total allocation size is not being pow2, and\nthe kernel allocating it as a single folio. E.g. 3 buffers,\n0.5 MB each, total = 1.5MB, and the kernel allocates a 2MB\nhuge page.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer argued that the series does not address registered buffers and suggested separating buffer allocation for io_uring",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There is nothing about registered buffers in this series. And even\nif you try to reuse buffer allocation out of it, it'll come with\na circular buffer you'll have no need for. And I'm pretty much\narguing about separating those for io_uring.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested reusing regions for allocations and mmap()ing, wrapping them into a registered buffer to avoid vmap'ing altogether.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, the easiest solution is to internally reuse regions for\nallocations and mmap()'ing and wrap it into a registered buffer.\nIt just need to make vmap'ing optional as it won't be needed.\n\n-- \nPavel Begunkov",
              "reply_to": "",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that the io_uring uapi should not be tied to specific use cases or requirements, such as uniform buffer sizes, ring size matching buffer count, and buffers being allocated by io_uring. He questioned why these constraints are necessary and suggested that the design should allow for more flexibility, including the ability to add memory at runtime.\n\nreviewer questioned the separation of buffers from rings, expressing uncertainty about the differences between in-kernel buffers with kernel addresses and user-visible buffers with user addresses",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "uncertainty",
                "lack of clear expectations"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, it's mainly about not keeping payload buffers and rings in the same\nobject from the io_uring uapi perspective.\n\n1. If it's an io_uring uapi, it shouldn't be fuse specific or with\na bunch of use case specific expectations attached. Why does it\nrequire all buffers to be uniform in size? Why does it require\nthe ring size to match the number of buffers? Why does it require\nbuffers to be allocated by io_uring in the first place? Maybe some\nsubsystem got memory from somewhere else and wants to do use it\nwith io_uring. Why does it need to know the total size at creation,\nand what would you do if you want to add more memory at runtime\nwhile using the same ring?\n\n2. If it's meant to be fuse specific and _not_ used with other requests\nlike recv/read/etc., then what's the point of having it as an io_uring\nuapi? Which also adds additional trouble like the once you're solving\nwith pinning.\n\nIf it's supposed to be used with other requests, then buffers and\nrings will have different in-kernel lifetime expectations imposed\nby io_uring, so having them together won't even help with\nmanagement.\n\nI have a strong opinion about the memmap.c change. For the\nrest, if you believe it's fine, just send it out and let Jens\ndecide.\n\n---\n\nIt's predicated on separating buffers from rings, see above,\nand assuming that I'm not sure what expectations are different\napart from one being in-kernel with kernel addresses and the\nother user visible with user addresses.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to Pavel Begunkov's feedback by agreeing that the circular buffer will be useful for Christoph's use case, which involves differently sized read payloads across requests. The author believes this will reduce memory allocation and enable sharing of buffers across entries.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed with the approach",
                "acknowledged a benefit"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think the circular buffer will be useful for Christoph's use case in\nthe same way it'll be useful for fuse's. The read payload could be\ndifferently sized across requests, so it's a lot of wasted space to\nhave to allocate a buffer large enough to support the max-size request\nper entry in the io_ring. With using a circular buffer, buffers have a\nway to be shared across entries, which means we can significantly\nreduce how much memory needs to be allocated.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author agrees that the patch's use case aligns with Christoph Hellwig's, but notes that his buffers need to be read-only, implying that a modification is needed to accommodate this difference.\n\nAuthor addressed Christoph's concern about making the mmap call return a read-only mapping by proposing to add a read-only flag in io_uring_register_buf_ring() and checking it when userspace makes the mmap call, or using IORING_MEM_REGION to allocate memory with a read-only flag. Author is willing to add this patch to the series if needed.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges feedback",
                "implies fix is needed",
                "willingness to add additional patch",
                "proposing alternative solutions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "(resending because I hit reply instead of reply-all)\n\nI think we have the exact same use case, except your buffers need to\nbe read-only. I think your use case benefits from the same memory wins\nwe'll get with incremental buffer consumption, which is the primary\nreason fuse is using a bufring instead of fixed buffers.\n\n---\n\nI think you can and it'll be very easy to do so. All that would be\nneeded is to pass in a read-only flag from the userspace side when it\nregisters the bufring, and then when userspace makes the mmap call to\nthe bufring, the kernel checks if that read-only flag is set on the\nbufring and if so returns a read-only mapping. I'm happy to add that\npatch to this series if that would make things easier for you. The\nio_uring_register_buffers() api registers fixed buffers (which have to\nbe user-allocated memory) so you would need to go through the\nio_uring_register_buf_ring() api once kmbufs are squashed into the\npbuf interface.\n\nWith going through IORING_MEM_REGION, this would work for your use\ncase as well. The user would have to register the mem region with\nio_uring_register_region() and pass in a read-only flag, and then the\nkernel will allocate the memory region. Then userspace would mmap the\nmemory region and on the kernel side, it would set the mapping to be\nread-only. When the kmbufring then gets registered, the buffers in it\nwill be empty. The filesystem will then have to populate the buffers\nin it from the mem region that was previously registered.\n\nThanks,\nJoanne",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bernd Schubert",
              "summary": "Reviewer questioned the usefulness of sharing buffers across io_uring entries, suggesting it would only reduce the ring size and not provide any benefits.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning usefulness",
                "suggested alternative"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Dunno, what we actually want is requests of multiple sizes. Sharing\nbuffers across entries sounds like just reducing the ring size - I\npersonally don't see the point here.\n\n\nThanks,\nBernd",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarified that 'sharing buffers across entries' means allowing different parts of a buffer to be used simultaneously by multiple io_uring entries, addressing Bernd's feedback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"sharing buffers across entries\" what I mean is different regions\nof the buffer can now be used concurrently by multiple entries.\n\nThanks,\nJoanne",
              "reply_to": "Bernd Schubert",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing concerns about the need for kernel-managed buffer rings in the context of fuse's use case, specifically to control when buffers get recycled back into the ring. The author explains that this is necessary because the server needs to write data back to the kernel in the same buffer after submitting an sqe, and then the client needs to recycle that buffer so it can be reused for a future cqe.\n\nAuthor acknowledged that the selected buffer index needs to be set in __io_uring_cmd_done() for userspace/server-side operations, and agreed to add this functionality.\n\nAuthor addressed Pavel Begunkov's feedback about using a registered memory region, agreeing it allows optimizations but questioning whether most use cases benefit from them, and suggesting that offering both simple and advanced kernel-managed buffer options is sufficient.\n\nThe author addressed Pavel Begunkov's concern that combining the interface for kernel-managed buffer rings (kmbufs) and user-provided buffer rings (pbufs) through a single uapi is confusing, particularly given the different expectations and behaviors of kmbufs. The author acknowledged their initial opinion but agreed to restructure in v2 by having kmbufs go through the pbuf uapi.\n\nAuthor responded to feedback about having a ring entry with no buffer associated with it, stating that this is similar to existing code and can be fixed by passing the number of buffers from the uapi for kernel-managed pbuf rings.\n\nAuthor responded to feedback from Pavel Begunkov by explaining that adding more memory to the registered memory region is not feasible and that users may need to allocate upfront, which could be challenging in certain scenarios.\n\nAuthor Joanne Koong addressed a concern about the lifetime of buffer memory in relation to the ring object, explaining that the buffers are only freed when the ring itself is freed.\n\nAuthor is addressing Pavel Begunkov's feedback about whether the patch should support both a simple kernel-managed pbuf interface and a more complex one that goes through a registered memory region, and is open to making changes based on reviewer input.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "clarification",
                "explanation",
                "acknowledged a fix is needed",
                "questioning the value of added complexity",
                "suggesting a simpler approach",
                "agreed to restructure",
                "no clear resolution signal",
                "author provides explanation",
                "acknowledged a challenge",
                "provided explanation",
                "open to change",
                "willing to make adjustments"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The most important part and the whole reason fuse needs the buffer\nring to be kernel-managed is because the kernel needs to control when\nbuffers get recycled back into the ring. For fuse's use case, the\nbuffer is used for passing data between the kernel and the server. We\ncan't have the server recycle the buffer because the server writes\nback data to the kernel in that buffer when it submits the sqe. After\nfuse receives the sqe and reads the reply from the server, it then\nneeds to recycle that buffer back into the ring so it can be reused\nfor a future cqe (eg sending a future request).\n\n---\n\nOn the userspace/server side, it uses the buffers for other io-uring\noperations (eg reading or writing the contents from/to a\nlocally-backed file).\n\n---\n\nMy main motivation for this is simplicity. I see (and thanks for\nexplaining) that using a registered mem region allows the use of some\noptimizations (the only one I know of right now is the PMD one you\nmentioned but maybe there's more I'm missing) that could be useful for\nsome workloads, but I don't think (and this could just be my lack of\nunderstanding of what more optimizations there are) most use cases of\nkmbufs benefit from those optimizations, so to me it feels like we're\nadding non-trivial complexity for no noticeable benefit.\n\nI feel like we get the best of both worlds by letting users have both:\nthe simple kernel-managed pbuf where the kernel allocates the buffers\nand the buffers are tied to the lifecycle of the ring, and the more\nadvanced kernel-managed pbuf where buffers are tied to a registered\nmemory region that the subsystem is responsible for later populating\nthe ring with.\n\n---\n\nimo it felt cleaner to have a new uapi for it because kmbufs and pbufs\nhave different expectations and behaviors (eg pbufs only work with\nuser-provided buffers and requires userspace to populate the ring\nbefore using it, whereas for kmbufs the kernel allocates the buffers\nand populates it for you; pbufs require userspace to recycle back the\nbuffer, whereas for kmbufs the kernel is the one in control of\nrecycling) and from the user pov it seemed confusing to have kmbufs as\npart of the pbuf ring uapi, instead of separating it out as a\ndifferent type of ringbuffer with a different expectation and\nbehavior. I was trying to make the point that combining the interface\nif we go with IORING_MEM_REGION gets even more confusing because now\npbufs that are kernel-managed are also empty at initialization and\nonly can point to areas inside a registered mem region and the\nresponsibility of populating it is now on whatever subsystem is using\nit.\n\nI still have this opinion but I also think in general, you likely know\nbetter than I do what kind of io-uring uapi is best for io-uring's\nusers. For v2 I'll have kmbufs go through the pbuf uapi.\n\n---\n\nI'm not really seeing what the purpose of having a ring entry with no\nbuffer associated with it is. In the existing code for non-kernel\nmanaged pbuf rings, there's the same tie between reg->ring_entries\nbeing used as the marker for how many buffers the ring supports. But\nif the number of buffers should be different than the number of ring\nentries, this can be easily fixed by passing in the number of buffers\nfrom the uapi for kernel-managed pbuf rings.\n\n---\n\nTo play devil's advocate, we also can't easily add more memory to the\nmem region once it's been registered. I think there's also a worse\npenalty where the user needs to know upfront how much memory to\nallocate for the mem region for the lifetime of the ring, which imo\nmay be hard to do (eg if a kernel-managed buf ring only needs to be\nregistered for some code paths and not others, the mem region\nregistration would still have to allocate the memory a potential kbuf\nring would use).\n\n---\n\nI'm a bit confused by this part. The buffer memory does have the same\nlifetime as the ring object, no? The buffers only get freed when the\nring itself is freed.\n\n---\n\nI appreciate you looking at this and giving your feedback and insight.\nThank you for doing so. I don't want to merge in something you're\nunhappy with.\n\nAre you open to having support for both a simple kernel-managed pbuf\ninterface and later on if/when the need arises, a kernel-managed pbuf\ninterface that goes through a registered memory region? If the answer\nis no, then I'll make the change to have kmbufs go through the\nregistered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that buffer rings are not suitable for storage read/write requests because they immediately bind to a buffer, whereas other types of requests like recv first poll the socket and then take a buffer from the ring. He also pointed out that someone needs to return buffers back into the kernel private ring, which is currently assumed to be handled by the fuse driver but poses a problem for normal rw requests.\n\nReviewer Pavel Begunkov suggested using IORING_MEM_REGION or a standalone registered buffer extension to provide buffers/memory without extra semantics, potentially yielding a finer API.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "problem with current implementation",
                "suggested alternative approach",
                "potential for improved API"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Provided buffer rings are not useful for storage read/write requests\nbecause they bind to a buffer right away, that's in contrast to some\nrecv request, where io_uring will first poll the socket to confirm\nthe data is there, and only then take a buffer from the buffer ring\nand copy into it. With storage rw it makes more sense to specify\nthe buffer directly gain control over where exactly data lands\nIOW, instead of the usual \"read data into a given pointer\" request\nsemantics like what read(2) gives you, buffer rings are rather\n\"read data somewhere and return a pointer to where you placed it\".\n\nAnother problem is that someone needs to return buffers back into\nthe buffer ring, and it's a kernel private ring. For this patchset\nit's assumed the fuse driver is going to be doing that, but there\nis no one for normal rw requests.\n\n---\n\nYes. You only need buffers, and it'll be better to base on sth that\ngives you buffers/memory without extra semantics, i.e.\nIORING_MEM_REGION. Or it can be a standalone registered buffer\nextension, likely reusing regions internally. That might even yield\na finer API.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-18",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer questioned whether kernel-managed buffer rings can be used with other requests, specifically IORING_OP_RECV with IOSQE_BUFFER_SELECT\n\nReviewer Pavel Begunkov commented on the patch, noting that there are two separate issues: (1) making buffers inseparable from buffer rings in the io_uring user API and (2) optionally allowing user memory for buffer creation. He suggests implementing this by passing an argument while creating a region.\n\nreviewer questioned the necessity of making buffer rings an io_uring API, suggesting it could be simpler to implement in fuse or as an implementation detail within io_uring\n\nReviewer Pavel Begunkov noted that the current implementation of kernel-managed buffer rings in io_uring is not reusable and specific to fuse use case, suggesting a middle ground approach where km rings can be registered together with memory as a pure region without buffer notion, allowing fuse to chunk it later.\n\nreviewer noted that the patch introduces a non-generic io_uring uapi, which is assumed to be generic in other parts of the code, and requested clarification on this design decision\n\nThe reviewer noted that the current implementation of __io_uring_cmd_done() only sets the buffer ring depth but does not account for the actual memory allocated by userspace, which could lead to issues if the user allocates more memory than the ring size or vice versa. The reviewer suggests considering dynamic allocation and de-fragmentation mechanisms.\n\nThe reviewer suggested that instead of passing the number of buffers to io_uring, the kernel should allocate a large chunk of memory and let fuse manage the buffer allocation.\n\nReviewer Pavel Begunkov agreed with the patch but noted that adding new memory would require a new mechanism, not necessarily tied to IORING_REGISTER_MEM_REGION.\n\nThe reviewer noted that unregistering a buffer ring does not guarantee the absence of inflight requests using buffers from the ring, and requested synchronization with all other io_uring requests to address this issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "question",
                "clarification",
                "no clear signal",
                "request for implementation",
                "questioning necessity",
                "suggesting alternative approaches",
                "requested changes",
                "design decision",
                "non-generic uapi",
                "agreed",
                "noted",
                "synchronization",
                "inflight requests"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Oops, typo. I was asking whether the buffer rings (not buffers) are\nsupposed to be used with other requests. E.g. submitting a\nIORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying\nyour kernel-managed buffer ring.\n\n---\n\nThere are two separate arguments. The first is about not making buffers\ninseparable from buffer rings in the io_uring user API. Whether it's\nIORING_REGISTER_MEM_REGION or something else is not that important.\nI have no objection if it's a part of fuse instead though, e.g. if\nfuse binds two objects together when you register it with fuse, or even\nif fuse create a buffer ring internally (assuming it doesn't indirectly\nleak into io_uring uapi).\n\nAnd the second was about optionally allowing user memory for buffer\ncreation as you're reusing the region abstraction. You can find pros\nand cons for both modes, and funnily enough, SQ/CQ were first kernel\nallocated and then people asked for backing it by user memory, and IIRC\nit was in the reverse order for pbuf rings.\n\nImplementing this is trivial as well, you just need to pass an argument\nwhile creating a region. All new region users use struct\nio_uring_region_desc for uapi and forward it to io_create_region()\nwithout caring if it's user or kernel allocated memory.\n\n---\n\nThe stress is on why it's an _io_uring_ API. It doesn't matter to me\nwhether it's a separate opcode or not. Currently, buffer rings don't give\nyou anything that can't be pure fuse, and it might be simpler to have\nit implemented in fuse than binding to some io_uring object. Or it could\ncreate buffer rings internally to reuse code but it doesn't become an\nio_uring uapi but rather implementation detail. And that predicates on\nwhether km rings are intended to be used with other / non-fuse requests.\n\n---\n\nI believe the source of disagreement is that you're thinking\nabout how it's going to look like for fuse specifically, and I\nbelieve you that it'll be nicer for the fuse use case. However,\non the other hand it's an io_uring uapi, and if it is an io_uring\nuapi, we need reusable blocks that are not specific to particular\nusers.\n\nIf it km rings has to stay an io_uring uapi, I guess a middle\nground would be to allow registering km rings together with memory,\nbut make it a pure region without a notion of a buffer, and let\nfuse to chunk it. Later, we can make payload memory allocation\noptional.\n\n---\n\nRight, intentionally so, because otherwise it's a fuse uapi that\npretends to be a generic io_uring uapi but it's not because of\nall assumptions in different places.\n\n---\n\nNot really, it tells the buffer ring depth but says nothing about\nhow much memory user space allocated and how it's pushed. It's a\nreasonable default but they could be different. For example, if you\nexpect adding more memory at runtime, you might create the buffer\nring a bit larger. Or when server processing takes a while and you\ncan't recycle until it finishes, you might have more buffers than\nyou need ring entries. Or you might might decide to split buffers\nand as you mentioned incremental consumption, which is an entire\nseparate topic because it doesn't do de-fragmentation and you'd\nneed to have it in fuse, just like user space does with pbufs.\n\n---\n\nMy entire point is that we're making lots of assumptions for io_uring\nuapi, and if it's moved to fuse because it knows better what it\nneeds, it should be a win.\n\nIOW, it sounds better if instead of passing the number of buffers to\nio_uring, you just ask it to create a large chunk of memory, and then\nfuse chunks it up and puts into the ring.\n\n---\n\nI agree, and you'd need something new in either case to add more\nmemory, and it doesn't need to be IORING_REGISTER_MEM_REGION\nspecifically.\n\n---\n\nUnregistering a buffer ring doesn't guarantee that there are no\ninflight requests that are still using buffers that came out of\nthe buffer ring. The fuse driver can wait/terminate its requests\nbefore unregisteration, but allow userspace issued IORING_OP_RECV\nto use this km buffer ring, and you'll need to somehow synchronise\nwith all other io_uring requests.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that a fix is needed for the selected buffer index in __io_uring_cmd_done() and promised to modify it in v2.\n\nAuthor confirmed that kernel-managed buffer rings are intended for use with other io-uring requests, specifically to avoid per-i/o page pinning overhead costs.\n\nAuthor Joanne Koong addressed Pavel Begunkov's feedback on the design of kernel-managed buffer rings, agreeing that having buffers owned by the ring and tied to its lifetime is a more generically useful concept. She proposed modifying the API to allow for dynamic allocation of memory regions and using the registered region's pages array to store associated pages. The author suggested repurposing struct io_uring_sqe fields to include an offset into the registered mem region, adding an IOSQE flag to indicate page lookup from the registered region, and sending the buffer id of the registered mem region through the 'IORING_CQE_F_BUFFER' mechanism.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for fix",
                "promised modification",
                "acknowledged the purpose of the feature",
                "confirmed its intended usage",
                "agreed with feedback",
                "proposed modifications"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sorry, I submitted v2 last night thinking the conversation on this\nthread had died. After reading through your reply, I'll modify v2.\n\n---\n\nYes the buffer rings are intended to be used with other io-uring\nrequests. The ideal scenario is that the user can then do the\nequivalent of IORING_OP_READ/WRITE_FIXED operations on the\nkernel-managed buffers and avoid the per-i/o page pinning overhead\ncosts.\n\n---\n\nI agree 100%. The api we add should be what's best for io-uring, not fuse.\n\nFor the majority of use cases, it seemed to me that having the buffers\nseparated from the buffer rings didn't yield perceptible benefits but\nadded complexity and more restrictions like having to statically know\nup front how big the mem region needs to be across the lifetime of the\nio-uring for anything the io-uring might use the mem region for. It\nseems more generically useful as a concept to have the buffers owned\nby the ring and tied to the lifetime of the ring. I like how with this\ndesign everything is self-contained and multiple subsystems can use it\nwithout having to reimplement functionality locally in the subsystem.\nOn the other hand, I see your point about how it might be something\nusers want in the future if they want complete control over which\nparts of the mem region get used as the backing buffers to do stuff\nlike PMD optimizations.\n\nI think this is a matter of opinion/preference and I think in general\nfor anything io-uring related, yours should take precedence.\n\nWith it going through a mem region, I don't think it should even go\nthrough the \"pbuf ring\" interface then if it's not going to specify\nthe number of entries and buffer sizes upfront, if support is added\nfor io-uring normal requests (eg IORING_OP_READ/WRITE) to use the\nbacking pages from a memory region and if we're able to guarantee that\nthe registered memory region will never be able to be unregistered by\nthe user. I think if we repurpose the\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n};\n\nfields in the struct io_uring_sqe to\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n  __u64 offset; /* offset into registered mem region */\n};\n\nand add some IOSQE_ flag to indicate it should find the pages from the\nregistered mem region, then that should work for normal requests.\nWhere on the kernel side, it looks up the associated pages stored in\nthe io_mapped_region's pages array for the offset passed in.\n\nRight now there's only a uapi to register a memory region and none to\nunregister one. Is it guaranteed that io-uring will never add\nsomething in the future that will let userspace unregister the memory\nregion or at least unregister it while it's being used (eg if we add\nfuture refcounting to it to track active uses of it)?\n\nIf so, then end-to-end, with it going through the mem region, it would\nbe something like:\n* user creates a mem region for the io-uring\n* user mmaps the mem region\n* user passes in offset into region, length of each buffer, and number\nof entries in the ring to the subsystem\n* subsystem creates a locally managed bufring and adds buffers to that\nring from the mem region\n* on the cqe side, it sends the buffer id of the registered mem region\nthrough the same \"IORING_CQE_F_BUFFER |  (buf_id <<\nIORING_CQE_BUFFER_SHIFT)\" mechanism\n\nDoes this design match what you had in mind / prefer?\n\nI think the above works for Christoph's use case too (as his and my\nuse case are the same) but if not, please let me know.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov questioned whether kernel-managed buffer rings (km rings) should be exposed as io_uring uapi, specifically asking if a server or user space program can issue I/O requests that consume buffers/entries from the km ring without fuse kernel code involved. He requested clarification on this point to inform the decision of exposing km rings in the uapi.\n\nReviewer Pavel Begunkov suggested reusing registered buffers instead of introducing a new mechanism for kernel-managed buffer rings, citing efficiency and similarity to zero-copy internally registered buffers as benefits.\n\nReviewer noted that kernel-managed buffer rings would hold page references or require pinning of regions, suggesting a different approach using registered buffers\n\nreviewer suggested adding a liburing helper for the fuse server to avoid dealing with mmap'ing\n\nreviewer expressed conditional approval, requesting confirmation that the patch allows for desired fast path optimizations",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "clarification needed",
                "alternative solution",
                "suggestion",
                "alternative",
                "conditional approval",
                "request for confirmation"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You mention OP_READ_FIXED and below agreed not exposing km rings\nan io_uring uapi, which makes me believe we're still talking about\ndifferent things.\n\nCorrect me if I'm wrong. Currently, only fuse cmds use the buffer\nring itself, I'm not talking about buffer, i.e. fuse cmds consume\nentries from the ring (!!! that's the part I'm interested in), then\nprocess them and tell the server \"this offset in the region has user\ndata to process or should be populated with data\".\n\nNaturally, the server should be able to use the buffers to issue\nsome I/O and process it in other ways, whether it's a normal\nOP_READ to which you pass the user space address (you can since\nit's mmap()'ed by the server) or something else is important but\na separate question than the one I'm trying to understand.\n\nSo I'm asking whether you expect that a server or other user space\nprogram should be able to issue a READ_OP_RECV, READ_OP_READ or any\nother similar request, which would consume buffers/entries from the\nkm ring without any fuse kernel code involved? Do you have some\nuse case for that in mind?\n\nUnderstanding that is the key in deciding whether km rings should\nbe exposed as io_uring uapi or not, regardless of where buffers\nto populate the ring come from.\n\n...\n\n---\n\nSo you already can do all that using the mmap()'ed region user\npointer, and you just want it to be more efficient, right?\nFor that let's just reuse registered buffers, we don't need a\nnew mechanism that needs to be propagated to all request types.\nAnd registered buffer are already optimised for I/O in a bunch\nof ways. And as a bonus, it'll be similar to the zero-copy\ninternally registered buffers if you still plan to add them.\n\nThe simplest way to do that is to create a registered buffer out\nof the mmap'ed region pointer. Pseudo code:\n\n// mmap'ed if it's kernel allocated.\n{region_ptr, region_size} = create_region();\n\nstruct iovec iov;\niov.iov_base = region_ptr;\niov.iov_len = region_size;\nio_uring_register_buffers(ring, &iov, 1);\n\n// later instead of this:\nptr = region_ptr + off;\nio_uring_prep_read(sqe, fd, ptr, ...);\n\n// you use registered buffers as usual:\nio_uring_prep_read_fixed(sqe, fd, off, regbuf_idx, ...);\n\n\nIIRC the registration would fail because it doesn't allow file\nbacked pages, but it should be fine if we know it's io_uring\nregion memory, so that would need to be patched.\n\nThere might be a bunch of other ways you can do that like\ncreate a kernel allocated registered buffer like what Cristoph\nwants, and then register it as a region. Or allow creating\nregistered buffers out of a region. etc.\n\nI wanted to unify registered buffers and regions internally\nat some point, but then drifted away from active io_uring core\ninfrastructure development, so I guess that could've been useful.\n\n---\n\nLet's talk about it when it's needed or something changes, but if\nyou do registered buffers instead as per above, they'll be holding\npage references and or have to pin the region in some other way.\n\n---\n\nFWIW, we should just add a liburing helper, so that fuse server\ndoesn't need to deal with mmap'ing.\n\n---\n\nThat's sounds clean to me _if_ it allows you to achieve all\n(fast path) optimisations you want to have. I hope it does?\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a question from Pavel Begunkov about whether the concept of kernel-managed buffer rings (kmbuf rings) is fuse-specific and whether it would be useful to optimize for READ_OP_RECV/READ_OP_READ operations directly on the ring. The author agrees that this optimization would be beneficial in certain scenarios, such as network-backed servers with high concurrency and unpredictable latencies, but notes that kmbuf rings are not exclusively fuse-specific and could be useful for other subsystems/users.\n\nAuthor expressed concern about added complexity and potential confusion in the design, specifically questioning the need for kernel-managed buffer rings when memory regions could be used instead.\n\nAuthor addressed Pavel's concern that the caller cannot guarantee the memory region will be registered as a fixed buffer, explaining that this would introduce extra overhead for every I/O operation and suggesting pinning to a registered memory region as an alternative.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a question",
                "provided additional context",
                "questioning",
                "expressed concern",
                "acknowledged a technical issue",
                "provided an explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for clarifying your question. Yes, this would be a useful\noptimization in the future for fuse servers with certain workload\ncharacteristics (eg network-backed servers with high concurrency and\nunpredictable latencies). I don't think the concept of kmbufrings is\nexclusively fuse-specific though (for example, Christoph's use case\nbeing a recent instance); I think other subsystems/users that'll use\nkmbuf rings would also generically find it useful to have the option\nof READ_OP_RECV/READ_OP_READ operating directly on the ring.\n\n---\n\nI feel like this design makes the interface more convoluted and now\nmuddies different concepts together by adding new complexity /\nrelationships between them whereas they were otherwise cleanly\nisolated. Maybe I'm just not seeing/understanding the overarching\nvision for why conceptually it makes sense for them to be tied\ntogether besides as a mechanism to tell io-uring requests where to\ncopy from by reusing what exists for fixed buffer ids. There's more\ncomplexity now on the kernel side (eg having to detect if the buffer\npassed in is kernel-allocated to know whether to pin the pages /\ncharge it against the user's RLIMIT_MEMLOCK limit) but I'm not\nunderstanding what we gain from it. I got the sense from your previous\ncomments that memory regions are the de facto way to go and should be\ndecoupled from other structures, so if that's the case, why doesn't it\nmake sense for io-uring to add native support for using memory regions\nfor io-uring requests? I feel like from the userspace side it makes\nthings more confusing with this extra layer of indirection that now\nhas to go through a fixed buffer.\n\n---\n\nI don't think we can guarantee that the caller will register the\nmemory region as a fixed buffer (eg if it doesn't need/want to use the\nbuffer for normal io-uring requests). On the kernel side, the internal\nbuffer entry uses the kaddr of the registered memory region buffer for\nany memcpys. If it's not guaranteed that registered memory regions\npersist for the lifetime of the ring, there'll have to be extra\noverhead for every I/O (eg grab the io-uring lock, checking if the mem\nregion is still registered, grab a refcount to that mem region, unlock\nthe ring, do the memcpy to the kaddr, then grab the io-uring lock\nagain, decrement the refcount, and unlock). Or I guess we could add\npinning to a registered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed confusion about the connection between kernel-managed buffer rings and a previous request from Christoph to check for mul overflow, use GFP_USER, and change the return type of PTR.\n\nReviewer Pavel Begunkov questioned the exposure of an internal kernel fuse API as an io_uring uapi, suggesting that it may have been discussed previously but was not clear from the patchset\n\nReviewer Pavel Begunkov suggested reusing an existing uapi for buffer management instead of introducing a new one, citing the importance of keeping the I/O path sane and avoiding adding a fourth way to pass buffers.\n\nPavel Begunkov noted that the current design uses regions instead of registered buffers, which he believes would be a better abstraction for copying client's data into user space; he was following the main I/O path and trying to make the setup path more flexible and reusable\n\nreviewer expressed skepticism about the value of introducing a new interface for passing buffers, citing existing alternatives and high bar for adoption\n\nReviewer Pavel Begunkov noted that the io_uring_cmd_done function should not set the selected buffer index because this is a user responsibility, as they can either use OP_READ/etc. with user addresses from mmap()ed regions or register and use OP_READ_FIXED.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "confusion",
                "lack of understanding",
                "requested clarification",
                "expressed uncertainty",
                "requested changes",
                "neutral comment",
                "request for clarification",
                "skepticism",
                "high bar"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sorry, I don't see relevance b/w km rings and what Christoph wants.\nI explained why in some sub-thread, but maybe someone can tell\nwhat I'm missing.\n\n---\n\nYep, it could be, potentially, it's just the patchset doesn't plumb\nit to other requests and uses it within fuse. It's just cases like\nthat always make me wonder, here it was why what is basically an\ninternal kernel fuse API is exposed as an io_uring uapi. Maybe there\nwas a discussion about it I missed?\n\n---\n\nThat would avoid doing a large revamp of uapi and plumbing it\nto each every request type when there is already a uapi that does\nwhat you want, does it well and have lots of things figured out.\nKeeping the I/O path sane is important, io_uring already has 3\ndifferent ways of passing buffers, let's not add a 4th one\nunless it achieves something meaningful.\n\n---\n\nSorry, maybe I wasn't clear. With what I see you're trying to do,\ni.e. copying client's data into user space (server), I think\nregistered buffers would be a better abstraction. However, I just\nwent with your design on top of regions, since it's not the first\niteration of the series and I wasn't following previous ones, and\nIIRC you was already using registered buffers in previous revisions\nbut moved from that for some reason. IOW, I was taking you main I/O\npath and was trying to make the setup path a bit more flexible and\nreusable.\n\n---\n\nThere is a high bar for adding a new interface for passing buffers\nthat needs to be propagated to a good number of request handlers,\nand there is already one that gives you all you need to write\nefficient user space.\n\n---\n\nIt's up to the user (i.e. fuse server) to either use OP_READ/etc. using\nuser addresses that you have in your design from mmap()ing regions, or\nregistering it and using OP_READ_FIXED.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] io_uring/rsrc: clean up buffer cloning arg validation (for 6.18-stable tree)",
          "message_id": "CAJnrk1YA9hk5Mv0BXFe+TcWLXsNLpWtcA-gy+k03zDt4f0z7zg@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1YA9hk5Mv0BXFe+TcWLXsNLpWtcA-gy+k03zDt4f0z7zg@mail.gmail.com/",
          "date": "2026-02-20T18:20:08Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-20",
          "patch_summary": "This patch cleans up the buffer cloning argument validation in io_uring/rsrc to fix a dependency issue for commit 5b804b8f1e0d in the 6.18-stable tree.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jens Axboe",
              "summary": "Approved the patch as it fixes a dependency issue for commit 5b804b8f1e0d in the 6.18-stable tree.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On 2/20/26 11:19 AM, Joanne Koong wrote:\n> Commit id upstream: b8201b50e403815f941d1c6581a27fdbfe7d0fd4\n> (\"io_uring/rsrc: clean up buffer cloning arg validation\")\n> Link to the patch:\n> https://lore.kernel.org/io-uring/20251204215116.2642044-1-joannelkoong@gmail.com/#t\n> Kernel version to apply it to: 6.18-stable tree\n> \n> Hi stable@,\n> \n> Chris Mason recently detected that this patch is a required dependency\n> for commit 5b804b8f1e0d (\"io_uring/rsrc: fix lost entries after cloned\n> range\") in the 6.18-stable tree [1]. Without this patch, the changes\n> in commit 5b804b8f1e0d use an incorrect value for nbufs when it\n> assigns \"i = nbufs\" [2].\n> \n> Could you please apply this patch to the 6.18-stable tree as a\n> dependency fix needed for commit 5b804b8f1e0d?\t\n> \n> Thanks,\n> Joanne\n> \n> [1] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=linux-6.18.y&id=5b804b8f1e0d66413774d43f7a4b78bba0ca6272\n> [2] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/rsrc.c?h=linux-6.18.y#n1252.\n\nFWIW, this is approved on my end. CC Greg.\n\n\n-- \nJens Axboe\n\n",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Greg Kroah-Hartman",
              "summary": "Queued up the patch for application in the 6.18-stable tree.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Sun, Feb 22, 2026 at 08:33:24AM -0700, Jens Axboe wrote:\n> On 2/20/26 11:19 AM, Joanne Koong wrote:\n> > Commit id upstream: b8201b50e403815f941d1c6581a27fdbfe7d0fd4\n> > (\"io_uring/rsrc: clean up buffer cloning arg validation\")\n> > Link to the patch:\n> > https://lore.kernel.org/io-uring/20251204215116.2642044-1-joannelkoong@gmail.com/#t\n> > Kernel version to apply it to: 6.18-stable tree\n> > \n> > Hi stable@,\n> > \n> > Chris Mason recently detected that this patch is a required dependency\n> > for commit 5b804b8f1e0d (\"io_uring/rsrc: fix lost entries after cloned\n> > range\") in the 6.18-stable tree [1]. Without this patch, the changes\n> > in commit 5b804b8f1e0d use an incorrect value for nbufs when it\n> > assigns \"i = nbufs\" [2].\n> > \n> > Could you please apply this patch to the 6.18-stable tree as a\n> > dependency fix needed for commit 5b804b8f1e0d?\t\n> > \n> > Thanks,\n> > Joanne\n> > \n> > [1] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=linux-6.18.y&id=5b804b8f1e0d66413774d43f7a4b78bba0ca6272\n> > [2] https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/rsrc.c?h=linux-6.18.y#n1252.\n> \n> FWIW, this is approved on my end. CC Greg.\n\nNow queued up, thanks.\n\ngreg k-h\n\n",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v1 03/11] io_uring/kbuf: add support for kernel-managed buffer rings",
          "message_id": "CAJnrk1a1FAARebZ0Aqw18zxtOy8WTMb2UfcAK6jQaigXiZbTfQ@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1a1FAARebZ0Aqw18zxtOy8WTMb2UfcAK6jQaigXiZbTfQ@mail.gmail.com/",
          "date": "2026-02-24T22:20:09Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patchset adds support for kernel-managed buffer rings, but the author's approach is questioned by Joanne Koong, who suggests leveraging existing io-uring infrastructure instead of re-implementing it.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joanne Koong",
              "summary": "Questioned the author's approach to implementing kernel-managed buffer rings, suggesting that existing io-uring infrastructure could be leveraged instead of re-implementing it. Raised concerns about potential issues with memory region unregistration.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/5] fuse: flush pending FUSE_RELEASE requests before sending FUSE_DESTROY",
          "message_id": "CAJnrk1YCh=CsFmxGwnK37d-31ravAOR8uLH+CrhpFzPX=ZTxUw@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1YCh=CsFmxGwnK37d-31ravAOR8uLH+CrhpFzPX=ZTxUw@mail.gmail.com/",
          "date": "2026-02-24T20:03:55Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": []
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 2/5] fuse: quiet down complaints in fuse_conn_limit_write",
          "message_id": "CAJnrk1bEm=pe2M367CsbQNYyUEdXCVzAyboqqHnSCxx7fxZKZA@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1bEm=pe2M367CsbQNYyUEdXCVzAyboqqHnSCxx7fxZKZA@mail.gmail.com/",
          "date": "2026-02-24T20:09:39Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 1/5] fuse: flush pending FUSE_RELEASE requests before sending FUSE_DESTROY",
          "message_id": "CAJnrk1ZZ=1jF4DUF-NyedLP-BJM_5d3s0zfD4oHGyR51PM9E7Q@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1ZZ=1jF4DUF-NyedLP-BJM_5d3s0zfD4oHGyR51PM9E7Q@mail.gmail.com/",
          "date": "2026-02-24T19:33:24Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": []
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Johannes Weiner",
      "primary_email": "hannes@cmpxchg.org",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH RFC 08/15] mm, swap: store and check memcg info in the swap table",
          "message_id": "aZ3KrfD_6vfxjRcs@cmpxchg.org",
          "url": "https://lore.kernel.org/all/aZ3KrfD_6vfxjRcs@cmpxchg.org/",
          "date": "2026-02-24T15:58:43Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch aims to store and check memcg info in the swap table, but one reviewer is questioning its necessity.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Johannes Weiner",
              "summary": "Raised questions about the necessity of storing memcg info in the swap table, suggesting that existing code already handles similar scenarios.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joshua Hahn",
      "primary_email": "joshua.hahnjy@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[RFC PATCH 0/6] mm/memcontrol: Make memcg limits tier-aware",
          "message_id": "20260223223830.586018-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260223223830.586018-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-24T00:19:15Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-24",
          "patch_summary": "This patch introduces a new sysfs entry to toggle between memory cgroup (memcg) limits that are proportional to the system's top-tier capacity ratio. The goal is to make memcg limits tier-aware, allowing for more efficient resource allocation and utilization. This is achieved by adding a boolean flag `tier_aware_memcg_limits` and two new sysfs attributes: `tier_aware_memcg_show` and `tier_aware_memcg_store`. When enabled, the memcg limits will be adjusted based on the system's top-tier capacity ratio, allowing for more efficient resource allocation. The patch is part of a larger series that aims to make memory management more efficient and scalable.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author addressed a concern about the performance impact of relying on per-memcg-lruvec statistics for limit checking, explaining that introducing a new cacheline in struct page_counter to track tiered memory limits and usage would reduce latency. The author confirmed that this approach is being taken instead of using lruvec stats.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "confirmed an alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On systems with tiered memory, there is currently no tracking of memory\nat the tier-memcg granularity. While per-memcg-lruvec serves at a finer\ngranularity that can be accumulated to give us the desired\nper-tier-memcg accounting, relying on these lruvec stats for limit\nchecking can prove touch too many hot paths too frequently and can\nintroduce increased latency for other memcg users.\n\nInstead, add a new cacheline in struct page_counter to track toptier\nmemcg limits and usage, as well as cached capacity values. This\ncacheline is only used by the mem_cgroup->memory page_counter.\n\nAlso, introduce helpers that use these new fields to calculate\nproportional toptier high and low values, based on the system's\ntoptier:total capacity ratio.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/page_counter.h | 22 +++++++++++++++++++++-\n mm/page_counter.c            | 34 ++++++++++++++++++++++++++++++++++\n 2 files changed, 55 insertions(+), 1 deletion(-)\n\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex d649b6bbbc87..128c1272c88c 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -5,6 +5,7 @@\n #include <linux/atomic.h>\n #include <linux/cache.h>\n #include <linux/limits.h>\n+#include <linux/nodemask.h>\n #include <asm/page.h>\n \n struct page_counter {\n@@ -31,9 +32,23 @@ struct page_counter {\n \t/* Latest cg2 reset watermark */\n \tunsigned long local_watermark;\n \n-\t/* Keep all the read most fields in a separete cacheline. */\n+\t/* Keep all the tiered memory fields in a separate cacheline. */\n \tCACHELINE_PADDING(_pad2_);\n \n+\tatomic_long_t toptier_usage;\n+\n+\t/* effective toptier-proportional low protection */\n+\tunsigned long etoptier_low;\n+\tatomic_long_t toptier_low_usage;\n+\tatomic_long_t children_toptier_low_usage;\n+\n+\t/* Cached toptier capacity for proportional limit calculations */\n+\tunsigned long toptier_capacity;\n+\tunsigned long total_capacity;\n+\n+\t/* Keep all the read most fields in a separate cacheline. */\n+\tCACHELINE_PADDING(_pad3_);\n+\n \tbool protection_support;\n \tbool track_failcnt;\n \tunsigned long min;\n@@ -61,6 +76,9 @@ static inline void page_counter_init(struct page_counter *counter,\n \tcounter->parent = parent;\n \tcounter->protection_support = protection_support;\n \tcounter->track_failcnt = false;\n+\tcounter->toptier_usage = (atomic_long_t)ATOMIC_LONG_INIT(0);\n+\tcounter->toptier_capacity = 0;\n+\tcounter->total_capacity = 0;\n }\n \n static inline unsigned long page_counter_read(struct page_counter *counter)\n@@ -103,6 +121,8 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n \t\t\t\t       bool recursive_protection);\n+unsigned long page_counter_toptier_high(struct page_counter *counter);\n+unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\n static inline void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t\t\t     struct page_counter *counter,\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex 661e0f2a5127..5ec97811c418 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -462,4 +462,38 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\tatomic_long_read(&parent->children_low_usage),\n \t\t\trecursive_protection));\n }\n+\n+unsigned long page_counter_toptier_high(struct page_counter *counter)\n+{\n+\tunsigned long high = READ_ONCE(counter->high);\n+\tunsigned long toptier_cap, total_cap;\n+\n+\tif (high == PAGE_COUNTER_MAX)\n+\t\treturn PAGE_COUNTER_MAX;\n+\n+\ttoptier_cap = counter->toptier_capacity;\n+\ttotal_cap = counter->total_capacity;\n+\n+\tif (!total_cap)\n+\t\treturn PAGE_COUNTER_MAX;\n+\n+\treturn mult_frac(high, toptier_cap, total_cap);\n+}\n+\n+unsigned long page_counter_toptier_low(struct page_counter *counter)\n+{\n+\tunsigned long low = READ_ONCE(counter->low);\n+\tunsigned long toptier_cap, total_cap;\n+\n+\tif (!low)\n+\t\treturn 0;\n+\n+\ttoptier_cap = counter->toptier_capacity;\n+\ttotal_cap = counter->total_capacity;\n+\n+\tif (!total_cap)\n+\t\treturn 0;\n+\n+\treturn mult_frac(low, toptier_cap, total_cap);\n+}\n #endif /* CONFIG_MEMCG || CONFIG_CGROUP_DMEM */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author addressed a concern about updating toptier statistics after charging or uncharging memory control groups (memcgs). They modified the `charge_memcg` function to update toptier fields only when charging succeeds, and added new functions `memcg_charge_toptier` and `memcg_uncharge_toptier` to handle this. The author also updated other functions like `uncharge_batch`, `uncharge_folio`, and `mem_cgroup_replace_folio` to account for toptier usage.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "modified code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Modify memcg charging and uncharging sites to also update toptier\nstatistics.\n\nUnfortunately, try_charge_memcg is unaware of the physical folio being\ncharged; it only deals with nr_pages. Instead of modifying\ntry_charge_memcg, instead adjust the toptier fields once\ntry_charge_memcg succeeds, inside charge_memcg.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n mm/memcontrol.c | 39 +++++++++++++++++++++++++++++++++++++++\n 1 file changed, 39 insertions(+)\n\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex f3e4a6ce7181..07464f02c529 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -1948,6 +1948,24 @@ static void memcg_uncharge(struct mem_cgroup *memcg, unsigned int nr_pages)\n \t\tpage_counter_uncharge(&memcg->memsw, nr_pages);\n }\n \n+static void memcg_charge_toptier(struct mem_cgroup *memcg,\n+\t\t\t\t unsigned long nr_pages)\n+{\n+\tstruct page_counter *c;\n+\n+\tfor (c = &memcg->memory; c; c = c->parent)\n+\t\tatomic_long_add(nr_pages, &c->toptier_usage);\n+}\n+\n+static void memcg_uncharge_toptier(struct mem_cgroup *memcg,\n+\t\t\t\t   unsigned long nr_pages)\n+{\n+\tstruct page_counter *c;\n+\n+\tfor (c = &memcg->memory; c; c = c->parent)\n+\t\tatomic_long_sub(nr_pages, &c->toptier_usage);\n+}\n+\n /*\n  * Returns stocks cached in percpu and reset cached information.\n  */\n@@ -4830,6 +4848,9 @@ static int charge_memcg(struct folio *folio, struct mem_cgroup *memcg,\n \tif (ret)\n \t\tgoto out;\n \n+\tif (node_is_toptier(folio_nid(folio)))\n+\t\tmemcg_charge_toptier(memcg, folio_nr_pages(folio));\n+\n \tcss_get(&memcg->css);\n \tcommit_charge(folio, memcg);\n \tmemcg1_commit_charge(folio, memcg);\n@@ -4921,6 +4942,7 @@ int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n struct uncharge_gather {\n \tstruct mem_cgroup *memcg;\n \tunsigned long nr_memory;\n+\tunsigned long nr_toptier;\n \tunsigned long pgpgout;\n \tunsigned long nr_kmem;\n \tint nid;\n@@ -4941,6 +4963,8 @@ static void uncharge_batch(const struct uncharge_gather *ug)\n \t\t}\n \t\tmemcg1_oom_recover(ug->memcg);\n \t}\n+\tif (ug->nr_toptier)\n+\t\tmemcg_uncharge_toptier(ug->memcg, ug->nr_toptier);\n \n \tmemcg1_uncharge_batch(ug->memcg, ug->pgpgout, ug->nr_memory, ug->nid);\n \n@@ -4989,6 +5013,9 @@ static void uncharge_folio(struct folio *folio, struct uncharge_gather *ug)\n \n \tnr_pages = folio_nr_pages(folio);\n \n+\tif (node_is_toptier(folio_nid(folio)))\n+\t\tug->nr_toptier += nr_pages;\n+\n \tif (folio_memcg_kmem(folio)) {\n \t\tug->nr_memory += nr_pages;\n \t\tug->nr_kmem += nr_pages;\n@@ -5072,6 +5099,10 @@ void mem_cgroup_replace_folio(struct folio *old, struct folio *new)\n \t\t\tpage_counter_charge(&memcg->memsw, nr_pages);\n \t}\n \n+\t/* The old folio's toptier_usage will be decremented when it is freed */\n+\tif (node_is_toptier(folio_nid(new)))\n+\t\tmemcg_charge_toptier(memcg, nr_pages);\n+\n \tcss_get(&memcg->css);\n \tcommit_charge(new, memcg);\n \tmemcg1_commit_charge(new, memcg);\n@@ -5091,6 +5122,7 @@ void mem_cgroup_replace_folio(struct folio *old, struct folio *new)\n void mem_cgroup_migrate(struct folio *old, struct folio *new)\n {\n \tstruct mem_cgroup *memcg;\n+\tint old_toptier, new_toptier;\n \n \tVM_BUG_ON_FOLIO(!folio_test_locked(old), old);\n \tVM_BUG_ON_FOLIO(!folio_test_locked(new), new);\n@@ -5111,6 +5143,13 @@ void mem_cgroup_migrate(struct folio *old, struct folio *new)\n \tif (!memcg)\n \t\treturn;\n \n+\told_toptier = node_is_toptier(folio_nid(old));\n+\tnew_toptier = node_is_toptier(folio_nid(new));\n+\tif (old_toptier && !new_toptier)\n+\t\tmemcg_uncharge_toptier(memcg, folio_nr_pages(old));\n+\telse if (!old_toptier && new_toptier)\n+\t\tmemcg_charge_toptier(memcg, folio_nr_pages(old));\n+\n \t/* Transfer the charge and the css ref */\n \tcommit_charge(new, memcg);\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "Author addressed a concern about the dynamic nature of toptier nodes and introduced functions to calculate and update toptier capacity during cpuset.mems changes and memory hotplug events.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a need for dynamic updates",
                "introduced new functions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "What a memcg considers to be a valid toptier node is defined by three\ncriteria: (1) The node has CPUs, (2) The node has online memory,\nand (3) The node is within the cgroup's cpuset.mems.\n\nOf the three, the second and third criteria are the only ones that can\nchange dynamically during runtime, via memory hotplug events and\ncpuset.mems changes, respectively.\n\nIntroduce functions to calculate and update toptier capacity, and call\nthem during cpuset.mems changes and memory hotplug events.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/memcontrol.h   |  6 ++++++\n include/linux/memory-tiers.h | 29 +++++++++++++++++++++++++\n include/linux/page_counter.h |  2 ++\n kernel/cgroup/cpuset.c       |  2 +-\n mm/memcontrol.c              | 17 +++++++++++++++\n mm/memory-tiers.c            | 41 ++++++++++++++++++++++++++++++++++++\n mm/page_counter.c            |  8 +++++++\n 7 files changed, 104 insertions(+), 1 deletion(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 5173a9f16721..900a36112b62 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -608,6 +608,8 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \t\t\t\t     struct mem_cgroup *memcg);\n \n+void update_memcg_toptier_capacity(void);\n+\n static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \t\t\t\t\t  struct mem_cgroup *memcg)\n {\n@@ -1116,6 +1118,10 @@ static inline void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n {\n }\n \n+static inline void update_memcg_toptier_capacity(void)\n+{\n+}\n+\n static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \t\t\t\t\t  struct mem_cgroup *memcg)\n {\ndiff --git a/include/linux/memory-tiers.h b/include/linux/memory-tiers.h\nindex 85440473effb..cf616885e0db 100644\n--- a/include/linux/memory-tiers.h\n+++ b/include/linux/memory-tiers.h\n@@ -53,6 +53,9 @@ int mt_perf_to_adistance(struct access_coordinate *perf, int *adist);\n struct memory_dev_type *mt_find_alloc_memory_type(int adist,\n \t\t\t\t\t\t  struct list_head *memory_types);\n void mt_put_memory_types(struct list_head *memory_types);\n+void mt_get_toptier_nodemask(nodemask_t *mask, const nodemask_t *allowed);\n+unsigned long mt_get_toptier_capacity(const nodemask_t *allowed);\n+unsigned long mt_get_total_capacity(const nodemask_t *allowed);\n #ifdef CONFIG_MIGRATION\n int next_demotion_node(int node, const nodemask_t *allowed_mask);\n void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets);\n@@ -152,5 +155,31 @@ static inline struct memory_dev_type *mt_find_alloc_memory_type(int adist,\n static inline void mt_put_memory_types(struct list_head *memory_types)\n {\n }\n+\n+static inline void mt_get_toptier_nodemask(nodemask_t *mask,\n+\t\t\t\t\t   const nodemask_t *allowed)\n+{\n+\t*mask = node_states[N_MEMORY];\n+\tif (allowed)\n+\t\tnodes_and(*mask, *mask, *allowed);\n+}\n+\n+static inline unsigned long mt_get_toptier_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (allowed && !node_isset(nid, *allowed))\n+\t\t\tcontinue;\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\t}\n+\treturn capacity;\n+}\n+\n+static inline unsigned long mt_get_total_capacity(const nodemask_t *allowed)\n+{\n+\treturn mt_get_toptier_capacity(allowed);\n+}\n #endif\t/* CONFIG_NUMA */\n #endif  /* _LINUX_MEMORY_TIERS_H */\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex 128c1272c88c..ada5f1dd75d4 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -121,6 +121,8 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n \t\t\t\t       bool recursive_protection);\n+void page_counter_update_toptier_capacity(struct page_counter *counter,\n+\t\t\t\t\t  const nodemask_t *allowed);\n unsigned long page_counter_toptier_high(struct page_counter *counter);\n unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 7607dfe516e6..e5641dc1af88 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -2620,7 +2620,6 @@ static void update_nodemasks_hier(struct cpuset *cs, nodemask_t *new_mems)\n \trcu_read_lock();\n \tcpuset_for_each_descendant_pre(cp, pos_css, cs) {\n \t\tstruct cpuset *parent = parent_cs(cp);\n-\n \t\tbool has_mems = nodes_and(*new_mems, cp->mems_allowed, parent->effective_mems);\n \n \t\t/*\n@@ -2701,6 +2700,7 @@ static int update_nodemask(struct cpuset *cs, struct cpuset *trialcs,\n \n \t/* use trialcs->mems_allowed as a temp variable */\n \tupdate_nodemasks_hier(cs, &trialcs->mems_allowed);\n+\tupdate_memcg_toptier_capacity();\n \treturn 0;\n }\n \ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 0be1e823d813..f3e4a6ce7181 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -54,6 +54,7 @@\n #include <linux/seq_file.h>\n #include <linux/vmpressure.h>\n #include <linux/memremap.h>\n+#include <linux/memory-tiers.h>\n #include <linux/mm_inline.h>\n #include <linux/swap_cgroup.h>\n #include <linux/cpu.h>\n@@ -3906,6 +3907,7 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \n \t\tpage_counter_init(&memcg->memory, &parent->memory, memcg_on_dfl);\n \t\tpage_counter_init(&memcg->swap, &parent->swap, false);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, NULL);\n #ifdef CONFIG_MEMCG_V1\n \t\tmemcg->memory.track_failcnt = !memcg_on_dfl;\n \t\tWRITE_ONCE(memcg->oom_kill_disable, READ_ONCE(parent->oom_kill_disable));\n@@ -3917,6 +3919,7 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \t\tinit_memcg_events();\n \t\tpage_counter_init(&memcg->memory, NULL, true);\n \t\tpage_counter_init(&memcg->swap, NULL, false);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, NULL);\n #ifdef CONFIG_MEMCG_V1\n \t\tpage_counter_init(&memcg->kmem, NULL, false);\n \t\tpage_counter_init(&memcg->tcpmem, NULL, false);\n@@ -4804,6 +4807,20 @@ void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \tpage_counter_calculate_protection(&root->memory, &memcg->memory, recursive_protection);\n }\n \n+void update_memcg_toptier_capacity(void)\n+{\n+\tstruct mem_cgroup *memcg;\n+\tnodemask_t allowed;\n+\n+\tfor_each_mem_cgroup(memcg) {\n+\t\tif (memcg == root_mem_cgroup)\n+\t\t\tcontinue;\n+\n+\t\tcpuset_nodes_allowed(memcg->css.cgroup, &allowed);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, &allowed);\n+\t}\n+}\n+\n static int charge_memcg(struct folio *folio, struct mem_cgroup *memcg,\n \t\t\tgfp_t gfp)\n {\ndiff --git a/mm/memory-tiers.c b/mm/memory-tiers.c\nindex a88256381519..259caaf4be8f 100644\n--- a/mm/memory-tiers.c\n+++ b/mm/memory-tiers.c\n@@ -889,6 +889,7 @@ static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\tmutex_lock(&memory_tier_lock);\n \t\tif (clear_node_memory_tier(nn->nid))\n \t\t\testablish_demotion_targets();\n+\t\tupdate_memcg_toptier_capacity();\n \t\tmutex_unlock(&memory_tier_lock);\n \t\tbreak;\n \tcase NODE_ADDED_FIRST_MEMORY:\n@@ -896,6 +897,7 @@ static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\tmemtier = set_node_memory_tier(nn->nid);\n \t\tif (!IS_ERR(memtier))\n \t\t\testablish_demotion_targets();\n+\t\tupdate_memcg_toptier_capacity();\n \t\tmutex_unlock(&memory_tier_lock);\n \t\tbreak;\n \t}\n@@ -941,6 +943,45 @@ bool numa_demotion_enabled = false;\n \n bool tier_aware_memcg_limits;\n \n+void mt_get_toptier_nodemask(nodemask_t *mask, const nodemask_t *allowed)\n+{\n+\tint nid;\n+\n+\t*mask = NODE_MASK_NONE;\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (node_is_toptier(nid))\n+\t\t\tnode_set(nid, *mask);\n+\t}\n+\tif (allowed)\n+\t\tnodes_and(*mask, *mask, *allowed);\n+}\n+\n+unsigned long mt_get_toptier_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\tnodemask_t mask;\n+\n+\tmt_get_toptier_nodemask(&mask, allowed);\n+\tfor_each_node_mask(nid, mask)\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\n+\treturn capacity;\n+}\n+\n+unsigned long mt_get_total_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (allowed && !node_isset(nid, *allowed))\n+\t\t\tcontinue;\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\t}\n+\treturn capacity;\n+}\n+\n #ifdef CONFIG_MIGRATION\n #ifdef CONFIG_SYSFS\n static ssize_t demotion_enabled_show(struct kobject *kobj,\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex 5ec97811c418..cf21c72bfd4e 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -11,6 +11,7 @@\n #include <linux/string.h>\n #include <linux/sched.h>\n #include <linux/bug.h>\n+#include <linux/memory-tiers.h>\n #include <asm/page.h>\n \n static bool track_protection(struct page_counter *c)\n@@ -463,6 +464,13 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\trecursive_protection));\n }\n \n+void page_counter_update_toptier_capacity(struct page_counter *counter,\n+\t\t\t\t\t  const nodemask_t *allowed)\n+{\n+\tcounter->toptier_capacity = mt_get_toptier_capacity(allowed);\n+\tcounter->total_capacity = mt_get_total_capacity(allowed);\n+}\n+\n unsigned long page_counter_toptier_high(struct page_counter *counter)\n {\n \tunsigned long high = READ_ONCE(counter->high);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author is addressing a concern about fairness in distributing toptier memory among workloads, which is currently impossible due to the lack of tier-aware memcg limits. The author explains that their patch extends the existing memory.low protection to be tier-aware and provides best-effort attempts at protecting a fair proportion of toptier memory. The enforcement of tier-aware memcg limits is gated behind a sysctl.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On machines serving multiple workloads whose memory is isolated via\nthe memory cgroup controller, it is currently impossible to enforce a\nfair distribution of toptier memory among the worloads, as the only\nenforcable limits have to do with total memory footprint, but not where\nthat memory resides.\n\nThis makes ensuring a consistent and baseline performance difficult, as\neach workload's performance is heavily impacted by workload-external\nfactors such as which other workloads are co-located in the same host,\nand the order at which different workloads are started.\n\nExtend the existing memory.low protection to be tier-aware in the\ncharging, enforcement, and protection calculation to provide\nbest-effort attempts at protecting a fair proportion of toptier memory.\n\nUpdates to protection and charging are performed in the same path as\nthe standard memcontrol equivalents. Enforcing tier-aware memcg limits\nhowever, are gated behind the sysctl tier_aware_memcg. This is so that\nruntime-enabling of tier aware limits can account for memory already\npresent in the system.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/memcontrol.h   | 15 +++++++++++----\n include/linux/page_counter.h |  7 ++++---\n kernel/cgroup/dmem.c         |  2 +-\n mm/memcontrol.c              | 14 ++++++++++++--\n mm/page_counter.c            | 35 ++++++++++++++++++++++++++++++++++-\n mm/vmscan.c                  | 13 +++++++++----\n 6 files changed, 71 insertions(+), 15 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 900a36112b62..a998a1e3b8b0 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -606,7 +606,9 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n }\n \n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t     struct mem_cgroup *memcg);\n+\t\t\t\t     struct mem_cgroup *memcg, bool toptier);\n+\n+unsigned long mem_cgroup_toptier_usage(struct mem_cgroup *memcg);\n \n void update_memcg_toptier_capacity(void);\n \n@@ -623,11 +625,15 @@ static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n }\n \n static inline bool mem_cgroup_below_low(struct mem_cgroup *target,\n-\t\t\t\t\tstruct mem_cgroup *memcg)\n+\t\t\t\t\tstruct mem_cgroup *memcg, bool toptier)\n {\n \tif (mem_cgroup_unprotected(target, memcg))\n \t\treturn false;\n \n+\tif (toptier)\n+\t\treturn READ_ONCE(memcg->memory.etoptier_low) >=\n+\t\t\t\t mem_cgroup_toptier_usage(memcg);\n+\n \treturn READ_ONCE(memcg->memory.elow) >=\n \t\tpage_counter_read(&memcg->memory);\n }\n@@ -1114,7 +1120,8 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n }\n \n static inline void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t\t\t   struct mem_cgroup *memcg)\n+\t\t\t\t\t\t   struct mem_cgroup *memcg,\n+\t\t\t\t\t\t   bool toptier)\n {\n }\n \n@@ -1128,7 +1135,7 @@ static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \treturn true;\n }\n static inline bool mem_cgroup_below_low(struct mem_cgroup *target,\n-\t\t\t\t\tstruct mem_cgroup *memcg)\n+\t\t\t\t\tstruct mem_cgroup *memcg, bool toptier)\n {\n \treturn false;\n }\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex ada5f1dd75d4..6635ee7b9575 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -120,15 +120,16 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n #if IS_ENABLED(CONFIG_MEMCG) || IS_ENABLED(CONFIG_CGROUP_DMEM)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n-\t\t\t\t       bool recursive_protection);\n+\t\t\t\t       bool recursive_protection, bool toptier);\n void page_counter_update_toptier_capacity(struct page_counter *counter,\n \t\t\t\t\t  const nodemask_t *allowed);\n unsigned long page_counter_toptier_high(struct page_counter *counter);\n unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\n static inline void page_counter_calculate_protection(struct page_counter *root,\n-\t\t\t\t\t\t     struct page_counter *counter,\n-\t\t\t\t\t\t     bool recursive_protection) {}\n+\t\t\t\t\t\tstruct page_counter *counter,\n+\t\t\t\t\t\tbool recursive_protection,\n+\t\t\t\t\t\tbool toptier) {}\n #endif\n \n #endif /* _LINUX_PAGE_COUNTER_H */\ndiff --git a/kernel/cgroup/dmem.c b/kernel/cgroup/dmem.c\nindex 1ea6afffa985..536d43c42de8 100644\n--- a/kernel/cgroup/dmem.c\n+++ b/kernel/cgroup/dmem.c\n@@ -277,7 +277,7 @@ dmem_cgroup_calculate_protection(struct dmem_cgroup_pool_state *limit_pool,\n \t\t\tcontinue;\n \n \t\tpage_counter_calculate_protection(\n-\t\t\tclimit, &found_pool->cnt, true);\n+\t\t\tclimit, &found_pool->cnt, true, false);\n \n \t\tif (found_pool == test_pool)\n \t\t\tbreak;\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 07464f02c529..8aa7ae361a73 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -4806,12 +4806,13 @@ struct cgroup_subsys memory_cgrp_subsys = {\n  * mem_cgroup_calculate_protection - check if memory consumption is in the normal range\n  * @root: the top ancestor of the sub-tree being checked\n  * @memcg: the memory cgroup to check\n+ * @toptier: whether the caller is in a toptier node\n  *\n  * WARNING: This function is not stateless! It can only be used as part\n  *          of a top-down tree iteration, not for isolated queries.\n  */\n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t     struct mem_cgroup *memcg)\n+\t\t\t\t     struct mem_cgroup *memcg, bool toptier)\n {\n \tbool recursive_protection =\n \t\tcgrp_dfl_root.flags & CGRP_ROOT_MEMORY_RECURSIVE_PROT;\n@@ -4822,7 +4823,16 @@ void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \tif (!root)\n \t\troot = root_mem_cgroup;\n \n-\tpage_counter_calculate_protection(&root->memory, &memcg->memory, recursive_protection);\n+\tpage_counter_calculate_protection(&root->memory, &memcg->memory,\n+\t\t\t\t\t  recursive_protection, toptier);\n+}\n+\n+unsigned long mem_cgroup_toptier_usage(struct mem_cgroup *memcg)\n+{\n+\tif (mem_cgroup_disabled() || !memcg)\n+\t\treturn 0;\n+\n+\treturn atomic_long_read(&memcg->memory.toptier_usage);\n }\n \n void update_memcg_toptier_capacity(void)\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex cf21c72bfd4e..79d46a1c4c0c 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -410,12 +410,39 @@ static unsigned long effective_protection(unsigned long usage,\n \treturn ep;\n }\n \n+static void calculate_protection_toptier(struct page_counter *counter,\n+\t\t\t\t\t bool recursive_protection)\n+{\n+\tstruct page_counter *parent = counter->parent;\n+\tunsigned long toptier_low;\n+\tunsigned long toptier_usage, parent_toptier_usage;\n+\tunsigned long toptier_protected, old_toptier_protected;\n+\tlong delta;\n+\n+\ttoptier_low = page_counter_toptier_low(counter);\n+\ttoptier_usage = atomic_long_read(&counter->toptier_usage);\n+\tparent_toptier_usage = atomic_long_read(&parent->toptier_usage);\n+\n+\t/* Propagate toptier low usage to parent for sibling distribution */\n+\ttoptier_protected = min(toptier_usage, toptier_low);\n+\told_toptier_protected = atomic_long_xchg(&counter->toptier_low_usage,\n+\t\t\t\t\t\t toptier_protected);\n+\tdelta = toptier_protected - old_toptier_protected;\n+\tatomic_long_add(delta, &parent->children_toptier_low_usage);\n+\n+\tWRITE_ONCE(counter->etoptier_low,\n+\t\t   effective_protection(toptier_usage, parent_toptier_usage,\n+\t\t   toptier_low, READ_ONCE(parent->etoptier_low),\n+\t\t   atomic_long_read(&parent->children_toptier_low_usage),\n+\t\t   recursive_protection));\n+}\n \n /**\n  * page_counter_calculate_protection - check if memory consumption is in the normal range\n  * @root: the top ancestor of the sub-tree being checked\n  * @counter: the page_counter the counter to update\n  * @recursive_protection: Whether to use memory_recursiveprot behavior.\n+ * @toptier: Whether to calculate toptier-proportional protection\n  *\n  * Calculates elow/emin thresholds for given page_counter.\n  *\n@@ -424,7 +451,7 @@ static unsigned long effective_protection(unsigned long usage,\n  */\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n-\t\t\t\t       bool recursive_protection)\n+\t\t\t\t       bool recursive_protection, bool toptier)\n {\n \tunsigned long usage, parent_usage;\n \tstruct page_counter *parent = counter->parent;\n@@ -446,6 +473,9 @@ void page_counter_calculate_protection(struct page_counter *root,\n \tif (parent == root) {\n \t\tcounter->emin = READ_ONCE(counter->min);\n \t\tcounter->elow = READ_ONCE(counter->low);\n+\t\tif (toptier)\n+\t\t\tWRITE_ONCE(counter->etoptier_low,\n+\t\t\t\t   page_counter_toptier_low(counter));\n \t\treturn;\n \t}\n \n@@ -462,6 +492,9 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\tREAD_ONCE(parent->elow),\n \t\t\tatomic_long_read(&parent->children_low_usage),\n \t\t\trecursive_protection));\n+\n+\tif (toptier)\n+\t\tcalculate_protection_toptier(counter, recursive_protection);\n }\n \n void page_counter_update_toptier_capacity(struct page_counter *counter,\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 6a87ac7be43c..5b4cb030a477 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -4144,6 +4144,7 @@ static void lru_gen_age_node(struct pglist_data *pgdat, struct scan_control *sc)\n \tstruct mem_cgroup *memcg;\n \tunsigned long min_ttl = READ_ONCE(lru_gen_min_ttl);\n \tbool reclaimable = !min_ttl;\n+\tbool toptier = node_is_toptier(pgdat->node_id);\n \n \tVM_WARN_ON_ONCE(!current_is_kswapd());\n \n@@ -4153,7 +4154,7 @@ static void lru_gen_age_node(struct pglist_data *pgdat, struct scan_control *sc)\n \tdo {\n \t\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);\n \n-\t\tmem_cgroup_calculate_protection(NULL, memcg);\n+\t\tmem_cgroup_calculate_protection(NULL, memcg, toptier);\n \n \t\tif (!reclaimable)\n \t\t\treclaimable = lruvec_is_reclaimable(lruvec, sc, min_ttl);\n@@ -4905,12 +4906,14 @@ static int shrink_one(struct lruvec *lruvec, struct scan_control *sc)\n \tunsigned long reclaimed = sc->nr_reclaimed;\n \tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n \tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n+\tbool toptier = tier_aware_memcg_limits &&\n+\t\t       node_is_toptier(pgdat->node_id);\n \n \t/* lru_gen_age_node() called mem_cgroup_calculate_protection() */\n \tif (mem_cgroup_below_min(NULL, memcg))\n \t\treturn MEMCG_LRU_YOUNG;\n \n-\tif (mem_cgroup_below_low(NULL, memcg)) {\n+\tif (mem_cgroup_below_low(NULL, memcg, toptier)) {\n \t\t/* see the comment on MEMCG_NR_GENS */\n \t\tif (READ_ONCE(lruvec->lrugen.seg) != MEMCG_LRU_TAIL)\n \t\t\treturn MEMCG_LRU_TAIL;\n@@ -5960,6 +5963,7 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t};\n \tstruct mem_cgroup_reclaim_cookie *partial = &reclaim;\n \tstruct mem_cgroup *memcg;\n+\tbool toptier = node_is_toptier(pgdat->node_id);\n \n \t/*\n \t * In most cases, direct reclaimers can do partial walks\n@@ -5987,7 +5991,7 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t\t */\n \t\tcond_resched();\n \n-\t\tmem_cgroup_calculate_protection(target_memcg, memcg);\n+\t\tmem_cgroup_calculate_protection(target_memcg, memcg, toptier);\n \n \t\tif (mem_cgroup_below_min(target_memcg, memcg)) {\n \t\t\t/*\n@@ -5995,7 +5999,8 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t\t\t * If there is no reclaimable memory, OOM.\n \t\t\t */\n \t\t\tcontinue;\n-\t\t} else if (mem_cgroup_below_low(target_memcg, memcg)) {\n+\t\t} else if (mem_cgroup_below_low(target_memcg, memcg,\n+\t\t\t\t\ttier_aware_memcg_limits && toptier)) {\n \t\t\t/*\n \t\t\t * Soft protection.\n \t\t\t * Respect the protection only as long as\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author addressed a concern about the interaction between memory cgroups and toptier usage limits by explaining how they plan to modify the existing high protection mechanism to be tier-aware, adding a new nodemask parameter to try_to_free_mem_cgroup_pages, and introducing a new function to calculate overage for toptier usage. The author confirmed that these changes will address the issue of workload-external factors impacting performance.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged concern",
                "planned fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On machines serving multiple workloads whose memory is isolated via the\nmemory cgroup controller, it is currently impossible to enforce a fair\ndistribution of toptier memory among the workloads, as the only\nenforcable limits have to do with total memory footprint, but not where\nthat memory resides.\n\nThis makes ensuring a consistent and baseline performance difficult, as\neach workload's performance is heavily impacted by workload-external\nfactors wuch as which other workloads are co-located in the same host,\nand the order at which different workloads are started.\n\nExtend the existing memory.high protection to be tier-aware in the\ncharging and enforcement to limit toptier-hogging for workloads.\n\nAlso, add a new nodemask parameter to try_to_free_mem_cgroup_pages,\nwhich can be used to selectively reclaim from memory at the\nmemcg-tier interection of a cgroup.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/swap.h |  3 +-\n mm/memcontrol-v1.c   |  6 ++--\n mm/memcontrol.c      | 85 +++++++++++++++++++++++++++++++++++++-------\n mm/vmscan.c          | 11 +++---\n 4 files changed, 84 insertions(+), 21 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 0effe3cc50f5..c6037ac7bf6e 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -368,7 +368,8 @@ extern unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t\t  unsigned long nr_pages,\n \t\t\t\t\t\t  gfp_t gfp_mask,\n \t\t\t\t\t\t  unsigned int reclaim_options,\n-\t\t\t\t\t\t  int *swappiness);\n+\t\t\t\t\t\t  int *swappiness,\n+\t\t\t\t\t\t  nodemask_t *allowed);\n extern unsigned long mem_cgroup_shrink_node(struct mem_cgroup *mem,\n \t\t\t\t\t\tgfp_t gfp_mask, bool noswap,\n \t\t\t\t\t\tpg_data_t *pgdat,\ndiff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.c\nindex 0b39ba608109..29630c7f3567 100644\n--- a/mm/memcontrol-v1.c\n+++ b/mm/memcontrol-v1.c\n@@ -1497,7 +1497,8 @@ static int mem_cgroup_resize_max(struct mem_cgroup *memcg,\n \t\t}\n \n \t\tif (!try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL,\n-\t\t\t\tmemsw ? 0 : MEMCG_RECLAIM_MAY_SWAP, NULL)) {\n+\t\t\t\tmemsw ? 0 : MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\tNULL, NULL)) {\n \t\t\tret = -EBUSY;\n \t\t\tbreak;\n \t\t}\n@@ -1529,7 +1530,8 @@ static int mem_cgroup_force_empty(struct mem_cgroup *memcg)\n \t\t\treturn -EINTR;\n \n \t\tif (!try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL,\n-\t\t\t\t\t\t  MEMCG_RECLAIM_MAY_SWAP, NULL))\n+\t\t\t\t\t\t  MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\t\t  NULL, NULL))\n \t\t\tnr_retries--;\n \t}\n \ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 8aa7ae361a73..ebd4a1b73c51 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -2184,18 +2184,30 @@ static unsigned long reclaim_high(struct mem_cgroup *memcg,\n \n \tdo {\n \t\tunsigned long pflags;\n-\n-\t\tif (page_counter_read(&memcg->memory) <=\n-\t\t    READ_ONCE(memcg->memory.high))\n+\t\tnodemask_t toptier_nodes, *reclaim_nodes;\n+\t\tbool mem_high_ok, toptier_high_ok;\n+\n+\t\tmt_get_toptier_nodemask(&toptier_nodes, NULL);\n+\t\tmem_high_ok = page_counter_read(&memcg->memory) <=\n+\t\t\t      READ_ONCE(memcg->memory.high);\n+\t\ttoptier_high_ok = !(tier_aware_memcg_limits &&\n+\t\t\t\t    mem_cgroup_toptier_usage(memcg) >\n+\t\t\t\t    page_counter_toptier_high(&memcg->memory));\n+\t\tif (mem_high_ok && toptier_high_ok)\n \t\t\tcontinue;\n \n+\t\tif (mem_high_ok && !toptier_high_ok)\n+\t\t\treclaim_nodes = &toptier_nodes;\n+\t\telse\n+\t\t\treclaim_nodes = NULL;\n+\n \t\tmemcg_memory_event(memcg, MEMCG_HIGH);\n \n \t\tpsi_memstall_enter(&pflags);\n \t\tnr_reclaimed += try_to_free_mem_cgroup_pages(memcg, nr_pages,\n \t\t\t\t\t\t\tgfp_mask,\n \t\t\t\t\t\t\tMEMCG_RECLAIM_MAY_SWAP,\n-\t\t\t\t\t\t\tNULL);\n+\t\t\t\t\t\t\tNULL, reclaim_nodes);\n \t\tpsi_memstall_leave(&pflags);\n \t} while ((memcg = parent_mem_cgroup(memcg)) &&\n \t\t !mem_cgroup_is_root(memcg));\n@@ -2296,6 +2308,24 @@ static u64 mem_find_max_overage(struct mem_cgroup *memcg)\n \treturn max_overage;\n }\n \n+static u64 toptier_find_max_overage(struct mem_cgroup *memcg)\n+{\n+\tu64 overage, max_overage = 0;\n+\n+\tif (!tier_aware_memcg_limits)\n+\t\treturn 0;\n+\n+\tdo {\n+\t\tunsigned long usage = mem_cgroup_toptier_usage(memcg);\n+\t\tunsigned long high = page_counter_toptier_high(&memcg->memory);\n+\n+\t\toverage = calculate_overage(usage, high);\n+\t\tmax_overage = max(overage, max_overage);\n+\t} while ((memcg = parent_mem_cgroup(memcg)) &&\n+\t\t  !mem_cgroup_is_root(memcg));\n+\n+\treturn max_overage;\n+}\n static u64 swap_find_max_overage(struct mem_cgroup *memcg)\n {\n \tu64 overage, max_overage = 0;\n@@ -2401,6 +2431,14 @@ void __mem_cgroup_handle_over_high(gfp_t gfp_mask)\n \tpenalty_jiffies += calculate_high_delay(memcg, nr_pages,\n \t\t\t\t\t\tswap_find_max_overage(memcg));\n \n+\t/*\n+\t * Don't double-penalize for toptier high overage if system-wide\n+\t * memory.high has already been breached.\n+\t */\n+\tif (!penalty_jiffies)\n+\t\tpenalty_jiffies += calculate_high_delay(memcg, nr_pages,\n+\t\t\t\t\ttoptier_find_max_overage(memcg));\n+\n \t/*\n \t * Clamp the max delay per usermode return so as to still keep the\n \t * application moving forwards and also permit diagnostics, albeit\n@@ -2503,7 +2541,8 @@ static int try_charge_memcg(struct mem_cgroup *memcg, gfp_t gfp_mask,\n \n \tpsi_memstall_enter(&pflags);\n \tnr_reclaimed = try_to_free_mem_cgroup_pages(mem_over_limit, nr_pages,\n-\t\t\t\t\t\t    gfp_mask, reclaim_options, NULL);\n+\t\t\t\t\t\t    gfp_mask, reclaim_options,\n+\t\t\t\t\t\t    NULL, NULL);\n \tpsi_memstall_leave(&pflags);\n \n \tif (mem_cgroup_margin(mem_over_limit) >= nr_pages)\n@@ -2592,23 +2631,26 @@ static int try_charge_memcg(struct mem_cgroup *memcg, gfp_t gfp_mask,\n \t * reclaim, the cost of mismatch is negligible.\n \t */\n \tdo {\n-\t\tbool mem_high, swap_high;\n+\t\tbool mem_high, swap_high, toptier_high = false;\n \n \t\tmem_high = page_counter_read(&memcg->memory) >\n \t\t\tREAD_ONCE(memcg->memory.high);\n \t\tswap_high = page_counter_read(&memcg->swap) >\n \t\t\tREAD_ONCE(memcg->swap.high);\n+\t\ttoptier_high = tier_aware_memcg_limits &&\n+\t\t\t       (mem_cgroup_toptier_usage(memcg) >\n+\t\t\t\tpage_counter_toptier_high(&memcg->memory));\n \n \t\t/* Don't bother a random interrupted task */\n \t\tif (!in_task()) {\n-\t\t\tif (mem_high) {\n+\t\t\tif (mem_high || toptier_high) {\n \t\t\t\tschedule_work(&memcg->high_work);\n \t\t\t\tbreak;\n \t\t\t}\n \t\t\tcontinue;\n \t\t}\n \n-\t\tif (mem_high || swap_high) {\n+\t\tif (mem_high || swap_high || toptier_high) {\n \t\t\t/*\n \t\t\t * The allocating tasks in this cgroup will need to do\n \t\t\t * reclaim or be throttled to prevent further growth\n@@ -4476,7 +4518,7 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n \tunsigned int nr_retries = MAX_RECLAIM_RETRIES;\n \tbool drained = false;\n-\tunsigned long high;\n+\tunsigned long high, toptier_high;\n \tint err;\n \n \tbuf = strstrip(buf);\n@@ -4485,15 +4527,22 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \t\treturn err;\n \n \tpage_counter_set_high(&memcg->memory, high);\n+\ttoptier_high = page_counter_toptier_high(&memcg->memory);\n \n \tif (of->file->f_flags & O_NONBLOCK)\n \t\tgoto out;\n \n \tfor (;;) {\n \t\tunsigned long nr_pages = page_counter_read(&memcg->memory);\n+\t\tunsigned long toptier_pages = mem_cgroup_toptier_usage(memcg);\n \t\tunsigned long reclaimed;\n+\t\tunsigned long to_free;\n+\t\tnodemask_t toptier_nodes, *reclaim_nodes;\n+\t\tbool mem_high_ok = nr_pages <= high;\n+\t\tbool toptier_high_ok = !(tier_aware_memcg_limits &&\n+\t\t\t\t\t toptier_pages > toptier_high);\n \n-\t\tif (nr_pages <= high)\n+\t\tif (mem_high_ok && toptier_high_ok)\n \t\t\tbreak;\n \n \t\tif (signal_pending(current))\n@@ -4505,8 +4554,17 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \t\t\tcontinue;\n \t\t}\n \n-\t\treclaimed = try_to_free_mem_cgroup_pages(memcg, nr_pages - high,\n-\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP, NULL);\n+\t\tmt_get_toptier_nodemask(&toptier_nodes, NULL);\n+\t\tif (mem_high_ok && !toptier_high_ok) {\n+\t\t\treclaim_nodes = &toptier_nodes;\n+\t\t\tto_free = toptier_pages - toptier_high;\n+\t\t} else {\n+\t\t\treclaim_nodes = NULL;\n+\t\t\tto_free = nr_pages - high;\n+\t\t}\n+\t\treclaimed = try_to_free_mem_cgroup_pages(memcg, to_free,\n+\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\tNULL, reclaim_nodes);\n \n \t\tif (!reclaimed && !nr_retries--)\n \t\t\tbreak;\n@@ -4558,7 +4616,8 @@ static ssize_t memory_max_write(struct kernfs_open_file *of,\n \n \t\tif (nr_reclaims) {\n \t\t\tif (!try_to_free_mem_cgroup_pages(memcg, nr_pages - max,\n-\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP, NULL))\n+\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\tNULL, NULL))\n \t\t\t\tnr_reclaims--;\n \t\t\tcontinue;\n \t\t}\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 5b4cb030a477..94498734b4f5 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -6652,7 +6652,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t   unsigned long nr_pages,\n \t\t\t\t\t   gfp_t gfp_mask,\n \t\t\t\t\t   unsigned int reclaim_options,\n-\t\t\t\t\t   int *swappiness)\n+\t\t\t\t\t   int *swappiness, nodemask_t *allowed)\n {\n \tunsigned long nr_reclaimed;\n \tunsigned int noreclaim_flag;\n@@ -6668,6 +6668,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t.may_unmap = 1,\n \t\t.may_swap = !!(reclaim_options & MEMCG_RECLAIM_MAY_SWAP),\n \t\t.proactive = !!(reclaim_options & MEMCG_RECLAIM_PROACTIVE),\n+\t\t.nodemask = allowed,\n \t};\n \t/*\n \t * Traverse the ZONELIST_FALLBACK zonelist of the current node to put\n@@ -6693,7 +6694,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t   unsigned long nr_pages,\n \t\t\t\t\t   gfp_t gfp_mask,\n \t\t\t\t\t   unsigned int reclaim_options,\n-\t\t\t\t\t   int *swappiness)\n+\t\t\t\t\t   int *swappiness, nodemask_t *allowed)\n {\n \treturn 0;\n }\n@@ -7806,9 +7807,9 @@ int user_proactive_reclaim(char *buf,\n \t\t\treclaim_options = MEMCG_RECLAIM_MAY_SWAP |\n \t\t\t\t\t  MEMCG_RECLAIM_PROACTIVE;\n \t\t\treclaimed = try_to_free_mem_cgroup_pages(memcg,\n-\t\t\t\t\t\t batch_size, gfp_mask,\n-\t\t\t\t\t\t reclaim_options,\n-\t\t\t\t\t\t swappiness == -1 ? NULL : &swappiness);\n+\t\t\t\t\tbatch_size, gfp_mask, reclaim_options,\n+\t\t\t\t\tswappiness == -1 ? NULL : &swappiness,\n+\t\t\t\t\tNULL);\n \t\t} else {\n \t\t\tstruct scan_control sc = {\n \t\t\t\t.gfp_mask = current_gfp_context(gfp_mask),\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author is addressing a concern that the patch does not account for systems where all memory is equal, and explaining how tier-aware memcg limits can provide better quality of service guarantees in systems with tiered memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "addressing_concern",
                "providing_explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Memory cgroups provide an interface that allow multiple workloads on a\nhost to co-exist, and establish both weak and strong memory isolation\nguarantees. For large servers and small embedded systems alike, memcgs\nprovide an effective way to provide a baseline quality of service for\nprotected workloads.\n\nThis works, because for the most part, all memory is equal (except for\nzram / zswap). Restricting a cgroup's memory footprint restricts how\nmuch it can hurt other workloads competing for memory. Likewise, setting\nmemory.low or memory.min limits can provide weak and strong guarantees\nto the performance of a cgroup.\n\nHowever, on systems with tiered memory (e.g. CXL / compressed memory),\nthe quality of service guarantees that memcg limits enforced become less\neffective, as memcg has no awareness of the physical location of its\ncharged memory. In other words, a workload that is well-behaved within\nits memcg limits may still be hurting the performance of other\nwell-behaving workloads on the system by hogging more than its\n\"fair share\" of toptier memory.\n\nIntroduce tier-aware memcg limits, which scale memory.low/high to\nreflect the ratio of toptier:total memory the cgroup has access.\n\nTake the following scenario as an example:\nOn a host with 3:1 toptier:lowtier, say 150G toptier, and 50Glowtier,\nsetting a cgroup's limits to:\n\tmemory.min:  15G\n\tmemory.low:  20G\n\tmemory.high: 40G\n\tmemory.max:  50G\n\nWill be enforced at the toptier as:\n\tmemory.min:          15G\n\tmemory.toptier_low:  15G (20 * 150/200)\n\tmemory.toptier_high: 30G (40 * 150/200)\n\tmemory.max:          50G\n\nLet's say that there are 4 such cgroups on the host. Previously, it would\nbe possible for 3 hosts to completely take over all of DRAM, while one\ncgroup could only access the lowtier memory. In the perspective of a\ntier-agnostic memcg limit enforcement, the three cgroups are all\nwell-behaved, consuming within their memory limits.\n\nThis is not to say that the scenario above is incorrect. In fact, for\nletting the hottest cgroups run in DRAM while pushing out colder cgroups\nto lowtier memory lets the system perform the most aggregate work total.\n\nBut for other scenarios, the target might not be maximizing aggregate\nwork, but maximizing the minimum performance guarantee for each\nindividual workload (think hosts shared across different users, such as\nVM hosting services).\n\nTo reflect these two scenarios, introduce a sysctl tier_aware_memcg,\nwhich allows the host to toggle between enforcing and overlooking\ntoptier memcg limit breaches.\n\nThis work is inspired & based off of Kaiyang Zhao's work from 2024 [1],\nwhere he referred to this concept as \"memory tiering fairness\".\nThe biggest difference in the implementations lie in how toptier memory\nis tracked; in his implementation, an lruvec stat aggregation is done on\neach usage check, while in this implementation, a new cacheline is\nintroduced in page_coutner to keep track of toptier usage (Kaiyang also\nintroduces a new cachline in page_counter, but only uses it to cache\ncapacity and thresholds). This implementation also extends the memory\nlimit enforcement to memory.high as well.\n\n[1] https://lore.kernel.org/linux-mm/20240920221202.1734227-1-kaiyang2@cs.cmu.edu/\n\n---\nJoshua Hahn (6):\n  mm/memory-tiers: Introduce tier-aware memcg limit sysfs\n  mm/page_counter: Introduce tiered memory awareness to page_counter\n  mm/memory-tiers, memcontrol: Introduce toptier capacity updates\n  mm/memcontrol: Charge and uncharge from toptier\n  mm/memcontrol, page_counter: Make memory.low tier-aware\n  mm/memcontrol: Make memory.high tier-aware\n\n include/linux/memcontrol.h   |  21 ++++-\n include/linux/memory-tiers.h |  30 +++++++\n include/linux/page_counter.h |  31 ++++++-\n include/linux/swap.h         |   3 +-\n kernel/cgroup/cpuset.c       |   2 +-\n kernel/cgroup/dmem.c         |   2 +-\n mm/memcontrol-v1.c           |   6 +-\n mm/memcontrol.c              | 155 +++++++++++++++++++++++++++++++----\n mm/memory-tiers.c            |  63 ++++++++++++++\n mm/page_counter.c            |  77 ++++++++++++++++-\n mm/vmscan.c                  |  24 ++++--\n 11 files changed, 376 insertions(+), 38 deletions(-)\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Michal Hocko",
              "summary": "Reviewer Michal Hocko questioned whether it's typical for active workingset sizes of all workloads to not fit into the top tier, suggesting that promotions would otherwise ensure most active memory is in the top tier.\n\nReviewer Michal Hocko expressed concerns that the current implementation focuses only on the top tier, potentially overlooking similar issues in other tiers, and questioned the need to duplicate limits for each/top tier.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "questioning typical behavior",
                "requesting clarification",
                "focusing only on top tier",
                "potential oversight of other tiers"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This assumes that the active workingset size of all workloads doesn't\nfit into the top tier right? Otherwise promotions would make sure to\nthat we have the most active memory in the top tier. Is this typical in\nreal life configurations?\n\nOr do you intend to limit memory consumption on particular tier even\nwithout an external pressure?\n\n---\n\nLet's spend some more time with the interface first. You seem to be\nfocusing only on the top tier with this interface, right? Is this really the\nright way to go long term? What makes you believe that we do not really\nhit the same issue with other tiers as well? Also do we want/need to\nduplicate all the limits for each/top tier? What is the reasoning for\nthe switch to be runtime sysctl rather than boot-time or cgroup mount\noption?\n\nI will likely have more questions but these are immediate ones after\nreading the cover. Please note I haven't really looked at the\nimplementation yet. I really want to understand usecases and interface\nfirst.\n-- \nMichal Hocko\nSUSE Labs",
              "reply_to": "Joshua Hahn",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "Author is addressing Michal's feedback by explaining their intention to discuss the project scope and scalability in a separate forum, LSFMMBPF, rather than revising the current patch series.\n\nAuthor acknowledged a concern about the impact of a workload violating its fair share of toptier memory, explaining that it mostly hurts other workloads when the aggregate working set size exceeds toptier memory capacity.\n\nAuthor acknowledged that traditional throughput-maximizing approach may not be suitable for all use cases and explained how tier-aware memcg limits can provide a more optimal solution for workloads prioritizing performance guarantees or low latency.\n\nAuthor agrees that the patch's tier-aware memcg limit feature is relevant to cloud providers and hyperscalers, providing two examples of realistic scenarios.\n\nThe author is addressing a concern about the interface for tier-aware memcg limits, specifically whether it should allow workloads to use more memory than their fair share in opportunistic mode. The author proposes two modes: fixed (limiting usage when a workload exceeds its fair share) and opportunistic (allowing workloads to use more memory but restricting them only when the top-tier is pressured). They ask for feedback on these options, suggesting that implementing opportunistic mode would require additional sysctl changes.\n\nAuthor acknowledges that the patch series was sent out prematurely and agrees it would have been better to send a proposal for LSFMMBPF first, indicating no immediate need for further revision.\n\nAuthor acknowledged that the current implementation only supports two-tiered systems and is open to revising it in the future when more complex systems become common.\n\nAuthor asked for clarification on whether reviewer's concern was about multiple nodes within a tier or multiple tiers within a tier, indicating uncertainty and need for further discussion.\n\nAuthor addressed Michal Hocko's concern that allowing cgroups to set their own mount options for tier-aware memcg limits could lead to inconsistent behavior and undermine the purpose of having a performance guarantee. Author agrees that this is not a good idea, but no specific fix or restructuring plan was mentioned.\n\nAuthor acknowledged that the feedback was positive and thanked the reviewer, indicating no further revision is needed.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "clarification",
                "separate discussion",
                "clarifying explanation",
                "no clear resolution signal",
                "acknowledged a different perspective",
                "explained reasoning",
                "agreement",
                "support",
                "asking for clarification",
                "proposing alternative solutions",
                "acknowledges feedback",
                "agrees with alternative approach",
                "open to revision",
                "acknowledged limitation",
                "uncertainty",
                "need_for_further_discussion",
                "acknowledged a problem",
                "agreed with feedback",
                "acknowledged",
                "thanked"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hello Michal,\n\nI hope that you are doing well! Thank you for taking the time to review my\nwork and leaving your thoughts.\n\nI wanted to note that I hope to bring this discussion to LSFMMBPF as well,\nto discuss what the scope of the project should be, what usecases there\nare (as I will note below), how to make this scalable and sustainable\nfor the future, etc. I'll send out a topic proposal later today. I had\nseparated the series from the proposal because I imagined that this\nseries would go through many versions, so it would be helpful to have\nthe topic as a unified place for pre-conference discussions.\n\n---\n\nYes, for the scenario above, a workload that is violating its fair share\nof toptier memory mostly hurts other workloads if the aggregate working\nset size of all workloads exceeds the size of toptier memory.\n\n---\n\nThis is true. And for a lot of usecases, this is 100% the right thing to do.\nHowever, with this patch I want to encourage a different perspective,\nwhich is to think about things in a per-workload perspective, and not a\nper-system perspective.\n\nHaving hot memory in high tiers and cold memory in low tiers is only\nlogical, since we increase the system's throughput and make the most\noptimal choices for latency. However, what about systems that care about\nobjectives other than simply maximizing throughput?\n\nIn the original cover letter I offered an example of VM hosting services\nthat care less about maximizing host-wide throughput, but more on ensuring\na bottomline performance guarantee for all workloads running on the system.\nFor the users on these services, they don't care that the host their VM is\nrunning on is maximizing throughput; rather, they care that their VM meets\nthe performance guarantees that their provider promised. If there is no\nway to know or enforce which tier of memory their workload lands on, either\nthe bottomline guarantee becomes very underestimated, or users must deal\nwith a high variance in performance.\n\nHere's another example: Let's say there is a host with multiple workloads,\neach serving queries for a database. The host would like to guarantee the\nlowest maximum latency possible, while maximizing the total throughput\nof the system. Once again in this situation, without tier-aware memcg\nlimits the host can maximize throughput, but can only make severely\nunderestimated promises on the bottom line.\n\n---\n\nI would say so. I think that the two examples above are realistic\nscenarios that cloud providers and hyperscalers might face on tiered systems.\n\n---\n\nThis is a great question, and one that I hope to discuss at LSFMMBPF\nto see how people expect an interface like this to work.\n\nOver the past few weeks, I have been discussing this idea during the\nLinux Memory Hotness and Promotion biweekly calls with Gregory Price [1].\nOne of the proposals that we made there (but did not include in this\nseries) is the idea of \"fixed\" vs. \"opportunistic\" reclaim.\n\nFixed mode is what we have here -- start limiting toptier usage whenever\na workload goes above its fair slice of toptier.\nOpportunistic mode would allow workloads to use more toptier memory than\nits fair share, but only be restricted when toptier is pressured.\n\nWhat do you think about these two options? For the stated goal of this\nseries, which is to help maximize the bottom line for workloads, fair\nshare seemed to make sense. Implementing opportunistic mode changes\non top of this work would most likely just be another sysctl.\n\n---\n\nThat sounds good with me, my goal was to bring this out as an RFC patchset\nso folks could look at the code and understand the motivation, and then send\nout the LSFMMBPF topic proposal. In retrospect I think I should have done\nit in the opposite order. I'm sorry if this caused any confusion.\n\n---\n\nYes, that's right. I'm not sure if this is the right way to go long-term\n(say, past the next 5 years). My thinking was that I can stick with doing\nthis for toptier vs. non-toptier memory for now, and deal with having\n3+ tiers in the future, when we start to have systems with that many tiers.\nAFAICT two-tiered systems are still ~relatively new, and I don't think\nthere are a lot of genuine usecases for enforcing mid-tier memory limits\nas of now. Of course, I would be excited to learn about these usecases\nand work this patchset to support them as well if anybody has them.\n\n---\n\nSorry, I'm not sure that I completely understood this question. Are you\nreferring to the case where we have multiple nodes in the toptier?\nIf so, then all of those nodes are treated the same, and don't have\nunique limits. Or are you referring to the case where we have multiple\ntiers in the toptier? If so, I hope the answer above can answer this too.\n\n---\n\nGood point : -) I don't think cgroup mount options are a good idea,\nsince this would mean that we can have a set of cgroups self-policing\ntheir toptier usage, while another cgroup allocates memory unrestricted.\nThis would punish the self-policing cgroup and we would lose the benefit\nof having a bottomline performance guarantee.\n\n---\n\nThat sounds good to me, thank you again for reviewing this work!\nI hope you have a great day : -)\nJoshua\n\n[1] https://lore.kernel.org/linux-mm/c8bc2dce-d4ec-c16e-8df4-2624c48cfc06@google.com/",
              "reply_to": "Michal Hocko",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price noted that the patch does not handle the case where tier-aware memcg limits are enabled, but the system has only one memory tier, which could lead to incorrect behavior and requested a fix for this edge case.\n\nReviewer Gregory Price expressed concern that the patch reduces the usefulness of secondary tiers of memory by introducing tier-aware memcg limits, which may lead to performance variance and discourage deployment on such machines.\n\nReviewer Gregory Price noted that tier-awareness is a significant blocker for deploying mixed workloads on large, dense memory systems with multiple tiers (2+), and suggested using the existing nobs (max/high/low/min) to proportionally control coherent memory tiers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "blocker",
                "significant"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Just injecting a few points here\n(disclosure: I have been in the development loop for this feature)\n\n---\n\nYes / No.  This makes the assumption that you always want this.\n\nBarring a minimum Quality of Service mechanism (as Joshua explains)\nthis reduces the usefulness of a secondary tier of memory.\n\nServices will just prefer not to be deployed to these kinds of\nmachines because the performance variance is too high.\n\n---\n\nThe answer is unequivocally yes.\n\nLacking tier-awareness is actually a huge blocker for deploying mixed\nworkloads on large, dense memory systems with multiple tiers (2+).\n\nTechnically we're already at 4-ish tiers: DDR, CXL, ZSWAP, SWAP.\n\nWe have zswap/swap controls in cgroups already, we just lack that same\ncontrol for coherent memory tiers.  This tries to use the existing nobs\n(max/high/low/min) to do what they already do - just proportionally.\n\n~Gregory",
              "reply_to": "Joshua Hahn",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kaiyang Zhao",
              "summary": "Reviewer Kaiyang Zhao noted that the results of a preprint paper on arXiv confirmed that co-colocated workloads can have working set sizes exceeding top-tier memory capacity, causing contention and significant variations in tail latency and throughput.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEUTRAL"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hello! I'm the author of the RFC in 2024. Just want to add that we've\nrecently released a preprint paper on arXiv that includes case studies\nwith a few of Meta's production workloads using a prototype version of\nthe patches.\n\nThe results confirmed that co-colocated workloads can have working set\nsizes exceeding the limited top-tier memory capacity given today's\nserver memory shapes and workload stacking settings, causing contention\nof top-tier memory. Workloads see significant variations in tail\nlatency and throughput depending on the share of top-tier tier memory\nthey get, which this patch set will alleviate.\n\nBest,\nKaiyang\n\n[1] https://arxiv.org/pdf/2602.08800",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH 0/6] mm/memcontrol: Make memcg limits tier-aware",
          "message_id": "20260224161357.2622501-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260224161357.2622501-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-24T18:06:59Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joshua Hahn (author)",
              "summary": "Author addressed a concern about the impact of relying on per-memcg-lruvec statistics for limit checking, explaining that it can introduce increased latency and proposing an alternative solution by adding new fields to struct page_counter to track tiered memory limits and usage.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On systems with tiered memory, there is currently no tracking of memory\nat the tier-memcg granularity. While per-memcg-lruvec serves at a finer\ngranularity that can be accumulated to give us the desired\nper-tier-memcg accounting, relying on these lruvec stats for limit\nchecking can prove touch too many hot paths too frequently and can\nintroduce increased latency for other memcg users.\n\nInstead, add a new cacheline in struct page_counter to track toptier\nmemcg limits and usage, as well as cached capacity values. This\ncacheline is only used by the mem_cgroup->memory page_counter.\n\nAlso, introduce helpers that use these new fields to calculate\nproportional toptier high and low values, based on the system's\ntoptier:total capacity ratio.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/page_counter.h | 22 +++++++++++++++++++++-\n mm/page_counter.c            | 34 ++++++++++++++++++++++++++++++++++\n 2 files changed, 55 insertions(+), 1 deletion(-)\n\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex d649b6bbbc87..128c1272c88c 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -5,6 +5,7 @@\n #include <linux/atomic.h>\n #include <linux/cache.h>\n #include <linux/limits.h>\n+#include <linux/nodemask.h>\n #include <asm/page.h>\n \n struct page_counter {\n@@ -31,9 +32,23 @@ struct page_counter {\n \t/* Latest cg2 reset watermark */\n \tunsigned long local_watermark;\n \n-\t/* Keep all the read most fields in a separete cacheline. */\n+\t/* Keep all the tiered memory fields in a separate cacheline. */\n \tCACHELINE_PADDING(_pad2_);\n \n+\tatomic_long_t toptier_usage;\n+\n+\t/* effective toptier-proportional low protection */\n+\tunsigned long etoptier_low;\n+\tatomic_long_t toptier_low_usage;\n+\tatomic_long_t children_toptier_low_usage;\n+\n+\t/* Cached toptier capacity for proportional limit calculations */\n+\tunsigned long toptier_capacity;\n+\tunsigned long total_capacity;\n+\n+\t/* Keep all the read most fields in a separate cacheline. */\n+\tCACHELINE_PADDING(_pad3_);\n+\n \tbool protection_support;\n \tbool track_failcnt;\n \tunsigned long min;\n@@ -61,6 +76,9 @@ static inline void page_counter_init(struct page_counter *counter,\n \tcounter->parent = parent;\n \tcounter->protection_support = protection_support;\n \tcounter->track_failcnt = false;\n+\tcounter->toptier_usage = (atomic_long_t)ATOMIC_LONG_INIT(0);\n+\tcounter->toptier_capacity = 0;\n+\tcounter->total_capacity = 0;\n }\n \n static inline unsigned long page_counter_read(struct page_counter *counter)\n@@ -103,6 +121,8 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n \t\t\t\t       bool recursive_protection);\n+unsigned long page_counter_toptier_high(struct page_counter *counter);\n+unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\n static inline void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t\t\t     struct page_counter *counter,\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex 661e0f2a5127..5ec97811c418 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -462,4 +462,38 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\tatomic_long_read(&parent->children_low_usage),\n \t\t\trecursive_protection));\n }\n+\n+unsigned long page_counter_toptier_high(struct page_counter *counter)\n+{\n+\tunsigned long high = READ_ONCE(counter->high);\n+\tunsigned long toptier_cap, total_cap;\n+\n+\tif (high == PAGE_COUNTER_MAX)\n+\t\treturn PAGE_COUNTER_MAX;\n+\n+\ttoptier_cap = counter->toptier_capacity;\n+\ttotal_cap = counter->total_capacity;\n+\n+\tif (!total_cap)\n+\t\treturn PAGE_COUNTER_MAX;\n+\n+\treturn mult_frac(high, toptier_cap, total_cap);\n+}\n+\n+unsigned long page_counter_toptier_low(struct page_counter *counter)\n+{\n+\tunsigned long low = READ_ONCE(counter->low);\n+\tunsigned long toptier_cap, total_cap;\n+\n+\tif (!low)\n+\t\treturn 0;\n+\n+\ttoptier_cap = counter->toptier_capacity;\n+\ttotal_cap = counter->total_capacity;\n+\n+\tif (!total_cap)\n+\t\treturn 0;\n+\n+\treturn mult_frac(low, toptier_cap, total_cap);\n+}\n #endif /* CONFIG_MEMCG || CONFIG_CGROUP_DMEM */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author addressed a concern about updating toptier statistics when charging or uncharging memory control groups (memcgs). They modified the `charge_memcg` function to update the toptier fields after try_charge_memcg succeeds, and also added new functions `memcg_charge_toptier` and `memcg_uncharge_toptier` to handle this. The author did not explicitly state that a fix is planned for v2, but the changes suggest an intention to address the issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "made changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Modify memcg charging and uncharging sites to also update toptier\nstatistics.\n\nUnfortunately, try_charge_memcg is unaware of the physical folio being\ncharged; it only deals with nr_pages. Instead of modifying\ntry_charge_memcg, instead adjust the toptier fields once\ntry_charge_memcg succeeds, inside charge_memcg.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n mm/memcontrol.c | 39 +++++++++++++++++++++++++++++++++++++++\n 1 file changed, 39 insertions(+)\n\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex f3e4a6ce7181..07464f02c529 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -1948,6 +1948,24 @@ static void memcg_uncharge(struct mem_cgroup *memcg, unsigned int nr_pages)\n \t\tpage_counter_uncharge(&memcg->memsw, nr_pages);\n }\n \n+static void memcg_charge_toptier(struct mem_cgroup *memcg,\n+\t\t\t\t unsigned long nr_pages)\n+{\n+\tstruct page_counter *c;\n+\n+\tfor (c = &memcg->memory; c; c = c->parent)\n+\t\tatomic_long_add(nr_pages, &c->toptier_usage);\n+}\n+\n+static void memcg_uncharge_toptier(struct mem_cgroup *memcg,\n+\t\t\t\t   unsigned long nr_pages)\n+{\n+\tstruct page_counter *c;\n+\n+\tfor (c = &memcg->memory; c; c = c->parent)\n+\t\tatomic_long_sub(nr_pages, &c->toptier_usage);\n+}\n+\n /*\n  * Returns stocks cached in percpu and reset cached information.\n  */\n@@ -4830,6 +4848,9 @@ static int charge_memcg(struct folio *folio, struct mem_cgroup *memcg,\n \tif (ret)\n \t\tgoto out;\n \n+\tif (node_is_toptier(folio_nid(folio)))\n+\t\tmemcg_charge_toptier(memcg, folio_nr_pages(folio));\n+\n \tcss_get(&memcg->css);\n \tcommit_charge(folio, memcg);\n \tmemcg1_commit_charge(folio, memcg);\n@@ -4921,6 +4942,7 @@ int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n struct uncharge_gather {\n \tstruct mem_cgroup *memcg;\n \tunsigned long nr_memory;\n+\tunsigned long nr_toptier;\n \tunsigned long pgpgout;\n \tunsigned long nr_kmem;\n \tint nid;\n@@ -4941,6 +4963,8 @@ static void uncharge_batch(const struct uncharge_gather *ug)\n \t\t}\n \t\tmemcg1_oom_recover(ug->memcg);\n \t}\n+\tif (ug->nr_toptier)\n+\t\tmemcg_uncharge_toptier(ug->memcg, ug->nr_toptier);\n \n \tmemcg1_uncharge_batch(ug->memcg, ug->pgpgout, ug->nr_memory, ug->nid);\n \n@@ -4989,6 +5013,9 @@ static void uncharge_folio(struct folio *folio, struct uncharge_gather *ug)\n \n \tnr_pages = folio_nr_pages(folio);\n \n+\tif (node_is_toptier(folio_nid(folio)))\n+\t\tug->nr_toptier += nr_pages;\n+\n \tif (folio_memcg_kmem(folio)) {\n \t\tug->nr_memory += nr_pages;\n \t\tug->nr_kmem += nr_pages;\n@@ -5072,6 +5099,10 @@ void mem_cgroup_replace_folio(struct folio *old, struct folio *new)\n \t\t\tpage_counter_charge(&memcg->memsw, nr_pages);\n \t}\n \n+\t/* The old folio's toptier_usage will be decremented when it is freed */\n+\tif (node_is_toptier(folio_nid(new)))\n+\t\tmemcg_charge_toptier(memcg, nr_pages);\n+\n \tcss_get(&memcg->css);\n \tcommit_charge(new, memcg);\n \tmemcg1_commit_charge(new, memcg);\n@@ -5091,6 +5122,7 @@ void mem_cgroup_replace_folio(struct folio *old, struct folio *new)\n void mem_cgroup_migrate(struct folio *old, struct folio *new)\n {\n \tstruct mem_cgroup *memcg;\n+\tint old_toptier, new_toptier;\n \n \tVM_BUG_ON_FOLIO(!folio_test_locked(old), old);\n \tVM_BUG_ON_FOLIO(!folio_test_locked(new), new);\n@@ -5111,6 +5143,13 @@ void mem_cgroup_migrate(struct folio *old, struct folio *new)\n \tif (!memcg)\n \t\treturn;\n \n+\told_toptier = node_is_toptier(folio_nid(old));\n+\tnew_toptier = node_is_toptier(folio_nid(new));\n+\tif (old_toptier && !new_toptier)\n+\t\tmemcg_uncharge_toptier(memcg, folio_nr_pages(old));\n+\telse if (!old_toptier && new_toptier)\n+\t\tmemcg_charge_toptier(memcg, folio_nr_pages(old));\n+\n \t/* Transfer the charge and the css ref */\n \tcommit_charge(new, memcg);\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author addressed a concern about the dynamic nature of toptier nodes and how they are defined by three criteria: CPUs, online memory, and cpuset.mems. The author explained that only two of these criteria can change dynamically during runtime (online memory and cpuset.mems), and introduced functions to calculate and update toptier capacity accordingly.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a need for dynamic updates",
                "explained reasoning behind implementation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "What a memcg considers to be a valid toptier node is defined by three\ncriteria: (1) The node has CPUs, (2) The node has online memory,\nand (3) The node is within the cgroup's cpuset.mems.\n\nOf the three, the second and third criteria are the only ones that can\nchange dynamically during runtime, via memory hotplug events and\ncpuset.mems changes, respectively.\n\nIntroduce functions to calculate and update toptier capacity, and call\nthem during cpuset.mems changes and memory hotplug events.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/memcontrol.h   |  6 ++++++\n include/linux/memory-tiers.h | 29 +++++++++++++++++++++++++\n include/linux/page_counter.h |  2 ++\n kernel/cgroup/cpuset.c       |  2 +-\n mm/memcontrol.c              | 17 +++++++++++++++\n mm/memory-tiers.c            | 41 ++++++++++++++++++++++++++++++++++++\n mm/page_counter.c            |  8 +++++++\n 7 files changed, 104 insertions(+), 1 deletion(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 5173a9f16721..900a36112b62 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -608,6 +608,8 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \t\t\t\t     struct mem_cgroup *memcg);\n \n+void update_memcg_toptier_capacity(void);\n+\n static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \t\t\t\t\t  struct mem_cgroup *memcg)\n {\n@@ -1116,6 +1118,10 @@ static inline void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n {\n }\n \n+static inline void update_memcg_toptier_capacity(void)\n+{\n+}\n+\n static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \t\t\t\t\t  struct mem_cgroup *memcg)\n {\ndiff --git a/include/linux/memory-tiers.h b/include/linux/memory-tiers.h\nindex 85440473effb..cf616885e0db 100644\n--- a/include/linux/memory-tiers.h\n+++ b/include/linux/memory-tiers.h\n@@ -53,6 +53,9 @@ int mt_perf_to_adistance(struct access_coordinate *perf, int *adist);\n struct memory_dev_type *mt_find_alloc_memory_type(int adist,\n \t\t\t\t\t\t  struct list_head *memory_types);\n void mt_put_memory_types(struct list_head *memory_types);\n+void mt_get_toptier_nodemask(nodemask_t *mask, const nodemask_t *allowed);\n+unsigned long mt_get_toptier_capacity(const nodemask_t *allowed);\n+unsigned long mt_get_total_capacity(const nodemask_t *allowed);\n #ifdef CONFIG_MIGRATION\n int next_demotion_node(int node, const nodemask_t *allowed_mask);\n void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets);\n@@ -152,5 +155,31 @@ static inline struct memory_dev_type *mt_find_alloc_memory_type(int adist,\n static inline void mt_put_memory_types(struct list_head *memory_types)\n {\n }\n+\n+static inline void mt_get_toptier_nodemask(nodemask_t *mask,\n+\t\t\t\t\t   const nodemask_t *allowed)\n+{\n+\t*mask = node_states[N_MEMORY];\n+\tif (allowed)\n+\t\tnodes_and(*mask, *mask, *allowed);\n+}\n+\n+static inline unsigned long mt_get_toptier_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (allowed && !node_isset(nid, *allowed))\n+\t\t\tcontinue;\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\t}\n+\treturn capacity;\n+}\n+\n+static inline unsigned long mt_get_total_capacity(const nodemask_t *allowed)\n+{\n+\treturn mt_get_toptier_capacity(allowed);\n+}\n #endif\t/* CONFIG_NUMA */\n #endif  /* _LINUX_MEMORY_TIERS_H */\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex 128c1272c88c..ada5f1dd75d4 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -121,6 +121,8 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n \t\t\t\t       bool recursive_protection);\n+void page_counter_update_toptier_capacity(struct page_counter *counter,\n+\t\t\t\t\t  const nodemask_t *allowed);\n unsigned long page_counter_toptier_high(struct page_counter *counter);\n unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 7607dfe516e6..e5641dc1af88 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -2620,7 +2620,6 @@ static void update_nodemasks_hier(struct cpuset *cs, nodemask_t *new_mems)\n \trcu_read_lock();\n \tcpuset_for_each_descendant_pre(cp, pos_css, cs) {\n \t\tstruct cpuset *parent = parent_cs(cp);\n-\n \t\tbool has_mems = nodes_and(*new_mems, cp->mems_allowed, parent->effective_mems);\n \n \t\t/*\n@@ -2701,6 +2700,7 @@ static int update_nodemask(struct cpuset *cs, struct cpuset *trialcs,\n \n \t/* use trialcs->mems_allowed as a temp variable */\n \tupdate_nodemasks_hier(cs, &trialcs->mems_allowed);\n+\tupdate_memcg_toptier_capacity();\n \treturn 0;\n }\n \ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 0be1e823d813..f3e4a6ce7181 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -54,6 +54,7 @@\n #include <linux/seq_file.h>\n #include <linux/vmpressure.h>\n #include <linux/memremap.h>\n+#include <linux/memory-tiers.h>\n #include <linux/mm_inline.h>\n #include <linux/swap_cgroup.h>\n #include <linux/cpu.h>\n@@ -3906,6 +3907,7 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \n \t\tpage_counter_init(&memcg->memory, &parent->memory, memcg_on_dfl);\n \t\tpage_counter_init(&memcg->swap, &parent->swap, false);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, NULL);\n #ifdef CONFIG_MEMCG_V1\n \t\tmemcg->memory.track_failcnt = !memcg_on_dfl;\n \t\tWRITE_ONCE(memcg->oom_kill_disable, READ_ONCE(parent->oom_kill_disable));\n@@ -3917,6 +3919,7 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \t\tinit_memcg_events();\n \t\tpage_counter_init(&memcg->memory, NULL, true);\n \t\tpage_counter_init(&memcg->swap, NULL, false);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, NULL);\n #ifdef CONFIG_MEMCG_V1\n \t\tpage_counter_init(&memcg->kmem, NULL, false);\n \t\tpage_counter_init(&memcg->tcpmem, NULL, false);\n@@ -4804,6 +4807,20 @@ void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \tpage_counter_calculate_protection(&root->memory, &memcg->memory, recursive_protection);\n }\n \n+void update_memcg_toptier_capacity(void)\n+{\n+\tstruct mem_cgroup *memcg;\n+\tnodemask_t allowed;\n+\n+\tfor_each_mem_cgroup(memcg) {\n+\t\tif (memcg == root_mem_cgroup)\n+\t\t\tcontinue;\n+\n+\t\tcpuset_nodes_allowed(memcg->css.cgroup, &allowed);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, &allowed);\n+\t}\n+}\n+\n static int charge_memcg(struct folio *folio, struct mem_cgroup *memcg,\n \t\t\tgfp_t gfp)\n {\ndiff --git a/mm/memory-tiers.c b/mm/memory-tiers.c\nindex a88256381519..259caaf4be8f 100644\n--- a/mm/memory-tiers.c\n+++ b/mm/memory-tiers.c\n@@ -889,6 +889,7 @@ static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\tmutex_lock(&memory_tier_lock);\n \t\tif (clear_node_memory_tier(nn->nid))\n \t\t\testablish_demotion_targets();\n+\t\tupdate_memcg_toptier_capacity();\n \t\tmutex_unlock(&memory_tier_lock);\n \t\tbreak;\n \tcase NODE_ADDED_FIRST_MEMORY:\n@@ -896,6 +897,7 @@ static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\tmemtier = set_node_memory_tier(nn->nid);\n \t\tif (!IS_ERR(memtier))\n \t\t\testablish_demotion_targets();\n+\t\tupdate_memcg_toptier_capacity();\n \t\tmutex_unlock(&memory_tier_lock);\n \t\tbreak;\n \t}\n@@ -941,6 +943,45 @@ bool numa_demotion_enabled = false;\n \n bool tier_aware_memcg_limits;\n \n+void mt_get_toptier_nodemask(nodemask_t *mask, const nodemask_t *allowed)\n+{\n+\tint nid;\n+\n+\t*mask = NODE_MASK_NONE;\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (node_is_toptier(nid))\n+\t\t\tnode_set(nid, *mask);\n+\t}\n+\tif (allowed)\n+\t\tnodes_and(*mask, *mask, *allowed);\n+}\n+\n+unsigned long mt_get_toptier_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\tnodemask_t mask;\n+\n+\tmt_get_toptier_nodemask(&mask, allowed);\n+\tfor_each_node_mask(nid, mask)\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\n+\treturn capacity;\n+}\n+\n+unsigned long mt_get_total_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (allowed && !node_isset(nid, *allowed))\n+\t\t\tcontinue;\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\t}\n+\treturn capacity;\n+}\n+\n #ifdef CONFIG_MIGRATION\n #ifdef CONFIG_SYSFS\n static ssize_t demotion_enabled_show(struct kobject *kobj,\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex 5ec97811c418..cf21c72bfd4e 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -11,6 +11,7 @@\n #include <linux/string.h>\n #include <linux/sched.h>\n #include <linux/bug.h>\n+#include <linux/memory-tiers.h>\n #include <asm/page.h>\n \n static bool track_protection(struct page_counter *c)\n@@ -463,6 +464,13 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\trecursive_protection));\n }\n \n+void page_counter_update_toptier_capacity(struct page_counter *counter,\n+\t\t\t\t\t  const nodemask_t *allowed)\n+{\n+\tcounter->toptier_capacity = mt_get_toptier_capacity(allowed);\n+\tcounter->total_capacity = mt_get_total_capacity(allowed);\n+}\n+\n unsigned long page_counter_toptier_high(struct page_counter *counter)\n {\n \tunsigned long high = READ_ONCE(counter->high);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author addressed a concern about fairness in memory distribution among workloads, explaining that current limits are based on total memory footprint rather than where the memory resides. They updated the existing memory.low protection to be tier-aware in charging, enforcement, and protection calculation, providing best-effort attempts at protecting a fair proportion of toptier memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On machines serving multiple workloads whose memory is isolated via\nthe memory cgroup controller, it is currently impossible to enforce a\nfair distribution of toptier memory among the worloads, as the only\nenforcable limits have to do with total memory footprint, but not where\nthat memory resides.\n\nThis makes ensuring a consistent and baseline performance difficult, as\neach workload's performance is heavily impacted by workload-external\nfactors such as which other workloads are co-located in the same host,\nand the order at which different workloads are started.\n\nExtend the existing memory.low protection to be tier-aware in the\ncharging, enforcement, and protection calculation to provide\nbest-effort attempts at protecting a fair proportion of toptier memory.\n\nUpdates to protection and charging are performed in the same path as\nthe standard memcontrol equivalents. Enforcing tier-aware memcg limits\nhowever, are gated behind the sysctl tier_aware_memcg. This is so that\nruntime-enabling of tier aware limits can account for memory already\npresent in the system.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/memcontrol.h   | 15 +++++++++++----\n include/linux/page_counter.h |  7 ++++---\n kernel/cgroup/dmem.c         |  2 +-\n mm/memcontrol.c              | 14 ++++++++++++--\n mm/page_counter.c            | 35 ++++++++++++++++++++++++++++++++++-\n mm/vmscan.c                  | 13 +++++++++----\n 6 files changed, 71 insertions(+), 15 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 900a36112b62..a998a1e3b8b0 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -606,7 +606,9 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n }\n \n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t     struct mem_cgroup *memcg);\n+\t\t\t\t     struct mem_cgroup *memcg, bool toptier);\n+\n+unsigned long mem_cgroup_toptier_usage(struct mem_cgroup *memcg);\n \n void update_memcg_toptier_capacity(void);\n \n@@ -623,11 +625,15 @@ static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n }\n \n static inline bool mem_cgroup_below_low(struct mem_cgroup *target,\n-\t\t\t\t\tstruct mem_cgroup *memcg)\n+\t\t\t\t\tstruct mem_cgroup *memcg, bool toptier)\n {\n \tif (mem_cgroup_unprotected(target, memcg))\n \t\treturn false;\n \n+\tif (toptier)\n+\t\treturn READ_ONCE(memcg->memory.etoptier_low) >=\n+\t\t\t\t mem_cgroup_toptier_usage(memcg);\n+\n \treturn READ_ONCE(memcg->memory.elow) >=\n \t\tpage_counter_read(&memcg->memory);\n }\n@@ -1114,7 +1120,8 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n }\n \n static inline void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t\t\t   struct mem_cgroup *memcg)\n+\t\t\t\t\t\t   struct mem_cgroup *memcg,\n+\t\t\t\t\t\t   bool toptier)\n {\n }\n \n@@ -1128,7 +1135,7 @@ static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \treturn true;\n }\n static inline bool mem_cgroup_below_low(struct mem_cgroup *target,\n-\t\t\t\t\tstruct mem_cgroup *memcg)\n+\t\t\t\t\tstruct mem_cgroup *memcg, bool toptier)\n {\n \treturn false;\n }\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex ada5f1dd75d4..6635ee7b9575 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -120,15 +120,16 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n #if IS_ENABLED(CONFIG_MEMCG) || IS_ENABLED(CONFIG_CGROUP_DMEM)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n-\t\t\t\t       bool recursive_protection);\n+\t\t\t\t       bool recursive_protection, bool toptier);\n void page_counter_update_toptier_capacity(struct page_counter *counter,\n \t\t\t\t\t  const nodemask_t *allowed);\n unsigned long page_counter_toptier_high(struct page_counter *counter);\n unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\n static inline void page_counter_calculate_protection(struct page_counter *root,\n-\t\t\t\t\t\t     struct page_counter *counter,\n-\t\t\t\t\t\t     bool recursive_protection) {}\n+\t\t\t\t\t\tstruct page_counter *counter,\n+\t\t\t\t\t\tbool recursive_protection,\n+\t\t\t\t\t\tbool toptier) {}\n #endif\n \n #endif /* _LINUX_PAGE_COUNTER_H */\ndiff --git a/kernel/cgroup/dmem.c b/kernel/cgroup/dmem.c\nindex 1ea6afffa985..536d43c42de8 100644\n--- a/kernel/cgroup/dmem.c\n+++ b/kernel/cgroup/dmem.c\n@@ -277,7 +277,7 @@ dmem_cgroup_calculate_protection(struct dmem_cgroup_pool_state *limit_pool,\n \t\t\tcontinue;\n \n \t\tpage_counter_calculate_protection(\n-\t\t\tclimit, &found_pool->cnt, true);\n+\t\t\tclimit, &found_pool->cnt, true, false);\n \n \t\tif (found_pool == test_pool)\n \t\t\tbreak;\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 07464f02c529..8aa7ae361a73 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -4806,12 +4806,13 @@ struct cgroup_subsys memory_cgrp_subsys = {\n  * mem_cgroup_calculate_protection - check if memory consumption is in the normal range\n  * @root: the top ancestor of the sub-tree being checked\n  * @memcg: the memory cgroup to check\n+ * @toptier: whether the caller is in a toptier node\n  *\n  * WARNING: This function is not stateless! It can only be used as part\n  *          of a top-down tree iteration, not for isolated queries.\n  */\n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t     struct mem_cgroup *memcg)\n+\t\t\t\t     struct mem_cgroup *memcg, bool toptier)\n {\n \tbool recursive_protection =\n \t\tcgrp_dfl_root.flags & CGRP_ROOT_MEMORY_RECURSIVE_PROT;\n@@ -4822,7 +4823,16 @@ void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \tif (!root)\n \t\troot = root_mem_cgroup;\n \n-\tpage_counter_calculate_protection(&root->memory, &memcg->memory, recursive_protection);\n+\tpage_counter_calculate_protection(&root->memory, &memcg->memory,\n+\t\t\t\t\t  recursive_protection, toptier);\n+}\n+\n+unsigned long mem_cgroup_toptier_usage(struct mem_cgroup *memcg)\n+{\n+\tif (mem_cgroup_disabled() || !memcg)\n+\t\treturn 0;\n+\n+\treturn atomic_long_read(&memcg->memory.toptier_usage);\n }\n \n void update_memcg_toptier_capacity(void)\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex cf21c72bfd4e..79d46a1c4c0c 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -410,12 +410,39 @@ static unsigned long effective_protection(unsigned long usage,\n \treturn ep;\n }\n \n+static void calculate_protection_toptier(struct page_counter *counter,\n+\t\t\t\t\t bool recursive_protection)\n+{\n+\tstruct page_counter *parent = counter->parent;\n+\tunsigned long toptier_low;\n+\tunsigned long toptier_usage, parent_toptier_usage;\n+\tunsigned long toptier_protected, old_toptier_protected;\n+\tlong delta;\n+\n+\ttoptier_low = page_counter_toptier_low(counter);\n+\ttoptier_usage = atomic_long_read(&counter->toptier_usage);\n+\tparent_toptier_usage = atomic_long_read(&parent->toptier_usage);\n+\n+\t/* Propagate toptier low usage to parent for sibling distribution */\n+\ttoptier_protected = min(toptier_usage, toptier_low);\n+\told_toptier_protected = atomic_long_xchg(&counter->toptier_low_usage,\n+\t\t\t\t\t\t toptier_protected);\n+\tdelta = toptier_protected - old_toptier_protected;\n+\tatomic_long_add(delta, &parent->children_toptier_low_usage);\n+\n+\tWRITE_ONCE(counter->etoptier_low,\n+\t\t   effective_protection(toptier_usage, parent_toptier_usage,\n+\t\t   toptier_low, READ_ONCE(parent->etoptier_low),\n+\t\t   atomic_long_read(&parent->children_toptier_low_usage),\n+\t\t   recursive_protection));\n+}\n \n /**\n  * page_counter_calculate_protection - check if memory consumption is in the normal range\n  * @root: the top ancestor of the sub-tree being checked\n  * @counter: the page_counter the counter to update\n  * @recursive_protection: Whether to use memory_recursiveprot behavior.\n+ * @toptier: Whether to calculate toptier-proportional protection\n  *\n  * Calculates elow/emin thresholds for given page_counter.\n  *\n@@ -424,7 +451,7 @@ static unsigned long effective_protection(unsigned long usage,\n  */\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n-\t\t\t\t       bool recursive_protection)\n+\t\t\t\t       bool recursive_protection, bool toptier)\n {\n \tunsigned long usage, parent_usage;\n \tstruct page_counter *parent = counter->parent;\n@@ -446,6 +473,9 @@ void page_counter_calculate_protection(struct page_counter *root,\n \tif (parent == root) {\n \t\tcounter->emin = READ_ONCE(counter->min);\n \t\tcounter->elow = READ_ONCE(counter->low);\n+\t\tif (toptier)\n+\t\t\tWRITE_ONCE(counter->etoptier_low,\n+\t\t\t\t   page_counter_toptier_low(counter));\n \t\treturn;\n \t}\n \n@@ -462,6 +492,9 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\tREAD_ONCE(parent->elow),\n \t\t\tatomic_long_read(&parent->children_low_usage),\n \t\t\trecursive_protection));\n+\n+\tif (toptier)\n+\t\tcalculate_protection_toptier(counter, recursive_protection);\n }\n \n void page_counter_update_toptier_capacity(struct page_counter *counter,\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 6a87ac7be43c..5b4cb030a477 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -4144,6 +4144,7 @@ static void lru_gen_age_node(struct pglist_data *pgdat, struct scan_control *sc)\n \tstruct mem_cgroup *memcg;\n \tunsigned long min_ttl = READ_ONCE(lru_gen_min_ttl);\n \tbool reclaimable = !min_ttl;\n+\tbool toptier = node_is_toptier(pgdat->node_id);\n \n \tVM_WARN_ON_ONCE(!current_is_kswapd());\n \n@@ -4153,7 +4154,7 @@ static void lru_gen_age_node(struct pglist_data *pgdat, struct scan_control *sc)\n \tdo {\n \t\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);\n \n-\t\tmem_cgroup_calculate_protection(NULL, memcg);\n+\t\tmem_cgroup_calculate_protection(NULL, memcg, toptier);\n \n \t\tif (!reclaimable)\n \t\t\treclaimable = lruvec_is_reclaimable(lruvec, sc, min_ttl);\n@@ -4905,12 +4906,14 @@ static int shrink_one(struct lruvec *lruvec, struct scan_control *sc)\n \tunsigned long reclaimed = sc->nr_reclaimed;\n \tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n \tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n+\tbool toptier = tier_aware_memcg_limits &&\n+\t\t       node_is_toptier(pgdat->node_id);\n \n \t/* lru_gen_age_node() called mem_cgroup_calculate_protection() */\n \tif (mem_cgroup_below_min(NULL, memcg))\n \t\treturn MEMCG_LRU_YOUNG;\n \n-\tif (mem_cgroup_below_low(NULL, memcg)) {\n+\tif (mem_cgroup_below_low(NULL, memcg, toptier)) {\n \t\t/* see the comment on MEMCG_NR_GENS */\n \t\tif (READ_ONCE(lruvec->lrugen.seg) != MEMCG_LRU_TAIL)\n \t\t\treturn MEMCG_LRU_TAIL;\n@@ -5960,6 +5963,7 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t};\n \tstruct mem_cgroup_reclaim_cookie *partial = &reclaim;\n \tstruct mem_cgroup *memcg;\n+\tbool toptier = node_is_toptier(pgdat->node_id);\n \n \t/*\n \t * In most cases, direct reclaimers can do partial walks\n@@ -5987,7 +5991,7 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t\t */\n \t\tcond_resched();\n \n-\t\tmem_cgroup_calculate_protection(target_memcg, memcg);\n+\t\tmem_cgroup_calculate_protection(target_memcg, memcg, toptier);\n \n \t\tif (mem_cgroup_below_min(target_memcg, memcg)) {\n \t\t\t/*\n@@ -5995,7 +5999,8 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t\t\t * If there is no reclaimable memory, OOM.\n \t\t\t */\n \t\t\tcontinue;\n-\t\t} else if (mem_cgroup_below_low(target_memcg, memcg)) {\n+\t\t} else if (mem_cgroup_below_low(target_memcg, memcg,\n+\t\t\t\t\ttier_aware_memcg_limits && toptier)) {\n \t\t\t/*\n \t\t\t * Soft protection.\n \t\t\t * Respect the protection only as long as\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author is addressing a concern about the fairness of memory distribution among workloads in isolated cgroups. They explained that the current limits only consider total memory footprint, not where it resides. The author proposed extending the existing memory.high protection to be tier-aware and adding a new nodemask parameter to try_to_free_mem_cgroup_pages for selective reclaim from memory at the memcg-tier intersection of a cgroup.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On machines serving multiple workloads whose memory is isolated via the\nmemory cgroup controller, it is currently impossible to enforce a fair\ndistribution of toptier memory among the workloads, as the only\nenforcable limits have to do with total memory footprint, but not where\nthat memory resides.\n\nThis makes ensuring a consistent and baseline performance difficult, as\neach workload's performance is heavily impacted by workload-external\nfactors wuch as which other workloads are co-located in the same host,\nand the order at which different workloads are started.\n\nExtend the existing memory.high protection to be tier-aware in the\ncharging and enforcement to limit toptier-hogging for workloads.\n\nAlso, add a new nodemask parameter to try_to_free_mem_cgroup_pages,\nwhich can be used to selectively reclaim from memory at the\nmemcg-tier interection of a cgroup.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/swap.h |  3 +-\n mm/memcontrol-v1.c   |  6 ++--\n mm/memcontrol.c      | 85 +++++++++++++++++++++++++++++++++++++-------\n mm/vmscan.c          | 11 +++---\n 4 files changed, 84 insertions(+), 21 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 0effe3cc50f5..c6037ac7bf6e 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -368,7 +368,8 @@ extern unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t\t  unsigned long nr_pages,\n \t\t\t\t\t\t  gfp_t gfp_mask,\n \t\t\t\t\t\t  unsigned int reclaim_options,\n-\t\t\t\t\t\t  int *swappiness);\n+\t\t\t\t\t\t  int *swappiness,\n+\t\t\t\t\t\t  nodemask_t *allowed);\n extern unsigned long mem_cgroup_shrink_node(struct mem_cgroup *mem,\n \t\t\t\t\t\tgfp_t gfp_mask, bool noswap,\n \t\t\t\t\t\tpg_data_t *pgdat,\ndiff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.c\nindex 0b39ba608109..29630c7f3567 100644\n--- a/mm/memcontrol-v1.c\n+++ b/mm/memcontrol-v1.c\n@@ -1497,7 +1497,8 @@ static int mem_cgroup_resize_max(struct mem_cgroup *memcg,\n \t\t}\n \n \t\tif (!try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL,\n-\t\t\t\tmemsw ? 0 : MEMCG_RECLAIM_MAY_SWAP, NULL)) {\n+\t\t\t\tmemsw ? 0 : MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\tNULL, NULL)) {\n \t\t\tret = -EBUSY;\n \t\t\tbreak;\n \t\t}\n@@ -1529,7 +1530,8 @@ static int mem_cgroup_force_empty(struct mem_cgroup *memcg)\n \t\t\treturn -EINTR;\n \n \t\tif (!try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL,\n-\t\t\t\t\t\t  MEMCG_RECLAIM_MAY_SWAP, NULL))\n+\t\t\t\t\t\t  MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\t\t  NULL, NULL))\n \t\t\tnr_retries--;\n \t}\n \ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 8aa7ae361a73..ebd4a1b73c51 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -2184,18 +2184,30 @@ static unsigned long reclaim_high(struct mem_cgroup *memcg,\n \n \tdo {\n \t\tunsigned long pflags;\n-\n-\t\tif (page_counter_read(&memcg->memory) <=\n-\t\t    READ_ONCE(memcg->memory.high))\n+\t\tnodemask_t toptier_nodes, *reclaim_nodes;\n+\t\tbool mem_high_ok, toptier_high_ok;\n+\n+\t\tmt_get_toptier_nodemask(&toptier_nodes, NULL);\n+\t\tmem_high_ok = page_counter_read(&memcg->memory) <=\n+\t\t\t      READ_ONCE(memcg->memory.high);\n+\t\ttoptier_high_ok = !(tier_aware_memcg_limits &&\n+\t\t\t\t    mem_cgroup_toptier_usage(memcg) >\n+\t\t\t\t    page_counter_toptier_high(&memcg->memory));\n+\t\tif (mem_high_ok && toptier_high_ok)\n \t\t\tcontinue;\n \n+\t\tif (mem_high_ok && !toptier_high_ok)\n+\t\t\treclaim_nodes = &toptier_nodes;\n+\t\telse\n+\t\t\treclaim_nodes = NULL;\n+\n \t\tmemcg_memory_event(memcg, MEMCG_HIGH);\n \n \t\tpsi_memstall_enter(&pflags);\n \t\tnr_reclaimed += try_to_free_mem_cgroup_pages(memcg, nr_pages,\n \t\t\t\t\t\t\tgfp_mask,\n \t\t\t\t\t\t\tMEMCG_RECLAIM_MAY_SWAP,\n-\t\t\t\t\t\t\tNULL);\n+\t\t\t\t\t\t\tNULL, reclaim_nodes);\n \t\tpsi_memstall_leave(&pflags);\n \t} while ((memcg = parent_mem_cgroup(memcg)) &&\n \t\t !mem_cgroup_is_root(memcg));\n@@ -2296,6 +2308,24 @@ static u64 mem_find_max_overage(struct mem_cgroup *memcg)\n \treturn max_overage;\n }\n \n+static u64 toptier_find_max_overage(struct mem_cgroup *memcg)\n+{\n+\tu64 overage, max_overage = 0;\n+\n+\tif (!tier_aware_memcg_limits)\n+\t\treturn 0;\n+\n+\tdo {\n+\t\tunsigned long usage = mem_cgroup_toptier_usage(memcg);\n+\t\tunsigned long high = page_counter_toptier_high(&memcg->memory);\n+\n+\t\toverage = calculate_overage(usage, high);\n+\t\tmax_overage = max(overage, max_overage);\n+\t} while ((memcg = parent_mem_cgroup(memcg)) &&\n+\t\t  !mem_cgroup_is_root(memcg));\n+\n+\treturn max_overage;\n+}\n static u64 swap_find_max_overage(struct mem_cgroup *memcg)\n {\n \tu64 overage, max_overage = 0;\n@@ -2401,6 +2431,14 @@ void __mem_cgroup_handle_over_high(gfp_t gfp_mask)\n \tpenalty_jiffies += calculate_high_delay(memcg, nr_pages,\n \t\t\t\t\t\tswap_find_max_overage(memcg));\n \n+\t/*\n+\t * Don't double-penalize for toptier high overage if system-wide\n+\t * memory.high has already been breached.\n+\t */\n+\tif (!penalty_jiffies)\n+\t\tpenalty_jiffies += calculate_high_delay(memcg, nr_pages,\n+\t\t\t\t\ttoptier_find_max_overage(memcg));\n+\n \t/*\n \t * Clamp the max delay per usermode return so as to still keep the\n \t * application moving forwards and also permit diagnostics, albeit\n@@ -2503,7 +2541,8 @@ static int try_charge_memcg(struct mem_cgroup *memcg, gfp_t gfp_mask,\n \n \tpsi_memstall_enter(&pflags);\n \tnr_reclaimed = try_to_free_mem_cgroup_pages(mem_over_limit, nr_pages,\n-\t\t\t\t\t\t    gfp_mask, reclaim_options, NULL);\n+\t\t\t\t\t\t    gfp_mask, reclaim_options,\n+\t\t\t\t\t\t    NULL, NULL);\n \tpsi_memstall_leave(&pflags);\n \n \tif (mem_cgroup_margin(mem_over_limit) >= nr_pages)\n@@ -2592,23 +2631,26 @@ static int try_charge_memcg(struct mem_cgroup *memcg, gfp_t gfp_mask,\n \t * reclaim, the cost of mismatch is negligible.\n \t */\n \tdo {\n-\t\tbool mem_high, swap_high;\n+\t\tbool mem_high, swap_high, toptier_high = false;\n \n \t\tmem_high = page_counter_read(&memcg->memory) >\n \t\t\tREAD_ONCE(memcg->memory.high);\n \t\tswap_high = page_counter_read(&memcg->swap) >\n \t\t\tREAD_ONCE(memcg->swap.high);\n+\t\ttoptier_high = tier_aware_memcg_limits &&\n+\t\t\t       (mem_cgroup_toptier_usage(memcg) >\n+\t\t\t\tpage_counter_toptier_high(&memcg->memory));\n \n \t\t/* Don't bother a random interrupted task */\n \t\tif (!in_task()) {\n-\t\t\tif (mem_high) {\n+\t\t\tif (mem_high || toptier_high) {\n \t\t\t\tschedule_work(&memcg->high_work);\n \t\t\t\tbreak;\n \t\t\t}\n \t\t\tcontinue;\n \t\t}\n \n-\t\tif (mem_high || swap_high) {\n+\t\tif (mem_high || swap_high || toptier_high) {\n \t\t\t/*\n \t\t\t * The allocating tasks in this cgroup will need to do\n \t\t\t * reclaim or be throttled to prevent further growth\n@@ -4476,7 +4518,7 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n \tunsigned int nr_retries = MAX_RECLAIM_RETRIES;\n \tbool drained = false;\n-\tunsigned long high;\n+\tunsigned long high, toptier_high;\n \tint err;\n \n \tbuf = strstrip(buf);\n@@ -4485,15 +4527,22 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \t\treturn err;\n \n \tpage_counter_set_high(&memcg->memory, high);\n+\ttoptier_high = page_counter_toptier_high(&memcg->memory);\n \n \tif (of->file->f_flags & O_NONBLOCK)\n \t\tgoto out;\n \n \tfor (;;) {\n \t\tunsigned long nr_pages = page_counter_read(&memcg->memory);\n+\t\tunsigned long toptier_pages = mem_cgroup_toptier_usage(memcg);\n \t\tunsigned long reclaimed;\n+\t\tunsigned long to_free;\n+\t\tnodemask_t toptier_nodes, *reclaim_nodes;\n+\t\tbool mem_high_ok = nr_pages <= high;\n+\t\tbool toptier_high_ok = !(tier_aware_memcg_limits &&\n+\t\t\t\t\t toptier_pages > toptier_high);\n \n-\t\tif (nr_pages <= high)\n+\t\tif (mem_high_ok && toptier_high_ok)\n \t\t\tbreak;\n \n \t\tif (signal_pending(current))\n@@ -4505,8 +4554,17 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \t\t\tcontinue;\n \t\t}\n \n-\t\treclaimed = try_to_free_mem_cgroup_pages(memcg, nr_pages - high,\n-\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP, NULL);\n+\t\tmt_get_toptier_nodemask(&toptier_nodes, NULL);\n+\t\tif (mem_high_ok && !toptier_high_ok) {\n+\t\t\treclaim_nodes = &toptier_nodes;\n+\t\t\tto_free = toptier_pages - toptier_high;\n+\t\t} else {\n+\t\t\treclaim_nodes = NULL;\n+\t\t\tto_free = nr_pages - high;\n+\t\t}\n+\t\treclaimed = try_to_free_mem_cgroup_pages(memcg, to_free,\n+\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\tNULL, reclaim_nodes);\n \n \t\tif (!reclaimed && !nr_retries--)\n \t\t\tbreak;\n@@ -4558,7 +4616,8 @@ static ssize_t memory_max_write(struct kernfs_open_file *of,\n \n \t\tif (nr_reclaims) {\n \t\t\tif (!try_to_free_mem_cgroup_pages(memcg, nr_pages - max,\n-\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP, NULL))\n+\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\tNULL, NULL))\n \t\t\t\tnr_reclaims--;\n \t\t\tcontinue;\n \t\t}\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 5b4cb030a477..94498734b4f5 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -6652,7 +6652,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t   unsigned long nr_pages,\n \t\t\t\t\t   gfp_t gfp_mask,\n \t\t\t\t\t   unsigned int reclaim_options,\n-\t\t\t\t\t   int *swappiness)\n+\t\t\t\t\t   int *swappiness, nodemask_t *allowed)\n {\n \tunsigned long nr_reclaimed;\n \tunsigned int noreclaim_flag;\n@@ -6668,6 +6668,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t.may_unmap = 1,\n \t\t.may_swap = !!(reclaim_options & MEMCG_RECLAIM_MAY_SWAP),\n \t\t.proactive = !!(reclaim_options & MEMCG_RECLAIM_PROACTIVE),\n+\t\t.nodemask = allowed,\n \t};\n \t/*\n \t * Traverse the ZONELIST_FALLBACK zonelist of the current node to put\n@@ -6693,7 +6694,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t   unsigned long nr_pages,\n \t\t\t\t\t   gfp_t gfp_mask,\n \t\t\t\t\t   unsigned int reclaim_options,\n-\t\t\t\t\t   int *swappiness)\n+\t\t\t\t\t   int *swappiness, nodemask_t *allowed)\n {\n \treturn 0;\n }\n@@ -7806,9 +7807,9 @@ int user_proactive_reclaim(char *buf,\n \t\t\treclaim_options = MEMCG_RECLAIM_MAY_SWAP |\n \t\t\t\t\t  MEMCG_RECLAIM_PROACTIVE;\n \t\t\treclaimed = try_to_free_mem_cgroup_pages(memcg,\n-\t\t\t\t\t\t batch_size, gfp_mask,\n-\t\t\t\t\t\t reclaim_options,\n-\t\t\t\t\t\t swappiness == -1 ? NULL : &swappiness);\n+\t\t\t\t\tbatch_size, gfp_mask, reclaim_options,\n+\t\t\t\t\tswappiness == -1 ? NULL : &swappiness,\n+\t\t\t\t\tNULL);\n \t\t} else {\n \t\t\tstruct scan_control sc = {\n \t\t\t\t.gfp_mask = current_gfp_context(gfp_mask),\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author is addressing a concern that the patch does not address the issue of tier-aware memcg limits being less effective on systems with tiered memory, where well-behaved workloads can still hurt other workloads by hogging more toptier memory than their 'fair share'. The author explains that introducing tier-aware memcg limits will scale memory.low/high to reflect the ratio of toptier:total memory a cgroup has access to, and provides an example scenario where this is beneficial. The author also introduces a sysctl to toggle between enforcing and overlooking toptier memcg limit breaches.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Memory cgroups provide an interface that allow multiple workloads on a\nhost to co-exist, and establish both weak and strong memory isolation\nguarantees. For large servers and small embedded systems alike, memcgs\nprovide an effective way to provide a baseline quality of service for\nprotected workloads.\n\nThis works, because for the most part, all memory is equal (except for\nzram / zswap). Restricting a cgroup's memory footprint restricts how\nmuch it can hurt other workloads competing for memory. Likewise, setting\nmemory.low or memory.min limits can provide weak and strong guarantees\nto the performance of a cgroup.\n\nHowever, on systems with tiered memory (e.g. CXL / compressed memory),\nthe quality of service guarantees that memcg limits enforced become less\neffective, as memcg has no awareness of the physical location of its\ncharged memory. In other words, a workload that is well-behaved within\nits memcg limits may still be hurting the performance of other\nwell-behaving workloads on the system by hogging more than its\n\"fair share\" of toptier memory.\n\nIntroduce tier-aware memcg limits, which scale memory.low/high to\nreflect the ratio of toptier:total memory the cgroup has access.\n\nTake the following scenario as an example:\nOn a host with 3:1 toptier:lowtier, say 150G toptier, and 50Glowtier,\nsetting a cgroup's limits to:\n\tmemory.min:  15G\n\tmemory.low:  20G\n\tmemory.high: 40G\n\tmemory.max:  50G\n\nWill be enforced at the toptier as:\n\tmemory.min:          15G\n\tmemory.toptier_low:  15G (20 * 150/200)\n\tmemory.toptier_high: 30G (40 * 150/200)\n\tmemory.max:          50G\n\nLet's say that there are 4 such cgroups on the host. Previously, it would\nbe possible for 3 hosts to completely take over all of DRAM, while one\ncgroup could only access the lowtier memory. In the perspective of a\ntier-agnostic memcg limit enforcement, the three cgroups are all\nwell-behaved, consuming within their memory limits.\n\nThis is not to say that the scenario above is incorrect. In fact, for\nletting the hottest cgroups run in DRAM while pushing out colder cgroups\nto lowtier memory lets the system perform the most aggregate work total.\n\nBut for other scenarios, the target might not be maximizing aggregate\nwork, but maximizing the minimum performance guarantee for each\nindividual workload (think hosts shared across different users, such as\nVM hosting services).\n\nTo reflect these two scenarios, introduce a sysctl tier_aware_memcg,\nwhich allows the host to toggle between enforcing and overlooking\ntoptier memcg limit breaches.\n\nThis work is inspired & based off of Kaiyang Zhao's work from 2024 [1],\nwhere he referred to this concept as \"memory tiering fairness\".\nThe biggest difference in the implementations lie in how toptier memory\nis tracked; in his implementation, an lruvec stat aggregation is done on\neach usage check, while in this implementation, a new cacheline is\nintroduced in page_coutner to keep track of toptier usage (Kaiyang also\nintroduces a new cachline in page_counter, but only uses it to cache\ncapacity and thresholds). This implementation also extends the memory\nlimit enforcement to memory.high as well.\n\n[1] https://lore.kernel.org/linux-mm/20240920221202.1734227-1-kaiyang2@cs.cmu.edu/\n\n---\nJoshua Hahn (6):\n  mm/memory-tiers: Introduce tier-aware memcg limit sysfs\n  mm/page_counter: Introduce tiered memory awareness to page_counter\n  mm/memory-tiers, memcontrol: Introduce toptier capacity updates\n  mm/memcontrol: Charge and uncharge from toptier\n  mm/memcontrol, page_counter: Make memory.low tier-aware\n  mm/memcontrol: Make memory.high tier-aware\n\n include/linux/memcontrol.h   |  21 ++++-\n include/linux/memory-tiers.h |  30 +++++++\n include/linux/page_counter.h |  31 ++++++-\n include/linux/swap.h         |   3 +-\n kernel/cgroup/cpuset.c       |   2 +-\n kernel/cgroup/dmem.c         |   2 +-\n mm/memcontrol-v1.c           |   6 +-\n mm/memcontrol.c              | 155 +++++++++++++++++++++++++++++++----\n mm/memory-tiers.c            |  63 ++++++++++++++\n mm/page_counter.c            |  77 ++++++++++++++++-\n mm/vmscan.c                  |  24 ++++--\n 11 files changed, 376 insertions(+), 38 deletions(-)\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Michal Hocko",
              "summary": "Reviewer Michal Hocko questioned whether the patch's assumption that active workingset sizes of all workloads don't fit into the top tier is typical in real-life configurations, and asked if memory consumption on particular tiers should be limited even without external pressure.\n\nReviewer Michal Hocko questioned whether focusing only on the top tier is a long-term solution, noting that similar issues may arise in other tiers and expressing concern about duplicating limits for each/top tier.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "questioning",
                "request for clarification",
                "requested changes",
                "immediate questions"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This assumes that the active workingset size of all workloads doesn't\nfit into the top tier right? Otherwise promotions would make sure to\nthat we have the most active memory in the top tier. Is this typical in\nreal life configurations?\n\nOr do you intend to limit memory consumption on particular tier even\nwithout an external pressure?\n\n---\n\nLet's spend some more time with the interface first. You seem to be\nfocusing only on the top tier with this interface, right? Is this really the\nright way to go long term? What makes you believe that we do not really\nhit the same issue with other tiers as well? Also do we want/need to\nduplicate all the limits for each/top tier? What is the reasoning for\nthe switch to be runtime sysctl rather than boot-time or cgroup mount\noption?\n\nI will likely have more questions but these are immediate ones after\nreading the cover. Please note I haven't really looked at the\nimplementation yet. I really want to understand usecases and interface\nfirst.\n-- \nMichal Hocko\nSUSE Labs",
              "reply_to": "Joshua Hahn",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "Author is addressing Michal's feedback by mentioning the intention to discuss project scope and use cases in LSFMMBPF, but does not directly address or acknowledge any specific technical concerns raised by Michal.\n\nAuthor acknowledged a concern about the impact of a workload violating its fair share of toptier memory, explaining that it mostly hurts other workloads when the aggregate working set size exceeds toptier memory capacity.\n\nAuthor acknowledged that the current approach may not be suitable for all use cases and proposed a different perspective of thinking about memory allocation in a per-workload context, rather than a per-system one.\n\nAuthor responded to Michal Hocko's concern about the realism of the patch examples, stating they are realistic scenarios for cloud providers and hyperscalers.\n\nAuthor acknowledges a concern about the interface's behavior and proposes two alternative modes: 'fixed' and 'opportunistic' reclaim, asking for feedback on which one aligns better with the reviewer's expectations.\n\nAuthor acknowledges that the patch series was sent out of order, and agrees it would have been better to send the LSFMMBPF topic proposal first, causing some potential confusion.\n\nAuthor acknowledged that the current implementation only addresses two-tiered systems and may not be suitable for future multi-tiered systems, but plans to revisit this issue in a later patchset.\n\nAuthor responded to a question about how tier-aware memcg limits handle cases with multiple nodes or tiers in the toptier, asking for clarification on what specific scenario is being referred to.\n\nAuthor addressed Michal Hocko's concern that allowing cgroups to set their own mount options for tier-aware memcg limits could lead to inconsistent behavior and undermine the purpose of having a performance guarantee, agreeing that this approach is not desirable.\n\nAuthor acknowledged that the reviewer's feedback was good and thanked them for reviewing, indicating no further action or revision planned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "clarifying discussion plans",
                "separating series from proposal",
                "clarification",
                "explanation",
                "acknowledged a potential issue",
                "proposed an alternative approach",
                "acknowledged feedback",
                "provided explanation",
                "acknowledges a concern",
                "proposes alternatives",
                "acknowledged a mistake",
                "apologized for confusion",
                "acknowledged limitations",
                "planned future work",
                "asking_for_clarification",
                "agreed with feedback",
                "acknowledgment",
                "appreciation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hello Michal,\n\nI hope that you are doing well! Thank you for taking the time to review my\nwork and leaving your thoughts.\n\nI wanted to note that I hope to bring this discussion to LSFMMBPF as well,\nto discuss what the scope of the project should be, what usecases there\nare (as I will note below), how to make this scalable and sustainable\nfor the future, etc. I'll send out a topic proposal later today. I had\nseparated the series from the proposal because I imagined that this\nseries would go through many versions, so it would be helpful to have\nthe topic as a unified place for pre-conference discussions.\n\n---\n\nYes, for the scenario above, a workload that is violating its fair share\nof toptier memory mostly hurts other workloads if the aggregate working\nset size of all workloads exceeds the size of toptier memory.\n\n---\n\nThis is true. And for a lot of usecases, this is 100% the right thing to do.\nHowever, with this patch I want to encourage a different perspective,\nwhich is to think about things in a per-workload perspective, and not a\nper-system perspective.\n\nHaving hot memory in high tiers and cold memory in low tiers is only\nlogical, since we increase the system's throughput and make the most\noptimal choices for latency. However, what about systems that care about\nobjectives other than simply maximizing throughput?\n\nIn the original cover letter I offered an example of VM hosting services\nthat care less about maximizing host-wide throughput, but more on ensuring\na bottomline performance guarantee for all workloads running on the system.\nFor the users on these services, they don't care that the host their VM is\nrunning on is maximizing throughput; rather, they care that their VM meets\nthe performance guarantees that their provider promised. If there is no\nway to know or enforce which tier of memory their workload lands on, either\nthe bottomline guarantee becomes very underestimated, or users must deal\nwith a high variance in performance.\n\nHere's another example: Let's say there is a host with multiple workloads,\neach serving queries for a database. The host would like to guarantee the\nlowest maximum latency possible, while maximizing the total throughput\nof the system. Once again in this situation, without tier-aware memcg\nlimits the host can maximize throughput, but can only make severely\nunderestimated promises on the bottom line.\n\n---\n\nI would say so. I think that the two examples above are realistic\nscenarios that cloud providers and hyperscalers might face on tiered systems.\n\n---\n\nThis is a great question, and one that I hope to discuss at LSFMMBPF\nto see how people expect an interface like this to work.\n\nOver the past few weeks, I have been discussing this idea during the\nLinux Memory Hotness and Promotion biweekly calls with Gregory Price [1].\nOne of the proposals that we made there (but did not include in this\nseries) is the idea of \"fixed\" vs. \"opportunistic\" reclaim.\n\nFixed mode is what we have here -- start limiting toptier usage whenever\na workload goes above its fair slice of toptier.\nOpportunistic mode would allow workloads to use more toptier memory than\nits fair share, but only be restricted when toptier is pressured.\n\nWhat do you think about these two options? For the stated goal of this\nseries, which is to help maximize the bottom line for workloads, fair\nshare seemed to make sense. Implementing opportunistic mode changes\non top of this work would most likely just be another sysctl.\n\n---\n\nThat sounds good with me, my goal was to bring this out as an RFC patchset\nso folks could look at the code and understand the motivation, and then send\nout the LSFMMBPF topic proposal. In retrospect I think I should have done\nit in the opposite order. I'm sorry if this caused any confusion.\n\n---\n\nYes, that's right. I'm not sure if this is the right way to go long-term\n(say, past the next 5 years). My thinking was that I can stick with doing\nthis for toptier vs. non-toptier memory for now, and deal with having\n3+ tiers in the future, when we start to have systems with that many tiers.\nAFAICT two-tiered systems are still ~relatively new, and I don't think\nthere are a lot of genuine usecases for enforcing mid-tier memory limits\nas of now. Of course, I would be excited to learn about these usecases\nand work this patchset to support them as well if anybody has them.\n\n---\n\nSorry, I'm not sure that I completely understood this question. Are you\nreferring to the case where we have multiple nodes in the toptier?\nIf so, then all of those nodes are treated the same, and don't have\nunique limits. Or are you referring to the case where we have multiple\ntiers in the toptier? If so, I hope the answer above can answer this too.\n\n---\n\nGood point : -) I don't think cgroup mount options are a good idea,\nsince this would mean that we can have a set of cgroups self-policing\ntheir toptier usage, while another cgroup allocates memory unrestricted.\nThis would punish the self-policing cgroup and we would lose the benefit\nof having a bottomline performance guarantee.\n\n---\n\nThat sounds good to me, thank you again for reviewing this work!\nI hope you have a great day : -)\nJoshua\n\n[1] https://lore.kernel.org/linux-mm/c8bc2dce-d4ec-c16e-8df4-2624c48cfc06@google.com/",
              "reply_to": "Michal Hocko",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer noted that the patch does not handle the case where tier-aware memcg limits are toggled off, and requested a check to ensure that the sysfs attribute is only accessed when the feature is enabled.\n\nReviewer Gregory Price expressed concern that the patch's assumption about always wanting tier-aware memcg limits may reduce the usefulness of secondary memory tiers, as services will prefer not to be deployed on machines with high performance variance.\n\nReviewer Gregory Price noted that lack of tier-awareness is a significant blocker for deploying mixed workloads on large, dense memory systems with multiple tiers (2+), and suggested using the existing knobs (max/high/low/min) to proportionally control coherent memory tiers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "concerns",
                "reduces usefulness",
                "blocker",
                "significant"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Just injecting a few points here\n(disclosure: I have been in the development loop for this feature)\n\n---\n\nYes / No.  This makes the assumption that you always want this.\n\nBarring a minimum Quality of Service mechanism (as Joshua explains)\nthis reduces the usefulness of a secondary tier of memory.\n\nServices will just prefer not to be deployed to these kinds of\nmachines because the performance variance is too high.\n\n---\n\nThe answer is unequivocally yes.\n\nLacking tier-awareness is actually a huge blocker for deploying mixed\nworkloads on large, dense memory systems with multiple tiers (2+).\n\nTechnically we're already at 4-ish tiers: DDR, CXL, ZSWAP, SWAP.\n\nWe have zswap/swap controls in cgroups already, we just lack that same\ncontrol for coherent memory tiers.  This tries to use the existing nobs\n(max/high/low/min) to do what they already do - just proportionally.\n\n~Gregory",
              "reply_to": "Joshua Hahn",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kaiyang Zhao",
              "summary": "Reviewer Kaiyang Zhao noted that current server memory shapes and workload stacking settings cause contention of top-tier memory, leading to significant variations in tail latency and throughput for co-colocated workloads, which this patch set aims to alleviate.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEUTRAL"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hello! I'm the author of the RFC in 2024. Just want to add that we've\nrecently released a preprint paper on arXiv that includes case studies\nwith a few of Meta's production workloads using a prototype version of\nthe patches.\n\nThe results confirmed that co-colocated workloads can have working set\nsizes exceeding the limited top-tier memory capacity given today's\nserver memory shapes and workload stacking settings, causing contention\nof top-tier memory. Workloads see significant variations in tail\nlatency and throughput depending on the share of top-tier tier memory\nthey get, which this patch set will alleviate.\n\nBest,\nKaiyang\n\n[1] https://arxiv.org/pdf/2602.08800",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "JP Kobryn",
      "primary_email": "inwardvessel@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Kiryl Shutsemau",
      "primary_email": "kas@kernel.org",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Leo Martins",
      "primary_email": "loemra.dev@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 3/3] btrfs: add tracepoint for search slot restart tracking",
          "message_id": "18c04d9a68f64fa5e36dde196306170d0fb437d9.1771884128.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/18c04d9a68f64fa5e36dde196306170d0fb437d9.1771884128.git.loemra.dev@gmail.com/",
          "date": "2026-02-24T19:22:55Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds a new tracepoint to the Btrfs kernel module, allowing for tracking of search slot restarts in btrfs_search_slot(). The tracepoint records the root, tree level, and reason for each restart, enabling more detailed analysis of COW amplification under memory pressure.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "The patch looks good, but the author should consider adding a comment to explain why the per-restart-site tracepoint is necessary and how it improves over the existing counter-based approach.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 2/3] btrfs: inhibit extent buffer writeback to prevent COW amplification",
          "message_id": "cc847a35e26cc4dfad18c59e3c525cea507ff440.1771884128.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/cc847a35e26cc4dfad18c59e3c525cea507ff440.1771884128.git.loemra.dev@gmail.com/",
          "date": "2026-02-24T19:22:54Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds a new tracepoint to the Btrfs kernel module, allowing for tracking of search slot restarts in btrfs_search_slot(). The tracepoint records the root, tree level, and reason for each restart, enabling more detailed analysis of COW amplification under memory pressure.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "The patch looks good, but the author should consider adding a comment to explain why the per-restart-site tracepoint is necessary and how it improves over the existing counter-based approach.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 1/3] btrfs: skip COW for written extent buffers allocated in current transaction",
          "message_id": "4ce911a475b998ddf76951629ad203e6440ab0ca.1771884128.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/4ce911a475b998ddf76951629ad203e6440ab0ca.1771884128.git.loemra.dev@gmail.com/",
          "date": "2026-02-24T19:22:52Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds a new tracepoint to the Btrfs kernel module, allowing for tracking of search slot restarts in btrfs_search_slot(). The tracepoint records the root, tree level, and reason for each restart, enabling more detailed analysis of COW amplification under memory pressure.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "The patch looks good, but the author should consider adding a comment to explain why the per-restart-site tracepoint is necessary and how it improves over the existing counter-based approach.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 0/3] btrfs: fix COW amplification under memory pressure",
          "message_id": "cover.1771884128.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/cover.1771884128.git.loemra.dev@gmail.com/",
          "date": "2026-02-24T19:22:51Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds a new tracepoint to the Btrfs kernel module, allowing for tracking of search slot restarts in btrfs_search_slot(). The tracepoint records the root, tree level, and reason for each restart, enabling more detailed analysis of COW amplification under memory pressure.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "The patch looks good, but the author should consider adding a comment to explain why the per-restart-site tracepoint is necessary and how it improves over the existing counter-based approach.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Mark Harmstone",
      "primary_email": "mark@harmstone.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix error message in btrfs_validate_super()",
          "message_id": "20260217185335.21013-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260217185335.21013-1-mark@harmstone.com/",
          "date": "2026-02-17T18:54:05Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-17",
          "patch_summary": "This patch fixes an error message in btrfs_validate_super() to correctly handle the superblock offset mismatch, updating the format specifier from %u to %llu.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Qu Wenruo",
              "summary": "Gave a Reviewed-by for the patch, indicating it looks good to them.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\n 2026/2/18 05:23, Mark Harmstone :\n> Fix the superblock offset mismatch error message in\n> btrfs_validate_super(): we changed it so that it considers all the\n> superblocks, but the message still assumes we're only looking at the\n> first one.\n> \n> The change from %u to %llu is because we're changing from a constant to\n> a u64.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 069ec957c35e (\"btrfs: Refactor btrfs_check_super_valid\")\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\n\nThanks,\nQu\n\n> ---\n>   fs/btrfs/disk-io.c | 4 ++--\n>   1 file changed, 2 insertions(+), 2 deletions(-)\n> \n> diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c\n> index 600287ac8eb7..f39008591631 100644\n> --- a/fs/btrfs/disk-io.c\n> +++ b/fs/btrfs/disk-io.c\n> @@ -2533,8 +2533,8 @@ int btrfs_validate_super(const struct btrfs_fs_info *fs_info,\n>   \n>   \tif (unlikely(mirror_num >= 0 &&\n>   \t\t     btrfs_super_bytenr(sb) != btrfs_sb_offset(mirror_num))) {\n> -\t\tbtrfs_err(fs_info, \"super offset mismatch %llu != %u\",\n> -\t\t\t  btrfs_super_bytenr(sb), BTRFS_SUPER_INFO_OFFSET);\n> +\t\tbtrfs_err(fs_info, \"super offset mismatch %llu != %llu\",\n> +\t\t\t  btrfs_super_bytenr(sb), btrfs_sb_offset(mirror_num));\n>   \t\tret = -EINVAL;\n>   \t}\n>   \n\n",
              "reply_to": "",
              "message_date": "2026-02-18",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "David Sterba",
              "summary": "Notified Mark about the edited subjects of error message fixing patches, but did not provide a formal review or feedback on the patch itself.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "INFORMATIONAL"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Tue, Feb 17, 2026 at 06:53:19PM +0000, Mark Harmstone wrote:\n> Fix the superblock offset mismatch error message in\n> btrfs_validate_super(): we changed it so that it considers all the\n> superblocks, but the message still assumes we're only looking at the\n> first one.\n> \n> The change from %u to %llu is because we're changing from a constant to\n> a u64.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 069ec957c35e (\"btrfs: Refactor btrfs_check_super_valid\")\n\nFYI I've edited the subjects of the error message fixing patches to be\nmore specific what is being fixed now that there are a few of them.\n\n",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Nhat Pham",
      "primary_email": "nphamcs@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH RFC 00/15] mm, swap: swap table phase IV with dynamic ghost swapfile",
          "message_id": "CAKEwX=NjRGxjQuvAnRoom=Ac_YptspMk1pwoq-2on46f1meuyw@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAKEwX=NjRGxjQuvAnRoom=Ac_YptspMk1pwoq-2on46f1meuyw@mail.gmail.com/",
          "date": "2026-02-24T21:57:05Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch aims to store and check memcg info in the swap table, but one reviewer is questioning its necessity.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Johannes Weiner",
              "summary": "Raised questions about the necessity of storing memcg info in the swap table, suggesting that existing code already handles similar scenarios.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Rik van Riel",
      "primary_email": "riel@surriel.com",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 3/5] mm: add a batched helper to clear the young flag for large folios",
          "message_id": "58e1883fe084d8284dac68dcd570f5a6c56c0abc.camel@surriel.com",
          "url": "https://lore.kernel.org/all/58e1883fe084d8284dac68dcd570f5a6c56c0abc.camel@surriel.com/",
          "date": "2026-02-24T22:05:10Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch adds a batched helper to clear the young flag for large folios.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Rik van Riel",
              "summary": "Raised a minor concern about an infinite loop in the patch's loop conditional, suggesting it be modified to avoid this issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 2/5] mm: rmap: add a ZONE_DEVICE folio warning in folio_referenced()",
          "message_id": "b3c1c739c233ccb32945ccaffdaf25fd3f96dd59.camel@surriel.com",
          "url": "https://lore.kernel.org/all/b3c1c739c233ccb32945ccaffdaf25fd3f96dd59.camel@surriel.com/",
          "date": "2026-02-24T02:39:48Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch adds a batched helper to clear the young flag for large folios.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Rik van Riel",
              "summary": "Raised a minor concern about an infinite loop in the patch's loop conditional, suggesting it be modified to avoid this issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 4/5] mm: support batched checking of the young flag for MGLRU",
          "message_id": "5957cdb584cad9007a58f43fb5a1c3b737fb0159.camel@surriel.com",
          "url": "https://lore.kernel.org/all/5957cdb584cad9007a58f43fb5a1c3b737fb0159.camel@surriel.com/",
          "date": "2026-02-24T22:13:38Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch adds a batched helper to clear the young flag for large folios.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Rik van Riel",
              "summary": "Raised a minor concern about an infinite loop in the patch's loop conditional, suggesting it be modified to avoid this issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 1/5] mm: use inline helper functions instead of ugly macros",
          "message_id": "01f4ffab0da9e4326a78f8b6eedce23dfb115e7a.camel@surriel.com",
          "url": "https://lore.kernel.org/all/01f4ffab0da9e4326a78f8b6eedce23dfb115e7a.camel@surriel.com/",
          "date": "2026-02-24T02:42:53Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch adds a batched helper to clear the young flag for large folios.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Rik van Riel",
              "summary": "Raised a minor concern about an infinite loop in the patch's loop conditional, suggesting it be modified to avoid this issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Shakeel Butt",
      "primary_email": "shakeel.butt@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "aZ3EE4JVDghZSq59@linux.dev",
          "url": "https://lore.kernel.org/all/aZ3EE4JVDghZSq59@linux.dev/",
          "date": "2026-02-24T15:31:19Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations in the Linux kernel, allowing for future instrumentation or debugging hooks to be added without modifying individual call sites. The wrappers are not yet used but prepare the code for subsequent patches. This change is intended to centralize zone lock operations behind a common interface, making it easier to add functionality in the future.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing reviewer feedback about direct zone lock acquire/release operations not being replaced with the newly introduced wrappers, and has confirmed that this change will be made in the next patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "next patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing concerns about the lack of visibility into zone lock contention and its impact on performance, particularly in memory-intensive workloads. They explain that existing instrumentation does not provide sufficient information to diagnose issues and propose adding dedicated tracepoint instrumentation to the zone lock, following a similar model to mmap_lock tracing. The author also mentions minor restructuring required for compaction changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging technical concerns",
                "proposing additional instrumentation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the zone lock wrappers interfering with compact_lock_irqsave() by introducing a new struct compact_lock to abstract the underlying lock type, which will allow compact_lock_irqsave() to operate correctly on both zone locks and raw spinlocks. The author confirmed that no functional change is intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock wrappers, explaining that they followed the mmap_lock pattern and ensured the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to introduce zone lock wrappers and tracepoints together, before mechanically converting users to the wrappers, to improve understanding of the changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason\n\nreviewer pointed out that the zone_lock_irqsave() macro should not return a value and suggested replacing it with an if-else statement\n\nReviewer Cheatham suggested moving zone lock wrapper changes, which are not yet used, to a later patch where they fit better with other similar changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested_reorder"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to introduce zone lock wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2) together instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "nit"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged a suggestion from reviewer Cheatham about reordering patches in the series, explained that they intentionally structured the series to keep refactoring and instrumentation changes separate, and stated their preference for maintaining the current order.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged suggestion",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the patch's priority should be reassessed in favor of improving the reading order of the series, but ultimately accepted the current implementation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK",
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author addressed Shakeel Butt's concern about using macros for zone lock wrappers, explaining that it's necessary to modify the flags variable passed by the caller and maintain consistency with existing locking patterns.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "explained reasoning",
                "acknowledged feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged that the zone lock wrappers are not valuable and agreed to remove them.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "will remove"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, I agree, there is no much value in this wrappers, will remove them,\nthanks!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] mm: allow __GFP_RETRY_MAYFAIL in vmalloc",
          "message_id": "aZ2zPzyoFUUNWdJ7@linux.dev",
          "url": "https://lore.kernel.org/all/aZ2zPzyoFUUNWdJ7@linux.dev/",
          "date": "2026-02-24T14:22:36Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch proposes to allow __GFP_RETRY_MAYFAIL in vmalloc, but reviewers question the need for documentation and suggest alternative approaches.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Michal Hocko",
              "summary": "Pointed out that the patch's purpose is unclear, suggesting that the author should clarify their intentions and consider alternative approaches.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Tue 24-02-26 06:22:15, Shakeel Butt wrote:\n> On Tue, Feb 24, 2026 at 06:03:13AM -0800, Christoph Hellwig wrote:\n> > On Tue, Feb 24, 2026 at 01:22:36PM +0100, Michal Hocko wrote:\n> > > One thing that we could do to improve __GFP_RETRY_MAYFAIL resp.\n> > > __GFP_NORETRY is to use NOWAIT allocation semantic for page table\n> > > allocations as those could be achieved by scoped allocation context.\n> > > This could cause pre-mature failure after the whole bunch of memory has\n> > > already been allocated for the backing pages but considering that page\n> > > table allocations should be more and more rare over system runtime it\n> > > might be just a reasonable workaround. WDYT?\n> > \n> > Why bother?  __GFP_RETRY_MAYFAIL has pretty lose semantics.  Trying\n> > too hard to allocate PTEs is not breaking the overall concept.\n> > \n> \n> One thing __GFP_RETRY_MAYFAIL is very clear about is to not trigger the\n> oom-killer which is not the case for GFP_KERNEL. There are users who explicitly\n> use __GFP_RETRY_MAYFAIL to avoid oom-killer.\n> \n> Mikulas, is that the reason you are using __GFP_RETRY_MAYFAIL in your use-case?\n\nyes https://lore.kernel.org/all/32bd9bed-a939-69c4-696d-f7f9a5fe31d8@redhat.com/T/#u\n-- \nMichal Hocko\nSUSE Labs\n\n",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH RFC 06/15] memcg, swap: reparent the swap entry on swapin if swapout cgroup is dead",
          "message_id": "aZ0oXHNMe7_3P9OT@linux.dev",
          "url": "https://lore.kernel.org/all/aZ0oXHNMe7_3P9OT@linux.dev/",
          "date": "2026-02-24T05:44:16Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song suggested that instead of returning the existing folio if the entry is already cached, the function should return an error code if allocation failed. They also proposed introducing proper wrappers to handle allocation failure in different ways for async swapin and readahead, as well as zswap swap out.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Instead of trying to return the existing folio if the entry is already\ncached, just return an error code if the allocation failed. And\nintroduce proper wrappers that handle the allocation failure in\ndifferent ways.\n\nFor async swapin and readahead, the caller only wants to ensure that a\nswap in read if the allocation succeeded, and for zswap swap out, the\ncaller will just abort if the allocation failed because the entry is\ngone or cached already.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/swap.h       |   3 +-\n mm/swap_state.c | 177 +++++++++++++++++++++++++++++---------------------------\n mm/zswap.c      |  15 ++---\n 3 files changed, 98 insertions(+), 97 deletions(-)\n\ndiff --git a/mm/swap.h b/mm/swap.h\nindex a77016f2423b..ad8b17a93758 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -281,8 +281,7 @@ struct folio *swap_cache_get_folio(swp_entry_t entry);\n void *swap_cache_get_shadow(swp_entry_t entry);\n void swap_cache_del_folio(struct folio *folio);\n struct folio *swap_cache_alloc_folio(swp_entry_t entry, gfp_t gfp_flags,\n-\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx,\n-\t\t\t\t     bool *alloced);\n+\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx);\n /* Below helpers require the caller to lock and pass in the swap cluster. */\n void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \t\t\t    struct folio *folio, swp_entry_t entry);\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 32d9d877bda8..53fa95059012 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -459,41 +459,24 @@ void swap_update_readahead(struct folio *folio, struct vm_area_struct *vma,\n  * All swap slots covered by the folio must have a non-zero swap count.\n  *\n  * Context: Caller must protect the swap device with reference count or locks.\n- * Return: Returns the folio being added on success. Returns the existing folio\n- * if @entry is already cached. Returns NULL if raced with swapin or swapoff.\n+ * Return: 0 if success, error code if failed.\n  */\n-static struct folio *__swap_cache_prepare_and_add(swp_entry_t entry,\n-\t\t\t\t\t\t  struct folio *folio,\n-\t\t\t\t\t\t  gfp_t gfp, bool charged)\n+static int __swap_cache_prepare_and_add(swp_entry_t entry,\n+\t\t\t\t\tstruct folio *folio,\n+\t\t\t\t\tgfp_t gfp, bool charged)\n {\n-\tstruct folio *swapcache = NULL;\n \tvoid *shadow;\n \tint ret;\n \n \t__folio_set_locked(folio);\n \t__folio_set_swapbacked(folio);\n-\tfor (;;) {\n-\t\tret = swap_cache_add_folio(folio, entry, &shadow);\n-\t\tif (!ret)\n-\t\t\tbreak;\n-\n-\t\t/*\n-\t\t * Large order allocation needs special handling on\n-\t\t * race: if a smaller folio exists in cache, swapin needs\n-\t\t * to fallback to order 0, and doing a swap cache lookup\n-\t\t * might return a folio that is irrelevant to the faulting\n-\t\t * entry because @entry is aligned down. Just return NULL.\n-\t\t */\n-\t\tif (ret != -EEXIST || folio_test_large(folio))\n-\t\t\tgoto failed;\n-\n-\t\tswapcache = swap_cache_get_folio(entry);\n-\t\tif (swapcache)\n-\t\t\tgoto failed;\n-\t}\n+\tret = swap_cache_add_folio(folio, entry, &shadow);\n+\tif (ret)\n+\t\tgoto failed;\n \n \tif (!charged && mem_cgroup_swapin_charge_folio(folio, NULL, gfp, entry)) {\n \t\tswap_cache_del_folio(folio);\n+\t\tret = -ENOMEM;\n \t\tgoto failed;\n \t}\n \n@@ -503,11 +486,11 @@ static struct folio *__swap_cache_prepare_and_add(swp_entry_t entry,\n \n \t/* Caller will initiate read into locked folio */\n \tfolio_add_lru(folio);\n-\treturn folio;\n+\treturn 0;\n \n failed:\n \tfolio_unlock(folio);\n-\treturn swapcache;\n+\treturn ret;\n }\n \n /**\n@@ -516,7 +499,6 @@ static struct folio *__swap_cache_prepare_and_add(swp_entry_t entry,\n  * @gfp_mask: memory allocation flags\n  * @mpol: NUMA memory allocation policy to be applied\n  * @ilx: NUMA interleave index, for use only when MPOL_INTERLEAVE\n- * @new_page_allocated: sets true if allocation happened, false otherwise\n  *\n  * Allocate a folio in the swap cache for one swap slot, typically before\n  * doing IO (e.g. swap in or zswap writeback). The swap slot indicated by\n@@ -524,18 +506,40 @@ static struct folio *__swap_cache_prepare_and_add(swp_entry_t entry,\n  * Currently only supports order 0.\n  *\n  * Context: Caller must protect the swap device with reference count or locks.\n- * Return: Returns the existing folio if @entry is cached already. Returns\n- * NULL if failed due to -ENOMEM or @entry have a swap count < 1.\n+ * Return: Returns the folio if allocation succeeded and folio is added to\n+ * swap cache. Returns error code if allocation failed due to race.\n  */\n struct folio *swap_cache_alloc_folio(swp_entry_t entry, gfp_t gfp_mask,\n-\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx,\n-\t\t\t\t     bool *new_page_allocated)\n+\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx)\n+{\n+\tint ret;\n+\tstruct folio *folio;\n+\n+\t/* Allocate a new folio to be added into the swap cache. */\n+\tfolio = folio_alloc_mpol(gfp_mask, 0, mpol, ilx, numa_node_id());\n+\tif (!folio)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\t/*\n+\t * Try add the new folio, it returns NULL if already exist,\n+\t * since folio is order 0.\n+\t */\n+\tret = __swap_cache_prepare_and_add(entry, folio, gfp_mask, false);\n+\tif (ret) {\n+\t\tfolio_put(folio);\n+\t\treturn ERR_PTR(ret);\n+\t}\n+\n+\treturn folio;\n+}\n+\n+static struct folio *swap_cache_read_folio(swp_entry_t entry, gfp_t gfp,\n+\t\t\t\t\t   struct mempolicy *mpol, pgoff_t ilx,\n+\t\t\t\t\t   struct swap_iocb **plug, bool readahead)\n {\n \tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n \tstruct folio *folio;\n-\tstruct folio *result = NULL;\n \n-\t*new_page_allocated = false;\n \t/* Check the swap cache again for readahead path. */\n \tfolio = swap_cache_get_folio(entry);\n \tif (folio)\n@@ -545,17 +549,24 @@ struct folio *swap_cache_alloc_folio(swp_entry_t entry, gfp_t gfp_mask,\n \tif (!swap_entry_swapped(si, entry))\n \t\treturn NULL;\n \n-\t/* Allocate a new folio to be added into the swap cache. */\n-\tfolio = folio_alloc_mpol(gfp_mask, 0, mpol, ilx, numa_node_id());\n-\tif (!folio)\n+\tdo {\n+\t\tfolio = swap_cache_get_folio(entry);\n+\t\tif (folio)\n+\t\t\treturn folio;\n+\n+\t\tfolio = swap_cache_alloc_folio(entry, gfp, mpol, ilx);\n+\t} while (PTR_ERR(folio) == -EEXIST);\n+\n+\tif (IS_ERR_OR_NULL(folio))\n \t\treturn NULL;\n-\t/* Try add the new folio, returns existing folio or NULL on failure. */\n-\tresult = __swap_cache_prepare_and_add(entry, folio, gfp_mask, false);\n-\tif (result == folio)\n-\t\t*new_page_allocated = true;\n-\telse\n-\t\tfolio_put(folio);\n-\treturn result;\n+\n+\tswap_read_folio(folio, plug);\n+\tif (readahead) {\n+\t\tfolio_set_readahead(folio);\n+\t\tcount_vm_event(SWAP_RA);\n+\t}\n+\n+\treturn folio;\n }\n \n /**\n@@ -574,15 +585,35 @@ struct folio *swap_cache_alloc_folio(swp_entry_t entry, gfp_t gfp_mask,\n  */\n struct folio *swapin_folio(swp_entry_t entry, struct folio *folio)\n {\n+\tint ret;\n \tstruct folio *swapcache;\n \tpgoff_t offset = swp_offset(entry);\n \tunsigned long nr_pages = folio_nr_pages(folio);\n \n \tentry = swp_entry(swp_type(entry), round_down(offset, nr_pages));\n-\tswapcache = __swap_cache_prepare_and_add(entry, folio, 0, true);\n-\tif (swapcache == folio)\n-\t\tswap_read_folio(folio, NULL);\n-\treturn swapcache;\n+\tfor (;;) {\n+\t\tret = __swap_cache_prepare_and_add(entry, folio, 0, true);\n+\t\tif (!ret) {\n+\t\t\tswap_read_folio(folio, NULL);\n+\t\t\tbreak;\n+\t\t}\n+\n+\t\t/*\n+\t\t * Large order allocation needs special handling on\n+\t\t * race: if a smaller folio exists in cache, swapin needs\n+\t\t * to fallback to order 0, and doing a swap cache lookup\n+\t\t * might return a folio that is irrelevant to the faulting\n+\t\t * entry because @entry is aligned down. Just return NULL.\n+\t\t */\n+\t\tif (ret != -EEXIST || nr_pages > 1)\n+\t\t\treturn NULL;\n+\n+\t\tswapcache = swap_cache_get_folio(entry);\n+\t\tif (swapcache)\n+\t\t\treturn swapcache;\n+\t}\n+\n+\treturn folio;\n }\n \n /*\n@@ -596,7 +627,6 @@ struct folio *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\tstruct swap_iocb **plug)\n {\n \tstruct swap_info_struct *si;\n-\tbool page_allocated;\n \tstruct mempolicy *mpol;\n \tpgoff_t ilx;\n \tstruct folio *folio;\n@@ -606,13 +636,9 @@ struct folio *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\treturn NULL;\n \n \tmpol = get_vma_policy(vma, addr, 0, &ilx);\n-\tfolio = swap_cache_alloc_folio(entry, gfp_mask, mpol, ilx,\n-\t\t\t\t       &page_allocated);\n+\tfolio = swap_cache_read_folio(entry, gfp_mask, mpol, ilx, plug, false);\n \tmpol_cond_put(mpol);\n \n-\tif (page_allocated)\n-\t\tswap_read_folio(folio, plug);\n-\n \tput_swap_device(si);\n \treturn folio;\n }\n@@ -697,7 +723,7 @@ static unsigned long swapin_nr_pages(unsigned long offset)\n  * are fairly likely to have been swapped out from the same node.\n  */\n struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n-\t\t\t\t    struct mempolicy *mpol, pgoff_t ilx)\n+\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx)\n {\n \tstruct folio *folio;\n \tunsigned long entry_offset = swp_offset(entry);\n@@ -707,7 +733,7 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n \tstruct blk_plug plug;\n \tstruct swap_iocb *splug = NULL;\n-\tbool page_allocated;\n+\tswp_entry_t ra_entry;\n \n \tmask = swapin_nr_pages(offset) - 1;\n \tif (!mask)\n@@ -724,18 +750,11 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \tblk_start_plug(&plug);\n \tfor (offset = start_offset; offset <= end_offset ; offset++) {\n \t\t/* Ok, do the async read-ahead now */\n-\t\tfolio = swap_cache_alloc_folio(\n-\t\t\tswp_entry(swp_type(entry), offset), gfp_mask, mpol, ilx,\n-\t\t\t&page_allocated);\n+\t\tra_entry = swp_entry(swp_type(entry), offset);\n+\t\tfolio = swap_cache_read_folio(ra_entry, gfp_mask, mpol, ilx,\n+\t\t\t\t\t      &splug, offset != entry_offset);\n \t\tif (!folio)\n \t\t\tcontinue;\n-\t\tif (page_allocated) {\n-\t\t\tswap_read_folio(folio, &splug);\n-\t\t\tif (offset != entry_offset) {\n-\t\t\t\tfolio_set_readahead(folio);\n-\t\t\t\tcount_vm_event(SWAP_RA);\n-\t\t\t}\n-\t\t}\n \t\tfolio_put(folio);\n \t}\n \tblk_finish_plug(&plug);\n@@ -743,11 +762,7 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \tlru_add_drain();\t/* Push any new pages onto the LRU now */\n skip:\n \t/* The page was likely read above, so no need for plugging here */\n-\tfolio = swap_cache_alloc_folio(entry, gfp_mask, mpol, ilx,\n-\t\t\t\t       &page_allocated);\n-\tif (unlikely(page_allocated))\n-\t\tswap_read_folio(folio, NULL);\n-\treturn folio;\n+\treturn swap_cache_read_folio(entry, gfp_mask, mpol, ilx, NULL, false);\n }\n \n static int swap_vma_ra_win(struct vm_fault *vmf, unsigned long *start,\n@@ -813,8 +828,7 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \tpte_t *pte = NULL, pentry;\n \tint win;\n \tunsigned long start, end, addr;\n-\tpgoff_t ilx;\n-\tbool page_allocated;\n+\tpgoff_t ilx = targ_ilx;\n \n \twin = swap_vma_ra_win(vmf, &start, &end);\n \tif (win == 1)\n@@ -848,19 +862,12 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \t\t\tif (!si)\n \t\t\t\tcontinue;\n \t\t}\n-\t\tfolio = swap_cache_alloc_folio(entry, gfp_mask, mpol, ilx,\n-\t\t\t\t\t       &page_allocated);\n+\t\tfolio = swap_cache_read_folio(entry, gfp_mask, mpol, ilx,\n+\t\t\t\t\t      &splug, addr != vmf->address);\n \t\tif (si)\n \t\t\tput_swap_device(si);\n \t\tif (!folio)\n \t\t\tcontinue;\n-\t\tif (page_allocated) {\n-\t\t\tswap_read_folio(folio, &splug);\n-\t\t\tif (addr != vmf->address) {\n-\t\t\t\tfolio_set_readahead(folio);\n-\t\t\t\tcount_vm_event(SWAP_RA);\n-\t\t\t}\n-\t\t}\n \t\tfolio_put(folio);\n \t}\n \tif (pte)\n@@ -870,10 +877,8 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \tlru_add_drain();\n skip:\n \t/* The folio was likely read above, so no need for plugging here */\n-\tfolio = swap_cache_alloc_folio(targ_entry, gfp_mask, mpol, targ_ilx,\n-\t\t\t\t       &page_allocated);\n-\tif (unlikely(page_allocated))\n-\t\tswap_read_folio(folio, NULL);\n+\tfolio = swap_cache_read_folio(targ_entry, gfp_mask, mpol, targ_ilx,\n+\t\t\t\t      NULL, false);\n \treturn folio;\n }\n \ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex af3f0fbb0558..f3aa83a99636 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -992,7 +992,6 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \tpgoff_t offset = swp_offset(swpentry);\n \tstruct folio *folio;\n \tstruct mempolicy *mpol;\n-\tbool folio_was_allocated;\n \tstruct swap_info_struct *si;\n \tint ret = 0;\n \n@@ -1003,22 +1002,20 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \n \tmpol = get_task_policy(current);\n \tfolio = swap_cache_alloc_folio(swpentry, GFP_KERNEL, mpol,\n-\t\t\t\t       NO_INTERLEAVE_INDEX, &folio_was_allocated);\n+\t\t\t\t       NO_INTERLEAVE_INDEX);\n \tput_swap_device(si);\n-\tif (!folio)\n-\t\treturn -ENOMEM;\n \n \t/*\n+\t * Swap cache allocaiton might fail due to OOM, raced with free\n+\t * or existing folio when we due to concurrent swapin or free.\n \t * Found an existing folio, we raced with swapin or concurrent\n \t * shrinker. We generally writeback cold folios from zswap, and\n \t * swapin means the folio just became hot, so skip this folio.\n \t * For unlikely concurrent shrinker case, it will be unlinked\n \t * and freed when invalidated by the concurrent shrinker anyway.\n \t */\n-\tif (!folio_was_allocated) {\n-\t\tret = -EEXIST;\n-\t\tgoto out;\n-\t}\n+\tif (IS_ERR(folio))\n+\t\treturn PTR_ERR(folio);\n \n \t/*\n \t * folio is locked, and the swapcache is now secured against\n@@ -1058,7 +1055,7 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \t__swap_writepage(folio, NULL);\n \n out:\n-\tif (ret && ret != -EEXIST) {\n+\tif (ret) {\n \t\tswap_cache_del_folio(folio);\n \t\tfolio_unlock(folio);\n \t}\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "The reviewer noted that the patch introduces a new function `thp_limit_gfp_mask` in `huge_mm.h`, which is used to limit the GFP flags for huge page allocations. However, they pointed out that this function is identical to an existing function `limit_gfp_mask` in `shmem.c`, and suggested removing the duplicate code.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "duplicate code",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No feature change, to be used later.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n include/linux/huge_mm.h | 24 ++++++++++++++++++++++++\n mm/shmem.c              | 30 +++---------------------------\n 2 files changed, 27 insertions(+), 27 deletions(-)\n\ndiff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h\nindex a4d9f964dfde..d522e798822d 100644\n--- a/include/linux/huge_mm.h\n+++ b/include/linux/huge_mm.h\n@@ -237,6 +237,30 @@ static inline bool thp_vma_suitable_order(struct vm_area_struct *vma,\n \treturn true;\n }\n \n+/*\n+ * Make sure huge_gfp is always more limited than limit_gfp.\n+ * Some of the flags set permissions, while others set limitations.\n+ */\n+static inline gfp_t thp_limit_gfp_mask(gfp_t huge_gfp, gfp_t limit_gfp)\n+{\n+\tgfp_t allowflags = __GFP_IO | __GFP_FS | __GFP_RECLAIM;\n+\tgfp_t denyflags = __GFP_NOWARN | __GFP_NORETRY;\n+\tgfp_t zoneflags = limit_gfp & GFP_ZONEMASK;\n+\tgfp_t result = huge_gfp & ~(allowflags | GFP_ZONEMASK);\n+\n+\t/* Allow allocations only from the originally specified zones. */\n+\tresult |= zoneflags;\n+\n+\t/*\n+\t * Minimize the result gfp by taking the union with the deny flags,\n+\t * and the intersection of the allow flags.\n+\t */\n+\tresult |= (limit_gfp & denyflags);\n+\tresult |= (huge_gfp & limit_gfp) & allowflags;\n+\n+\treturn result;\n+}\n+\n /*\n  * Filter the bitfield of input orders to the ones suitable for use in the vma.\n  * See thp_vma_suitable_order().\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex b976b40fd442..9f054b5aae8e 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -1788,30 +1788,6 @@ static struct folio *shmem_swapin_cluster(swp_entry_t swap, gfp_t gfp,\n \treturn folio;\n }\n \n-/*\n- * Make sure huge_gfp is always more limited than limit_gfp.\n- * Some of the flags set permissions, while others set limitations.\n- */\n-static gfp_t limit_gfp_mask(gfp_t huge_gfp, gfp_t limit_gfp)\n-{\n-\tgfp_t allowflags = __GFP_IO | __GFP_FS | __GFP_RECLAIM;\n-\tgfp_t denyflags = __GFP_NOWARN | __GFP_NORETRY;\n-\tgfp_t zoneflags = limit_gfp & GFP_ZONEMASK;\n-\tgfp_t result = huge_gfp & ~(allowflags | GFP_ZONEMASK);\n-\n-\t/* Allow allocations only from the originally specified zones. */\n-\tresult |= zoneflags;\n-\n-\t/*\n-\t * Minimize the result gfp by taking the union with the deny flags,\n-\t * and the intersection of the allow flags.\n-\t */\n-\tresult |= (limit_gfp & denyflags);\n-\tresult |= (huge_gfp & limit_gfp) & allowflags;\n-\n-\treturn result;\n-}\n-\n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n bool shmem_hpage_pmd_enabled(void)\n {\n@@ -2062,7 +2038,7 @@ static struct folio *shmem_swap_alloc_folio(struct inode *inode,\n \t\t     non_swapcache_batch(entry, nr_pages) != nr_pages)\n \t\t\tgoto fallback;\n \n-\t\talloc_gfp = limit_gfp_mask(vma_thp_gfp_mask(vma), gfp);\n+\t\talloc_gfp = thp_limit_gfp_mask(vma_thp_gfp_mask(vma), gfp);\n \t}\n retry:\n \tnew = shmem_alloc_folio(alloc_gfp, order, info, index);\n@@ -2138,7 +2114,7 @@ static int shmem_replace_folio(struct folio **foliop, gfp_t gfp,\n \tif (nr_pages > 1) {\n \t\tgfp_t huge_gfp = vma_thp_gfp_mask(vma);\n \n-\t\tgfp = limit_gfp_mask(huge_gfp, gfp);\n+\t\tgfp = thp_limit_gfp_mask(huge_gfp, gfp);\n \t}\n #endif\n \n@@ -2545,7 +2521,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,\n \t\tgfp_t huge_gfp;\n \n \t\thuge_gfp = vma_thp_gfp_mask(vma);\n-\t\thuge_gfp = limit_gfp_mask(huge_gfp, gfp);\n+\t\thuge_gfp = thp_limit_gfp_mask(huge_gfp, gfp);\n \t\tfolio = shmem_alloc_and_add_folio(vmf, huge_gfp,\n \t\t\t\tinode, index, fault_mm, orders);\n \t\tif (!IS_ERR(folio)) {\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "The reviewer requested that the patch be split into smaller, more manageable commits to make it easier for others to review.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No feature change, make later commits easier to review.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/swap_state.c | 55 ++++++++++++++++++++++++++++++-------------------------\n 1 file changed, 30 insertions(+), 25 deletions(-)\n\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 53fa95059012..1e340faea9ac 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -137,6 +137,28 @@ void *swap_cache_get_shadow(swp_entry_t entry)\n \treturn NULL;\n }\n \n+static int __swap_cache_add_check(struct swap_cluster_info *ci,\n+\t\t\t\t  unsigned int ci_off, unsigned int nr,\n+\t\t\t\t  void **shadow)\n+{\n+\tunsigned int ci_end = ci_off + nr;\n+\tunsigned long old_tb;\n+\n+\tif (unlikely(!ci->table))\n+\t\treturn -ENOENT;\n+\tdo {\n+\t\told_tb = __swap_table_get(ci, ci_off);\n+\t\tif (unlikely(swp_tb_is_folio(old_tb)))\n+\t\t\treturn -EEXIST;\n+\t\tif (unlikely(!__swp_tb_get_count(old_tb)))\n+\t\t\treturn -ENOENT;\n+\t\tif (swp_tb_is_shadow(old_tb))\n+\t\t\t*shadow = swp_tb_to_shadow(old_tb);\n+\t} while (++ci_off < ci_end);\n+\n+\treturn 0;\n+}\n+\n void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \t\t\t    struct folio *folio, swp_entry_t entry)\n {\n@@ -179,43 +201,26 @@ static int swap_cache_add_folio(struct folio *folio, swp_entry_t entry,\n {\n \tint err;\n \tvoid *shadow = NULL;\n-\tunsigned long old_tb;\n+\tunsigned int ci_off;\n \tstruct swap_info_struct *si;\n \tstruct swap_cluster_info *ci;\n-\tunsigned int ci_start, ci_off, ci_end;\n \tunsigned long nr_pages = folio_nr_pages(folio);\n \n \tsi = __swap_entry_to_info(entry);\n-\tci_start = swp_cluster_offset(entry);\n-\tci_end = ci_start + nr_pages;\n-\tci_off = ci_start;\n \tci = swap_cluster_lock(si, swp_offset(entry));\n-\tif (unlikely(!ci->table)) {\n-\t\terr = -ENOENT;\n-\t\tgoto failed;\n+\tci_off = swp_cluster_offset(entry);\n+\terr = __swap_cache_add_check(ci, ci_off, nr_pages, &shadow);\n+\tif (err) {\n+\t\tswap_cluster_unlock(ci);\n+\t\treturn err;\n \t}\n-\tdo {\n-\t\told_tb = __swap_table_get(ci, ci_off);\n-\t\tif (unlikely(swp_tb_is_folio(old_tb))) {\n-\t\t\terr = -EEXIST;\n-\t\t\tgoto failed;\n-\t\t}\n-\t\tif (unlikely(!__swp_tb_get_count(old_tb))) {\n-\t\t\terr = -ENOENT;\n-\t\t\tgoto failed;\n-\t\t}\n-\t\tif (swp_tb_is_shadow(old_tb))\n-\t\t\tshadow = swp_tb_to_shadow(old_tb);\n-\t} while (++ci_off < ci_end);\n+\n \t__swap_cache_add_folio(ci, folio, entry);\n \tswap_cluster_unlock(ci);\n \tif (shadowp)\n \t\t*shadowp = shadow;\n-\treturn 0;\n \n-failed:\n-\tswap_cluster_unlock(ci);\n-\treturn err;\n+\treturn 0;\n }\n \n /**\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song suggested modifying swap_cache_alloc_folio to handle larger orders, which would allow for direct allocation of large folios in the swap cache. This change also adjusts synchronization and fallback processes to make them less racy.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "To make it possible to allocate large folios directly in swap cache, let\nswap_cache_alloc_folio handle larger orders too.\n\nThis slightly changes how allocation is synchronized. Now, whoever first\nsuccessfully allocates a folio in the swap cache will be the one who\ncharges it and performs the swap-in. Raced swapin now should avoid a\nredundant charge and just wait for the swapin to finish.\n\nLarge order fallback is also moved to the swap cache layer. This should\nmake the fallback process less racy, too.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/swap.h       |   3 +-\n mm/swap_state.c | 193 +++++++++++++++++++++++++++++++++++++++++---------------\n mm/zswap.c      |   2 +-\n 3 files changed, 145 insertions(+), 53 deletions(-)\n\ndiff --git a/mm/swap.h b/mm/swap.h\nindex ad8b17a93758..6774af10a943 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -280,7 +280,8 @@ bool swap_cache_has_folio(swp_entry_t entry);\n struct folio *swap_cache_get_folio(swp_entry_t entry);\n void *swap_cache_get_shadow(swp_entry_t entry);\n void swap_cache_del_folio(struct folio *folio);\n-struct folio *swap_cache_alloc_folio(swp_entry_t entry, gfp_t gfp_flags,\n+struct folio *swap_cache_alloc_folio(swp_entry_t target_entry, gfp_t gfp_mask,\n+\t\t\t\t     unsigned long orders, struct vm_fault *vmf,\n \t\t\t\t     struct mempolicy *mpol, pgoff_t ilx);\n /* Below helpers require the caller to lock and pass in the swap cluster. */\n void __swap_cache_add_folio(struct swap_cluster_info *ci,\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 1e340faea9ac..e32b06a1f229 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -137,26 +137,39 @@ void *swap_cache_get_shadow(swp_entry_t entry)\n \treturn NULL;\n }\n \n-static int __swap_cache_add_check(struct swap_cluster_info *ci,\n-\t\t\t\t  unsigned int ci_off, unsigned int nr,\n-\t\t\t\t  void **shadow)\n+static int __swap_cache_check_batch(struct swap_cluster_info *ci,\n+\t\t\t\t    unsigned int ci_off, unsigned int ci_targ,\n+\t\t\t\t    unsigned int nr, void **shadowp)\n {\n \tunsigned int ci_end = ci_off + nr;\n \tunsigned long old_tb;\n \n \tif (unlikely(!ci->table))\n \t\treturn -ENOENT;\n+\n \tdo {\n \t\told_tb = __swap_table_get(ci, ci_off);\n-\t\tif (unlikely(swp_tb_is_folio(old_tb)))\n-\t\t\treturn -EEXIST;\n-\t\tif (unlikely(!__swp_tb_get_count(old_tb)))\n-\t\t\treturn -ENOENT;\n+\t\tif (unlikely(swp_tb_is_folio(old_tb)) ||\n+\t\t    unlikely(!__swp_tb_get_count(old_tb)))\n+\t\t\tbreak;\n \t\tif (swp_tb_is_shadow(old_tb))\n-\t\t\t*shadow = swp_tb_to_shadow(old_tb);\n+\t\t\t*shadowp = swp_tb_to_shadow(old_tb);\n \t} while (++ci_off < ci_end);\n \n-\treturn 0;\n+\tif (likely(ci_off == ci_end))\n+\t\treturn 0;\n+\n+\t/*\n+\t * If the target slot is not suitable for adding swap cache, return\n+\t * -EEXIST or -ENOENT. If the batch is not suitable, could be a\n+\t * race with concurrent free or cache add, return -EBUSY.\n+\t */\n+\told_tb = __swap_table_get(ci, ci_targ);\n+\tif (swp_tb_is_folio(old_tb))\n+\t\treturn -EEXIST;\n+\tif (!__swp_tb_get_count(old_tb))\n+\t\treturn -ENOENT;\n+\treturn -EBUSY;\n }\n \n void __swap_cache_add_folio(struct swap_cluster_info *ci,\n@@ -209,7 +222,7 @@ static int swap_cache_add_folio(struct folio *folio, swp_entry_t entry,\n \tsi = __swap_entry_to_info(entry);\n \tci = swap_cluster_lock(si, swp_offset(entry));\n \tci_off = swp_cluster_offset(entry);\n-\terr = __swap_cache_add_check(ci, ci_off, nr_pages, &shadow);\n+\terr = __swap_cache_check_batch(ci, ci_off, ci_off, nr_pages, &shadow);\n \tif (err) {\n \t\tswap_cluster_unlock(ci);\n \t\treturn err;\n@@ -223,6 +236,124 @@ static int swap_cache_add_folio(struct folio *folio, swp_entry_t entry,\n \treturn 0;\n }\n \n+static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n+\t\t\t\t\tswp_entry_t targ_entry, gfp_t gfp,\n+\t\t\t\t\tunsigned int order, struct vm_fault *vmf,\n+\t\t\t\t\tstruct mempolicy *mpol, pgoff_t ilx)\n+{\n+\tint err;\n+\tswp_entry_t entry;\n+\tstruct folio *folio;\n+\tvoid *shadow = NULL, *shadow_check = NULL;\n+\tunsigned long address, nr_pages = 1 << order;\n+\tunsigned int ci_off, ci_targ = swp_cluster_offset(targ_entry);\n+\n+\tentry.val = round_down(targ_entry.val, nr_pages);\n+\tci_off = round_down(ci_targ, nr_pages);\n+\n+\t/* First check if the range is available */\n+\tspin_lock(&ci->lock);\n+\terr = __swap_cache_check_batch(ci, ci_off, ci_targ, nr_pages, &shadow);\n+\tspin_unlock(&ci->lock);\n+\tif (unlikely(err))\n+\t\treturn ERR_PTR(err);\n+\n+\tif (vmf) {\n+\t\tif (order)\n+\t\t\tgfp = thp_limit_gfp_mask(vma_thp_gfp_mask(vmf->vma), gfp);\n+\t\taddress = round_down(vmf->address, PAGE_SIZE << order);\n+\t\tfolio = vma_alloc_folio(gfp, order, vmf->vma, address);\n+\t} else {\n+\t\tfolio = folio_alloc_mpol(gfp, order, mpol, ilx, numa_node_id());\n+\t}\n+\tif (unlikely(!folio))\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\t/* Double check the range is still not in conflict */\n+\tspin_lock(&ci->lock);\n+\terr = __swap_cache_check_batch(ci, ci_off, ci_targ, nr_pages, &shadow_check);\n+\tif (unlikely(err) || shadow_check != shadow) {\n+\t\tspin_unlock(&ci->lock);\n+\t\tfolio_put(folio);\n+\n+\t\t/* If shadow changed, just try again */\n+\t\treturn ERR_PTR(err ? err : -EAGAIN);\n+\t}\n+\n+\t__folio_set_locked(folio);\n+\t__folio_set_swapbacked(folio);\n+\t__swap_cache_add_folio(ci, folio, entry);\n+\tspin_unlock(&ci->lock);\n+\n+\tif (mem_cgroup_swapin_charge_folio(folio, vmf ? vmf->vma->vm_mm : NULL,\n+\t\t\t\t\t   gfp, entry)) {\n+\t\tspin_lock(&ci->lock);\n+\t\t__swap_cache_del_folio(ci, folio, shadow);\n+\t\tspin_unlock(&ci->lock);\n+\t\tfolio_unlock(folio);\n+\t\tfolio_put(folio);\n+\t\tcount_mthp_stat(order, MTHP_STAT_SWPIN_FALLBACK_CHARGE);\n+\t\treturn ERR_PTR(-ENOMEM);\n+\t}\n+\n+\t/* For memsw accouting, swap is uncharged when folio is added to swap cache */\n+\tmemcg1_swapin(entry, 1 << order);\n+\tif (shadow)\n+\t\tworkingset_refault(folio, shadow);\n+\n+\t/* Caller will initiate read into locked new_folio */\n+\tfolio_add_lru(folio);\n+\n+\treturn folio;\n+}\n+\n+/**\n+ * swap_cache_alloc_folio - Allocate folio for swapped out slot in swap cache.\n+ * @targ_entry: swap entry indicating the target slot\n+ * @orders: allocation orders\n+ * @vmf: fault information\n+ * @gfp_mask: memory allocation flags\n+ * @mpol: NUMA memory allocation policy to be applied\n+ * @ilx: NUMA interleave index, for use only when MPOL_INTERLEAVE\n+ *\n+ * Allocate a folio in the swap cache for one swap slot, typically before\n+ * doing IO (e.g. swap in or zswap writeback). The swap slot indicated by\n+ * @targ_entry must have a non-zero swap count (swapped out).\n+ *\n+ * Context: Caller must protect the swap device with reference count or locks.\n+ * Return: Returns the folio if allocation successed and folio is added to\n+ * swap cache. Returns error code if allocation failed due to race.\n+ */\n+struct folio *swap_cache_alloc_folio(swp_entry_t targ_entry, gfp_t gfp_mask,\n+\t\t\t\t     unsigned long orders, struct vm_fault *vmf,\n+\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx)\n+{\n+\tint order;\n+\tstruct folio *folio;\n+\tstruct swap_cluster_info *ci;\n+\n+\tci = __swap_entry_to_cluster(targ_entry);\n+\torder = orders ? highest_order(orders) : 0;\n+\tfor (;;) {\n+\t\tfolio = __swap_cache_alloc(ci, targ_entry, gfp_mask, order,\n+\t\t\t\t\t   vmf, mpol, ilx);\n+\t\tif (!IS_ERR(folio))\n+\t\t\treturn folio;\n+\t\tif (PTR_ERR(folio) == -EAGAIN)\n+\t\t\tcontinue;\n+\t\t/* Only -EBUSY means we should fallback and retry. */\n+\t\tif (PTR_ERR(folio) != -EBUSY)\n+\t\t\treturn folio;\n+\t\tcount_mthp_stat(order, MTHP_STAT_SWPIN_FALLBACK);\n+\t\torder = next_order(&orders, order);\n+\t\tif (!orders)\n+\t\t\tbreak;\n+\t}\n+\t/* Should never reach here, order 0 should not fail with -EBUSY. */\n+\tWARN_ON_ONCE(1);\n+\treturn ERR_PTR(-EINVAL);\n+}\n+\n /**\n  * __swap_cache_del_folio - Removes a folio from the swap cache.\n  * @ci: The locked swap cluster.\n@@ -498,46 +629,6 @@ static int __swap_cache_prepare_and_add(swp_entry_t entry,\n \treturn ret;\n }\n \n-/**\n- * swap_cache_alloc_folio - Allocate folio for swapped out slot in swap cache.\n- * @entry: the swapped out swap entry to be binded to the folio.\n- * @gfp_mask: memory allocation flags\n- * @mpol: NUMA memory allocation policy to be applied\n- * @ilx: NUMA interleave index, for use only when MPOL_INTERLEAVE\n- *\n- * Allocate a folio in the swap cache for one swap slot, typically before\n- * doing IO (e.g. swap in or zswap writeback). The swap slot indicated by\n- * @entry must have a non-zero swap count (swapped out).\n- * Currently only supports order 0.\n- *\n- * Context: Caller must protect the swap device with reference count or locks.\n- * Return: Returns the folio if allocation succeeded and folio is added to\n- * swap cache. Returns error code if allocation failed due to race.\n- */\n-struct folio *swap_cache_alloc_folio(swp_entry_t entry, gfp_t gfp_mask,\n-\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx)\n-{\n-\tint ret;\n-\tstruct folio *folio;\n-\n-\t/* Allocate a new folio to be added into the swap cache. */\n-\tfolio = folio_alloc_mpol(gfp_mask, 0, mpol, ilx, numa_node_id());\n-\tif (!folio)\n-\t\treturn ERR_PTR(-ENOMEM);\n-\n-\t/*\n-\t * Try add the new folio, it returns NULL if already exist,\n-\t * since folio is order 0.\n-\t */\n-\tret = __swap_cache_prepare_and_add(entry, folio, gfp_mask, false);\n-\tif (ret) {\n-\t\tfolio_put(folio);\n-\t\treturn ERR_PTR(ret);\n-\t}\n-\n-\treturn folio;\n-}\n-\n static struct folio *swap_cache_read_folio(swp_entry_t entry, gfp_t gfp,\n \t\t\t\t\t   struct mempolicy *mpol, pgoff_t ilx,\n \t\t\t\t\t   struct swap_iocb **plug, bool readahead)\n@@ -559,7 +650,7 @@ static struct folio *swap_cache_read_folio(swp_entry_t entry, gfp_t gfp,\n \t\tif (folio)\n \t\t\treturn folio;\n \n-\t\tfolio = swap_cache_alloc_folio(entry, gfp, mpol, ilx);\n+\t\tfolio = swap_cache_alloc_folio(entry, gfp, 0, NULL, mpol, ilx);\n \t} while (PTR_ERR(folio) == -EEXIST);\n \n \tif (IS_ERR_OR_NULL(folio))\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex f3aa83a99636..5d83539a8bba 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -1001,7 +1001,7 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \t\treturn -EEXIST;\n \n \tmpol = get_task_policy(current);\n-\tfolio = swap_cache_alloc_folio(swpentry, GFP_KERNEL, mpol,\n+\tfolio = swap_cache_alloc_folio(swpentry, GFP_KERNEL, 0, NULL, mpol,\n \t\t\t\t       NO_INTERLEAVE_INDEX);\n \tput_swap_device(si);\n \n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "The reviewer noted that the patch always charges swapin folios into the dead cgroup's parent cgroup, which can lead to a situation where the folio->swap entry belongs to a cgroup that is not folio->memcg. The reviewer suggested several possible solutions, including dynamically allocating a swap cluster trampoline cgroup table and tolerating a 2-byte per slot overhead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "As a result this will always charge the swapin folio into the dead\ncgroup's parent cgroup, and ensure folio->swap belongs to folio_memcg.\nThis only affects some uncommon behavior if we move the process between\nmemcg.\n\nWhen a process that previously swapped some memory is moved to another\ncgroup, and the cgroup where the swap occurred is dead, folios for\nswap in of old swap entries will be charged into the new cgroup.\nCombined with the lazy freeing of swap cache, this leads to a strange\nsituation where the folio->swap entry belongs to a cgroup that is not\nfolio->memcg.\n\nSwapin from dead zombie memcg might be rare in practise, cgroups are\nofflined only after the workload in it is gone, which requires zapping\nthe page table first, and releases all swap entries. Shmem is\na bit different, but shmem always has swap count == 1, and force\nreleases the swap cache. So, for shmem charging into the new memcg and\nrelease entry does look more sensible.\n\nHowever, to make things easier to understand for an RFC, let's just\nalways charge to the parent cgroup if the leaf cgroup is dead. This may\nnot be the best design, but it makes the following work much easier to\ndemonstrate.\n\nFor a better solution, we can later:\n\n- Dynamically allocate a swap cluster trampoline cgroup table\n  (ci->memcg_table) and use that for zombie swapin only. Which is\n  actually OK and may not cause a mess in the code level, since the\n  incoming swap table compaction will require table expansion on swap-in\n  as well.\n\n- Just tolerate a 2-byte per slot overhead all the time, which is also\n  acceptable.\n\n- Limit the charge to parent behavior to only one situation: when the\n  swap count > 2 and the process is migrated to another cgroup after\n  swapout, these entries. This is even more rare to see in practice, I\n  think.\n\nFor reference, the memory ownership model of cgroup v2:\n\n\"\"\"\nA memory area is charged to the cgroup which instantiated it and stays\ncharged to the cgroup until the area is released.  Migrating a process\nto a different cgroup doesn't move the memory usages that it\ninstantiated while in the previous cgroup to the new cgroup.\n\nA memory area may be used by processes belonging to different cgroups.\nTo which cgroup the area will be charged is in-deterministic; however,\nover time, the memory area is likely to end up in a cgroup which has\nenough memory allowance to avoid high reclaim pressure.\n\nIf a cgroup sweeps a considerable amount of memory which is expected\nto be accessed repeatedly by other cgroups, it may make sense to use\nPOSIX_FADV_DONTNEED to relinquish the ownership of memory areas\nbelonging to the affected files to ensure correct memory ownership.\n\"\"\"\n\nSo I think all of the solutions mentioned above, including this commit,\nare not wrong.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/memcontrol.c | 53 +++++++++++++++++++++++++++++++++++++++++++++++++----\n 1 file changed, 49 insertions(+), 4 deletions(-)\n\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 73f622f7a72b..b2898719e935 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -4803,22 +4803,67 @@ int mem_cgroup_charge_hugetlb(struct folio *folio, gfp_t gfp)\n int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n \t\t\t\t  gfp_t gfp, swp_entry_t entry)\n {\n-\tstruct mem_cgroup *memcg;\n-\tunsigned short id;\n+\tstruct mem_cgroup *memcg, *swap_memcg;\n+\tunsigned short id, parent_id;\n+\tunsigned int nr_pages;\n \tint ret;\n \n \tif (mem_cgroup_disabled())\n \t\treturn 0;\n \n \tid = lookup_swap_cgroup_id(entry);\n+\tnr_pages = folio_nr_pages(folio);\n+\n \trcu_read_lock();\n-\tmemcg = mem_cgroup_from_private_id(id);\n-\tif (!memcg || !css_tryget_online(&memcg->css))\n+\tswap_memcg = mem_cgroup_from_private_id(id);\n+\tif (!swap_memcg) {\n+\t\tWARN_ON_ONCE(id);\n \t\tmemcg = get_mem_cgroup_from_mm(mm);\n+\t} else {\n+\t\tmemcg = swap_memcg;\n+\t\t/* Find the nearest online ancestor if dead, for reparent */\n+\t\twhile (!css_tryget_online(&memcg->css))\n+\t\t\tmemcg = parent_mem_cgroup(memcg);\n+\t}\n \trcu_read_unlock();\n \n \tret = charge_memcg(folio, memcg, gfp);\n+\tif (ret)\n+\t\tgoto out;\n+\n+\t/*\n+\t * If the swap entry's memcg is dead, reparent the swap charge\n+\t * from swap_memcg to memcg.\n+\t *\n+\t * If memcg is also being offlined, the charge will be moved to\n+\t * its parent again.\n+\t */\n+\tif (swap_memcg && memcg != swap_memcg) {\n+\t\tstruct mem_cgroup *parent_memcg;\n \n+\t\tparent_memcg = mem_cgroup_private_id_get_online(memcg, nr_pages);\n+\t\tparent_id = mem_cgroup_private_id(parent_memcg);\n+\n+\t\tWARN_ON(id != swap_cgroup_clear(entry, nr_pages));\n+\t\tswap_cgroup_record(folio, parent_id, entry);\n+\n+\t\tif (do_memsw_account()) {\n+\t\t\tif (!mem_cgroup_is_root(parent_memcg))\n+\t\t\t\tpage_counter_charge(&parent_memcg->memsw, nr_pages);\n+\t\t\tpage_counter_uncharge(&swap_memcg->memsw, nr_pages);\n+\t\t} else {\n+\t\t\tif (!mem_cgroup_is_root(parent_memcg))\n+\t\t\t\tpage_counter_charge(&parent_memcg->swap, nr_pages);\n+\t\t\tpage_counter_uncharge(&swap_memcg->swap, nr_pages);\n+\t\t}\n+\n+\t\tmod_memcg_state(parent_memcg, MEMCG_SWAP, nr_pages);\n+\t\tmod_memcg_state(swap_memcg, MEMCG_SWAP, -nr_pages);\n+\n+\t\t/* Release the dead cgroup after reparent */\n+\t\tmem_cgroup_private_id_put(swap_memcg, nr_pages);\n+\t}\n+out:\n \tcss_put(&memcg->css);\n \treturn ret;\n }\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "The reviewer noted that the patch introduces a potential deadlock between swap_free() and reclaim paths due to the acquisition of the per-vswap spinlock while holding the folio lock, and requested the lock be dropped before calling try_to_unmap().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential deadlock",
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "Now the large order allocation is supported in swap cache, making both\nanon and shmem use this instead of implementing their own different\nmethod for doing so.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/memory.c     |  77 +++++---------------------\n mm/shmem.c      |  94 ++++++++------------------------\n mm/swap.h       |  30 ++---------\n mm/swap_state.c | 163 ++++++++++++--------------------------------------------\n mm/swapfile.c   |   3 +-\n 5 files changed, 76 insertions(+), 291 deletions(-)\n\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 21bf2517fbce..e58f976508b3 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -4520,26 +4520,6 @@ static vm_fault_t handle_pte_marker(struct vm_fault *vmf)\n \treturn VM_FAULT_SIGBUS;\n }\n \n-static struct folio *__alloc_swap_folio(struct vm_fault *vmf)\n-{\n-\tstruct vm_area_struct *vma = vmf->vma;\n-\tstruct folio *folio;\n-\tsoftleaf_t entry;\n-\n-\tfolio = vma_alloc_folio(GFP_HIGHUSER_MOVABLE, 0, vma, vmf->address);\n-\tif (!folio)\n-\t\treturn NULL;\n-\n-\tentry = softleaf_from_pte(vmf->orig_pte);\n-\tif (mem_cgroup_swapin_charge_folio(folio, vma->vm_mm,\n-\t\t\t\t\t   GFP_KERNEL, entry)) {\n-\t\tfolio_put(folio);\n-\t\treturn NULL;\n-\t}\n-\n-\treturn folio;\n-}\n-\n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n /*\n  * Check if the PTEs within a range are contiguous swap entries\n@@ -4569,8 +4549,6 @@ static bool can_swapin_thp(struct vm_fault *vmf, pte_t *ptep, int nr_pages)\n \t */\n \tif (unlikely(swap_zeromap_batch(entry, nr_pages, NULL) != nr_pages))\n \t\treturn false;\n-\tif (unlikely(non_swapcache_batch(entry, nr_pages) != nr_pages))\n-\t\treturn false;\n \n \treturn true;\n }\n@@ -4598,16 +4576,14 @@ static inline unsigned long thp_swap_suitable_orders(pgoff_t swp_offset,\n \treturn orders;\n }\n \n-static struct folio *alloc_swap_folio(struct vm_fault *vmf)\n+static unsigned long thp_swapin_suiltable_orders(struct vm_fault *vmf)\n {\n \tstruct vm_area_struct *vma = vmf->vma;\n \tunsigned long orders;\n-\tstruct folio *folio;\n \tunsigned long addr;\n \tsoftleaf_t entry;\n \tspinlock_t *ptl;\n \tpte_t *pte;\n-\tgfp_t gfp;\n \tint order;\n \n \t/*\n@@ -4615,7 +4591,7 @@ static struct folio *alloc_swap_folio(struct vm_fault *vmf)\n \t * maintain the uffd semantics.\n \t */\n \tif (unlikely(userfaultfd_armed(vma)))\n-\t\tgoto fallback;\n+\t\treturn 0;\n \n \t/*\n \t * A large swapped out folio could be partially or fully in zswap. We\n@@ -4623,7 +4599,7 @@ static struct folio *alloc_swap_folio(struct vm_fault *vmf)\n \t * folio.\n \t */\n \tif (!zswap_never_enabled())\n-\t\tgoto fallback;\n+\t\treturn 0;\n \n \tentry = softleaf_from_pte(vmf->orig_pte);\n \t/*\n@@ -4637,12 +4613,12 @@ static struct folio *alloc_swap_folio(struct vm_fault *vmf)\n \t\t\t\t\t  vmf->address, orders);\n \n \tif (!orders)\n-\t\tgoto fallback;\n+\t\treturn 0;\n \n \tpte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd,\n \t\t\t\t  vmf->address & PMD_MASK, &ptl);\n \tif (unlikely(!pte))\n-\t\tgoto fallback;\n+\t\treturn 0;\n \n \t/*\n \t * For do_swap_page, find the highest order where the aligned range is\n@@ -4658,29 +4634,12 @@ static struct folio *alloc_swap_folio(struct vm_fault *vmf)\n \n \tpte_unmap_unlock(pte, ptl);\n \n-\t/* Try allocating the highest of the remaining orders. */\n-\tgfp = vma_thp_gfp_mask(vma);\n-\twhile (orders) {\n-\t\taddr = ALIGN_DOWN(vmf->address, PAGE_SIZE << order);\n-\t\tfolio = vma_alloc_folio(gfp, order, vma, addr);\n-\t\tif (folio) {\n-\t\t\tif (!mem_cgroup_swapin_charge_folio(folio, vma->vm_mm,\n-\t\t\t\t\t\t\t    gfp, entry))\n-\t\t\t\treturn folio;\n-\t\t\tcount_mthp_stat(order, MTHP_STAT_SWPIN_FALLBACK_CHARGE);\n-\t\t\tfolio_put(folio);\n-\t\t}\n-\t\tcount_mthp_stat(order, MTHP_STAT_SWPIN_FALLBACK);\n-\t\torder = next_order(&orders, order);\n-\t}\n-\n-fallback:\n-\treturn __alloc_swap_folio(vmf);\n+\treturn orders;\n }\n #else /* !CONFIG_TRANSPARENT_HUGEPAGE */\n-static struct folio *alloc_swap_folio(struct vm_fault *vmf)\n+static unsigned long thp_swapin_suiltable_orders(struct vm_fault *vmf)\n {\n-\treturn __alloc_swap_folio(vmf);\n+\treturn 0;\n }\n #endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n \n@@ -4785,21 +4744,13 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \tif (folio)\n \t\tswap_update_readahead(folio, vma, vmf->address);\n \tif (!folio) {\n-\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO)) {\n-\t\t\tfolio = alloc_swap_folio(vmf);\n-\t\t\tif (folio) {\n-\t\t\t\t/*\n-\t\t\t\t * folio is charged, so swapin can only fail due\n-\t\t\t\t * to raced swapin and return NULL.\n-\t\t\t\t */\n-\t\t\t\tswapcache = swapin_folio(entry, folio);\n-\t\t\t\tif (swapcache != folio)\n-\t\t\t\t\tfolio_put(folio);\n-\t\t\t\tfolio = swapcache;\n-\t\t\t}\n-\t\t} else {\n+\t\t/* Swapin bypass readahead for SWP_SYNCHRONOUS_IO devices */\n+\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO))\n+\t\t\tfolio = swapin_entry(entry, GFP_HIGHUSER_MOVABLE,\n+\t\t\t\t\t     thp_swapin_suiltable_orders(vmf),\n+\t\t\t\t\t     vmf, NULL, 0);\n+\t\telse\n \t\t\tfolio = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE, vmf);\n-\t\t}\n \n \t\tif (!folio) {\n \t\t\t/*\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 9f054b5aae8e..0a19ac82ec77 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -159,7 +159,7 @@ static unsigned long shmem_default_max_inodes(void)\n \n static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \t\t\tstruct folio **foliop, enum sgp_type sgp, gfp_t gfp,\n-\t\t\tstruct vm_area_struct *vma, vm_fault_t *fault_type);\n+\t\t\tstruct vm_fault *vmf, vm_fault_t *fault_type);\n \n static inline struct shmem_sb_info *SHMEM_SB(struct super_block *sb)\n {\n@@ -2014,68 +2014,24 @@ static struct folio *shmem_alloc_and_add_folio(struct vm_fault *vmf,\n }\n \n static struct folio *shmem_swap_alloc_folio(struct inode *inode,\n-\t\tstruct vm_area_struct *vma, pgoff_t index,\n+\t\tstruct vm_fault *vmf, pgoff_t index,\n \t\tswp_entry_t entry, int order, gfp_t gfp)\n {\n+\tpgoff_t ilx;\n+\tstruct folio *folio;\n+\tstruct mempolicy *mpol;\n+\tunsigned long orders = BIT(order);\n \tstruct shmem_inode_info *info = SHMEM_I(inode);\n-\tstruct folio *new, *swapcache;\n-\tint nr_pages = 1 << order;\n-\tgfp_t alloc_gfp = gfp;\n-\n-\tif (!IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE)) {\n-\t\tif (WARN_ON_ONCE(order))\n-\t\t\treturn ERR_PTR(-EINVAL);\n-\t} else if (order) {\n-\t\t/*\n-\t\t * If uffd is active for the vma, we need per-page fault\n-\t\t * fidelity to maintain the uffd semantics, then fallback\n-\t\t * to swapin order-0 folio, as well as for zswap case.\n-\t\t * Any existing sub folio in the swap cache also blocks\n-\t\t * mTHP swapin.\n-\t\t */\n-\t\tif ((vma && unlikely(userfaultfd_armed(vma))) ||\n-\t\t     !zswap_never_enabled() ||\n-\t\t     non_swapcache_batch(entry, nr_pages) != nr_pages)\n-\t\t\tgoto fallback;\n \n-\t\talloc_gfp = thp_limit_gfp_mask(vma_thp_gfp_mask(vma), gfp);\n-\t}\n-retry:\n-\tnew = shmem_alloc_folio(alloc_gfp, order, info, index);\n-\tif (!new) {\n-\t\tnew = ERR_PTR(-ENOMEM);\n-\t\tgoto fallback;\n-\t}\n+\tif ((vmf && unlikely(userfaultfd_armed(vmf->vma))) ||\n+\t     !zswap_never_enabled())\n+\t\torders = 0;\n \n-\tif (mem_cgroup_swapin_charge_folio(new, vma ? vma->vm_mm : NULL,\n-\t\t\t\t\t   alloc_gfp, entry)) {\n-\t\tfolio_put(new);\n-\t\tnew = ERR_PTR(-ENOMEM);\n-\t\tgoto fallback;\n-\t}\n+\tmpol = shmem_get_pgoff_policy(info, index, order, &ilx);\n+\tfolio = swapin_entry(entry, gfp, orders, vmf, mpol, ilx);\n+\tmpol_cond_put(mpol);\n \n-\tswapcache = swapin_folio(entry, new);\n-\tif (swapcache != new) {\n-\t\tfolio_put(new);\n-\t\tif (!swapcache) {\n-\t\t\t/*\n-\t\t\t * The new folio is charged already, swapin can\n-\t\t\t * only fail due to another raced swapin.\n-\t\t\t */\n-\t\t\tnew = ERR_PTR(-EEXIST);\n-\t\t\tgoto fallback;\n-\t\t}\n-\t}\n-\treturn swapcache;\n-fallback:\n-\t/* Order 0 swapin failed, nothing to fallback to, abort */\n-\tif (!order)\n-\t\treturn new;\n-\tentry.val += index - round_down(index, nr_pages);\n-\talloc_gfp = gfp;\n-\tnr_pages = 1;\n-\torder = 0;\n-\tgoto retry;\n+\treturn folio;\n }\n \n /*\n@@ -2262,11 +2218,12 @@ static int shmem_split_large_entry(struct inode *inode, pgoff_t index,\n  */\n static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \t\t\t     struct folio **foliop, enum sgp_type sgp,\n-\t\t\t     gfp_t gfp, struct vm_area_struct *vma,\n+\t\t\t     gfp_t gfp, struct vm_fault *vmf,\n \t\t\t     vm_fault_t *fault_type)\n {\n \tstruct address_space *mapping = inode->i_mapping;\n-\tstruct mm_struct *fault_mm = vma ? vma->vm_mm : NULL;\n+\tstruct vm_area_struct *vma = vmf ? vmf->vma : NULL;\n+\tstruct mm_struct *fault_mm = vmf ? vmf->vma->vm_mm : NULL;\n \tstruct shmem_inode_info *info = SHMEM_I(inode);\n \tswp_entry_t swap;\n \tsoftleaf_t index_entry;\n@@ -2307,20 +2264,15 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \tif (!folio) {\n \t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO)) {\n \t\t\t/* Direct swapin skipping swap cache & readahead */\n-\t\t\tfolio = shmem_swap_alloc_folio(inode, vma, index,\n-\t\t\t\t\t\t       index_entry, order, gfp);\n-\t\t\tif (IS_ERR(folio)) {\n-\t\t\t\terror = PTR_ERR(folio);\n-\t\t\t\tfolio = NULL;\n-\t\t\t\tgoto failed;\n-\t\t\t}\n+\t\t\tfolio = shmem_swap_alloc_folio(inode, vmf, index,\n+\t\t\t\t\t\t       swap, order, gfp);\n \t\t} else {\n \t\t\t/* Cached swapin only supports order 0 folio */\n \t\t\tfolio = shmem_swapin_cluster(swap, gfp, info, index);\n-\t\t\tif (!folio) {\n-\t\t\t\terror = -ENOMEM;\n-\t\t\t\tgoto failed;\n-\t\t\t}\n+\t\t}\n+\t\tif (!folio) {\n+\t\t\terror = -ENOMEM;\n+\t\t\tgoto failed;\n \t\t}\n \t\tif (fault_type) {\n \t\t\t*fault_type |= VM_FAULT_MAJOR;\n@@ -2468,7 +2420,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,\n \n \tif (xa_is_value(folio)) {\n \t\terror = shmem_swapin_folio(inode, index, &folio,\n-\t\t\t\t\t   sgp, gfp, vma, fault_type);\n+\t\t\t\t\t   sgp, gfp, vmf, fault_type);\n \t\tif (error == -EEXIST)\n \t\t\tgoto repeat;\n \ndiff --git a/mm/swap.h b/mm/swap.h\nindex 6774af10a943..80c2f1bf7a57 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -300,7 +300,8 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t flag,\n \t\tstruct mempolicy *mpol, pgoff_t ilx);\n struct folio *swapin_readahead(swp_entry_t entry, gfp_t flag,\n \t\tstruct vm_fault *vmf);\n-struct folio *swapin_folio(swp_entry_t entry, struct folio *folio);\n+struct folio *swapin_entry(swp_entry_t entry, gfp_t flag, unsigned long orders,\n+\t\t\t   struct vm_fault *vmf, struct mempolicy *mpol, pgoff_t ilx);\n void swap_update_readahead(struct folio *folio, struct vm_area_struct *vma,\n \t\t\t   unsigned long addr);\n \n@@ -334,24 +335,6 @@ static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n \t\treturn find_next_bit(sis->zeromap, end, start) - start;\n }\n \n-static inline int non_swapcache_batch(swp_entry_t entry, int max_nr)\n-{\n-\tint i;\n-\n-\t/*\n-\t * While allocating a large folio and doing mTHP swapin, we need to\n-\t * ensure all entries are not cached, otherwise, the mTHP folio will\n-\t * be in conflict with the folio in swap cache.\n-\t */\n-\tfor (i = 0; i < max_nr; i++) {\n-\t\tif (swap_cache_has_folio(entry))\n-\t\t\treturn i;\n-\t\tentry.val++;\n-\t}\n-\n-\treturn i;\n-}\n-\n #else /* CONFIG_SWAP */\n struct swap_iocb;\n static inline struct swap_cluster_info *swap_cluster_lock(\n@@ -433,7 +416,9 @@ static inline struct folio *swapin_readahead(swp_entry_t swp, gfp_t gfp_mask,\n \treturn NULL;\n }\n \n-static inline struct folio *swapin_folio(swp_entry_t entry, struct folio *folio)\n+static inline struct folio *swapin_entry(\n+\tswp_entry_t entry, gfp_t flag, unsigned long orders,\n+\tstruct vm_fault *vmf, struct mempolicy *mpol, pgoff_t ilx)\n {\n \treturn NULL;\n }\n@@ -493,10 +478,5 @@ static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n {\n \treturn 0;\n }\n-\n-static inline int non_swapcache_batch(swp_entry_t entry, int max_nr)\n-{\n-\treturn 0;\n-}\n #endif /* CONFIG_SWAP */\n #endif /* _MM_SWAP_H */\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex e32b06a1f229..0a2a4e084cf2 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -199,43 +199,6 @@ void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \tlruvec_stat_mod_folio(folio, NR_SWAPCACHE, nr_pages);\n }\n \n-/**\n- * swap_cache_add_folio - Add a folio into the swap cache.\n- * @folio: The folio to be added.\n- * @entry: The swap entry corresponding to the folio.\n- * @gfp: gfp_mask for XArray node allocation.\n- * @shadowp: If a shadow is found, return the shadow.\n- *\n- * Context: Caller must ensure @entry is valid and protect the swap device\n- * with reference count or locks.\n- */\n-static int swap_cache_add_folio(struct folio *folio, swp_entry_t entry,\n-\t\t\t\tvoid **shadowp)\n-{\n-\tint err;\n-\tvoid *shadow = NULL;\n-\tunsigned int ci_off;\n-\tstruct swap_info_struct *si;\n-\tstruct swap_cluster_info *ci;\n-\tunsigned long nr_pages = folio_nr_pages(folio);\n-\n-\tsi = __swap_entry_to_info(entry);\n-\tci = swap_cluster_lock(si, swp_offset(entry));\n-\tci_off = swp_cluster_offset(entry);\n-\terr = __swap_cache_check_batch(ci, ci_off, ci_off, nr_pages, &shadow);\n-\tif (err) {\n-\t\tswap_cluster_unlock(ci);\n-\t\treturn err;\n-\t}\n-\n-\t__swap_cache_add_folio(ci, folio, entry);\n-\tswap_cluster_unlock(ci);\n-\tif (shadowp)\n-\t\t*shadowp = shadow;\n-\n-\treturn 0;\n-}\n-\n static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \t\t\t\t\tswp_entry_t targ_entry, gfp_t gfp,\n \t\t\t\t\tunsigned int order, struct vm_fault *vmf,\n@@ -328,30 +291,28 @@ struct folio *swap_cache_alloc_folio(swp_entry_t targ_entry, gfp_t gfp_mask,\n \t\t\t\t     unsigned long orders, struct vm_fault *vmf,\n \t\t\t\t     struct mempolicy *mpol, pgoff_t ilx)\n {\n-\tint order;\n+\tint order, err;\n \tstruct folio *folio;\n \tstruct swap_cluster_info *ci;\n \n+\t/* Always allow order 0 so swap won't fail under pressure. */\n+\torder = orders ? highest_order(orders |= BIT(0)) : 0;\n \tci = __swap_entry_to_cluster(targ_entry);\n-\torder = orders ? highest_order(orders) : 0;\n \tfor (;;) {\n \t\tfolio = __swap_cache_alloc(ci, targ_entry, gfp_mask, order,\n \t\t\t\t\t   vmf, mpol, ilx);\n \t\tif (!IS_ERR(folio))\n \t\t\treturn folio;\n-\t\tif (PTR_ERR(folio) == -EAGAIN)\n+\t\terr = PTR_ERR(folio);\n+\t\tif (err == -EAGAIN)\n \t\t\tcontinue;\n-\t\t/* Only -EBUSY means we should fallback and retry. */\n-\t\tif (PTR_ERR(folio) != -EBUSY)\n-\t\t\treturn folio;\n+\t\tif (!order || (err != -EBUSY && err != -ENOMEM))\n+\t\t\tbreak;\n \t\tcount_mthp_stat(order, MTHP_STAT_SWPIN_FALLBACK);\n \t\torder = next_order(&orders, order);\n-\t\tif (!orders)\n-\t\t\tbreak;\n \t}\n-\t/* Should never reach here, order 0 should not fail with -EBUSY. */\n-\tWARN_ON_ONCE(1);\n-\treturn ERR_PTR(-EINVAL);\n+\n+\treturn ERR_PTR(err);\n }\n \n /**\n@@ -584,51 +545,6 @@ void swap_update_readahead(struct folio *folio, struct vm_area_struct *vma,\n \t}\n }\n \n-/**\n- * __swap_cache_prepare_and_add - Prepare the folio and add it to swap cache.\n- * @entry: swap entry to be bound to the folio.\n- * @folio: folio to be added.\n- * @gfp: memory allocation flags for charge, can be 0 if @charged if true.\n- * @charged: if the folio is already charged.\n- *\n- * Update the swap_map and add folio as swap cache, typically before swapin.\n- * All swap slots covered by the folio must have a non-zero swap count.\n- *\n- * Context: Caller must protect the swap device with reference count or locks.\n- * Return: 0 if success, error code if failed.\n- */\n-static int __swap_cache_prepare_and_add(swp_entry_t entry,\n-\t\t\t\t\tstruct folio *folio,\n-\t\t\t\t\tgfp_t gfp, bool charged)\n-{\n-\tvoid *shadow;\n-\tint ret;\n-\n-\t__folio_set_locked(folio);\n-\t__folio_set_swapbacked(folio);\n-\tret = swap_cache_add_folio(folio, entry, &shadow);\n-\tif (ret)\n-\t\tgoto failed;\n-\n-\tif (!charged && mem_cgroup_swapin_charge_folio(folio, NULL, gfp, entry)) {\n-\t\tswap_cache_del_folio(folio);\n-\t\tret = -ENOMEM;\n-\t\tgoto failed;\n-\t}\n-\n-\tmemcg1_swapin(entry, folio_nr_pages(folio));\n-\tif (shadow)\n-\t\tworkingset_refault(folio, shadow);\n-\n-\t/* Caller will initiate read into locked folio */\n-\tfolio_add_lru(folio);\n-\treturn 0;\n-\n-failed:\n-\tfolio_unlock(folio);\n-\treturn ret;\n-}\n-\n static struct folio *swap_cache_read_folio(swp_entry_t entry, gfp_t gfp,\n \t\t\t\t\t   struct mempolicy *mpol, pgoff_t ilx,\n \t\t\t\t\t   struct swap_iocb **plug, bool readahead)\n@@ -649,7 +565,6 @@ static struct folio *swap_cache_read_folio(swp_entry_t entry, gfp_t gfp,\n \t\tfolio = swap_cache_get_folio(entry);\n \t\tif (folio)\n \t\t\treturn folio;\n-\n \t\tfolio = swap_cache_alloc_folio(entry, gfp, 0, NULL, mpol, ilx);\n \t} while (PTR_ERR(folio) == -EEXIST);\n \n@@ -666,49 +581,37 @@ static struct folio *swap_cache_read_folio(swp_entry_t entry, gfp_t gfp,\n }\n \n /**\n- * swapin_folio - swap-in one or multiple entries skipping readahead.\n- * @entry: starting swap entry to swap in\n- * @folio: a new allocated and charged folio\n+ * swapin_entry - swap-in one or multiple entries skipping readahead.\n+ * @entry: swap entry indicating the target slot\n+ * @gfp_mask: memory allocation flags\n+ * @orders: allocation orders\n+ * @vmf: fault information\n+ * @mpol: NUMA memory allocation policy to be applied\n+ * @ilx: NUMA interleave index, for use only when MPOL_INTERLEAVE\n  *\n- * Reads @entry into @folio, @folio will be added to the swap cache.\n- * If @folio is a large folio, the @entry will be rounded down to align\n- * with the folio size.\n+ * This would allocate a folio suit given @orders, or return the existing\n+ * folio in the swap cache for @entry. This initiates the IO, too, if needed.\n+ * @entry could be rounded down if @orders allows large allocation.\n  *\n- * Return: returns pointer to @folio on success. If folio is a large folio\n- * and this raced with another swapin, NULL will be returned to allow fallback\n- * to order 0. Else, if another folio was already added to the swap cache,\n- * return that swap cache folio instead.\n+ * Context: Caller must ensure @entry is valid and pin the swap device with refcount.\n+ * Return: Returns the folio on success, returns error code if failed.\n  */\n-struct folio *swapin_folio(swp_entry_t entry, struct folio *folio)\n+struct folio *swapin_entry(swp_entry_t entry, gfp_t gfp, unsigned long orders,\n+\t\t\t   struct vm_fault *vmf, struct mempolicy *mpol, pgoff_t ilx)\n {\n-\tint ret;\n-\tstruct folio *swapcache;\n-\tpgoff_t offset = swp_offset(entry);\n-\tunsigned long nr_pages = folio_nr_pages(folio);\n-\n-\tentry = swp_entry(swp_type(entry), round_down(offset, nr_pages));\n-\tfor (;;) {\n-\t\tret = __swap_cache_prepare_and_add(entry, folio, 0, true);\n-\t\tif (!ret) {\n-\t\t\tswap_read_folio(folio, NULL);\n-\t\t\tbreak;\n-\t\t}\n+\tstruct folio *folio;\n \n-\t\t/*\n-\t\t * Large order allocation needs special handling on\n-\t\t * race: if a smaller folio exists in cache, swapin needs\n-\t\t * to fallback to order 0, and doing a swap cache lookup\n-\t\t * might return a folio that is irrelevant to the faulting\n-\t\t * entry because @entry is aligned down. Just return NULL.\n-\t\t */\n-\t\tif (ret != -EEXIST || nr_pages > 1)\n-\t\t\treturn NULL;\n+\tdo {\n+\t\tfolio = swap_cache_get_folio(entry);\n+\t\tif (folio)\n+\t\t\treturn folio;\n+\t\tfolio = swap_cache_alloc_folio(entry, gfp, orders, vmf, mpol, ilx);\n+\t} while (PTR_ERR(folio) == -EEXIST);\n \n-\t\tswapcache = swap_cache_get_folio(entry);\n-\t\tif (swapcache)\n-\t\t\treturn swapcache;\n-\t}\n+\tif (IS_ERR(folio))\n+\t\treturn NULL;\n \n+\tswap_read_folio(folio, NULL);\n \treturn folio;\n }\n \ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 06b37efad2bd..7e7614a5181a 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1833,8 +1833,7 @@ void folio_put_swap(struct folio *folio, struct page *subpage)\n  *   do_swap_page()\n  *     ...\t\t\t\tswapoff+swapon\n  *     swap_cache_alloc_folio()\n- *       swap_cache_add_folio()\n- *         // check swap_map\n+ *       // check swap_map\n  *     // verify PTE not changed\n  *\n  * In __swap_duplicate(), the swap_map need to be checked before\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "The reviewer noted that the patch changes refault counting to the nearest online memcg level, which is different from file folios and may lead to incorrect behavior for anon shadows.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "lock ordering violation",
                "refault accounting"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "To prepare for merging the swap_cgroup_ctrl into the swap table, store\nthe memcg info in the swap table on swapout.\n\nThis is done by using the existing shadow format.\n\nNote this also changes the refault counting at the nearest online memcg\nlevel:\n\nUnlike file folios, anon folios are mostly exclusive to one mem cgroup,\nand each cgroup is likely to have different characteristics.\n\nWhen commit b910718a948a (\"mm: vmscan: detect file thrashing at the\nreclaim root\") moved the refault accounting to the reclaim root level,\nanon shadows don't even exist, and it's explicitly for file pages. Later\ncommit aae466b0052e (\"mm/swap: implement workingset detection for\nanonymous LRU\") added anon shadows following a similar design. And in\nshrink_lruvec, an active LRU's shrinking is done regardlessly when it's\nlow.\n\nFor MGLRU, it's a bit different, but with the PID refault control, it's\nmore accurate to let the nearest online memcg take the refault feedback\ntoo.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/internal.h   | 20 ++++++++++++++++++++\n mm/swap.h       |  7 ++++---\n mm/swap_state.c | 50 +++++++++++++++++++++++++++++++++-----------------\n mm/swapfile.c   |  4 +++-\n mm/vmscan.c     |  6 +-----\n mm/workingset.c | 16 +++++++++++-----\n 6 files changed, 72 insertions(+), 31 deletions(-)\n\ndiff --git a/mm/internal.h b/mm/internal.h\nindex cb0af847d7d9..5bbe081c9048 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1714,6 +1714,7 @@ static inline void shrinker_debugfs_remove(struct dentry *debugfs_entry,\n #endif /* CONFIG_SHRINKER_DEBUG */\n \n /* Only track the nodes of mappings with shadow entries */\n+#define WORKINGSET_SHIFT 1\n void workingset_update_node(struct xa_node *node);\n extern struct list_lru shadow_nodes;\n #define mapping_set_update(xas, mapping) do {\t\t\t\\\n@@ -1722,6 +1723,25 @@ extern struct list_lru shadow_nodes;\n \t\txas_set_lru(xas, &shadow_nodes);\t\t\\\n \t}\t\t\t\t\t\t\t\\\n } while (0)\n+static inline unsigned short shadow_to_memcgid(void *shadow)\n+{\n+\tunsigned long entry = xa_to_value(shadow);\n+\tunsigned short memcgid;\n+\n+\tentry >>= (WORKINGSET_SHIFT + NODES_SHIFT);\n+\tmemcgid = entry & ((1UL << MEM_CGROUP_ID_SHIFT) - 1);\n+\n+\treturn memcgid;\n+}\n+static inline void *memcgid_to_shadow(unsigned short memcgid)\n+{\n+\tunsigned long val;\n+\n+\tval = memcgid;\n+\tval <<= (NODES_SHIFT + WORKINGSET_SHIFT);\n+\n+\treturn xa_mk_value(val);\n+}\n \n /* mremap.c */\n unsigned long move_page_tables(struct pagetable_move_control *pmc);\ndiff --git a/mm/swap.h b/mm/swap.h\nindex da41e9cea46d..c95f5fafea42 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -265,6 +265,8 @@ static inline bool folio_matches_swap_entry(const struct folio *folio,\n \treturn folio_entry.val == round_down(entry.val, nr_pages);\n }\n \n+bool folio_maybe_swapped(struct folio *folio);\n+\n /*\n  * All swap cache helpers below require the caller to ensure the swap entries\n  * used are valid and stabilize the device by any of the following ways:\n@@ -286,9 +288,8 @@ struct folio *swap_cache_alloc_folio(swp_entry_t target_entry, gfp_t gfp_mask,\n /* Below helpers require the caller to lock and pass in the swap cluster. */\n void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \t\t\t    struct folio *folio, swp_entry_t entry);\n-void __swap_cache_del_folio(struct swap_cluster_info *ci,\n-\t\t\t    struct folio *folio, void *shadow,\n-\t\t\t    bool charged, bool reclaim);\n+void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n+\t\t\t    void *shadow, bool charged, bool reclaim);\n void __swap_cache_replace_folio(struct swap_cluster_info *ci,\n \t\t\t\tstruct folio *old, struct folio *new);\n \ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 40f037576c5f..cc4bf40320ef 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -143,22 +143,11 @@ static int __swap_cache_check_batch(struct swap_cluster_info *ci,\n {\n \tunsigned int ci_end = ci_off + nr;\n \tunsigned long old_tb;\n+\tunsigned int memcgid;\n \n \tif (unlikely(!ci->table))\n \t\treturn -ENOENT;\n \n-\tdo {\n-\t\told_tb = __swap_table_get(ci, ci_off);\n-\t\tif (unlikely(swp_tb_is_folio(old_tb)) ||\n-\t\t    unlikely(!__swp_tb_get_count(old_tb)))\n-\t\t\tbreak;\n-\t\tif (swp_tb_is_shadow(old_tb))\n-\t\t\t*shadowp = swp_tb_to_shadow(old_tb);\n-\t} while (++ci_off < ci_end);\n-\n-\tif (likely(ci_off == ci_end))\n-\t\treturn 0;\n-\n \t/*\n \t * If the target slot is not suitable for adding swap cache, return\n \t * -EEXIST or -ENOENT. If the batch is not suitable, could be a\n@@ -169,7 +158,21 @@ static int __swap_cache_check_batch(struct swap_cluster_info *ci,\n \t\treturn -EEXIST;\n \tif (!__swp_tb_get_count(old_tb))\n \t\treturn -ENOENT;\n-\treturn -EBUSY;\n+\tif (WARN_ON_ONCE(!swp_tb_is_shadow(old_tb)))\n+\t\treturn -ENOENT;\n+\t*shadowp = swp_tb_to_shadow(old_tb);\n+\tmemcgid = shadow_to_memcgid(*shadowp);\n+\n+\tWARN_ON_ONCE(!mem_cgroup_disabled() && !memcgid);\n+\tdo {\n+\t\told_tb = __swap_table_get(ci, ci_off);\n+\t\tif (unlikely(swp_tb_is_folio(old_tb)) ||\n+\t\t    unlikely(!__swp_tb_get_count(old_tb)) ||\n+\t\t    memcgid != shadow_to_memcgid(swp_tb_to_shadow(old_tb)))\n+\t\t\treturn -EBUSY;\n+\t} while (++ci_off < ci_end);\n+\n+\treturn 0;\n }\n \n void __swap_cache_add_folio(struct swap_cluster_info *ci,\n@@ -261,8 +264,7 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \n \t/* For memsw accouting, swap is uncharged when folio is added to swap cache */\n \tmemcg1_swapin(folio);\n-\tif (shadow)\n-\t\tworkingset_refault(folio, shadow);\n+\tworkingset_refault(folio, shadow);\n \n \t/* Caller will initiate read into locked new_folio */\n \tfolio_add_lru(folio);\n@@ -319,7 +321,8 @@ struct folio *swap_cache_alloc_folio(swp_entry_t targ_entry, gfp_t gfp_mask,\n  * __swap_cache_del_folio - Removes a folio from the swap cache.\n  * @ci: The locked swap cluster.\n  * @folio: The folio.\n- * @shadow: shadow value to be filled in the swap cache.\n+ * @shadow: Shadow to restore when the folio is not charged. Ignored when\n+ *          @charged is true, as the shadow is computed internally.\n  * @charged: If folio->swap is charged to folio->memcg.\n  * @reclaim: If the folio is being reclaimed. When true on cgroup v1,\n  *           the memory charge is transferred from memory to swap.\n@@ -336,6 +339,7 @@ void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n \tint count;\n \tunsigned long old_tb;\n \tstruct swap_info_struct *si;\n+\tstruct mem_cgroup *memcg = NULL;\n \tswp_entry_t entry = folio->swap;\n \tunsigned int ci_start, ci_off, ci_end;\n \tbool folio_swapped = false, need_free = false;\n@@ -353,7 +357,13 @@ void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n \t * charging (e.g. swapin charge failure, or swap alloc charge failure).\n \t */\n \tif (charged)\n-\t\tmem_cgroup_swap_free_folio(folio, reclaim);\n+\t\tmemcg = mem_cgroup_swap_free_folio(folio, reclaim);\n+\tif (reclaim) {\n+\t\tWARN_ON(!charged);\n+\t\tshadow = workingset_eviction(folio, memcg);\n+\t} else if (memcg) {\n+\t\tshadow = memcgid_to_shadow(mem_cgroup_private_id(memcg));\n+\t}\n \n \tsi = __swap_entry_to_info(entry);\n \tci_start = swp_cluster_offset(entry);\n@@ -392,6 +402,11 @@ void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n  * swap_cache_del_folio - Removes a folio from the swap cache.\n  * @folio: The folio.\n  *\n+ * Force delete a folio from the swap cache. This is only safe to use for\n+ * folios that are not swapped out (swap count == 0) to release the swap\n+ * space from being pinned by swap cache, or remove a clean and charged\n+ * folio that no one modified or is still using.\n+ *\n  * Same as __swap_cache_del_folio, but handles lock and refcount. The\n  * caller must ensure the folio is either clean or has a swap count\n  * equal to zero, or it may cause data loss.\n@@ -404,6 +419,7 @@ void swap_cache_del_folio(struct folio *folio)\n \tswp_entry_t entry = folio->swap;\n \n \tci = swap_cluster_lock(__swap_entry_to_info(entry), swp_offset(entry));\n+\tVM_WARN_ON_ONCE(folio_test_dirty(folio) && folio_maybe_swapped(folio));\n \t__swap_cache_del_folio(ci, folio, NULL, true, false);\n \tswap_cluster_unlock(ci);\n \ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex c0169bce46c9..2cd3e260f1bf 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1972,9 +1972,11 @@ int swp_swapcount(swp_entry_t entry)\n  * decrease of swap count is possible through swap_put_entries_direct, so this\n  * may return a false positive.\n  *\n+ * Caller can hold the ci lock to get a stable result.\n+ *\n  * Context: Caller must ensure the folio is locked and in the swap cache.\n  */\n-static bool folio_maybe_swapped(struct folio *folio)\n+bool folio_maybe_swapped(struct folio *folio)\n {\n \tswp_entry_t entry = folio->swap;\n \tstruct swap_cluster_info *ci;\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 5112f81cf875..4565c9c3ac60 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -755,11 +755,7 @@ static int __remove_mapping(struct address_space *mapping, struct folio *folio,\n \t}\n \n \tif (folio_test_swapcache(folio)) {\n-\t\tswp_entry_t swap = folio->swap;\n-\n-\t\tif (reclaimed && !mapping_exiting(mapping))\n-\t\t\tshadow = workingset_eviction(folio, target_memcg);\n-\t\t__swap_cache_del_folio(ci, folio, shadow, true, true);\n+\t\t__swap_cache_del_folio(ci, folio, NULL, true, true);\n \t\tswap_cluster_unlock_irq(ci);\n \t} else {\n \t\tvoid (*free_folio)(struct folio *);\ndiff --git a/mm/workingset.c b/mm/workingset.c\nindex 37a94979900f..765a954baefa 100644\n--- a/mm/workingset.c\n+++ b/mm/workingset.c\n@@ -202,12 +202,18 @@ static unsigned int bucket_order[ANON_AND_FILE] __read_mostly;\n static void *pack_shadow(int memcgid, pg_data_t *pgdat, unsigned long eviction,\n \t\t\t bool workingset, bool file)\n {\n+\tvoid *shadow;\n+\n \teviction &= file ? EVICTION_MASK : EVICTION_MASK_ANON;\n \teviction = (eviction << MEM_CGROUP_ID_SHIFT) | memcgid;\n \teviction = (eviction << NODES_SHIFT) | pgdat->node_id;\n \teviction = (eviction << WORKINGSET_SHIFT) | workingset;\n \n-\treturn xa_mk_value(eviction);\n+\tshadow = xa_mk_value(eviction);\n+\t/* Sanity check for retrieving memcgid from anon shadow. */\n+\tVM_WARN_ON_ONCE(shadow_to_memcgid(shadow) != memcgid);\n+\n+\treturn shadow;\n }\n \n static void unpack_shadow(void *shadow, int *memcgidp, pg_data_t **pgdat,\n@@ -232,7 +238,7 @@ static void unpack_shadow(void *shadow, int *memcgidp, pg_data_t **pgdat,\n \n #ifdef CONFIG_LRU_GEN\n \n-static void *lru_gen_eviction(struct folio *folio)\n+static void *lru_gen_eviction(struct folio *folio, struct mem_cgroup *memcg)\n {\n \tint hist;\n \tunsigned long token;\n@@ -244,7 +250,6 @@ static void *lru_gen_eviction(struct folio *folio)\n \tint refs = folio_lru_refs(folio);\n \tbool workingset = folio_test_workingset(folio);\n \tint tier = lru_tier_from_refs(refs, workingset);\n-\tstruct mem_cgroup *memcg = folio_memcg(folio);\n \tstruct pglist_data *pgdat = folio_pgdat(folio);\n \n \tBUILD_BUG_ON(LRU_GEN_WIDTH + LRU_REFS_WIDTH >\n@@ -252,6 +257,7 @@ static void *lru_gen_eviction(struct folio *folio)\n \n \tlruvec = mem_cgroup_lruvec(memcg, pgdat);\n \tlrugen = &lruvec->lrugen;\n+\tmemcg = lruvec_memcg(lruvec);\n \tmin_seq = READ_ONCE(lrugen->min_seq[type]);\n \ttoken = (min_seq << LRU_REFS_WIDTH) | max(refs - 1, 0);\n \n@@ -329,7 +335,7 @@ static void lru_gen_refault(struct folio *folio, void *shadow)\n \n #else /* !CONFIG_LRU_GEN */\n \n-static void *lru_gen_eviction(struct folio *folio)\n+static void *lru_gen_eviction(struct folio *folio, struct mem_cgroup *target_memcg)\n {\n \treturn NULL;\n }\n@@ -396,7 +402,7 @@ void *workingset_eviction(struct folio *folio, struct mem_cgroup *target_memcg)\n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n \n \tif (lru_gen_enabled())\n-\t\treturn lru_gen_eviction(folio);\n+\t\treturn lru_gen_eviction(folio, target_memcg);\n \n \tlruvec = mem_cgroup_lruvec(target_memcg, pgdat);\n \t/* XXX: target_memcg can be NULL, go through lruvec */\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that vswap_free() acquires the per-vswap spinlock while holding the folio lock, creating a potential deadlock with reclaim paths, and suggested dropping the lock before calling try_to_unmap().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential deadlock",
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "To make sure folio->swap always belongs to folio->memcg, when doing the\ncharge, charge against folio->memcg. Defer the recording of swap cgroup\ninfo, do a reparent, and record the nearest online ancestor on swap cache\nremoval only.\n\nThen, a folio is in the swap cache, and the folio itself is owned by the\nmemcg. Hence, through the folio, the memcg also owns folio->swap. The\nextra pinning of the swap cgroup info record is not needed and can be\nreleased.\n\nThis should be fine for both cgroup v2 and v1. There should be no\nuserspace observable behavior.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n include/linux/memcontrol.h |   8 ++--\n include/linux/swap.h       |  24 +++++++++--\n mm/memcontrol-v1.c         |  77 ++++++++++++++++-----------------\n mm/memcontrol.c            | 104 ++++++++++++++++++++++++++++++++-------------\n mm/swap.h                  |   6 ++-\n mm/swap_cgroup.c           |   5 +--\n mm/swap_state.c            |  26 +++++++++---\n mm/swapfile.c              |  15 +++++--\n mm/vmscan.c                |   3 +-\n 9 files changed, 173 insertions(+), 95 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 70b685a85bf4..0b37d4faf785 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -1896,8 +1896,8 @@ static inline void mem_cgroup_exit_user_fault(void)\n \tcurrent->in_user_fault = 0;\n }\n \n-void memcg1_swapout(struct folio *folio, swp_entry_t entry);\n-void memcg1_swapin(swp_entry_t entry, unsigned int nr_pages);\n+void memcg1_swapout(struct folio *folio, struct mem_cgroup *swap_memcg);\n+void memcg1_swapin(struct folio *folio);\n \n #else /* CONFIG_MEMCG_V1 */\n static inline\n@@ -1926,11 +1926,11 @@ static inline void mem_cgroup_exit_user_fault(void)\n {\n }\n \n-static inline void memcg1_swapout(struct folio *folio, swp_entry_t entry)\n+static inline void memcg1_swapout(struct folio *folio, struct mem_cgroup *_memcg)\n {\n }\n \n-static inline void memcg1_swapin(swp_entry_t entry, unsigned int nr_pages)\n+static inline void memcg1_swapin(struct folio *folio)\n {\n }\n \ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 0effe3cc50f5..66cf657a1f35 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -580,12 +580,22 @@ static inline int mem_cgroup_try_charge_swap(struct folio *folio,\n \treturn __mem_cgroup_try_charge_swap(folio, entry);\n }\n \n-extern void __mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages);\n-static inline void mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages)\n+extern void __mem_cgroup_uncharge_swap(unsigned short id, unsigned int nr_pages);\n+static inline void mem_cgroup_uncharge_swap(unsigned short id, unsigned int nr_pages)\n {\n \tif (mem_cgroup_disabled())\n \t\treturn;\n-\t__mem_cgroup_uncharge_swap(entry, nr_pages);\n+\t__mem_cgroup_uncharge_swap(id, nr_pages);\n+}\n+\n+struct mem_cgroup *__mem_cgroup_swap_free_folio(struct folio *folio,\n+\t\t\t\t\t       bool reclaim);\n+static inline struct mem_cgroup *mem_cgroup_swap_free_folio(struct folio *folio,\n+\t\t\t\t\t\t\t    bool reclaim)\n+{\n+\tif (mem_cgroup_disabled())\n+\t\treturn NULL;\n+\treturn __mem_cgroup_swap_free_folio(folio, reclaim);\n }\n \n extern long mem_cgroup_get_nr_swap_pages(struct mem_cgroup *memcg);\n@@ -597,11 +607,17 @@ static inline int mem_cgroup_try_charge_swap(struct folio *folio,\n \treturn 0;\n }\n \n-static inline void mem_cgroup_uncharge_swap(swp_entry_t entry,\n+static inline void mem_cgroup_uncharge_swap(unsigned short id,\n \t\t\t\t\t    unsigned int nr_pages)\n {\n }\n \n+static inline struct mem_cgroup *mem_cgroup_swap_free_folio(struct folio *folio,\n+\t\t\t\t\t\t\t    bool reclaim)\n+{\n+\treturn NULL;\n+}\n+\n static inline long mem_cgroup_get_nr_swap_pages(struct mem_cgroup *memcg)\n {\n \treturn get_nr_swap_pages();\ndiff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.c\nindex a7c78b0987df..038e630dc7e1 100644\n--- a/mm/memcontrol-v1.c\n+++ b/mm/memcontrol-v1.c\n@@ -606,29 +606,21 @@ void memcg1_commit_charge(struct folio *folio, struct mem_cgroup *memcg)\n /**\n  * memcg1_swapout - transfer a memsw charge to swap\n  * @folio: folio whose memsw charge to transfer\n- * @entry: swap entry to move the charge to\n- *\n- * Transfer the memsw charge of @folio to @entry.\n+ * @swap_memcg: cgroup that will be charged, must be online ancestor\n+ *              of folio's memcg.\n  */\n-void memcg1_swapout(struct folio *folio, swp_entry_t entry)\n+void memcg1_swapout(struct folio *folio, struct mem_cgroup *swap_memcg)\n {\n-\tstruct mem_cgroup *memcg, *swap_memcg;\n+\tstruct mem_cgroup *memcg;\n \tunsigned int nr_entries;\n+\tunsigned long flags;\n \n-\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);\n-\tVM_BUG_ON_FOLIO(folio_ref_count(folio), folio);\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn;\n-\n-\tif (!do_memsw_account())\n-\t\treturn;\n+\t/* The folio must be getting reclaimed. */\n+\tVM_WARN_ON_ONCE_FOLIO(folio_mapped(folio), folio);\n \n \tmemcg = folio_memcg(folio);\n \n \tVM_WARN_ON_ONCE_FOLIO(!memcg, folio);\n-\tif (!memcg)\n-\t\treturn;\n \n \t/*\n \t * In case the memcg owning these pages has been offlined and doesn't\n@@ -636,14 +628,15 @@ void memcg1_swapout(struct folio *folio, swp_entry_t entry)\n \t * ancestor for the swap instead and transfer the memory+swap charge.\n \t */\n \tnr_entries = folio_nr_pages(folio);\n-\tswap_memcg = mem_cgroup_private_id_get_online(memcg, nr_entries);\n \tmod_memcg_state(swap_memcg, MEMCG_SWAP, nr_entries);\n \n-\tswap_cgroup_record(folio, mem_cgroup_private_id(swap_memcg), entry);\n-\n \tfolio_unqueue_deferred_split(folio);\n-\tfolio->memcg_data = 0;\n \n+\t/*\n+\t * Free the folio charge now so memsw won't be double uncharged:\n+\t * memsw is now charged by the swap record.\n+\t */\n+\tfolio->memcg_data = 0;\n \tif (!mem_cgroup_is_root(memcg))\n \t\tpage_counter_uncharge(&memcg->memory, nr_entries);\n \n@@ -653,33 +646,34 @@ void memcg1_swapout(struct folio *folio, swp_entry_t entry)\n \t\tpage_counter_uncharge(&memcg->memsw, nr_entries);\n \t}\n \n-\t/*\n-\t * Interrupts should be disabled here because the caller holds the\n-\t * i_pages lock which is taken with interrupts-off. It is\n-\t * important here to have the interrupts disabled because it is the\n-\t * only synchronisation we have for updating the per-CPU variables.\n-\t */\n+\tlocal_irq_save(flags);\n \tpreempt_disable_nested();\n-\tVM_WARN_ON_IRQS_ENABLED();\n \tmemcg1_charge_statistics(memcg, -folio_nr_pages(folio));\n \tpreempt_enable_nested();\n+\tlocal_irq_restore(flags);\n \tmemcg1_check_events(memcg, folio_nid(folio));\n \n \tcss_put(&memcg->css);\n }\n \n /*\n- * memcg1_swapin - uncharge swap slot\n- * @entry: the first swap entry for which the pages are charged\n- * @nr_pages: number of pages which will be uncharged\n+ * memcg1_swapin - uncharge memsw for the swap slot on swapin\n+ * @folio: the folio being swapped in, already charged to memory\n  *\n  * Call this function after successfully adding the charged page to swapcache.\n- *\n- * Note: This function assumes the page for which swap slot is being uncharged\n- * is order 0 page.\n+ * The swap cgroup tracking has already been released by\n+ * mem_cgroup_swapin_charge_folio(), so we only need to drop the duplicate\n+ * memsw charge that was placed on the swap entry during swapout.\n  */\n-void memcg1_swapin(swp_entry_t entry, unsigned int nr_pages)\n+void memcg1_swapin(struct folio *folio)\n {\n+\tstruct mem_cgroup *memcg;\n+\tunsigned int nr_pages;\n+\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_memcg_charged(folio), folio);\n+\n \t/*\n \t * Cgroup1's unified memory+swap counter has been charged with the\n \t * new swapcache page, finish the transfer by uncharging the swap\n@@ -692,14 +686,15 @@ void memcg1_swapin(swp_entry_t entry, unsigned int nr_pages)\n \t * correspond 1:1 to page and swap slot lifetimes: we charge the\n \t * page to memory here, and uncharge swap when the slot is freed.\n \t */\n-\tif (do_memsw_account()) {\n-\t\t/*\n-\t\t * The swap entry might not get freed for a long time,\n-\t\t * let's not wait for it.  The page already received a\n-\t\t * memory+swap charge, drop the swap entry duplicate.\n-\t\t */\n-\t\tmem_cgroup_uncharge_swap(entry, nr_pages);\n-\t}\n+\tif (!do_memsw_account())\n+\t\treturn;\n+\n+\tmemcg = folio_memcg(folio);\n+\tnr_pages = folio_nr_pages(folio);\n+\n+\tif (!mem_cgroup_is_root(memcg))\n+\t\tpage_counter_uncharge(&memcg->memsw, nr_pages);\n+\tmod_memcg_state(memcg, MEMCG_SWAP, -nr_pages);\n }\n \n void memcg1_uncharge_batch(struct mem_cgroup *memcg, unsigned long pgpgout,\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex b2898719e935..d9ff44b77409 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -4804,8 +4804,8 @@ int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n \t\t\t\t  gfp_t gfp, swp_entry_t entry)\n {\n \tstruct mem_cgroup *memcg, *swap_memcg;\n-\tunsigned short id, parent_id;\n \tunsigned int nr_pages;\n+\tunsigned short id;\n \tint ret;\n \n \tif (mem_cgroup_disabled())\n@@ -4831,37 +4831,31 @@ int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n \tif (ret)\n \t\tgoto out;\n \n+\t/*\n+\t * On successful charge, the folio itself now belongs to the memcg,\n+\t * so is folio->swap. So we can release the swap cgroup table's\n+\t * pinning of the private id.\n+\t */\n+\tswap_cgroup_clear(folio->swap, nr_pages);\n+\tmem_cgroup_private_id_put(swap_memcg, nr_pages);\n+\n \t/*\n \t * If the swap entry's memcg is dead, reparent the swap charge\n \t * from swap_memcg to memcg.\n-\t *\n-\t * If memcg is also being offlined, the charge will be moved to\n-\t * its parent again.\n \t */\n \tif (swap_memcg && memcg != swap_memcg) {\n-\t\tstruct mem_cgroup *parent_memcg;\n-\n-\t\tparent_memcg = mem_cgroup_private_id_get_online(memcg, nr_pages);\n-\t\tparent_id = mem_cgroup_private_id(parent_memcg);\n-\n-\t\tWARN_ON(id != swap_cgroup_clear(entry, nr_pages));\n-\t\tswap_cgroup_record(folio, parent_id, entry);\n-\n \t\tif (do_memsw_account()) {\n-\t\t\tif (!mem_cgroup_is_root(parent_memcg))\n-\t\t\t\tpage_counter_charge(&parent_memcg->memsw, nr_pages);\n+\t\t\tif (!mem_cgroup_is_root(memcg))\n+\t\t\t\tpage_counter_charge(&memcg->memsw, nr_pages);\n \t\t\tpage_counter_uncharge(&swap_memcg->memsw, nr_pages);\n \t\t} else {\n-\t\t\tif (!mem_cgroup_is_root(parent_memcg))\n-\t\t\t\tpage_counter_charge(&parent_memcg->swap, nr_pages);\n+\t\t\tif (!mem_cgroup_is_root(memcg))\n+\t\t\t\tpage_counter_charge(&memcg->swap, nr_pages);\n \t\t\tpage_counter_uncharge(&swap_memcg->swap, nr_pages);\n \t\t}\n \n-\t\tmod_memcg_state(parent_memcg, MEMCG_SWAP, nr_pages);\n+\t\tmod_memcg_state(memcg, MEMCG_SWAP, nr_pages);\n \t\tmod_memcg_state(swap_memcg, MEMCG_SWAP, -nr_pages);\n-\n-\t\t/* Release the dead cgroup after reparent */\n-\t\tmem_cgroup_private_id_put(swap_memcg, nr_pages);\n \t}\n out:\n \tcss_put(&memcg->css);\n@@ -5260,33 +5254,32 @@ int __mem_cgroup_try_charge_swap(struct folio *folio, swp_entry_t entry)\n \t\treturn 0;\n \t}\n \n-\tmemcg = mem_cgroup_private_id_get_online(memcg, nr_pages);\n-\n+\t/*\n+\t * Charge the swap counter against the folio's memcg directly.\n+\t * The private id pinning and swap cgroup recording are deferred\n+\t * to __mem_cgroup_swap_free_folio() when the folio leaves the\n+\t * swap cache.  No _id_get_online here means no _id_put on error.\n+\t */\n \tif (!mem_cgroup_is_root(memcg) &&\n \t    !page_counter_try_charge(&memcg->swap, nr_pages, &counter)) {\n \t\tmemcg_memory_event(memcg, MEMCG_SWAP_MAX);\n \t\tmemcg_memory_event(memcg, MEMCG_SWAP_FAIL);\n-\t\tmem_cgroup_private_id_put(memcg, nr_pages);\n \t\treturn -ENOMEM;\n \t}\n \tmod_memcg_state(memcg, MEMCG_SWAP, nr_pages);\n \n-\tswap_cgroup_record(folio, mem_cgroup_private_id(memcg), entry);\n-\n \treturn 0;\n }\n \n /**\n  * __mem_cgroup_uncharge_swap - uncharge swap space\n- * @entry: swap entry to uncharge\n+ * @id: private id of the mem_cgroup to uncharge\n  * @nr_pages: the amount of swap space to uncharge\n  */\n-void __mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages)\n+void __mem_cgroup_uncharge_swap(unsigned short id, unsigned int nr_pages)\n {\n \tstruct mem_cgroup *memcg;\n-\tunsigned short id;\n \n-\tid = swap_cgroup_clear(entry, nr_pages);\n \trcu_read_lock();\n \tmemcg = mem_cgroup_from_private_id(id);\n \tif (memcg) {\n@@ -5302,6 +5295,59 @@ void __mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages)\n \trcu_read_unlock();\n }\n \n+/**\n+ * __mem_cgroup_swap_free_folio - Folio is being freed from swap cache.\n+ * @folio: folio being freed.\n+ * @reclaim: true if the folio is being reclaimed.\n+ *\n+ * For cgroup V2, swap entries are charged to folio's memcg by the time\n+ * swap allocator adds it into the swap cache by mem_cgroup_try_charge_swap.\n+ * The ownership of folio->swap to folio->memcg is constrained by the folio\n+ * in swap cache. If the folio is being removed from swap cache, the\n+ * constraint will be gone so need to grab the memcg's private id for long\n+ * term tracking.\n+ *\n+ * For cgroup V1, the memory-to-swap charge transfer is also performed on\n+ * the folio reclaim path.\n+ *\n+ * It's unlikely but possible that the folio's memcg is dead, in that case\n+ * we reparent and recharge the parent. Recorded cgroup is changed to\n+ * parent too.\n+ *\n+ * Return: Pointer to the mem cgroup being pinned by the charge.\n+ */\n+struct mem_cgroup *__mem_cgroup_swap_free_folio(struct folio *folio,\n+\t\t\t\t\t       bool reclaim)\n+{\n+\tunsigned int nr_pages = folio_nr_pages(folio);\n+\tstruct mem_cgroup *memcg, *swap_memcg;\n+\tswp_entry_t entry = folio->swap;\n+\tunsigned short id;\n+\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_memcg_charged(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n+\n+\t/*\n+\t * Pin the nearest online ancestor's private id for long term\n+\t * swap cgroup tracking.  If memcg is still alive, swap_memcg\n+\t * will be the same as memcg. Else, it's reparented.\n+\t */\n+\tmemcg = folio_memcg(folio);\n+\tswap_memcg = mem_cgroup_private_id_get_online(memcg, nr_pages);\n+\tid = mem_cgroup_private_id(swap_memcg);\n+\tswap_cgroup_record(folio, id, entry);\n+\n+\tif (reclaim && do_memsw_account()) {\n+\t\tmemcg1_swapout(folio, swap_memcg);\n+\t} else if (memcg != swap_memcg) {\n+\t\tif (!mem_cgroup_is_root(swap_memcg))\n+\t\t\tpage_counter_charge(&swap_memcg->swap, nr_pages);\n+\t\tpage_counter_uncharge(&memcg->swap, nr_pages);\n+\t}\n+\n+\treturn swap_memcg;\n+}\n+\n long mem_cgroup_get_nr_swap_pages(struct mem_cgroup *memcg)\n {\n \tlong nr_swap_pages = get_nr_swap_pages();\ndiff --git a/mm/swap.h b/mm/swap.h\nindex 80c2f1bf7a57..da41e9cea46d 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -287,7 +287,8 @@ struct folio *swap_cache_alloc_folio(swp_entry_t target_entry, gfp_t gfp_mask,\n void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \t\t\t    struct folio *folio, swp_entry_t entry);\n void __swap_cache_del_folio(struct swap_cluster_info *ci,\n-\t\t\t    struct folio *folio, swp_entry_t entry, void *shadow);\n+\t\t\t    struct folio *folio, void *shadow,\n+\t\t\t    bool charged, bool reclaim);\n void __swap_cache_replace_folio(struct swap_cluster_info *ci,\n \t\t\t\tstruct folio *old, struct folio *new);\n \n@@ -459,7 +460,8 @@ static inline void swap_cache_del_folio(struct folio *folio)\n }\n \n static inline void __swap_cache_del_folio(struct swap_cluster_info *ci,\n-\t\tstruct folio *folio, swp_entry_t entry, void *shadow)\n+\t\tstruct folio *folio, void *shadow,\n+\t\tbool charged, bool reclaim)\n {\n }\n \ndiff --git a/mm/swap_cgroup.c b/mm/swap_cgroup.c\nindex de779fed8c21..b5a7f21c3afe 100644\n--- a/mm/swap_cgroup.c\n+++ b/mm/swap_cgroup.c\n@@ -54,8 +54,7 @@ static unsigned short __swap_cgroup_id_xchg(struct swap_cgroup *map,\n /**\n  * swap_cgroup_record - record mem_cgroup for a set of swap entries.\n  * These entries must belong to one single folio, and that folio\n- * must be being charged for swap space (swap out), and these\n- * entries must not have been charged\n+ * must be being charged for swap space (swap out).\n  *\n  * @folio: the folio that the swap entry belongs to\n  * @id: mem_cgroup ID to be recorded\n@@ -75,7 +74,7 @@ void swap_cgroup_record(struct folio *folio, unsigned short id,\n \n \tdo {\n \t\told = __swap_cgroup_id_xchg(map, offset, id);\n-\t\tVM_BUG_ON(old);\n+\t\tVM_WARN_ON_ONCE(old);\n \t} while (++offset != end);\n }\n \ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 0a2a4e084cf2..40f037576c5f 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -251,7 +251,7 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \tif (mem_cgroup_swapin_charge_folio(folio, vmf ? vmf->vma->vm_mm : NULL,\n \t\t\t\t\t   gfp, entry)) {\n \t\tspin_lock(&ci->lock);\n-\t\t__swap_cache_del_folio(ci, folio, shadow);\n+\t\t__swap_cache_del_folio(ci, folio, shadow, false, false);\n \t\tspin_unlock(&ci->lock);\n \t\tfolio_unlock(folio);\n \t\tfolio_put(folio);\n@@ -260,7 +260,7 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \t}\n \n \t/* For memsw accouting, swap is uncharged when folio is added to swap cache */\n-\tmemcg1_swapin(entry, 1 << order);\n+\tmemcg1_swapin(folio);\n \tif (shadow)\n \t\tworkingset_refault(folio, shadow);\n \n@@ -319,21 +319,24 @@ struct folio *swap_cache_alloc_folio(swp_entry_t targ_entry, gfp_t gfp_mask,\n  * __swap_cache_del_folio - Removes a folio from the swap cache.\n  * @ci: The locked swap cluster.\n  * @folio: The folio.\n- * @entry: The first swap entry that the folio corresponds to.\n  * @shadow: shadow value to be filled in the swap cache.\n+ * @charged: If folio->swap is charged to folio->memcg.\n+ * @reclaim: If the folio is being reclaimed. When true on cgroup v1,\n+ *           the memory charge is transferred from memory to swap.\n  *\n  * Removes a folio from the swap cache and fills a shadow in place.\n  * This won't put the folio's refcount. The caller has to do that.\n  *\n- * Context: Caller must ensure the folio is locked and in the swap cache\n- * using the index of @entry, and lock the cluster that holds the entries.\n+ * Context: Caller must ensure the folio is locked and in the swap cache,\n+ * and lock the cluster that holds the entries.\n  */\n void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n-\t\t\t    swp_entry_t entry, void *shadow)\n+\t\t\t    void *shadow, bool charged, bool reclaim)\n {\n \tint count;\n \tunsigned long old_tb;\n \tstruct swap_info_struct *si;\n+\tswp_entry_t entry = folio->swap;\n \tunsigned int ci_start, ci_off, ci_end;\n \tbool folio_swapped = false, need_free = false;\n \tunsigned long nr_pages = folio_nr_pages(folio);\n@@ -343,6 +346,15 @@ void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n \tVM_WARN_ON_ONCE_FOLIO(folio_test_writeback(folio), folio);\n \n+\t/*\n+\t * If the folio's swap entry is charged to its memcg, record the\n+\t * swap cgroup for long-term tracking before the folio leaves the\n+\t * swap cache.  Not charged when the folio never completed memcg\n+\t * charging (e.g. swapin charge failure, or swap alloc charge failure).\n+\t */\n+\tif (charged)\n+\t\tmem_cgroup_swap_free_folio(folio, reclaim);\n+\n \tsi = __swap_entry_to_info(entry);\n \tci_start = swp_cluster_offset(entry);\n \tci_end = ci_start + nr_pages;\n@@ -392,7 +404,7 @@ void swap_cache_del_folio(struct folio *folio)\n \tswp_entry_t entry = folio->swap;\n \n \tci = swap_cluster_lock(__swap_entry_to_info(entry), swp_offset(entry));\n-\t__swap_cache_del_folio(ci, folio, entry, NULL);\n+\t__swap_cache_del_folio(ci, folio, NULL, true, false);\n \tswap_cluster_unlock(ci);\n \n \tfolio_ref_sub(folio, folio_nr_pages(folio));\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 7e7614a5181a..c0169bce46c9 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1703,6 +1703,7 @@ int folio_alloc_swap(struct folio *folio)\n {\n \tunsigned int order = folio_order(folio);\n \tunsigned int size = 1 << order;\n+\tstruct swap_cluster_info *ci;\n \n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n \tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n@@ -1737,8 +1738,12 @@ int folio_alloc_swap(struct folio *folio)\n \t}\n \n \t/* Need to call this even if allocation failed, for MEMCG_SWAP_FAIL. */\n-\tif (unlikely(mem_cgroup_try_charge_swap(folio, folio->swap)))\n-\t\tswap_cache_del_folio(folio);\n+\tif (unlikely(mem_cgroup_try_charge_swap(folio, folio->swap))) {\n+\t\tci = swap_cluster_lock(__swap_entry_to_info(folio->swap),\n+\t\t\t\t       swp_offset(folio->swap));\n+\t\t__swap_cache_del_folio(ci, folio, NULL, false, false);\n+\t\tswap_cluster_unlock(ci);\n+\t}\n \n \tif (unlikely(!folio_test_swapcache(folio)))\n \t\treturn -ENOMEM;\n@@ -1879,6 +1884,7 @@ void __swap_cluster_free_entries(struct swap_info_struct *si,\n \t\t\t\t unsigned int ci_start, unsigned int nr_pages)\n {\n \tunsigned long old_tb;\n+\tunsigned short id;\n \tunsigned int ci_off = ci_start, ci_end = ci_start + nr_pages;\n \tunsigned long offset = cluster_offset(si, ci) + ci_start;\n \n@@ -1892,7 +1898,10 @@ void __swap_cluster_free_entries(struct swap_info_struct *si,\n \t\t__swap_table_set(ci, ci_off, null_to_swp_tb());\n \t} while (++ci_off < ci_end);\n \n-\tmem_cgroup_uncharge_swap(swp_entry(si->type, offset), nr_pages);\n+\tid = swap_cgroup_clear(swp_entry(si->type, offset), nr_pages);\n+\tif (id)\n+\t\tmem_cgroup_uncharge_swap(id, nr_pages);\n+\n \tswap_range_free(si, offset, nr_pages);\n \tswap_cluster_assert_empty(ci, ci_start, nr_pages, false);\n \ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 44e4fcd6463c..5112f81cf875 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -759,8 +759,7 @@ static int __remove_mapping(struct address_space *mapping, struct folio *folio,\n \n \t\tif (reclaimed && !mapping_exiting(mapping))\n \t\t\tshadow = workingset_eviction(folio, target_memcg);\n-\t\tmemcg1_swapout(folio, swap);\n-\t\t__swap_cache_del_folio(ci, folio, swap, shadow);\n+\t\t__swap_cache_del_folio(ci, folio, shadow, true, true);\n \t\tswap_cluster_unlock_irq(ci);\n \t} else {\n \t\tvoid (*free_folio)(struct folio *);\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "The reviewer suggested modifying the code to handle different memory control groups (memcg) simultaneously, rather than relying on the caller to ensure all slots are in the same memcg.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Instead of let the caller ensures all slots are in the same memcg, the\nmake it be able to handle different memcg at once.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/swapfile.c | 31 +++++++++++++++++++++++++------\n 1 file changed, 25 insertions(+), 6 deletions(-)\n\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 2cd3e260f1bf..cd2d3b2ca6f0 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1883,10 +1883,13 @@ void __swap_cluster_free_entries(struct swap_info_struct *si,\n \t\t\t\t struct swap_cluster_info *ci,\n \t\t\t\t unsigned int ci_start, unsigned int nr_pages)\n {\n+\tvoid *shadow;\n \tunsigned long old_tb;\n-\tunsigned short id;\n+\tunsigned int type = si->type;\n+\tunsigned int id = 0, id_iter, id_check;\n \tunsigned int ci_off = ci_start, ci_end = ci_start + nr_pages;\n-\tunsigned long offset = cluster_offset(si, ci) + ci_start;\n+\tunsigned long offset = cluster_offset(si, ci);\n+\tunsigned int ci_batch = ci_off;\n \n \tVM_WARN_ON(ci->count < nr_pages);\n \n@@ -1896,13 +1899,29 @@ void __swap_cluster_free_entries(struct swap_info_struct *si,\n \t\t/* Release the last ref, or after swap cache is dropped */\n \t\tVM_WARN_ON(!swp_tb_is_shadow(old_tb) || __swp_tb_get_count(old_tb) > 1);\n \t\t__swap_table_set(ci, ci_off, null_to_swp_tb());\n+\n+\t\tshadow = swp_tb_to_shadow(old_tb);\n+\t\tid_iter = shadow_to_memcgid(shadow);\n+\t\tif (id != id_iter) {\n+\t\t\tif (id) {\n+\t\t\t\tid_check = swap_cgroup_clear(swp_entry(type, offset + ci_batch),\n+\t\t\t\t\t\t\t     ci_off - ci_batch);\n+\t\t\t\tWARN_ON(id != id_check);\n+\t\t\t\tmem_cgroup_uncharge_swap(id, ci_off - ci_batch);\n+\t\t\t}\n+\t\t\tid = id_iter;\n+\t\t\tci_batch = ci_off;\n+\t\t}\n \t} while (++ci_off < ci_end);\n \n-\tid = swap_cgroup_clear(swp_entry(si->type, offset), nr_pages);\n-\tif (id)\n-\t\tmem_cgroup_uncharge_swap(id, nr_pages);\n+\tif (id) {\n+\t\tid_check = swap_cgroup_clear(swp_entry(type, offset + ci_batch),\n+\t\t\t\t\t     ci_off - ci_batch);\n+\t\tWARN_ON(id != id_check);\n+\t\tmem_cgroup_uncharge_swap(id, ci_off - ci_batch);\n+\t}\n \n-\tswap_range_free(si, offset, nr_pages);\n+\tswap_range_free(si, offset + ci_start, nr_pages);\n \tswap_cluster_assert_empty(ci, ci_start, nr_pages, false);\n \n \tif (!ci->count)\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "The reviewer suggested transitioning mem_cgroup_swapin_charge_folio() to receive the memcg id from the caller via the swap table shadow entry, and removing the per-PTE cgroup id batching break from swap_pte_batch().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Transition mem_cgroup_swapin_charge_folio() to receive the memcg id\nfrom the caller via the swap table shadow entry, demoting the old\nswap cgroup array lookup to a sanity check. Also removes the per-PTE\ncgroup id batching break from swap_pte_batch() since now swap is able to\nfree slots across mem cgroups.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n include/linux/memcontrol.h | 6 ++++--\n mm/internal.h              | 4 ----\n mm/memcontrol.c            | 9 ++++++---\n mm/swap_state.c            | 5 ++++-\n 4 files changed, 14 insertions(+), 10 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 0b37d4faf785..8fc794baf736 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -667,7 +667,8 @@ static inline int mem_cgroup_charge(struct folio *folio, struct mm_struct *mm,\n int mem_cgroup_charge_hugetlb(struct folio* folio, gfp_t gfp);\n \n int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n-\t\t\t\t  gfp_t gfp, swp_entry_t entry);\n+\t\t\t\t   gfp_t gfp, swp_entry_t entry,\n+\t\t\t\t   unsigned short id);\n \n void __mem_cgroup_uncharge(struct folio *folio);\n \n@@ -1145,7 +1146,8 @@ static inline int mem_cgroup_charge_hugetlb(struct folio* folio, gfp_t gfp)\n }\n \n static inline int mem_cgroup_swapin_charge_folio(struct folio *folio,\n-\t\t\tstruct mm_struct *mm, gfp_t gfp, swp_entry_t entry)\n+\t\t\tstruct mm_struct *mm, gfp_t gfp, swp_entry_t entry,\n+\t\t\tunsigned short id)\n {\n \treturn 0;\n }\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 5bbe081c9048..416d3401aa17 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -452,12 +452,10 @@ static inline int swap_pte_batch(pte_t *start_ptep, int max_nr, pte_t pte)\n \tconst pte_t *end_ptep = start_ptep + max_nr;\n \tconst softleaf_t entry = softleaf_from_pte(pte);\n \tpte_t *ptep = start_ptep + 1;\n-\tunsigned short cgroup_id;\n \n \tVM_WARN_ON(max_nr < 1);\n \tVM_WARN_ON(!softleaf_is_swap(entry));\n \n-\tcgroup_id = lookup_swap_cgroup_id(entry);\n \twhile (ptep < end_ptep) {\n \t\tsoftleaf_t entry;\n \n@@ -466,8 +464,6 @@ static inline int swap_pte_batch(pte_t *start_ptep, int max_nr, pte_t pte)\n \t\tif (!pte_same(pte, expected_pte))\n \t\t\tbreak;\n \t\tentry = softleaf_from_pte(pte);\n-\t\tif (lookup_swap_cgroup_id(entry) != cgroup_id)\n-\t\t\tbreak;\n \t\texpected_pte = pte_next_swp_offset(expected_pte);\n \t\tptep++;\n \t}\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex d9ff44b77409..d0f50019d733 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -4794,6 +4794,7 @@ int mem_cgroup_charge_hugetlb(struct folio *folio, gfp_t gfp)\n  * @mm: mm context of the victim\n  * @gfp: reclaim mode\n  * @entry: swap entry for which the folio is allocated\n+ * @id: the mem cgroup id\n  *\n  * This function charges a folio allocated for swapin. Please call this before\n  * adding the folio to the swapcache.\n@@ -4801,19 +4802,21 @@ int mem_cgroup_charge_hugetlb(struct folio *folio, gfp_t gfp)\n  * Returns 0 on success. Otherwise, an error code is returned.\n  */\n int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n-\t\t\t\t  gfp_t gfp, swp_entry_t entry)\n+\t\t\t\t   gfp_t gfp, swp_entry_t entry, unsigned short id)\n {\n \tstruct mem_cgroup *memcg, *swap_memcg;\n+\tunsigned short memcg_id;\n \tunsigned int nr_pages;\n-\tunsigned short id;\n \tint ret;\n \n \tif (mem_cgroup_disabled())\n \t\treturn 0;\n \n-\tid = lookup_swap_cgroup_id(entry);\n+\tmemcg_id = lookup_swap_cgroup_id(entry);\n \tnr_pages = folio_nr_pages(folio);\n \n+\tWARN_ON_ONCE(id != memcg_id);\n+\n \trcu_read_lock();\n \tswap_memcg = mem_cgroup_from_private_id(id);\n \tif (!swap_memcg) {\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex cc4bf40320ef..5ab3a41fe42c 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -251,8 +251,11 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \t__swap_cache_add_folio(ci, folio, entry);\n \tspin_unlock(&ci->lock);\n \n+\t/* With swap table, we must have a shadow, for memcg tracking */\n+\tWARN_ON(!shadow);\n+\n \tif (mem_cgroup_swapin_charge_folio(folio, vmf ? vmf->vma->vm_mm : NULL,\n-\t\t\t\t\t   gfp, entry)) {\n+\t\t\t\t\t   gfp, entry, shadow_to_memcgid(shadow))) {\n \t\tspin_lock(&ci->lock);\n \t\t__swap_cache_del_folio(ci, folio, shadow, false, false);\n \t\tspin_unlock(&ci->lock);\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "The reviewer noted that since swap table now contains the swap cgroup info all the time, the swap cgroup array can be dropped.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Now swap table contains the swap cgropu info all the time, the swap\ncgroup array can be dropped.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n MAINTAINERS                 |   1 -\n include/linux/memcontrol.h  |   6 +-\n include/linux/swap_cgroup.h |  47 ------------\n mm/Makefile                 |   3 -\n mm/internal.h               |   1 -\n mm/memcontrol-v1.c          |   1 -\n mm/memcontrol.c             |  19 ++---\n mm/swap_cgroup.c            | 171 --------------------------------------------\n mm/swap_state.c             |   3 +-\n mm/swapfile.c               |  23 +-----\n 10 files changed, 11 insertions(+), 264 deletions(-)\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex aa1734a12887..05e633611e0b 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -6571,7 +6571,6 @@ F:\tmm/memcontrol.c\n F:\tmm/memcontrol-v1.c\n F:\tmm/memcontrol-v1.h\n F:\tmm/page_counter.c\n-F:\tmm/swap_cgroup.c\n F:\tsamples/cgroup/*\n F:\ttools/testing/selftests/cgroup/memcg_protection.m\n F:\ttools/testing/selftests/cgroup/test_hugetlb_memcg.c\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 8fc794baf736..4bfe905bffb0 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -667,8 +667,7 @@ static inline int mem_cgroup_charge(struct folio *folio, struct mm_struct *mm,\n int mem_cgroup_charge_hugetlb(struct folio* folio, gfp_t gfp);\n \n int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n-\t\t\t\t   gfp_t gfp, swp_entry_t entry,\n-\t\t\t\t   unsigned short id);\n+\t\t\t\t   gfp_t gfp, unsigned short id);\n \n void __mem_cgroup_uncharge(struct folio *folio);\n \n@@ -1146,8 +1145,7 @@ static inline int mem_cgroup_charge_hugetlb(struct folio* folio, gfp_t gfp)\n }\n \n static inline int mem_cgroup_swapin_charge_folio(struct folio *folio,\n-\t\t\tstruct mm_struct *mm, gfp_t gfp, swp_entry_t entry,\n-\t\t\tunsigned short id)\n+\t\t\tstruct mm_struct *mm, gfp_t gfp, unsigned short id)\n {\n \treturn 0;\n }\ndiff --git a/include/linux/swap_cgroup.h b/include/linux/swap_cgroup.h\ndeleted file mode 100644\nindex 91cdf12190a0..000000000000\n--- a/include/linux/swap_cgroup.h\n+++ /dev/null\n@@ -1,47 +0,0 @@\n-/* SPDX-License-Identifier: GPL-2.0 */\n-#ifndef __LINUX_SWAP_CGROUP_H\n-#define __LINUX_SWAP_CGROUP_H\n-\n-#include <linux/swap.h>\n-\n-#if defined(CONFIG_MEMCG) && defined(CONFIG_SWAP)\n-\n-extern void swap_cgroup_record(struct folio *folio, unsigned short id, swp_entry_t ent);\n-extern unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents);\n-extern unsigned short lookup_swap_cgroup_id(swp_entry_t ent);\n-extern int swap_cgroup_swapon(int type, unsigned long max_pages);\n-extern void swap_cgroup_swapoff(int type);\n-\n-#else\n-\n-static inline\n-void swap_cgroup_record(struct folio *folio, unsigned short id, swp_entry_t ent)\n-{\n-}\n-\n-static inline\n-unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents)\n-{\n-\treturn 0;\n-}\n-\n-static inline\n-unsigned short lookup_swap_cgroup_id(swp_entry_t ent)\n-{\n-\treturn 0;\n-}\n-\n-static inline int\n-swap_cgroup_swapon(int type, unsigned long max_pages)\n-{\n-\treturn 0;\n-}\n-\n-static inline void swap_cgroup_swapoff(int type)\n-{\n-\treturn;\n-}\n-\n-#endif\n-\n-#endif /* __LINUX_SWAP_CGROUP_H */\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 8ad2ab08244e..eff9f9e7e061 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -103,9 +103,6 @@ obj-$(CONFIG_PAGE_COUNTER) += page_counter.o\n obj-$(CONFIG_LIVEUPDATE_MEMFD) += memfd_luo.o\n obj-$(CONFIG_MEMCG_V1) += memcontrol-v1.o\n obj-$(CONFIG_MEMCG) += memcontrol.o vmpressure.o\n-ifdef CONFIG_SWAP\n-obj-$(CONFIG_MEMCG) += swap_cgroup.o\n-endif\n ifdef CONFIG_BPF_SYSCALL\n obj-$(CONFIG_MEMCG) += bpf_memcontrol.o\n endif\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 416d3401aa17..26691885d75f 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -16,7 +16,6 @@\n #include <linux/rmap.h>\n #include <linux/swap.h>\n #include <linux/leafops.h>\n-#include <linux/swap_cgroup.h>\n #include <linux/tracepoint-defs.h>\n \n /* Internal core VMA manipulation functions. */\ndiff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.c\nindex 038e630dc7e1..eff18eda0707 100644\n--- a/mm/memcontrol-v1.c\n+++ b/mm/memcontrol-v1.c\n@@ -5,7 +5,6 @@\n #include <linux/mm_inline.h>\n #include <linux/pagewalk.h>\n #include <linux/backing-dev.h>\n-#include <linux/swap_cgroup.h>\n #include <linux/eventfd.h>\n #include <linux/poll.h>\n #include <linux/sort.h>\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex d0f50019d733..8d0c9f3a011e 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -54,7 +54,6 @@\n #include <linux/vmpressure.h>\n #include <linux/memremap.h>\n #include <linux/mm_inline.h>\n-#include <linux/swap_cgroup.h>\n #include <linux/cpu.h>\n #include <linux/oom.h>\n #include <linux/lockdep.h>\n@@ -4793,7 +4792,6 @@ int mem_cgroup_charge_hugetlb(struct folio *folio, gfp_t gfp)\n  * @folio: folio to charge.\n  * @mm: mm context of the victim\n  * @gfp: reclaim mode\n- * @entry: swap entry for which the folio is allocated\n  * @id: the mem cgroup id\n  *\n  * This function charges a folio allocated for swapin. Please call this before\n@@ -4802,21 +4800,17 @@ int mem_cgroup_charge_hugetlb(struct folio *folio, gfp_t gfp)\n  * Returns 0 on success. Otherwise, an error code is returned.\n  */\n int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n-\t\t\t\t   gfp_t gfp, swp_entry_t entry, unsigned short id)\n+\t\t\t\t   gfp_t gfp, unsigned short id)\n {\n \tstruct mem_cgroup *memcg, *swap_memcg;\n-\tunsigned short memcg_id;\n \tunsigned int nr_pages;\n \tint ret;\n \n \tif (mem_cgroup_disabled())\n \t\treturn 0;\n \n-\tmemcg_id = lookup_swap_cgroup_id(entry);\n \tnr_pages = folio_nr_pages(folio);\n \n-\tWARN_ON_ONCE(id != memcg_id);\n-\n \trcu_read_lock();\n \tswap_memcg = mem_cgroup_from_private_id(id);\n \tif (!swap_memcg) {\n@@ -4836,10 +4830,11 @@ int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n \n \t/*\n \t * On successful charge, the folio itself now belongs to the memcg,\n-\t * so is folio->swap. So we can release the swap cgroup table's\n-\t * pinning of the private id.\n+\t * so is folio->swap. And the folio takes place of the shadow in\n+\t * the swap table so we can release the shadow's pinning of the\n+\t * private id.\n \t */\n-\tswap_cgroup_clear(folio->swap, nr_pages);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n \tmem_cgroup_private_id_put(swap_memcg, nr_pages);\n \n \t/*\n@@ -5324,8 +5319,6 @@ struct mem_cgroup *__mem_cgroup_swap_free_folio(struct folio *folio,\n {\n \tunsigned int nr_pages = folio_nr_pages(folio);\n \tstruct mem_cgroup *memcg, *swap_memcg;\n-\tswp_entry_t entry = folio->swap;\n-\tunsigned short id;\n \n \tVM_WARN_ON_ONCE_FOLIO(!folio_memcg_charged(folio), folio);\n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n@@ -5337,8 +5330,6 @@ struct mem_cgroup *__mem_cgroup_swap_free_folio(struct folio *folio,\n \t */\n \tmemcg = folio_memcg(folio);\n \tswap_memcg = mem_cgroup_private_id_get_online(memcg, nr_pages);\n-\tid = mem_cgroup_private_id(swap_memcg);\n-\tswap_cgroup_record(folio, id, entry);\n \n \tif (reclaim && do_memsw_account()) {\n \t\tmemcg1_swapout(folio, swap_memcg);\ndiff --git a/mm/swap_cgroup.c b/mm/swap_cgroup.c\ndeleted file mode 100644\nindex b5a7f21c3afe..000000000000\n--- a/mm/swap_cgroup.c\n+++ /dev/null\n@@ -1,171 +0,0 @@\n-// SPDX-License-Identifier: GPL-2.0\n-#include <linux/swap_cgroup.h>\n-#include <linux/vmalloc.h>\n-#include <linux/mm.h>\n-\n-#include <linux/swapops.h> /* depends on mm.h include */\n-\n-static DEFINE_MUTEX(swap_cgroup_mutex);\n-\n-/* Pack two cgroup id (short) of two entries in one swap_cgroup (atomic_t) */\n-#define ID_PER_SC (sizeof(struct swap_cgroup) / sizeof(unsigned short))\n-#define ID_SHIFT (BITS_PER_TYPE(unsigned short))\n-#define ID_MASK (BIT(ID_SHIFT) - 1)\n-struct swap_cgroup {\n-\tatomic_t ids;\n-};\n-\n-struct swap_cgroup_ctrl {\n-\tstruct swap_cgroup *map;\n-};\n-\n-static struct swap_cgroup_ctrl swap_cgroup_ctrl[MAX_SWAPFILES];\n-\n-static unsigned short __swap_cgroup_id_lookup(struct swap_cgroup *map,\n-\t\t\t\t\t      pgoff_t offset)\n-{\n-\tunsigned int shift = (offset % ID_PER_SC) * ID_SHIFT;\n-\tunsigned int old_ids = atomic_read(&map[offset / ID_PER_SC].ids);\n-\n-\tBUILD_BUG_ON(!is_power_of_2(ID_PER_SC));\n-\tBUILD_BUG_ON(sizeof(struct swap_cgroup) != sizeof(atomic_t));\n-\n-\treturn (old_ids >> shift) & ID_MASK;\n-}\n-\n-static unsigned short __swap_cgroup_id_xchg(struct swap_cgroup *map,\n-\t\t\t\t\t    pgoff_t offset,\n-\t\t\t\t\t    unsigned short new_id)\n-{\n-\tunsigned short old_id;\n-\tstruct swap_cgroup *sc = &map[offset / ID_PER_SC];\n-\tunsigned int shift = (offset % ID_PER_SC) * ID_SHIFT;\n-\tunsigned int new_ids, old_ids = atomic_read(&sc->ids);\n-\n-\tdo {\n-\t\told_id = (old_ids >> shift) & ID_MASK;\n-\t\tnew_ids = (old_ids & ~(ID_MASK << shift));\n-\t\tnew_ids |= ((unsigned int)new_id) << shift;\n-\t} while (!atomic_try_cmpxchg(&sc->ids, &old_ids, new_ids));\n-\n-\treturn old_id;\n-}\n-\n-/**\n- * swap_cgroup_record - record mem_cgroup for a set of swap entries.\n- * These entries must belong to one single folio, and that folio\n- * must be being charged for swap space (swap out).\n- *\n- * @folio: the folio that the swap entry belongs to\n- * @id: mem_cgroup ID to be recorded\n- * @ent: the first swap entry to be recorded\n- */\n-void swap_cgroup_record(struct folio *folio, unsigned short id,\n-\t\t\tswp_entry_t ent)\n-{\n-\tunsigned int nr_ents = folio_nr_pages(folio);\n-\tstruct swap_cgroup *map;\n-\tpgoff_t offset, end;\n-\tunsigned short old;\n-\n-\toffset = swp_offset(ent);\n-\tend = offset + nr_ents;\n-\tmap = swap_cgroup_ctrl[swp_type(ent)].map;\n-\n-\tdo {\n-\t\told = __swap_cgroup_id_xchg(map, offset, id);\n-\t\tVM_WARN_ON_ONCE(old);\n-\t} while (++offset != end);\n-}\n-\n-/**\n- * swap_cgroup_clear - clear mem_cgroup for a set of swap entries.\n- * These entries must be being uncharged from swap. They either\n- * belongs to one single folio in the swap cache (swap in for\n- * cgroup v1), or no longer have any users (slot freeing).\n- *\n- * @ent: the first swap entry to be recorded into\n- * @nr_ents: number of swap entries to be recorded\n- *\n- * Returns the existing old value.\n- */\n-unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents)\n-{\n-\tpgoff_t offset, end;\n-\tstruct swap_cgroup *map;\n-\tunsigned short old, iter = 0;\n-\n-\toffset = swp_offset(ent);\n-\tend = offset + nr_ents;\n-\tmap = swap_cgroup_ctrl[swp_type(ent)].map;\n-\n-\tdo {\n-\t\told = __swap_cgroup_id_xchg(map, offset, 0);\n-\t\tif (!iter)\n-\t\t\titer = old;\n-\t\tVM_BUG_ON(iter != old);\n-\t} while (++offset != end);\n-\n-\treturn old;\n-}\n-\n-/**\n- * lookup_swap_cgroup_id - lookup mem_cgroup id tied to swap entry\n- * @ent: swap entry to be looked up.\n- *\n- * Returns ID of mem_cgroup at success. 0 at failure. (0 is invalid ID)\n- */\n-unsigned short lookup_swap_cgroup_id(swp_entry_t ent)\n-{\n-\tstruct swap_cgroup_ctrl *ctrl;\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn 0;\n-\n-\tctrl = &swap_cgroup_ctrl[swp_type(ent)];\n-\treturn __swap_cgroup_id_lookup(ctrl->map, swp_offset(ent));\n-}\n-\n-int swap_cgroup_swapon(int type, unsigned long max_pages)\n-{\n-\tstruct swap_cgroup *map;\n-\tstruct swap_cgroup_ctrl *ctrl;\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn 0;\n-\n-\tBUILD_BUG_ON(sizeof(unsigned short) * ID_PER_SC !=\n-\t\t     sizeof(struct swap_cgroup));\n-\tmap = vzalloc(DIV_ROUND_UP(max_pages, ID_PER_SC) *\n-\t\t      sizeof(struct swap_cgroup));\n-\tif (!map)\n-\t\tgoto nomem;\n-\n-\tctrl = &swap_cgroup_ctrl[type];\n-\tmutex_lock(&swap_cgroup_mutex);\n-\tctrl->map = map;\n-\tmutex_unlock(&swap_cgroup_mutex);\n-\n-\treturn 0;\n-nomem:\n-\tpr_info(\"couldn't allocate enough memory for swap_cgroup\\n\");\n-\tpr_info(\"swap_cgroup can be disabled by swapaccount=0 boot option\\n\");\n-\treturn -ENOMEM;\n-}\n-\n-void swap_cgroup_swapoff(int type)\n-{\n-\tstruct swap_cgroup *map;\n-\tstruct swap_cgroup_ctrl *ctrl;\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn;\n-\n-\tmutex_lock(&swap_cgroup_mutex);\n-\tctrl = &swap_cgroup_ctrl[type];\n-\tmap = ctrl->map;\n-\tctrl->map = NULL;\n-\tmutex_unlock(&swap_cgroup_mutex);\n-\n-\tvfree(map);\n-}\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 5ab3a41fe42c..c6ba15de4094 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -163,7 +163,6 @@ static int __swap_cache_check_batch(struct swap_cluster_info *ci,\n \t*shadowp = swp_tb_to_shadow(old_tb);\n \tmemcgid = shadow_to_memcgid(*shadowp);\n \n-\tWARN_ON_ONCE(!mem_cgroup_disabled() && !memcgid);\n \tdo {\n \t\told_tb = __swap_table_get(ci, ci_off);\n \t\tif (unlikely(swp_tb_is_folio(old_tb)) ||\n@@ -255,7 +254,7 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \tWARN_ON(!shadow);\n \n \tif (mem_cgroup_swapin_charge_folio(folio, vmf ? vmf->vma->vm_mm : NULL,\n-\t\t\t\t\t   gfp, entry, shadow_to_memcgid(shadow))) {\n+\t\t\t\t\t   gfp, shadow_to_memcgid(shadow))) {\n \t\tspin_lock(&ci->lock);\n \t\t__swap_cache_del_folio(ci, folio, shadow, false, false);\n \t\tspin_unlock(&ci->lock);\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex cd2d3b2ca6f0..de34f1990209 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -45,7 +45,6 @@\n \n #include <asm/tlbflush.h>\n #include <linux/leafops.h>\n-#include <linux/swap_cgroup.h>\n #include \"swap_table.h\"\n #include \"internal.h\"\n #include \"swap_table.h\"\n@@ -1885,8 +1884,7 @@ void __swap_cluster_free_entries(struct swap_info_struct *si,\n {\n \tvoid *shadow;\n \tunsigned long old_tb;\n-\tunsigned int type = si->type;\n-\tunsigned int id = 0, id_iter, id_check;\n+\tunsigned int id = 0, id_iter;\n \tunsigned int ci_off = ci_start, ci_end = ci_start + nr_pages;\n \tunsigned long offset = cluster_offset(si, ci);\n \tunsigned int ci_batch = ci_off;\n@@ -1903,23 +1901,15 @@ void __swap_cluster_free_entries(struct swap_info_struct *si,\n \t\tshadow = swp_tb_to_shadow(old_tb);\n \t\tid_iter = shadow_to_memcgid(shadow);\n \t\tif (id != id_iter) {\n-\t\t\tif (id) {\n-\t\t\t\tid_check = swap_cgroup_clear(swp_entry(type, offset + ci_batch),\n-\t\t\t\t\t\t\t     ci_off - ci_batch);\n-\t\t\t\tWARN_ON(id != id_check);\n+\t\t\tif (id)\n \t\t\t\tmem_cgroup_uncharge_swap(id, ci_off - ci_batch);\n-\t\t\t}\n \t\t\tid = id_iter;\n \t\t\tci_batch = ci_off;\n \t\t}\n \t} while (++ci_off < ci_end);\n \n-\tif (id) {\n-\t\tid_check = swap_cgroup_clear(swp_entry(type, offset + ci_batch),\n-\t\t\t\t\t     ci_off - ci_batch);\n-\t\tWARN_ON(id != id_check);\n+\tif (id)\n \t\tmem_cgroup_uncharge_swap(id, ci_off - ci_batch);\n-\t}\n \n \tswap_range_free(si, offset + ci_start, nr_pages);\n \tswap_cluster_assert_empty(ci, ci_start, nr_pages, false);\n@@ -3034,8 +3024,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tp->global_cluster = NULL;\n \tkvfree(zeromap);\n \tfree_swap_cluster_info(cluster_info, maxpages);\n-\t/* Destroy swap account information */\n-\tswap_cgroup_swapoff(p->type);\n \n \tinode = mapping->host;\n \n@@ -3567,10 +3555,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tif (error)\n \t\tgoto bad_swap_unlock_inode;\n \n-\terror = swap_cgroup_swapon(si->type, maxpages);\n-\tif (error)\n-\t\tgoto bad_swap_unlock_inode;\n-\n \t/*\n \t * Use kvmalloc_array instead of bitmap_zalloc as the allocation order might\n \t * be above MAX_PAGE_ORDER incase of a large swap file.\n@@ -3681,7 +3665,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tsi->global_cluster = NULL;\n \tinode = NULL;\n \tdestroy_swap_extents(si, swap_file);\n-\tswap_cgroup_swapoff(si->type);\n \tfree_swap_cluster_info(si->cluster_info, si->max);\n \tsi->cluster_info = NULL;\n \tkvfree(si->zeromap);\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that the patch can simplify the swap infrastructure by merging the zeromap into the swap table, and suggested reserving one bit for counting to achieve this.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "suggested improvement",
                "requested change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "By reserving one bit for the counting part, we can easily merge the\nzeromap into the swap table.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n include/linux/swap.h |   1 -\n mm/memory.c          |  12 ++----\n mm/page_io.c         |  28 ++++++++++----\n mm/swap.h            |  31 ----------------\n mm/swap_state.c      |  23 ++++++++----\n mm/swap_table.h      | 103 +++++++++++++++++++++++++++++++++++++++++++--------\n mm/swapfile.c        |  27 +-------------\n 7 files changed, 127 insertions(+), 98 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 66cf657a1f35..bc871d8a1e99 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -254,7 +254,6 @@ struct swap_info_struct {\n \tstruct plist_node list;\t\t/* entry in swap_active_head */\n \tsigned char\ttype;\t\t/* strange name for an index */\n \tunsigned int\tmax;\t\t/* size of this swap device */\n-\tunsigned long *zeromap;\t\t/* kvmalloc'ed bitmap to track zero pages */\n \tstruct swap_cluster_info *cluster_info; /* cluster info. Only for SSD */\n \tstruct list_head free_clusters; /* free clusters list */\n \tstruct list_head full_clusters; /* full clusters list */\ndiff --git a/mm/memory.c b/mm/memory.c\nindex e58f976508b3..8df169fced0d 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -88,6 +88,7 @@\n \n #include \"pgalloc-track.h\"\n #include \"internal.h\"\n+#include \"swap_table.h\"\n #include \"swap.h\"\n \n #if defined(LAST_CPUPID_NOT_IN_PAGE_FLAGS) && !defined(CONFIG_COMPILE_TEST)\n@@ -4522,13 +4523,11 @@ static vm_fault_t handle_pte_marker(struct vm_fault *vmf)\n \n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n /*\n- * Check if the PTEs within a range are contiguous swap entries\n- * and have consistent swapcache, zeromap.\n+ * Check if the PTEs within a range are contiguous swap entries.\n  */\n static bool can_swapin_thp(struct vm_fault *vmf, pte_t *ptep, int nr_pages)\n {\n \tunsigned long addr;\n-\tsoftleaf_t entry;\n \tint idx;\n \tpte_t pte;\n \n@@ -4538,18 +4537,13 @@ static bool can_swapin_thp(struct vm_fault *vmf, pte_t *ptep, int nr_pages)\n \n \tif (!pte_same(pte, pte_move_swp_offset(vmf->orig_pte, -idx)))\n \t\treturn false;\n-\tentry = softleaf_from_pte(pte);\n-\tif (swap_pte_batch(ptep, nr_pages, pte) != nr_pages)\n-\t\treturn false;\n-\n \t/*\n \t * swap_read_folio() can't handle the case a large folio is hybridly\n \t * from different backends. And they are likely corner cases. Similar\n \t * things might be added once zswap support large folios.\n \t */\n-\tif (unlikely(swap_zeromap_batch(entry, nr_pages, NULL) != nr_pages))\n+\tif (swap_pte_batch(ptep, nr_pages, pte) != nr_pages)\n \t\treturn false;\n-\n \treturn true;\n }\n \ndiff --git a/mm/page_io.c b/mm/page_io.c\nindex a2c034660c80..5a0b5034489b 100644\n--- a/mm/page_io.c\n+++ b/mm/page_io.c\n@@ -26,6 +26,7 @@\n #include <linux/delayacct.h>\n #include <linux/zswap.h>\n #include \"swap.h\"\n+#include \"swap_table.h\"\n \n static void __end_swap_bio_write(struct bio *bio)\n {\n@@ -204,15 +205,20 @@ static bool is_folio_zero_filled(struct folio *folio)\n static void swap_zeromap_folio_set(struct folio *folio)\n {\n \tstruct obj_cgroup *objcg = get_obj_cgroup_from_folio(folio);\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n \tint nr_pages = folio_nr_pages(folio);\n+\tstruct swap_cluster_info *ci;\n \tswp_entry_t entry;\n \tunsigned int i;\n \n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n+\n+\tci = swap_cluster_get_and_lock(folio);\n \tfor (i = 0; i < folio_nr_pages(folio); i++) {\n \t\tentry = page_swap_entry(folio_page(folio, i));\n-\t\tset_bit(swp_offset(entry), sis->zeromap);\n+\t\t__swap_table_set_zero(ci, swp_cluster_offset(entry));\n \t}\n+\tswap_cluster_unlock(ci);\n \n \tcount_vm_events(SWPOUT_ZERO, nr_pages);\n \tif (objcg) {\n@@ -223,14 +229,19 @@ static void swap_zeromap_folio_set(struct folio *folio)\n \n static void swap_zeromap_folio_clear(struct folio *folio)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tstruct swap_cluster_info *ci;\n \tswp_entry_t entry;\n \tunsigned int i;\n \n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n+\n+\tci = swap_cluster_get_and_lock(folio);\n \tfor (i = 0; i < folio_nr_pages(folio); i++) {\n \t\tentry = page_swap_entry(folio_page(folio, i));\n-\t\tclear_bit(swp_offset(entry), sis->zeromap);\n+\t\t__swap_table_clear_zero(ci, swp_cluster_offset(entry));\n \t}\n+\tswap_cluster_unlock(ci);\n }\n \n /*\n@@ -255,10 +266,9 @@ int swap_writeout(struct folio *folio, struct swap_iocb **swap_plug)\n \t}\n \n \t/*\n-\t * Use a bitmap (zeromap) to avoid doing IO for zero-filled pages.\n-\t * The bits in zeromap are protected by the locked swapcache folio\n-\t * and atomic updates are used to protect against read-modify-write\n-\t * corruption due to other zero swap entries seeing concurrent updates.\n+\t * Use the swap table zero mark to avoid doing IO for zero-filled\n+\t * pages. The zero mark is protected by the cluster lock, which is\n+\t * acquired internally by swap_zeromap_folio_set/clear.\n \t */\n \tif (is_folio_zero_filled(folio)) {\n \t\tswap_zeromap_folio_set(folio);\n@@ -511,6 +521,8 @@ static bool swap_read_folio_zeromap(struct folio *folio)\n \tstruct obj_cgroup *objcg;\n \tbool is_zeromap;\n \n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n+\n \t/*\n \t * Swapping in a large folio that is partially in the zeromap is not\n \t * currently handled. Return true without marking the folio uptodate so\ndiff --git a/mm/swap.h b/mm/swap.h\nindex c95f5fafea42..cb1ab20d83d5 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -312,31 +312,6 @@ static inline unsigned int folio_swap_flags(struct folio *folio)\n \treturn __swap_entry_to_info(folio->swap)->flags;\n }\n \n-/*\n- * Return the count of contiguous swap entries that share the same\n- * zeromap status as the starting entry. If is_zeromap is not NULL,\n- * it will return the zeromap status of the starting entry.\n- */\n-static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n-\t\tbool *is_zeromap)\n-{\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(entry);\n-\tunsigned long start = swp_offset(entry);\n-\tunsigned long end = start + max_nr;\n-\tbool first_bit;\n-\n-\tfirst_bit = test_bit(start, sis->zeromap);\n-\tif (is_zeromap)\n-\t\t*is_zeromap = first_bit;\n-\n-\tif (max_nr <= 1)\n-\t\treturn max_nr;\n-\tif (first_bit)\n-\t\treturn find_next_zero_bit(sis->zeromap, end, start) - start;\n-\telse\n-\t\treturn find_next_bit(sis->zeromap, end, start) - start;\n-}\n-\n #else /* CONFIG_SWAP */\n struct swap_iocb;\n static inline struct swap_cluster_info *swap_cluster_lock(\n@@ -475,11 +450,5 @@ static inline unsigned int folio_swap_flags(struct folio *folio)\n {\n \treturn 0;\n }\n-\n-static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n-\t\tbool *has_zeromap)\n-{\n-\treturn 0;\n-}\n #endif /* CONFIG_SWAP */\n #endif /* _MM_SWAP_H */\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex c6ba15de4094..419419e18a47 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -138,6 +138,7 @@ void *swap_cache_get_shadow(swp_entry_t entry)\n }\n \n static int __swap_cache_check_batch(struct swap_cluster_info *ci,\n+\t\t\t\t    swp_entry_t entry,\n \t\t\t\t    unsigned int ci_off, unsigned int ci_targ,\n \t\t\t\t    unsigned int nr, void **shadowp)\n {\n@@ -148,6 +149,13 @@ static int __swap_cache_check_batch(struct swap_cluster_info *ci,\n \tif (unlikely(!ci->table))\n \t\treturn -ENOENT;\n \n+\t/*\n+\t * TODO: Swap of large folio that is partially in the zeromap\n+\t * is not supported.\n+\t */\n+\tif (nr > 1 && swap_zeromap_batch(entry, nr, NULL) != nr)\n+\t\treturn -EBUSY;\n+\n \t/*\n \t * If the target slot is not suitable for adding swap cache, return\n \t * -EEXIST or -ENOENT. If the batch is not suitable, could be a\n@@ -190,7 +198,7 @@ void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \tdo {\n \t\told_tb = __swap_table_get(ci, ci_off);\n \t\tVM_WARN_ON_ONCE(swp_tb_is_folio(old_tb));\n-\t\t__swap_table_set(ci, ci_off, pfn_to_swp_tb(pfn, __swp_tb_get_count(old_tb)));\n+\t\t__swap_table_set(ci, ci_off, pfn_to_swp_tb(pfn, __swp_tb_get_flags(old_tb)));\n \t} while (++ci_off < ci_end);\n \n \tfolio_ref_add(folio, nr_pages);\n@@ -218,7 +226,7 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \n \t/* First check if the range is available */\n \tspin_lock(&ci->lock);\n-\terr = __swap_cache_check_batch(ci, ci_off, ci_targ, nr_pages, &shadow);\n+\terr = __swap_cache_check_batch(ci, entry, ci_off, ci_targ, nr_pages, &shadow);\n \tspin_unlock(&ci->lock);\n \tif (unlikely(err))\n \t\treturn ERR_PTR(err);\n@@ -236,7 +244,7 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \n \t/* Double check the range is still not in conflict */\n \tspin_lock(&ci->lock);\n-\terr = __swap_cache_check_batch(ci, ci_off, ci_targ, nr_pages, &shadow_check);\n+\terr = __swap_cache_check_batch(ci, entry, ci_off, ci_targ, nr_pages, &shadow_check);\n \tif (unlikely(err) || shadow_check != shadow) {\n \t\tspin_unlock(&ci->lock);\n \t\tfolio_put(folio);\n@@ -338,7 +346,6 @@ struct folio *swap_cache_alloc_folio(swp_entry_t targ_entry, gfp_t gfp_mask,\n void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n \t\t\t    void *shadow, bool charged, bool reclaim)\n {\n-\tint count;\n \tunsigned long old_tb;\n \tstruct swap_info_struct *si;\n \tstruct mem_cgroup *memcg = NULL;\n@@ -375,13 +382,13 @@ void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n \t\told_tb = __swap_table_get(ci, ci_off);\n \t\tWARN_ON_ONCE(!swp_tb_is_folio(old_tb) ||\n \t\t\t     swp_tb_to_folio(old_tb) != folio);\n-\t\tcount = __swp_tb_get_count(old_tb);\n-\t\tif (count)\n+\t\tif (__swp_tb_get_count(old_tb))\n \t\t\tfolio_swapped = true;\n \t\telse\n \t\t\tneed_free = true;\n \t\t/* If shadow is NULL, we sets an empty shadow. */\n-\t\t__swap_table_set(ci, ci_off, shadow_to_swp_tb(shadow, count));\n+\t\t__swap_table_set(ci, ci_off, shadow_to_swp_tb(shadow,\n+\t\t\t\t __swp_tb_get_flags(old_tb)));\n \t} while (++ci_off < ci_end);\n \n \tfolio->swap.val = 0;\n@@ -460,7 +467,7 @@ void __swap_cache_replace_folio(struct swap_cluster_info *ci,\n \tdo {\n \t\told_tb = __swap_table_get(ci, ci_off);\n \t\tWARN_ON_ONCE(!swp_tb_is_folio(old_tb) || swp_tb_to_folio(old_tb) != old);\n-\t\t__swap_table_set(ci, ci_off, pfn_to_swp_tb(pfn, __swp_tb_get_count(old_tb)));\n+\t\t__swap_table_set(ci, ci_off, pfn_to_swp_tb(pfn, __swp_tb_get_flags(old_tb)));\n \t} while (++ci_off < ci_end);\n \n \t/*\ndiff --git a/mm/swap_table.h b/mm/swap_table.h\nindex 8415ffbe2b9c..6d3d773e1908 100644\n--- a/mm/swap_table.h\n+++ b/mm/swap_table.h\n@@ -21,12 +21,14 @@ struct swap_table {\n  * Swap table entry type and bits layouts:\n  *\n  * NULL:     |---------------- 0 ---------------| - Free slot\n- * Shadow:   | SWAP_COUNT |---- SHADOW_VAL ---|1| - Swapped out slot\n- * PFN:      | SWAP_COUNT |------ PFN -------|10| - Cached slot\n+ * Shadow:   |SWAP_COUNT|Z|---- SHADOW_VAL ---|1| - Swapped out slot\n+ * PFN:      |SWAP_COUNT|Z|------ PFN -------|10| - Cached slot\n  * Pointer:  |----------- Pointer ----------|100| - (Unused)\n  * Bad:      |------------- 1 -------------|1000| - Bad slot\n  *\n- * SWAP_COUNT is `SWP_TB_COUNT_BITS` long, each entry is an atomic long.\n+ * COUNT is `SWP_TB_COUNT_BITS` long, Z is the `SWP_TB_ZERO_MARK` bit,\n+ * and together they form the `SWP_TB_FLAGS_BITS` wide flags field.\n+ * Each entry is an atomic long.\n  *\n  * Usages:\n  *\n@@ -70,16 +72,21 @@ struct swap_table {\n #define SWP_TB_PFN_MARK_MASK\t(BIT(SWP_TB_PFN_MARK_BITS) - 1)\n \n /* SWAP_COUNT part for PFN or shadow, the width can be shrunk or extended */\n-#define SWP_TB_COUNT_BITS      min(4, BITS_PER_LONG - SWP_TB_PFN_BITS)\n+#define SWP_TB_FLAGS_BITS\tmin(5, BITS_PER_LONG - SWP_TB_PFN_BITS)\n+#define SWP_TB_COUNT_BITS\t(SWP_TB_FLAGS_BITS - 1)\n+#define SWP_TB_FLAGS_MASK\t(~((~0UL) >> SWP_TB_FLAGS_BITS))\n #define SWP_TB_COUNT_MASK      (~((~0UL) >> SWP_TB_COUNT_BITS))\n+#define SWP_TB_FLAGS_SHIFT     (BITS_PER_LONG - SWP_TB_FLAGS_BITS)\n #define SWP_TB_COUNT_SHIFT     (BITS_PER_LONG - SWP_TB_COUNT_BITS)\n #define SWP_TB_COUNT_MAX       ((1 << SWP_TB_COUNT_BITS) - 1)\n \n+#define SWP_TB_ZERO_MARK\tBIT(BITS_PER_LONG - SWP_TB_COUNT_BITS - 1)\n+\n /* Bad slot: ends with 0b1000 and rests of bits are all 1 */\n #define SWP_TB_BAD\t\t((~0UL) << 3)\n \n /* Macro for shadow offset calculation */\n-#define SWAP_COUNT_SHIFT\tSWP_TB_COUNT_BITS\n+#define SWAP_COUNT_SHIFT\tSWP_TB_FLAGS_BITS\n \n /*\n  * Helpers for casting one type of info into a swap table entry.\n@@ -102,35 +109,43 @@ static inline unsigned long __count_to_swp_tb(unsigned char count)\n \treturn ((unsigned long)count) << SWP_TB_COUNT_SHIFT;\n }\n \n-static inline unsigned long pfn_to_swp_tb(unsigned long pfn, unsigned int count)\n+static inline unsigned long __flags_to_swp_tb(unsigned char flags)\n+{\n+\tBUILD_BUG_ON(SWP_TB_FLAGS_BITS > BITS_PER_BYTE);\n+\tVM_WARN_ON((flags >> 1) > SWP_TB_COUNT_MAX);\n+\treturn ((unsigned long)flags) << SWP_TB_FLAGS_SHIFT;\n+}\n+\n+\n+static inline unsigned long pfn_to_swp_tb(unsigned long pfn, unsigned char flags)\n {\n \tunsigned long swp_tb;\n \n \tBUILD_BUG_ON(sizeof(unsigned long) != sizeof(void *));\n \tBUILD_BUG_ON(SWAP_CACHE_PFN_BITS >\n-\t\t     (BITS_PER_LONG - SWP_TB_PFN_MARK_BITS - SWP_TB_COUNT_BITS));\n+\t\t     (BITS_PER_LONG - SWP_TB_PFN_MARK_BITS - SWP_TB_FLAGS_BITS));\n \n \tswp_tb = (pfn << SWP_TB_PFN_MARK_BITS) | SWP_TB_PFN_MARK;\n-\tVM_WARN_ON_ONCE(swp_tb & SWP_TB_COUNT_MASK);\n+\tVM_WARN_ON_ONCE(swp_tb & SWP_TB_FLAGS_MASK);\n \n-\treturn swp_tb | __count_to_swp_tb(count);\n+\treturn swp_tb | __flags_to_swp_tb(flags);\n }\n \n-static inline unsigned long folio_to_swp_tb(struct folio *folio, unsigned int count)\n+static inline unsigned long folio_to_swp_tb(struct folio *folio, unsigned char flags)\n {\n-\treturn pfn_to_swp_tb(folio_pfn(folio), count);\n+\treturn pfn_to_swp_tb(folio_pfn(folio), flags);\n }\n \n-static inline unsigned long shadow_to_swp_tb(void *shadow, unsigned int count)\n+static inline unsigned long shadow_to_swp_tb(void *shadow, unsigned char flags)\n {\n \tBUILD_BUG_ON((BITS_PER_XA_VALUE + 1) !=\n \t\t     BITS_PER_BYTE * sizeof(unsigned long));\n \tBUILD_BUG_ON((unsigned long)xa_mk_value(0) != SWP_TB_SHADOW_MARK);\n \n \tVM_WARN_ON_ONCE(shadow && !xa_is_value(shadow));\n-\tVM_WARN_ON_ONCE(shadow && ((unsigned long)shadow & SWP_TB_COUNT_MASK));\n+\tVM_WARN_ON_ONCE(shadow && ((unsigned long)shadow & SWP_TB_FLAGS_MASK));\n \n-\treturn (unsigned long)shadow | __count_to_swp_tb(count) | SWP_TB_SHADOW_MARK;\n+\treturn (unsigned long)shadow | SWP_TB_SHADOW_MARK | __flags_to_swp_tb(flags);\n }\n \n /*\n@@ -168,14 +183,14 @@ static inline bool swp_tb_is_countable(unsigned long swp_tb)\n static inline struct folio *swp_tb_to_folio(unsigned long swp_tb)\n {\n \tVM_WARN_ON(!swp_tb_is_folio(swp_tb));\n-\treturn pfn_folio((swp_tb & ~SWP_TB_COUNT_MASK) >> SWP_TB_PFN_MARK_BITS);\n+\treturn pfn_folio((swp_tb & ~SWP_TB_FLAGS_MASK) >> SWP_TB_PFN_MARK_BITS);\n }\n \n static inline void *swp_tb_to_shadow(unsigned long swp_tb)\n {\n \tVM_WARN_ON(!swp_tb_is_shadow(swp_tb));\n \t/* No shift needed, xa_value is stored as it is in the lower bits. */\n-\treturn (void *)(swp_tb & ~SWP_TB_COUNT_MASK);\n+\treturn (void *)(swp_tb & ~SWP_TB_FLAGS_MASK);\n }\n \n static inline unsigned char __swp_tb_get_count(unsigned long swp_tb)\n@@ -184,6 +199,12 @@ static inline unsigned char __swp_tb_get_count(unsigned long swp_tb)\n \treturn ((swp_tb & SWP_TB_COUNT_MASK) >> SWP_TB_COUNT_SHIFT);\n }\n \n+static inline unsigned char __swp_tb_get_flags(unsigned long swp_tb)\n+{\n+\tVM_WARN_ON(!swp_tb_is_countable(swp_tb));\n+\treturn ((swp_tb & SWP_TB_FLAGS_MASK) >> SWP_TB_FLAGS_SHIFT);\n+}\n+\n static inline int swp_tb_get_count(unsigned long swp_tb)\n {\n \tif (swp_tb_is_countable(swp_tb))\n@@ -247,4 +268,54 @@ static inline unsigned long swap_table_get(struct swap_cluster_info *ci,\n \n \treturn swp_tb;\n }\n+\n+static inline void __swap_table_set_zero(struct swap_cluster_info *ci,\n+\t\t\t\t\t unsigned int ci_off)\n+{\n+\tunsigned long swp_tb = __swap_table_get(ci, ci_off);\n+\n+\tVM_WARN_ON(!swp_tb_is_countable(swp_tb));\n+\tswp_tb |= SWP_TB_ZERO_MARK;\n+\t__swap_table_set(ci, ci_off, swp_tb);\n+}\n+\n+static inline void __swap_table_clear_zero(struct swap_cluster_info *ci,\n+\t\t\t\t\t   unsigned int ci_off)\n+{\n+\tunsigned long swp_tb = __swap_table_get(ci, ci_off);\n+\n+\tVM_WARN_ON(!swp_tb_is_countable(swp_tb));\n+\tswp_tb &= ~SWP_TB_ZERO_MARK;\n+\t__swap_table_set(ci, ci_off, swp_tb);\n+}\n+\n+/**\n+ * Return the count of contiguous swap entries that share the same\n+ * zeromap status as the starting entry. If is_zerop is not NULL,\n+ * it will return the zeromap status of the starting entry.\n+ *\n+ * Context: Caller must ensure the cluster containing the entries\n+ * that will be checked won't be freed.\n+ */\n+static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n+\t\t\t\t     bool *is_zerop)\n+{\n+\tbool is_zero;\n+\tunsigned long swp_tb;\n+\tstruct swap_cluster_info *ci = __swap_entry_to_cluster(entry);\n+\tunsigned int ci_start = swp_cluster_offset(entry), ci_off, ci_end;\n+\n+\tci_off = ci_start;\n+\tci_end = ci_off + max_nr;\n+\tswp_tb = swap_table_get(ci, ci_off);\n+\tis_zero = !!(swp_tb & SWP_TB_ZERO_MARK);\n+\tif (is_zerop)\n+\t\t*is_zerop = is_zero;\n+\twhile (++ci_off < ci_end) {\n+\t\tswp_tb = swap_table_get(ci, ci_off);\n+\t\tif (is_zero != !!(swp_tb & SWP_TB_ZERO_MARK))\n+\t\t\tbreak;\n+\t}\n+\treturn ci_off - ci_start;\n+}\n #endif\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex de34f1990209..4018e8694b72 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -918,7 +918,7 @@ static bool __swap_cluster_alloc_entries(struct swap_info_struct *si,\n \t\tnr_pages = 1;\n \t\tswap_cluster_assert_empty(ci, ci_off, 1, false);\n \t\t/* Sets a fake shadow as placeholder */\n-\t\t__swap_table_set(ci, ci_off, shadow_to_swp_tb(NULL, 1));\n+\t\t__swap_table_set(ci, ci_off, __swp_tb_mk_count(shadow_to_swp_tb(NULL, 0), 1));\n \t} else {\n \t\t/* Allocation without folio is only possible with hibernation */\n \t\tWARN_ON_ONCE(1);\n@@ -1308,14 +1308,8 @@ static void swap_range_free(struct swap_info_struct *si, unsigned long offset,\n \tvoid (*swap_slot_free_notify)(struct block_device *, unsigned long);\n \tunsigned int i;\n \n-\t/*\n-\t * Use atomic clear_bit operations only on zeromap instead of non-atomic\n-\t * bitmap_clear to prevent adjacent bits corruption due to simultaneous writes.\n-\t */\n-\tfor (i = 0; i < nr_entries; i++) {\n-\t\tclear_bit(offset + i, si->zeromap);\n+\tfor (i = 0; i < nr_entries; i++)\n \t\tzswap_invalidate(swp_entry(si->type, offset + i));\n-\t}\n \n \tif (si->flags & SWP_BLKDEV)\n \t\tswap_slot_free_notify =\n@@ -2921,7 +2915,6 @@ static void flush_percpu_swap_cluster(struct swap_info_struct *si)\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n-\tunsigned long *zeromap;\n \tstruct swap_cluster_info *cluster_info;\n \tstruct file *swap_file, *victim;\n \tstruct address_space *mapping;\n@@ -3009,8 +3002,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tswap_file = p->swap_file;\n \tp->swap_file = NULL;\n-\tzeromap = p->zeromap;\n-\tp->zeromap = NULL;\n \tmaxpages = p->max;\n \tcluster_info = p->cluster_info;\n \tp->max = 0;\n@@ -3022,7 +3013,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tmutex_unlock(&swapon_mutex);\n \tkfree(p->global_cluster);\n \tp->global_cluster = NULL;\n-\tkvfree(zeromap);\n \tfree_swap_cluster_info(cluster_info, maxpages);\n \n \tinode = mapping->host;\n@@ -3555,17 +3545,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tif (error)\n \t\tgoto bad_swap_unlock_inode;\n \n-\t/*\n-\t * Use kvmalloc_array instead of bitmap_zalloc as the allocation order might\n-\t * be above MAX_PAGE_ORDER incase of a large swap file.\n-\t */\n-\tsi->zeromap = kvmalloc_array(BITS_TO_LONGS(maxpages), sizeof(long),\n-\t\t\t\t     GFP_KERNEL | __GFP_ZERO);\n-\tif (!si->zeromap) {\n-\t\terror = -ENOMEM;\n-\t\tgoto bad_swap_unlock_inode;\n-\t}\n-\n \tif (si->bdev && bdev_stable_writes(si->bdev))\n \t\tsi->flags |= SWP_STABLE_WRITES;\n \n@@ -3667,8 +3646,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tdestroy_swap_extents(si, swap_file);\n \tfree_swap_cluster_info(si->cluster_info, si->max);\n \tsi->cluster_info = NULL;\n-\tkvfree(si->zeromap);\n-\tsi->zeromap = NULL;\n \t/*\n \t * Clear the SWP_USED flag after all resources are freed so\n \t * alloc_swap_info can reuse this si safely.\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer noted that the current zswap implementation requires a backing swapfile, which wastes swapfile space, and suggested using a ghost swapfile instead, which only contains the swapfile header and does not waste space.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "From: Chris Li <chrisl@kernel.org>\n\nThe current zswap requires a backing swapfile. The swap slot used\nby zswap is not able to be used by the swapfile. That waste swapfile\nspace.\n\nThe ghost swapfile is a swapfile that only contains the swapfile header\nfor zswap. The swapfile header indicate the size of the swapfile. There\nis no swap data section in the ghost swapfile, therefore, no waste of\nswapfile space.  As such, any write to a ghost swapfile will fail. To\nprevents accidental read or write of ghost swapfile, bdev of\nswap_info_struct is set to NULL. Ghost swapfile will also set the SSD\nflag because there is no rotation disk access when using zswap.\n\nThe zswap write back has been disabled if all swapfiles in the system\nare ghost swap files.\n\nSigned-off-by: Chris Li <chrisl@kernel.org>\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n include/linux/swap.h |  2 ++\n mm/page_io.c         | 18 +++++++++++++++---\n mm/swap.h            |  2 +-\n mm/swapfile.c        | 42 +++++++++++++++++++++++++++++++++++++-----\n mm/zswap.c           | 12 +++++++++---\n 5 files changed, 64 insertions(+), 12 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex bc871d8a1e99..3b2efd319f44 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -215,6 +215,7 @@ enum {\n \tSWP_PAGE_DISCARD = (1 << 10),\t/* freed swap page-cluster discards */\n \tSWP_STABLE_WRITES = (1 << 11),\t/* no overwrite PG_writeback pages */\n \tSWP_SYNCHRONOUS_IO = (1 << 12),\t/* synchronous IO is efficient */\n+\tSWP_GHOST\t= (1 << 13),\t/* not backed by anything */\n \t\t\t\t\t/* add others here before... */\n };\n \n@@ -419,6 +420,7 @@ void free_folio_and_swap_cache(struct folio *folio);\n void free_pages_and_swap_cache(struct encoded_page **, int);\n /* linux/mm/swapfile.c */\n extern atomic_long_t nr_swap_pages;\n+extern atomic_t nr_real_swapfiles;\n extern long total_swap_pages;\n extern atomic_t nr_rotate_swap;\n \ndiff --git a/mm/page_io.c b/mm/page_io.c\nindex 5a0b5034489b..f4a5fc0863f5 100644\n--- a/mm/page_io.c\n+++ b/mm/page_io.c\n@@ -291,8 +291,7 @@ int swap_writeout(struct folio *folio, struct swap_iocb **swap_plug)\n \t\treturn AOP_WRITEPAGE_ACTIVATE;\n \t}\n \n-\t__swap_writepage(folio, swap_plug);\n-\treturn 0;\n+\treturn __swap_writepage(folio, swap_plug);\n out_unlock:\n \tfolio_unlock(folio);\n \treturn ret;\n@@ -454,11 +453,18 @@ static void swap_writepage_bdev_async(struct folio *folio,\n \tsubmit_bio(bio);\n }\n \n-void __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug)\n+int __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug)\n {\n \tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n \n \tVM_BUG_ON_FOLIO(!folio_test_swapcache(folio), folio);\n+\n+\tif (sis->flags & SWP_GHOST) {\n+\t\t/* Prevent the page from getting reclaimed. */\n+\t\tfolio_set_dirty(folio);\n+\t\treturn AOP_WRITEPAGE_ACTIVATE;\n+\t}\n+\n \t/*\n \t * ->flags can be updated non-atomically (scan_swap_map_slots),\n \t * but that will never affect SWP_FS_OPS, so the data_race\n@@ -475,6 +481,7 @@ void __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug)\n \t\tswap_writepage_bdev_sync(folio, sis);\n \telse\n \t\tswap_writepage_bdev_async(folio, sis);\n+\treturn 0;\n }\n \n void swap_write_unplug(struct swap_iocb *sio)\n@@ -649,6 +656,11 @@ void swap_read_folio(struct folio *folio, struct swap_iocb **plug)\n \tif (zswap_load(folio) != -ENOENT)\n \t\tgoto finish;\n \n+\tif (unlikely(sis->flags & SWP_GHOST)) {\n+\t\tfolio_unlock(folio);\n+\t\tgoto finish;\n+\t}\n+\n \t/* We have to read from slower devices. Increase zswap protection. */\n \tzswap_folio_swapin(folio);\n \ndiff --git a/mm/swap.h b/mm/swap.h\nindex cb1ab20d83d5..55aa6d904afd 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -226,7 +226,7 @@ static inline void swap_read_unplug(struct swap_iocb *plug)\n }\n void swap_write_unplug(struct swap_iocb *sio);\n int swap_writeout(struct folio *folio, struct swap_iocb **swap_plug);\n-void __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug);\n+int __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug);\n \n /* linux/mm/swap_state.c */\n extern struct address_space swap_space __read_mostly;\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 4018e8694b72..65666c43cbd5 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -67,6 +67,7 @@ static void move_cluster(struct swap_info_struct *si,\n static DEFINE_SPINLOCK(swap_lock);\n static unsigned int nr_swapfiles;\n atomic_long_t nr_swap_pages;\n+atomic_t nr_real_swapfiles;\n /*\n  * Some modules use swappable objects and may try to swap them out under\n  * memory pressure (via the shrinker). Before doing so, they may wish to\n@@ -1211,6 +1212,8 @@ static void del_from_avail_list(struct swap_info_struct *si, bool swapoff)\n \t\t\tgoto skip;\n \t}\n \n+\tif (!(si->flags & SWP_GHOST))\n+\t\tatomic_sub(1, &nr_real_swapfiles);\n \tplist_del(&si->avail_list, &swap_avail_head);\n \n skip:\n@@ -1253,6 +1256,8 @@ static void add_to_avail_list(struct swap_info_struct *si, bool swapon)\n \t}\n \n \tplist_add(&si->avail_list, &swap_avail_head);\n+\tif (!(si->flags & SWP_GHOST))\n+\t\tatomic_add(1, &nr_real_swapfiles);\n \n skip:\n \tspin_unlock(&swap_avail_lock);\n@@ -2793,6 +2798,11 @@ static int setup_swap_extents(struct swap_info_struct *sis,\n \tstruct inode *inode = mapping->host;\n \tint ret;\n \n+\tif (sis->flags & SWP_GHOST) {\n+\t\t*span = 0;\n+\t\treturn 0;\n+\t}\n+\n \tif (S_ISBLK(inode->i_mode)) {\n \t\tret = add_swap_extent(sis, 0, sis->max, 0);\n \t\t*span = sis->pages;\n@@ -2992,7 +3002,8 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tdestroy_swap_extents(p, p->swap_file);\n \n-\tif (!(p->flags & SWP_SOLIDSTATE))\n+\tif (!(p->flags & SWP_GHOST) &&\n+\t    !(p->flags & SWP_SOLIDSTATE))\n \t\tatomic_dec(&nr_rotate_swap);\n \n \tmutex_lock(&swapon_mutex);\n@@ -3102,6 +3113,19 @@ static void swap_stop(struct seq_file *swap, void *v)\n \tmutex_unlock(&swapon_mutex);\n }\n \n+static const char *swap_type_str(struct swap_info_struct *si)\n+{\n+\tstruct file *file = si->swap_file;\n+\n+\tif (si->flags & SWP_GHOST)\n+\t\treturn \"ghost\\t\";\n+\n+\tif (S_ISBLK(file_inode(file)->i_mode))\n+\t\treturn \"partition\";\n+\n+\treturn \"file\\t\";\n+}\n+\n static int swap_show(struct seq_file *swap, void *v)\n {\n \tstruct swap_info_struct *si = v;\n@@ -3121,8 +3145,7 @@ static int swap_show(struct seq_file *swap, void *v)\n \tlen = seq_file_path(swap, file, \" \\t\\n\\\\\");\n \tseq_printf(swap, \"%*s%s\\t%lu\\t%s%lu\\t%s%d\\n\",\n \t\t\tlen < 40 ? 40 - len : 1, \" \",\n-\t\t\tS_ISBLK(file_inode(file)->i_mode) ?\n-\t\t\t\t\"partition\" : \"file\\t\",\n+\t\t\tswap_type_str(si),\n \t\t\tbytes, bytes < 10000000 ? \"\\t\" : \"\",\n \t\t\tinuse, inuse < 10000000 ? \"\\t\" : \"\",\n \t\t\tsi->prio);\n@@ -3254,7 +3277,6 @@ static int claim_swapfile(struct swap_info_struct *si, struct inode *inode)\n \treturn 0;\n }\n \n-\n /*\n  * Find out how many pages are allowed for a single swap device. There\n  * are two limiting factors:\n@@ -3300,6 +3322,7 @@ static unsigned long read_swap_header(struct swap_info_struct *si,\n \tunsigned long maxpages;\n \tunsigned long swapfilepages;\n \tunsigned long last_page;\n+\tloff_t size;\n \n \tif (memcmp(\"SWAPSPACE2\", swap_header->magic.magic, 10)) {\n \t\tpr_err(\"Unable to find swap-space signature\\n\");\n@@ -3342,7 +3365,16 @@ static unsigned long read_swap_header(struct swap_info_struct *si,\n \n \tif (!maxpages)\n \t\treturn 0;\n-\tswapfilepages = i_size_read(inode) >> PAGE_SHIFT;\n+\n+\tsize = i_size_read(inode);\n+\tif (size == PAGE_SIZE) {\n+\t\t/* Ghost swapfile */\n+\t\tsi->bdev = NULL;\n+\t\tsi->flags |= SWP_GHOST | SWP_SOLIDSTATE;\n+\t\treturn maxpages;\n+\t}\n+\n+\tswapfilepages = size >> PAGE_SHIFT;\n \tif (swapfilepages && maxpages > swapfilepages) {\n \t\tpr_warn(\"Swap area shorter than signature indicates\\n\");\n \t\treturn 0;\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex 5d83539a8bba..e470f697e770 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -995,11 +995,16 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \tstruct swap_info_struct *si;\n \tint ret = 0;\n \n-\t/* try to allocate swap cache folio */\n \tsi = get_swap_device(swpentry);\n \tif (!si)\n \t\treturn -EEXIST;\n \n+\tif (si->flags & SWP_GHOST) {\n+\t\tput_swap_device(si);\n+\t\treturn -EINVAL;\n+\t}\n+\n+\t/* try to allocate swap cache folio */\n \tmpol = get_task_policy(current);\n \tfolio = swap_cache_alloc_folio(swpentry, GFP_KERNEL, 0, NULL, mpol,\n \t\t\t\t       NO_INTERLEAVE_INDEX);\n@@ -1052,7 +1057,8 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \tfolio_set_reclaim(folio);\n \n \t/* start writeback */\n-\t__swap_writepage(folio, NULL);\n+\tret = __swap_writepage(folio, NULL);\n+\tWARN_ON_ONCE(ret);\n \n out:\n \tif (ret) {\n@@ -1536,7 +1542,7 @@ bool zswap_store(struct folio *folio)\n \tzswap_pool_put(pool);\n put_objcg:\n \tobj_cgroup_put(objcg);\n-\tif (!ret && zswap_pool_reached_full)\n+\tif (!ret && zswap_pool_reached_full && atomic_read(&nr_real_swapfiles))\n \t\tqueue_work(shrink_wq, &zswap_shrink_work);\n check_old:\n \t/*\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "The reviewer suggested using /dev/ghostswap as a special device to simplify ghost swap setup for userspace, and provided code changes to implement this feature.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEUTRAL"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Use /dev/ghostswap as a special device so userspace can setup ghost\nswap easily without any extra tools.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n drivers/char/mem.c   | 39 +++++++++++++++++++++++++++++++++++++++\n include/linux/swap.h |  2 ++\n mm/swapfile.c        | 22 +++++++++++++++++++---\n 3 files changed, 60 insertions(+), 3 deletions(-)\n\ndiff --git a/drivers/char/mem.c b/drivers/char/mem.c\nindex cca4529431f8..8d0eb3f7d191 100644\n--- a/drivers/char/mem.c\n+++ b/drivers/char/mem.c\n@@ -30,6 +30,7 @@\n #include <linux/uio.h>\n #include <linux/uaccess.h>\n #include <linux/security.h>\n+#include <linux/swap.h>\n \n #define DEVMEM_MINOR\t1\n #define DEVPORT_MINOR\t4\n@@ -667,6 +668,41 @@ static const struct file_operations null_fops = {\n \t.uring_cmd\t= uring_cmd_null,\n };\n \n+#ifdef CONFIG_SWAP\n+static ssize_t read_ghostswap(struct file *file, char __user *buf,\n+\t\t\t      size_t count, loff_t *ppos)\n+{\n+\tunion swap_header *hdr;\n+\tsize_t to_copy;\n+\n+\tif (*ppos >= PAGE_SIZE)\n+\t\treturn 0;\n+\n+\thdr = kzalloc(PAGE_SIZE, GFP_KERNEL);\n+\tif (!hdr)\n+\t\treturn -ENOMEM;\n+\n+\thdr->info.version = 1;\n+\thdr->info.last_page = totalram_pages() - 1;\n+\tmemcpy(hdr->magic.magic, \"SWAPSPACE2\", 10);\n+\tto_copy = min_t(size_t, count, PAGE_SIZE - *ppos);\n+\tif (copy_to_user(buf, (char *)hdr + *ppos, to_copy)) {\n+\t\tkfree(hdr);\n+\t\treturn -EFAULT;\n+\t}\n+\n+\tkfree(hdr);\n+\t*ppos += to_copy;\n+\treturn to_copy;\n+}\n+\n+static const struct file_operations ghostswap_fops = {\n+\t.llseek\t\t= null_lseek,\n+\t.read\t\t= read_ghostswap,\n+\t.write\t\t= write_null,\n+};\n+#endif\n+\n #ifdef CONFIG_DEVPORT\n static const struct file_operations port_fops = {\n \t.llseek\t\t= memory_lseek,\n@@ -718,6 +754,9 @@ static const struct memdev {\n #ifdef CONFIG_PRINTK\n \t[11] = { \"kmsg\", &kmsg_fops, 0, 0644 },\n #endif\n+#ifdef CONFIG_SWAP\n+\t[DEVGHOST_MINOR] = { \"ghostswap\", &ghostswap_fops, 0, 0660 },\n+#endif\n };\n \n static int memory_open(struct inode *inode, struct file *filp)\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 3b2efd319f44..b57a4a40f4fe 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -421,6 +421,8 @@ void free_pages_and_swap_cache(struct encoded_page **, int);\n /* linux/mm/swapfile.c */\n extern atomic_long_t nr_swap_pages;\n extern atomic_t nr_real_swapfiles;\n+\n+#define DEVGHOST_MINOR\t13\t/* /dev/ghostswap char device minor */\n extern long total_swap_pages;\n extern atomic_t nr_rotate_swap;\n \ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 65666c43cbd5..d054f40ec75f 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -42,6 +42,7 @@\n #include <linux/suspend.h>\n #include <linux/zswap.h>\n #include <linux/plist.h>\n+#include <linux/major.h>\n \n #include <asm/tlbflush.h>\n #include <linux/leafops.h>\n@@ -1703,6 +1704,7 @@ int folio_alloc_swap(struct folio *folio)\n \tunsigned int size = 1 << order;\n \tstruct swap_cluster_info *ci;\n \n+\tVM_WARN_ON_FOLIO(folio_test_swapcache(folio), folio);\n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n \tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n \n@@ -3421,6 +3423,10 @@ static int setup_swap_clusters_info(struct swap_info_struct *si,\n \terr = swap_cluster_setup_bad_slot(si, cluster_info, 0, false);\n \tif (err)\n \t\tgoto err;\n+\n+\tif (!swap_header)\n+\t\tgoto setup_cluster_info;\n+\n \tfor (i = 0; i < swap_header->info.nr_badpages; i++) {\n \t\tunsigned int page_nr = swap_header->info.badpages[i];\n \n@@ -3440,6 +3446,7 @@ static int setup_swap_clusters_info(struct swap_info_struct *si,\n \t\t\tgoto err;\n \t}\n \n+setup_cluster_info:\n \tINIT_LIST_HEAD(&si->free_clusters);\n \tINIT_LIST_HEAD(&si->full_clusters);\n \tINIT_LIST_HEAD(&si->discard_clusters);\n@@ -3476,7 +3483,7 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tstruct dentry *dentry;\n \tint prio;\n \tint error;\n-\tunion swap_header *swap_header;\n+\tunion swap_header *swap_header = NULL;\n \tint nr_extents;\n \tsector_t span;\n \tunsigned long maxpages;\n@@ -3528,6 +3535,15 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \t\tgoto bad_swap_unlock_inode;\n \t}\n \n+\t/* /dev/ghostswap: synthesize a ghost swap device. */\n+\tif (S_ISCHR(inode->i_mode) &&\n+\t    imajor(inode) == MEM_MAJOR && iminor(inode) == DEVGHOST_MINOR) {\n+\t\tmaxpages = round_up(totalram_pages(), SWAPFILE_CLUSTER);\n+\t\tsi->flags |= SWP_GHOST | SWP_SOLIDSTATE;\n+\t\tsi->bdev = NULL;\n+\t\tgoto setup;\n+\t}\n+\n \t/*\n \t * The swap subsystem needs a major overhaul to support this.\n \t * It doesn't work yet so just disable it for now.\n@@ -3550,13 +3566,13 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \t\tgoto bad_swap_unlock_inode;\n \t}\n \tswap_header = kmap_local_folio(folio, 0);\n-\n \tmaxpages = read_swap_header(si, swap_header, inode);\n \tif (unlikely(!maxpages)) {\n \t\terror = -EINVAL;\n \t\tgoto bad_swap_unlock_inode;\n \t}\n \n+setup:\n \tsi->max = maxpages;\n \tsi->pages = maxpages - 1;\n \tnr_extents = setup_swap_extents(si, swap_file, &span);\n@@ -3585,7 +3601,7 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \n \tif (si->bdev && bdev_nonrot(si->bdev)) {\n \t\tsi->flags |= SWP_SOLIDSTATE;\n-\t} else {\n+\t} else if (!(si->flags & SWP_SOLIDSTATE)) {\n \t\tatomic_inc(&nr_rotate_swap);\n \t\tinced_nr_rotate_swap = true;\n \t}\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "The reviewer noted that the folio lock can be used to stabilize virtual table data, and suggested removing the global percpu cluster cache as proposed in [1] to simplify swap entry writeback and make it easier to use tiering and priority. They also mentioned that all allocations are currently using atomic, which can be improved by adapting sleep allocation support from the swap table.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "improvement suggestions"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Now, the ghost swap file is completely dynamic. For easier testing, this\ncommit makes the /dev/ghostswap 8 times the size of total ram by\ndefault.\n\nNOTE: This commit is still a minimal proof of concept, so many parts of\nthe implementation can be improved.\n\nAnd we have a ci_dyn->virtual_table that's is ready to be used (not used\nyet). For example, storing zswap's metadata. In theory the folio lock can\nbe used to stablize it's virtual table data.\n\ne.g., Swap entry writeback can also be done easily using a\nfolio_realloc_swap, skip the folio->swap's device and use underlying\ndevices, it will be easier to do if we remove the global percpu cluster\ncache as suggested by [1] and should just work with tiering and priority.\nJust put the folio->swap as a reverse entry in the lower layer's swap\ntable, and collect lower level's swap entry in the virtual_table, then\nit's all good.\n\nAnd right now all allocations are using atomic, which can also be\nimproved as the swap table already has sleep allocation support,\njust need to adapt it.\n\nThe RCU lock protection convention can also be simplified.\n\nBut without all that, this works pretty well. We can have a \"virtual\nswap\" of any size with zero overhead, common stress tests are showing\na very nice performance, while ordinary swaps have zero overhead,\nand everything is runtime configurable.\n\nBut don't be too surprised if some corner cases are not well covered\nyet, as most works are still focusing on the infrastructure.\n\nLink: https://lore.kernel.org/linux-mm/20260126065242.1221862-5-youngjun.park@lge.com/ [1]\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n include/linux/swap.h |   1 +\n mm/swap.h            |  44 +++++++++++++---\n mm/swap_state.c      |  35 ++++++++-----\n mm/swap_table.h      |   2 +\n mm/swapfile.c        | 145 +++++++++++++++++++++++++++++++++++++++++++++++----\n 5 files changed, 199 insertions(+), 28 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex b57a4a40f4fe..41d7eae56d65 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -284,6 +284,7 @@ struct swap_info_struct {\n \tstruct work_struct reclaim_work; /* reclaim worker */\n \tstruct list_head discard_clusters; /* discard clusters list */\n \tstruct plist_node avail_list;   /* entry in swap_avail_head */\n+\tstruct xarray cluster_info_pool; /* Xarray for ghost swap cluster info */\n };\n \n static inline swp_entry_t page_swap_entry(struct page *page)\ndiff --git a/mm/swap.h b/mm/swap.h\nindex 55aa6d904afd..7a4d1d939842 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -41,6 +41,13 @@ struct swap_cluster_info {\n \tstruct list_head list;\n };\n \n+struct swap_cluster_info_dynamic {\n+\tstruct swap_cluster_info ci;\t/* Underlying cluster info */\n+\tunsigned int index;\t\t/* for cluster_index() */\n+\tstruct rcu_head rcu;\t\t/* For kfree_rcu deferred free */\n+\t/* unsigned long *virtual_table; And we can easily have a virtual table */\n+};\n+\n /* All on-list cluster must have a non-zero flag. */\n enum swap_cluster_flags {\n \tCLUSTER_FLAG_NONE = 0, /* For temporary off-list cluster */\n@@ -51,6 +58,7 @@ enum swap_cluster_flags {\n \tCLUSTER_FLAG_USABLE = CLUSTER_FLAG_FRAG,\n \tCLUSTER_FLAG_FULL,\n \tCLUSTER_FLAG_DISCARD,\n+\tCLUSTER_FLAG_DEAD,\t/* Ghost cluster pending kfree_rcu */\n \tCLUSTER_FLAG_MAX,\n };\n \n@@ -84,9 +92,19 @@ static inline struct swap_info_struct *__swap_entry_to_info(swp_entry_t entry)\n static inline struct swap_cluster_info *__swap_offset_to_cluster(\n \t\tstruct swap_info_struct *si, pgoff_t offset)\n {\n+\tunsigned int cluster_idx = offset / SWAPFILE_CLUSTER;\n+\n \tVM_WARN_ON_ONCE(percpu_ref_is_zero(&si->users)); /* race with swapoff */\n \tVM_WARN_ON_ONCE(offset >= roundup(si->max, SWAPFILE_CLUSTER));\n-\treturn &si->cluster_info[offset / SWAPFILE_CLUSTER];\n+\n+\tif (si->flags & SWP_GHOST) {\n+\t\tstruct swap_cluster_info_dynamic *ci_dyn;\n+\n+\t\tci_dyn = xa_load(&si->cluster_info_pool, cluster_idx);\n+\t\treturn ci_dyn ? &ci_dyn->ci : NULL;\n+\t}\n+\n+\treturn &si->cluster_info[cluster_idx];\n }\n \n static inline struct swap_cluster_info *__swap_entry_to_cluster(swp_entry_t entry)\n@@ -98,7 +116,7 @@ static inline struct swap_cluster_info *__swap_entry_to_cluster(swp_entry_t entr\n static __always_inline struct swap_cluster_info *__swap_cluster_lock(\n \t\tstruct swap_info_struct *si, unsigned long offset, bool irq)\n {\n-\tstruct swap_cluster_info *ci = __swap_offset_to_cluster(si, offset);\n+\tstruct swap_cluster_info *ci;\n \n \t/*\n \t * Nothing modifies swap cache in an IRQ context. All access to\n@@ -111,10 +129,24 @@ static __always_inline struct swap_cluster_info *__swap_cluster_lock(\n \t */\n \tVM_WARN_ON_ONCE(!in_task());\n \tVM_WARN_ON_ONCE(percpu_ref_is_zero(&si->users)); /* race with swapoff */\n-\tif (irq)\n-\t\tspin_lock_irq(&ci->lock);\n-\telse\n-\t\tspin_lock(&ci->lock);\n+\n+\trcu_read_lock();\n+\tci = __swap_offset_to_cluster(si, offset);\n+\tif (ci) {\n+\t\tif (irq)\n+\t\t\tspin_lock_irq(&ci->lock);\n+\t\telse\n+\t\t\tspin_lock(&ci->lock);\n+\n+\t\tif (ci->flags == CLUSTER_FLAG_DEAD) {\n+\t\t\tif (irq)\n+\t\t\t\tspin_unlock_irq(&ci->lock);\n+\t\t\telse\n+\t\t\t\tspin_unlock(&ci->lock);\n+\t\t\tci = NULL;\n+\t\t}\n+\t}\n+\trcu_read_unlock();\n \treturn ci;\n }\n \ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 419419e18a47..1c3600a93ecd 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -90,8 +90,10 @@ struct folio *swap_cache_get_folio(swp_entry_t entry)\n \tstruct folio *folio;\n \n \tfor (;;) {\n+\t\trcu_read_lock();\n \t\tswp_tb = swap_table_get(__swap_entry_to_cluster(entry),\n \t\t\t\t\tswp_cluster_offset(entry));\n+\t\trcu_read_unlock();\n \t\tif (!swp_tb_is_folio(swp_tb))\n \t\t\treturn NULL;\n \t\tfolio = swp_tb_to_folio(swp_tb);\n@@ -113,8 +115,10 @@ bool swap_cache_has_folio(swp_entry_t entry)\n {\n \tunsigned long swp_tb;\n \n+\trcu_read_lock();\n \tswp_tb = swap_table_get(__swap_entry_to_cluster(entry),\n \t\t\t\tswp_cluster_offset(entry));\n+\trcu_read_unlock();\n \treturn swp_tb_is_folio(swp_tb);\n }\n \n@@ -130,8 +134,10 @@ void *swap_cache_get_shadow(swp_entry_t entry)\n {\n \tunsigned long swp_tb;\n \n+\trcu_read_lock();\n \tswp_tb = swap_table_get(__swap_entry_to_cluster(entry),\n \t\t\t\tswp_cluster_offset(entry));\n+\trcu_read_unlock();\n \tif (swp_tb_is_shadow(swp_tb))\n \t\treturn swp_tb_to_shadow(swp_tb);\n \treturn NULL;\n@@ -209,14 +215,14 @@ void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \tlruvec_stat_mod_folio(folio, NR_SWAPCACHE, nr_pages);\n }\n \n-static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n-\t\t\t\t\tswp_entry_t targ_entry, gfp_t gfp,\n+static struct folio *__swap_cache_alloc(swp_entry_t targ_entry, gfp_t gfp,\n \t\t\t\t\tunsigned int order, struct vm_fault *vmf,\n \t\t\t\t\tstruct mempolicy *mpol, pgoff_t ilx)\n {\n \tint err;\n \tswp_entry_t entry;\n \tstruct folio *folio;\n+\tstruct swap_cluster_info *ci;\n \tvoid *shadow = NULL, *shadow_check = NULL;\n \tunsigned long address, nr_pages = 1 << order;\n \tunsigned int ci_off, ci_targ = swp_cluster_offset(targ_entry);\n@@ -225,9 +231,12 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \tci_off = round_down(ci_targ, nr_pages);\n \n \t/* First check if the range is available */\n-\tspin_lock(&ci->lock);\n-\terr = __swap_cache_check_batch(ci, entry, ci_off, ci_targ, nr_pages, &shadow);\n-\tspin_unlock(&ci->lock);\n+\terr = -ENOENT;\n+\tci = swap_cluster_lock(__swap_entry_to_info(entry), swp_offset(entry));\n+\tif (ci) {\n+\t\terr = __swap_cache_check_batch(ci, entry, ci_off, ci_targ, nr_pages, &shadow);\n+\t\tswap_cluster_unlock(ci);\n+\t}\n \tif (unlikely(err))\n \t\treturn ERR_PTR(err);\n \n@@ -243,10 +252,13 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \t\treturn ERR_PTR(-ENOMEM);\n \n \t/* Double check the range is still not in conflict */\n-\tspin_lock(&ci->lock);\n-\terr = __swap_cache_check_batch(ci, entry, ci_off, ci_targ, nr_pages, &shadow_check);\n+\terr = -ENOENT;\n+\tci = swap_cluster_lock(__swap_entry_to_info(entry), swp_offset(entry));\n+\tif (ci)\n+\t\terr = __swap_cache_check_batch(ci, entry, ci_off, ci_targ, nr_pages, &shadow_check);\n \tif (unlikely(err) || shadow_check != shadow) {\n-\t\tspin_unlock(&ci->lock);\n+\t\tif (ci)\n+\t\t\tswap_cluster_unlock(ci);\n \t\tfolio_put(folio);\n \n \t\t/* If shadow changed, just try again */\n@@ -256,13 +268,14 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \t__folio_set_locked(folio);\n \t__folio_set_swapbacked(folio);\n \t__swap_cache_add_folio(ci, folio, entry);\n-\tspin_unlock(&ci->lock);\n+\tswap_cluster_unlock(ci);\n \n \t/* With swap table, we must have a shadow, for memcg tracking */\n \tWARN_ON(!shadow);\n \n \tif (mem_cgroup_swapin_charge_folio(folio, vmf ? vmf->vma->vm_mm : NULL,\n \t\t\t\t\t   gfp, shadow_to_memcgid(shadow))) {\n+\t\t/* The folio pins the cluster */\n \t\tspin_lock(&ci->lock);\n \t\t__swap_cache_del_folio(ci, folio, shadow, false, false);\n \t\tspin_unlock(&ci->lock);\n@@ -305,13 +318,11 @@ struct folio *swap_cache_alloc_folio(swp_entry_t targ_entry, gfp_t gfp_mask,\n {\n \tint order, err;\n \tstruct folio *folio;\n-\tstruct swap_cluster_info *ci;\n \n \t/* Always allow order 0 so swap won't fail under pressure. */\n \torder = orders ? highest_order(orders |= BIT(0)) : 0;\n-\tci = __swap_entry_to_cluster(targ_entry);\n \tfor (;;) {\n-\t\tfolio = __swap_cache_alloc(ci, targ_entry, gfp_mask, order,\n+\t\tfolio = __swap_cache_alloc(targ_entry, gfp_mask, order,\n \t\t\t\t\t   vmf, mpol, ilx);\n \t\tif (!IS_ERR(folio))\n \t\t\treturn folio;\ndiff --git a/mm/swap_table.h b/mm/swap_table.h\nindex 6d3d773e1908..867bcfff0e3c 100644\n--- a/mm/swap_table.h\n+++ b/mm/swap_table.h\n@@ -260,6 +260,8 @@ static inline unsigned long swap_table_get(struct swap_cluster_info *ci,\n \tunsigned long swp_tb;\n \n \tVM_WARN_ON_ONCE(off >= SWAPFILE_CLUSTER);\n+\tif (!ci)\n+\t\treturn SWP_TB_NULL;\n \n \trcu_read_lock();\n \ttable = rcu_dereference(ci->table);\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex d054f40ec75f..f0682c8c8f53 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -404,6 +404,8 @@ static inline bool cluster_is_usable(struct swap_cluster_info *ci, int order)\n static inline unsigned int cluster_index(struct swap_info_struct *si,\n \t\t\t\t\t struct swap_cluster_info *ci)\n {\n+\tif (si->flags & SWP_GHOST)\n+\t\treturn container_of(ci, struct swap_cluster_info_dynamic, ci)->index;\n \treturn ci - si->cluster_info;\n }\n \n@@ -708,6 +710,22 @@ static void free_cluster(struct swap_info_struct *si, struct swap_cluster_info *\n \t\treturn;\n \t}\n \n+\tif (si->flags & SWP_GHOST) {\n+\t\tstruct swap_cluster_info_dynamic *ci_dyn;\n+\n+\t\tci_dyn = container_of(ci, struct swap_cluster_info_dynamic, ci);\n+\t\tif (ci->flags != CLUSTER_FLAG_NONE) {\n+\t\t\tspin_lock(&si->lock);\n+\t\t\tlist_del(&ci->list);\n+\t\t\tspin_unlock(&si->lock);\n+\t\t}\n+\t\tswap_cluster_free_table(ci);\n+\t\txa_erase(&si->cluster_info_pool, ci_dyn->index);\n+\t\tci->flags = CLUSTER_FLAG_DEAD;\n+\t\tkfree_rcu(ci_dyn, rcu);\n+\t\treturn;\n+\t}\n+\n \t__free_cluster(si, ci);\n }\n \n@@ -814,15 +832,17 @@ static int swap_cluster_setup_bad_slot(struct swap_info_struct *si,\n  * stolen by a lower order). @usable will be set to false if that happens.\n  */\n static bool cluster_reclaim_range(struct swap_info_struct *si,\n-\t\t\t\t  struct swap_cluster_info *ci,\n+\t\t\t\t  struct swap_cluster_info **pcip,\n \t\t\t\t  unsigned long start, unsigned int order,\n \t\t\t\t  bool *usable)\n {\n+\tstruct swap_cluster_info *ci = *pcip;\n \tunsigned int nr_pages = 1 << order;\n \tunsigned long offset = start, end = start + nr_pages;\n \tunsigned long swp_tb;\n \n \tspin_unlock(&ci->lock);\n+\trcu_read_lock();\n \tdo {\n \t\tswp_tb = swap_table_get(ci, offset % SWAPFILE_CLUSTER);\n \t\tif (swp_tb_get_count(swp_tb))\n@@ -831,7 +851,15 @@ static bool cluster_reclaim_range(struct swap_info_struct *si,\n \t\t\tif (__try_to_reclaim_swap(si, offset, TTRS_ANYWAY) < 0)\n \t\t\t\tbreak;\n \t} while (++offset < end);\n-\tspin_lock(&ci->lock);\n+\trcu_read_unlock();\n+\n+\t/* Re-lookup: ghost cluster may have been freed while lock was dropped */\n+\tci = swap_cluster_lock(si, start);\n+\t*pcip = ci;\n+\tif (!ci) {\n+\t\t*usable = false;\n+\t\treturn false;\n+\t}\n \n \t/*\n \t * We just dropped ci->lock so cluster could be used by another\n@@ -979,7 +1007,8 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n \t\tif (!cluster_scan_range(si, ci, offset, nr_pages, &need_reclaim))\n \t\t\tcontinue;\n \t\tif (need_reclaim) {\n-\t\t\tret = cluster_reclaim_range(si, ci, offset, order, &usable);\n+\t\t\tret = cluster_reclaim_range(si, &ci, offset, order,\n+\t\t\t\t\t\t    &usable);\n \t\t\tif (!usable)\n \t\t\t\tgoto out;\n \t\t\tif (cluster_is_empty(ci))\n@@ -1005,8 +1034,10 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n \t * should use a new cluster, and move the failed cluster to where it\n \t * should be.\n \t */\n-\trelocate_cluster(si, ci);\n-\tswap_cluster_unlock(ci);\n+\tif (ci) {\n+\t\trelocate_cluster(si, ci);\n+\t\tswap_cluster_unlock(ci);\n+\t}\n \tif (si->flags & SWP_SOLIDSTATE) {\n \t\tthis_cpu_write(percpu_swap_cluster.offset[order], next);\n \t\tthis_cpu_write(percpu_swap_cluster.si[order], si);\n@@ -1038,6 +1069,44 @@ static unsigned int alloc_swap_scan_list(struct swap_info_struct *si,\n \treturn found;\n }\n \n+static unsigned int alloc_swap_scan_dynamic(struct swap_info_struct *si,\n+\t\t\t\t\t    struct folio *folio)\n+{\n+\tstruct swap_cluster_info_dynamic *ci_dyn;\n+\tstruct swap_cluster_info *ci;\n+\tstruct swap_table *table;\n+\tunsigned long offset;\n+\n+\tWARN_ON(!(si->flags & SWP_GHOST));\n+\n+\tci_dyn = kzalloc(sizeof(*ci_dyn), GFP_ATOMIC);\n+\tif (!ci_dyn)\n+\t\treturn SWAP_ENTRY_INVALID;\n+\n+\ttable = swap_table_alloc(GFP_ATOMIC);\n+\tif (!table) {\n+\t\tkfree(ci_dyn);\n+\t\treturn SWAP_ENTRY_INVALID;\n+\t}\n+\n+\tspin_lock_init(&ci_dyn->ci.lock);\n+\tINIT_LIST_HEAD(&ci_dyn->ci.list);\n+\trcu_assign_pointer(ci_dyn->ci.table, table);\n+\n+\tif (xa_alloc(&si->cluster_info_pool, &ci_dyn->index, ci_dyn,\n+\t\t     XA_LIMIT(1, DIV_ROUND_UP(si->max, SWAPFILE_CLUSTER) - 1),\n+\t\t     GFP_ATOMIC)) {\n+\t\tswap_table_free(table);\n+\t\tkfree(ci_dyn);\n+\t\treturn SWAP_ENTRY_INVALID;\n+\t}\n+\n+\tci = &ci_dyn->ci;\n+\tspin_lock(&ci->lock);\n+\toffset = cluster_offset(si, ci);\n+\treturn alloc_swap_scan_cluster(si, ci, folio, offset);\n+}\n+\n static void swap_reclaim_full_clusters(struct swap_info_struct *si, bool force)\n {\n \tlong to_scan = 1;\n@@ -1060,7 +1129,9 @@ static void swap_reclaim_full_clusters(struct swap_info_struct *si, bool force)\n \t\t\t\tspin_unlock(&ci->lock);\n \t\t\t\tnr_reclaim = __try_to_reclaim_swap(si, offset,\n \t\t\t\t\t\t\t\t   TTRS_ANYWAY);\n-\t\t\t\tspin_lock(&ci->lock);\n+\t\t\t\tci = swap_cluster_lock(si, offset);\n+\t\t\t\tif (!ci)\n+\t\t\t\t\tgoto next;\n \t\t\t\tif (nr_reclaim) {\n \t\t\t\t\toffset += abs(nr_reclaim);\n \t\t\t\t\tcontinue;\n@@ -1074,6 +1145,7 @@ static void swap_reclaim_full_clusters(struct swap_info_struct *si, bool force)\n \t\t\trelocate_cluster(si, ci);\n \n \t\tswap_cluster_unlock(ci);\n+next:\n \t\tif (to_scan <= 0)\n \t\t\tbreak;\n \t}\n@@ -1136,6 +1208,12 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n \n+\tif (si->flags & SWP_GHOST) {\n+\t\tfound = alloc_swap_scan_dynamic(si, folio);\n+\t\tif (found)\n+\t\t\tgoto done;\n+\t}\n+\n \tif (!(si->flags & SWP_PAGE_DISCARD)) {\n \t\tfound = alloc_swap_scan_list(si, &si->free_clusters, folio, false);\n \t\tif (found)\n@@ -1375,7 +1453,8 @@ static bool swap_alloc_fast(struct folio *folio)\n \t\treturn false;\n \n \tci = swap_cluster_lock(si, offset);\n-\talloc_swap_scan_cluster(si, ci, folio, offset);\n+\tif (ci)\n+\t\talloc_swap_scan_cluster(si, ci, folio, offset);\n \tput_swap_device(si);\n \treturn folio_test_swapcache(folio);\n }\n@@ -1476,6 +1555,7 @@ int swap_retry_table_alloc(swp_entry_t entry, gfp_t gfp)\n \tif (!si)\n \t\treturn 0;\n \n+\t/* Entry is in use (being faulted in), so its cluster is alive. */\n \tci = __swap_offset_to_cluster(si, offset);\n \tret = swap_extend_table_alloc(si, ci, gfp);\n \n@@ -1996,6 +2076,7 @@ bool folio_maybe_swapped(struct folio *folio)\n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n \n+\t/* Folio is locked and in swap cache, so ci->count > 0: cluster is alive. */\n \tci = __swap_entry_to_cluster(entry);\n \tci_off = swp_cluster_offset(entry);\n \tci_end = ci_off + folio_nr_pages(folio);\n@@ -2124,7 +2205,8 @@ swp_entry_t swap_alloc_hibernation_slot(int type)\n \tpcp_offset = this_cpu_read(percpu_swap_cluster.offset[0]);\n \tif (pcp_si == si && pcp_offset) {\n \t\tci = swap_cluster_lock(si, pcp_offset);\n-\t\toffset = alloc_swap_scan_cluster(si, ci, NULL, pcp_offset);\n+\t\tif (ci)\n+\t\t\toffset = alloc_swap_scan_cluster(si, ci, NULL, pcp_offset);\n \t}\n \tif (offset == SWAP_ENTRY_INVALID)\n \t\toffset = cluster_alloc_swap_entry(si, NULL);\n@@ -2413,8 +2495,10 @@ static int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,\n \t\t\t\t\t\t&vmf);\n \t\t}\n \t\tif (!folio) {\n+\t\t\trcu_read_lock();\n \t\t\tswp_tb = swap_table_get(__swap_entry_to_cluster(entry),\n \t\t\t\t\t\tswp_cluster_offset(entry));\n+\t\t\trcu_read_unlock();\n \t\t\tif (swp_tb_get_count(swp_tb) <= 0)\n \t\t\t\tcontinue;\n \t\t\treturn -ENOMEM;\n@@ -2560,8 +2644,10 @@ static unsigned int find_next_to_unuse(struct swap_info_struct *si,\n \t * allocations from this area (while holding swap_lock).\n \t */\n \tfor (i = prev + 1; i < si->max; i++) {\n+\t\trcu_read_lock();\n \t\tswp_tb = swap_table_get(__swap_offset_to_cluster(si, i),\n \t\t\t\t\ti % SWAPFILE_CLUSTER);\n+\t\trcu_read_unlock();\n \t\tif (!swp_tb_is_null(swp_tb) && !swp_tb_is_bad(swp_tb))\n \t\t\tbreak;\n \t\tif ((i % LATENCY_LIMIT) == 0)\n@@ -2874,6 +2960,8 @@ static void wait_for_allocation(struct swap_info_struct *si)\n \tstruct swap_cluster_info *ci;\n \n \tBUG_ON(si->flags & SWP_WRITEOK);\n+\tif (si->flags & SWP_GHOST)\n+\t\treturn;\n \n \tfor (offset = 0; offset < end; offset += SWAPFILE_CLUSTER) {\n \t\tci = swap_cluster_lock(si, offset);\n@@ -3394,10 +3482,47 @@ static int setup_swap_clusters_info(struct swap_info_struct *si,\n \t\t\t\t    unsigned long maxpages)\n {\n \tunsigned long nr_clusters = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);\n-\tstruct swap_cluster_info *cluster_info;\n+\tstruct swap_cluster_info *cluster_info = NULL;\n+\tstruct swap_cluster_info_dynamic *ci_dyn;\n \tint err = -ENOMEM;\n \tunsigned long i;\n \n+\t/* For SWP_GHOST files, initialize Xarray pool instead of static array */\n+\tif (si->flags & SWP_GHOST) {\n+\t\t/*\n+\t\t * Pre-allocate cluster 0 and mark slot 0 (header page)\n+\t\t * as bad so the allocator never hands out page offset 0.\n+\t\t */\n+\t\tci_dyn = kzalloc(sizeof(*ci_dyn), GFP_KERNEL);\n+\t\tif (!ci_dyn)\n+\t\t\tgoto err;\n+\t\tspin_lock_init(&ci_dyn->ci.lock);\n+\t\tINIT_LIST_HEAD(&ci_dyn->ci.list);\n+\n+\t\tnr_clusters = 0;\n+\t\txa_init_flags(&si->cluster_info_pool, XA_FLAGS_ALLOC);\n+\t\terr = xa_insert(&si->cluster_info_pool, 0, ci_dyn, GFP_KERNEL);\n+\t\tif (err) {\n+\t\t\tkfree(ci_dyn);\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\terr = swap_cluster_setup_bad_slot(si, &ci_dyn->ci, 0, false);\n+\t\tif (err) {\n+\t\t\tstruct swap_table *table;\n+\n+\t\t\txa_erase(&si->cluster_info_pool, 0);\n+\t\t\ttable = (void *)rcu_dereference_protected(ci_dyn->ci.table, true);\n+\t\t\tif (table)\n+\t\t\t\tswap_table_free(table);\n+\t\t\tkfree(ci_dyn);\n+\t\t\txa_destroy(&si->cluster_info_pool);\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\tgoto setup_cluster_info;\n+\t}\n+\n \tcluster_info = kvcalloc(nr_clusters, sizeof(*cluster_info), GFP_KERNEL);\n \tif (!cluster_info)\n \t\tgoto err;\n@@ -3538,7 +3663,7 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \t/* /dev/ghostswap: synthesize a ghost swap device. */\n \tif (S_ISCHR(inode->i_mode) &&\n \t    imajor(inode) == MEM_MAJOR && iminor(inode) == DEVGHOST_MINOR) {\n-\t\tmaxpages = round_up(totalram_pages(), SWAPFILE_CLUSTER);\n+\t\tmaxpages = round_up(totalram_pages(), SWAPFILE_CLUSTER) * 8;\n \t\tsi->flags |= SWP_GHOST | SWP_SOLIDSTATE;\n \t\tsi->bdev = NULL;\n \t\tgoto setup;\n\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that the dynamic ghost file is a minimal proof of concept and requested further development before merging.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "minimal proof of concept",
                "requested further development"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "NOTE for an RFC quality series: Swap table P4 is patch 1 - 12, and the\ndynamic ghost file is patch 13 - 15. Putting them together as RFC for\neasier review and discussions. Swap table P4 is stable and good to merge\nif we are OK with a few memcg reparent behavior (there is also a\nsolution if we don't), dynamic ghost swap is yet a minimal proof of\nconcept. See patch 15 for more details. And see below for Swap table 4\ncover letter (nice performance gain and memory save).\n\nThis is based on the latest mm-unstable, swap table P3 [1] and patches\n[2] and [3], [4]. Sending this out early, as it might be helpful for us\nto get a cleaner picture of the ongoing efforts, make the discussions easier.\n\nSummary: With this approach, we can have an infinitely or dynamically\nlarge ghost which could be identical to \"virtual swap\", and support\nevery feature we need while being *runtime configurable* with *zero\noverhead* for plain swap and keep the infrastructure unified. Also\nhighly compatible with YoungJun's swap tiering [5], and other ideas like\nswap table compaction, swapops, as it aligns with a few proposals [6]\n[7] [8] [9] [10].\n\nIn the past two years, most efforts have focused on the swap\ninfrastructure, and we have made tremendous gains in performance,\nkeeping the memory usage reasonable or lower, and also greatly cleaned\nup and simplified the API and conventions.\n\nNow the infrastructures are almost ready, after P4, implementing an\ninfinitely or dynamically large swapfile can be done in a very easy to\nmaintain and flexible way, code change is minimal and progressive\nfor review, and makes future optimization like swap table compaction\ndoable too, since the infrastructure is all the same for all swaps.\n\nThe dynamic swap file is now using Xarray for the cluster info, and\ninside the cluster, it's all the same swap allocator, swap table, and\nexisting infrastructures. A virtual table is available for any extra\ndata or usage. See below for the benefits and what we can achieve.\n\nHuge thanks to Chris Li for the layered swap table and ghost swapfile\nidea, without whom the work here can't be archived. Also, thanks to Nhat\nfor pushing and suggesting using an Xarray for the swapfile [11] for\ndynamic size. I was originally planning to use a dynamic cluster\narray, which requires a bit more adaptation, cleanup, and convention\nchanges. But during the discussion there, I got the inspiration that\nXarray can be used as the intermediate step, making this approach\ndoable with minimal changes. Just keep using it in the future, it\nmight not hurt too, as Xarray is only limited to ghost / virtual\nfiles, so plain swaps won't have any extra overhead for lookup or high\nrisk of swapout allocation failure.\n\nI'm fully open and totally fine for suggestions on naming or API\nstrategy, and others are highly welcome to keep the work going using\nthis flexible approach. Following this approach, we will have all the\nfollowing things progressively (some are already or almost there):\n\n- 8 bytes per slot memory usage, when using only plain swap.\n  - And the memory usage can be reduced to 3 or only 1 byte.\n- 16 bytes per slot memory usage, when using ghost / virtual zswap.\n  - Zswap can just use ci_dyn->virtual_table to free up it's content\n    completely.\n  - And the memory usage can be reduced to 11 or 8 bytes using the same\n    code above.\n  - 24 bytes only if including reverse mapping is in use.\n- Minimal code review or maintenance burden. All layers are using the exact\n  same infrastructure for metadata / allocation / synchronization, making\n  all API and conventions consistent and easy to maintain.\n- Writeback, migration and compaction are easily supportable since both\n  reverse mapping and reallocation are prepared. We just need a\n  folio_realloc_swap to allocate new entries for the existing entry, and\n  fill the swap table with a reserve map entry.\n- Fast swapoff: Just read into ghost / virtual swap cache.\n- Zero static data (mostly due to swap table P4), even the clusters are\n  dynamic (If using Xarray, only for ghost / virtual swap file).\n- So we can have an infinitely sized swap space with no static data\n  overhead.\n- Everything is runtime configurable, and high-performance. An\n  uncompressible workload or an offline batch workload can directly use a\n  plain or remote swap for the lowest interference, memory usage, or for\n  best performance.\n- Highly compatible with YoungJun's swap tiering, even the ghost / virtual\n  file can be just a tier. For example, if you have a huge NBD that doesn't\n  care about fragmentation and compression, or the workload is\n  uncompressible, setting the workload to use NBD's tier will give you only\n  8 bytes of overhead per slot and peak performance, bypassing everything.\n  Meanwhile, other workloads or cgroups can still use the ghost layer with\n  compression or defragmentation using 16 bytes (zswap only) or 24 bytes\n  (ghost swap with physical writeback) overhead.\n- No force or breaking change to any existing allocation, priority, swap\n  setup, or reclaim strategy. Ghost / virtual swap can be enabled or\n  disabled using swapon / swapoff.\n\nAnd if you consider these ops are too complex to set up and maintain, we\ncan then only allow one ghost / virtual file, make it infinitely large,\nand be the default one and top tier, then it achieves the identical thing\nto virtual swap space, but with much fewer LOC changed and being runtime\noptional.\n\nCurrently, the dynamic ghost files are just reported as ordinary swap files\nin /proc/swaps and we can have multiple ones, so users will have a full\nview of what's going on. This is a very easy-to-change design decision.\nI'm open to ideas about how we should present this to users. e.g., Hiding\nit will make it more \"virtual\", but I don't think that's a good idea.\n\nThe size of the swapfile (si->max) is now just a number, which could be\nchangeable at runtime if we have a proper idea how to expose that and\nmight need some audit of a few remaining users. But right now, we can\nalready easily have a huge swap device with no overhead, for example:\n\nfree -m\n               total        used        free      shared  buff/cache   available\nMem:            1465         250         927           1         356        1215\nSwap:       15269887           0    15269887\n\nAnd for easier testing, I added a /dev/ghostswap in this RFC. `swapon\n/dev/ghostswap` enables that. Without swapon /dev/ghostswap, any existing\nusers, including ZRAM, won't observe any change.\n\n===\n\nOriginal cover letter for swap table phase IV:\n\nThis series unifies the allocation and charging process of anon and shmem,\nprovides better synchronization, and consolidates cgroup tracking, hence\ndropping the cgroup array and improving the performance of mTHP by about\n~15%.\n\nStill testing with build kernel under great pressure, enabling mTHP 256kB,\non an EPYC 7K62 using 16G ZRAM, make -j48 with 1G memory limit, 12 test\nruns:\n\nBefore: 2215.55s system, 2:53.03 elapsed\nAfter:  1852.14s system, 2:41.44 elapsed (16.4% faster system time)\n\nIn some workloads, the speed gain is more than that since this reduces\nmemory thrashing, so even IO-bound work could benefit a lot, and I no\nlonger see any: \"Huh VM_FAULT_OOM leaked out to the #PF handler. Retrying\nPF\", it was shown from time to time before this series.\n\nNow, the swap cache layer ensures a folio will be the exclusive owner of\nthe swap slot, then charge it, which leads to much smaller thrashing when\nunder pressure.\n\nAnd besides, the swap cgroup static array is gone, so for example, mounting\na 1TB swap device saves about 512MB of memory:\n\nBefore:\n        total     used     free     shared  buff/cache available\nMem:    1465      854      331      1       347        610\nSwap:   1048575   0        1048575\n\nAfter:\n        total     used     free     shared  buff/cache available\nMem:    1465      332      838      1       363        1133\nSwap:   1048575   0        1048575\n\nIt saves us ~512M of memory, we now have close to 0 static overhead.\n\nLink: https://lore.kernel.org/linux-mm/20260218-swap-table-p3-v3-0-f4e34be021a7@tencent.com/ [1]\nLink: https://lore.kernel.org/linux-mm/20260213-memcg-privid-v1-1-d8cb7afcf831@tencent.com/ [2]\nLink: https://lore.kernel.org/linux-mm/20260211-shmem-swap-gfp-v1-1-e9781099a861@tencent.com/ [3]\nLink: https://lore.kernel.org/linux-mm/20260216-hibernate-perf-v4-0-1ba9f0bf1ec9@tencent.com/ [4]\nLink: https://lore.kernel.org/linux-mm/20260217000950.4015880-1-youngjun.park@lge.com/ [5]\nLink: https://lore.kernel.org/all/CAMgjq7BvQ0ZXvyLGp2YP96+i+6COCBBJCYmjXHGBnfisCAb8VA@mail.gmail.com/ [6]\nLink: https://lwn.net/Articles/974587/ [7]\nLink: https://lwn.net/Articles/932077/ [8]\nLink: https://lwn.net/Articles/1016136/ [9]\nLink: https://lore.kernel.org/linux-mm/20260208215839.87595-1-nphamcs@gmail.com/ [10]\nLink: https://lore.kernel.org/linux-mm/CAKEwX=OUni7PuUqGQUhbMDtErurFN_i=1RgzyQsNXy4LABhXoA@mail.gmail.com/ [11]\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\nChris Li (1):\n      mm: ghost swapfile support for zswap\n\nKairui Song (14):\n      mm: move thp_limit_gfp_mask to header\n      mm, swap: simplify swap_cache_alloc_folio\n      mm, swap: move conflict checking logic of out swap cache adding\n      mm, swap: add support for large order folios in swap cache directly\n      mm, swap: unify large folio allocation\n      memcg, swap: reparent the swap entry on swapin if swapout cgroup is dead\n      memcg, swap: defer the recording of memcg info and reparent flexibly\n      mm, swap: store and check memcg info in the swap table\n      mm, swap: support flexible batch freeing of slots in different memcg\n      mm, swap: always retrieve memcg id from swap table\n      mm/swap, memcg: remove swap cgroup array\n      mm, swap: merge zeromap into swap table\n      mm, swap: add a special device for ghost swap setup\n      mm, swap: allocate cluster dynamically for ghost swapfile\n\n MAINTAINERS                 |   1 -\n drivers/char/mem.c          |  39 ++++\n include/linux/huge_mm.h     |  24 +++\n include/linux/memcontrol.h  |  12 +-\n include/linux/swap.h        |  30 ++-\n include/linux/swap_cgroup.h |  47 -----\n mm/Makefile                 |   3 -\n mm/internal.h               |  25 ++-\n mm/memcontrol-v1.c          |  78 ++++----\n mm/memcontrol.c             | 119 ++++++++++--\n mm/memory.c                 |  89 ++-------\n mm/page_io.c                |  46 +++--\n mm/shmem.c                  | 122 +++---------\n mm/swap.h                   | 122 +++++-------\n mm/swap_cgroup.c            | 172 ----------------\n mm/swap_state.c             | 464 ++++++++++++++++++++++++--------------------\n mm/swap_table.h             | 105 ++++++++--\n mm/swapfile.c               | 278 ++++++++++++++++++++------\n mm/vmscan.c                 |   7 +-\n mm/workingset.c             |  16 +-\n mm/zswap.c                  |  29 +--\n 21 files changed, 977 insertions(+), 851 deletions(-)\n---\nbase-commit: 4750368e2cd365ac1e02c6919013c8871f35d8f9\nchange-id: 20260111-swap-table-p4-98ee92baa7c4\n\nBest regards,\n-- \nKairui Song <kasong@tencent.com>",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Barry Song",
              "summary": "Reviewer Barry Song noted that the dynamic ghost swapfile implementation uses Xarray for cluster info, but he is concerned about potential overhead and compatibility issues when using it for plain swaps.\n\nReviewer Barry Song expressed strong disagreement with the naming convention of 'ghost' for the dynamic swapfile, finding it arbitrary and not descriptive of its functionality, and suggested replacing it with a more fitting name such as 'vswap', while also noting that Nhat is already using this name\n\nBarry Song expressed concern that the dynamic ghost swapfile should not be represented as a real file in the filesystem, even if it remains visible in /proc/swaps, and suggested that this approach seems unnatural when using a filesystem like ext4.\n\nReviewer Barry Song questioned the nature and implementation of /dev/ghostswap, expressing concern that it is a block device or character device, and suggesting that coupling it with a memdev character device feels odd.\n\nReviewer Barry Song noted that the patch introduces a new swapfile type, but did not provide any specific feedback or suggestions for improvement.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential overhead",
                "compatibility issues",
                "disagreement",
                "requested changes",
                "coupling with memdev",
                "feels very odd",
                "lack of specific feedback"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, Feb 20, 2026 at 7:42AM Kairui Song via B4 Relay\n<devnull+kasong.tencent.com@kernel.org> wrote:\n\n---\n\nTo be honest, I really dislike the name \"ghost.\" I would\nprefer something that reflects its actual functionality.\n\"Ghost\" does not describe what it does and feels rather\narbitrary.\n\nI suggest retiring the name \"ghost\" and replacing it with\nsomething more appropriate. \"vswap\" could be a good option,\nbut Nhat is already using that name.\n\n---\n\nEven if it remains visible in /proc/swaps, I would rather\nnot represent it as a real file in any filesystem. Putting\na \"ghost\" swapfile on something like ext4 seems unnatural.\n\n---\n\n/dev/ghostswap is assumed to be a virtual block device or\nsomething similar? If it is a block device, how is its size\nrelated to si->size?\n\nLooking at [PATCH RFC 14/15] mm, swap: add a special device\nfor ghost swap setup, it appears to be a character device.\nThis feels very odd to me. Im not in favor of coupling the\nghost swapfile with a memdev character device.\nA cdev should be a true character device.\n\n---\n\nThanks\nBarry",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-21",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer noted that the dynamic ghost swapfile patches (13-15) retain the original naming convention from Chris Li, which could be easily changed by a simple 'search and replace' operation.\n\nReviewer Kairui Song expressed agreement and approval for the patch, mentioning a minor deviation from their previous idea but noting that plain swap still has zero overhead.\n\nReviewer Kairui Song provided an example output of the swapon command after applying the patch series, showing a ghost swap device and its properties. They suggested renaming the device to 'xswap' and demonstrated how to enable or disable it using swapon and swapoff commands.\n\nThe reviewer clarified that the 'dynamic ghost swapfile' is a placeholder for testing purposes, unrelated to the si->size field.\n\nThe reviewer noted that the patch introduces a placeholder for a virtual device, which is an alternative to Chris' dummy header approach, and suggested making the size of this device dynamic by either passing it from userspace or automatically increasing it every time a new cluster is used.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor",
                "trivial",
                "agreement",
                "approval",
                "no clear opinion on the technical merits of the patch",
                "clarification",
                "no clear signal"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Barry,\n\nThat can be easily changed by \"search and replace\", I just kept the\nname since patch 13 is directly from Chris and I just didn't change\nit.\n\n---\n\nThat looks good to me too, you can also check the slide from LSFMM\nlast year page 23 to see how I imaged thing would workout at that\ntime:\nhttps://drive.google.com/file/d/1_QKlXErUkQ-TXmJJy79fJoLPui9TGK1S/view\n\nThe actual layout will be a bit different from that slide, since the\nredirect entry will be in the lower devices, the virtual device will\nhave an extra virtual table to hold its redirect entry. But still I'm\nglad that plain swap still has zero overhead so ZRAM or high\nperformance NVME is still good.\n\n---\n\nHow do you think about this? Here is the output after this sereis:\n# swapon\nNAME           TYPE       SIZE USED PRIO\n/dev/ghostswap ghost     11.5G 821M   -1\n/dev/ram0      partition 1024G 9.9M   -1\n/dev/vdb2      partition    2G 112K   -1\n\nOr we can rename it to:\n# swapon\nNAME           TYPE       SIZE USED PRIO\n/dev/xswap     xswap     11.5G 821M   -1\n/dev/ram0      partition 1024G 9.9M   -1\n/dev/vdb2      partition    2G 112K   -1\n\nswapon /dev/xswap will enable this layer (for now I just hardcoded it\nto be 8 times the size of total ram). swapoff /dev/xswap disables it.\nWe can also change the priority.\n\nWe can also hide it.\n\n---\n\nIt's not a real device, just a placeholder to make swapon usable\nwithout any modification for easier testing (some user space\nimplementation doesn't work well with dummy header). And it has\nnothing to do with the si->size.\n\n---\n\nNo coupling at all, it's just a place holder so swapon (the syscall)\nknows it's a virtual device, which is just an alternative to the dummy\nheader approach from Chris, so people can test it easier.\n\nThe si->size is just a number and any value can be given. I just\nhaven't decided how we should pass the number to the kernel or just\nmake it dynamic: e.g. set it to total ram size and increase by 2M\nevery time a new cluster is used.",
              "reply_to": "Barry Song",
              "message_date": "2026-02-21",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Barry Song",
              "summary": "Reviewer Barry Song suggested replacing the dynamic ghost swapfile with a 'virtual' block device, /dev/xswap, which would display its size as 11.5G via ls -l, and is considered more natural than using a cdev placeholder.\n\nReviewer Barry Song noted that the dynamic ghost swapfile appears as a character device in /dev/, which could lead to user confusion and unexpected interactions with udev rules, requesting a different naming convention or approach.\n\nReviewer Barry Song noted that using a character device (cdev) as a placeholder introduces behavioral coupling, where the same code serves different purposes for swap and non-swap use cases.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "behavioral coupling",
                "requested change"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Id rather have a virtual block device, /dev/xswap, with\nits size displayed as 11.5G via `ls -l filename`. This is\nalso more natural than relying on a cdev placeholder.\n\nIf\n\n---\n\nI understand it is a placeholder for swap, but if it appears\nas /dev/ghostfile, users browsing /dev/ will see it as a\nreal cdev. A /dev/chardev is intended for user read/write\naccess.\nAlso, udev rules can act on an exported cdev. This couples\nus with a lot of userspace behavior.\n\n---\n\nUsing a cdev as a placeholder has introduced behavioral\ncoupling. For swap, it serves as a placeholder; for anything\noutside swap, it behaves as a regular cdev.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-21",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "The reviewer found the patch's semantics to be an improvement over the current state and expressed a positive opinion.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "improvement",
                "positive opinion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Those semantics look good to me. I think it's better than the status\nquo, actually.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "The reviewer noted that refault evaluation should happen at the level that drove eviction, specifically when pages get reclaimed in a round-robin fashion, to prevent retaining stale workingset in one subgroup while the other is thrashing.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "refault evaluation needs to happen at the right level",
                "risk of retaining stale workingset"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This is not correct.\n\nAs much as I like the idea of storing the swap_cgroup association\ninside the shadow entry, the refault evaluation needs to happen at the\nlevel that drove eviction.\n\nConsider a workload that is split into cgroups purely for accounting,\nnot for setting different limits:\n\nworkload (limit domain)\n`- component A\n`- component B\n\nThis means the two components must compete freely, and it must behave\nas if there is only one LRU. When pages get reclaimed in a round-robin\nfashion, both A and B get aged at the same pace. Likewise, when pages\nin A refault, they must challenge the *combined* workingset of both A\nand B, not just the local pages.\n\nOtherwise, you risk retaining stale workingset in one subgroup while\nthe other one is thrashing. This breaks userspace expectations.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "The reviewer noted that the patch introduces duplicate metadata for every page written to disk through zswap, one in the ghost swapfile and another in the backend swapfile.\n\nReviewer Johannes Weiner requested extending the patch to include disk swap functionality, citing performance issues with scattered I/O during swapoff operations.\n\nThe reviewer expressed concern that the dynamic ghost swapfile approach makes free(1) output misleading, as it presents a swap space unrelated to actual swap capacity, and obscures actual disk swap capacity.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "arbitrary restriction",
                "misleading presentation"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That seems to tie us pretty permanently to duplicate metadata.\n\nFor every page that was written to disk through zswap, we have an\nentry in the ghost swapfile, and an entry in the backend swapfile, no?\n\n---\n\nCan we get this for disk swap as well? ;)\n\nZswap swapoff is already fairly fast, albeit CPU intense. It's the\nscattered IO that makes swapoff on disks so terrible.\n\n---\n\nI'm not a fan of this. This makes free(1) output kind of useless, and\nvery misleading. The swap space presented here has nothing to do with\nactual swap capacity, and the actual disk swap capacity is obscured.\n\nAnd how would a user choose this size? How would a distribution?\n\nThe only limit is compression ratio, and you don't know this in\nadvance. This restriction seems pretty arbitrary and avoidable.\n\nThere is no good technical reason to present this in any sort of\nstatic fashion.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham",
              "summary": "Reviewer Nhat Pham noted that the dynamic ghost swapfile uses Xarray for cluster info, but expressed concern about potential overhead and risk of swapout allocation failure for plain swaps.\n\nReviewer Nhat Pham expressed concerns about the placement of metadata (swap count, cgroup, etc.) in the dynamic ghost swap file design, citing issues with storing it at the backend layer and proposing to store it in the top layer instead.\n\nReviewer Nhat Pham noted that the patch series lacks several key features required for a virtual swap setup, including charging, backend decision making and efficient transfer, virtual swap freeing, and swapoff logic. He argued that implementing these features would make the code more complex and suggested comparing it to the existing vswap series which already includes these features.\n\nReviewer Nhat Pham expressed concern that exposing virtual swap state to users in the swapfile summary view is confusing and poorly reflects physical state, suggesting sysfs debug counters for troubleshooting instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential overhead",
                "risk of swapout allocation failure",
                "requested changes",
                "suggested comparison with existing vswap series"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Thu, Feb 19, 2026 at 3:42PM Kairui Song via B4 Relay\n<devnull+kasong.tencent.com@kernel.org> wrote:\n\n---\n\nThanks for your effort. Dynamic swap space is a very important\nconsideration anyone deploying compressed swapping backend on large\nmemory systems in general. And yeah, I think using a radix tree/xarray\nis easiest out-of-the-box solution for this - thanks for citing me :P\n\nI still have some confusion and concerns though. Johannes already made\nsome good points - I'll just add some thoughts from my point of view,\nhaving gone back and forth with virtual swap designs:\n\n1. At which layer should the metadata (swap count, swap cgroup, etc.) live?\n\nI remember that in your LSFMMBPF presentation (time flies), your\nproposal was to store a redirection entry in the top layer, and keep\nall the metadata at the bottom (i.e backend) layer? This has problems\n- for once, you might not know suitable backend at swap allocation\ntime, but only at writeout time. For e.g, in certain zswap setups, we\nreject the incompressible page and cycle it back to the active LRU, so\nwe have no space in zswap layer to store swap entry metadata (note\nthat at this point the swap entry cannot be freed, because we have\nalready unmapped the page from the PTEs (and would require a page\ntable walk to undo this a la swapoff). Similarly, when we\nexclusive-load a page from zswap, we invalidate the zswap metadata\nstruct, so we will no longer have a space for the swap entry metadata.\n\nThe zero-filled (or same-filled) swap entry case is an even more\negregious example :) It really shouldn't be a state in any backend -\nit should be a completely independent backend.\n\nThe only design that makes sense is to store metadata in the top layer\nas well. It's what I'm doing for my virtual swap patch series, but if\nwe're pursuing this opt-in swapfile direction we are going to\nduplicate metadata :)\n\n---\n\n2. I think the \"fewer LOC changed\" claim here is misleading ;)\n\nA lot of the behaviors that is required in a virtual swap setup is\nmissing from this patch series. You are essentially just implementing\na swapfile with a dynamic allocator. You still need a bunch more logic\nto support a proper multi-tier virtual swap setup - just on top of my\nmind:\n\na. Charging: virtual swap usage not be charged the same as the\nphysical swap usage, especially when you have a zswap + disk swap\nsetup, powered by virtual swap. For once, I don't believe in sizing\nvirtual swap, but also a latency-sensitive cgroup allowe to use only\nzswap (backed by virtual swap) is using and competing for resources\nvery differently from a cgroup whose memory is incompressible and only\nallowed to use disk swap.\n\nb. Backend decision making and efficient backend transfer - as you\nsaid, \"folio_realloc_swap\" is yet to be implemented :) And as I\nmention earlier, we CANNOT determine swap backend before PTE unmap\ntime, because backend suitability is content-dependent. You will have\nto add extra logic to handle this nuanced swap allocation behavior.\n\nc. Virtual swap freeing - it requires more work, as you have to free\nboth the virtual swap entry itself, as well as digging into the\nphysical backend layer.\n\nd. Swapoff - now you have to both page tables and virtual swap table.\n\nBy the time you implement all of this, I think it will be MORE\ncomplex, especially since you want to maintain BOTH the new setup and\nthe old non-virtual swap setup. You'll have to litter the codes with a\nbunch of ifs (or ifdefs) to check - hey do we have a virtual swapfile?\nHey is this a virtual swap slot? Etc. Etc. everywhere, from the PTE\ninfra (zapping, page fault, etc.), to cgroup infra, to physical swap\narchitecture.\n\nComparing this line of work by itself with the vswap series, which\nalready comes with all of these included, is a bit apples-to-oranges\n(and especially with the fact that vswap simplifies logic and removes\nLoCs in a lot of places too, such as in swapoff. The delta LoC is only\n300-400 IIRC?).\n\n---\n\n3. I don't think we should expose virtual swap state to users (in this\ncase, in the swapfile summary view i.e in free). It is just confusing,\nas it poorly reflects the physical state (be it compressed memory\nfootprint, or actual disk usage). We obviously should expose a bunch\nof sysfs debug counters for troubleshootings, but for average users,\nit should be all transparent.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer pointed out that there is only one entry in the ghost swapfile, which serves as a reverse mapping to the physical swap slot, allowing for O(1) time swapoff and migration.\n\nReviewer Kairui Song clarified that the patch is related to disk swap, not zswap, and explained how swapoff of a physical entry loads swap data into a virtual slot.\n\nReviewer suggested modifying the interface design of the dynamic ghost swapfile to make it flexible and changeable, proposing setting a super large value and hiding it to achieve zero overhead for existing ZRAM or plain swap users.\n\nReviewer noted that in practice, they limit their ZRAM setup to 1/4 or 1:1 of total RAM to avoid endless reclaim and OOM, implying a concern about the infinite size ZSWAP enabled by this series.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "clarification",
                "explanation",
                "context",
                "interface design",
                "flexible changes",
                "concern",
                "implying"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, only one entry in the ghost swapfile (xswap or virtual swap file,\nanyway it's just a name). The one in the physical swap is a reverse\nmapping entry, it tells which slot in the ghost swapfile is pointing\nto the physical slot, so swapoff / migration of the physical slot can\nbe done in O(1) time.\n\nSo, zero duplicate of any data.\n\n---\n\nI am talking about disk swap here, not zswap. Swapoff of a physical\nentry just loads the swap data in the virtual slot according to the\nreverse mapping entry.\n\n---\n\nIt can be dynamic (just si->max += 2M on every cluster allocation\nsince it's really just a number now). Can be hidden, and can have an\ninfinite size. That's just an interface design that can be flexibly\nchanged.\n\nFor example if we just set this to a super large value and hide it, it\nwill look identical to vss from userspace perspect, but stay optional\nand zero overhead for existing ZRAM or plain swap users.\n\n---\n\nJust as a reference: In practice we limit our ZRAM setup to 1/4 or 1:1\nof the total RAM to avoid the machine goto endless reclaim and never\ngo OOM.\n\nBut we can also have an infinite size ZSWAP now, with this series.",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that the patch is already addressing the issue of storing metadata at the top layer, eliminating the need for a reverse mapping in the lower layer.\n\nThe reviewer expressed concerns that the dynamic allocator is incomplete and suggested that other parts of the series, such as unified folio allocation for swapin, could be merged separately to provide immediate benefits without requiring the virtual swap or memcg part.\n\nReviewer noted that an alternative solution exists for handling memcg reparent behavior, which involves having a ci->memcg_table and allowing each layer to have its own charge design, resulting in lower overhead than the original approach.\n\nReviewer pointed out that folio_alloc_swap occurs before unmap and realloc, which contradicts their previous statement about not doing that at all.\n\nReviewer suggested a simple modification to the swapoff logic by introducing a reverse map slot in the ghost swap layer, allowing for easy handling of swapoff operations.\n\nReviewer noted that using the same infrastructure for both ghost and virtual swap files is beneficial as it allows for reuse and unification of code, reducing overhead and complexity.\n\nThe reviewer noted that the old swapoff command should not be removed immediately, as it provides a guarantee to clear up the swap cache, which is essential for certain workloads, especially in cloud environments where minor page faults can trigger anon faults. The reviewer suggested that this functionality should be preserved.\n\nReviewer Kairui Song expressed concern that the dynamic ghost swapfile interface may not be transparent to users, suggesting an alternative approach using sysfs.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "no issues remaining",
                "requested changes",
                "suggested collaboration",
                "alternative solution",
                "lower overhead",
                "contradiction",
                "confusion",
                "reusing existing infrastructure",
                "unified codebase",
                "preservation of old swapoff functionality"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It's already doing that, storing metadata at the top layer, only a\nreverse mapping in the lower layer.\n\nSo none of these issues are still there. Don't worry, I do remember\nthat conversation and kept that in mind :)\n\n---\n\nI left that part undone kind of on purpose, since it's only RFC, and\nin hope that there could be collaboration.\n\nAnd the dynamic allocator is only ~200 LOC now. Other parts of this\nseries are not only for virtual swap. For example the unified folio\nalloc for swapin, which gives us 15% performance gain in real\nworkloads, can still get merged and benifit all of us without\ninvolving the virtual swap or memcg part.\n\nAnd meanwhile, with the later patches, we don't have to re-implement\nthe whole infrastructure to have a virtual table. And future plans\nlike data compaction should benifit every layer naturally (same\ninfra).\n\n---\n\nAh, now as you mention it, I see in the beginning of this series I\nadded: \"Swap table P4 is stable and good to merge if we are OK with a\nfew memcg reparent behavior (there is also a solution if we don't)\".\nThe \"other solution\" also fits your different charge idea here. Just\nhave a ci->memcg_table, then each layer can have their own charge\ndesign, and the shadow is still only used for refault check. That\ngives us 10 bytes per slot overhead though, but still lower than\nbefore and stays completely dynamic.\n\nAlso, no duplicated memcg, since the upper layer and lower layer\nshould be charged differently. If they don't, then just let\nci->memcg_table stay NULL.\n\n---\n\nAnd we are not doing that at all. folio_alloc_swap happens before\nunmap, but realloc happens after that. VSS does the same thing.\n\n---\n\nSwapoff is actually easy here... If it sees a reverse map slot, read\ninto the upper layer. Else goto the old logic. Then it's done. If\nghost swap is the layer with highest priority, then every slot is a\nreverse map slot.\n\n---\n\nIt is using the same infrastructure, which means a lot of things are\nreused and unified. Isn't that a good sign? And again we don't need to\nre-implement the whole infra.\n\nAnd if you need multiple layers then there will be more \"if\"s and\noverhead however you implement it. But with unified infra, each layer\ncan stay optional. And checking \"si->flags & GHOST / VIRTUAL\" really\nshouldn't be costly or trouble some at all, compared to a mandatory\nlayer with layers of Xarray walk.\n\nAnd we can move, maintain the virt part in a separate place.\n\n---\n\nOne thing I want to highlight here is that the old swapoff really\nshouldn't just die. That gives us no chance to clear up the swap cache\nat all (vss holding swap data in RAM is also just swap cache). Pages\nstill in swap cache means minor page faults will still trigger. If the\nworkload is opaque but we knows a high load of traffic is coming and\nwants to get rid of any performance bottleneck by reading all folios\ninto the right place, swapoff gives the guarantee that no anon fault\nwill be ever triggered, that happens a lot in multiple tenant cloud\nenvironments, and these workload are opaque so madvise doesn't apply.\n\n---\n\nUsing sysfs can also be a choice, that's really just a demonstration\ninterface. But I do think it's worse if the user has no idea what is\nactually going on.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt requested that the commit message be made self-contained, specifically citing that the current opening statement is confusing.\n\nReviewer Shakeel Butt questioned the reason behind the difference in cgroup between folio->swap and folio->memcg, implying that this discrepancy may cause issues.\n\nReviewer Shakeel Butt questioned whether the patch's behavior is consistent across different types of memory backed by shmem, specifically MAP_SHARED and memfd, as well as cow anon memory shared between parent and child processes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested clarification",
                "requested clarification on shmem behavior"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I directly jump to this patch and the opening statement is confusing. Please\nmake the commit message self contained.\n\n---\n\nWhy is this an issue (i.e. folio->swap's cgroup different from\nfolio->memcg)?\n\n---\n\nIs this behavior same for all types of memory backed by shmem (i.e. MAP_SHARED,\nmemfd etc)? What about cow anon memory shared between parent and child\nprocesses?",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song noted that tracking folio->swap using folio->memcg could lead to a lock ordering violation, and suggested avoiding an external array to record folio->swap's memcgid.\n\nReviewer noted that when a memory cgroup is dead and its ID is referenced in a swap entry's memcgid record, the swap entry will incorrectly recharge the swap-in folio.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "lock ordering issue",
                "requested change",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It's an issue for this series, if we want to track the folio->swap\nusing folio->memcg to avoid an external array to record folio->swap's\nmemcgid.\n\n---\n\nIt's the same. If the memcg is dead and a swap entry's memcgid record\npoints to the dead memcg, then whoever reads this swap entry recharges\nthe swapin folio.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kairui Song",
              "summary": "Reviewer Kairui Song questioned the necessity of a specific change made in the patch, citing that it was introduced before anon shadow was implemented and may not be a significant issue, suggesting alternative approaches such as using ci->memcg_table or exploring ways to reduce memory usage with MGLRU and aging feedback.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Johannes, thanks for pointing this out.\n\nI'm just not sure how much of a real problem this is. The refault\nchallenge change was made in commit b910718a948a which was before anon\nshadow was introduced. And shadows could get reclaimed, especially\nwhen under pressure (and we could be doing that again by reclaiming\nfull_clusters with swap tables). And MGLRU simply ignores the\ntarget_memcg here yet it performs surprisingly well with multiple\nmemcg setups. And I did find a comment in workingset.c saying the\nkernel used to activate all pages, which is also fine. And that commit\nalso mentioned the active list shrinking, but anon active list gets\nshrinked just fine without refault feedback in shrink_lruvec under\ncan_age_anon_pages.\n\nSo in this RFC I just be a bit aggressive and changed it. I can do\nsome tests with different memory size setup.\n\nIf we are not OK with it, then just use a ci->memcg_table then we are\nfine, everything is still dynamic but single slot usage could be a bit\nhigher, 8 bytes to 10 bytes: and maybe find a way later to make\nci->memcg_table NULL and shrink back to 8 bytes with, e.g. MGLRU and\nbalance the memcg with things like aging feed back maybe (the later\npart is just idea but seems doable?).",
              "reply_to": "Johannes Weiner",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Johannes Weiner",
              "summary": "The reviewer, Johannes Weiner, expressed concerns that the patch does not properly account for anon refault detection and cache replacement strategies when dealing with large anonymous working sets, particularly in the context of modern compression and flash swap. He emphasized that the code was driven by real production problems and requested a deeper understanding of its design before making significant changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested_changes",
                "concerns_about_design"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "*if inactive anon is empty, as part of the second\n                     chance logic\n\nPlease try to understand *why* this code is the way it is before\nthrowing it all out. It was driven by real production problems. The\nfact that some workloads don't care is not prove that many don't hurt\nif you break this.\n\nAnon refault detection was added for that reason: Once you have swap,\nyou facilitate anon workingsets that exceed memory capacity. At that\npoint, cache replacement strategies apply. Scan resistance matters.\n\nWith fast modern compression and flash swap, the anon set alone can be\nlarger than memory capacity. Everything that\n6a3ed2123a78de22a9e2b2855068a8d89f8e14f4 says about file cache starts\napplying to anonymous pages: you don't want to throw out the hot anon\nworkingset just because somebody is doing a one-off burst scan through\na larger set of cold, swapped out pages.\n\nLike I said in the LSFMM thread, there is no difference between anon\nand file. There didn't use to be historically. The LRU lists were\nsplit mechanically because noswap systems became common (lots of RAM +\nrotational drives = sad swap) and there was no point in scanning/aging\nanonymous memory if there is no swap space.\n\nBut no reasonable argument has been put forth why anon should be aged\ncompletely differently than file when you DO have swap.\n\nThere is more explanation of Why for the cgroup behavior in the cover\nletter portion of 53138cea7f398d2cdd0fa22adeec7e16093e1ebd.",
              "reply_to": "Kairui Song",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "aZ3EQRZ1XRLsGlzX@linux.dev",
          "url": "https://lore.kernel.org/all/aZ3EQRZ1XRLsGlzX@linux.dev/",
          "date": "2026-02-24T15:32:03Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations in the Linux kernel, allowing for future instrumentation or debugging hooks to be added without modifying individual call sites. The wrappers are not yet used but prepare the code for subsequent patches. This change is intended to centralize zone lock operations behind a common interface, making it easier to add functionality in the future.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing reviewer feedback about direct zone lock acquire/release operations not being replaced with the newly introduced wrappers, and has confirmed that this change will be made in the next patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "next patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author is addressing concerns about the lack of visibility into zone lock contention and its impact on performance, particularly in memory-intensive workloads. They explain that existing instrumentation does not provide sufficient information to diagnose issues and propose adding dedicated tracepoint instrumentation to the zone lock, following a similar model to mmap_lock tracing. The author also mentions minor restructuring required for compaction changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging technical concerns",
                "proposing additional instrumentation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the zone lock wrappers interfering with compact_lock_irqsave() by introducing a new struct compact_lock to abstract the underlying lock type, which will allow compact_lock_irqsave() to operate correctly on both zone locks and raw spinlocks. The author confirmed that no functional change is intended.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "no functional change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "The author addressed a concern about the performance impact of adding tracepoint instrumentation to the zone lock wrappers, explaining that they followed the mmap_lock pattern and ensured the fast path is unaffected when tracing is disabled.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reordering the series to introduce zone lock wrappers and tracepoints together, before mechanically converting users to the wrappers, to improve understanding of the changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing the zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason\n\nreviewer pointed out that the zone_lock_irqsave() macro should not return a value and suggested replacing it with an if-else statement\n\nReviewer Cheatham suggested moving zone lock wrapper changes, which are not yet used, to a later patch where they fit better with other similar changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "requested_reorder"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to introduce zone lock wrappers, stating it's 'just different taste' and suggesting squashing patches (1) and (2) together instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "nit"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged a suggestion from reviewer Cheatham about reordering patches in the series, explained that they intentionally structured the series to keep refactoring and instrumentation changes separate, and stated their preference for maintaining the current order.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged suggestion",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the patch's priority should be reassessed in favor of improving the reading order of the series, but ultimately accepted the current implementation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK",
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author addressed Shakeel Butt's concern about using macros for zone lock wrappers, explaining that it's necessary to modify the flags variable passed by the caller and maintain consistency with existing locking patterns.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "explained reasoning",
                "acknowledged feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The reason for using macros in those two cases is that they need to\nmodify the flags variable passed by the caller, just like\nspin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same\nconvention here.\n\nIf we used normal inline functions instead, we would need to pass a\npointer to flags, which would change the call sites and diverge from the\nexisting *_irqsave() locking pattern.\n\nThere is also a difference between zone_lock_irqsave() and\nzone_trylock_irqsave() implementations: the former is implemented as a\ndo { } while (0) macro since it does not return a value, while the\nlatter uses a GCC extension in order to return the trylock result. This\nmatches spin_lock_* convention as well.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dmitry Ilvokhin (author)",
              "summary": "Author acknowledged that the zone lock wrappers are not valuable and agreed to remove them.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "will remove"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, I agree, there is no much value in this wrappers, will remove them,\nthanks!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Usama Arif",
      "primary_email": "usama.arif@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    }
  ]
}