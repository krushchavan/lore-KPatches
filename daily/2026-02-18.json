{
  "date": "2026-02-18",
  "report_file": "2026-02-18_ollama_llama3.1-8b.html",
  "llm_backends": [
    [
      "ollama",
      "llama3.1:8b"
    ]
  ],
  "generation_time_seconds": 22360.628803014755,
  "developer_reports": [
    {
      "name": "Alexandre Ghiti",
      "primary_email": "alexghiti@rivosinc.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Boris Burkov",
      "primary_email": "boris@bur.io",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Dmitry Ilvokhin",
      "primary_email": "d@ilvokhin.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Gregory Price",
      "primary_email": "gourry@gourry.net",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Jeff Layton",
      "primary_email": "jlayton@kernel.org",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH nfs-utils v2 0/4] nfsdctl: properly handle older kernels that don't support min-threads",
          "message_id": "20260204-minthreads-v2-0-a7eba34201e9@kernel.org",
          "url": "https://lore.kernel.org/all/20260204-minthreads-v2-0-a7eba34201e9@kernel.org/",
          "date": "2026-02-04T16:48:47Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-04",
          "patch_summary": "This patch series addresses an issue where the nfsdctl tool fails when used with older kernels that don't support the min-threads attribute. The patches add a mechanism for nfsdctl to determine which attributes are supported by the kernel and handle unsupported settings accordingly. This is achieved by querying the netlink policy before sending the min-threads attribute, and removing the dependency on UAPI headers by maintaining a private header file.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about the dependency on system headers for min-threads support, explaining that maintaining their own copy of nfsd_netlink.h in-tree allows them to unconditionally compile in this feature.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I originally had this depend on the system header, but if we maintain\nour copy of nfsd_netlink.h in tree, then we can unconditionally compile\nin support for the MIN_THREADS option.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n configure.ac                 |  6 ++----\n utils/nfsdctl/nfsd_netlink.h |  2 ++\n utils/nfsdctl/nfsdctl.c      | 16 +---------------\n 3 files changed, 5 insertions(+), 19 deletions(-)\n\ndiff --git a/configure.ac b/configure.ac\nindex 5cc1e9186542c975abf200edbef30bc8f6ecb8ee..a65c7837b8cb72eaa2f3dbb89069599c074be4ec 100644\n--- a/configure.ac\n+++ b/configure.ac\n@@ -262,12 +262,10 @@ AC_ARG_ENABLE(nfsdctl,\n \t\tPKG_CHECK_MODULES(LIBNLGENL3, libnl-genl-3.0 >= 3.1)\n \t\tPKG_CHECK_MODULES(LIBREADLINE, readline)\n \t\tAC_CHECK_HEADERS(linux/nfsd_netlink.h)\n-\t\tAC_CHECK_DECLS([NFSD_A_SERVER_MIN_THREADS], , ,\n-\t\t\t       [#include <linux/nfsd_netlink.h>])\n \n-\t\t# ensure we have the pool-mode commands\n+\t\t# ensure we have the MIN_THREADS attribute\n \t\tAC_COMPILE_IFELSE([AC_LANG_PROGRAM([[#include <linux/nfsd_netlink.h>]],\n-\t\t\t\t                   [[int foo = NFSD_CMD_POOL_MODE_GET;]])],\n+\t\t\t\t                   [[int foo = NFSD_A_SERVER_MIN_THREADS;]])],\n \t\t\t\t   [AC_DEFINE([USE_SYSTEM_NFSD_NETLINK_H], 1,\n \t\t\t\t\t      [\"Use system's linux/nfsd_netlink.h\"])])\n \t\tAC_COMPILE_IFELSE([AC_LANG_PROGRAM([[#include <linux/lockd_netlink.h>]],\ndiff --git a/utils/nfsdctl/nfsd_netlink.h b/utils/nfsdctl/nfsd_netlink.h\nindex 887cbd12b695f2398c96976ba2d70e68ee0d93c0..e9efbc9e63d83ed25fcd790b7a877c0023638f15 100644\n--- a/utils/nfsdctl/nfsd_netlink.h\n+++ b/utils/nfsdctl/nfsd_netlink.h\n@@ -2,6 +2,7 @@\n /* Do not edit directly, auto-generated from: */\n /*\tDocumentation/netlink/specs/nfsd.yaml */\n /* YNL-GEN uapi header */\n+/* To regenerate run: tools/net/ynl/ynl-regen.sh */\n \n #ifndef _UAPI_LINUX_NFSD_NETLINK_H\n #define _UAPI_LINUX_NFSD_NETLINK_H\n@@ -34,6 +35,7 @@ enum {\n \tNFSD_A_SERVER_GRACETIME,\n \tNFSD_A_SERVER_LEASETIME,\n \tNFSD_A_SERVER_SCOPE,\n+\tNFSD_A_SERVER_MIN_THREADS,\n \n \t__NFSD_A_SERVER_MAX,\n \tNFSD_A_SERVER_MAX = (__NFSD_A_SERVER_MAX - 1)\ndiff --git a/utils/nfsdctl/nfsdctl.c b/utils/nfsdctl/nfsdctl.c\nindex c906a2c8ba6d357e182d341a30610e367e74c093..6b3c98009488d1687e7e751eaed6c4f1d9613d39 100644\n--- a/utils/nfsdctl/nfsdctl.c\n+++ b/utils/nfsdctl/nfsdctl.c\n@@ -324,11 +324,9 @@ static void parse_threads_get(struct genlmsghdr *gnlh)\n \t\tcase NFSD_A_SERVER_THREADS:\n \t\t\tpool_threads[i++] = nla_get_u32(attr);\n \t\t\tbreak;\n-#if HAVE_DECL_NFSD_A_SERVER_MIN_THREADS\n \t\tcase NFSD_A_SERVER_MIN_THREADS:\n \t\t\tprintf(\"min-threads: %u\\n\", nla_get_u32(attr));\n \t\t\tbreak;\n-#endif\n \t\tdefault:\n \t\t\tbreak;\n \t\t}\n@@ -546,10 +544,8 @@ static int threads_doit(struct nl_sock *sock, int cmd, int grace, int lease,\n \t\t\tnla_put_u32(msg, NFSD_A_SERVER_LEASETIME, lease);\n \t\tif (scope)\n \t\t\tnla_put_string(msg, NFSD_A_SERVER_SCOPE, scope);\n-#if HAVE_DECL_NFSD_A_SERVER_MIN_THREADS\n \t\tif (minthreads >= 0)\n \t\t\tnla_put_u32(msg, NFSD_A_SERVER_MIN_THREADS, minthreads);\n-#endif\n \t\tfor (i = 0; i < pool_count; ++i)\n \t\t\tnla_put_u32(msg, NFSD_A_SERVER_THREADS, pool_threads[i]);\n \t}\n@@ -591,24 +587,16 @@ out:\n static void threads_usage(void)\n {\n \tprintf(\"Usage: %s threads { --min-threads=X } [ pool0_count ] [ pool1_count ] ...\\n\", taskname);\n-#if HAVE_DECL_NFSD_A_SERVER_MIN_THREADS\n \tprintf(\"    --min-threads= set the minimum thread count per pool to value\\n\");\n-#endif\n \tprintf(\"    pool0_count: thread count for pool0, etc...\\n\");\n \tprintf(\"Omit any arguments to show current thread counts.\\n\");\n }\n \n-#if HAVE_DECL_NFSD_A_SERVER_MIN_THREADS\n static const struct option threads_options[] = {\n \t{ \"help\", no_argument, NULL, 'h' },\n \t{ \"min-threads\", required_argument, NULL, 'm' },\n \t{ },\n };\n-#define THREADS_OPTSTRING \"hm:\"\n-#else\n-#define threads_options help_only_options\n-#define THREADS_OPTSTRING \"h\"\n-#endif\n \n static int threads_func(struct nl_sock *sock, int argc, char **argv)\n {\n@@ -618,12 +606,11 @@ static int threads_func(struct nl_sock *sock, int argc, char **argv)\n \tint opt, pools = 0;\n \n \toptind = 1;\n-\twhile ((opt = getopt_long(argc, argv, THREADS_OPTSTRING, threads_options, NULL)) != -1) {\n+\twhile ((opt = getopt_long(argc, argv, \"hm:\", threads_options, NULL)) != -1) {\n \t\tswitch (opt) {\n \t\tcase 'h':\n \t\t\tthreads_usage();\n \t\t\treturn 0;\n-#if HAVE_DECL_NFSD_A_SERVER_MIN_THREADS\n \t\tcase 'm':\n \t\t\terrno = 0;\n \t\t\tminthreads = strtoul(optarg, NULL, 0);\n@@ -632,7 +619,6 @@ static int threads_func(struct nl_sock *sock, int argc, char **argv)\n \t\t\t\treturn 1;\n \t\t\t}\n \t\t\tbreak;\n-#endif\n \t\t}\n \t}\n \n\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-02-04",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author is addressing a concern about resolving the string name to an id for every netlink call, which was previously done in each function. The author has restructured the code to resolve family names once and keep them, adding two new functions: `resolve_family` and `lockd_nl_family_setup`, as well as `nfsd_nl_family_setup`. A fix is planned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a need for restructure",
                "added new code to resolve family names once"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The current code resolves the string name to an id for every netlink\ncall. Just resolve the family names once and keep them.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n utils/nfsdctl/nfsdctl.c | 83 ++++++++++++++++++++++++++++++++++++++-----------\n 1 file changed, 64 insertions(+), 19 deletions(-)\n\ndiff --git a/utils/nfsdctl/nfsdctl.c b/utils/nfsdctl/nfsdctl.c\nindex 6b3c98009488d1687e7e751eaed6c4f1d9613d39..86a4a944d4e131f1114ca358d81779de0a034872 100644\n--- a/utils/nfsdctl/nfsdctl.c\n+++ b/utils/nfsdctl/nfsdctl.c\n@@ -46,9 +46,11 @@\n #include \"conffile.h\"\n #include \"xlog.h\"\n \n-/* compile note:\n- * gcc -I/usr/include/libnl3/ -o <prog-name> <prog-name>.c -lnl-3 -lnl-genl-3\n- */\n+/* The index of the \"lockd\" netlink family */\n+static int lockd_nl_family;\n+\n+/* The index of the \"nfsd\" netlink family */\n+static int nfsd_nl_family;\n \n struct nfs_version {\n \tuint8_t\tmajor;\n@@ -433,24 +435,18 @@ static struct nl_sock *netlink_sock_alloc(void)\n \treturn sock;\n }\n \n-static struct nl_msg *netlink_msg_alloc(struct nl_sock *sock, const char *family)\n+static struct nl_msg *netlink_msg_alloc(struct nl_sock *sock, int family)\n {\n \tstruct nl_msg *msg;\n \tint id;\n \n-\tid = genl_ctrl_resolve(sock, family);\n-\tif (id < 0) {\n-\t\txlog(L_ERROR, \"failed to resolve %s generic netlink family\", family);\n-\t\treturn NULL;\n-\t}\n-\n \tmsg = nlmsg_alloc();\n \tif (!msg) {\n \t\txlog(L_ERROR, \"failed to allocate netlink message\");\n \t\treturn NULL;\n \t}\n \n-\tif (!genlmsg_put(msg, 0, 0, id, 0, 0, 0, 0)) {\n+\tif (!genlmsg_put(msg, 0, 0, family, 0, 0, 0, 0)) {\n \t\txlog(L_ERROR, \"failed to add generic netlink headers to netlink message\");\n \t\tnlmsg_free(msg);\n \t\treturn NULL;\n@@ -459,6 +455,31 @@ static struct nl_msg *netlink_msg_alloc(struct nl_sock *sock, const char *family\n \treturn msg;\n }\n \n+static int resolve_family(struct nl_sock *sock, const char *name)\n+{\n+\tint family = genl_ctrl_resolve(sock, name);\n+\n+\tif (family < 0) {\n+\t\txlog(L_ERROR, \"failed to resolve %s generic netlink family: %d\", name, family);\n+\t\tfamily = 0;\n+\t}\n+\treturn family;\n+}\n+\n+static int lockd_nl_family_setup(struct nl_sock *sock)\n+{\n+\tif (!lockd_nl_family)\n+\t\tlockd_nl_family = resolve_family(sock, LOCKD_FAMILY_NAME);\n+\treturn lockd_nl_family;\n+}\n+\n+static int nfsd_nl_family_setup(struct nl_sock *sock)\n+{\n+\tif (!nfsd_nl_family)\n+\t\tnfsd_nl_family = resolve_family(sock, NFSD_FAMILY_NAME);\n+\treturn nfsd_nl_family;\n+}\n+\n static void status_usage(void)\n {\n \tprintf(\"Usage: %s status\\n\", taskname);\n@@ -482,7 +503,10 @@ static int status_func(struct nl_sock *sock, int argc, char ** argv)\n \t\t}\n \t}\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -530,7 +554,10 @@ static int threads_doit(struct nl_sock *sock, int cmd, int grace, int lease,\n \tstruct nl_cb *cb;\n \tint ret;\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -660,7 +687,10 @@ static int fetch_nfsd_versions(struct nl_sock *sock)\n \tstruct nl_cb *cb;\n \tint ret;\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -725,7 +755,10 @@ static int set_nfsd_versions(struct nl_sock *sock)\n \tstruct nl_cb *cb;\n \tint i, ret;\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -906,7 +939,10 @@ static int fetch_current_listeners(struct nl_sock *sock)\n \tstruct nl_cb *cb;\n \tint ret;\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -1151,7 +1187,10 @@ static int set_listeners(struct nl_sock *sock)\n \tstruct nl_cb *cb;\n \tint i, ret;\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -1272,7 +1311,10 @@ static int pool_mode_doit(struct nl_sock *sock, int cmd, const char *pool_mode)\n \tstruct nl_cb *cb;\n \tint ret;\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -1365,7 +1407,10 @@ static int lockd_config_doit(struct nl_sock *sock, int cmd, int grace, int tcppo\n \t\t\treturn 0;\n \t}\n \n-\tmsg = netlink_msg_alloc(sock, LOCKD_FAMILY_NAME);\n+\tif (!lockd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, lockd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-02-04",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about handling older kernels that don't support the min-threads setting by explaining that genetlink will reject unknown attributes by default and that silently ignoring it is less than ideal. The author described how they plan to handle this in userland, querying the kernel for the policy of the threads operation and determining the highest attribute index it supports. They also explained that for older kernels, they will fail if the --min-threads option is passed and log a warning and ignore the setting for autostart.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ben reported a problem when using new nfs-utils with an old kernel that\ndoesn't support the min-threads setting. While netlink is an extensible\nformat, genetlink (which we are using) will reject unknown attributes by\ndefault with -EINVAL.\n\nWe could fix this in the kernel by having it ignore unknown attributes,\nbut there is no way to fix old kernels and silently ignoring it is less\nthan ideal. By handling this in userland, we can properly error out when\nthe kernel doesn't support this attribute.\n\nWhen starting, have nfsdctl query the kernel for the \"policy\" of the\nthreads operation, and determine the highest attribute index it\nsupports.  For the \"threads\" command, have it fail if the --min-threads\noption is passed and the kernel doesn't support it. For \"autostart\", log\na warning and ignore the setting.\n\nFixes: 00e2e62b8998 (\"nfsdctl: add support for min-threads parameter\")\nReported-by: Ben Coddington <bcodding@hammerspace.com>\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n utils/nfsdctl/nfsdctl.c | 95 ++++++++++++++++++++++++++++++++++++++++++++++++-\n 1 file changed, 94 insertions(+), 1 deletion(-)\n\ndiff --git a/utils/nfsdctl/nfsdctl.c b/utils/nfsdctl/nfsdctl.c\nindex 86a4a944d4e131f1114ca358d81779de0a034872..4a3744a1c22e6beac7c039bded05fc087a121200 100644\n--- a/utils/nfsdctl/nfsdctl.c\n+++ b/utils/nfsdctl/nfsdctl.c\n@@ -52,6 +52,9 @@ static int lockd_nl_family;\n /* The index of the \"nfsd\" netlink family */\n static int nfsd_nl_family;\n \n+/* The highest attribute index supported by NFSD_CMD_THREADS_SET on this kernel */\n+int nfsd_threads_max_nlattr;\n+\n struct nfs_version {\n \tuint8_t\tmajor;\n \tuint8_t\tminor;\n@@ -480,6 +483,83 @@ static int nfsd_nl_family_setup(struct nl_sock *sock)\n \treturn nfsd_nl_family;\n }\n \n+static int getpolicy_handler(struct nl_msg *msg, void *arg)\n+{\n+\tstruct genlmsghdr *gnlh = nlmsg_data(nlmsg_hdr(msg));\n+\tstruct nlattr *attr;\n+\tint rem;\n+\n+\tnla_for_each_attr(attr, genlmsg_attrdata(gnlh, 0), genlmsg_attrlen(gnlh, 0), rem) {\n+\t\tstruct nlattr *a, *b;\n+\t\tint i, j, index;\n+\n+\t\tif (nla_type(attr) == CTRL_ATTR_POLICY) {\n+\t\t\tnla_for_each_nested(a, attr, i) {\n+\t\t\t\tnla_for_each_nested(b, a, j) {\n+\t\t\t\t\tint idx = nla_type(b);\n+\n+\t\t\t\t\tif (nfsd_threads_max_nlattr < idx)\n+\t\t\t\t\t\tnfsd_threads_max_nlattr = idx;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn NL_SKIP;\n+}\n+\n+static int query_nfsd_nl_policy(struct nl_sock *sock)\n+{\n+\tstruct genlmsghdr *ghdr;\n+\tstruct nlmsghdr *nlh;\n+\tstruct nl_msg *msg;\n+\tstruct nl_cb *cb;\n+\tint opt, ret, id;\n+\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, GENL_ID_CTRL);\n+\tif (!msg)\n+\t\treturn 1;\n+\n+\tnlh = nlmsg_hdr(msg);\n+\tnlh->nlmsg_flags |= NLM_F_DUMP;\n+\tghdr = nlmsg_data(nlh);\n+\tghdr->cmd = CTRL_CMD_GETPOLICY;\n+\n+\tcb = nl_cb_alloc(NL_CB_CUSTOM);\n+\tif (!cb) {\n+\t\txlog(L_ERROR, \"failed to allocate netlink callbacks\");\n+\t\tret = 1;\n+\t\tgoto out;\n+\t}\n+\n+\tnla_put_u16(msg, CTRL_ATTR_FAMILY_ID, nfsd_nl_family);\n+\tnla_put_u32(msg, CTRL_ATTR_OP, NFSD_CMD_THREADS_SET);\n+\n+\tret = nl_send_auto(sock, msg);\n+\tif (ret < 0)\n+\t\tgoto out_cb;\n+\n+\tret = 1;\n+\tnl_cb_err(cb, NL_CB_CUSTOM, error_handler, &ret);\n+\tnl_cb_set(cb, NL_CB_FINISH, NL_CB_CUSTOM, finish_handler, &ret);\n+\tnl_cb_set(cb, NL_CB_ACK, NL_CB_CUSTOM, ack_handler, &ret);\n+\tnl_cb_set(cb, NL_CB_VALID, NL_CB_CUSTOM, getpolicy_handler, NULL);\n+\n+\twhile (ret > 0)\n+\t\tnl_recvmsgs(sock, cb);\n+\tif (ret < 0) {\n+\t\txlog(L_ERROR, \"Error: %s\", strerror(-ret));\n+\t\tret = 1;\n+\t}\n+out_cb:\n+\tnl_cb_put(cb);\n+out:\n+\tnlmsg_free(msg);\n+\treturn ret;\n+}\n+\n static void status_usage(void)\n {\n \tprintf(\"Usage: %s status\\n\", taskname);\n@@ -639,6 +719,11 @@ static int threads_func(struct nl_sock *sock, int argc, char **argv)\n \t\t\tthreads_usage();\n \t\t\treturn 0;\n \t\tcase 'm':\n+\t\t\tif (nfsd_threads_max_nlattr < NFSD_A_SERVER_MIN_THREADS) {\n+\t\t\t\txlog(L_ERROR, \"This kernel does not support dynamic threading.\");\n+\t\t\t\treturn 1;\n+\t\t\t}\n+\n \t\t\terrno = 0;\n \t\t\tminthreads = strtoul(optarg, NULL, 0);\n \t\t\tif (minthreads == ULONG_MAX && errno != 0) {\n@@ -1743,7 +1828,12 @@ static int autostart_func(struct nl_sock *sock, int argc, char ** argv)\n \n \tlease = conf_get_num(\"nfsd\", \"lease-time\", 0);\n \tscope = conf_get_str(\"nfsd\", \"scope\");\n-\tminthreads = conf_get_num(\"nfsd\", \"min-threads\", 0);\n+\tminthreads = conf_get_num(\"nfsd\", \"min-threads\", -1);\n+\n+\tif (minthreads >= 0 && nfsd_threads_max_nlattr < NFSD_A_SERVER_MIN_THREADS) {\n+\t\txlog(L_WARNING, \"This kernel does not support dynamic threading. min-threads setting ignored.\");\n+\t\tminthreads = -1;\n+\t}\n \n \tret = threads_doit(sock, NFSD_CMD_THREADS_SET, grace, lease, pools,\n \t\t\t   threads, scope, minthreads);\n@@ -1936,6 +2026,9 @@ int main(int argc, char **argv)\n \t\treturn 1;\n \t}\n \n+\tif (query_nfsd_nl_policy(sock))\n+\t\treturn 1;\n+\n \tif (optind > argc) {\n \t\tusage();\n \t\treturn 1;\n\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-02-04",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "Author addressed a concern about error handling in the lockd_config_doit function, specifically that xlog messages were missing error codes and could be improved for clarity. The author made targeted changes to add error codes to these messages.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged feedback",
                "made targeted changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Signed-off-by: Jeff Layton <jlayton@kernel.org>\n---\n utils/nfsdctl/nfsdctl.c | 10 +++++-----\n 1 file changed, 5 insertions(+), 5 deletions(-)\n\ndiff --git a/utils/nfsdctl/nfsdctl.c b/utils/nfsdctl/nfsdctl.c\nindex 4a3744a1c22e6beac7c039bded05fc087a121200..2b01f705874a4a3cad04042f6dfad22a66a7536f 100644\n--- a/utils/nfsdctl/nfsdctl.c\n+++ b/utils/nfsdctl/nfsdctl.c\n@@ -1514,14 +1514,14 @@ static int lockd_config_doit(struct nl_sock *sock, int cmd, int grace, int tcppo\n \n \tcb = nl_cb_alloc(NL_CB_CUSTOM);\n \tif (!cb) {\n-\t\txlog(L_ERROR, \"failed to allocate netlink callbacks\\n\");\n+\t\txlog(L_ERROR, \"failed to allocate netlink callbacks\");\n \t\tret = 1;\n \t\tgoto out;\n \t}\n \n \tret = nl_send_auto(sock, msg);\n \tif (ret < 0) {\n-\t\txlog(L_ERROR, \"send failed (%d)!\\n\", ret);\n+\t\txlog(L_ERROR, \"send failed (%d)!\", ret);\n \t\tgoto out_cb;\n \t}\n \n@@ -1534,7 +1534,7 @@ static int lockd_config_doit(struct nl_sock *sock, int cmd, int grace, int tcppo\n \twhile (ret > 0)\n \t\tnl_recvmsgs(sock, cb);\n \tif (ret < 0) {\n-\t\txlog(L_ERROR, \"Error: %s\\n\", strerror(-ret));\n+\t\txlog(L_ERROR, \"Error: %s\", strerror(-ret));\n \t\tret = 1;\n \t}\n out_cb:\n@@ -1554,7 +1554,7 @@ static int get_service(const char *svc)\n \n \tret = getaddrinfo(NULL, svc, &hints, &res);\n \tif (ret) {\n-\t\txlog(L_ERROR, \"getaddrinfo of \\\"%s\\\" failed: %s\\n\",\n+\t\txlog(L_ERROR, \"getaddrinfo of \\\"%s\\\" failed: %s\",\n \t\t\tsvc, gai_strerror(ret));\n \t\treturn -1;\n \t}\n@@ -1575,7 +1575,7 @@ static int get_service(const char *svc)\n \t\t}\n \t\tbreak;\n \tdefault:\n-\t\txlog(L_ERROR, \"Bad address family: %d\\n\", res->ai_family);\n+\t\txlog(L_ERROR, \"Bad address family: %d\", res->ai_family);\n \t\tport = -1;\n \t}\n \tfreeaddrinfo(res);\n\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-02-04",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "Author is responding to a review request for the patch series, acknowledging that it's ready for merge and doesn't address any specific technical concerns.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging readiness for merge"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Steve, ping? Can we get this merged?\n\nThanks,\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Steve Dickson",
              "summary": "reviewer noted that the patch does not handle older kernels correctly, specifically mentioning that the kernel's min-threads attribute is not properly checked before being passed down to userland",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "lack of explicit handling for older kernels",
                "inadequate checking of kernel attributes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm on it... thanks for the ping!\n\nsteved.",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Benjamin Coddington",
              "summary": "Reviewer gave Reviewed-by and Tested-by tags, indicating they have reviewed the patch and tested it successfully.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by",
                "Tested-by"
              ],
              "raw_body": "I mean to send R-b and T-b on these - because yes, both were done!\n\nReviewed-by: Benjamin Coddington <bcodding@hammerspace.com>\nTested-by: Benjamin Coddington <bcodding@hammerspace.com>\n\nBen",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v3 1/4] open: new O_REGULAR flag support",
          "message_id": "19c990e9bf42cdc9c7b9bef5f4407fce30d35e54.camel@kernel.org",
          "url": "https://lore.kernel.org/all/19c990e9bf42cdc9c7b9bef5f4407fce30d35e54.camel@kernel.org/",
          "date": "2026-02-18T19:32:34Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "The author is addressing concerns about the O_REGULAR flag's behavior when used in combination with other flags like O_CREAT and O_DIRECTORY. They explained that either a regular file is created or if the path already exists, it is opened if it's a regular file; otherwise, -ENOTREG is returned. The author also clarified that -EINVAL is returned when O_REGULAR is combined with O_DIRECTORY because it doesn't make sense to open a path that is both a directory and a regular file.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This flag indicates the path should be opened if it's a regular file.\nThis is useful to write secure programs that want to avoid being tricked\ninto opening device nodes with special semantics while thinking they\noperate on regular files.\n\nA corresponding error code ENOTREG has been introduced. For example, if\nopen is called on path /dev/null with O_REGULAR in the flag param, it\nwill return -ENOTREG.\n\nWhen used in combination with O_CREAT, either the regular file is\ncreated, or if the path already exists, it is opened if it's a regular\nfile. Otherwise, -ENOTREG is returned.\n\n-EINVAL is returned when O_REGULAR is combined with O_DIRECTORY (not\npart of O_TMPFILE) because it doesn't make sense to open a path that\nis both a directory and a regular file.\n\nSigned-off-by: Dorjoy Chowdhury <dorjoychy111@gmail.com>\n---\n arch/alpha/include/uapi/asm/errno.h        | 2 ++\n arch/alpha/include/uapi/asm/fcntl.h        | 1 +\n arch/mips/include/uapi/asm/errno.h         | 2 ++\n arch/parisc/include/uapi/asm/errno.h       | 2 ++\n arch/parisc/include/uapi/asm/fcntl.h       | 1 +\n arch/sparc/include/uapi/asm/errno.h        | 2 ++\n arch/sparc/include/uapi/asm/fcntl.h        | 1 +\n fs/fcntl.c                                 | 2 +-\n fs/namei.c                                 | 6 ++++++\n fs/open.c                                  | 4 +++-\n include/linux/fcntl.h                      | 2 +-\n include/uapi/asm-generic/errno.h           | 2 ++\n include/uapi/asm-generic/fcntl.h           | 4 ++++\n tools/arch/alpha/include/uapi/asm/errno.h  | 2 ++\n tools/arch/mips/include/uapi/asm/errno.h   | 2 ++\n tools/arch/parisc/include/uapi/asm/errno.h | 2 ++\n tools/arch/sparc/include/uapi/asm/errno.h  | 2 ++\n tools/include/uapi/asm-generic/errno.h     | 2 ++\n 18 files changed, 38 insertions(+), 3 deletions(-)\n\ndiff --git a/arch/alpha/include/uapi/asm/errno.h b/arch/alpha/include/uapi/asm/errno.h\nindex 6791f6508632..8bbcaa9024f9 100644\n--- a/arch/alpha/include/uapi/asm/errno.h\n+++ b/arch/alpha/include/uapi/asm/errno.h\n@@ -127,4 +127,6 @@\n \n #define EHWPOISON\t139\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t140\t/* Not a regular file */\n+\n #endif\ndiff --git a/arch/alpha/include/uapi/asm/fcntl.h b/arch/alpha/include/uapi/asm/fcntl.h\nindex 50bdc8e8a271..4da5a64c23bd 100644\n--- a/arch/alpha/include/uapi/asm/fcntl.h\n+++ b/arch/alpha/include/uapi/asm/fcntl.h\n@@ -34,6 +34,7 @@\n \n #define O_PATH\t\t040000000\n #define __O_TMPFILE\t0100000000\n+#define O_REGULAR\t0200000000\n \n #define F_GETLK\t\t7\n #define F_SETLK\t\t8\ndiff --git a/arch/mips/include/uapi/asm/errno.h b/arch/mips/include/uapi/asm/errno.h\nindex c01ed91b1ef4..293c78777254 100644\n--- a/arch/mips/include/uapi/asm/errno.h\n+++ b/arch/mips/include/uapi/asm/errno.h\n@@ -126,6 +126,8 @@\n \n #define EHWPOISON\t168\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t169\t/* Not a regular file */\n+\n #define EDQUOT\t\t1133\t/* Quota exceeded */\n \n \ndiff --git a/arch/parisc/include/uapi/asm/errno.h b/arch/parisc/include/uapi/asm/errno.h\nindex 8cbc07c1903e..442917484f99 100644\n--- a/arch/parisc/include/uapi/asm/errno.h\n+++ b/arch/parisc/include/uapi/asm/errno.h\n@@ -124,4 +124,6 @@\n \n #define EHWPOISON\t257\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t258\t/* Not a regular file */\n+\n #endif\ndiff --git a/arch/parisc/include/uapi/asm/fcntl.h b/arch/parisc/include/uapi/asm/fcntl.h\nindex 03dee816cb13..0cc3320fe326 100644\n--- a/arch/parisc/include/uapi/asm/fcntl.h\n+++ b/arch/parisc/include/uapi/asm/fcntl.h\n@@ -19,6 +19,7 @@\n \n #define O_PATH\t\t020000000\n #define __O_TMPFILE\t040000000\n+#define O_REGULAR\t0100000000\n \n #define F_GETLK64\t8\n #define F_SETLK64\t9\ndiff --git a/arch/sparc/include/uapi/asm/errno.h b/arch/sparc/include/uapi/asm/errno.h\nindex 4a41e7835fd5..8dce0bfeab74 100644\n--- a/arch/sparc/include/uapi/asm/errno.h\n+++ b/arch/sparc/include/uapi/asm/errno.h\n@@ -117,4 +117,6 @@\n \n #define EHWPOISON\t135\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t136\t/* Not a regular file */\n+\n #endif\ndiff --git a/arch/sparc/include/uapi/asm/fcntl.h b/arch/sparc/include/uapi/asm/fcntl.h\nindex 67dae75e5274..a93d18d2c23e 100644\n--- a/arch/sparc/include/uapi/asm/fcntl.h\n+++ b/arch/sparc/include/uapi/asm/fcntl.h\n@@ -37,6 +37,7 @@\n \n #define O_PATH\t\t0x1000000\n #define __O_TMPFILE\t0x2000000\n+#define O_REGULAR\t0x4000000\n \n #define F_GETOWN\t5\t/*  for sockets. */\n #define F_SETOWN\t6\t/*  for sockets. */\ndiff --git a/fs/fcntl.c b/fs/fcntl.c\nindex f93dbca08435..62ab4ad2b6f5 100644\n--- a/fs/fcntl.c\n+++ b/fs/fcntl.c\n@@ -1169,7 +1169,7 @@ static int __init fcntl_init(void)\n \t * Exceptions: O_NONBLOCK is a two bit define on parisc; O_NDELAY\n \t * is defined as O_NONBLOCK on some platforms and not on others.\n \t */\n-\tBUILD_BUG_ON(20 - 1 /* for O_RDONLY being 0 */ !=\n+\tBUILD_BUG_ON(21 - 1 /* for O_RDONLY being 0 */ !=\n \t\tHWEIGHT32(\n \t\t\t(VALID_OPEN_FLAGS & ~(O_NONBLOCK | O_NDELAY)) |\n \t\t\t__FMODE_EXEC));\ndiff --git a/fs/namei.c b/fs/namei.c\nindex b28ecb699f32..f5504ae4b03c 100644\n--- a/fs/namei.c\n+++ b/fs/namei.c\n@@ -4616,6 +4616,10 @@ static int do_open(struct nameidata *nd,\n \t\tif (unlikely(error))\n \t\t\treturn error;\n \t}\n+\n+\tif ((open_flag & O_REGULAR) && !d_is_reg(nd->path.dentry))\n+\t\treturn -ENOTREG;\n+\n \tif ((nd->flags & LOOKUP_DIRECTORY) && !d_can_lookup(nd->path.dentry))\n \t\treturn -ENOTDIR;\n \n@@ -4765,6 +4769,8 @@ static int do_o_path(struct nameidata *nd, unsigned flags, struct file *file)\n \tstruct path path;\n \tint error = path_lookupat(nd, flags, &path);\n \tif (!error) {\n+\t\tif ((file->f_flags & O_REGULAR) && !d_is_reg(path.dentry))\n+\t\t\treturn -ENOTREG;\n \t\taudit_inode(nd->name, path.dentry, 0);\n \t\terror = vfs_open(&path, file);\n \t\tpath_put(&path);\ndiff --git a/fs/open.c b/fs/open.c\nindex 74c4c1462b3e..82153e21907e 100644\n--- a/fs/open.c\n+++ b/fs/open.c\n@@ -1173,7 +1173,7 @@ struct file *kernel_file_open(const struct path *path, int flags,\n EXPORT_SYMBOL_GPL(kernel_file_open);\n \n #define WILL_CREATE(flags)\t(flags & (O_CREAT | __O_TMPFILE))\n-#define O_PATH_FLAGS\t\t(O_DIRECTORY | O_NOFOLLOW | O_PATH | O_CLOEXEC)\n+#define O_PATH_FLAGS\t\t(O_DIRECTORY | O_NOFOLLOW | O_PATH | O_CLOEXEC | O_REGULAR)\n \n inline struct open_how build_open_how(int flags, umode_t mode)\n {\n@@ -1250,6 +1250,8 @@ inline int build_open_flags(const struct open_how *how, struct open_flags *op)\n \t\t\treturn -EINVAL;\n \t\tif (!(acc_mode & MAY_WRITE))\n \t\t\treturn -EINVAL;\n+\t} else if ((flags & O_DIRECTORY) && (flags & O_REGULAR)) {\n+\t\treturn -EINVAL;\n \t}\n \tif (flags & O_PATH) {\n \t\t/* O_PATH only permits certain other flags to be set. */\ndiff --git a/include/linux/fcntl.h b/include/linux/fcntl.h\nindex a332e79b3207..4fd07b0e0a17 100644\n--- a/include/linux/fcntl.h\n+++ b/include/linux/fcntl.h\n@@ -10,7 +10,7 @@\n \t(O_RDONLY | O_WRONLY | O_RDWR | O_CREAT | O_EXCL | O_NOCTTY | O_TRUNC | \\\n \t O_APPEND | O_NDELAY | O_NONBLOCK | __O_SYNC | O_DSYNC | \\\n \t FASYNC\t| O_DIRECT | O_LARGEFILE | O_DIRECTORY | O_NOFOLLOW | \\\n-\t O_NOATIME | O_CLOEXEC | O_PATH | __O_TMPFILE)\n+\t O_NOATIME | O_CLOEXEC | O_PATH | __O_TMPFILE | O_REGULAR)\n \n /* List of all valid flags for the how->resolve argument: */\n #define VALID_RESOLVE_FLAGS \\\ndiff --git a/include/uapi/asm-generic/errno.h b/include/uapi/asm-generic/errno.h\nindex 92e7ae493ee3..2216ab9aa32e 100644\n--- a/include/uapi/asm-generic/errno.h\n+++ b/include/uapi/asm-generic/errno.h\n@@ -122,4 +122,6 @@\n \n #define EHWPOISON\t133\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t134\t/* Not a regular file */\n+\n #endif\ndiff --git a/include/uapi/asm-generic/fcntl.h b/include/uapi/asm-generic/fcntl.h\nindex 613475285643..3468b352a575 100644\n--- a/include/uapi/asm-generic/fcntl.h\n+++ b/include/uapi/asm-generic/fcntl.h\n@@ -88,6 +88,10 @@\n #define __O_TMPFILE\t020000000\n #endif\n \n+#ifndef O_REGULAR\n+#define O_REGULAR\t040000000\n+#endif\n+\n /* a horrid kludge trying to make sure that this will fail on old kernels */\n #define O_TMPFILE (__O_TMPFILE | O_DIRECTORY)\n \ndiff --git a/tools/arch/alpha/include/uapi/asm/errno.h b/tools/arch/alpha/include/uapi/asm/errno.h\nindex 6791f6508632..8bbcaa9024f9 100644\n--- a/tools/arch/alpha/include/uapi/asm/errno.h\n+++ b/tools/arch/alpha/include/uapi/asm/errno.h\n@@ -127,4 +127,6 @@\n \n #define EHWPOISON\t139\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t140\t/* Not a regular file */\n+\n #endif\ndiff --git a/tools/arch/mips/include/uapi/asm/errno.h b/tools/arch/mips/include/uapi/asm/errno.h\nindex c01ed91b1ef4..293c78777254 100644\n--- a/tools/arch/mips/include/uapi/asm/errno.h\n+++ b/tools/arch/mips/include/uapi/asm/errno.h\n@@ -126,6 +126,8 @@\n \n #define EHWPOISON\t168\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t169\t/* Not a regular file */\n+\n #define EDQUOT\t\t1133\t/* Quota exceeded */\n \n \ndiff --git a/tools/arch/parisc/include/uapi/asm/errno.h b/tools/arch/parisc/include/uapi/asm/errno.h\nindex 8cbc07c1903e..442917484f99 100644\n--- a/tools/arch/parisc/include/uapi/asm/errno.h\n+++ b/tools/arch/parisc/include/uapi/asm/errno.h\n@@ -124,4 +124,6 @@\n \n #define EHWPOISON\t257\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t258\t/* Not a regular file */\n+\n #endif\ndiff --git a/tools/arch/sparc/include/uapi/asm/errno.h b/tools/arch/sparc/include/uapi/asm/errno.h\nindex 4a41e7835fd5..8dce0bfeab74 100644\n--- a/tools/arch/sparc/include/uapi/asm/errno.h\n+++ b/tools/arch/sparc/include/uapi/asm/errno.h\n@@ -117,4 +117,6 @@\n \n #define EHWPOISON\t135\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t136\t/* Not a regular file */\n+\n #endif\ndiff --git a/tools/include/uapi/asm-generic/errno.h b/tools/include/uapi/asm-generic/errno.h\nindex 92e7ae493ee3..2216ab9aa32e 100644\n--- a/tools/include/uapi/asm-generic/errno.h\n+++ b/tools/include/uapi/asm-generic/errno.h\n@@ -122,4 +122,6 @@\n \n #define EHWPOISON\t133\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t134\t/* Not a regular file */\n+\n #endif\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-01-27",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "The author is addressing a concern about the implementation of O_REGULAR flag support, specifically that it doesn't work as intended in certain cases. They are adding a new test case to cover this scenario and updating the existing code to handle it correctly.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Just a happy path test.\n\nSigned-off-by: Dorjoy Chowdhury <dorjoychy111@gmail.com>\n---\n .../testing/selftests/openat2/openat2_test.c  | 37 ++++++++++++++++++-\n 1 file changed, 36 insertions(+), 1 deletion(-)\n\ndiff --git a/tools/testing/selftests/openat2/openat2_test.c b/tools/testing/selftests/openat2/openat2_test.c\nindex 0e161ef9e9e4..011ebc9af4e5 100644\n--- a/tools/testing/selftests/openat2/openat2_test.c\n+++ b/tools/testing/selftests/openat2/openat2_test.c\n@@ -320,8 +320,42 @@ void test_openat2_flags(void)\n \t}\n }\n \n+#ifndef O_REGULAR\n+#define O_REGULAR 040000000\n+#endif\n+\n+#ifndef ENOTREG\n+#define ENOTREG 134\n+#endif\n+\n+void test_openat2_o_regular_flag(void)\n+{\n+\tif (!openat2_supported) {\n+\t\tksft_test_result_skip(\"Skipping %s as openat2 is not supported\\n\", __func__);\n+\t\treturn;\n+\t}\n+\n+\tstruct open_how how = {\n+\t\t.flags = O_REGULAR | O_RDONLY\n+\t};\n+\n+\tint fd = sys_openat2(AT_FDCWD, \"/dev/null\", &how);\n+\n+\tif (fd == ENOENT) {\n+\t\tksft_test_result_skip(\"Skipping %s as there is no /dev/null\\n\", __func__);\n+\t\treturn;\n+\t}\n+\n+\tif (fd != -ENOTREG) {\n+\t\tksft_test_result_fail(\"openat2 should return ENOTREG\\n\");\n+\t\treturn;\n+\t}\n+\n+\tksft_test_result_pass(\"%s succeeded\\n\", __func__);\n+}\n+\n #define NUM_TESTS (NUM_OPENAT2_STRUCT_VARIATIONS * NUM_OPENAT2_STRUCT_TESTS + \\\n-\t\t   NUM_OPENAT2_FLAG_TESTS)\n+\t\t   NUM_OPENAT2_FLAG_TESTS + 1)\n \n int main(int argc, char **argv)\n {\n@@ -330,6 +364,7 @@ int main(int argc, char **argv)\n \n \ttest_openat2_struct();\n \ttest_openat2_flags();\n+\ttest_openat2_o_regular_flag();\n \n \tif (ksft_get_fail_cnt() + ksft_get_error_cnt() > 0)\n \t\tksft_exit_fail();\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-01-27",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "The author is addressing a concern about the O_REGULAR flag's behavior when combined with other flags like O_CREAT, specifically in arch/sparc/include/uapi/asm/fcntl.h. They are following the convention in include/uapi/asm-generic/fcntl.h and other architecture-specific files by converting hex values to octal, which resolves the issue.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "resolved",
                "convention"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Following the convention in include/uapi/asm-generic/fcntl.h and other\narchitecture specific arch/*/include/uapi/asm/fcntl.h files.\n\nSigned-off-by: Dorjoy Chowdhury <dorjoychy111@gmail.com>\n---\n arch/sparc/include/uapi/asm/fcntl.h | 36 ++++++++++++++---------------\n 1 file changed, 18 insertions(+), 18 deletions(-)\n\ndiff --git a/arch/sparc/include/uapi/asm/fcntl.h b/arch/sparc/include/uapi/asm/fcntl.h\nindex a93d18d2c23e..3c16f1a66a6a 100644\n--- a/arch/sparc/include/uapi/asm/fcntl.h\n+++ b/arch/sparc/include/uapi/asm/fcntl.h\n@@ -2,23 +2,23 @@\n #ifndef _SPARC_FCNTL_H\n #define _SPARC_FCNTL_H\n \n-#define O_APPEND\t0x0008\n-#define FASYNC\t\t0x0040\t/* fcntl, for BSD compatibility */\n-#define O_CREAT\t\t0x0200\t/* not fcntl */\n-#define O_TRUNC\t\t0x0400\t/* not fcntl */\n-#define O_EXCL\t\t0x0800\t/* not fcntl */\n-#define O_DSYNC\t\t0x2000\t/* used to be O_SYNC, see below */\n-#define O_NONBLOCK\t0x4000\n+#define O_APPEND\t0000000010\n+#define FASYNC\t\t0000000100\t/* fcntl, for BSD compatibility */\n+#define O_CREAT\t\t0000001000\t/* not fcntl */\n+#define O_TRUNC\t\t0000002000\t/* not fcntl */\n+#define O_EXCL\t\t0000004000\t/* not fcntl */\n+#define O_DSYNC\t\t0000020000\t/* used to be O_SYNC, see below */\n+#define O_NONBLOCK\t0000040000\n #if defined(__sparc__) && defined(__arch64__)\n-#define O_NDELAY\t0x0004\n+#define O_NDELAY\t0000000004\n #else\n-#define O_NDELAY\t(0x0004 | O_NONBLOCK)\n+#define O_NDELAY\t(0000000004 | O_NONBLOCK)\n #endif\n-#define O_NOCTTY\t0x8000\t/* not fcntl */\n-#define O_LARGEFILE\t0x40000\n-#define O_DIRECT        0x100000 /* direct disk access hint */\n-#define O_NOATIME\t0x200000\n-#define O_CLOEXEC\t0x400000\n+#define O_NOCTTY\t0000100000\t/* not fcntl */\n+#define O_LARGEFILE\t0001000000\n+#define O_DIRECT        0004000000 /* direct disk access hint */\n+#define O_NOATIME\t0010000000\n+#define O_CLOEXEC\t0020000000\n /*\n  * Before Linux 2.6.33 only O_DSYNC semantics were implemented, but using\n  * the O_SYNC flag.  We continue to use the existing numerical value\n@@ -32,12 +32,12 @@\n  *\n  * Note: __O_SYNC must never be used directly.\n  */\n-#define __O_SYNC\t0x800000\n+#define __O_SYNC\t0040000000\n #define O_SYNC\t\t(__O_SYNC|O_DSYNC)\n \n-#define O_PATH\t\t0x1000000\n-#define __O_TMPFILE\t0x2000000\n-#define O_REGULAR\t0x4000000\n+#define O_PATH\t\t0100000000\n+#define __O_TMPFILE\t0200000000\n+#define O_REGULAR\t0400000000\n \n #define F_GETOWN\t5\t/*  for sockets. */\n #define F_SETOWN\t6\t/*  for sockets. */\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-01-27",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "The author addressed a concern about the O_REGULAR flag's behavior in combination with other flags like O_CREAT, explaining that they followed the convention in include/uapi/asm-generic/fcntl.h and other architecture-specific files by converting the hex values to octal. The author made changes to arch/mips/include/uapi/asm/fcntl.h to match this convention.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Following the convention in include/uapi/asm-generic/fcntl.h and other\narchitecture specific arch/*/include/uapi/asm/fcntl.h files.\n\nSigned-off-by: Dorjoy Chowdhury <dorjoychy111@gmail.com>\n---\n arch/mips/include/uapi/asm/fcntl.h | 22 +++++++++++-----------\n 1 file changed, 11 insertions(+), 11 deletions(-)\n\ndiff --git a/arch/mips/include/uapi/asm/fcntl.h b/arch/mips/include/uapi/asm/fcntl.h\nindex 0369a38e3d4f..6aa3f49df17e 100644\n--- a/arch/mips/include/uapi/asm/fcntl.h\n+++ b/arch/mips/include/uapi/asm/fcntl.h\n@@ -11,15 +11,15 @@\n \n #include <asm/sgidefs.h>\n \n-#define O_APPEND\t0x0008\n-#define O_DSYNC\t\t0x0010\t/* used to be O_SYNC, see below */\n-#define O_NONBLOCK\t0x0080\n-#define O_CREAT\t\t0x0100\t/* not fcntl */\n-#define O_TRUNC\t\t0x0200\t/* not fcntl */\n-#define O_EXCL\t\t0x0400\t/* not fcntl */\n-#define O_NOCTTY\t0x0800\t/* not fcntl */\n-#define FASYNC\t\t0x1000\t/* fcntl, for BSD compatibility */\n-#define O_LARGEFILE\t0x2000\t/* allow large file opens */\n+#define O_APPEND\t0000010\n+#define O_DSYNC\t\t0000020\t/* used to be O_SYNC, see below */\n+#define O_NONBLOCK\t0000200\n+#define O_CREAT\t\t0000400\t/* not fcntl */\n+#define O_TRUNC\t\t0001000\t/* not fcntl */\n+#define O_EXCL\t\t0002000\t/* not fcntl */\n+#define O_NOCTTY\t0004000\t/* not fcntl */\n+#define FASYNC\t\t0010000\t/* fcntl, for BSD compatibility */\n+#define O_LARGEFILE\t0020000\t/* allow large file opens */\n /*\n  * Before Linux 2.6.33 only O_DSYNC semantics were implemented, but using\n  * the O_SYNC flag.  We continue to use the existing numerical value\n@@ -33,9 +33,9 @@\n  *\n  * Note: __O_SYNC must never be used directly.\n  */\n-#define __O_SYNC\t0x4000\n+#define __O_SYNC\t0040000\n #define O_SYNC\t\t(__O_SYNC|O_DSYNC)\n-#define O_DIRECT\t0x8000\t/* direct disk access hint */\n+#define O_DIRECT\t0100000\t/* direct disk access hint */\n \n #define F_GETLK\t\t14\n #define F_SETLK\t\t6\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-01-27",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer suggested including information about the UAPI group's request and past discussion at LPC in the commit message, specifically recommending a link to the relevant kernel feature documentation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "As you mention in your cover letter, this is something that the UAPI\ngroup has asked for in the past[1] and was even discussed at a recent\nLPC (maybe LPC 2024?) -- thanks for the patch!\n\nIn the next posting of this patchset, I would suggest including this\ninformation in the *commit message* with a link (commit messages end up\nin the git history, cover letters are a little harder to search for when\ndoing \"git blame\").\n\n[1]: https://uapi-group.org/kernel-features/#ability-to-only-open-regular-files",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that using the O_REGULAR flag with O_PATH is unnecessary because O_PATH file descriptors do not risk opening device inodes, and suggested an alternative method of safely opening files by first checking if they are regular files using fstat(2) on an O_PATH descriptor.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It doesn't really make sense to use this flag with O_PATH -- O_PATH file\ndescriptors do not actually open the target inode and so there is no\nrisk to doing this.\n\nIn fact the method of safely opening files while avoiding device inodes\non Linux today is to open an O_PATH, then use fstat(2) to check whether\nit is a regular file, and then re-open the file descriptor through\n/proc/self/fd/$n. (This is totally race-safe.)\n\nMy main reason for pushing back against this it's really quite\npreferable to avoid expanding the set of O_* flags which work with\nO_PATH if they don't add much -- O_PATH has really unfortunate behaviour\nwith ignoring other flags and openat2(2) finally fixed that by blocking\nignored flag combinations.",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that legacy open(2)/openat(2) do not reject invalid flag arguments, making it difficult to add a new security-critical flag like O_REGULAR without breaking userspace or relying on caching issues; suggested using openat2(2)-only API and adding a rejection mask in struct open_how instead of burning another O_* flag bit.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Legacy open(2)/openat(2) do not reject invalid flag arguments, which\nmeans that you cannot trivially add a new security-critical flag to them\nfor two reasons:\n\n * You cannot easily rely on them because old kernels will not return\n   -EINVAL, meaning you cannot be sure that the flag is supported. You\n   can try to test-run it, but the operation needs to be a non-dangerous\n   operation to try (and caching this has its own issues, such as with\n   programs that apply seccomp filters later).\n\n   To be fair, since you reject O_DIRECTORY|O_REGULAR there is a\n   relatively easy way to detect this, but the caveats about problems\n   with caching still apply.\n\n * Old programs might pass garbage bits that have been ignored thus far,\n   which means that making them have meaning can break userspace. Given\n   the age of open(2) this is a very hard thing to guarantee and is one\n   of many reasons I wrote openat2(2) and finally added proper flag\n   checking.\n\n   This is something your patch doesn't deal with and I don't think can\n   be done in a satisfactory way (because the behaviour relies on more\n   than just the arguments).\n\nFor reference, this is why O_TMPFILE includes O_DIRECTORY and requires\nan O_ACCMODE with write bits -- this combination will fail on old\nkernels, which allows you to rely on it and also guarantees that no\nexisting older programs passed that flag combination already and\nhappened to work on older kernels. This kind of trick won't work for\nO_REGULAR, unfortunately.\n\nIn my view, this should be an openat2(2)-only API. In addition, I would\npropose that (instead of burning another O_* flag bit for this as a\nspecial-purpose API just for regular files) you could have a mask of\nwhich S_IFMT bits should be rejected as a new field in \"struct\nopen_how\". This would let you reject sockets or device inodes but permit\nFIFOs and regular files or directories, for instance. This could even be\ndone without a new O_* flag at all (the zero-value how->sfmt_mask would\nallow everything and so would work well with extensible structs), but we\ncould add an O2_* flag anyway.",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer suggested adding a description in the commit or cover letter explaining why a new errno is needed, specifically recommending ENXIO or EPROTONOSUPPORT/EPROTOTYPE as alternatives to ENOTREG.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "We are probably a little too reticent to add new errnos, but in this\ncase I think that there should be some description in the commit or\ncover letter about why a new errno is needed. ENXIO or\nEPROTONOSUPPORT/EPROTOTYPE is what you would typically use (yes, they\naren't a _perfect_ match but one of the common occurrences in syscall\ndesign is to read through errno(7) and figure out what errnos kind of\nfit what you need to express).\n\nThen to be fair, the existence of ENOTBLK, ENOTDIR, ENOTSOCK, etc. kind\nof justify the existence of ENOTREG too. Unfortunately, you won't be\nable to use ENOTREG if you go with my idea of having mask bits in\nopen_how... (And what errno should we use then...? Hm.)\n\n-- \nAleksa Sarai\nhttps://www.cyphar.com/",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that the patch lacks handling for ->atomic_open() operations, which most filesystems cannot support properly, and suggested adding patches to return -EINVAL if O_REGULAR is set.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "missing functionality",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "One thing this patch is missing is handling for ->atomic_open(). I\nimagine most of the filesystems that provide that op can't support\nO_REGULAR properly (maybe cifs can? idk). What you probably want to do\nis add in some patches that make all of the atomic_open operations in\nthe kernel return -EINVAL if O_REGULAR is set.\n\nThen, once the basic support is in, you or someone else can go back and\nimplement support for O_REGULAR where possible.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-27",
              "analysis_source": "llm"
            },
            {
              "author": "Mateusz Guzik",
              "summary": "Reviewer Mateusz Guzik expressed concerns about the O_ flag namespace being reused by openat2 syscall, which may lead to confusion and incorrect assumptions when passing flags to open and openat functions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "concerns",
                "confusion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "fwiw +1 from me, the O_ flag situation is already terrible even without\nthe validation woes.\n\nI find it most unfortunate the openat2 syscall reuses the O_ namespace.\nFor my taste it would be best closed for business, with all new flag\nadditions using a different space.\n\nI can easily see people passing O_WHATEVER to open and openat by blindly\nassuming they are supported just based on the name.\n\nthat's a side mini-rant, too late to do anything here now",
              "reply_to": "Aleksa Sarai",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Mateusz Guzik",
              "summary": "Reviewer Mateusz Guzik noted that the current implementation of O_REGULAR flag using bitmasks has overlapping bits with existing file type definitions (S_IFBLK and S_IFDIR), making it impossible to select specific types, and suggested providing new defines with unique bits or modifying semantics to only allow desired bits.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "overlapping_bits",
                "semantics_mismatch"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this works because the vars have overlapping bits:\n  #define S_IFBLK  0060000\n  #define S_IFDIR  0040000\n\nSo you very much can't select what you want off of a bitmask.\n\nAt best the field could be used to select the one type you are fine with.\n\nIf one was to pursue the idea, some other defines with unique bits would\nneed to be provided. But even then, semantics should be to only *allow*\nthe bits you are fine with and reject the rest.\n\nBut I'm not at all confident this is worth any effort -- with\nO_DIRECTORY already being there and O_REGULAR proposed, is there a use\ncase which wants something else?",
              "reply_to": "Aleksa Sarai",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Mateusz Guzik",
              "summary": "Reviewer Mateusz Guzik suggested adding specific error numbers to indicate the type of file that cannot be opened with O_REGULAR flag, such as EPIPE for pipes or EBADTYPE for other types",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The most useful behavior would indicate what was found (e.g., a pipe).\n\nThe easiest way to do it would create errnos for all types (EISDIR\nalready exists for one), but I can't seriously propose that.\n\nGoing the other way, EBADTYPE or something else reusable would be my\nidea.",
              "reply_to": "Aleksa Sarai",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author responded to feedback about the behavior of O_REGULAR flag in atomic_open by asking for clarification on where and what needs to be fixed, indicating uncertainty about the expected behavior when using O_CREAT | O_REGULAR.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thank you for the feedback. I don't quite understand what I need to\nfix. I thought open system calls always create regular files, so\natomic_open probably always creates regular files? Can you please give\nme some more details as to where I need to fix this and what the\nactual bug here is that is related to atomic_open?  I think I had done\nsome normal testing and when using O_CREAT | O_REGULAR, if the file\ndoesn't exist, the file gets created and the file that gets created is\na regular file, so it probably makes sense? Or should the behavior be\nthat if file doesn't exist, -EINVAL is returned and if file exists it\nis opened if regular, otherwise -ENOTREG is returned?\n\nRegards,\nDorjoy",
              "reply_to": "Jeff Layton",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that atomic_open() is a separate codepath for network filesystems, which doesn't call the normal open codepath and may ignore O_REGULAR flag, suggesting an interim step to return -EINVAL in atomic_open operations when O_DIRECTORY is set.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "interim solution"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "atomic_open() is a combination lookup+open for when the dentry isn't\npresent in the dcache. The normal open codepath that you're patching\ndoes not get called in this case when ->atomic_open is set for the\nfilesystem. It's mostly used by network filesystems that need to\noptimize away the lookup since it's wasted round trip, and is often\nracy anyway. Your patchset doesn't address those filesystems. They will\nlikely end up ignoring O_REGULAR in that case, which is not what you\nwant.\n\nWhat I was suggesting is that, as an interim step, you find all of the\natomic_open operations in the kernel (there are maybe a dozen or so),\nand just make them return -EINVAL if someone sets O_DIRECTORY. Later,\nyou or someone else can then go back and do a proper implementation of\nO_REGULAR handling on those filesystems, at least on the ones where\nit's possible. You will probably also need to similarly patch the\nopen() routines for those filesystems too. Otherwise you'll get\ninconsistent behavior vs. when the dentry is in the cache.\n\nOne note: I think NFS probably can support O_DIRECTORY, since its OPEN\ncall only works on files. We'll need to change how we handle errors\nfrom the server when it's set though.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "The author is clarifying the intention behind returning -EINVAL when both O_REGULAR and O_DIRECTORY flags are set, pointing out that they already handle this case in the build_open_flags function and questioning whether additional checks are needed in atomic_open implementations.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "questioning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sorry, I am just trying to fully understand this. Do you mean to\nreturn -EINVAL from all atomic_open implementations in the kernel if\nboth O_REGULAR and O_DIRECTORY are set (or just only if O_REGULAR is\nset, we need to return -EINVAL)? I am already returning -EINVAL when\nboth these are set from the build_open_flags function, so that should\nalready handle the cases, right? I think after atomic_open get called,\nall code paths eventually go through the do_open function where I have\nthis check \"if ((open_flag & O_REGULAR) && !d_is_reg(nd->path.dentry))\nreturn -ENOTREG\". This is right before if ((nd->flags &\nLOOKUP_DIRECTORY) && !d_can_lookup(nd->path.dentry)) return -ENOTDIR;\nwhich I had initially followed. So should I just return -EINVAL from\nthe atomic_open functions too if both O_REGULAR and O_DIRECTORY are\nset? Sorry if I am misunderstanding this.\n\nRegards,\nDorjoy",
              "reply_to": "Jeff Layton",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author is considering renaming the O_REGULAR flag to O2_REGULAR, creating a new flag set for openat2 flags, and removing the sfmt_mask handling code.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "considering alternative approach",
                "open to suggestions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Good discussion. So should I just rename the O_REGULAR to O2_REGULAR\nand create a VALID_OPENAT2_FLAGS and no need to do how->sfmt_mask\nstuff?",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author acknowledges that ENOTREG might be an acceptable error code, but no clear resolution or agreement on the specific error code to use.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledges feedback",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Good point. Maybe ENOTREG is acceptable too?\n\nRegards,\nDorjoy",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer suggested that new flag additions, like O_REGULAR, should follow the naming convention of being prefixed with 'O2_' or 'OEXT_', instead of just 'O_'.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "convention",
                "naming"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "We don't have any openat2(2)-only O_* flags yet, I agree that new flag\nadditions (except for very rare cases where you can make them backward\ncompatible -- such as a hypothetical O_EMPTYPATH) should be O2_* or\nOEXT_* or something.",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that the new O_REGULAR flag should be placed in the 64-bit flag space, exclusive to openat2(2), to avoid confusion and potential conflicts with existing flags.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yeah, if we don't do that it'll lead to confusion. openat2(2) has\nexclusive rights to the 64-bit flag bits so we could start with those\nbefore we need to cross with the O_* flag space.",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that the O_REGULAR flag has an unfortunate overlap with block/char devices and directories, suggesting filtering on S_IFCHR to block both types of devices",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "overlap",
                "unfortunate"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Well, you can filter on S_IFCHR if you want to block both block/char\ndevices, but yeah the overlap is quite unfortunate... (That would also\nmean blocking directories would also block S_IFBLK -- I remembered there\nwas an overlap but I forgot it coincided with S_IFDIR... Damn wacky\nAPIs.)",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that the current flags (O_REGULAR, O_DIRECTORY, and O_NOFOLLOW) do not cover all possible use cases, specifically allowing FIFOs, regular files, and directories while blocking everything else.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There's also O_NOFOLLOW in a similar vein.\n\nI can see someone wanting to permit FIFOs, regular files, and\ndirectories being fine but blocking everything else. None of O_REGULAR,\nO_DIRECTORY, nor O_NOFOLLOW provide that.",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that using three to five error numbers (errnos) for the O_REGULAR flag would be wasteful, given that there are already four existing errnos that are logical inverses.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "wasteful use of resources"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It might be kinda neat from a potential re-use perspective in other APIs\nbut yeah it would be quite wasteful to burn 3-5 errnos for this when we\nalready have ~4 that are logical inverses.",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer suggested reusing the existing ENOTREG error code and wording its message to make it applicable for other places in the kernel.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested improvement"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think that would be reasonable and if you word the error message\ncarefully you can even see it being a fairly generic errno for other\nplaces to use.\n\n-- \nAleksa Sarai\nhttps://www.cyphar.com/",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Christian Brauner",
              "summary": "Reviewer Christian Brauner expressed concerns about adding O_REGULAR flag support outside of openat2(), suggesting it should be prefixed with OPENAT2_ and not made exclusive with O_DIRECTORY, and instead proposed using EFTYPE as the errno code.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yeah, we shouldn't add support for this outside of openat2(). We also\nshouldn't call this OEXT_* or O2_*. Let's just follow the pattern where\nwe prefix the flag space with the name of the system call\nOPENAT2_REGULAR.\n\nThere's also no real need to make O_DIRECTORY exclusive with\nOPENAT2_REGULAR. Callers could legimitately want to open a directory or\nregular file but not anything else. If someone wants to operate on a\nwhole filesystem tree but only wants to interact with regular files and\ndirectories and ignore devices, sockets, fifos etc it's very handy to\njust be able to set both in flags.\n\nFrankly, this shouldn't be a flag at all but we already have O_DIRECTORY\nin there so no need to move this into a new field.\n\nAdd EFTYPE as the errno code. Some of the bsds including macos already\nhave that.",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Christian Brauner",
              "summary": "Reviewer Christian Brauner noted that the patch's intention to block O_REGULAR for ->atomic_open:: is problematic because it may break filesystems that handle this case correctly, and suggested bypassing ->atomic_open:: for OPENAT2_REGULAR without O_CREAT or implementing a protocol extension.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "concern about potential breakage"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "So I think you're proposing two separate things or there's a typo:\n\n(1) blocking O_DIRECTORY for ->atomic_open::\n(2) blocking O_REGULAR for ->atomic_open::\n\nThe (1) point implies that O_DIRECTORY currently doesn't work correctly\nwith atomic open for all filesystems.\n\nEver since 43b450632676 (\"open: return EINVAL for O_DIRECTORY |\nO_CREAT\") O_DIRECTORY with O_CREAT is blocked. It was accidently allowed\nand completely broken before that.\n\nFor O_DIRECTORY without O_CREAT the kernel will pass that down through\n->atomic_open:: to the filesystem.\n\nThe worry that I see is that a network filesystem via ->atomic_open::\nsomehow already called open on the server side on something that wasn't\na directory. At that point the damage such as side-effects from device\nopening is already done.\n                                    \nBut I suspect that every filesystem implementing ->atomic_open:: just\ndoes finish_no_open() and punts to the VFS for the actual open. And the\nVFS will catch it in do_open() for it actually opens the file. So the\nonly real worry for O_DIRECTORY I see is that there's an fs that handles\nit wrong.\n\nFor (2) it is problematic as there surely are filesystems with\n->atomic_open:: that do handle the ~O_CREAT case and return with\nFMODE_OPENED. So that'll be problematic if the intention is to not\ntrigger an actual open on a non-regular file such as a\ndevice/socket/fifo etc. before the VFS had a chance to validate what's\ngoing on.\n\nSo I'm not excited about having this 70% working and punting on\n->atomic_open:: waiting for someone to fix this. One option would be to\nbypass ->atomic_open:: for OPENAT2_REGULAR without O_CREAT and fallback\nto racy and pricy lookup + open for now. How problematic would that be?\nIf possible I'd prefer this a lot over merging something that works\nhalf-way.\n\nI guess to make that really work you'd need some protocol extension?",
              "reply_to": "Jeff Layton",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "reviewer noted that sending an immediate close would have unintended consequences",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEUTRAL",
                "side effects"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Exactly. I guess you could send an immediate close, but that's not\nwithout side effects.",
              "reply_to": "Christian Brauner",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that O_REGULAR flag support may need to be handled differently in various file systems, specifically mentioning NFS and cifs as potential issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "potential issues"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "For NFS, I think we're OK. The OPEN call on NFSv4 only works for\nregular files, so it should be able to handle O_REGULAR. We just need\nto rejigger the error handling when it's set (just return an error\ninstead of doing the open of a directory or whatever it is).\n\nThe others (at a quick glance):\n\ncifs: I don't see a way to specify an O_REGULAR equivalent to the\nSMB2_CREATE call and it looks like it can create directories. Maybe\nSteveF (cc'ed) knows if this is possible?\n\nceph: I think CEPH_MDS_OP_OPEN might only work for files, in which case\nO_REGULAR can probably be supported similarly to NFS.\n\nfuse: probably ok? Does finish_no_open() in most cases. May depend on\nthe userland driver though.\n\ngfs2: is ok, it just does finish_no_open() in most cases anyway\n\nvboxsf: does finish_no_open on non-creates, so you could probably just\npunt to that if O_REGULAR is set.\n\nSo, it's probably possible to do this across the board. I'm not sure\nabout cifs though.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Christian Brauner",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton pointed out that SMB2 has an existing flag (FILE_NON_DIRECTORY_FILE) that enforces the same behavior as O_REGULAR, and suggested considering its implications on file types like directories, named pipes, and printer files.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "considering existing SMB2 flag",
                "implications on file types"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "SMB2 does have this flag:\n\nFILE_NON_DIRECTORY_FILE 0x00000040\n\nIf the name of the file being created or opened matches with an\nexisting directory file, the server MUST fail the request with\nSTATUS_FILE_IS_A_DIRECTORY. This flag MUST NOT be used with\nFILE_DIRECTORY_FILE or the server MUST fail the request with\nSTATUS_INVALID_PARAMETER.\n\nSMB2 can also present named pipes and printer files. Not sure if there\nis a way to exclude those with this.",
              "reply_to": "",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that the O_REGULAR flag does not prevent opening directories, and requested a review of the MDS (Memory Description Structure) code to determine if it can be done in a non-racy way.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "Actually I'm wrong here. That op can open a directory. We'll need\nsomeone to look at the MDS code and tell us whether this can be done in\na non-racy way.",
              "reply_to": "",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that O_DIRECTORY flag handling is not broken on other file systems, specifically mentioning NFS as an example, and expressed no concerns about the patch",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no specific technical issue raised"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It looks like NFS at least handles O_DIRECTORY properly. I don't have\nany reason to believe that O_DIRECTORY handling is broken on the\nothers.",
              "reply_to": "Christian Brauner",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author agrees that OPENAT2_REGULAR is a better option than OEXT_*/O2_* options, but asks if it should be defined outside the 32-bit range to avoid conflicts with existing flags and only need to be defined in include/uapi/asm-generic/fcntl.h.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question",
                "request for feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for the feedback. I agree that OPENAT2_REGULAR is better than\nthe other OEXT_*/O2_* options. Right now in the patch, the O_REGULAR\ntook the next slot in all the fcntl files. Should OPENAT2_REGULAR be a\nbit outside of the 32bits? That way it won't take any of the regular\nO_* bits and we would only need to define it in\ninclude/uapi/asm-generic/fcntl.h file and not need it in\narch/*/fcntl.h files. What do you think?",
              "reply_to": "Christian Brauner",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author acknowledged a need to address Christian Brauner's feedback about the O_REGULAR flag's interaction with other flags like O_CREAT, agreeing to fix this in v4.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a need for revision",
                "agreed to fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Great suggestion. Will fixup in v4 submission.\n\nRegards,\nDorjoy",
              "reply_to": "Christian Brauner",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that using a bitmask of S_IFMT to reject opening certain file types would be similar in spirit to O_NOFOLLOW, but pointed out that S_IFBLK is a combination of S_IFCHR and S_IFDIR, which complicates the idea.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEUTRAL"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You could even say O_NOFOLLOW is kinda like that too.\n\nIn my other mail I proposed a bitmask of S_IFMT to reject opening (which\nwould let you allow FIFOs and regular files but block devices, etc).\nUnfortunately I forgot that S_IFBLK is S_IFCHR|S_IFDIR. This isn't fatal\nto the idea but it kinda sucks. Grr.",
              "reply_to": "Christian Brauner",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author agrees that introducing a new how->sfmt_allow field with separate bits for file types is a good suggestion, but expresses concern about the complexity of using different bits as an API.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledges feedback",
                "expresses concern"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It is a good suggestion. I guess we can still introduce a new\nhow->sfmt_allow field and have new bits (instead of keeping in sync\nwith S_IF* ones) that allow types and just start with regular file\nallow bit for now, right? But I guess it would be cumbersome for users\nas an api to use different bits?\n\nRegards,\nDorjoy",
              "reply_to": "Aleksa Sarai",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author is seeking clarification on which code path the O_REGULAR flag's error handling should occur in, specifically asking if it's inode_operations.atomic_open or file_operations.open.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thank you for the details. Do you remember which codepath this is? Is\nthis the inode_operations.atomic_open codepath or file_operations.open\ncodepath? I am a bit confused also about where exactly the error\nhandling that needs to be done.",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "The author is clarifying that the atomic_open code paths are being addressed, specifically mentioning finish_no_open in inode_operations.atomic_open.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "These are all inode_operations.atomic_open code paths, right? Because\nyou mentioned finish_no_open and I see finish_no_open in the\natomic_open code paths as opposed to file_operations.open code paths.\n\nRegards,\nDorjoy",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton noted that nfs_atomic_open() might not work efficiently with the new O_REGULAR flag, but after reevaluating, he thinks it will work without changes; however, he suggested an optimization opportunity where if open_context() returns -EISDIR or similar, the function can immediately return an error when O_REGULAR is set",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested optimization",
                "potential inefficiency"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I was thinking nfs_atomic_open().\n\nLooking now, I think it might actually work OK without changes. It just\nmight not be terribly efficient about it.\n\nIf the open_context() call returns -EISDIR or similar, then you really\ndon't need to do the call to nfs_lookup() and the like. You can just\nreturn an immediate error when O_REGULAR is set since you know it's not\nsuitable to be opened.",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that while most cases will fall back to finish_no_open(), there may be opportunities for optimization, similar to how he mentioned optimizing NFS, and requested a deeper dive and testing of these cases.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "optimization",
                "deeper dive"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Note that this was just a cursory look. Someone will need to do a\ndeeper dive and test these cases.\n\nI think most will end up working ok, since most fall back to doing a\nfinish_no_open(). There may be opportunities to optimize some of these\ncases though (similarly to how I mentioned with NFS).\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "The author acknowledges that O_REGULAR needs to be handled as an unknown flag in addition to NFS, and is considering this for future revisions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Right. And I guess we don't need to worry about O_REGULAR being an\nunknown flag when it gets sent to the server (not only for NFS / but\nothers as well)?",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author acknowledged that O_REGULAR flag handling is needed in additional filesystems and asked if modifying corresponding file_operations.open code paths is also required.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "asked for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I can try to look into these and see if I can implement handling for\nO_REGULAR flag for these filesystems in the atomic_open code paths.\nThanks for the details.\n\nWill I need to modify the corresponding file_operations.open code\npaths as well along with atomic_open code paths?\n\nRegards,\nDorjoy",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton noted that POSIX flags like O_REGULAR should not be sent in NFSv4 requests, as they have their own set of flags and O_REGULAR is already implied in an OPEN call on the wire.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "flag",
                "NFSv4"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You shouldn't. We don't send POSIX flags in NFSv4 requests. It has its\nown set of flags. In the case of NFSv4, O_REGULAR is already implied in\nan OPEN call on the wire. OPEN only operates on regular files.",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that when a valid dentry exists, the O_REGULAR flag can be checked without calling ->open(), and suggested re-evaluating the code to avoid unnecessary calls.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Probably not.\n\nThe main thing to keep in mind is that ->open is used when we already\nhave a dentry for the target of the open. ->atomic_open is used when we\ndon't have one yet or the one we have has failed revalidation.\n\nIf you have a valid dentry, then you should be able to satisfy the\nO_REGULAR check without having to call into ->open at all.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author acknowledged Jeff's feedback and expressed gratitude, indicating no further action is needed.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledgment",
                "gratitude"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Understood. Thanks!\n\nRegards,\nDorjoy",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v3 1/4] open: new O_REGULAR flag support",
          "message_id": "67ba3b2b52f7dd1f46e5aa75dd9ea0c75f178374.camel@kernel.org",
          "url": "https://lore.kernel.org/all/67ba3b2b52f7dd1f46e5aa75dd9ea0c75f178374.camel@kernel.org/",
          "date": "2026-02-18T19:01:44Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "The author addressed reviewer feedback about the O_REGULAR flag's behavior when combined with other flags like O_CREAT and O_DIRECTORY. The author explained that -ENOTREG is returned in this case, as it doesn't make sense to open a path that is both a directory and a regular file.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "neutral explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This flag indicates the path should be opened if it's a regular file.\nThis is useful to write secure programs that want to avoid being tricked\ninto opening device nodes with special semantics while thinking they\noperate on regular files.\n\nA corresponding error code ENOTREG has been introduced. For example, if\nopen is called on path /dev/null with O_REGULAR in the flag param, it\nwill return -ENOTREG.\n\nWhen used in combination with O_CREAT, either the regular file is\ncreated, or if the path already exists, it is opened if it's a regular\nfile. Otherwise, -ENOTREG is returned.\n\n-EINVAL is returned when O_REGULAR is combined with O_DIRECTORY (not\npart of O_TMPFILE) because it doesn't make sense to open a path that\nis both a directory and a regular file.\n\nSigned-off-by: Dorjoy Chowdhury <dorjoychy111@gmail.com>\n---\n arch/alpha/include/uapi/asm/errno.h        | 2 ++\n arch/alpha/include/uapi/asm/fcntl.h        | 1 +\n arch/mips/include/uapi/asm/errno.h         | 2 ++\n arch/parisc/include/uapi/asm/errno.h       | 2 ++\n arch/parisc/include/uapi/asm/fcntl.h       | 1 +\n arch/sparc/include/uapi/asm/errno.h        | 2 ++\n arch/sparc/include/uapi/asm/fcntl.h        | 1 +\n fs/fcntl.c                                 | 2 +-\n fs/namei.c                                 | 6 ++++++\n fs/open.c                                  | 4 +++-\n include/linux/fcntl.h                      | 2 +-\n include/uapi/asm-generic/errno.h           | 2 ++\n include/uapi/asm-generic/fcntl.h           | 4 ++++\n tools/arch/alpha/include/uapi/asm/errno.h  | 2 ++\n tools/arch/mips/include/uapi/asm/errno.h   | 2 ++\n tools/arch/parisc/include/uapi/asm/errno.h | 2 ++\n tools/arch/sparc/include/uapi/asm/errno.h  | 2 ++\n tools/include/uapi/asm-generic/errno.h     | 2 ++\n 18 files changed, 38 insertions(+), 3 deletions(-)\n\ndiff --git a/arch/alpha/include/uapi/asm/errno.h b/arch/alpha/include/uapi/asm/errno.h\nindex 6791f6508632..8bbcaa9024f9 100644\n--- a/arch/alpha/include/uapi/asm/errno.h\n+++ b/arch/alpha/include/uapi/asm/errno.h\n@@ -127,4 +127,6 @@\n \n #define EHWPOISON\t139\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t140\t/* Not a regular file */\n+\n #endif\ndiff --git a/arch/alpha/include/uapi/asm/fcntl.h b/arch/alpha/include/uapi/asm/fcntl.h\nindex 50bdc8e8a271..4da5a64c23bd 100644\n--- a/arch/alpha/include/uapi/asm/fcntl.h\n+++ b/arch/alpha/include/uapi/asm/fcntl.h\n@@ -34,6 +34,7 @@\n \n #define O_PATH\t\t040000000\n #define __O_TMPFILE\t0100000000\n+#define O_REGULAR\t0200000000\n \n #define F_GETLK\t\t7\n #define F_SETLK\t\t8\ndiff --git a/arch/mips/include/uapi/asm/errno.h b/arch/mips/include/uapi/asm/errno.h\nindex c01ed91b1ef4..293c78777254 100644\n--- a/arch/mips/include/uapi/asm/errno.h\n+++ b/arch/mips/include/uapi/asm/errno.h\n@@ -126,6 +126,8 @@\n \n #define EHWPOISON\t168\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t169\t/* Not a regular file */\n+\n #define EDQUOT\t\t1133\t/* Quota exceeded */\n \n \ndiff --git a/arch/parisc/include/uapi/asm/errno.h b/arch/parisc/include/uapi/asm/errno.h\nindex 8cbc07c1903e..442917484f99 100644\n--- a/arch/parisc/include/uapi/asm/errno.h\n+++ b/arch/parisc/include/uapi/asm/errno.h\n@@ -124,4 +124,6 @@\n \n #define EHWPOISON\t257\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t258\t/* Not a regular file */\n+\n #endif\ndiff --git a/arch/parisc/include/uapi/asm/fcntl.h b/arch/parisc/include/uapi/asm/fcntl.h\nindex 03dee816cb13..0cc3320fe326 100644\n--- a/arch/parisc/include/uapi/asm/fcntl.h\n+++ b/arch/parisc/include/uapi/asm/fcntl.h\n@@ -19,6 +19,7 @@\n \n #define O_PATH\t\t020000000\n #define __O_TMPFILE\t040000000\n+#define O_REGULAR\t0100000000\n \n #define F_GETLK64\t8\n #define F_SETLK64\t9\ndiff --git a/arch/sparc/include/uapi/asm/errno.h b/arch/sparc/include/uapi/asm/errno.h\nindex 4a41e7835fd5..8dce0bfeab74 100644\n--- a/arch/sparc/include/uapi/asm/errno.h\n+++ b/arch/sparc/include/uapi/asm/errno.h\n@@ -117,4 +117,6 @@\n \n #define EHWPOISON\t135\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t136\t/* Not a regular file */\n+\n #endif\ndiff --git a/arch/sparc/include/uapi/asm/fcntl.h b/arch/sparc/include/uapi/asm/fcntl.h\nindex 67dae75e5274..a93d18d2c23e 100644\n--- a/arch/sparc/include/uapi/asm/fcntl.h\n+++ b/arch/sparc/include/uapi/asm/fcntl.h\n@@ -37,6 +37,7 @@\n \n #define O_PATH\t\t0x1000000\n #define __O_TMPFILE\t0x2000000\n+#define O_REGULAR\t0x4000000\n \n #define F_GETOWN\t5\t/*  for sockets. */\n #define F_SETOWN\t6\t/*  for sockets. */\ndiff --git a/fs/fcntl.c b/fs/fcntl.c\nindex f93dbca08435..62ab4ad2b6f5 100644\n--- a/fs/fcntl.c\n+++ b/fs/fcntl.c\n@@ -1169,7 +1169,7 @@ static int __init fcntl_init(void)\n \t * Exceptions: O_NONBLOCK is a two bit define on parisc; O_NDELAY\n \t * is defined as O_NONBLOCK on some platforms and not on others.\n \t */\n-\tBUILD_BUG_ON(20 - 1 /* for O_RDONLY being 0 */ !=\n+\tBUILD_BUG_ON(21 - 1 /* for O_RDONLY being 0 */ !=\n \t\tHWEIGHT32(\n \t\t\t(VALID_OPEN_FLAGS & ~(O_NONBLOCK | O_NDELAY)) |\n \t\t\t__FMODE_EXEC));\ndiff --git a/fs/namei.c b/fs/namei.c\nindex b28ecb699f32..f5504ae4b03c 100644\n--- a/fs/namei.c\n+++ b/fs/namei.c\n@@ -4616,6 +4616,10 @@ static int do_open(struct nameidata *nd,\n \t\tif (unlikely(error))\n \t\t\treturn error;\n \t}\n+\n+\tif ((open_flag & O_REGULAR) && !d_is_reg(nd->path.dentry))\n+\t\treturn -ENOTREG;\n+\n \tif ((nd->flags & LOOKUP_DIRECTORY) && !d_can_lookup(nd->path.dentry))\n \t\treturn -ENOTDIR;\n \n@@ -4765,6 +4769,8 @@ static int do_o_path(struct nameidata *nd, unsigned flags, struct file *file)\n \tstruct path path;\n \tint error = path_lookupat(nd, flags, &path);\n \tif (!error) {\n+\t\tif ((file->f_flags & O_REGULAR) && !d_is_reg(path.dentry))\n+\t\t\treturn -ENOTREG;\n \t\taudit_inode(nd->name, path.dentry, 0);\n \t\terror = vfs_open(&path, file);\n \t\tpath_put(&path);\ndiff --git a/fs/open.c b/fs/open.c\nindex 74c4c1462b3e..82153e21907e 100644\n--- a/fs/open.c\n+++ b/fs/open.c\n@@ -1173,7 +1173,7 @@ struct file *kernel_file_open(const struct path *path, int flags,\n EXPORT_SYMBOL_GPL(kernel_file_open);\n \n #define WILL_CREATE(flags)\t(flags & (O_CREAT | __O_TMPFILE))\n-#define O_PATH_FLAGS\t\t(O_DIRECTORY | O_NOFOLLOW | O_PATH | O_CLOEXEC)\n+#define O_PATH_FLAGS\t\t(O_DIRECTORY | O_NOFOLLOW | O_PATH | O_CLOEXEC | O_REGULAR)\n \n inline struct open_how build_open_how(int flags, umode_t mode)\n {\n@@ -1250,6 +1250,8 @@ inline int build_open_flags(const struct open_how *how, struct open_flags *op)\n \t\t\treturn -EINVAL;\n \t\tif (!(acc_mode & MAY_WRITE))\n \t\t\treturn -EINVAL;\n+\t} else if ((flags & O_DIRECTORY) && (flags & O_REGULAR)) {\n+\t\treturn -EINVAL;\n \t}\n \tif (flags & O_PATH) {\n \t\t/* O_PATH only permits certain other flags to be set. */\ndiff --git a/include/linux/fcntl.h b/include/linux/fcntl.h\nindex a332e79b3207..4fd07b0e0a17 100644\n--- a/include/linux/fcntl.h\n+++ b/include/linux/fcntl.h\n@@ -10,7 +10,7 @@\n \t(O_RDONLY | O_WRONLY | O_RDWR | O_CREAT | O_EXCL | O_NOCTTY | O_TRUNC | \\\n \t O_APPEND | O_NDELAY | O_NONBLOCK | __O_SYNC | O_DSYNC | \\\n \t FASYNC\t| O_DIRECT | O_LARGEFILE | O_DIRECTORY | O_NOFOLLOW | \\\n-\t O_NOATIME | O_CLOEXEC | O_PATH | __O_TMPFILE)\n+\t O_NOATIME | O_CLOEXEC | O_PATH | __O_TMPFILE | O_REGULAR)\n \n /* List of all valid flags for the how->resolve argument: */\n #define VALID_RESOLVE_FLAGS \\\ndiff --git a/include/uapi/asm-generic/errno.h b/include/uapi/asm-generic/errno.h\nindex 92e7ae493ee3..2216ab9aa32e 100644\n--- a/include/uapi/asm-generic/errno.h\n+++ b/include/uapi/asm-generic/errno.h\n@@ -122,4 +122,6 @@\n \n #define EHWPOISON\t133\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t134\t/* Not a regular file */\n+\n #endif\ndiff --git a/include/uapi/asm-generic/fcntl.h b/include/uapi/asm-generic/fcntl.h\nindex 613475285643..3468b352a575 100644\n--- a/include/uapi/asm-generic/fcntl.h\n+++ b/include/uapi/asm-generic/fcntl.h\n@@ -88,6 +88,10 @@\n #define __O_TMPFILE\t020000000\n #endif\n \n+#ifndef O_REGULAR\n+#define O_REGULAR\t040000000\n+#endif\n+\n /* a horrid kludge trying to make sure that this will fail on old kernels */\n #define O_TMPFILE (__O_TMPFILE | O_DIRECTORY)\n \ndiff --git a/tools/arch/alpha/include/uapi/asm/errno.h b/tools/arch/alpha/include/uapi/asm/errno.h\nindex 6791f6508632..8bbcaa9024f9 100644\n--- a/tools/arch/alpha/include/uapi/asm/errno.h\n+++ b/tools/arch/alpha/include/uapi/asm/errno.h\n@@ -127,4 +127,6 @@\n \n #define EHWPOISON\t139\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t140\t/* Not a regular file */\n+\n #endif\ndiff --git a/tools/arch/mips/include/uapi/asm/errno.h b/tools/arch/mips/include/uapi/asm/errno.h\nindex c01ed91b1ef4..293c78777254 100644\n--- a/tools/arch/mips/include/uapi/asm/errno.h\n+++ b/tools/arch/mips/include/uapi/asm/errno.h\n@@ -126,6 +126,8 @@\n \n #define EHWPOISON\t168\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t169\t/* Not a regular file */\n+\n #define EDQUOT\t\t1133\t/* Quota exceeded */\n \n \ndiff --git a/tools/arch/parisc/include/uapi/asm/errno.h b/tools/arch/parisc/include/uapi/asm/errno.h\nindex 8cbc07c1903e..442917484f99 100644\n--- a/tools/arch/parisc/include/uapi/asm/errno.h\n+++ b/tools/arch/parisc/include/uapi/asm/errno.h\n@@ -124,4 +124,6 @@\n \n #define EHWPOISON\t257\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t258\t/* Not a regular file */\n+\n #endif\ndiff --git a/tools/arch/sparc/include/uapi/asm/errno.h b/tools/arch/sparc/include/uapi/asm/errno.h\nindex 4a41e7835fd5..8dce0bfeab74 100644\n--- a/tools/arch/sparc/include/uapi/asm/errno.h\n+++ b/tools/arch/sparc/include/uapi/asm/errno.h\n@@ -117,4 +117,6 @@\n \n #define EHWPOISON\t135\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t136\t/* Not a regular file */\n+\n #endif\ndiff --git a/tools/include/uapi/asm-generic/errno.h b/tools/include/uapi/asm-generic/errno.h\nindex 92e7ae493ee3..2216ab9aa32e 100644\n--- a/tools/include/uapi/asm-generic/errno.h\n+++ b/tools/include/uapi/asm-generic/errno.h\n@@ -122,4 +122,6 @@\n \n #define EHWPOISON\t133\t/* Memory page has hardware error */\n \n+#define ENOTREG\t\t134\t/* Not a regular file */\n+\n #endif\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-01-27",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "The author is addressing a concern about the implementation of O_REGULAR flag in happy path testing, and has added a new test case to cover this scenario.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged feedback",
                "added new test case"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Just a happy path test.\n\nSigned-off-by: Dorjoy Chowdhury <dorjoychy111@gmail.com>\n---\n .../testing/selftests/openat2/openat2_test.c  | 37 ++++++++++++++++++-\n 1 file changed, 36 insertions(+), 1 deletion(-)\n\ndiff --git a/tools/testing/selftests/openat2/openat2_test.c b/tools/testing/selftests/openat2/openat2_test.c\nindex 0e161ef9e9e4..011ebc9af4e5 100644\n--- a/tools/testing/selftests/openat2/openat2_test.c\n+++ b/tools/testing/selftests/openat2/openat2_test.c\n@@ -320,8 +320,42 @@ void test_openat2_flags(void)\n \t}\n }\n \n+#ifndef O_REGULAR\n+#define O_REGULAR 040000000\n+#endif\n+\n+#ifndef ENOTREG\n+#define ENOTREG 134\n+#endif\n+\n+void test_openat2_o_regular_flag(void)\n+{\n+\tif (!openat2_supported) {\n+\t\tksft_test_result_skip(\"Skipping %s as openat2 is not supported\\n\", __func__);\n+\t\treturn;\n+\t}\n+\n+\tstruct open_how how = {\n+\t\t.flags = O_REGULAR | O_RDONLY\n+\t};\n+\n+\tint fd = sys_openat2(AT_FDCWD, \"/dev/null\", &how);\n+\n+\tif (fd == ENOENT) {\n+\t\tksft_test_result_skip(\"Skipping %s as there is no /dev/null\\n\", __func__);\n+\t\treturn;\n+\t}\n+\n+\tif (fd != -ENOTREG) {\n+\t\tksft_test_result_fail(\"openat2 should return ENOTREG\\n\");\n+\t\treturn;\n+\t}\n+\n+\tksft_test_result_pass(\"%s succeeded\\n\", __func__);\n+}\n+\n #define NUM_TESTS (NUM_OPENAT2_STRUCT_VARIATIONS * NUM_OPENAT2_STRUCT_TESTS + \\\n-\t\t   NUM_OPENAT2_FLAG_TESTS)\n+\t\t   NUM_OPENAT2_FLAG_TESTS + 1)\n \n int main(int argc, char **argv)\n {\n@@ -330,6 +364,7 @@ int main(int argc, char **argv)\n \n \ttest_openat2_struct();\n \ttest_openat2_flags();\n+\ttest_openat2_o_regular_flag();\n \n \tif (ksft_get_fail_cnt() + ksft_get_error_cnt() > 0)\n \t\tksft_exit_fail();\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-01-27",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "The author addressed a concern about the O_REGULAR flag's behavior in arch/sparc/include/uapi/asm/fcntl.h, explaining that they followed the convention of other architecture-specific files and made changes to match the generic fcntl.h file. The patch includes modifications to ensure consistency across architectures.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged concern",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Following the convention in include/uapi/asm-generic/fcntl.h and other\narchitecture specific arch/*/include/uapi/asm/fcntl.h files.\n\nSigned-off-by: Dorjoy Chowdhury <dorjoychy111@gmail.com>\n---\n arch/sparc/include/uapi/asm/fcntl.h | 36 ++++++++++++++---------------\n 1 file changed, 18 insertions(+), 18 deletions(-)\n\ndiff --git a/arch/sparc/include/uapi/asm/fcntl.h b/arch/sparc/include/uapi/asm/fcntl.h\nindex a93d18d2c23e..3c16f1a66a6a 100644\n--- a/arch/sparc/include/uapi/asm/fcntl.h\n+++ b/arch/sparc/include/uapi/asm/fcntl.h\n@@ -2,23 +2,23 @@\n #ifndef _SPARC_FCNTL_H\n #define _SPARC_FCNTL_H\n \n-#define O_APPEND\t0x0008\n-#define FASYNC\t\t0x0040\t/* fcntl, for BSD compatibility */\n-#define O_CREAT\t\t0x0200\t/* not fcntl */\n-#define O_TRUNC\t\t0x0400\t/* not fcntl */\n-#define O_EXCL\t\t0x0800\t/* not fcntl */\n-#define O_DSYNC\t\t0x2000\t/* used to be O_SYNC, see below */\n-#define O_NONBLOCK\t0x4000\n+#define O_APPEND\t0000000010\n+#define FASYNC\t\t0000000100\t/* fcntl, for BSD compatibility */\n+#define O_CREAT\t\t0000001000\t/* not fcntl */\n+#define O_TRUNC\t\t0000002000\t/* not fcntl */\n+#define O_EXCL\t\t0000004000\t/* not fcntl */\n+#define O_DSYNC\t\t0000020000\t/* used to be O_SYNC, see below */\n+#define O_NONBLOCK\t0000040000\n #if defined(__sparc__) && defined(__arch64__)\n-#define O_NDELAY\t0x0004\n+#define O_NDELAY\t0000000004\n #else\n-#define O_NDELAY\t(0x0004 | O_NONBLOCK)\n+#define O_NDELAY\t(0000000004 | O_NONBLOCK)\n #endif\n-#define O_NOCTTY\t0x8000\t/* not fcntl */\n-#define O_LARGEFILE\t0x40000\n-#define O_DIRECT        0x100000 /* direct disk access hint */\n-#define O_NOATIME\t0x200000\n-#define O_CLOEXEC\t0x400000\n+#define O_NOCTTY\t0000100000\t/* not fcntl */\n+#define O_LARGEFILE\t0001000000\n+#define O_DIRECT        0004000000 /* direct disk access hint */\n+#define O_NOATIME\t0010000000\n+#define O_CLOEXEC\t0020000000\n /*\n  * Before Linux 2.6.33 only O_DSYNC semantics were implemented, but using\n  * the O_SYNC flag.  We continue to use the existing numerical value\n@@ -32,12 +32,12 @@\n  *\n  * Note: __O_SYNC must never be used directly.\n  */\n-#define __O_SYNC\t0x800000\n+#define __O_SYNC\t0040000000\n #define O_SYNC\t\t(__O_SYNC|O_DSYNC)\n \n-#define O_PATH\t\t0x1000000\n-#define __O_TMPFILE\t0x2000000\n-#define O_REGULAR\t0x4000000\n+#define O_PATH\t\t0100000000\n+#define __O_TMPFILE\t0200000000\n+#define O_REGULAR\t0400000000\n \n #define F_GETOWN\t5\t/*  for sockets. */\n #define F_SETOWN\t6\t/*  for sockets. */\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-01-27",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "The author addressed a concern about the consistency of O_* flag macros across architecture-specific fcntl.h files, explaining that they followed the convention in include/uapi/asm-generic/fcntl.h and made corresponding changes to arch/mips/include/uapi/asm/fcntl.h.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Following the convention in include/uapi/asm-generic/fcntl.h and other\narchitecture specific arch/*/include/uapi/asm/fcntl.h files.\n\nSigned-off-by: Dorjoy Chowdhury <dorjoychy111@gmail.com>\n---\n arch/mips/include/uapi/asm/fcntl.h | 22 +++++++++++-----------\n 1 file changed, 11 insertions(+), 11 deletions(-)\n\ndiff --git a/arch/mips/include/uapi/asm/fcntl.h b/arch/mips/include/uapi/asm/fcntl.h\nindex 0369a38e3d4f..6aa3f49df17e 100644\n--- a/arch/mips/include/uapi/asm/fcntl.h\n+++ b/arch/mips/include/uapi/asm/fcntl.h\n@@ -11,15 +11,15 @@\n \n #include <asm/sgidefs.h>\n \n-#define O_APPEND\t0x0008\n-#define O_DSYNC\t\t0x0010\t/* used to be O_SYNC, see below */\n-#define O_NONBLOCK\t0x0080\n-#define O_CREAT\t\t0x0100\t/* not fcntl */\n-#define O_TRUNC\t\t0x0200\t/* not fcntl */\n-#define O_EXCL\t\t0x0400\t/* not fcntl */\n-#define O_NOCTTY\t0x0800\t/* not fcntl */\n-#define FASYNC\t\t0x1000\t/* fcntl, for BSD compatibility */\n-#define O_LARGEFILE\t0x2000\t/* allow large file opens */\n+#define O_APPEND\t0000010\n+#define O_DSYNC\t\t0000020\t/* used to be O_SYNC, see below */\n+#define O_NONBLOCK\t0000200\n+#define O_CREAT\t\t0000400\t/* not fcntl */\n+#define O_TRUNC\t\t0001000\t/* not fcntl */\n+#define O_EXCL\t\t0002000\t/* not fcntl */\n+#define O_NOCTTY\t0004000\t/* not fcntl */\n+#define FASYNC\t\t0010000\t/* fcntl, for BSD compatibility */\n+#define O_LARGEFILE\t0020000\t/* allow large file opens */\n /*\n  * Before Linux 2.6.33 only O_DSYNC semantics were implemented, but using\n  * the O_SYNC flag.  We continue to use the existing numerical value\n@@ -33,9 +33,9 @@\n  *\n  * Note: __O_SYNC must never be used directly.\n  */\n-#define __O_SYNC\t0x4000\n+#define __O_SYNC\t0040000\n #define O_SYNC\t\t(__O_SYNC|O_DSYNC)\n-#define O_DIRECT\t0x8000\t/* direct disk access hint */\n+#define O_DIRECT\t0100000\t/* direct disk access hint */\n \n #define F_GETLK\t\t14\n #define F_SETLK\t\t6\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-01-27",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer suggested including information about the UAPI group's request for O_REGULAR flag support in the commit message, specifically a link to the kernel feature documentation and a mention of its discussion at LPC 2024.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request_for_additional_info"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "As you mention in your cover letter, this is something that the UAPI\ngroup has asked for in the past[1] and was even discussed at a recent\nLPC (maybe LPC 2024?) -- thanks for the patch!\n\nIn the next posting of this patchset, I would suggest including this\ninformation in the *commit message* with a link (commit messages end up\nin the git history, cover letters are a little harder to search for when\ndoing \"git blame\").\n\n[1]: https://uapi-group.org/kernel-features/#ability-to-only-open-regular-files",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that using the O_REGULAR flag with O_PATH does not make sense, as O_PATH file descriptors do not open the target inode and thus there is no risk of opening a device inode. Instead, they suggested using an O_PATH to check if it's a regular file via fstat(2) before re-opening the file descriptor through /proc/self/fd/$n.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It doesn't really make sense to use this flag with O_PATH -- O_PATH file\ndescriptors do not actually open the target inode and so there is no\nrisk to doing this.\n\nIn fact the method of safely opening files while avoiding device inodes\non Linux today is to open an O_PATH, then use fstat(2) to check whether\nit is a regular file, and then re-open the file descriptor through\n/proc/self/fd/$n. (This is totally race-safe.)\n\nMy main reason for pushing back against this it's really quite\npreferable to avoid expanding the set of O_* flags which work with\nO_PATH if they don't add much -- O_PATH has really unfortunate behaviour\nwith ignoring other flags and openat2(2) finally fixed that by blocking\nignored flag combinations.",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that legacy open(2)/openat(2) do not reject invalid flag arguments, making it difficult to add a new security-critical flag like O_REGULAR without breaking userspace or relying on caching issues. They suggested using the openat2(2)-only API and instead proposing a mask of S_IFMT bits to be rejected in struct open_how.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Legacy open(2)/openat(2) do not reject invalid flag arguments, which\nmeans that you cannot trivially add a new security-critical flag to them\nfor two reasons:\n\n * You cannot easily rely on them because old kernels will not return\n   -EINVAL, meaning you cannot be sure that the flag is supported. You\n   can try to test-run it, but the operation needs to be a non-dangerous\n   operation to try (and caching this has its own issues, such as with\n   programs that apply seccomp filters later).\n\n   To be fair, since you reject O_DIRECTORY|O_REGULAR there is a\n   relatively easy way to detect this, but the caveats about problems\n   with caching still apply.\n\n * Old programs might pass garbage bits that have been ignored thus far,\n   which means that making them have meaning can break userspace. Given\n   the age of open(2) this is a very hard thing to guarantee and is one\n   of many reasons I wrote openat2(2) and finally added proper flag\n   checking.\n\n   This is something your patch doesn't deal with and I don't think can\n   be done in a satisfactory way (because the behaviour relies on more\n   than just the arguments).\n\nFor reference, this is why O_TMPFILE includes O_DIRECTORY and requires\nan O_ACCMODE with write bits -- this combination will fail on old\nkernels, which allows you to rely on it and also guarantees that no\nexisting older programs passed that flag combination already and\nhappened to work on older kernels. This kind of trick won't work for\nO_REGULAR, unfortunately.\n\nIn my view, this should be an openat2(2)-only API. In addition, I would\npropose that (instead of burning another O_* flag bit for this as a\nspecial-purpose API just for regular files) you could have a mask of\nwhich S_IFMT bits should be rejected as a new field in \"struct\nopen_how\". This would let you reject sockets or device inodes but permit\nFIFOs and regular files or directories, for instance. This could even be\ndone without a new O_* flag at all (the zero-value how->sfmt_mask would\nallow everything and so would work well with extensible structs), but we\ncould add an O2_* flag anyway.",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer requested a description of why a new errno is needed in the commit or cover letter, suggesting ENXIO or EPROTONOSUPPORT/EPROTOTYPE as alternatives to ENOTREG.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "We are probably a little too reticent to add new errnos, but in this\ncase I think that there should be some description in the commit or\ncover letter about why a new errno is needed. ENXIO or\nEPROTONOSUPPORT/EPROTOTYPE is what you would typically use (yes, they\naren't a _perfect_ match but one of the common occurrences in syscall\ndesign is to read through errno(7) and figure out what errnos kind of\nfit what you need to express).\n\nThen to be fair, the existence of ENOTBLK, ENOTDIR, ENOTSOCK, etc. kind\nof justify the existence of ENOTREG too. Unfortunately, you won't be\nable to use ENOTREG if you go with my idea of having mask bits in\nopen_how... (And what errno should we use then...? Hm.)\n\n-- \nAleksa Sarai\nhttps://www.cyphar.com/",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that the patch lacks handling for ->atomic_open() operations, which cannot properly support O_REGULAR flag in most filesystems, and suggested adding patches to return -EINVAL if O_REGULAR is set",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "missing_handling",
                "requested_changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "One thing this patch is missing is handling for ->atomic_open(). I\nimagine most of the filesystems that provide that op can't support\nO_REGULAR properly (maybe cifs can? idk). What you probably want to do\nis add in some patches that make all of the atomic_open operations in\nthe kernel return -EINVAL if O_REGULAR is set.\n\nThen, once the basic support is in, you or someone else can go back and\nimplement support for O_REGULAR where possible.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-27",
              "analysis_source": "llm"
            },
            {
              "author": "Mateusz Guzik",
              "summary": "Reviewer Mateusz Guzik expressed concern about the reuse of O_ namespace in openat2 syscall, which may lead to confusion and incorrect assumptions when passing flags to open and openat functions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "concern",
                "confusion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "fwiw +1 from me, the O_ flag situation is already terrible even without\nthe validation woes.\n\nI find it most unfortunate the openat2 syscall reuses the O_ namespace.\nFor my taste it would be best closed for business, with all new flag\nadditions using a different space.\n\nI can easily see people passing O_WHATEVER to open and openat by blindly\nassuming they are supported just based on the name.\n\nthat's a side mini-rant, too late to do anything here now",
              "reply_to": "Aleksa Sarai",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Mateusz Guzik",
              "summary": "Reviewer Mateusz Guzik noted that the current implementation of the O_REGULAR flag using bitmasks has overlapping bits with existing file type definitions, making it impossible to select specific types. He suggested redefining the field to only allow selection of desired types and rejecting the rest.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think this works because the vars have overlapping bits:\n  #define S_IFBLK  0060000\n  #define S_IFDIR  0040000\n\nSo you very much can't select what you want off of a bitmask.\n\nAt best the field could be used to select the one type you are fine with.\n\nIf one was to pursue the idea, some other defines with unique bits would\nneed to be provided. But even then, semantics should be to only *allow*\nthe bits you are fine with and reject the rest.\n\nBut I'm not at all confident this is worth any effort -- with\nO_DIRECTORY already being there and O_REGULAR proposed, is there a use\ncase which wants something else?",
              "reply_to": "Aleksa Sarai",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Mateusz Guzik",
              "summary": "Reviewer Mateusz Guzik suggested adding specific error codes to indicate the type of file opened when O_REGULAR flag is used, and proposed using EBADTYPE or a similar reusable code",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The most useful behavior would indicate what was found (e.g., a pipe).\n\nThe easiest way to do it would create errnos for all types (EISDIR\nalready exists for one), but I can't seriously propose that.\n\nGoing the other way, EBADTYPE or something else reusable would be my\nidea.",
              "reply_to": "Aleksa Sarai",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author responded to feedback about the behavior of O_REGULAR flag in combination with O_CREAT, asking for clarification on where and what specific bug needs fixing, indicating uncertainty about expected behavior when file doesn't exist or exists but is not regular.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thank you for the feedback. I don't quite understand what I need to\nfix. I thought open system calls always create regular files, so\natomic_open probably always creates regular files? Can you please give\nme some more details as to where I need to fix this and what the\nactual bug here is that is related to atomic_open?  I think I had done\nsome normal testing and when using O_CREAT | O_REGULAR, if the file\ndoesn't exist, the file gets created and the file that gets created is\na regular file, so it probably makes sense? Or should the behavior be\nthat if file doesn't exist, -EINVAL is returned and if file exists it\nis opened if regular, otherwise -ENOTREG is returned?\n\nRegards,\nDorjoy",
              "reply_to": "Jeff Layton",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "reviewer noted that atomic_open() is a separate codepath for network filesystems, which will ignore O_REGULAR flag and requested an interim step to return -EINVAL from atomic_open operations when O_DIRECTORY is set",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "atomic_open() is a combination lookup+open for when the dentry isn't\npresent in the dcache. The normal open codepath that you're patching\ndoes not get called in this case when ->atomic_open is set for the\nfilesystem. It's mostly used by network filesystems that need to\noptimize away the lookup since it's wasted round trip, and is often\nracy anyway. Your patchset doesn't address those filesystems. They will\nlikely end up ignoring O_REGULAR in that case, which is not what you\nwant.\n\nWhat I was suggesting is that, as an interim step, you find all of the\natomic_open operations in the kernel (there are maybe a dozen or so),\nand just make them return -EINVAL if someone sets O_DIRECTORY. Later,\nyou or someone else can then go back and do a proper implementation of\nO_REGULAR handling on those filesystems, at least on the ones where\nit's possible. You will probably also need to similarly patch the\nopen() routines for those filesystems too. Otherwise you'll get\ninconsistent behavior vs. when the dentry is in the cache.\n\nOne note: I think NFS probably can support O_DIRECTORY, since its OPEN\ncall only works on files. We'll need to change how we handle errors\nfrom the server when it's set though.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author is seeking clarification on whether to return -EINVAL from atomic_open implementations when both O_REGULAR and O_DIRECTORY flags are set, as they already handle this case in the build_open_flags function.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification sought",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sorry, I am just trying to fully understand this. Do you mean to\nreturn -EINVAL from all atomic_open implementations in the kernel if\nboth O_REGULAR and O_DIRECTORY are set (or just only if O_REGULAR is\nset, we need to return -EINVAL)? I am already returning -EINVAL when\nboth these are set from the build_open_flags function, so that should\nalready handle the cases, right? I think after atomic_open get called,\nall code paths eventually go through the do_open function where I have\nthis check \"if ((open_flag & O_REGULAR) && !d_is_reg(nd->path.dentry))\nreturn -ENOTREG\". This is right before if ((nd->flags &\nLOOKUP_DIRECTORY) && !d_can_lookup(nd->path.dentry)) return -ENOTDIR;\nwhich I had initially followed. So should I just return -EINVAL from\nthe atomic_open functions too if both O_REGULAR and O_DIRECTORY are\nset? Sorry if I am misunderstanding this.\n\nRegards,\nDorjoy",
              "reply_to": "Jeff Layton",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author is considering renaming O_REGULAR to O2_REGULAR, creating a new flag set for openat2 flags, and removing the sfmt_mask handling.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "considering alternative approach",
                "open to feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Good discussion. So should I just rename the O_REGULAR to O2_REGULAR\nand create a VALID_OPENAT2_FLAGS and no need to do how->sfmt_mask\nstuff?",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author acknowledges that using ENOTREG instead of a custom error code might be acceptable, but no clear resolution or agreement on the approach.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledges feedback",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Good point. Maybe ENOTREG is acceptable too?\n\nRegards,\nDorjoy",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer suggested that new flag additions, except for rare cases where they can be backward compatible, should use the O2_* or OEXT_* naming convention instead of O_*",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggestion",
                "convention"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "We don't have any openat2(2)-only O_* flags yet, I agree that new flag\nadditions (except for very rare cases where you can make them backward\ncompatible -- such as a hypothetical O_EMPTYPATH) should be O2_* or\nOEXT_* or something.",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that since openat2(2) has exclusive rights to the 64-bit flag bits, it would be better to start using those before potentially conflicting with the O_* flag space.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yeah, if we don't do that it'll lead to confusion. openat2(2) has\nexclusive rights to the 64-bit flag bits so we could start with those\nbefore we need to cross with the O_* flag space.",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that the O_REGULAR flag has unfortunate overlaps with other file types, specifically block/char devices and directories, and suggested using S_IFCHR to filter both types",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "overlap",
                "unfortunate"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "Well, you can filter on S_IFCHR if you want to block both block/char\ndevices, but yeah the overlap is quite unfortunate... (That would also\nmean blocking directories would also block S_IFBLK -- I remembered there\nwas an overlap but I forgot it coincided with S_IFDIR... Damn wacky\nAPIs.)",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that the current flags (O_REGULAR, O_DIRECTORY, and O_NOFOLLOW) do not allow for a more permissive mode where only specific types of files are allowed to be opened, and suggested adding such a flag",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There's also O_NOFOLLOW in a similar vein.\n\nI can see someone wanting to permit FIFOs, regular files, and\ndirectories being fine but blocking everything else. None of O_REGULAR,\nO_DIRECTORY, nor O_NOFOLLOW provide that.",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that using 3-5 new error numbers (ENOTREG, etc.) to implement the O_REGULAR flag would be wasteful, given that there are already ~4 existing error numbers that are logical inverses.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "wasteful use of resources",
                "potential for reuse in other APIs"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It might be kinda neat from a potential re-use perspective in other APIs\nbut yeah it would be quite wasteful to burn 3-5 errnos for this when we\nalready have ~4 that are logical inverses.",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer suggested reusing the existing ENOTREG error code for O_REGULAR flag, noting it could be a generic errno for other places to use.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think that would be reasonable and if you word the error message\ncarefully you can even see it being a fairly generic errno for other\nplaces to use.\n\n-- \nAleksa Sarai\nhttps://www.cyphar.com/",
              "reply_to": "Mateusz Guzik",
              "message_date": "2026-01-28",
              "analysis_source": "llm"
            },
            {
              "author": "Christian Brauner",
              "summary": "Reviewer Christian Brauner noted that the O_REGULAR flag should be added to openat2() instead of open(), and suggested renaming it to OPENAT2_REGULAR, as well as adding EFTYPE as an errno code.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yeah, we shouldn't add support for this outside of openat2(). We also\nshouldn't call this OEXT_* or O2_*. Let's just follow the pattern where\nwe prefix the flag space with the name of the system call\nOPENAT2_REGULAR.\n\nThere's also no real need to make O_DIRECTORY exclusive with\nOPENAT2_REGULAR. Callers could legimitately want to open a directory or\nregular file but not anything else. If someone wants to operate on a\nwhole filesystem tree but only wants to interact with regular files and\ndirectories and ignore devices, sockets, fifos etc it's very handy to\njust be able to set both in flags.\n\nFrankly, this shouldn't be a flag at all but we already have O_DIRECTORY\nin there so no need to move this into a new field.\n\nAdd EFTYPE as the errno code. Some of the bsds including macos already\nhave that.",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Christian Brauner",
              "summary": "Reviewer Christian Brauner noted that the patch's O_REGULAR flag behavior is problematic for filesystems implementing ->atomic_open::, as it may trigger an actual open on a non-regular file before the VFS has a chance to validate what's going on. He suggested bypassing ->atomic_open:: for OPENAT2_REGULAR without O_CREAT and falling back to racy and pricy lookup + open instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "concerns about patch functionality"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "So I think you're proposing two separate things or there's a typo:\n\n(1) blocking O_DIRECTORY for ->atomic_open::\n(2) blocking O_REGULAR for ->atomic_open::\n\nThe (1) point implies that O_DIRECTORY currently doesn't work correctly\nwith atomic open for all filesystems.\n\nEver since 43b450632676 (\"open: return EINVAL for O_DIRECTORY |\nO_CREAT\") O_DIRECTORY with O_CREAT is blocked. It was accidently allowed\nand completely broken before that.\n\nFor O_DIRECTORY without O_CREAT the kernel will pass that down through\n->atomic_open:: to the filesystem.\n\nThe worry that I see is that a network filesystem via ->atomic_open::\nsomehow already called open on the server side on something that wasn't\na directory. At that point the damage such as side-effects from device\nopening is already done.\n                                    \nBut I suspect that every filesystem implementing ->atomic_open:: just\ndoes finish_no_open() and punts to the VFS for the actual open. And the\nVFS will catch it in do_open() for it actually opens the file. So the\nonly real worry for O_DIRECTORY I see is that there's an fs that handles\nit wrong.\n\nFor (2) it is problematic as there surely are filesystems with\n->atomic_open:: that do handle the ~O_CREAT case and return with\nFMODE_OPENED. So that'll be problematic if the intention is to not\ntrigger an actual open on a non-regular file such as a\ndevice/socket/fifo etc. before the VFS had a chance to validate what's\ngoing on.\n\nSo I'm not excited about having this 70% working and punting on\n->atomic_open:: waiting for someone to fix this. One option would be to\nbypass ->atomic_open:: for OPENAT2_REGULAR without O_CREAT and fallback\nto racy and pricy lookup + open for now. How problematic would that be?\nIf possible I'd prefer this a lot over merging something that works\nhalf-way.\n\nI guess to make that really work you'd need some protocol extension?",
              "reply_to": "Jeff Layton",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "reviewer noted that sending an immediate close would have unintended consequences",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "side effects"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Exactly. I guess you could send an immediate close, but that's not\nwithout side effects.",
              "reply_to": "Christian Brauner",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that O_REGULAR flag may need special handling for various file systems, including NFS, CIFS, and others, as some of these file systems can create directories or have different open semantics.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "potential issues"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "For NFS, I think we're OK. The OPEN call on NFSv4 only works for\nregular files, so it should be able to handle O_REGULAR. We just need\nto rejigger the error handling when it's set (just return an error\ninstead of doing the open of a directory or whatever it is).\n\nThe others (at a quick glance):\n\ncifs: I don't see a way to specify an O_REGULAR equivalent to the\nSMB2_CREATE call and it looks like it can create directories. Maybe\nSteveF (cc'ed) knows if this is possible?\n\nceph: I think CEPH_MDS_OP_OPEN might only work for files, in which case\nO_REGULAR can probably be supported similarly to NFS.\n\nfuse: probably ok? Does finish_no_open() in most cases. May depend on\nthe userland driver though.\n\ngfs2: is ok, it just does finish_no_open() in most cases anyway\n\nvboxsf: does finish_no_open on non-creates, so you could probably just\npunt to that if O_REGULAR is set.\n\nSo, it's probably possible to do this across the board. I'm not sure\nabout cifs though.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Christian Brauner",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Jeff Layton pointed out that SMB2 has a similar flag (FILE_NON_DIRECTORY_FILE) which requires the server to fail requests to open directories, and suggested considering how to handle named pipes and printer files",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "SMB2 does have this flag:\n\nFILE_NON_DIRECTORY_FILE 0x00000040\n\nIf the name of the file being created or opened matches with an\nexisting directory file, the server MUST fail the request with\nSTATUS_FILE_IS_A_DIRECTORY. This flag MUST NOT be used with\nFILE_DIRECTORY_FILE or the server MUST fail the request with\nSTATUS_INVALID_PARAMETER.\n\nSMB2 can also present named pipes and printer files. Not sure if there\nis a way to exclude those with this.",
              "reply_to": "",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that the O_REGULAR flag does not prevent opening directories, and requested further investigation into whether a non-racy solution exists for the MDS code.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "further investigation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Actually I'm wrong here. That op can open a directory. We'll need\nsomeone to look at the MDS code and tell us whether this can be done in\na non-racy way.",
              "reply_to": "",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that O_DIRECTORY flag handling appears to be intact in NFS, and expressed no concerns about other architectures.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no specific technical issue raised"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It looks like NFS at least handles O_DIRECTORY properly. I don't have\nany reason to believe that O_DIRECTORY handling is broken on the\nothers.",
              "reply_to": "Christian Brauner",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author agrees that OPENAT2_REGULAR is a better option than OEXT_*/O2_* options, but asks if it should be defined outside the 32-bit range to avoid conflicts with existing O_* bits and only require definition in include/uapi/asm-generic/fcntl.h.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "asking for clarification",
                "considering alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for the feedback. I agree that OPENAT2_REGULAR is better than\nthe other OEXT_*/O2_* options. Right now in the patch, the O_REGULAR\ntook the next slot in all the fcntl files. Should OPENAT2_REGULAR be a\nbit outside of the 32bits? That way it won't take any of the regular\nO_* bits and we would only need to define it in\ninclude/uapi/asm-generic/fcntl.h file and not need it in\narch/*/fcntl.h files. What do you think?",
              "reply_to": "Christian Brauner",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author acknowledged that the O_REGULAR flag's behavior when combined with other flags like O_CREAT may not be semantically correct and agreed to fix it in v4.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a potential issue",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Great suggestion. Will fixup in v4 submission.\n\nRegards,\nDorjoy",
              "reply_to": "Christian Brauner",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Aleksa Sarai",
              "summary": "Reviewer noted that using a bitmask of S_IFMT to reject opening certain file types has issues due to overlapping values, and suggested an alternative approach",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "alternative approach",
                "suggested"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You could even say O_NOFOLLOW is kinda like that too.\n\nIn my other mail I proposed a bitmask of S_IFMT to reject opening (which\nwould let you allow FIFOs and regular files but block devices, etc).\nUnfortunately I forgot that S_IFBLK is S_IFCHR|S_IFDIR. This isn't fatal\nto the idea but it kinda sucks. Grr.",
              "reply_to": "Christian Brauner",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author agrees that introducing a new how->sfmt_allow field with separate bits for file types is a good approach, but expresses concern about the user API complexity.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a suggestion",
                "expressed concern"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It is a good suggestion. I guess we can still introduce a new\nhow->sfmt_allow field and have new bits (instead of keeping in sync\nwith S_IF* ones) that allow types and just start with regular file\nallow bit for now, right? But I guess it would be cumbersome for users\nas an api to use different bits?\n\nRegards,\nDorjoy",
              "reply_to": "Aleksa Sarai",
              "message_date": "2026-01-29",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author is seeking clarification on which codepath the O_REGULAR flag's error handling should occur, indicating uncertainty about implementing the patch correctly.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thank you for the details. Do you remember which codepath this is? Is\nthis the inode_operations.atomic_open codepath or file_operations.open\ncodepath? I am a bit confused also about where exactly the error\nhandling that needs to be done.",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "The author is clarifying that the atomic_open code paths are being addressed, specifically mentioning finish_no_open in these paths.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "These are all inode_operations.atomic_open code paths, right? Because\nyou mentioned finish_no_open and I see finish_no_open in the\natomic_open code paths as opposed to file_operations.open code paths.\n\nRegards,\nDorjoy",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton noted that nfs_atomic_open() might not be efficient due to unnecessary calls when the open_context() call returns an error, and suggested returning an immediate error in such cases",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I was thinking nfs_atomic_open().\n\nLooking now, I think it might actually work OK without changes. It just\nmight not be terribly efficient about it.\n\nIf the open_context() call returns -EISDIR or similar, then you really\ndon't need to do the call to nfs_lookup() and the like. You can just\nreturn an immediate error when O_REGULAR is set since you know it's not\nsuitable to be opened.",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that the patch's behavior may not be optimal for all cases, suggesting a deeper review and testing to ensure correctness, particularly in scenarios where finish_no_open() is used.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Note that this was just a cursory look. Someone will need to do a\ndeeper dive and test these cases.\n\nI think most will end up working ok, since most fall back to doing a\nfinish_no_open(). There may be opportunities to optimize some of these\ncases though (similarly to how I mentioned with NFS).\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "The author acknowledges that O_REGULAR being an unknown flag is a concern, but does not explicitly state whether they plan to address it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledges concern",
                "does not commit to fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Right. And I guess we don't need to worry about O_REGULAR being an\nunknown flag when it gets sent to the server (not only for NFS / but\nothers as well)?",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author acknowledged a concern about handling O_REGULAR flag in other filesystems and promised to implement it, but asked for clarification on whether file_operations.open code paths also need modification.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "promised to implement"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I can try to look into these and see if I can implement handling for\nO_REGULAR flag for these filesystems in the atomic_open code paths.\nThanks for the details.\n\nWill I need to modify the corresponding file_operations.open code\npaths as well along with atomic_open code paths?\n\nRegards,\nDorjoy",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton noted that POSIX flags, including O_REGULAR, should not be sent in NFSv4 requests as they have their own set of flags; instead, O_REGULAR is already implied in an OPEN call on the wire and only operates on regular files.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "flag",
                "NFSv4"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You shouldn't. We don't send POSIX flags in NFSv4 requests. It has its\nown set of flags. In the case of NFSv4, O_REGULAR is already implied in\nan OPEN call on the wire. OPEN only operates on regular files.",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer noted that when a valid dentry exists, O_REGULAR checks can be satisfied without calling ->open(), suggesting an optimization opportunity.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "optimization",
                "performance"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Probably not.\n\nThe main thing to keep in mind is that ->open is used when we already\nhave a dentry for the target of the open. ->atomic_open is used when we\ndon't have one yet or the one we have has failed revalidation.\n\nIf you have a valid dentry, then you should be able to satisfy the\nO_REGULAR check without having to call into ->open at all.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Dorjoy Chowdhury",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Dorjoy Chowdhury (author)",
              "summary": "Author acknowledged Jeff Layton's feedback and thanked him for understanding",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged",
                "thanked"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Understood. Thanks!\n\nRegards,\nDorjoy",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] VFS idmappings support in NFS",
          "message_id": "a0eab8f07873e38fa4c5d958de6b75761d690874.camel@kernel.org",
          "url": "https://lore.kernel.org/all/a0eab8f07873e38fa4c5d958de6b75761d690874.camel@kernel.org/",
          "date": "2026-02-18T15:08:16Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton requested an example of how to use VFS idmappings with NFS in a real environment, specifically for the AUTH_SYS case without NFSv4 idmapping involved.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request_for_clarity",
                "need_for_example"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "IIUC, people mostly use vfs-layer idmappings because they want to remap\nthe uid/gid values of files that get stored on the backing store (disk,\nceph MDS, or whatever).\n\nI've never used idmappings myself much in practice. Could you lay out\nan example of how you would use them with NFS in a real environment so\nI understand the problem better? I'd start by assuming a simple setup\nwith AUTH_SYS and no NFSv4 idmapping involved, since that case should\nbe fairly straightforward.\n\nMixing in AUTH_GSS and real idmapping will be where things get harder,\nso let's not worry about those cases for now.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Alexander Mikhalitsyn",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Alexander Mikhalitsyn (author)",
              "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, but did not provide a clear resolution signal.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "agreed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Jeff,\n\nthanks for such a fast reply! ;)",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Alexander Mikhalitsyn (author)",
              "summary": "The author is addressing a concern about the use case for VFS idmappings in NFS, specifically how it would work when delegating filesystems to containers. The author explains that their project uses idmapped mounts as a way to 'delegate' filesystems to containers and applies the VFS idmap equal to the container's user namespace, making the behavior described by Jeff Layton possible.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "For me, from the point of LXC/Incus project, idmapped mounts are used as\na way to \"delegate\" filesystems (or subtrees) to the containers:\n1. We, of course, assume that container enables user namespaces and\nuser can't mount a filesystem\ninside because it has no FS_USERNS_MOUNT flag set (like in case of Cephfs, NFS,\nCIFS and many others).\n2. At the same time host's system administrator wants to avoid\nremapping between container's user ns and\nsb->s_user_ns (which is init_user_ns for those filesystems). [\nmotivation here is that in many\ncases you may want to have the same subtree to be shared with other\ncontainers and even host users too and\nyou want UIDs to be \"compatible\", i.e UID 1000 in one container and\nUID 1000 in another container should\nland as UID 1000 on the filesystem's inode ]\n\nFor this usecase, when we bind-mount filesystem to container, we apply\nVFS idmap equal to container's\nuser namespace. This makes a behavior I described.\n\nBut this is just one use case. I'm pretty sure there are some more\naround here :)\nI know that folks from Preferred Networks (preferred.jp) are also\ninterested in VFS idmap support in NFS,\nprobably they can share some ideas/use cases too.",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Alexander Mikhalitsyn (author)",
              "summary": "Author acknowledged that the NFS server needs to handle path-based UID/GID restrictions, which is a challenge for VFS idmappings support in NFS, and agreed to discuss this further.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a challenge",
                "agreed to discuss"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Kind regards,\nAlex",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Trond Myklebust",
              "summary": "Reviewer noted that strong authentication in NFS and RPC protocols defeats client-side idmapping schemes because the server cannot know the client's UIDs or GIDs, and suggested that a generic implementation would require exchanging and storing information between client and server to allow file owner/group owner mapping.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "protocol extensions",
                "security model"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you do need to worry about those cases. As the NFS and RPC\nprotocols stand today, strong authentication will defeat any client\nside idmapping scheme, because the server can't know what uids or gids\nthe client is using on its end; it just knows about the account that\nwas used to authenticate.\n\nI think if you do want to implement something generic, you're going to\nhave to consider how the client and server can exchange (and store) the\ninformation needed to allow the client to perform the mapping of file\nowners/group owners on its end. The client would presumably also need\nto be in charge of enforcing permissions for such mappings.\nIt would be a very different security model than the one used by NFS\ntoday, and almost certainly require protocol extensions.\n\n-- \nTrond Myklebust\nLinux NFS client maintainer, Hammerspace\ntrondmy@kernel.org, trond.myklebust@hammerspace.com",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton expressed concern about the complexity of implementing VFS idmappings support in NFS, noting that multiple layers could be involved and requesting a clear understanding of a simple use-case before introducing additional complexities.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "complexity",
                "multiple layers"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oh, we absolutely need to worry about them, but this is a difficult\ntopic to get our arms around. We can potentially have several layers\nthat are doing idmapping, so I want to understand a simple use-case\nfirst. Once that's clear I plan to start throwing in monkey wrenches.",
              "reply_to": "Trond Myklebust",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer Jeff Layton expressed uncertainty about the use-case for VFS idmappings in NFS, suggesting that shifting UIDs at a higher level might be sufficient without modifying the protocol.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "lack of understanding"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That may be, but I still don't fully understand the use-case here.\nMaybe they'd be content with just shifting UIDs at a higher level\nwithout changing the protocol? Without understanding how they intend to\nuse this, it's hard to know what's needed.\n\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Trond Myklebust",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Alexander Mikhalitsyn (author)",
              "summary": "Author is responding to Jeff Layton's feedback, acknowledging that their earlier reply may not have clarified the LXC/Incus use case and offering to provide a more detailed explanation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification sought",
                "more information offered"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Please, let me know if my earlier reply doesn't clarify LXC/Incus use case.\nI can prepare a more detailed explanation with command line/configuration\nexamples with pleasure.",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Alexander Mikhalitsyn (author)",
              "summary": "The author acknowledges that the approach of keeping NFS protocol changes to a minimum was not acceptable in the past, but is open to revisiting it and proposes an iterative approach where the initial version can be simple and then more complex cases can be added later.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no_problem",
                "look_positively"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If you ask me, I have no problem or I would say more, I look positively\non the way \"keep it high level & don't touch NFS protocol\" ;-)\nBut I remember a very tight discussion (good context [1]) about Cephfs and\nthis way wasn't considered as acceptable back then (and we had to make\na protocol extension).\nWe can always go iteratively, and first version can be simple and then on-demand\nwe can support more tricky cases if this is acceptable for you guys.\nYou set the rules. ;-)\n\n[1] https://lore.kernel.org/lkml/f3864ed6-8c97-8a7a-f268-dab29eb2fb21@redhat.com/\n\nKind regards,\nAlex",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "The reviewer noted that for NFS, idmapping can be achieved by converting RPC credentials at the client layer, and suggested this approach as an alternative to requiring a protocol extension.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "alternative solution",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Ok: so you have a process running in a userns as UID 2000 and you want\nto use vfs layer idmapping so that when you create a file as that user\nthat it ends up being owned by UID 1000. Is that basically correct?\n\nTypically, the RPC credentials used in an OPEN or CREATE call is what\ndetermines its ownership (at least until a SETATTR comes in). With\nAUTH_SYS, the credential is just a uid and set of gids.\n\nSo in this case, it sounds like you would need just do that conversion\n(maybe at the RPC client layer?) when issuing an RPC. You don't really\nneed a protocol extension for that case.\n\nAs Trond points out though, AUTH_GSS and NFSv4 idmapping will make this\nmore complex. Once you're using kerberos credentials for\nauthentication, you don't have much control over what the UIDs and GIDs\nwill be on newly-created files, but is that really a problem? As long\nas all of the clients have a consistent view, I wouldn't think so.",
              "reply_to": "Alexander Mikhalitsyn",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton",
              "summary": "Reviewer suggested focusing on a single simple problem in VFS idmappings support for NFS, rather than trying to tackle multiple use-cases at once.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear technical objection or suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, we don't want to focus too much on a single use-case, but I find\nit helpful to focus on a single simple problem first.\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "Alexander Mikhalitsyn",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Alexander Mikhalitsyn (author)",
              "summary": "The author is addressing a concern about how VFS idmaps interact with NFS mounts and user namespaces, explaining that the UID mapping works as expected in their specific scenario but acknowledging that the issue of 'caller UID/GID are needed everywhere' still needs to be addressed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledging a technical challenge",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "In our case, we have a UID 1000 (inside user namespace), which mapped to\nsomething like 10000 + 1000 (in the init_user_ns). And then we have\nNFS mount (sb->s_user_ns = init_user_ns, ofc), so if user UID 1000\n(inside the container)\ncreates a file, it will be 11000, right? But we do bind-mount of that\nNFS mount+VFS idmap,\nso that once file is created it has owner_uid = 1000. (This scenario\nis covered by [1] and [2])\n\n[1] https://docs.kernel.org/filesystems/idmappings.html#example-3\n[2] https://docs.kernel.org/filesystems/idmappings.html#example-3-reconsidered",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Alexander Mikhalitsyn (author)",
              "summary": "Author agrees to provide a starting point for the NFS implementation by preparing RFC patches for a simple case, indicating a willingness to move forward with the patch series.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreement",
                "willingness to progress"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, I could prepare RFC patches before LSF/MM/BPF for that simple case so\nwe have something to start with.",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "NeilBrown",
              "summary": "The reviewer pointed out that when using krb5 and NFSv3, the mapping between krb5 identity and uid should be consistent on both client and server to ensure correct ownership of files. They also noted that for NFSv4 and idmapper, the kernel idmapping is not needed and the idmapper daemon should run in user-namespace to map from on-the-wire names to app-level uids.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You also need to consider the conversion when receiving an RPC.\n\nIf you use krb5 and NFSv3 then you really want the mapping between krb5\nidentity and uid to be the same on client and server, so then when an\napplication creates a file and the stats it, it sees that it owns it.\n\nIf I use a krb5 identity in an idmapped NFS filesystem I'll want the\nserver to map the identity to the \"underlying\" uid (was would be stored\nin a local filesystem) and then when the client gets a GETATTR reply,\nthe VFS maps back to the uid seen by the application.\n\nWith NFSv4 and the idmapper you wouldn't need (or want) the kernel\nidmapping to be used at all.  You would want the idmapper deamon to run\nin the user-namespace and map from on-the-wire names to the appropriate\napp-level uids.\nThis would mean that a given NFS mount would need to be an a given user\nnamespace.  Maybe that isn't desired.\n\nIf it is important for a given NFS mount to be available in multiple\nuser namespaces, then the idmapper daemon would need to map to the\nunderlying uid, and the VFS mapping would map that up to the app-level\nuid.\n\nNeilBrown",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kohei Sugihara",
              "summary": "Reviewer Kohei Sugihara expressed interest in the proposed VFS idmappings support in NFS, indicating that his team has a relevant use case they would like to discuss.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no specific technical concerns or suggestions"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Thank you Alex for the proposal and quick follow-ups. We're really\ninterested in this feature and we'd like to share our use case.",
              "reply_to": "NeilBrown",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kohei Sugihara",
              "summary": "Kohei Sugihara noted that NFS is complex due to its variety of mount options and security features, making it challenging to support ID-mapping. He proposed two possible models for handling UID/GID: (a) the NFS client sends the container's UID/GID on the wire and the server stores them as such, or (b) the server stores the host's UID/GID and the client/VFS maps it to the container's UID/GID. Sugihara expressed a preference for model (a) but acknowledged potential security/policy implications.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "complexity",
                "security features"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Our use case is running multi-tenant Kubernetes clusters with\nKubernetes User Namespaces [1]. Basically we need to share a single\nstorage endpoint among multiple pods using ReadWriteMany (RWX) access\nmode. Implementations that support both RWX and ID-mapped mount are\nlimited [2].\n\nNFS is operationally common, so I am interested in supporting NFS for\nID-mapping, but NFS is complex due to its variety of mount options and\nsecurity features as Trond mentioned. We'd like to share our use case\nand define the minimum goal. Our goal is here:\n\n- 1: Mount the same NFS export as a persistent volume from multiple\nKubernetes Pods running on different compute nodes. Each tenant has\nits own exports.\n- 2: UID/GID in a container in the pod can be configurable to an\narbitrary value by runAsUser/runAsGroup (e.g. runAsUser/Group is set\nto 1000).\n- 3: We can access the export from the container as 1000:1000. At\nminimum, ownership should be consistent from the container view (i.e.\nstat shows 1000:1000 for files that the container creates). Today,\nID-mapped mount does not support NFS. The NFS client ends up using the\nhost-mapped uid/gid (e.g. container 1000 becomes host 11000), so the\ncontainer view becomes inconsistent across nodes.\n\nThere are (at least) two possible models here:\na) the NFS client sends 1000:1000 on the wire and the server stores\n1000:1000 (so server-side ownership matches the container uid/gid), or\nb) the server stores the host uid/gid (e.g. 11000:11000) and the\nclient/VFS maps it so that the container still sees 1000:1000.\nMy intuition is that (a) is simpler for a multi-node RWX setup, but it\nmay have security / policy implications depending on how the server\ndoes authorization (especially with sec=sys). I think its worth\ndiscussing what the safe and reasonable minimum should be.\n\nIn this case, UID/GID in the host node is not deterministic for the\nprocess in the container due to user_namespaces(7), so we need to do\nID-mapping to unify UID/GID between container and file system. Also,\nwe likely need to consider both request and reply paths (e.g. GETATTR)\nto keep the view consistent.",
              "reply_to": "NeilBrown",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Kohei Sugihara",
              "summary": "Reviewer noted that the proposed ID-mapping solution may violate NFS protocol requirements and suggested starting with a small case to test its feasibility, while also considering client-side implementation to cover existing server implementations.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "concerns about consistency"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Neil, thank you for your comment. We initially expected it to be in\nNFSv4. I totally agree with you and exactly our concern is how do we\nmake it consistent with idmapd(8). In the Kubernetes case, we cannot\npass CAP_SYS_ADMIN to allow pods to mount NFS directly, so mount will\nbe done on the host. As you mentioned, we think we can share a single\nNFS export from multiple hosts and pods, so I think introducing\nID-mapping into the VFS layer (with referencing local id-mapping\ntable) is appropriate.\n\nWe can start by picking a small case. My concern was whether this\ncould violate NFS protocol or not, whether things can be done on the\nclient side or not, and this topic is suitable for dealing with this\nas the VFS community. If things can be done on the client side, we can\ncover existing NFS server implementations (e.g. OpenZFS, proprietary\nappliances). I believe this can be applied to recent containerized\nruntime environments, even this small working set.\n\nAdding more context, Kubernetes and the container community actively\nwork on host isolation using the Linux user namespace feature.\nRecently they experienced RCE vulnerabilities on container runtime but\nit could be mitigated by host isolation using the user namespace\nisolation [3]. Along with migrating the runtime environment to user\nnamespace, extending file system support will be worth discussing.\n\nKind regards,\nKohei\n\n[1] https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/\n[2] https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes\n[3] https://lpc.events/event/19/contributions/2065/",
              "reply_to": "NeilBrown",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Demi Obenour",
              "summary": "The reviewer noted that the secure case of NFS has similar problems to unprivileged virtiofsd on a system without user namespaces, where there's no way to store files with the UID/GID specified by VFS due to security restrictions. They proposed storing mapped UID and GID as a user.* xattr as a workaround, which requires no special permissions and scales to NFS servers with over 2^16 users.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The secure case (strong authentication) has similar problems to\nunprivileged virtiofsd on a system with user namespaces disabled.\nIn both cases, there is no way to store the files with the UID/GID/etc\nthat the VFS says they should have.  The server (NFS) or kernel\n(virtiofsd) simply will not (and, for security reasons, *must not*)\nallow this.\n\nI proposed a workaround for virtiofsd [1] that I will also propose\nhere: store the mapped UID and GID as a user.* xattr.  This requires\nno special permissions, and so it completely solves this problem.\nIt is also the only solution I know of that scales to NFS servers\nwith over 2^16 users, which might well exist.\n\nThe only better solution I can think of is to replace the numeric\nUID/GID with hierarchical identifier, such as a Windows-style SID.\nThose are much more complex, though.\n\n[1]: https://gitlab.com/virtio-fs/virtiofsd/-/issues/225\n-- \nSincerely,\nDemi Marie Obenour (she/her/hers)",
              "reply_to": "Alexander Mikhalitsyn",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH nfs-utils v2 0/4] nfsdctl: properly handle older kernels that don't support min-threads",
          "message_id": "3c9dbad30b43bc02e07d8e2a8af31702eb793366.camel@kernel.org",
          "url": "https://lore.kernel.org/all/3c9dbad30b43bc02e07d8e2a8af31702eb793366.camel@kernel.org/",
          "date": "2026-02-18T14:19:14Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about the dependency on system headers for min-threads support, explaining that maintaining their own copy of nfsd_netlink.h allows them to unconditionally compile in this feature.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I originally had this depend on the system header, but if we maintain\nour copy of nfsd_netlink.h in tree, then we can unconditionally compile\nin support for the MIN_THREADS option.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n configure.ac                 |  6 ++----\n utils/nfsdctl/nfsd_netlink.h |  2 ++\n utils/nfsdctl/nfsdctl.c      | 16 +---------------\n 3 files changed, 5 insertions(+), 19 deletions(-)\n\ndiff --git a/configure.ac b/configure.ac\nindex 5cc1e9186542c975abf200edbef30bc8f6ecb8ee..a65c7837b8cb72eaa2f3dbb89069599c074be4ec 100644\n--- a/configure.ac\n+++ b/configure.ac\n@@ -262,12 +262,10 @@ AC_ARG_ENABLE(nfsdctl,\n \t\tPKG_CHECK_MODULES(LIBNLGENL3, libnl-genl-3.0 >= 3.1)\n \t\tPKG_CHECK_MODULES(LIBREADLINE, readline)\n \t\tAC_CHECK_HEADERS(linux/nfsd_netlink.h)\n-\t\tAC_CHECK_DECLS([NFSD_A_SERVER_MIN_THREADS], , ,\n-\t\t\t       [#include <linux/nfsd_netlink.h>])\n \n-\t\t# ensure we have the pool-mode commands\n+\t\t# ensure we have the MIN_THREADS attribute\n \t\tAC_COMPILE_IFELSE([AC_LANG_PROGRAM([[#include <linux/nfsd_netlink.h>]],\n-\t\t\t\t                   [[int foo = NFSD_CMD_POOL_MODE_GET;]])],\n+\t\t\t\t                   [[int foo = NFSD_A_SERVER_MIN_THREADS;]])],\n \t\t\t\t   [AC_DEFINE([USE_SYSTEM_NFSD_NETLINK_H], 1,\n \t\t\t\t\t      [\"Use system's linux/nfsd_netlink.h\"])])\n \t\tAC_COMPILE_IFELSE([AC_LANG_PROGRAM([[#include <linux/lockd_netlink.h>]],\ndiff --git a/utils/nfsdctl/nfsd_netlink.h b/utils/nfsdctl/nfsd_netlink.h\nindex 887cbd12b695f2398c96976ba2d70e68ee0d93c0..e9efbc9e63d83ed25fcd790b7a877c0023638f15 100644\n--- a/utils/nfsdctl/nfsd_netlink.h\n+++ b/utils/nfsdctl/nfsd_netlink.h\n@@ -2,6 +2,7 @@\n /* Do not edit directly, auto-generated from: */\n /*\tDocumentation/netlink/specs/nfsd.yaml */\n /* YNL-GEN uapi header */\n+/* To regenerate run: tools/net/ynl/ynl-regen.sh */\n \n #ifndef _UAPI_LINUX_NFSD_NETLINK_H\n #define _UAPI_LINUX_NFSD_NETLINK_H\n@@ -34,6 +35,7 @@ enum {\n \tNFSD_A_SERVER_GRACETIME,\n \tNFSD_A_SERVER_LEASETIME,\n \tNFSD_A_SERVER_SCOPE,\n+\tNFSD_A_SERVER_MIN_THREADS,\n \n \t__NFSD_A_SERVER_MAX,\n \tNFSD_A_SERVER_MAX = (__NFSD_A_SERVER_MAX - 1)\ndiff --git a/utils/nfsdctl/nfsdctl.c b/utils/nfsdctl/nfsdctl.c\nindex c906a2c8ba6d357e182d341a30610e367e74c093..6b3c98009488d1687e7e751eaed6c4f1d9613d39 100644\n--- a/utils/nfsdctl/nfsdctl.c\n+++ b/utils/nfsdctl/nfsdctl.c\n@@ -324,11 +324,9 @@ static void parse_threads_get(struct genlmsghdr *gnlh)\n \t\tcase NFSD_A_SERVER_THREADS:\n \t\t\tpool_threads[i++] = nla_get_u32(attr);\n \t\t\tbreak;\n-#if HAVE_DECL_NFSD_A_SERVER_MIN_THREADS\n \t\tcase NFSD_A_SERVER_MIN_THREADS:\n \t\t\tprintf(\"min-threads: %u\\n\", nla_get_u32(attr));\n \t\t\tbreak;\n-#endif\n \t\tdefault:\n \t\t\tbreak;\n \t\t}\n@@ -546,10 +544,8 @@ static int threads_doit(struct nl_sock *sock, int cmd, int grace, int lease,\n \t\t\tnla_put_u32(msg, NFSD_A_SERVER_LEASETIME, lease);\n \t\tif (scope)\n \t\t\tnla_put_string(msg, NFSD_A_SERVER_SCOPE, scope);\n-#if HAVE_DECL_NFSD_A_SERVER_MIN_THREADS\n \t\tif (minthreads >= 0)\n \t\t\tnla_put_u32(msg, NFSD_A_SERVER_MIN_THREADS, minthreads);\n-#endif\n \t\tfor (i = 0; i < pool_count; ++i)\n \t\t\tnla_put_u32(msg, NFSD_A_SERVER_THREADS, pool_threads[i]);\n \t}\n@@ -591,24 +587,16 @@ out:\n static void threads_usage(void)\n {\n \tprintf(\"Usage: %s threads { --min-threads=X } [ pool0_count ] [ pool1_count ] ...\\n\", taskname);\n-#if HAVE_DECL_NFSD_A_SERVER_MIN_THREADS\n \tprintf(\"    --min-threads= set the minimum thread count per pool to value\\n\");\n-#endif\n \tprintf(\"    pool0_count: thread count for pool0, etc...\\n\");\n \tprintf(\"Omit any arguments to show current thread counts.\\n\");\n }\n \n-#if HAVE_DECL_NFSD_A_SERVER_MIN_THREADS\n static const struct option threads_options[] = {\n \t{ \"help\", no_argument, NULL, 'h' },\n \t{ \"min-threads\", required_argument, NULL, 'm' },\n \t{ },\n };\n-#define THREADS_OPTSTRING \"hm:\"\n-#else\n-#define threads_options help_only_options\n-#define THREADS_OPTSTRING \"h\"\n-#endif\n \n static int threads_func(struct nl_sock *sock, int argc, char **argv)\n {\n@@ -618,12 +606,11 @@ static int threads_func(struct nl_sock *sock, int argc, char **argv)\n \tint opt, pools = 0;\n \n \toptind = 1;\n-\twhile ((opt = getopt_long(argc, argv, THREADS_OPTSTRING, threads_options, NULL)) != -1) {\n+\twhile ((opt = getopt_long(argc, argv, \"hm:\", threads_options, NULL)) != -1) {\n \t\tswitch (opt) {\n \t\tcase 'h':\n \t\t\tthreads_usage();\n \t\t\treturn 0;\n-#if HAVE_DECL_NFSD_A_SERVER_MIN_THREADS\n \t\tcase 'm':\n \t\t\terrno = 0;\n \t\t\tminthreads = strtoul(optarg, NULL, 0);\n@@ -632,7 +619,6 @@ static int threads_func(struct nl_sock *sock, int argc, char **argv)\n \t\t\t\treturn 1;\n \t\t\t}\n \t\t\tbreak;\n-#endif\n \t\t}\n \t}\n \n\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-02-04",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author is addressing a concern about resolving the string name to an id for every netlink call, which was inefficient and resulted in repeated calls to genl_ctrl_resolve(). The author has restructured the code to resolve family names once and keep them, improving performance.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged improvement",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The current code resolves the string name to an id for every netlink\ncall. Just resolve the family names once and keep them.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n utils/nfsdctl/nfsdctl.c | 83 ++++++++++++++++++++++++++++++++++++++-----------\n 1 file changed, 64 insertions(+), 19 deletions(-)\n\ndiff --git a/utils/nfsdctl/nfsdctl.c b/utils/nfsdctl/nfsdctl.c\nindex 6b3c98009488d1687e7e751eaed6c4f1d9613d39..86a4a944d4e131f1114ca358d81779de0a034872 100644\n--- a/utils/nfsdctl/nfsdctl.c\n+++ b/utils/nfsdctl/nfsdctl.c\n@@ -46,9 +46,11 @@\n #include \"conffile.h\"\n #include \"xlog.h\"\n \n-/* compile note:\n- * gcc -I/usr/include/libnl3/ -o <prog-name> <prog-name>.c -lnl-3 -lnl-genl-3\n- */\n+/* The index of the \"lockd\" netlink family */\n+static int lockd_nl_family;\n+\n+/* The index of the \"nfsd\" netlink family */\n+static int nfsd_nl_family;\n \n struct nfs_version {\n \tuint8_t\tmajor;\n@@ -433,24 +435,18 @@ static struct nl_sock *netlink_sock_alloc(void)\n \treturn sock;\n }\n \n-static struct nl_msg *netlink_msg_alloc(struct nl_sock *sock, const char *family)\n+static struct nl_msg *netlink_msg_alloc(struct nl_sock *sock, int family)\n {\n \tstruct nl_msg *msg;\n \tint id;\n \n-\tid = genl_ctrl_resolve(sock, family);\n-\tif (id < 0) {\n-\t\txlog(L_ERROR, \"failed to resolve %s generic netlink family\", family);\n-\t\treturn NULL;\n-\t}\n-\n \tmsg = nlmsg_alloc();\n \tif (!msg) {\n \t\txlog(L_ERROR, \"failed to allocate netlink message\");\n \t\treturn NULL;\n \t}\n \n-\tif (!genlmsg_put(msg, 0, 0, id, 0, 0, 0, 0)) {\n+\tif (!genlmsg_put(msg, 0, 0, family, 0, 0, 0, 0)) {\n \t\txlog(L_ERROR, \"failed to add generic netlink headers to netlink message\");\n \t\tnlmsg_free(msg);\n \t\treturn NULL;\n@@ -459,6 +455,31 @@ static struct nl_msg *netlink_msg_alloc(struct nl_sock *sock, const char *family\n \treturn msg;\n }\n \n+static int resolve_family(struct nl_sock *sock, const char *name)\n+{\n+\tint family = genl_ctrl_resolve(sock, name);\n+\n+\tif (family < 0) {\n+\t\txlog(L_ERROR, \"failed to resolve %s generic netlink family: %d\", name, family);\n+\t\tfamily = 0;\n+\t}\n+\treturn family;\n+}\n+\n+static int lockd_nl_family_setup(struct nl_sock *sock)\n+{\n+\tif (!lockd_nl_family)\n+\t\tlockd_nl_family = resolve_family(sock, LOCKD_FAMILY_NAME);\n+\treturn lockd_nl_family;\n+}\n+\n+static int nfsd_nl_family_setup(struct nl_sock *sock)\n+{\n+\tif (!nfsd_nl_family)\n+\t\tnfsd_nl_family = resolve_family(sock, NFSD_FAMILY_NAME);\n+\treturn nfsd_nl_family;\n+}\n+\n static void status_usage(void)\n {\n \tprintf(\"Usage: %s status\\n\", taskname);\n@@ -482,7 +503,10 @@ static int status_func(struct nl_sock *sock, int argc, char ** argv)\n \t\t}\n \t}\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -530,7 +554,10 @@ static int threads_doit(struct nl_sock *sock, int cmd, int grace, int lease,\n \tstruct nl_cb *cb;\n \tint ret;\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -660,7 +687,10 @@ static int fetch_nfsd_versions(struct nl_sock *sock)\n \tstruct nl_cb *cb;\n \tint ret;\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -725,7 +755,10 @@ static int set_nfsd_versions(struct nl_sock *sock)\n \tstruct nl_cb *cb;\n \tint i, ret;\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -906,7 +939,10 @@ static int fetch_current_listeners(struct nl_sock *sock)\n \tstruct nl_cb *cb;\n \tint ret;\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -1151,7 +1187,10 @@ static int set_listeners(struct nl_sock *sock)\n \tstruct nl_cb *cb;\n \tint i, ret;\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -1272,7 +1311,10 @@ static int pool_mode_doit(struct nl_sock *sock, int cmd, const char *pool_mode)\n \tstruct nl_cb *cb;\n \tint ret;\n \n-\tmsg = netlink_msg_alloc(sock, NFSD_FAMILY_NAME);\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, nfsd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n@@ -1365,7 +1407,10 @@ static int lockd_config_doit(struct nl_sock *sock, int cmd, int grace, int tcppo\n \t\t\treturn 0;\n \t}\n \n-\tmsg = netlink_msg_alloc(sock, LOCKD_FAMILY_NAME);\n+\tif (!lockd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, lockd_nl_family);\n \tif (!msg)\n \t\treturn 1;\n \n\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-02-04",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about handling unknown attributes in genetlink by explaining that it will reject them by default, and thus userland needs to handle this case properly. The author described how the patch series adds a mechanism for nfsdctl to query the kernel for supported attributes and use that information to determine whether to pass down the min-threads attribute or report an error.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ben reported a problem when using new nfs-utils with an old kernel that\ndoesn't support the min-threads setting. While netlink is an extensible\nformat, genetlink (which we are using) will reject unknown attributes by\ndefault with -EINVAL.\n\nWe could fix this in the kernel by having it ignore unknown attributes,\nbut there is no way to fix old kernels and silently ignoring it is less\nthan ideal. By handling this in userland, we can properly error out when\nthe kernel doesn't support this attribute.\n\nWhen starting, have nfsdctl query the kernel for the \"policy\" of the\nthreads operation, and determine the highest attribute index it\nsupports.  For the \"threads\" command, have it fail if the --min-threads\noption is passed and the kernel doesn't support it. For \"autostart\", log\na warning and ignore the setting.\n\nFixes: 00e2e62b8998 (\"nfsdctl: add support for min-threads parameter\")\nReported-by: Ben Coddington <bcodding@hammerspace.com>\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n utils/nfsdctl/nfsdctl.c | 95 ++++++++++++++++++++++++++++++++++++++++++++++++-\n 1 file changed, 94 insertions(+), 1 deletion(-)\n\ndiff --git a/utils/nfsdctl/nfsdctl.c b/utils/nfsdctl/nfsdctl.c\nindex 86a4a944d4e131f1114ca358d81779de0a034872..4a3744a1c22e6beac7c039bded05fc087a121200 100644\n--- a/utils/nfsdctl/nfsdctl.c\n+++ b/utils/nfsdctl/nfsdctl.c\n@@ -52,6 +52,9 @@ static int lockd_nl_family;\n /* The index of the \"nfsd\" netlink family */\n static int nfsd_nl_family;\n \n+/* The highest attribute index supported by NFSD_CMD_THREADS_SET on this kernel */\n+int nfsd_threads_max_nlattr;\n+\n struct nfs_version {\n \tuint8_t\tmajor;\n \tuint8_t\tminor;\n@@ -480,6 +483,83 @@ static int nfsd_nl_family_setup(struct nl_sock *sock)\n \treturn nfsd_nl_family;\n }\n \n+static int getpolicy_handler(struct nl_msg *msg, void *arg)\n+{\n+\tstruct genlmsghdr *gnlh = nlmsg_data(nlmsg_hdr(msg));\n+\tstruct nlattr *attr;\n+\tint rem;\n+\n+\tnla_for_each_attr(attr, genlmsg_attrdata(gnlh, 0), genlmsg_attrlen(gnlh, 0), rem) {\n+\t\tstruct nlattr *a, *b;\n+\t\tint i, j, index;\n+\n+\t\tif (nla_type(attr) == CTRL_ATTR_POLICY) {\n+\t\t\tnla_for_each_nested(a, attr, i) {\n+\t\t\t\tnla_for_each_nested(b, a, j) {\n+\t\t\t\t\tint idx = nla_type(b);\n+\n+\t\t\t\t\tif (nfsd_threads_max_nlattr < idx)\n+\t\t\t\t\t\tnfsd_threads_max_nlattr = idx;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn NL_SKIP;\n+}\n+\n+static int query_nfsd_nl_policy(struct nl_sock *sock)\n+{\n+\tstruct genlmsghdr *ghdr;\n+\tstruct nlmsghdr *nlh;\n+\tstruct nl_msg *msg;\n+\tstruct nl_cb *cb;\n+\tint opt, ret, id;\n+\n+\tif (!nfsd_nl_family_setup(sock))\n+\t\treturn 1;\n+\n+\tmsg = netlink_msg_alloc(sock, GENL_ID_CTRL);\n+\tif (!msg)\n+\t\treturn 1;\n+\n+\tnlh = nlmsg_hdr(msg);\n+\tnlh->nlmsg_flags |= NLM_F_DUMP;\n+\tghdr = nlmsg_data(nlh);\n+\tghdr->cmd = CTRL_CMD_GETPOLICY;\n+\n+\tcb = nl_cb_alloc(NL_CB_CUSTOM);\n+\tif (!cb) {\n+\t\txlog(L_ERROR, \"failed to allocate netlink callbacks\");\n+\t\tret = 1;\n+\t\tgoto out;\n+\t}\n+\n+\tnla_put_u16(msg, CTRL_ATTR_FAMILY_ID, nfsd_nl_family);\n+\tnla_put_u32(msg, CTRL_ATTR_OP, NFSD_CMD_THREADS_SET);\n+\n+\tret = nl_send_auto(sock, msg);\n+\tif (ret < 0)\n+\t\tgoto out_cb;\n+\n+\tret = 1;\n+\tnl_cb_err(cb, NL_CB_CUSTOM, error_handler, &ret);\n+\tnl_cb_set(cb, NL_CB_FINISH, NL_CB_CUSTOM, finish_handler, &ret);\n+\tnl_cb_set(cb, NL_CB_ACK, NL_CB_CUSTOM, ack_handler, &ret);\n+\tnl_cb_set(cb, NL_CB_VALID, NL_CB_CUSTOM, getpolicy_handler, NULL);\n+\n+\twhile (ret > 0)\n+\t\tnl_recvmsgs(sock, cb);\n+\tif (ret < 0) {\n+\t\txlog(L_ERROR, \"Error: %s\", strerror(-ret));\n+\t\tret = 1;\n+\t}\n+out_cb:\n+\tnl_cb_put(cb);\n+out:\n+\tnlmsg_free(msg);\n+\treturn ret;\n+}\n+\n static void status_usage(void)\n {\n \tprintf(\"Usage: %s status\\n\", taskname);\n@@ -639,6 +719,11 @@ static int threads_func(struct nl_sock *sock, int argc, char **argv)\n \t\t\tthreads_usage();\n \t\t\treturn 0;\n \t\tcase 'm':\n+\t\t\tif (nfsd_threads_max_nlattr < NFSD_A_SERVER_MIN_THREADS) {\n+\t\t\t\txlog(L_ERROR, \"This kernel does not support dynamic threading.\");\n+\t\t\t\treturn 1;\n+\t\t\t}\n+\n \t\t\terrno = 0;\n \t\t\tminthreads = strtoul(optarg, NULL, 0);\n \t\t\tif (minthreads == ULONG_MAX && errno != 0) {\n@@ -1743,7 +1828,12 @@ static int autostart_func(struct nl_sock *sock, int argc, char ** argv)\n \n \tlease = conf_get_num(\"nfsd\", \"lease-time\", 0);\n \tscope = conf_get_str(\"nfsd\", \"scope\");\n-\tminthreads = conf_get_num(\"nfsd\", \"min-threads\", 0);\n+\tminthreads = conf_get_num(\"nfsd\", \"min-threads\", -1);\n+\n+\tif (minthreads >= 0 && nfsd_threads_max_nlattr < NFSD_A_SERVER_MIN_THREADS) {\n+\t\txlog(L_WARNING, \"This kernel does not support dynamic threading. min-threads setting ignored.\");\n+\t\tminthreads = -1;\n+\t}\n \n \tret = threads_doit(sock, NFSD_CMD_THREADS_SET, grace, lease, pools,\n \t\t\t   threads, scope, minthreads);\n@@ -1936,6 +2026,9 @@ int main(int argc, char **argv)\n \t\treturn 1;\n \t}\n \n+\tif (query_nfsd_nl_policy(sock))\n+\t\treturn 1;\n+\n \tif (optind > argc) {\n \t\tusage();\n \t\treturn 1;\n\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-02-04",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "The author addressed a concern about inconsistent error messages in the lockd_config_doit function, agreeing to change the log statements to remove the trailing newline characters and exclamation marks.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed to make changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Signed-off-by: Jeff Layton <jlayton@kernel.org>\n---\n utils/nfsdctl/nfsdctl.c | 10 +++++-----\n 1 file changed, 5 insertions(+), 5 deletions(-)\n\ndiff --git a/utils/nfsdctl/nfsdctl.c b/utils/nfsdctl/nfsdctl.c\nindex 4a3744a1c22e6beac7c039bded05fc087a121200..2b01f705874a4a3cad04042f6dfad22a66a7536f 100644\n--- a/utils/nfsdctl/nfsdctl.c\n+++ b/utils/nfsdctl/nfsdctl.c\n@@ -1514,14 +1514,14 @@ static int lockd_config_doit(struct nl_sock *sock, int cmd, int grace, int tcppo\n \n \tcb = nl_cb_alloc(NL_CB_CUSTOM);\n \tif (!cb) {\n-\t\txlog(L_ERROR, \"failed to allocate netlink callbacks\\n\");\n+\t\txlog(L_ERROR, \"failed to allocate netlink callbacks\");\n \t\tret = 1;\n \t\tgoto out;\n \t}\n \n \tret = nl_send_auto(sock, msg);\n \tif (ret < 0) {\n-\t\txlog(L_ERROR, \"send failed (%d)!\\n\", ret);\n+\t\txlog(L_ERROR, \"send failed (%d)!\", ret);\n \t\tgoto out_cb;\n \t}\n \n@@ -1534,7 +1534,7 @@ static int lockd_config_doit(struct nl_sock *sock, int cmd, int grace, int tcppo\n \twhile (ret > 0)\n \t\tnl_recvmsgs(sock, cb);\n \tif (ret < 0) {\n-\t\txlog(L_ERROR, \"Error: %s\\n\", strerror(-ret));\n+\t\txlog(L_ERROR, \"Error: %s\", strerror(-ret));\n \t\tret = 1;\n \t}\n out_cb:\n@@ -1554,7 +1554,7 @@ static int get_service(const char *svc)\n \n \tret = getaddrinfo(NULL, svc, &hints, &res);\n \tif (ret) {\n-\t\txlog(L_ERROR, \"getaddrinfo of \\\"%s\\\" failed: %s\\n\",\n+\t\txlog(L_ERROR, \"getaddrinfo of \\\"%s\\\" failed: %s\",\n \t\t\tsvc, gai_strerror(ret));\n \t\treturn -1;\n \t}\n@@ -1575,7 +1575,7 @@ static int get_service(const char *svc)\n \t\t}\n \t\tbreak;\n \tdefault:\n-\t\txlog(L_ERROR, \"Bad address family: %d\\n\", res->ai_family);\n+\t\txlog(L_ERROR, \"Bad address family: %d\", res->ai_family);\n \t\tport = -1;\n \t}\n \tfreeaddrinfo(res);\n\n-- \n2.52.0",
              "reply_to": "",
              "message_date": "2026-02-04",
              "analysis_source": "llm"
            },
            {
              "author": "Jeff Layton (author)",
              "summary": "Author is addressing a request for status update on the patch series, stating that it's ready to be merged.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "request_for_status_update"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Steve, ping? Can we get this merged?\n\nThanks,\n-- \nJeff Layton <jlayton@kernel.org>",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Steve Dickson",
              "summary": "Reviewer noted that the patch does not handle the case where the kernel does not support min-threads, and suggested adding a check to return an error in this case.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm on it... thanks for the ping!\n\nsteved.",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Benjamin Coddington",
              "summary": "Reviewer provided Reviewed-by and Tested-by tags, confirming that the patches were reviewed and tested by them.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by",
                "Tested-by"
              ],
              "raw_body": "I mean to send R-b and T-b on these - because yes, both were done!\n\nReviewed-by: Benjamin Coddington <bcodding@hammerspace.com>\nTested-by: Benjamin Coddington <bcodding@hammerspace.com>\n\nBen",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [
        {
          "activity_type": "discussion_posted",
          "subject": "[LSF/MM/BPF TOPIC] Should we make inode->i_ino a u64?",
          "message_id": "08f8444c7237566ffb4ba8c9eb0ab4b4a5f14440.camel@kernel.org",
          "url": "https://lore.kernel.org/all/08f8444c7237566ffb4ba8c9eb0ab4b4a5f14440.camel@kernel.org/",
          "date": "2026-02-18T15:36:03Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Proposal to convert inode->i_ino from unsigned long to u64, simplifying handling of 64-bit inode numbers on 32-bit architectures.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Christian Brauner",
              "summary": "Encouraged author to proceed with the change, suggesting they ask a Claude instance for Linus' past opinions on deprecating 32-bit architectures.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "encouragement"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Wed, Feb 18, 2026 at 10:36:01AM -0500, Jeff Layton wrote:\n> For historical reasons, the inode->i_ino field is an unsigned long.\n> Because it's only 32 bits on 32-bit CPUs, this has caused a lot of fs-\n> specific hacks on filesystems that have native 64-bit inode numbers\n> when running a 32-bit arch.\n> \n> It would be a lot simpler if we just converted i_ino to be 64-bits and\n> dealt with the conversion at the kernel's edges. This would be a non-\n> event for the most part on 64-bit arches since unsigned long is already\n> 64 bits there.\n> \n> The kernel itself doesn't deal much with i_ino, so the internal changes\n> look fairly straightforward. The bulk of the patches will be to format\n> strings and to tracepoints.\n> \n> I think that the biggest problem will be that this will grow struct\n> inode on 32-bit arches by at least 4 bytes. That may have cacheline\n> alignment and slab sizing implications. We're actively talking about\n> deprecating 32-bit arches in the future however, so maybe we can\n> rationalize that away.\n\nIf you already have a Claude instance open you may want ask it to please\nfind the last ten mails about 32-bit that Linus sent and what his\nopinions are about worrying about it when doing such changes... :)\n\n> FWIW, I had Claude spin up a plan to do this (attached). It's not bad.\n> I'm tempted to tell it generate patches for this, since this is mostly\n> a mechanical change, but I'm curious whether anyone else might have\n> reasons that we shouldn't go ahead and do it.\n\nPlease just do it. I didn't have time to do it myself yet.\n\n",
              "reply_to": "",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "errors": []
    },
    {
      "name": "Joanne Koong",
      "primary_email": "joannelkoong@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 0/9] io_uring: add kernel-managed buffer rings",
          "message_id": "20260218025207.1425553-1-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260218025207.1425553-1-joannelkoong@gmail.com/",
          "date": "2026-02-18T02:56:36Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series introduces kernel-managed buffer rings for io_uring, where the kernel allocates and manages buffers on behalf of the application. This simplifies the process for applications by eliminating the need to allocate and manage backing buffers. The patches provide a straightforward interface for kernel-managed buffer rings, which suffices for most use cases, but do not preclude future support for user-registered memory regions. The changes are based on Jens' io-uring tree and include updates from previous discussions and feedback.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the inefficiency of allocating memory in large regions, which can cause worse TLB performance. They reworked io_region_allocate_pages() to allocate memory in 2MB chunks, attempting a compound allocation for each chunk. This change is necessary to use kernel-managed ring buffers, at least until the GFP_KERNEL_ACCOUNT flag issue is fixed.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Currently, io_region_allocate_pages() tries a single compound allocation\nfor the entire region, and falls back to alloc_pages_bulk_node() if that\nfails.\n\nWhen allocating a large region, trying to do a single compound\nallocation may be unrealistic while allocating page by page may be\ninefficient and cause worse TLB performance.\n\nRework io_region_allocate_pages() to allocate memory in 2MB chunks,\nattempting a compound allocation for each chunk.\n\nReplace IO_REGION_F_SINGLE_REF with IO_REGION_F_COMPOUND_PAGES to\nreflect that the page array may contain tail pages from multiple\ncompound allocations.\n\nCurrently, alloc_pages_bulk_node() fails when the GFP_KERNEL_ACCOUNT gfp\nflag is set. This makes this commit a necessary change in order to use\nkernel-managed ring buffers (which will allocate regions of large\nsizes), at least until that issue is fixed.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/memmap.c | 87 ++++++++++++++++++++++++++++++++++-------------\n 1 file changed, 64 insertions(+), 23 deletions(-)\n\ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 89f56609e50a..6e91960aa8fc 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -15,6 +15,28 @@\n #include \"rsrc.h\"\n #include \"zcrx.h\"\n \n+static void release_compound_pages(struct page **pages, unsigned long nr_pages)\n+{\n+\tstruct page *page;\n+\tunsigned int nr, i = 0;\n+\n+\twhile (nr_pages) {\n+\t\tpage = pages[i];\n+\n+\t\tif (!page || WARN_ON_ONCE(page != compound_head(page)))\n+\t\t\treturn;\n+\n+\t\tnr = compound_nr(page);\n+\t\tput_page(page);\n+\n+\t\tif (nr >= nr_pages)\n+\t\t\treturn;\n+\n+\t\ti += nr;\n+\t\tnr_pages -= nr;\n+\t}\n+}\n+\n static bool io_mem_alloc_compound(struct page **pages, int nr_pages,\n \t\t\t\t  size_t size, gfp_t gfp)\n {\n@@ -84,22 +106,19 @@ enum {\n \tIO_REGION_F_VMAP\t\t\t= 1,\n \t/* memory is provided by user and pinned by the kernel */\n \tIO_REGION_F_USER_PROVIDED\t\t= 2,\n-\t/* only the first page in the array is ref'ed */\n-\tIO_REGION_F_SINGLE_REF\t\t\t= 4,\n+\t/* memory may contain tail pages from compound allocations */\n+\tIO_REGION_F_COMPOUND_PAGES\t\t= 4,\n };\n \n void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n {\n \tif (mr->pages) {\n-\t\tlong nr_refs = mr->nr_pages;\n-\n-\t\tif (mr->flags & IO_REGION_F_SINGLE_REF)\n-\t\t\tnr_refs = 1;\n-\n \t\tif (mr->flags & IO_REGION_F_USER_PROVIDED)\n-\t\t\tunpin_user_pages(mr->pages, nr_refs);\n+\t\t\tunpin_user_pages(mr->pages, mr->nr_pages);\n+\t\telse if (mr->flags & IO_REGION_F_COMPOUND_PAGES)\n+\t\t\trelease_compound_pages(mr->pages, mr->nr_pages);\n \t\telse\n-\t\t\trelease_pages(mr->pages, nr_refs);\n+\t\t\trelease_pages(mr->pages, mr->nr_pages);\n \n \t\tkvfree(mr->pages);\n \t}\n@@ -154,28 +173,50 @@ static int io_region_allocate_pages(struct io_mapped_region *mr,\n \t\t\t\t    unsigned long mmap_offset)\n {\n \tgfp_t gfp = GFP_KERNEL_ACCOUNT | __GFP_ZERO | __GFP_NOWARN;\n-\tsize_t size = io_region_size(mr);\n \tunsigned long nr_allocated;\n-\tstruct page **pages;\n+\tstruct page **pages, **cur_pages;\n+\tunsigned chunk_size, chunk_nr_pages;\n+\tunsigned int pages_left;\n \n \tpages = kvmalloc_array(mr->nr_pages, sizeof(*pages), gfp);\n \tif (!pages)\n \t\treturn -ENOMEM;\n \n-\tif (io_mem_alloc_compound(pages, mr->nr_pages, size, gfp)) {\n-\t\tmr->flags |= IO_REGION_F_SINGLE_REF;\n-\t\tgoto done;\n-\t}\n+\tchunk_size = SZ_2M;\n+\tchunk_nr_pages = chunk_size >> PAGE_SHIFT;\n+\tpages_left = mr->nr_pages;\n+\tcur_pages = pages;\n+\n+\twhile (pages_left) {\n+\t\tunsigned int nr_pages = min(pages_left,\n+\t\t\t\t\t    chunk_nr_pages);\n+\n+\t\tif (io_mem_alloc_compound(cur_pages, nr_pages,\n+\t\t\t\t\t  nr_pages << PAGE_SHIFT, gfp)) {\n+\t\t\tmr->flags |= IO_REGION_F_COMPOUND_PAGES;\n+\t\t\tcur_pages += nr_pages;\n+\t\t\tpages_left -= nr_pages;\n+\t\t\tcontinue;\n+\t\t}\n \n-\tnr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,\n-\t\t\t\t\t     mr->nr_pages, pages);\n-\tif (nr_allocated != mr->nr_pages) {\n-\t\tif (nr_allocated)\n-\t\t\trelease_pages(pages, nr_allocated);\n-\t\tkvfree(pages);\n-\t\treturn -ENOMEM;\n+\t\tnr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,\n+\t\t\t\t\t\t     nr_pages, cur_pages);\n+\t\tif (nr_allocated != nr_pages) {\n+\t\t\tunsigned int total =\n+\t\t\t\t(cur_pages - pages) + nr_allocated;\n+\n+\t\t\tif (mr->flags & IO_REGION_F_COMPOUND_PAGES)\n+\t\t\t\trelease_compound_pages(pages, total);\n+\t\t\telse\n+\t\t\t\trelease_pages(pages, total);\n+\t\t\tkvfree(pages);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tcur_pages += nr_pages;\n+\t\tpages_left -= nr_pages;\n \t}\n-done:\n+\n \treg->mmap_offset = mmap_offset;\n \tmr->pages = pages;\n \treturn 0;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about kernel-managed buffer rings not being supported by the pbuf interface, explaining that they are created through the pbuf interface with the IOU_PBUF_RING_KERNEL_MANAGED flag set and modified to allow taking in a buf_size instead of a ring_addr. The caller must also set the IOU_PBUF_RING_MMAP flag to indicate that the kernel will allocate the memory for the ring.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for kernel-managed buffer rings, which allow the kernel to\nallocate and manage the backing buffers for a buffer ring, rather than\nrequiring the application to provide and manage them.\n\nInternally, the IOBL_KERNEL_MANAGED flag marks buffer lists as\nkernel-managed for appropriate handling in the I/O path.\n\nAt the uapi level, kernel-managed buffer rings are created through the\npbuf interface with the IOU_PBUF_RING_KERNEL_MANAGED flag set. The\nio_uring_buf_reg struct is modified to allow taking in a buf_size\ninstead of a ring_addr. To create a kernel-managed buffer ring, the\ncaller must set the IOU_PBUF_RING_MMAP flag as well to indicate that the\nkernel will allocate the memory for the ring. When the caller mmaps the\nring, they will get back a virtual mapping to the buffer memory.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h | 14 +++++-\n io_uring/kbuf.c               | 95 +++++++++++++++++++++++++++++------\n io_uring/kbuf.h               |  6 ++-\n 3 files changed, 97 insertions(+), 18 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex 6750c383a2ab..278b56a87745 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -885,15 +885,27 @@ struct io_uring_buf_ring {\n  *\t\t\tuse of it will consume only as much as it needs. This\n  *\t\t\trequires that both the kernel and application keep\n  *\t\t\ttrack of where the current read/recv index is at.\n+ * IOU_PBUF_RING_KERNEL_MANAGED: If set, kernel allocates the memory for the\n+ *\t\t\tring and its buffers. The application must set the\n+ *\t\t\tbuffer size through reg->buf_size. The buffers are\n+ *\t\t\trecycled by the kernel. IOU_PBUF_RING_MMAP must be set\n+ *\t\t\tas well. When the caller makes a subsequent mmap call,\n+ *\t\t\tthe virtual mapping returned is a contiguous mapping of\n+ *\t\t\tthe buffers. IOU_PBUF_RING_INC is not yet supported.\n  */\n enum io_uring_register_pbuf_ring_flags {\n \tIOU_PBUF_RING_MMAP\t= 1,\n \tIOU_PBUF_RING_INC\t= 2,\n+\tIOU_PBUF_RING_KERNEL_MANAGED = 4,\n };\n \n /* argument for IORING_(UN)REGISTER_PBUF_RING */\n struct io_uring_buf_reg {\n-\t__u64\tring_addr;\n+\tunion {\n+\t\t__u64\tring_addr;\n+\t\t/* used if reg->flags & IOU_PBUF_RING_KERNEL_MANAGED */\n+\t\t__u32   buf_size;\n+\t};\n \t__u32\tring_entries;\n \t__u16\tbgid;\n \t__u16\tflags;\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 67d4fe576473..816200e91b1f 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -427,10 +427,13 @@ static int io_remove_buffers_legacy(struct io_ring_ctx *ctx,\n \n static void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)\n {\n-\tif (bl->flags & IOBL_BUF_RING)\n+\tif (bl->flags & IOBL_BUF_RING) {\n \t\tio_free_region(ctx->user, &bl->region);\n-\telse\n+\t\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\t\tkfree(bl->buf_ring);\n+\t} else {\n \t\tio_remove_buffers_legacy(ctx, bl, -1U);\n+\t}\n \n \tkfree(bl);\n }\n@@ -596,6 +599,51 @@ int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags)\n \treturn IOU_COMPLETE;\n }\n \n+static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_buffer_list *bl,\n+\t\t\t       const struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_uring_region_desc rd;\n+\tstruct io_uring_buf_ring *ring;\n+\tunsigned long ring_size;\n+\tvoid *buf_region;\n+\tunsigned int i;\n+\tint ret;\n+\n+\t/* allocate pages for the ring structure */\n+\tring_size = flex_array_size(ring, bufs, reg->ring_entries);\n+\tring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);\n+\tif (!ring)\n+\t\treturn -ENOMEM;\n+\n+\tmemset(&rd, 0, sizeof(rd));\n+\trd.size = (u64)reg->buf_size * reg->ring_entries;\n+\n+\tret = io_create_region(ctx, &bl->region, &rd, 0);\n+\tif (ret) {\n+\t\tkfree(ring);\n+\t\treturn ret;\n+\t}\n+\n+\t/* initialize ring buf entries to point to the buffers */\n+\tbuf_region = io_region_get_ptr(&bl->region);\n+\tfor (i = 0; i < reg->ring_entries; i++) {\n+\t\tstruct io_uring_buf *buf = &ring->bufs[i];\n+\n+\t\tbuf->addr = (u64)(uintptr_t)buf_region;\n+\t\tbuf->len = reg->buf_size;\n+\t\tbuf->bid = i;\n+\n+\t\tbuf_region += reg->buf_size;\n+\t}\n+\tring->tail = reg->ring_entries;\n+\n+\tbl->buf_ring = ring;\n+\tbl->flags |= IOBL_KERNEL_MANAGED;\n+\n+\treturn 0;\n+}\n+\n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n {\n \tstruct io_uring_buf_reg reg;\n@@ -612,7 +660,8 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t\treturn -EFAULT;\n \tif (!mem_is_zero(reg.resv, sizeof(reg.resv)))\n \t\treturn -EINVAL;\n-\tif (reg.flags & ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC))\n+\tif (reg.flags & ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC |\n+\t\t\t  IOU_PBUF_RING_KERNEL_MANAGED))\n \t\treturn -EINVAL;\n \tif (!is_power_of_2(reg.ring_entries))\n \t\treturn -EINVAL;\n@@ -620,6 +669,15 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \tif (reg.ring_entries >= 65536)\n \t\treturn -EINVAL;\n \n+\tif (reg.flags & IOU_PBUF_RING_KERNEL_MANAGED) {\n+\t\tif (!(reg.flags & IOU_PBUF_RING_MMAP))\n+\t\t\treturn -EINVAL;\n+\t\tif (reg.flags & IOU_PBUF_RING_INC)\n+\t\t\treturn -EINVAL;\n+\t\tif (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))\n+\t\t\treturn -EINVAL;\n+\t}\n+\n \tbl = io_buffer_get_list(ctx, reg.bgid);\n \tif (bl) {\n \t\t/* if mapped buffer ring OR classic exists, don't allow */\n@@ -634,17 +692,26 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \n \tmmap_offset = (unsigned long)reg.bgid << IORING_OFF_PBUF_SHIFT;\n \tring_size = flex_array_size(br, bufs, reg.ring_entries);\n-\n \tmemset(&rd, 0, sizeof(rd));\n-\trd.size = PAGE_ALIGN(ring_size);\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP)) {\n-\t\trd.user_addr = reg.ring_addr;\n-\t\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n+\n+\tif (reg.flags & IOU_PBUF_RING_KERNEL_MANAGED) {\n+\t\tret = io_setup_kmbuf_ring(ctx, bl, &reg);\n+\t\tif (ret) {\n+\t\t\tkfree(bl);\n+\t\t\treturn ret;\n+\t\t}\n+\t} else {\n+\t\trd.size = PAGE_ALIGN(ring_size);\n+\t\tif (!(reg.flags & IOU_PBUF_RING_MMAP)) {\n+\t\t\trd.user_addr = reg.ring_addr;\n+\t\t\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n+\t\t}\n+\t\tret = io_create_region(ctx, &bl->region, &rd, mmap_offset);\n+\t\tif (ret)\n+\t\t\tgoto fail;\n+\t\tbl->buf_ring = io_region_get_ptr(&bl->region);\n \t}\n-\tret = io_create_region(ctx, &bl->region, &rd, mmap_offset);\n-\tif (ret)\n-\t\tgoto fail;\n-\tbr = io_region_get_ptr(&bl->region);\n+\tbr = bl->buf_ring;\n \n #ifdef SHM_COLOUR\n \t/*\n@@ -666,15 +733,13 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \tbl->nr_entries = reg.ring_entries;\n \tbl->mask = reg.ring_entries - 1;\n \tbl->flags |= IOBL_BUF_RING;\n-\tbl->buf_ring = br;\n \tif (reg.flags & IOU_PBUF_RING_INC)\n \t\tbl->flags |= IOBL_INC;\n \tret = io_buffer_add_list(ctx, bl, reg.bgid);\n \tif (!ret)\n \t\treturn 0;\n fail:\n-\tio_free_region(ctx->user, &bl->region);\n-\tkfree(bl);\n+\tio_put_bl(ctx, bl);\n \treturn ret;\n }\n \ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex bf15e26520d3..38dd5fe6716e 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -7,9 +7,11 @@\n \n enum {\n \t/* ring mapped provided buffers */\n-\tIOBL_BUF_RING\t= 1,\n+\tIOBL_BUF_RING\t\t= 1,\n \t/* buffers are consumed incrementally rather than always fully */\n-\tIOBL_INC\t= 2,\n+\tIOBL_INC\t\t= 2,\n+\t/* buffers are kernel managed */\n+\tIOBL_KERNEL_MANAGED\t= 4,\n };\n \n struct io_buffer_list {\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about distinguishing between kernel-managed buffer addresses and negative values when error checking, by modifying the io_br_sel struct to separate address and value fields for kernel-managed buffers. The author also added auto-commit logic for selected kernel-managed buffers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "added new code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Allow kernel-managed buffers to be selected. This requires modifying the\nio_br_sel struct to separate the fields for address and val, since a\nkernel address cannot be distinguished from a negative val when error\nchecking.\n\nAuto-commit any selected kernel-managed buffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring_types.h |  8 ++++----\n io_uring/kbuf.c                | 16 ++++++++++++----\n 2 files changed, 16 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 3e4a82a6f817..36cc2e0346d9 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -93,13 +93,13 @@ struct io_mapped_region {\n  */\n struct io_br_sel {\n \tstruct io_buffer_list *buf_list;\n-\t/*\n-\t * Some selection parts return the user address, others return an error.\n-\t */\n \tunion {\n+\t\t/* for classic/ring provided buffers */\n \t\tvoid __user *addr;\n-\t\tssize_t val;\n+\t\t/* for kernel-managed buffers */\n+\t\tvoid *kaddr;\n \t};\n+\tssize_t val;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 816200e91b1f..efcc6540f948 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -155,7 +155,8 @@ static int io_provided_buffers_select(struct io_kiocb *req, size_t *len,\n \treturn 1;\n }\n \n-static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n+static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n+\t\t\t     unsigned int issue_flags)\n {\n \t/*\n \t* If we came in unlocked, we have no choice but to consume the\n@@ -170,7 +171,11 @@ static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n \tif (issue_flags & IO_URING_F_UNLOCKED)\n \t\treturn true;\n \n-\t/* uring_cmd commits kbuf upfront, no need to auto-commit */\n+\t/* kernel-managed buffers are auto-committed */\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\treturn true;\n+\n+\t/* multishot uring_cmd commits kbuf upfront, no need to auto-commit */\n \tif (!io_file_can_poll(req) && req->opcode != IORING_OP_URING_CMD)\n \t\treturn true;\n \treturn false;\n@@ -200,9 +205,12 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n-\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n+\telse\n+\t\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n \n-\tif (io_should_commit(req, issue_flags)) {\n+\tif (io_should_commit(req, bl, issue_flags)) {\n \t\tio_kbuf_commit(req, sel.buf_list, *len, 1);\n \t\tsel.buf_list = NULL;\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about buffer rings being unregistered while pinned by the kernel, and provided a mechanism for kernel subsystems to safely access buffer ring contents. The author added APIs to pin and unpin buffer rings, preventing userspace from unregistering a buffer ring while it is pinned. This change is necessary for fuse to pin the buffer ring because fuse may need to select a buffer in atomic contexts.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged concern",
                "provided solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add kernel APIs to pin and unpin buffer rings, preventing userspace from\nunregistering a buffer ring while it is pinned by the kernel.\n\nThis provides a mechanism for kernel subsystems to safely access buffer\nring contents while ensuring the buffer ring remains valid. A pinned\nbuffer ring cannot be unregistered until explicitly unpinned. On the\nuserspace side, trying to unregister a pinned buffer will return -EBUSY.\n\nThis is a preparatory change for upcoming fuse usage of kernel-managed\nbuffer rings. It is necessary for fuse to pin the buffer ring because\nfuse may need to select a buffer in atomic contexts, which it can only\ndo so by using the underlying buffer list pointer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 17 +++++++++++\n io_uring/kbuf.c              | 55 ++++++++++++++++++++++++++++++++++++\n io_uring/kbuf.h              |  5 ++++\n 3 files changed, 77 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 375fd048c4cb..bd681d8ab1d4 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -84,6 +84,10 @@ struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n \t\t\t\t struct io_br_sel *sel, unsigned int issue_flags);\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **out_bl);\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t    unsigned issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -126,6 +130,19 @@ static inline bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n {\n \treturn true;\n }\n+static inline int io_uring_buf_ring_pin(struct io_uring_cmd *cmd,\n+\t\t\t\t\tunsigned buf_group,\n+\t\t\t\t\tunsigned issue_flags,\n+\t\t\t\t\tstruct io_buffer_list **bl)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n+static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned buf_group,\n+\t\t\t\t\t  unsigned issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex efcc6540f948..1d86ad7803fd 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -9,6 +9,7 @@\n #include <linux/poll.h>\n #include <linux/vmalloc.h>\n #include <linux/io_uring.h>\n+#include <linux/io_uring/cmd.h>\n \n #include <uapi/linux/io_uring.h>\n \n@@ -237,6 +238,58 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \treturn sel;\n }\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **out_bl)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (!bl || !(bl->flags & IOBL_BUF_RING))\n+\t\tgoto err;\n+\n+\tif (unlikely(bl->flags & IOBL_PINNED)) {\n+\t\tret = -EALREADY;\n+\t\tgoto err;\n+\t}\n+\n+\tbl->flags |= IOBL_PINNED;\n+\tret = 0;\n+\t*out_bl = bl;\n+err:\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_pin);\n+\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t       unsigned issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tunsigned int required_flags;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (!bl)\n+\t\tgoto err;\n+\n+\trequired_flags = IOBL_BUF_RING | IOBL_PINNED;\n+\tif ((bl->flags & required_flags) == required_flags) {\n+\t\tbl->flags &= ~IOBL_PINNED;\n+\t\tret = 0;\n+\t}\n+err:\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_unpin);\n+\n /* cap it at a reasonable 256, will be one page even for 4K */\n #define PEEK_MAX_IMPORT\t\t256\n \n@@ -768,6 +821,8 @@ int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t\treturn -ENOENT;\n \tif (!(bl->flags & IOBL_BUF_RING))\n \t\treturn -EINVAL;\n+\tif (bl->flags & IOBL_PINNED)\n+\t\treturn -EBUSY;\n \n \tscoped_guard(mutex, &ctx->mmap_lock)\n \t\txa_erase(&ctx->io_bl_xa, bl->bgid);\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 38dd5fe6716e..006e8a73a117 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -12,6 +12,11 @@ enum {\n \tIOBL_INC\t\t= 2,\n \t/* buffers are kernel managed */\n \tIOBL_KERNEL_MANAGED\t= 4,\n+\t/*\n+\t * buffer ring is pinned and cannot be unregistered by userspace until\n+\t * it has been unpinned\n+\t */\n+\tIOBL_PINNED\t\t= 8,\n };\n \n struct io_buffer_list {\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_buffer_select() function not returning the id of the selected buffer for kernel-managed buffer rings, and responded by modifying the function to return the buffer id.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Return the id of the selected buffer in io_buffer_select(). This is\nneeded for kernel-managed buffer rings to later recycle the selected\nbuffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h   | 2 +-\n include/linux/io_uring_types.h | 2 ++\n io_uring/kbuf.c                | 7 +++++--\n 3 files changed, 8 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex bd681d8ab1d4..31f47cce99f5 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);\n \n /*\n  * Select a buffer from the provided buffer group for multishot uring_cmd.\n- * Returns the selected buffer address and size.\n+ * Returns the selected buffer address, size, and id.\n  */\n struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n \t\t\t\t\t    unsigned buf_group, size_t *len,\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 36cc2e0346d9..5a56bb341337 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -100,6 +100,8 @@ struct io_br_sel {\n \t\tvoid *kaddr;\n \t};\n \tssize_t val;\n+\t/* id of the selected buffer */\n+\tunsigned buf_id;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 1d86ad7803fd..d20221f1b9b2 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -206,6 +206,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n+\tsel.buf_id = req->buf_index;\n \tif (bl->flags & IOBL_KERNEL_MANAGED)\n \t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n \telse\n@@ -229,10 +230,12 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \n \tbl = io_buffer_get_list(ctx, buf_group);\n \tif (likely(bl)) {\n-\t\tif (bl->flags & IOBL_BUF_RING)\n+\t\tif (bl->flags & IOBL_BUF_RING) {\n \t\t\tsel = io_ring_buffer_select(req, len, bl, issue_flags);\n-\t\telse\n+\t\t} else {\n \t\t\tsel.addr = io_provided_buffer_select(req, len, bl);\n+\t\t\tsel.buf_id = req->buf_index;\n+\t\t}\n \t}\n \tio_ring_submit_unlock(req->ctx, issue_flags);\n \treturn sel;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about adding an interface for buffers to be recycled back into a kernel-managed buffer ring, and provided the necessary code changes to implement this feature.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "added new function io_uring_kmbuf_recycle",
                "modified existing code in io_uring/kbuf.c"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add an interface for buffers to be recycled back into a kernel-managed\nbuffer ring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 11 +++++++++\n io_uring/kbuf.c              | 48 ++++++++++++++++++++++++++++++++++++\n 2 files changed, 59 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 31f47cce99f5..5cebcd6d50e6 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -88,6 +88,10 @@ int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t  unsigned issue_flags, struct io_buffer_list **out_bl);\n int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t    unsigned issue_flags);\n+\n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -143,6 +147,13 @@ static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n+\t\t\t\t\t unsigned int buf_group, u64 addr,\n+\t\t\t\t\t unsigned int len, unsigned int bid,\n+\t\t\t\t\t unsigned int issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex d20221f1b9b2..6e4dd1e003f4 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -102,6 +102,54 @@ void io_kbuf_drop_legacy(struct io_kiocb *req)\n \treq->kbuf = NULL;\n }\n \n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags)\n+{\n+\tstruct io_kiocb *req = cmd_to_io_kiocb(cmd);\n+\tstruct io_ring_ctx *ctx = req->ctx;\n+\tstruct io_uring_buf_ring *br;\n+\tstruct io_uring_buf *buf;\n+\tstruct io_buffer_list *bl;\n+\tunsigned int required_flags;\n+\tint ret = -EINVAL;\n+\n+\tif (WARN_ON_ONCE(req->flags & REQ_F_BUFFERS_COMMIT))\n+\t\treturn ret;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\n+\tif (!bl)\n+\t\tgoto err;\n+\n+\trequired_flags = IOBL_BUF_RING | IOBL_KERNEL_MANAGED;\n+\tif (WARN_ON_ONCE((bl->flags & required_flags) != required_flags))\n+\t\tgoto err;\n+\n+\tbr = bl->buf_ring;\n+\n+\tif (WARN_ON_ONCE((__u16)(br->tail - bl->head) >= bl->nr_entries))\n+\t\tgoto err;\n+\n+\tbuf = &br->bufs[(br->tail) & bl->mask];\n+\n+\tbuf->addr = addr;\n+\tbuf->len = len;\n+\tbuf->bid = bid;\n+\n+\treq->flags &= ~REQ_F_BUFFER_RING;\n+\n+\tbr->tail++;\n+\tret = 0;\n+\n+err:\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_kmbuf_recycle);\n+\n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)\n {\n \tstruct io_ring_ctx *ctx = req->ctx;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring_is_kmbuf_ring() function, which was missing an implementation for kernel-managed buffer rings. The author provided a new patch that adds this functionality, including a preparatory patch to ensure the buffer ring registered by the server is a kernel-managed buffer ring.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "new patch provided"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "io_uring_is_kmbuf_ring() returns true if there is a kernel-managed\nbuffer ring at the specified buffer group.\n\nThis is a preparatory patch for upcoming fuse kernel-managed buffer\nsupport, which needs to ensure the buffer ring registered by the server\nis a kernel-managed buffer ring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h |  9 +++++++++\n io_uring/kbuf.c              | 20 ++++++++++++++++++++\n 2 files changed, 29 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 5cebcd6d50e6..dce6a0ce8538 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -92,6 +92,9 @@ int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t   u64 addr, unsigned int len, unsigned int bid,\n \t\t\t   unsigned int issue_flags);\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -154,6 +157,12 @@ static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned int buf_group,\n+\t\t\t\t\t  unsigned int issue_flags)\n+{\n+\treturn false;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 6e4dd1e003f4..bd10c830cd30 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -917,3 +917,23 @@ struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n \t\treturn NULL;\n \treturn &bl->region;\n }\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tbool is_kmbuf_ring = false;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (likely(bl) && (bl->flags & IOBL_KERNEL_MANAGED)) {\n+\t\tWARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING));\n+\t\tis_kmbuf_ring = true;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn is_kmbuf_ring;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_is_kmbuf_ring);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring mutex being held by in-progress commits and needing to select a buffer atomically. They agreed to export io_ring_buffer_select() so that it can be used without grabbing the mutex, which will be necessary for fuse io-uring.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Export io_ring_buffer_select() so that it may be used by callers who\npass in a pinned bufring without needing to grab the io_uring mutex.\n\nThis is a preparatory patch that will be needed by fuse io-uring, which\nwill need to select a buffer from a kernel-managed bufring while the\nuring mutex may already be held by in-progress commits, and may need to\nselect a buffer in atomic contexts.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 14 ++++++++++++++\n io_uring/kbuf.c              |  7 ++++---\n 2 files changed, 18 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex dce6a0ce8538..ac8925fa81f6 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -95,6 +95,10 @@ int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \n bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t    unsigned int issue_flags);\n+\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -163,6 +167,16 @@ static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n {\n \treturn false;\n }\n+static inline struct io_br_sel io_ring_buffer_select(struct io_kiocb *req,\n+\t\t\t\t\t\t     size_t *len,\n+\t\t\t\t\t\t     struct io_buffer_list *bl,\n+\t\t\t\t\t\t     unsigned int issue_flags)\n+{\n+\tstruct io_br_sel sel = {\n+\t\t.val = -EOPNOTSUPP,\n+\t};\n+\treturn sel;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex bd10c830cd30..fcc64e4a6a29 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -230,9 +230,9 @@ static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n \treturn false;\n }\n \n-static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n-\t\t\t\t\t      struct io_buffer_list *bl,\n-\t\t\t\t\t      unsigned int issue_flags)\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags)\n {\n \tstruct io_uring_buf_ring *br = bl->buf_ring;\n \t__u16 tail, head = bl->head;\n@@ -266,6 +266,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \t}\n \treturn sel;\n }\n+EXPORT_SYMBOL_GPL(io_ring_buffer_select);\n \n struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \t\t\t\t  unsigned buf_group, unsigned int issue_flags)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about indicating which buffer was selected in the completion queue entry, agreed to add a flag (IORING_CQE_F_BUFFER) and encode the buffer index if a buffer was selected.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed to address issue",
                "added new code to fix problem"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When uring_cmd operations select a buffer, the completion queue entry\nshould indicate which buffer was selected.\n\nSet IORING_CQE_F_BUFFER on the completed entry and encode the buffer\nindex if a buffer was selected.\n\nThis change is needed in order to relay to userspace which selected\nbuffer contains the data.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/uring_cmd.c | 6 +++++-\n 1 file changed, 5 insertions(+), 1 deletion(-)\n\ndiff --git a/io_uring/uring_cmd.c b/io_uring/uring_cmd.c\nindex ee7b49f47cb5..6d38df1a812d 100644\n--- a/io_uring/uring_cmd.c\n+++ b/io_uring/uring_cmd.c\n@@ -151,6 +151,7 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \t\t       unsigned issue_flags, bool is_cqe32)\n {\n \tstruct io_kiocb *req = cmd_to_io_kiocb(ioucmd);\n+\tu32 cflags = 0;\n \n \tif (WARN_ON_ONCE(req->flags & REQ_F_APOLL_MULTISHOT))\n \t\treturn;\n@@ -160,7 +161,10 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \tif (ret < 0)\n \t\treq_set_fail(req);\n \n-\tio_req_set_res(req, ret, 0);\n+\tif (req->flags & (REQ_F_BUFFER_SELECTED | REQ_F_BUFFER_RING))\n+\t\tcflags |= IORING_CQE_F_BUFFER |\n+\t\t\t(req->buf_index << IORING_CQE_BUFFER_SHIFT);\n+\tio_req_set_res(req, ret, cflags);\n \tif (is_cqe32) {\n \t\tif (req->ctx->flags & IORING_SETUP_CQE_MIXED)\n \t\t\treq->cqe.flags |= IORING_CQE_F_32;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "syzbot ci",
              "summary": "syzbot ci reported a general protection fault in io_remove_buffers_legacy, likely due to a null pointer dereference when trying to remove buffers from the ring; the issue is not explicitly stated but can be inferred from the provided stacktrace and KASAN report",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "general protection fault",
                "null-ptr-deref"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "syzbot ci has tested the following series\n\n[v2] io_uring: add kernel-managed buffer rings\nhttps://lore.kernel.org/all/20260218025207.1425553-1-joannelkoong@gmail.com\n* [PATCH v2 1/9] io_uring/memmap: chunk allocations in io_region_allocate_pages()\n* [PATCH v2 2/9] io_uring/kbuf: add support for kernel-managed buffer rings\n* [PATCH v2 3/9] io_uring/kbuf: support kernel-managed buffer rings in buffer selection\n* [PATCH v2 4/9] io_uring/kbuf: add buffer ring pinning/unpinning\n* [PATCH v2 5/9] io_uring/kbuf: return buffer id in buffer selection\n* [PATCH v2 6/9] io_uring/kbuf: add recycling for kernel managed buffer rings\n* [PATCH v2 7/9] io_uring/kbuf: add io_uring_is_kmbuf_ring()\n* [PATCH v2 8/9] io_uring/kbuf: export io_ring_buffer_select()\n* [PATCH v2 9/9] io_uring/cmd: set selected buffer index in __io_uring_cmd_done()\n\nand found the following issue:\ngeneral protection fault in io_remove_buffers_legacy\n\nFull report is available here:\nhttps://ci.syzbot.org/series/ddeaf464-c69b-4166-b0cf-53c9d51e4820\n\n***\n\ngeneral protection fault in io_remove_buffers_legacy\n\ntree:      torvalds\nURL:       https://kernel.googlesource.com/pub/scm/linux/kernel/git/torvalds/linux\nbase:      2961f841b025fb234860bac26dfb7fa7cb0fb122\narch:      amd64\ncompiler:  Debian clang version 21.1.8 (++20251221033036+2078da43e25a-1~exp1~20251221153213.50), Debian LLD 21.1.8\nconfig:    https://ci.syzbot.org/builds/ab5ad5aa-2757-4d66-a2c5-391a8417535d/config\nC repro:   https://ci.syzbot.org/findings/061747e2-36f1-499b-ac34-38cefffbce63/c_repro\nsyz repro: https://ci.syzbot.org/findings/061747e2-36f1-499b-ac34-38cefffbce63/syz_repro\n\nOops: general protection fault, probably for non-canonical address 0xdffffc0000000001: 0000 [#1] SMP KASAN PTI\nKASAN: null-ptr-deref in range [0x0000000000000008-0x000000000000000f]\nCPU: 1 UID: 0 PID: 5967 Comm: syz.0.17 Not tainted syzkaller #0 PREEMPT(full) \nHardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 1.16.2-debian-1.16.2-1 04/01/2014\nRIP: 0010:__list_del_entry_valid_or_report+0x25/0x190 lib/list_debug.c:49\nCode: 90 90 90 90 90 f3 0f 1e fa 41 57 41 56 41 55 41 54 53 48 89 fb 49 bd 00 00 00 00 00 fc ff df 48 83 c7 08 48 89 f8 48 c1 e8 03 <42> 80 3c 28 00 74 05 e8 df 8c 77 fd 4c 8b 7b 08 48 89 d8 48 c1 e8\nRSP: 0018:ffffc900040a7b68 EFLAGS: 00010202\nRAX: 0000000000000001 RBX: 0000000000000000 RCX: 1ffff11035ee2732\nRDX: 1ffff11035ee2730 RSI: 00000000ffffffff RDI: 0000000000000008\nRBP: dffffc0000000000 R08: ffff8881af7139b7 R09: 0000000000000000\nR10: ffff8881af7139a0 R11: ffffed1035ee2737 R12: ffff8881af713980\nR13: dffffc0000000000 R14: 00000000ffffffff R15: 0000000000000000\nFS:  0000555560587500(0000) GS:ffff8882a9466000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 0000200000001000 CR3: 0000000175cd2000 CR4: 00000000000006f0\nCall Trace:\n <TASK>\n __list_del_entry_valid include/linux/list.h:132 [inline]\n __list_del_entry include/linux/list.h:223 [inline]\n list_del include/linux/list.h:237 [inline]\n io_remove_buffers_legacy+0x139/0x310 io_uring/kbuf.c:533\n io_put_bl+0x62/0x120 io_uring/kbuf.c:548\n io_register_pbuf_ring+0x6c0/0x7d0 io_uring/kbuf.c:855\n __io_uring_register io_uring/register.c:838 [inline]\n __do_sys_io_uring_register io_uring/register.c:1024 [inline]\n __se_sys_io_uring_register+0xc3e/0x19a0 io_uring/register.c:1001\n do_syscall_x64 arch/x86/entry/syscall_64.c:63 [inline]\n do_syscall_64+0x14d/0xf80 arch/x86/entry/syscall_64.c:94\n entry_SYSCALL_64_after_hwframe+0x77/0x7f\nRIP: 0033:0x7f056859bf79\nCode: ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 e8 ff ff ff f7 d8 64 89 01 48\nRSP: 002b:00007fffd8dcfaf8 EFLAGS: 00000246 ORIG_RAX: 00000000000001ab\nRAX: ffffffffffffffda RBX: 00007f0568815fa0 RCX: 00007f056859bf79\nRDX: 0000200000000040 RSI: 0000000000000016 RDI: 0000000000000004\nRBP: 00007f05686327e0 R08: 0000000000000000 R09: 0000000000000000\nR10: 0000000000000001 R11: 0000000000000246 R12: 0000000000000000\nR13: 00007f0568815fac R14: 00007f0568815fa0 R15: 00007f0568815fa0\n </TASK>\nModules linked in:\n---[ end trace 0000000000000000 ]---\nRIP: 0010:__list_del_entry_valid_or_report+0x25/0x190 lib/list_debug.c:49\nCode: 90 90 90 90 90 f3 0f 1e fa 41 57 41 56 41 55 41 54 53 48 89 fb 49 bd 00 00 00 00 00 fc ff df 48 83 c7 08 48 89 f8 48 c1 e8 03 <42> 80 3c 28 00 74 05 e8 df 8c 77 fd 4c 8b 7b 08 48 89 d8 48 c1 e8\nRSP: 0018:ffffc900040a7b68 EFLAGS: 00010202\nRAX: 0000000000000001 RBX: 0000000000000000 RCX: 1ffff11035ee2732\nRDX: 1ffff11035ee2730 RSI: 00000000ffffffff RDI: 0000000000000008\nRBP: dffffc0000000000 R08: ffff8881af7139b7 R09: 0000000000000000\nR10: ffff8881af7139a0 R11: ffffed1035ee2737 R12: ffff8881af713980\nR13: dffffc0000000000 R14: 00000000ffffffff R15: 0000000000000000\nFS:  0000555560587500(0000) GS:ffff8882a9466000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00007f25f1e17095 CR3: 0000000175cd2000 CR4: 00000000000006f0\n----------------\nCode disassembly (best guess):\n   0:\t90                   \tnop\n   1:\t90                   \tnop\n   2:\t90                   \tnop\n   3:\t90                   \tnop\n   4:\t90                   \tnop\n   5:\tf3 0f 1e fa          \tendbr64\n   9:\t41 57                \tpush   %r15\n   b:\t41 56                \tpush   %r14\n   d:\t41 55                \tpush   %r13\n   f:\t41 54                \tpush   %r12\n  11:\t53                   \tpush   %rbx\n  12:\t48 89 fb             \tmov    %rdi,%rbx\n  15:\t49 bd 00 00 00 00 00 \tmovabs $0xdffffc0000000000,%r13\n  1c:\tfc ff df\n  1f:\t48 83 c7 08          \tadd    $0x8,%rdi\n  23:\t48 89 f8             \tmov    %rdi,%rax\n  26:\t48 c1 e8 03          \tshr    $0x3,%rax\n* 2a:\t42 80 3c 28 00       \tcmpb   $0x0,(%rax,%r13,1) <-- trapping instruction\n  2f:\t74 05                \tje     0x36\n  31:\te8 df 8c 77 fd       \tcall   0xfd778d15\n  36:\t4c 8b 7b 08          \tmov    0x8(%rbx),%r15\n  3a:\t48 89 d8             \tmov    %rbx,%rax\n  3d:\t48                   \trex.W\n  3e:\tc1                   \t.byte 0xc1\n  3f:\te8                   \t.byte 0xe8\n\n\n***\n\nIf these findings have caused you to resend the series or submit a\nseparate fix, please add the following tag to your commit message:\n  Tested-by: syzbot@syzkaller.appspotmail.com\n\n---\nThis report is generated by a bot. It may contain errors.\nsyzbot ci engineers can be reached at syzkaller@googlegroups.com.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged the need for a revised patch version (v2) to address feedback from the conversation in thread [1], without providing specific details on the changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for further revision"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm going to update v2 with another version, as per the conversation\nin this thread [1].\n\nThanks,\nJoanne\n\n[1] https://lore.kernel.org/linux-fsdevel/20260210002852.1394504-1-joannelkoong@gmail.com/T/#t",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v1 11/11] io_uring/cmd: set selected buffer index in __io_uring_cmd_done()",
          "message_id": "20260210002852.1394504-12-joannelkoong@gmail.com",
          "url": "https://lore.kernel.org/all/20260210002852.1394504-12-joannelkoong@gmail.com/",
          "date": "2026-02-10T00:31:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-10",
          "patch_summary": "This patch adds the ability to set the selected buffer index in the __io_uring_cmd_done() function, which is part of the io_uring command handling code. This change allows the kernel-managed buffer ring feature to work correctly by keeping track of the currently selected buffer. The patch builds on top of a series of changes that introduce kernel-managed buffer rings, where the kernel allocates and manages buffers for applications using io_uring. The approach taken is to modify the existing command handling code to accommodate the new buffer selection logic.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the refactoring of io_register_pbuf_ring() into generic helpers, explaining that this is a preparatory change for upcoming kernel-managed buffer ring support and promising to reuse these helpers in the future.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "preparatory change",
                "reuse helpers"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Refactor the logic in io_register_pbuf_ring() into generic helpers:\n- io_copy_and_validate_buf_reg(): Copy out user arg and validate user\n  arg and buffer registration parameters\n- io_alloc_new_buffer_list(): Allocate and initialize a new buffer\n  list for the given buffer group ID\n- io_setup_pbuf_ring(): Sets up the physical buffer ring region and\n  handles memory mapping for provided buffer rings\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport which will need to reuse some of these helpers.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c | 129 +++++++++++++++++++++++++++++++-----------------\n 1 file changed, 85 insertions(+), 44 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 67d4fe576473..850b836f32ee 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -596,55 +596,73 @@ int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags)\n \treturn IOU_COMPLETE;\n }\n \n-int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+static int io_copy_and_validate_buf_reg(const void __user *arg,\n+\t\t\t\t\tstruct io_uring_buf_reg *reg,\n+\t\t\t\t\tunsigned int permitted_flags)\n {\n-\tstruct io_uring_buf_reg reg;\n-\tstruct io_buffer_list *bl;\n-\tstruct io_uring_region_desc rd;\n-\tstruct io_uring_buf_ring *br;\n-\tunsigned long mmap_offset;\n-\tunsigned long ring_size;\n-\tint ret;\n-\n-\tlockdep_assert_held(&ctx->uring_lock);\n-\n-\tif (copy_from_user(&reg, arg, sizeof(reg)))\n+\tif (copy_from_user(reg, arg, sizeof(*reg)))\n \t\treturn -EFAULT;\n-\tif (!mem_is_zero(reg.resv, sizeof(reg.resv)))\n+\n+\tif (!mem_is_zero(reg->resv, sizeof(reg->resv)))\n \t\treturn -EINVAL;\n-\tif (reg.flags & ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC))\n+\tif (reg->flags & ~permitted_flags)\n \t\treturn -EINVAL;\n-\tif (!is_power_of_2(reg.ring_entries))\n+\tif (!is_power_of_2(reg->ring_entries))\n \t\treturn -EINVAL;\n \t/* cannot disambiguate full vs empty due to head/tail size */\n-\tif (reg.ring_entries >= 65536)\n+\tif (reg->ring_entries >= 65536)\n \t\treturn -EINVAL;\n+\treturn 0;\n+}\n \n-\tbl = io_buffer_get_list(ctx, reg.bgid);\n-\tif (bl) {\n+static struct io_buffer_list *\n+io_alloc_new_buffer_list(struct io_ring_ctx *ctx,\n+\t\t\t const struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_buffer_list *list;\n+\n+\tlist = io_buffer_get_list(ctx, reg->bgid);\n+\tif (list) {\n \t\t/* if mapped buffer ring OR classic exists, don't allow */\n-\t\tif (bl->flags & IOBL_BUF_RING || !list_empty(&bl->buf_list))\n-\t\t\treturn -EEXIST;\n-\t\tio_destroy_bl(ctx, bl);\n+\t\tif (list->flags & IOBL_BUF_RING || !list_empty(&list->buf_list))\n+\t\t\treturn ERR_PTR(-EEXIST);\n+\t\tio_destroy_bl(ctx, list);\n \t}\n \n-\tbl = kzalloc(sizeof(*bl), GFP_KERNEL_ACCOUNT);\n-\tif (!bl)\n-\t\treturn -ENOMEM;\n+\tlist = kzalloc(sizeof(*list), GFP_KERNEL_ACCOUNT);\n+\tif (!list)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tlist->nr_entries = reg->ring_entries;\n+\tlist->mask = reg->ring_entries - 1;\n+\tlist->flags = IOBL_BUF_RING;\n+\n+\treturn list;\n+}\n+\n+static int io_setup_pbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t      const struct io_uring_buf_reg *reg,\n+\t\t\t      struct io_buffer_list *bl)\n+{\n+\tstruct io_uring_region_desc rd;\n+\tunsigned long mmap_offset;\n+\tunsigned long ring_size;\n+\tint ret;\n \n-\tmmap_offset = (unsigned long)reg.bgid << IORING_OFF_PBUF_SHIFT;\n-\tring_size = flex_array_size(br, bufs, reg.ring_entries);\n+\tmmap_offset = (unsigned long)reg->bgid << IORING_OFF_PBUF_SHIFT;\n+\tring_size = flex_array_size(bl->buf_ring, bufs, reg->ring_entries);\n \n \tmemset(&rd, 0, sizeof(rd));\n \trd.size = PAGE_ALIGN(ring_size);\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP)) {\n-\t\trd.user_addr = reg.ring_addr;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP)) {\n+\t\trd.user_addr = reg->ring_addr;\n \t\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n \t}\n+\n \tret = io_create_region(ctx, &bl->region, &rd, mmap_offset);\n \tif (ret)\n-\t\tgoto fail;\n-\tbr = io_region_get_ptr(&bl->region);\n+\t\treturn ret;\n+\tbl->buf_ring = io_region_get_ptr(&bl->region);\n \n #ifdef SHM_COLOUR\n \t/*\n@@ -656,25 +674,48 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t * should use IOU_PBUF_RING_MMAP instead, and liburing will handle\n \t * this transparently.\n \t */\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP) &&\n-\t    ((reg.ring_addr | (unsigned long)br) & (SHM_COLOUR - 1))) {\n-\t\tret = -EINVAL;\n-\t\tgoto fail;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP) &&\n+\t    ((reg->ring_addr | (unsigned long)bl->buf_ring) &\n+\t     (SHM_COLOUR - 1))) {\n+\t\tio_free_region(ctx->user, &bl->region);\n+\t\treturn -EINVAL;\n \t}\n #endif\n \n-\tbl->nr_entries = reg.ring_entries;\n-\tbl->mask = reg.ring_entries - 1;\n-\tbl->flags |= IOBL_BUF_RING;\n-\tbl->buf_ring = br;\n-\tif (reg.flags & IOU_PBUF_RING_INC)\n+\tif (reg->flags & IOU_PBUF_RING_INC)\n \t\tbl->flags |= IOBL_INC;\n+\n+\treturn 0;\n+}\n+\n+int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tunsigned int permitted_flags;\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tpermitted_flags = IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC;\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, permitted_flags);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_pbuf_ring(ctx, &reg, bl);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n \tret = io_buffer_add_list(ctx, bl, reg.bgid);\n-\tif (!ret)\n-\t\treturn 0;\n-fail:\n-\tio_free_region(ctx->user, &bl->region);\n-\tkfree(bl);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n \treturn ret;\n }\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a request to rename io_unregister_pbuf_ring() to a more generic name, as it will be used for both provided buffer rings and kernel-managed buffer rings. The author agrees with the suggestion and has made the change in this patch.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreement",
                "preparatory_change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Use the more generic name io_unregister_buf_ring() as this function will\nbe used for unregistering both provided buffer rings and kernel-managed\nbuffer rings.\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c     | 2 +-\n io_uring/kbuf.h     | 2 +-\n io_uring/register.c | 2 +-\n 3 files changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 850b836f32ee..aa9b70b72db4 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -719,7 +719,7 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \treturn ret;\n }\n \n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n {\n \tstruct io_uring_buf_reg reg;\n \tstruct io_buffer_list *bl;\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex bf15e26520d3..40b44f4fdb15 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -74,7 +74,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags);\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 594b1f2ce875..0882cb34f851 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -841,7 +841,7 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-\t\tret = io_unregister_pbuf_ring(ctx, arg);\n+\t\tret = io_unregister_buf_ring(ctx, arg);\n \t\tbreak;\n \tcase IORING_REGISTER_SYNC_CANCEL:\n \t\tret = -EINVAL;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about the implementation of kernel-managed buffer rings, specifically how to handle the allocation and management of buffers for these rings. The author explains that they have reused the existing validation and buffer list allocation helpers from earlier refactoring, and added new functions to support kernel-managed buffer rings. They also mention that the IOBL_KERNEL_MANAGED flag marks buffer lists as kernel-managed for appropriate handling in the I/O path.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "neutral explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for kernel-managed buffer rings (kmbuf rings), which allow\nthe kernel to allocate and manage the backing buffers for a buffer\nring, rather than requiring the application to provide and manage them.\n\nThis introduces two new registration opcodes:\n- IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring\n- IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring\n\nThe existing io_uring_buf_reg structure is extended with a union to\nsupport both application-provided buffer rings (pbuf) and kernel-managed\nbuffer rings (kmbuf):\n- For pbuf rings: ring_addr specifies the user-provided ring address\n- For kmbuf rings: buf_size specifies the size of each buffer. buf_size\n  must be non-zero and page-aligned.\n\nThe implementation follows the same pattern as pbuf ring registration,\nreusing the validation and buffer list allocation helpers introduced in\nearlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as\nkernel-managed for appropriate handling in the I/O path.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  15 ++++-\n io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-\n io_uring/kbuf.h               |   7 ++-\n io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++\n io_uring/memmap.h             |   4 ++\n io_uring/register.c           |   7 +++\n 6 files changed, 219 insertions(+), 6 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex fc473af6feb4..a0889c1744bd 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -715,6 +715,10 @@ enum io_uring_register_op {\n \t/* register bpf filtering programs */\n \tIORING_REGISTER_BPF_FILTER\t\t= 37,\n \n+\t/* register/unregister kernel-managed ring buffer group */\n+\tIORING_REGISTER_KMBUF_RING\t\t= 38,\n+\tIORING_UNREGISTER_KMBUF_RING\t\t= 39,\n+\n \t/* this goes last */\n \tIORING_REGISTER_LAST,\n \n@@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {\n \tIOU_PBUF_RING_INC\t= 2,\n };\n \n-/* argument for IORING_(UN)REGISTER_PBUF_RING */\n+/* argument for IORING_(UN)REGISTER_PBUF_RING and\n+ * IORING_(UN)REGISTER_KMBUF_RING\n+ */\n struct io_uring_buf_reg {\n-\t__u64\tring_addr;\n+\tunion {\n+\t\t/* used for pbuf rings */\n+\t\t__u64\tring_addr;\n+\t\t/* used for kmbuf rings */\n+\t\t__u32   buf_size;\n+\t};\n \t__u32\tring_entries;\n \t__u16\tbgid;\n \t__u16\tflags;\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex aa9b70b72db4..9bc36451d083 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -427,10 +427,13 @@ static int io_remove_buffers_legacy(struct io_ring_ctx *ctx,\n \n static void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)\n {\n-\tif (bl->flags & IOBL_BUF_RING)\n+\tif (bl->flags & IOBL_BUF_RING) {\n \t\tio_free_region(ctx->user, &bl->region);\n-\telse\n+\t\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\t\tkfree(bl->buf_ring);\n+\t} else {\n \t\tio_remove_buffers_legacy(ctx, bl, -1U);\n+\t}\n \n \tkfree(bl);\n }\n@@ -779,3 +782,77 @@ struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n \t\treturn NULL;\n \treturn &bl->region;\n }\n+\n+static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_buffer_list *bl,\n+\t\t\t       struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_uring_buf_ring *ring;\n+\tunsigned long ring_size;\n+\tvoid *buf_region;\n+\tunsigned int i;\n+\tint ret;\n+\n+\t/* allocate pages for the ring structure */\n+\tring_size = flex_array_size(ring, bufs, bl->nr_entries);\n+\tring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);\n+\tif (!ring)\n+\t\treturn -ENOMEM;\n+\n+\tret = io_create_region_multi_buf(ctx, &bl->region, bl->nr_entries,\n+\t\t\t\t\t reg->buf_size);\n+\tif (ret) {\n+\t\tkfree(ring);\n+\t\treturn ret;\n+\t}\n+\n+\t/* initialize ring buf entries to point to the buffers */\n+\tbuf_region = bl->region.ptr;\n+\tfor (i = 0; i < bl->nr_entries; i++) {\n+\t\tstruct io_uring_buf *buf = &ring->bufs[i];\n+\n+\t\tbuf->addr = (u64)(uintptr_t)buf_region;\n+\t\tbuf->len = reg->buf_size;\n+\t\tbuf->bid = i;\n+\n+\t\tbuf_region += reg->buf_size;\n+\t}\n+\tring->tail = bl->nr_entries;\n+\n+\tbl->buf_ring = ring;\n+\tbl->flags |= IOBL_KERNEL_MANAGED;\n+\n+\treturn 0;\n+}\n+\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, 0);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tif (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_kmbuf_ring(ctx, bl, &reg);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n+\tret = io_buffer_add_list(ctx, bl, reg.bgid);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n+\treturn ret;\n+}\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 40b44f4fdb15..62c80a1ebf03 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -7,9 +7,11 @@\n \n enum {\n \t/* ring mapped provided buffers */\n-\tIOBL_BUF_RING\t= 1,\n+\tIOBL_BUF_RING\t\t= 1,\n \t/* buffers are consumed incrementally rather than always fully */\n-\tIOBL_INC\t= 2,\n+\tIOBL_INC\t\t= 2,\n+\t/* buffers are kernel managed */\n+\tIOBL_KERNEL_MANAGED\t= 4,\n };\n \n struct io_buffer_list {\n@@ -74,6 +76,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 89f56609e50a..8d37e93c0433 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -15,6 +15,28 @@\n #include \"rsrc.h\"\n #include \"zcrx.h\"\n \n+static void release_multi_buf_pages(struct page **pages, unsigned long nr_pages)\n+{\n+\tstruct page *page;\n+\tunsigned int nr, i = 0;\n+\n+\twhile (nr_pages) {\n+\t\tpage = pages[i];\n+\n+\t\tif (!page || WARN_ON_ONCE(page != compound_head(page)))\n+\t\t\treturn;\n+\n+\t\tnr = compound_nr(page);\n+\t\tput_page(page);\n+\n+\t\tif (WARN_ON_ONCE(nr > nr_pages))\n+\t\t\treturn;\n+\n+\t\ti += nr;\n+\t\tnr_pages -= nr;\n+\t}\n+}\n+\n static bool io_mem_alloc_compound(struct page **pages, int nr_pages,\n \t\t\t\t  size_t size, gfp_t gfp)\n {\n@@ -86,6 +108,8 @@ enum {\n \tIO_REGION_F_USER_PROVIDED\t\t= 2,\n \t/* only the first page in the array is ref'ed */\n \tIO_REGION_F_SINGLE_REF\t\t\t= 4,\n+\t/* pages in the array belong to multiple discrete allocations */\n+\tIO_REGION_F_MULTI_BUF\t\t\t= 8,\n };\n \n void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n@@ -98,6 +122,8 @@ void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n \n \t\tif (mr->flags & IO_REGION_F_USER_PROVIDED)\n \t\t\tunpin_user_pages(mr->pages, nr_refs);\n+\t\telse if (mr->flags & IO_REGION_F_MULTI_BUF)\n+\t\t\trelease_multi_buf_pages(mr->pages, nr_refs);\n \t\telse\n \t\t\trelease_pages(mr->pages, nr_refs);\n \n@@ -149,6 +175,54 @@ static int io_region_pin_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+static int io_region_allocate_pages_multi_buf(struct io_mapped_region *mr,\n+\t\t\t\t\t      unsigned int nr_bufs,\n+\t\t\t\t\t      unsigned int buf_size)\n+{\n+\tgfp_t gfp = GFP_USER | __GFP_ACCOUNT | __GFP_ZERO | __GFP_NOWARN;\n+\tstruct page **pages, **cur_pages;\n+\tunsigned int nr_allocated;\n+\tunsigned int buf_pages;\n+\tunsigned int i;\n+\n+\tif (!PAGE_ALIGNED(buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbuf_pages = buf_size >> PAGE_SHIFT;\n+\n+\tpages = kvmalloc_array(mr->nr_pages, sizeof(*pages), gfp);\n+\tif (!pages)\n+\t\treturn -ENOMEM;\n+\n+\tcur_pages = pages;\n+\n+\tfor (i = 0; i < nr_bufs; i++) {\n+\t\tif (io_mem_alloc_compound(cur_pages, buf_pages, buf_size,\n+\t\t\t\t\t  gfp)) {\n+\t\t\tcur_pages += buf_pages;\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tnr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,\n+\t\t\t\t\t\t     buf_pages, cur_pages);\n+\t\tif (nr_allocated != buf_pages) {\n+\t\t\tunsigned int total =\n+\t\t\t\t(cur_pages - pages) + nr_allocated;\n+\n+\t\t\trelease_multi_buf_pages(pages, total);\n+\t\t\tkvfree(pages);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tcur_pages += buf_pages;\n+\t}\n+\n+\tmr->flags |= IO_REGION_F_MULTI_BUF;\n+\tmr->pages = pages;\n+\n+\treturn 0;\n+}\n+\n static int io_region_allocate_pages(struct io_mapped_region *mr,\n \t\t\t\t    struct io_uring_region_desc *reg,\n \t\t\t\t    unsigned long mmap_offset)\n@@ -181,6 +255,43 @@ static int io_region_allocate_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size)\n+{\n+\tunsigned int nr_pages;\n+\tint ret;\n+\n+\tif (WARN_ON_ONCE(mr->pages || mr->ptr || mr->nr_pages))\n+\t\treturn -EFAULT;\n+\n+\tif (WARN_ON_ONCE(!nr_bufs || !buf_size || !PAGE_ALIGNED(buf_size)))\n+\t\treturn -EINVAL;\n+\n+\tif (check_mul_overflow(buf_size >> PAGE_SHIFT, nr_bufs, &nr_pages))\n+\t\treturn -EINVAL;\n+\n+\tif (ctx->user) {\n+\t\tret = __io_account_mem(ctx->user, nr_pages);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\t}\n+\tmr->nr_pages = nr_pages;\n+\n+\tret = io_region_allocate_pages_multi_buf(mr, nr_bufs, buf_size);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\tret = io_region_init_ptr(mr);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\treturn 0;\n+out_free:\n+\tio_free_region(ctx->user, mr);\n+\treturn ret;\n+}\n+\n int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset)\ndiff --git a/io_uring/memmap.h b/io_uring/memmap.h\nindex f4cfbb6b9a1f..3aa1167462ae 100644\n--- a/io_uring/memmap.h\n+++ b/io_uring/memmap.h\n@@ -22,6 +22,10 @@ int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset);\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size);\n+\n static inline void *io_region_get_ptr(struct io_mapped_region *mr)\n {\n \treturn mr->ptr;\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 0882cb34f851..2db8daaf8fde 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -837,7 +837,14 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\t\tbreak;\n \t\tret = io_register_pbuf_ring(ctx, arg);\n \t\tbreak;\n+\tcase IORING_REGISTER_KMBUF_RING:\n+\t\tret = -EINVAL;\n+\t\tif (!arg || nr_args != 1)\n+\t\t\tbreak;\n+\t\tret = io_register_kmbuf_ring(ctx, arg);\n+\t\tbreak;\n \tcase IORING_UNREGISTER_PBUF_RING:\n+\tcase IORING_UNREGISTER_KMBUF_RING:\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the implementation of kernel-managed buffer rings (kmbuf) and how it interacts with existing buffer ring logic, specifically in the io_buf_get_region() helper function. The author explained that kmbuf rings use the same pattern as application-provided buffer rings (pbuf), but introduced new mmap offset constants to encode the buffer group ID. They also added a kernel_managed parameter to the io_buf_get_region() function to handle the case where the buffer ring is managed by the kernel.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for mmapping kernel-managed buffer rings (kmbuf) to\nuserspace, allowing applications to access the kernel-allocated buffers.\n\nSimilar to application-provided buffer rings (pbuf), kmbuf rings use the\nbuffer group ID encoded in the mmap offset to identify which buffer ring\nto map. The implementation follows the same pattern as pbuf rings.\n\nNew mmap offset constants are introduced:\n  - IORING_OFF_KMBUF_RING (0x88000000): Base offset for kmbuf mappings\n  - IORING_OFF_KMBUF_SHIFT (16): Shift value to encode buffer group ID\n\nThe mmap offset encodes the bgid shifted by IORING_OFF_KMBUF_SHIFT.\nThe io_buf_get_region() helper retrieves the appropriate region.\n\nThis allows userspace to mmap the kernel-allocated buffer region and\naccess the buffers directly.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  2 ++\n io_uring/kbuf.c               | 11 +++++++++--\n io_uring/kbuf.h               |  5 +++--\n io_uring/memmap.c             |  5 ++++-\n 4 files changed, 18 insertions(+), 5 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex a0889c1744bd..42a2812c9922 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -545,6 +545,8 @@ struct io_uring_cqe {\n #define IORING_OFF_SQES\t\t\t0x10000000ULL\n #define IORING_OFF_PBUF_RING\t\t0x80000000ULL\n #define IORING_OFF_PBUF_SHIFT\t\t16\n+#define IORING_OFF_KMBUF_RING\t\t0x88000000ULL\n+#define IORING_OFF_KMBUF_SHIFT\t\t16\n #define IORING_OFF_MMAP_MASK\t\t0xf8000000ULL\n \n /*\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9bc36451d083..ccf5b213087b 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -770,16 +770,23 @@ int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg)\n \treturn 0;\n }\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid)\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed)\n {\n \tstruct io_buffer_list *bl;\n+\tbool is_kernel_managed;\n \n \tlockdep_assert_held(&ctx->mmap_lock);\n \n \tbl = xa_load(&ctx->io_bl_xa, bgid);\n \tif (!bl || !(bl->flags & IOBL_BUF_RING))\n \t\treturn NULL;\n+\n+\tis_kernel_managed = !!(bl->flags & IOBL_KERNEL_MANAGED);\n+\tif (is_kernel_managed != kernel_managed)\n+\t\treturn NULL;\n+\n \treturn &bl->region;\n }\n \ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 62c80a1ebf03..11d165888b8e 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -88,8 +88,9 @@ unsigned int __io_put_kbufs(struct io_kiocb *req, struct io_buffer_list *bl,\n bool io_kbuf_commit(struct io_kiocb *req,\n \t\t    struct io_buffer_list *bl, int len, int nr);\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid);\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed);\n \n static inline bool io_kbuf_recycle_ring(struct io_kiocb *req,\n \t\t\t\t\tstruct io_buffer_list *bl)\ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 8d37e93c0433..916315122323 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -356,7 +356,10 @@ static struct io_mapped_region *io_mmap_get_region(struct io_ring_ctx *ctx,\n \t\treturn &ctx->sq_region;\n \tcase IORING_OFF_PBUF_RING:\n \t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_PBUF_SHIFT;\n-\t\treturn io_pbuf_get_region(ctx, id);\n+\t\treturn io_buf_get_region(ctx, id, false);\n+\tcase IORING_OFF_KMBUF_RING:\n+\t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_KMBUF_SHIFT;\n+\t\treturn io_buf_get_region(ctx, id, true);\n \tcase IORING_MAP_OFF_PARAM_REGION:\n \t\treturn &ctx->param_region;\n \tcase IORING_MAP_OFF_ZCRX_REGION:\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about distinguishing between kernel-managed buffer addresses and negative values when error checking, explaining that the io_br_sel struct needs to be modified to separate address and value fields for kernel-managed buffers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Allow kernel-managed buffers to be selected. This requires modifying the\nio_br_sel struct to separate the fields for address and val, since a\nkernel address cannot be distinguished from a negative val when error\nchecking.\n\nAuto-commit any selected kernel-managed buffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring_types.h |  8 ++++----\n io_uring/kbuf.c                | 16 ++++++++++++----\n 2 files changed, 16 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 3e4a82a6f817..36cc2e0346d9 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -93,13 +93,13 @@ struct io_mapped_region {\n  */\n struct io_br_sel {\n \tstruct io_buffer_list *buf_list;\n-\t/*\n-\t * Some selection parts return the user address, others return an error.\n-\t */\n \tunion {\n+\t\t/* for classic/ring provided buffers */\n \t\tvoid __user *addr;\n-\t\tssize_t val;\n+\t\t/* for kernel-managed buffers */\n+\t\tvoid *kaddr;\n \t};\n+\tssize_t val;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex ccf5b213087b..1e8395270227 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -155,7 +155,8 @@ static int io_provided_buffers_select(struct io_kiocb *req, size_t *len,\n \treturn 1;\n }\n \n-static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n+static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n+\t\t\t     unsigned int issue_flags)\n {\n \t/*\n \t* If we came in unlocked, we have no choice but to consume the\n@@ -170,7 +171,11 @@ static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n \tif (issue_flags & IO_URING_F_UNLOCKED)\n \t\treturn true;\n \n-\t/* uring_cmd commits kbuf upfront, no need to auto-commit */\n+\t/* kernel-managed buffers are auto-committed */\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\treturn true;\n+\n+\t/* multishot uring_cmd commits kbuf upfront, no need to auto-commit */\n \tif (!io_file_can_poll(req) && req->opcode != IORING_OP_URING_CMD)\n \t\treturn true;\n \treturn false;\n@@ -200,9 +205,12 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n-\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n+\telse\n+\t\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n \n-\tif (io_should_commit(req, issue_flags)) {\n+\tif (io_should_commit(req, bl, issue_flags)) {\n \t\tio_kbuf_commit(req, sel.buf_list, *len, 1);\n \t\tsel.buf_list = NULL;\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about userspace unregistering a buffer ring while it is pinned by the kernel, explaining that adding kernel APIs to pin and unpin buffer rings will prevent this issue. The author provided code changes to implement these new APIs.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix",
                "provided implementation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add kernel APIs to pin and unpin buffer rings, preventing userspace from\nunregistering a buffer ring while it is pinned by the kernel.\n\nThis provides a mechanism for kernel subsystems to safely access buffer\nring contents while ensuring the buffer ring remains valid. A pinned\nbuffer ring cannot be unregistered until explicitly unpinned. On the\nuserspace side, trying to unregister a pinned buffer will return -EBUSY.\n\nThis is a preparatory change for upcoming fuse usage of kernel-managed\nbuffer rings. It is necessary for fuse to pin the buffer ring because\nfuse may need to select a buffer in atomic contexts, which it can only\ndo so by using the underlying buffer list pointer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 17 +++++++++++++\n io_uring/kbuf.c              | 48 ++++++++++++++++++++++++++++++++++++\n io_uring/kbuf.h              |  5 ++++\n 3 files changed, 70 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 375fd048c4cb..702b1903e6ee 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -84,6 +84,10 @@ struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n \t\t\t\t struct io_br_sel *sel, unsigned int issue_flags);\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t    unsigned issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -126,6 +130,19 @@ static inline bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n {\n \treturn true;\n }\n+static inline int io_uring_buf_ring_pin(struct io_uring_cmd *cmd,\n+\t\t\t\t\tunsigned buf_group,\n+\t\t\t\t\tunsigned issue_flags,\n+\t\t\t\t\tstruct io_buffer_list **bl)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n+static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned buf_group,\n+\t\t\t\t\t  unsigned issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 1e8395270227..dee1764ed19f 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -9,6 +9,7 @@\n #include <linux/poll.h>\n #include <linux/vmalloc.h>\n #include <linux/io_uring.h>\n+#include <linux/io_uring/cmd.h>\n \n #include <uapi/linux/io_uring.h>\n \n@@ -237,6 +238,51 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \treturn sel;\n }\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *buffer_list;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbuffer_list = io_buffer_get_list(ctx, buf_group);\n+\tif (buffer_list && (buffer_list->flags & IOBL_BUF_RING)) {\n+\t\tif (unlikely(buffer_list->flags & IOBL_PINNED)) {\n+\t\t\tret = -EALREADY;\n+\t\t} else {\n+\t\t\tbuffer_list->flags |= IOBL_PINNED;\n+\t\t\tret = 0;\n+\t\t\t*bl = buffer_list;\n+\t\t}\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_pin);\n+\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t       unsigned issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (bl && (bl->flags & IOBL_BUF_RING) && (bl->flags & IOBL_PINNED)) {\n+\t\tbl->flags &= ~IOBL_PINNED;\n+\t\tret = 0;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_unpin);\n+\n /* cap it at a reasonable 256, will be one page even for 4K */\n #define PEEK_MAX_IMPORT\t\t256\n \n@@ -747,6 +793,8 @@ int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t\treturn -ENOENT;\n \tif (!(bl->flags & IOBL_BUF_RING))\n \t\treturn -EINVAL;\n+\tif (bl->flags & IOBL_PINNED)\n+\t\treturn -EBUSY;\n \n \tscoped_guard(mutex, &ctx->mmap_lock)\n \t\txa_erase(&ctx->io_bl_xa, bl->bgid);\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 11d165888b8e..781630c2cc10 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -12,6 +12,11 @@ enum {\n \tIOBL_INC\t\t= 2,\n \t/* buffers are kernel managed */\n \tIOBL_KERNEL_MANAGED\t= 4,\n+\t/*\n+\t * buffer ring is pinned and cannot be unregistered by userspace until\n+\t * it has been unpinned\n+\t */\n+\tIOBL_PINNED\t\t= 8,\n };\n \n struct io_buffer_list {\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the need for an interface to recycle buffers back into kernel-managed buffer rings, explained that this is a preparatory patch for fuse over io-uring and added a new function io_uring_kmbuf_recycle() to handle recycling. The author did not explicitly state that they will fix or restructure anything in response to the concern.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "neutral explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add an interface for buffers to be recycled back into a kernel-managed\nbuffer ring.\n\nThis is a preparatory patch for fuse over io-uring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 11 +++++++++\n io_uring/kbuf.c              | 44 ++++++++++++++++++++++++++++++++++++\n 2 files changed, 55 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 702b1903e6ee..a488e945f883 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -88,6 +88,10 @@ int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t    unsigned issue_flags);\n+\n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -143,6 +147,13 @@ static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n+\t\t\t\t\t unsigned int buf_group, u64 addr,\n+\t\t\t\t\t unsigned int len, unsigned int bid,\n+\t\t\t\t\t unsigned int issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex dee1764ed19f..17b6178be4ce 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -102,6 +102,50 @@ void io_kbuf_drop_legacy(struct io_kiocb *req)\n \treq->kbuf = NULL;\n }\n \n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags)\n+{\n+\tstruct io_kiocb *req = cmd_to_io_kiocb(cmd);\n+\tstruct io_ring_ctx *ctx = req->ctx;\n+\tstruct io_uring_buf_ring *br;\n+\tstruct io_uring_buf *buf;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tif (WARN_ON_ONCE(req->flags & REQ_F_BUFFERS_COMMIT))\n+\t\treturn ret;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\n+\tif (!bl || WARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING)) ||\n+\t    WARN_ON_ONCE(!(bl->flags & IOBL_KERNEL_MANAGED)))\n+\t\tgoto done;\n+\n+\tbr = bl->buf_ring;\n+\n+\tif (WARN_ON_ONCE((br->tail - bl->head) >= bl->nr_entries))\n+\t\tgoto done;\n+\n+\tbuf = &br->bufs[(br->tail) & bl->mask];\n+\n+\tbuf->addr = addr;\n+\tbuf->len = len;\n+\tbuf->bid = bid;\n+\n+\treq->flags &= ~REQ_F_BUFFER_RING;\n+\n+\tbr->tail++;\n+\tret = 0;\n+\n+done:\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_kmbuf_recycle);\n+\n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)\n {\n \tstruct io_ring_ctx *ctx = req->ctx;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the __io_uring_cmd_done() function not setting the selected buffer index for kernel-managed buffer rings, explaining that they added the io_uring_is_kmbuf_ring() function to check if a buffer ring is kernel-managed and will update the cmd->buf_index accordingly.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "preparatory patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "io_uring_is_kmbuf_ring() returns true if there is a kernel-managed\nbuffer ring at the specified buffer group.\n\nThis is a preparatory patch for upcoming fuse kernel-managed buffer\nsupport, which needs to ensure the buffer ring registered by the server\nis a kernel-managed buffer ring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h |  9 +++++++++\n io_uring/kbuf.c              | 20 ++++++++++++++++++++\n 2 files changed, 29 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex a488e945f883..04a937f6f4d3 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -92,6 +92,9 @@ int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t   u64 addr, unsigned int len, unsigned int bid,\n \t\t\t   unsigned int issue_flags);\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -154,6 +157,12 @@ static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned int buf_group,\n+\t\t\t\t\t  unsigned int issue_flags)\n+{\n+\treturn false;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 17b6178be4ce..797cc2f0a5e9 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -963,3 +963,23 @@ int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \n \treturn ret;\n }\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tbool is_kmbuf_ring = false;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (likely(bl) && (bl->flags & IOBL_KERNEL_MANAGED)) {\n+\t\tWARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING));\n+\t\tis_kmbuf_ring = true;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn is_kmbuf_ring;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_is_kmbuf_ring);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about the io_uring/cmd: set selected buffer index in __io_uring_cmd_done() patch, specifically that it doesn't export io_ring_buffer_select(). The author agrees to make this change and has already done so by adding an EXPORT_SYMBOL_GPL() macro. This preparatory patch will be needed for fuse io-uring, which needs to select a buffer from a kernel-managed bufring while the uring mutex may already be held.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed_to_make_change",
                "already_implemented"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Export io_ring_buffer_select() so that it may be used by callers who\npass in a pinned bufring without needing to grab the io_uring mutex.\n\nThis is a preparatory patch that will be needed by fuse io-uring, which\nwill need to select a buffer from a kernel-managed bufring while the\nuring mutex may already be held by in-progress commits, and may need to\nselect a buffer in atomic contexts.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 14 ++++++++++++++\n io_uring/kbuf.c              |  7 ++++---\n 2 files changed, 18 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 04a937f6f4d3..d4b5943bdeb1 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -95,6 +95,10 @@ int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \n bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t    unsigned int issue_flags);\n+\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -163,6 +167,16 @@ static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n {\n \treturn false;\n }\n+static inline struct io_br_sel io_ring_buffer_select(struct io_kiocb *req,\n+\t\t\t\t\t\t     size_t *len,\n+\t\t\t\t\t\t     struct io_buffer_list *bl,\n+\t\t\t\t\t\t     unsigned int issue_flags)\n+{\n+\tstruct io_br_sel sel = {\n+\t\t.val = -EOPNOTSUPP,\n+\t};\n+\treturn sel;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 797cc2f0a5e9..9a93f10d3214 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -226,9 +226,9 @@ static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n \treturn false;\n }\n \n-static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n-\t\t\t\t\t      struct io_buffer_list *bl,\n-\t\t\t\t\t      unsigned int issue_flags)\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags)\n {\n \tstruct io_uring_buf_ring *br = bl->buf_ring;\n \t__u16 tail, head = bl->head;\n@@ -261,6 +261,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \t}\n \treturn sel;\n }\n+EXPORT_SYMBOL_GPL(io_ring_buffer_select);\n \n struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \t\t\t\t  unsigned buf_group, unsigned int issue_flags)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring_cmd_buffer_select() function not returning the selected buffer's id for kernel-managed buffer rings, and modified the function to return the id in addition to the address and size.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Return the id of the selected buffer in io_buffer_select(). This is\nneeded for kernel-managed buffer rings to later recycle the selected\nbuffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h   | 2 +-\n include/linux/io_uring_types.h | 2 ++\n io_uring/kbuf.c                | 7 +++++--\n 3 files changed, 8 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex d4b5943bdeb1..94df2bdebe77 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);\n \n /*\n  * Select a buffer from the provided buffer group for multishot uring_cmd.\n- * Returns the selected buffer address and size.\n+ * Returns the selected buffer address, size, and id.\n  */\n struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n \t\t\t\t\t    unsigned buf_group, size_t *len,\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 36cc2e0346d9..5a56bb341337 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -100,6 +100,8 @@ struct io_br_sel {\n \t\tvoid *kaddr;\n \t};\n \tssize_t val;\n+\t/* id of the selected buffer */\n+\tunsigned buf_id;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9a93f10d3214..24c1e34ea23e 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -250,6 +250,7 @@ struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n+\tsel.buf_id = req->buf_index;\n \tif (bl->flags & IOBL_KERNEL_MANAGED)\n \t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n \telse\n@@ -274,10 +275,12 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \n \tbl = io_buffer_get_list(ctx, buf_group);\n \tif (likely(bl)) {\n-\t\tif (bl->flags & IOBL_BUF_RING)\n+\t\tif (bl->flags & IOBL_BUF_RING) {\n \t\t\tsel = io_ring_buffer_select(req, len, bl, issue_flags);\n-\t\telse\n+\t\t} else {\n \t\t\tsel.addr = io_provided_buffer_select(req, len, bl);\n+\t\t\tsel.buf_id = req->buf_index;\n+\t\t}\n \t}\n \tio_ring_submit_unlock(req->ctx, issue_flags);\n \treturn sel;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about indicating which buffer was selected in the completion queue entry, explaining that this is necessary for fuse to relay the information to userspace and agreeing to add the IORING_CQE_F_BUFFER flag along with the buffer index.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed to address a concern",
                "explained the necessity of the change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When uring_cmd operations select a buffer, the completion queue entry\nshould indicate which buffer was selected.\n\nSet IORING_CQE_F_BUFFER on the completed entry and encode the buffer\nindex if a buffer was selected.\n\nThis will be needed for fuse, which needs to relay to userspace which\nselected buffer contains the data.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/uring_cmd.c | 6 +++++-\n 1 file changed, 5 insertions(+), 1 deletion(-)\n\ndiff --git a/io_uring/uring_cmd.c b/io_uring/uring_cmd.c\nindex ee7b49f47cb5..6d38df1a812d 100644\n--- a/io_uring/uring_cmd.c\n+++ b/io_uring/uring_cmd.c\n@@ -151,6 +151,7 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \t\t       unsigned issue_flags, bool is_cqe32)\n {\n \tstruct io_kiocb *req = cmd_to_io_kiocb(ioucmd);\n+\tu32 cflags = 0;\n \n \tif (WARN_ON_ONCE(req->flags & REQ_F_APOLL_MULTISHOT))\n \t\treturn;\n@@ -160,7 +161,10 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \tif (ret < 0)\n \t\treq_set_fail(req);\n \n-\tio_req_set_res(req, ret, 0);\n+\tif (req->flags & (REQ_F_BUFFER_SELECTED | REQ_F_BUFFER_RING))\n+\t\tcflags |= IORING_CQE_F_BUFFER |\n+\t\t\t(req->buf_index << IORING_CQE_BUFFER_SHIFT);\n+\tio_req_set_res(req, ret, cflags);\n \tif (is_cqe32) {\n \t\tif (req->ctx->flags & IORING_SETUP_CQE_MIXED)\n \t\t\treq->cqe.flags |= IORING_CQE_F_32;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "The reviewer suggested adding a WARN_ON_ONCE() check to prevent int promotion from affecting the calculation of (br->tail - bl->head) >= bl->nr_entries, and acknowledged that having multiple WARN_ON_ONCE() instances is not a significant issue for now.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested change",
                "acknowledged minor issue"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you want:\n\n\tif (WARN_ON_ONCE((__u16)(br->tail - bl->head) >= bl->nr_entries))\n\nhere to avoid int promotion from messing this up if tail has wrapped.\n\nIn general, across the patches for the WARN_ON_ONCE(), it's not a huge\nissue to have a litter of them for now. Hopefully we can prune some of\nthese down the line, however.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe questioned the need to set the selected buffer index in __io_uring_cmd_done(), suggesting that req->buf_index could be used instead, and requested clarification on why this is necessary.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "lack of clarity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm probably missing something here, but why can't the caller just use\nreq->buf_index for this?\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe requested a branch with all patches, including user code, for easier cross-referencing and evaluation of helper functions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "cross referencing",
                "evaluation of helpers"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Generally looks pretty good - for context, do you have a branch with\nthese patches and the users on top too? Makes it a bit easier for cross\nreferencing, as some of these really do need an exposed user to make a\ngood judgement on the helpers.\n\nI know there's the older series, but I'm assuming the latter patches\nchanged somewhat too, and it'd be nicer to look at a current set rather\nthan go back to the older ones.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested refactoring io_pbuf_get_region() to handle kernel-managed buffer rings by adding a new helper function, io_kbuf_get_region(), and modifying the existing function to check for the IOBL_KERNEL_MANAGED flag.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor nit",
                "more readable"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "For this, I think just add another helper - leave io_pbuf_get_region()\nand add a bl->flags & IOBL_KERNEL_MANAGED error check in there, and\nadd a io_kbuf_get_region() or similar and have a !(bl->flags &\nIOBL_KERNEL_MANAGED) error check in that one.\n\nThat's easier to read, and there's little reason to avoid duplicating\nthe xa_load() part.\n\nMinor nit, but imho it's more readable that way.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Jens Axboe suggested using a pointer to struct io_buffer_list *bl instead of passing it by value, and recommended either returning an ERR_PTR or renaming the parameter to **blret",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggestion",
                "recommendation"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Probably use the usual struct io_buffer_list *bl here and either use an\nERR_PTR return, or rename the passed on **bl to **blret or something.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Jens Axboe suggested a more efficient way to check for pinned buffer rings by using a bitwise AND operation on the bl->flags, and also recommended adding an early return statement if bl is NULL.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested improvement",
                "code optimization"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Usually done as:\n\n\tif ((bl->flags & (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))\n\nand maybe then just have an earlier\n\n\tif (!bl)\n\t\tgoto err;",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested that the patch should not limit the length of io_uring command strings, citing that exceeding 80 characters is acceptable for this specific use case.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested change",
                "clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "to avoid making it way too long. For io_uring, it's fine to exceed 80\nchars where it makes sense.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer noted that the patch unnecessarily restricts the ability to use user-passed memory when creating a region, potentially missing out on optimizations such as huge page usage",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "restricting user-passed memory",
                "missing optimization opportunities"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "If you're creating a region, there should be no reason why it\ncan't work with user passed memory. You're fencing yourself off\noptimisations that are already there like huge pages.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that io_create_region() should be used instead of allocating buffers directly in IORING_REGISTER_KMBUF_RING, and suggested stripping buffer allocation from this function to improve abstractions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Please use io_create_region(), the new function does nothing new\nand only violates abstractions.\n\nProvided buffer rings with kernel addresses could be an interesting\nabstraction, but why is it also responsible for allocating buffers?\nWhat I'd do:\n\n1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.\n2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.\n    Or maybe don't expose it to the user at all and create it from\n    fuse via internal API.\n3. Require the user to register a memory region of appropriate size,\n    see IORING_REGISTER_MEM_REGION, ctx->param_region. Make fuse\n    populating the buffer ring using the memory region.\n\nI wanted to make regions shareable anyway (need it for other purposes),\nI can toss patches for that tomorrow.\n\nA separate question is whether extending buffer rings is the right\napproach as it seems like you're only using it for fuse requests and\nnot for passing buffers to normal requests, but I don't see the\nbig picture here.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that the patch's alignment requirement for buffers is unnecessary and wasteful due to the removal of io_create_region_multi_buf(), suggesting a potential optimization opportunity.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "optimization",
                "wasted memory"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "With io_create_region_multi_buf() gone, you shouldn't need\nto align every buffer, that could be a lot of wasted memory\n(thinking about 64KB pages).",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Caleb Mateos",
              "summary": "Reviewer Caleb Mateos noted that the patch's optimization in __io_uring_cmd_done() is unnecessary, as modern compilers will automatically perform this optimization and potentially optimize it further.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes",
                "optimization"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, modern compilers will perform this optimization automatically.\nThey'll even optimize it further to !(~bl->flags &\n(IOBL_BUF_RING|IOBL_PINNED)): https://godbolt.org/z/xGoP4TfhP\n\nBest,\nCaleb",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested that the code should follow a common pattern for better readability, citing that the current implementation is easier to read compared to the original.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested improvement",
                "cited example"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure, it's not about that, it's more about the common way of doing it,\nwhich makes it easier to read for people. FWIW, your example is easier\nto read too than the original.\n\n-- \nJens Axboe",
              "reply_to": "Caleb Mateos",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author is asking for clarification on whether kernel-allocated buffers can handle huge pages, specifically referencing the alloc_pages() call in io_mem_alloc_compound().",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Are there any optimizations with user-allocated buffers that wouldn't\nbe possible with kernel-allocated buffers? For huge pages, can't the\nkernel do this as well (eg I see in io_mem_alloc_compound(), it calls\ninto alloc_pages() with order > 0)?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong acknowledged that separate checks are needed between io_create_region() and io_create_region_multi_buf(), disagreed with the suggestion to use only io_create_region(), and did not promise a fix.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "disagreement",
                "lack of commitment to fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "There's separate checks needed between io_create_region() and\nio_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag\nchecking) and different allocation calls (eg\nio_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).\nMaybe I'm misinterpreting your comment (or the code), but I'm not\nseeing how this can just use io_create_region().",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to feedback about buffer allocation, stating that kernel-managed buffers simplify interface and lifecycle management, and guarantee contiguous page allocation, but asked for elaboration on the benefits of user-space allocation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "asked_for_clarification",
                "provided_explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Conceptually, I think it makes the interface and lifecycle management\nsimpler/cleaner. With registering it from userspace, imo there's\nadditional complications with no tangible benefits, eg it's not\nguaranteed that the memory regions registered for the buffers are the\nsame size, with allocating it from the kernel-side we can guarantee\nthat the pages are allocated physically contiguously, userspace setup\nwith user-allocated buffers is less straightforward, etc. In general,\nI'm just not really seeing what advantages there are in allocating the\nbuffers from userspace. Could you elaborate on that part more?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern about the impact of squashing kernel-managed buffer rings into existing pbuf rings, explaining that pbuf rings would need to support pinning and citing a dropped patch from earlier in the series.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical issue",
                "provided explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If kmbuf rings are squashed into pbuf rings, then pbuf rings will need\nto support pinning. In fuse, there are some contexts where you can't\ngrab the uring mutex because you're running in atomic context and this\ncan be encountered while recycling the buffer. I originally had a\npatch adding pinning to pbuf rings (to mitigate the overhead of\nregistered buffers lookups) but dropped it when Jens and Caleb didn't\nlike the idea. But for kmbuf rings, pinning will be necessary for\nfuse.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to Pavel Begunkov's question about the meaning of 'normal requests' in the context of io_uring buffer rings, clarifying that for fuse's use case there are only fuse requests.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "What are 'normal requests'? For fuse's use case, there are only fuse requests.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that accessing the selected buffer index from the caller side can be cumbersome and proposed an alternative solution, such as introducing a helper function to retrieve the buffer ID.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "proposed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The caller can, but from the caller side they only have access to the\ncmd so they would need to do something like\n\nstruct io_kiocb *req = cmd_to_iocb_kiocb(ent->cmd);\nbuf_id = req->buf_index;\n\nwhich may be kind of ugly with looking inside io-uring internals.\nMaybe a helper here would be nicer, something like\nio_uring_cmd_buf_id() or io_uring_req_buf_id(). It seemed cleaner to\nme to just return the buf id as part of the io_br_sel struct, but I'm\nhappy to do it another way if you have a preference.\n\nThanks,\nJoanne",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged the need for further revision, promised to address reviewer's comments in v2, and mentioned that another discussion is pending before submitting v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for revision",
                "promised fix in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for reviewing the patches. The branch containing the userside\nchanges on top of these patches is in [1]. I'll make the changes you\npointed out in your other comments as part of v2. Once the discussion\nwith Pavel is resolved / figured out with the changes he wants for v2,\nI'll submit v2.\n\nThanks,\nJoanne\n\n[1] https://github.com/joannekoong/linux/commits/fuse_zero_copy/",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that allocating 1MB in kernel space will not result in a PMD mappable huge page, unlike user space allocation which can register 2MB and reuse the rest for other purposes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change",
                "technical concern"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, there is handful of differences. To name one, 1MB allocation won't\nget you a PMD mappable huge page, while user space can allocate 2MB,\nregister the first 1MB and reuse the rest for other purposes.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer suggested that instead of modifying io_create_region() to be less strict, the caller should filter arguments to ensure only necessary types are passed",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If io_create_region() is too strict, let's discuss that in\nexamples if there are any, but it's likely not a good idea changing\nthat. If it's too lax, filter arguments in the caller. IOW, don't\npass IORING_MEM_REGION_TYPE_USER if it's not used.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the memmap.c changes in this patch are unnecessary and can be dropped because they only provide contiguous memory within a single buffer, which is already achieved by default io_create_region(). He suggested removing these changes to avoid disabling the usefulness of io_mem_alloc_compound() and to simplify the code.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I saw that and saying that all memmap.c changes can get dropped.\nYou're using it as one big virtually contig kernel memory range then\nchunked into buffers, and that's pretty much what you're getting with\nnormal io_create_region(). I get that you only need it to be\ncontiguous within a single buffer, but that's not what you're doing,\nand it'll be only worse than default io_create_region() e.g.\neffectively disabling any usefulness of io_mem_alloc_compound(),\nand ultimately you don't need to care.\n\nRegions shouldn't know anything about your buffers, how it's\nsubdivided after, etc.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested adding a check for user-provided memory to the io_create_region() call, similar to how it's handled in the code snippet he provided.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change",
                "code example"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "struct io_uring_region_desc rd = {};\ntotal_size = nr_bufs * buf_size;\nrd.size = PAGE_ALIGN(total_size);\nio_create_region(&region, &rd);\n\nAdd something like this for user provided memory:\n\nif (use_user_memory) {\n\trd.user_addr = uaddr;\n\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n}",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov suggested separating ring population from kernel API, noting that the fuse kernel module can populate rings and achieve the same layout as the current implementation, and proposed using empty rings to enable future IO operations.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested separation",
                "proposed alternative"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think I follow. I'm saying that it might be interesting\nto separate rings from how and with what they're populated on the\nkernel API level, but the fuse kernel module can do the population\nand get exactly same layout as you currently have:\n\nint fuse_create_ring(size_t region_offset /* user space argument */) {\n\tstruct io_mapped_region *mr = get_mem_region(ctx);\n\t// that can take full control of the ring\n\tring = grab_empty_ring(io_uring_ctx);\n\n\tsize = nr_bufs * buf_size;\n\tif (region_offset + size > get_size(mr)) // + other validation\n\t\treturn error;\n\n\tbuf = mr_get_ptr(mr) + offset;\n\tfor (i = 0; i < nr_bufs; i++) {\n\t\tring_push_buffer(ring, buf, buf_size);\n\t\tbuf += buf_size;\n\t}\n}\n\nfuse might not care, but with empty rings other users will get a\nchannel they can use to do IO (e.g. read requests) using their\nkernel addresses in the future.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested that instead of introducing a new set of UAPI functions for kernel-managed buffer rings, the existing pbuf implementation could be modified to support this feature by adding a flag and piggybacking on its functionality.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested an alternative approach",
                "did not express strong opinion"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It'd change uapi but not internals, you already piggy back it\non pbuf implementation and differentiate with a flag.\n\nIt could basically be:\n\nif (flags & IOU_PBUF_RING_KM)\n\tbl->flags |= IOBL_KERNEL_MANAGED;\n\nPinning can be gated on that flag as well. Pretty likely uapi\nand internals will be a bit cleaner, but that's not a huge deal,\njust don't see why would you roll out a separate set of uapi\n([un]register, offsets, etc.) when essentially it can be treated\nas the same thing.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer noted that the patch did not provide buffer rings when pinning the registered buffer table, and suggested using a single larger registered buffer instead",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IIRC, you was pinning the registered buffer table and not provided\nbuffer rings? Which would indeed be a bad idea. Thinking about it,\nfwiw, instead of creating multiple registered buffers and trying to\nlock the entire table, you could've kept all memory in one larger\nregistered buffer and pinned only it. It's already refcounted, so\nshouldn't have been much of a problem.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov expressed concerns that creating many small regions for kernel-managed buffer rings would lead to extra mmap()s, user space management overhead, and wasted space, also questioning the ring bound memory approach due to potential issues with buffer lifetimes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "expressed concerns"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "To explain why, I don't think that creating many small regions\nis a good direction going forward. In case of kernel allocation,\nit's extra mmap()s, extra user space management, and wasted space.\nFor user provided memory it's over-accounting and extra memory\nfootprint. It'll also give you better lifecycle guarantees, i.e.\nyou won't be able to free buffers while there are requests for the\ncontext. I'm not so sure about ring bound memory, let's say I have\nmy suspicions, and you'd need to be extra careful about buffer\nlifetimes even after a fuse instance dies.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that kernel-managed buffer rings would be particularly useful for operations like read and recv, where the kernel can fill the buffers without requiring opcode-specific code changes in kbuf.c.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Any kind of read/recv/etc. that can use provided buffers. It's\nwhere kernel memory filled rings would shine, as you'd be able\nto use them together without changing any opcode specific code.\nI.e. not changes in read request implementation, only kbuf.c\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that any pages mapped to userspace can be allocated in the kernel, and he likes the design of having a buffer ring only mapped read-only into userspace for zero-copy raids.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "appreciation for design",
                "positive comment on implementation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Any pages mapped to userspace can be allocated in the kernel as well.\n\nAnd I really do like this design, because it means we can have a\nbuffer ring that is only mapped read-only into userspace.  That way\nwe can still do zero-copy raids if the device requires stable pages\nfor checksumming or raid.  I was going to implement this as soon\nas this series lands upstream.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong addressed Pavel Begunkov's feedback that io_uring/cmd: set selected buffer index in __io_uring_cmd_done() should use io_create_region(), explaining that it fails due to allocating too much memory at once, and instead uses io_region_allocate_pages_multi_buf() to bypass allocation errors.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When I originally implemented it, I had it use\nio_region_allocate_pages() but this fails because it's allocating way\ntoo much memory at once. For fuse's use case, each buffer is usually\nat least 1 MB if not more. Allocating the memory one buffer a time in\nio_region_allocate_pages_multi_buf() bypasses the allocation errors I\nwas seeing. That's the main reason I don't think this can just use\nio_create_region().",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarifies her understanding of Pavel's feedback, confirming that she and Christoph initially thought the user should allocate buffers but now believes a different kernel allocation method is sufficient.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "understanding"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oh okay, from your first message I (and I think christoph too) thought\nwhat you were saying is that the user should be responsible for\nallocating the buffers with complete ownership over them, and then\njust pass those allocated to the kernel to use. But what you're saying\nis that just use a different way for getting the kernel to allocate\nthe buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am\nI reading this correctly?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the complexity of combining kernel-managed buffer rings (kmbufs) with regular pbufs, agreeing that having separate APIs makes it clearer what each one does. They plan to restructure in v2 unless someone objects.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "imo, it looked cleaner as a separate api because it has different\nexpectations and behaviors and squashing kmbuf into the pbuf api makes\nthe pbuf api needlessly more complex. Though I guess from the\nuserspace pov, liburing could have a wrapper that takes care of\nsetting up the pbuf details for kernel-managed pbufs. But in my head,\nhaving pbufs vs. kmbufs makes it clearer what each one does vs regular\npbufs vs. pbufs that are kernel-managed.\n\nEspecially with now having kmbufs go through the ioring mem region\ninterface, it makes things more confusing imo if they're combined, eg\npbufs that are kernel-managed are created empty and then populated\nfrom the kernel side by whatever subsystem is using them. Right now\nthere's only one mem region supported per ring, but in the future if\nthere's the possibility that multiple mem regions can be registered\n(eg if userspace doesn't know upfront what mem region length they'll\nneed), then we should also probably add in a region id param for the\nregistration arg, which if kmbuf rings go through the pbuf ring\nregistration api, is not possible to do.\n\nBut I'm happy to combine the interfaces and go with your suggestion.\nI'll make this change for v2 unless someone else objects.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that she previously dropped a patch to pin the registered buffer table, not the pbuf ring as initially claimed, but did not indicate plans for a fix or further revision.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged prior mistake",
                "did not commit to fixing"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah, you're right I misremembered and the objections / patch I\ndropped was pinning the registered buffer table, not the pbuf ring",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's suggestion that sparse buffers populated by the kernel should be automatically pinned, but expressed concerns about the user experience and potential performance impact if unregistration requires individual buffer handling.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "concerns about user experience",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hmm, I'm not sure this idea would work for sparse buffers populated by\nthe kernel, unless those are automatically pinned too but then from\nthe user POV for unregistration they'd need to unregister buffers\nindividually instead of just calling IORING_UNREGISTER_BUFFERS but it\nmight be annoying for them to now need to know which buffers are\npinned vs not. When i benchmarked the fuse code with vs without pinned\nregistered buffers, it didn't seem to make much of a difference\nperformance-wise thankfully, so I just dropped it.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author is addressing a concern about buffer allocation, specifically whether individual buffers should be allocated separately by the kernel. She agrees that separate allocation could bypass memory allocation issues and is open to making this change if it's possible to allocate the entire region at once. However, she doesn't see how separate allocation would lead to extra mmapping or userspace management.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "open to revising patch",
                "no clear agreement on feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "To clarify, is this in reply to why the individual buffers shouldn't\nbe allocated separately by the kernel?\nI added a comment about this above in the discussion about\nio_region_allocate_pages_multi_buf(), and if the memory allocation\nissue I was seeing is bypassable and the region can be allocated all\nat once, I'm happy to make that change. With having the allocation be\nseparate buffers though, I'm not sure I agree that there are extra\nmmaps / userspace management. All the pages across the buffers are\nvmapped together and the userspace just needs to do 1 mmap call for\nthem. On the userspace side, I don't think there's more management\nsince the mmapped address represents the range across all the buffers.\nI'm not seeing how there's wasted space either since the only\nrequirement is that the buffer size is page aligned. I think also\nthere's a higher chance of the entire buffer region being physically\ncontiguous if each buffer is allocated separately vs. all the buffers\nare allocated as 1 region. I don't feel strongly about this either way\nand I'm happy to allocate the entire region at once if that's\npossible.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author asks for clarification on reviewer's concerns about over-accounting and extra memory footprint in kernel-managed buffer rings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "asking for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Just out of curiosity, could you elaborate on the over-accounting and\nextra memory footprint? I was under the impression it would be the\nsame since the accounting gets adjusted by the total bytes allocated?\nFor the extra memory footprint, is the extra footprint from the\nmetadata to describe each buffer region, or are you referring to\nsomething else?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed concerns about API inconsistencies and buffer allocation in the kernel-managed buffer ring series by outlining planned changes for version 2, including modifying the userspace API to use PBUF_RING instead of KMBUF_RING and having kernel buffer allocation go through IORING_REGISTER_MEM_REGION.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "planned revisions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for your input on the series. To iterate / sum up, these are\nchanges for v2 I'll be making:\n- api-wise from userspace/liburing: get rid of KMBUF_RING api\ninterface and have users go through PBUF_RING api instead with a flag\nindicating the ring is kernel-managed\n- have kernel buffer allocation go through IORING_REGISTER_MEM_REGION\ninstead, which means when the pbuf ring is created and the\nkernel-managed flag is set, the ring will be empty. The memory region\nwill need to be registered before the mmap call to the ring fd.\n- add apis for subsystems to populate a kernel-managed buffer ring\nwith addresses from the registered mem region\n\nDoes this align with your understanding of the conversation as well or\nis there anything I'm missing?\n\nAnd Christoph, do these changes for v2 work for your use case as well?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig argued against setting the selected buffer index in __io_uring_cmd_done(), citing a need for kernel-controlled allocation and guaranteeing user processes can only read memory, not write to it.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm arguing exactly against this.  For my use case I need a setup\nwhere the kernel controls the allocation fully and guarantees user\nprocesses can only read the memory but never write to it.  I'd love\nto be able to piggy back than onto your work.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that using power-of-2 round ups for memory allocations will waste memory, as 1MB allocations will not become 2MB huge pages, and questioned the handling of 1GB huge pages, suggesting users can make better placement decisions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "waste of memory",
                "questioning allocation strategy"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "pow2 round ups will waste memory. 1MB allocations will never\nbecome 2MB huge pages. And there is a separate question of\n1GB huge pages. The user can be smarter about all placement\ndecisions.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer suggests that the io_uring uapi should include fields for user-provided memory, making it an optional feature for pbuf rings/regions/etc.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested feature",
                "optional"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That's an interesting case. To be clear, user provided memory is\nan optional feature for pbuf rings / regions / etc., and I think\nthe io_uring uapi should leave fields for the feature. However, I\nhave nothing against fuse refusing to bind to buffer rings it\ndoesn't like.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested modifying IORING_REGISTER_MEM_REGION to support read-only registrations, and proposed adding a new registration flag or rejecting unsupported setups during initialization.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "neutral suggestion",
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IORING_REGISTER_MEM_REGION supports both types of allocations. It can\nhave a new registration flag for read-only, and then you either make\nthe bounce avoidance optional or reject binding fuse to unsupported\nsetups during init. Any arguments against that? I need to go over\nJoanne's reply, but I don't see any contradiction in principal with\nyour use case.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarifies that kernel-managed buffer rings are used internally by the kernel and not explicitly registered by userspace, which is a different use case from IORING_REGISTER_MEM_REGION.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"control the allocation fully\" do you mean for your use case, the\nallocation/setup isn't triggered by userspace but is initiated by the\nkernel (eg user never explicitly registers any kbuf ring, the kernel\njust uses the kbuf ring data structure internally and users can read\nthe buffer contents)? If userspace initiates the setup of the kbuf\nring, going through IORING_REGISTER_MEM_REGION would be semantically\nthe same, except the buffer allocation by the kernel now happens\nbefore the ring is created and then later populated into the ring.\nuserspace would still need to make an mmap call to the region and the\nkernel could enforce that as read-only. But if userspace doesn't\ninitiate the setup, then going through IORING_REGISTER_MEM_REGION gets\nuglier.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged a potential over-engineering of the current design, suggesting an alternative approach where a straightforward kernel-managed buffer ring is added first and then an interface for populated rings can be added later.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "overkill",
                "over-engineered"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "So i guess the flow would have to be:\na) user calls io_uring_register_region(&ring, &mem_region_reg) with\nmem_region_reg.region_uptr's size field set to the total buffer size\n(and mem_region_reg.flags read-only bit set if needed)\n     kernel allocates region\nb) user calls mmap() to get the address of the region. If read-only\nbit was set, it gets a read-only address\nc) user calls io_uring_register_buf_ring(&ring, &buf_reg, flags) with\nbuf_reg.flags |= IOU_PBUF_RING_KERNEL_MANAGED\n     kernel creates an empty kernel-managed ring. None of the buffers\nare populated\nd) user tells X subsystem to populate the ring starting from offset Z\nin the registered mem region\ne) on the kernel side, the subsystem populates the ring starting from\noffset Z, filling it up using the buf_size and ring_entries values\nthat the user registered the ring with in c)\n\nTo be completely honest, the more I look at this the more this feels\nlike overkill / over-engineered to me. I get that now the user can do\nthe PMD optimization, but does that actually lead to noticeable\nperformance benefits? It seems especially confusing with them going\nthrough the same pbuf ring interface but having totally different\nexpectations.\n\nWhat about adding a straightforward kmbuf ring that goes through the\npbuf interface (eg the design in this patchset) and then in the future\nadding an interface for pbuf rings (both kernel-managed and\nnon-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if\nusers end up needing/wanting to have their rings populated that way?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer noted that instead of optimizing for buffer index selection, it would be simpler and more effective to address TLB pressure by rounding up the buffer allocation to a multiple of PTE levels.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested change",
                "alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sure.  But if the application cares that much about TLB pressure\nI'd just round up to nice multtiple of PTE levels.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer Christoph Hellwig asked for clarification on what 'pbuf' means in the context of io_uring buffer rings, indicating confusion about the fixed buffer API and suggesting that web searches have become unreliable.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Can you clarify what you mean with 'pbuf'?  The only fixed buffer API I\nknow is io_uring_register_buffers* which always takes user provided\nbuffers, so I have a hard time parsing what you're saying there.  But\nthat might just be sign that I'm no expert in io_uring APIs, and that\nweb searches have degraded to the point of not being very useful\nanymore.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer noted that IORING_REGISTER_MEM_REGION is primarily intended for command queues (cqs) and expressed confusion about its use in this patch, suggesting a potential mismatch between the commit message and documentation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "potential mismatch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "IORING_REGISTER_MEM_REGION seems to be all about cqs from both your\ncommit message and the public documentation.  I'm confused.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "reviewer noted that the patch does not address their specific use case of block and file system I/O, which is different from the fuse over io_uring series",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "use case mismatch"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "My use case is not about fuse, but good old block and file system\nI/O.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that io_uring_register_buffers() only pins memory, allowing applications or other processes to modify it, which can cause issues for file systems and storage devices that need to verify checksums or rebuild data from parity.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "implied need for additional work"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The idea is that the application tells the kernel that it wants to use\na fixed buffer pool for reads.  Right now the application does this\nusing io_uring_register_buffers().  The problem with that is that\nio_uring_register_buffers ends up just doing a pin of the memory,\nbut the application or, in case of shared memory, someone else could\nstill modify the memory.  If the underlying file system or storage\ndevice needs verify checksums, or worse rebuild data from parity\n(or uncompress), it needs to ensure that the memory it is operating\non can't be modified by someone else.\n\nSo I've been thinking of a version of io_uring_register_buffers where\nthe buffers are not provided by the application, but instead by the\nkernel and mapped into the application address space read-only for\na while, and I thought I could implement this on top of your series,\nbut I have to admit I haven't really looked into the details all\nthat much.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that the PMD mapping is not a significant concern, and both AMD and ARM architectures have optimizations for contiguous PTEs.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "neutral comment about optimization relevance"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes.  The PMD mapping also is not that relevant.  Both AMD (implicit)\nand ARM (explicit) have optimizations for contiguous PTEs that are\nalmost as valuable.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer noted that the patch does not clearly distinguish between provided buffer rings and kernel-managed buffer rings, and expressed concerns about inflexibility in allocating payload memory",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "infelicities in design",
                "lacking clarity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Registered, aka fixed, buffers are the ones you pass to\nIORING_OP_[READ,WRITE]_FIXED and some other requests. It's normally\ncreated by io_uring_register_buffers*() / IORING_REGISTER_BUFFERS*\nwith user memory, but there are special cases when it's installed\ninternally by other kernel components, e.g. ublk.\nThis series has nothing to do with them, and relevant parts of\nthe discussion here don't mention them either.\n\nProvided buffer rings, a.k.a pbuf rings, IORING_REGISTER_PBUF_RING\nis a kernel-user shared ring. The entries are user buffers\n{uaddr, size}. The user space adds entries, the kernel (io_uring\nrequests) consumes them and issues I/O using the user addresses.\nE.g. you can issue a IORING_OP_RECV request (+IOSQE_BUFFER_SELECT)\nand it'll grab a buffer from the ring instead of using sqe->addr.\n\npbuf rings, IORING_REGISTER_MEM_REGION, completion/submission\nqueues and all other kernel-user rings/etc. are internally based\non so called regions. All of them support both user allocated\nmemory and kernel allocations + mmap.\n\nThis series essentially creates provided buffer rings, where\n1. the ring now contains kernel addresses\n2. the ring itself is in-kernel only and not shared with user space\n3. it also allocates kernel buffers (as a region), populates the ring\n    with them, and allows mapping the buffers into the user space.\n\nFuse is doing both adding (kernel) buffers to the ring and consuming\nthem. At which point it's not clear:\n\n1. Why it even needs io_uring provided buffer rings, it can be all\n    contained in fuse. Maybe it's trying to reuse pbuf ring code as\n    basically an internal memory allocator, but then why expose buffer\n    rings as an io_uring uapi instead of keeping it internally.\n\n    That's also why I mentioned whether those buffers are supposed to\n    be used with other types of io_uring requests like recv, etc.\n\n2. Why making io_uring to allocate payload memory. The answer to which\n    is probably to reuse the region api with mmap and so on. And why\n    payload buffers are inseparably created together with the ring\n    and via a new io_uring uapi.\n\n    And yes, I believe in the current form it's inflexible, it requires\n    a new io_uring uapi. It requires the number of buffers to match\n    the number of ring entries, which are related but not the same\n    thing. You can't easily add more memory as it's bound to the ring\n    object. The buffer memory won't even have same lifetime as the\n    ring object -- allow using that km buffer ring with recv requests\n    and highly likely I'll most likely give you a way to crash the\n    kernel.\n\nBut hey, I'm tired. I don't have any beef here and am only trying\nto make it a bit cleaner and flexible for fuse in the first place\nwithout even questioning the I/O path. If everyone believes\neverything is right, just ask Jens to merge it.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed concern that the current implementation of kernel-managed buffer rings in io_uring uses regions for huge payload buffers, which was not the original intention.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Think of it as an area of memory for kernel-user communication. Used\nfor syscall parameters passing to avoid copy_from_user, but I added\nit for a bunch of use cases. We'll hopefully get support at some\npoint for passing request arguments like struct iovec. BPF patches\nuse it for communication. I need to respin patches placing SQ/CQ onto\nit (avoid some memory waste).\n\nTbh, I never meant it nor io_uring regions to be used for huge\npayload buffers, but this series already uses regions for that.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed confusion about the patch's implementation, pointing out that io_uring doesn't currently return buffers into the ring, which is necessary for kernel-managed buffer rings to work.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "confusion",
                "implementation issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Then I'm confused. Take a look at the other reply, this series is\nabout buffer rings with kernel memory, it can't work without a kernel\ncomponent returning buffers into the ring, and io_uring doesn't do\nthat. But maybe you're thinking about adding some more elaborate API.\n\nIIUC, Joanne also wants to add support for fuse installing registered\nbuffers, which would allow zero-copy, but those got split out of\nthis series.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that the patch's current implementation of setting the selected buffer index in __io_uring_cmd_done() has a potential issue, and suggested working around it by wrapping the code into a loop.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential issue",
                "suggested workaround"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Let's fix that then. For now, just work it around by wrapping\ninto a loop.\n\nBtw, I thought you're going to use it for metadata like some\nfuse headers and payloads would be zero copied by installing\nit as registered buffers.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the patch should disentangle memory allocation from ring creation in the io_uring uapi and move ring population into fuse, instead of doing it at creation, to prevent user space from accessing the ring.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "disagreement with current implementation",
                "request for change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The main point is disentangling memory allocation from ring\ncreation in the io_uring uapi, and moving ring population\ninto fuse instead of doing it at creation. And it'll still be\npopulated by the kernel (fuse), user space doesn't have access\nto the ring. IORING_REGISTER_MEM_REGION is just the easiest way\nto achieve that without any extra uapi.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the differences between the two buffer allocation paths are minimal and suggested that they could be handled by a single opcode, but did not strongly object to making them separate opcodes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no strong opinion",
                "open to alternative"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It appeared to me that they're different because of special\nregion path and embedded buffer allocations, and otherwise\ndifferences would be minimal. But if you think it's still\nbetter to be made as a separate opcode, I'm not opposing it,\ngo for it.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that without patches using kernel-managed buffer rings, it's inconvenient to test the functionality, and suggested a control path io-uring command (FUSE_CMD_BIND_BUFFER_RING) for fuse to bind a buffer ring, passing necessary parameters through a struct. He also questioned the need to expose the ring part as an io_uring uapi.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "inconvenience",
                "suggested change"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Not having patches using the functionality is inconvenient. How\nfuse looks up the buffer ring from io_uring? I could imagine you\nhave some control path io-uring command:\n\ncase FUSE_CMD_BIND_BUFFER_RING:\n\treturn bind_queue(params);\n\nThen you can pass all necessary parameters to it, pseudo code:\n\nstruct fuse_bind_kmbuf_ring_params {\n\tregion_id;\n\tbuf_ring_id;\n\t...\n};\n\nbind_queue(cmd, struct fuse_bind_kmbuf_ring_params *p)\n{\n\tregion = io_uring_get_region(cmd, p->region_id);\n\t// get exclusive access:\n\tbuf_ring = io_uring_get_buf_ring(cmd, p->buf_ring_id);\n\n\tif (!validate_buf_ring(buf_ring))\n\t\treturn NOTSUPPORTED;\n\n\tio_uring_pin(buf_ring);\n\tfuse_populate_buf_ring(buf_ring, region, ...);\n}\n\nDoes that match expectations? I don't think you even need\nthe ring part exposed as an io_uring uapi, tbh, as it\nstays completely in fuse and doesn't meaningfully interact\nwith the rest of io_uring.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer questioned the necessity of setting the selected buffer index in __io_uring_cmd_done(), suggesting that IORING_REGISTER_MEM_REGION could be used instead",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That was about an argument for using IORING_REGISTER_MEM_REGION\ninstead a separate region. And it's separate from whether\nbuffers should be bound to the ring.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that when allocating huge pages for non-pow2 buffer sizes, the kernel may allocate a larger huge page than needed, potentially wasting memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "potential memory waste"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I shouldn't affect you much since you have such large buffers,\nbut imagine the total allocation size is not being pow2, and\nthe kernel allocating it as a single folio. E.g. 3 buffers,\n0.5 MB each, total = 1.5MB, and the kernel allocates a 2MB\nhuge page.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer argued that the patch series does not address registered buffers and suggested separating buffer allocation for io_uring",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There is nothing about registered buffers in this series. And even\nif you try to reuse buffer allocation out of it, it'll come with\na circular buffer you'll have no need for. And I'm pretty much\narguing about separating those for io_uring.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested reusing regions for allocations and mmap() operations, wrapping them in a registered buffer to make vmap'ing optional.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, the easiest solution is to internally reuse regions for\nallocations and mmap()'ing and wrap it into a registered buffer.\nIt just need to make vmap'ing optional as it won't be needed.\n\n-- \nPavel Begunkov",
              "reply_to": "",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that the io_uring uapi should not impose fuse-specific requirements on buffer size, ring size, and allocation, as this limits its use cases and flexibility. He questioned why it requires uniform buffers, matching ring sizes, and io_uring-allocated buffers, and suggested allowing for more dynamic memory management.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "strong opinion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, it's mainly about not keeping payload buffers and rings in the same\nobject from the io_uring uapi perspective.\n\n1. If it's an io_uring uapi, it shouldn't be fuse specific or with\na bunch of use case specific expectations attached. Why does it\nrequire all buffers to be uniform in size? Why does it require\nthe ring size to match the number of buffers? Why does it require\nbuffers to be allocated by io_uring in the first place? Maybe some\nsubsystem got memory from somewhere else and wants to do use it\nwith io_uring. Why does it need to know the total size at creation,\nand what would you do if you want to add more memory at runtime\nwhile using the same ring?\n\n2. If it's meant to be fuse specific and _not_ used with other requests\nlike recv/read/etc., then what's the point of having it as an io_uring\nuapi? Which also adds additional trouble like the once you're solving\nwith pinning.\n\nIf it's supposed to be used with other requests, then buffers and\nrings will have different in-kernel lifetime expectations imposed\nby io_uring, so having them together won't even help with\nmanagement.\n\nI have a strong opinion about the memmap.c change. For the\nrest, if you believe it's fine, just send it out and let Jens\ndecide.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer questioned the separation of buffers from rings, expressing uncertainty about the differences between in-kernel buffers with kernel addresses and user-visible buffers with user addresses",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "lack of clear expectations"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It's predicated on separating buffers from rings, see above,\nand assuming that I'm not sure what expectations are different\napart from one being in-kernel with kernel addresses and the\nother user visible with user addresses.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern about wasted space in io_ring buffers by explaining that a circular buffer will allow shared usage and reduce memory allocation, confirming the usefulness for Christoph's use case.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged usefulness",
                "confirmed applicability"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think the circular buffer will be useful for Christoph's use case in\nthe same way it'll be useful for fuse's. The read payload could be\ndifferently sized across requests, so it's a lot of wasted space to\nhave to allocate a buffer large enough to support the max-size request\nper entry in the io_ring. With using a circular buffer, buffers have a\nway to be shared across entries, which means we can significantly\nreduce how much memory needs to be allocated.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that Christoph Hellwig's use case for read-only buffers aligns with the benefits of kernel-managed buffer rings, but did not explicitly confirm whether a fix is planned to address any specific technical issue.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged same use case",
                "aligned with primary reason"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "(resending because I hit reply instead of reply-all)\n\nI think we have the exact same use case, except your buffers need to\nbe read-only. I think your use case benefits from the same memory wins\nwe'll get with incremental buffer consumption, which is the primary\nreason fuse is using a bufring instead of fixed buffers.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Christoph's concern about how to handle read-only mappings for kernel-managed buffer rings, suggesting a solution involving a read-only flag and explaining the necessary changes to existing APIs.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying explanation",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think you can and it'll be very easy to do so. All that would be\nneeded is to pass in a read-only flag from the userspace side when it\nregisters the bufring, and then when userspace makes the mmap call to\nthe bufring, the kernel checks if that read-only flag is set on the\nbufring and if so returns a read-only mapping. I'm happy to add that\npatch to this series if that would make things easier for you. The\nio_uring_register_buffers() api registers fixed buffers (which have to\nbe user-allocated memory) so you would need to go through the\nio_uring_register_buf_ring() api once kmbufs are squashed into the\npbuf interface.\n\nWith going through IORING_MEM_REGION, this would work for your use\ncase as well. The user would have to register the mem region with\nio_uring_register_region() and pass in a read-only flag, and then the\nkernel will allocate the memory region. Then userspace would mmap the\nmemory region and on the kernel side, it would set the mapping to be\nread-only. When the kmbufring then gets registered, the buffers in it\nwill be empty. The filesystem will then have to populate the buffers\nin it from the mem region that was previously registered.\n\nThanks,\nJoanne",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Bernd Schubert",
              "summary": "Reviewer questioned the purpose of sharing buffers across io_uring entries, suggesting it would only reduce the ring size and not provide any benefits.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested clarification",
                "expressed skepticism"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Dunno, what we actually want is requests of multiple sizes. Sharing\nbuffers across entries sounds like just reducing the ring size - I\npersonally don't see the point here.\n\n\nThanks,\nBernd",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarified that kernel-managed buffer rings allow concurrent use of different buffer regions across multiple io_uring entries, addressing a concern about sharing buffers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"sharing buffers across entries\" what I mean is different regions\nof the buffer can now be used concurrently by multiple entries.\n\nThanks,\nJoanne",
              "reply_to": "Bernd Schubert",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed Pavel Begunkov's concern about why kernel-managed buffer rings are necessary, explaining that the server needs to control when buffers get recycled back into the ring for fuse's use case.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "explained reasoning",
                "addressed concern"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The most important part and the whole reason fuse needs the buffer\nring to be kernel-managed is because the kernel needs to control when\nbuffers get recycled back into the ring. For fuse's use case, the\nbuffer is used for passing data between the kernel and the server. We\ncan't have the server recycle the buffer because the server writes\nback data to the kernel in that buffer when it submits the sqe. After\nfuse receives the sqe and reads the reply from the server, it then\nneeds to recycle that buffer back into the ring so it can be reused\nfor a future cqe (eg sending a future request).",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that the selected buffer index needs to be set in __io_uring_cmd_done() for userspace/server-side io-uring operations, and agreed to add this functionality.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged",
                "agreed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On the userspace/server side, it uses the buffers for other io-uring\noperations (eg reading or writing the contents from/to a\nlocally-backed file).",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to feedback about adding complexity by introducing registered memory regions, stating that most use cases of kernel-managed buffers do not benefit from optimizations like PMD, and expressing a preference for simplicity.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "lack of understanding of additional optimizations",
                "preference for simplicity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "My main motivation for this is simplicity. I see (and thanks for\nexplaining) that using a registered mem region allows the use of some\noptimizations (the only one I know of right now is the PMD one you\nmentioned but maybe there's more I'm missing) that could be useful for\nsome workloads, but I don't think (and this could just be my lack of\nunderstanding of what more optimizations there are) most use cases of\nkmbufs benefit from those optimizations, so to me it feels like we're\nadding non-trivial complexity for no noticeable benefit.\n\nI feel like we get the best of both worlds by letting users have both:\nthe simple kernel-managed pbuf where the kernel allocates the buffers\nand the buffers are tied to the lifecycle of the ring, and the more\nadvanced kernel-managed pbuf where buffers are tied to a registered\nmemory region that the subsystem is responsible for later populating\nthe ring with.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed Pavel Begunkov's concern that combining kernel-managed buffer rings (kmbufs) and user-provided buffer rings (pbufs) in a single API would be confusing, as they have different expectations and behaviors. The author initially thought it was cleaner to separate them into distinct APIs but is willing to revisit this decision for v2, suggesting kmbufs will go through the pbuf uapi.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a design trade-off",
                "willingness to reconsider"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "imo it felt cleaner to have a new uapi for it because kmbufs and pbufs\nhave different expectations and behaviors (eg pbufs only work with\nuser-provided buffers and requires userspace to populate the ring\nbefore using it, whereas for kmbufs the kernel allocates the buffers\nand populates it for you; pbufs require userspace to recycle back the\nbuffer, whereas for kmbufs the kernel is the one in control of\nrecycling) and from the user pov it seemed confusing to have kmbufs as\npart of the pbuf ring uapi, instead of separating it out as a\ndifferent type of ringbuffer with a different expectation and\nbehavior. I was trying to make the point that combining the interface\nif we go with IORING_MEM_REGION gets even more confusing because now\npbufs that are kernel-managed are also empty at initialization and\nonly can point to areas inside a registered mem region and the\nresponsibility of populating it is now on whatever subsystem is using\nit.\n\nI still have this opinion but I also think in general, you likely know\nbetter than I do what kind of io-uring uapi is best for io-uring's\nusers. For v2 I'll have kmbufs go through the pbuf uapi.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to feedback that the patch does not handle the case where a ring entry has no buffer associated with it, explaining that this can be fixed by passing in the number of buffers from the uapi for kernel-managed pbuf rings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm not really seeing what the purpose of having a ring entry with no\nbuffer associated with it is. In the existing code for non-kernel\nmanaged pbuf rings, there's the same tie between reg->ring_entries\nbeing used as the marker for how many buffers the ring supports. But\nif the number of buffers should be different than the number of ring\nentries, this can be easily fixed by passing in the number of buffers\nfrom the uapi for kernel-managed pbuf rings.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that there are limitations in kernel-managed buffer rings, specifically regarding dynamic memory allocation and upfront memory planning, but did not commit to revising the patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged limitations",
                "did not commit to revision"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "To play devil's advocate, we also can't easily add more memory to the\nmem region once it's been registered. I think there's also a worse\npenalty where the user needs to know upfront how much memory to\nallocate for the mem region for the lifetime of the ring, which imo\nmay be hard to do (eg if a kernel-managed buf ring only needs to be\nregistered for some code paths and not others, the mem region\nregistration would still have to allocate the memory a potential kbuf\nring would use).",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel's concern that the selected buffer index in __io_uring_cmd_done() may not be set correctly by explaining that the buffer memory has the same lifetime as the ring object and is freed when the ring is freed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm a bit confused by this part. The buffer memory does have the same\nlifetime as the ring object, no? The buffers only get freed when the\nring itself is freed.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author is addressing feedback about whether kernel-managed buffer rings should be restricted to a registered memory region, and is open to adding support for both simple and registered memory regions if the reviewer prefers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "open to compromise",
                "willing to make changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I appreciate you looking at this and giving your feedback and insight.\nThank you for doing so. I don't want to merge in something you're\nunhappy with.\n\nAre you open to having support for both a simple kernel-managed pbuf\ninterface and later on if/when the need arises, a kernel-managed pbuf\ninterface that goes through a registered memory region? If the answer\nis no, then I'll make the change to have kmbufs go through the\nregistered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer pointed out that buffer rings are not suitable for storage read/write operations because they bind to a buffer immediately, whereas other types of requests like recv allow io_uring to first poll the socket and then take a buffer from the ring. They also noted that someone needs to return buffers back into the kernel private ring, which is currently assumed to be handled by the fuse driver but poses a problem for normal rw requests.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "problematic design"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Provided buffer rings are not useful for storage read/write requests\nbecause they bind to a buffer right away, that's in contrast to some\nrecv request, where io_uring will first poll the socket to confirm\nthe data is there, and only then take a buffer from the buffer ring\nand copy into it. With storage rw it makes more sense to specify\nthe buffer directly gain control over where exactly data lands\nIOW, instead of the usual \"read data into a given pointer\" request\nsemantics like what read(2) gives you, buffer rings are rather\n\"read data somewhere and return a pointer to where you placed it\".\n\nAnother problem is that someone needs to return buffers back into\nthe buffer ring, and it's a kernel private ring. For this patchset\nit's assumed the fuse driver is going to be doing that, but there\nis no one for normal rw requests.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested using IORING_MEM_REGION or a standalone registered buffer extension to provide buffers/memory without extra semantics, potentially yielding a finer API.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested improvement",
                "potential for better design"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes. You only need buffers, and it'll be better to base on sth that\ngives you buffers/memory without extra semantics, i.e.\nIORING_MEM_REGION. Or it can be a standalone registered buffer\nextension, likely reusing regions internally. That might even yield\na finer API.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer asked whether kernel-managed buffer rings can be used with other requests, specifically IORING_OP_RECV with IOSQE_BUFFER_SELECT set and bgid specifying the kernel-managed buffer ring",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification_request"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Oops, typo. I was asking whether the buffer rings (not buffers) are\nsupposed to be used with other requests. E.g. submitting a\nIORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying\nyour kernel-managed buffer ring.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that the patch introduces two separate concerns: making buffers inseparable from buffer rings in the io_uring user API and optionally allowing user memory for buffer creation. They suggested implementing this by passing an argument while creating a region, reusing the existing struct io_uring_region_desc.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no objection",
                "trivial to implement"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There are two separate arguments. The first is about not making buffers\ninseparable from buffer rings in the io_uring user API. Whether it's\nIORING_REGISTER_MEM_REGION or something else is not that important.\nI have no objection if it's a part of fuse instead though, e.g. if\nfuse binds two objects together when you register it with fuse, or even\nif fuse create a buffer ring internally (assuming it doesn't indirectly\nleak into io_uring uapi).\n\nAnd the second was about optionally allowing user memory for buffer\ncreation as you're reusing the region abstraction. You can find pros\nand cons for both modes, and funnily enough, SQ/CQ were first kernel\nallocated and then people asked for backing it by user memory, and IIRC\nit was in the reverse order for pbuf rings.\n\nImplementing this is trivial as well, you just need to pass an argument\nwhile creating a region. All new region users use struct\nio_uring_region_desc for uapi and forward it to io_create_region()\nwithout caring if it's user or kernel allocated memory.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer questioned the necessity of exposing buffer ring management as an io_uring API, suggesting it could be implemented within fuse or as an implementation detail",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning the design",
                "suggesting alternative approaches"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The stress is on why it's an _io_uring_ API. It doesn't matter to me\nwhether it's a separate opcode or not. Currently, buffer rings don't give\nyou anything that can't be pure fuse, and it might be simpler to have\nit implemented in fuse than binding to some io_uring object. Or it could\ncreate buffer rings internally to reuse code but it doesn't become an\nio_uring uapi but rather implementation detail. And that predicates on\nwhether km rings are intended to be used with other / non-fuse requests.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed concerns that the io_uring uapi should be reusable for all users, not just fuse, and suggested allowing registration of kernel-managed buffer rings together with memory as a pure region without buffer notion to make it more flexible.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I believe the source of disagreement is that you're thinking\nabout how it's going to look like for fuse specifically, and I\nbelieve you that it'll be nicer for the fuse use case. However,\non the other hand it's an io_uring uapi, and if it is an io_uring\nuapi, we need reusable blocks that are not specific to particular\nusers.\n\nIf it km rings has to stay an io_uring uapi, I guess a middle\nground would be to allow registering km rings together with memory,\nbut make it a pure region without a notion of a buffer, and let\nfuse to chunk it. Later, we can make payload memory allocation\noptional.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer noted that the patch introduces a non-generic io_uring interface by assuming fuse-specific behavior, and requested that the implementation be revised to align with generic io_uring semantics",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right, intentionally so, because otherwise it's a fuse uapi that\npretends to be a generic io_uring uapi but it's not because of\nall assumptions in different places.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that the current implementation of __io_uring_cmd_done() only sets the buffer ring depth but does not account for the memory allocated by the user space, which could lead to issues such as running out of buffers or inefficient use of resources. The reviewer suggested considering a more dynamic approach to buffer allocation and potentially splitting buffers to improve efficiency.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "potential performance issue"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Not really, it tells the buffer ring depth but says nothing about\nhow much memory user space allocated and how it's pushed. It's a\nreasonable default but they could be different. For example, if you\nexpect adding more memory at runtime, you might create the buffer\nring a bit larger. Or when server processing takes a while and you\ncan't recycle until it finishes, you might have more buffers than\nyou need ring entries. Or you might might decide to split buffers\nand as you mentioned incremental consumption, which is an entire\nseparate topic because it doesn't do de-fragmentation and you'd\nneed to have it in fuse, just like user space does with pbufs.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer suggested rethinking io_uring uapi by having it allocate a large block of memory and letting fuse manage buffer chunking, instead of passing buffer count to io_uring",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested improvement",
                "no clear objection"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "My entire point is that we're making lots of assumptions for io_uring\nuapi, and if it's moved to fuse because it knows better what it\nneeds, it should be a win.\n\nIOW, it sounds better if instead of passing the number of buffers to\nio_uring, you just ask it to create a large chunk of memory, and then\nfuse chunks it up and puts into the ring.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov agreed with the patch but noted that adding memory would require a new mechanism, not necessarily tied to IORING_REGISTER_MEM_REGION.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "agreement",
                "request for additional consideration"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I agree, and you'd need something new in either case to add more\nmemory, and it doesn't need to be IORING_REGISTER_MEM_REGION\nspecifically.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that unregistering a kernel-managed buffer ring does not guarantee that there are no in-flight requests using buffers from the ring, and requested synchronization with all other io_uring requests to handle this scenario.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "synchronization",
                "inflight requests"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Unregistering a buffer ring doesn't guarantee that there are no\ninflight requests that are still using buffers that came out of\nthe buffer ring. The fuse driver can wait/terminate its requests\nbefore unregisteration, but allow userspace issued IORING_OP_RECV\nto use this km buffer ring, and you'll need to somehow synchronise\nwith all other io_uring requests.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that a fix is needed for the selected buffer index in __io_uring_cmd_done() and promised to modify it in version 2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "promised to modify"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sorry, I submitted v2 last night thinking the conversation on this\nthread had died. After reading through your reply, I'll modify v2.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarified that kernel-managed buffer rings are intended for use with other io-uring requests, specifically to avoid per-i/o page pinning overhead costs.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes the buffer rings are intended to be used with other io-uring\nrequests. The ideal scenario is that the user can then do the\nequivalent of IORING_OP_READ/WRITE_FIXED operations on the\nkernel-managed buffers and avoid the per-i/o page pinning overhead\ncosts.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong addressed Pavel Begunkov's feedback on the design of kernel-managed buffer rings, agreeing that having buffers owned by the ring and tied to its lifetime is more generically useful. She proposed a new design where io-uring normal requests use backing pages from a memory region, with the registered memory region guaranteed not to be unregistered by the user.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "agreed",
                "proposed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree 100%. The api we add should be what's best for io-uring, not fuse.\n\nFor the majority of use cases, it seemed to me that having the buffers\nseparated from the buffer rings didn't yield perceptible benefits but\nadded complexity and more restrictions like having to statically know\nup front how big the mem region needs to be across the lifetime of the\nio-uring for anything the io-uring might use the mem region for. It\nseems more generically useful as a concept to have the buffers owned\nby the ring and tied to the lifetime of the ring. I like how with this\ndesign everything is self-contained and multiple subsystems can use it\nwithout having to reimplement functionality locally in the subsystem.\nOn the other hand, I see your point about how it might be something\nusers want in the future if they want complete control over which\nparts of the mem region get used as the backing buffers to do stuff\nlike PMD optimizations.\n\nI think this is a matter of opinion/preference and I think in general\nfor anything io-uring related, yours should take precedence.\n\nWith it going through a mem region, I don't think it should even go\nthrough the \"pbuf ring\" interface then if it's not going to specify\nthe number of entries and buffer sizes upfront, if support is added\nfor io-uring normal requests (eg IORING_OP_READ/WRITE) to use the\nbacking pages from a memory region and if we're able to guarantee that\nthe registered memory region will never be able to be unregistered by\nthe user. I think if we repurpose the\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n};\n\nfields in the struct io_uring_sqe to\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n  __u64 offset; /* offset into registered mem region */\n};\n\nand add some IOSQE_ flag to indicate it should find the pages from the\nregistered mem region, then that should work for normal requests.\nWhere on the kernel side, it looks up the associated pages stored in\nthe io_mapped_region's pages array for the offset passed in.\n\nRight now there's only a uapi to register a memory region and none to\nunregister one. Is it guaranteed that io-uring will never add\nsomething in the future that will let userspace unregister the memory\nregion or at least unregister it while it's being used (eg if we add\nfuture refcounting to it to track active uses of it)?\n\nIf so, then end-to-end, with it going through the mem region, it would\nbe something like:\n* user creates a mem region for the io-uring\n* user mmaps the mem region\n* user passes in offset into region, length of each buffer, and number\nof entries in the ring to the subsystem\n* subsystem creates a locally managed bufring and adds buffers to that\nring from the mem region\n* on the cqe side, it sends the buffer id of the registered mem region\nthrough the same \"IORING_CQE_F_BUFFER |  (buf_id <<\nIORING_CQE_BUFFER_SHIFT)\" mechanism\n\nDoes this design match what you had in mind / prefer?\n\nI think the above works for Christoph's use case too (as his and my\nuse case are the same) but if not, please let me know.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov questioned whether a server or user space program can issue READ_OP_RECV or similar requests that consume buffers/entries from kernel-managed buffer rings without fuse kernel code involvement, and asked for clarification on the expected use case.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested clarification",
                "questioned implementation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You mention OP_READ_FIXED and below agreed not exposing km rings\nan io_uring uapi, which makes me believe we're still talking about\ndifferent things.\n\nCorrect me if I'm wrong. Currently, only fuse cmds use the buffer\nring itself, I'm not talking about buffer, i.e. fuse cmds consume\nentries from the ring (!!! that's the part I'm interested in), then\nprocess them and tell the server \"this offset in the region has user\ndata to process or should be populated with data\".\n\nNaturally, the server should be able to use the buffers to issue\nsome I/O and process it in other ways, whether it's a normal\nOP_READ to which you pass the user space address (you can since\nit's mmap()'ed by the server) or something else is important but\na separate question than the one I'm trying to understand.\n\nSo I'm asking whether you expect that a server or other user space\nprogram should be able to issue a READ_OP_RECV, READ_OP_READ or any\nother similar request, which would consume buffers/entries from the\nkm ring without any fuse kernel code involved? Do you have some\nuse case for that in mind?\n\nUnderstanding that is the key in deciding whether km rings should\nbe exposed as io_uring uapi or not, regardless of where buffers\nto populate the ring come from.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested reusing registered buffers instead of introducing a new mechanism for kernel-managed buffer rings, citing efficiency and similarity to zero-copy internally registered buffers as benefits.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "alternative solution"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "So you already can do all that using the mmap()'ed region user\npointer, and you just want it to be more efficient, right?\nFor that let's just reuse registered buffers, we don't need a\nnew mechanism that needs to be propagated to all request types.\nAnd registered buffer are already optimised for I/O in a bunch\nof ways. And as a bonus, it'll be similar to the zero-copy\ninternally registered buffers if you still plan to add them.\n\nThe simplest way to do that is to create a registered buffer out\nof the mmap'ed region pointer. Pseudo code:\n\n// mmap'ed if it's kernel allocated.\n{region_ptr, region_size} = create_region();\n\nstruct iovec iov;\niov.iov_base = region_ptr;\niov.iov_len = region_size;\nio_uring_register_buffers(ring, &iov, 1);\n\n// later instead of this:\nptr = region_ptr + off;\nio_uring_prep_read(sqe, fd, ptr, ...);\n\n// you use registered buffers as usual:\nio_uring_prep_read_fixed(sqe, fd, off, regbuf_idx, ...);\n\n\nIIRC the registration would fail because it doesn't allow file\nbacked pages, but it should be fine if we know it's io_uring\nregion memory, so that would need to be patched.\n\nThere might be a bunch of other ways you can do that like\ncreate a kernel allocated registered buffer like what Cristoph\nwants, and then register it as a region. Or allow creating\nregistered buffers out of a region. etc.\n\nI wanted to unify registered buffers and regions internally\nat some point, but then drifted away from active io_uring core\ninfrastructure development, so I guess that could've been useful.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that registered buffers would hold page references or require pinning the region if used instead of kernel-managed buffer rings",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear opinion on patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Let's talk about it when it's needed or something changes, but if\nyou do registered buffers instead as per above, they'll be holding\npage references and or have to pin the region in some other way.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested adding a liburing helper to handle mmap'ing for the fuse server, eliminating the need for it to directly manage memory",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested improvement"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, we should just add a liburing helper, so that fuse server\ndoesn't need to deal with mmap'ing.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer expressed conditional approval, requesting confirmation that the patch allows for desired fast path optimizations",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "conditional approval",
                "request for confirmation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That's sounds clean to me _if_ it allows you to achieve all\n(fast path) optimisations you want to have. I hope it does?\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed Pavel Begunkov's question about whether the concept of kernel-managed buffer rings is exclusive to fuse servers, and responded that while it may be useful for fuse servers, other subsystems or users could also benefit from this feature.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a point",
                "provided additional context"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for clarifying your question. Yes, this would be a useful\noptimization in the future for fuse servers with certain workload\ncharacteristics (eg network-backed servers with high concurrency and\nunpredictable latencies). I don't think the concept of kmbufrings is\nexclusively fuse-specific though (for example, Christoph's use case\nbeing a recent instance); I think other subsystems/users that'll use\nkmbuf rings would also generically find it useful to have the option\nof READ_OP_RECV/READ_OP_READ operating directly on the ring.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author expressed concerns about added complexity in the design, specifically the introduction of kernel-managed buffer rings and the need for userspace to handle fixed buffers. She questioned the benefits of this approach and suggested that native support for memory regions would be more straightforward.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "expressed concerns"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I feel like this design makes the interface more convoluted and now\nmuddies different concepts together by adding new complexity /\nrelationships between them whereas they were otherwise cleanly\nisolated. Maybe I'm just not seeing/understanding the overarching\nvision for why conceptually it makes sense for them to be tied\ntogether besides as a mechanism to tell io-uring requests where to\ncopy from by reusing what exists for fixed buffer ids. There's more\ncomplexity now on the kernel side (eg having to detect if the buffer\npassed in is kernel-allocated to know whether to pin the pages /\ncharge it against the user's RLIMIT_MEMLOCK limit) but I'm not\nunderstanding what we gain from it. I got the sense from your previous\ncomments that memory regions are the de facto way to go and should be\ndecoupled from other structures, so if that's the case, why doesn't it\nmake sense for io-uring to add native support for using memory regions\nfor io-uring requests? I feel like from the userspace side it makes\nthings more confusing with this extra layer of indirection that now\nhas to go through a fixed buffer.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel's concern that the caller cannot guarantee the registered memory region will persist for the lifetime of the ring, and explained that this would introduce extra overhead for every I/O operation. The author suggested either adding pinning to a registered memory region or using existing locking mechanisms.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't think we can guarantee that the caller will register the\nmemory region as a fixed buffer (eg if it doesn't need/want to use the\nbuffer for normal io-uring requests). On the kernel side, the internal\nbuffer entry uses the kaddr of the registered memory region buffer for\nany memcpys. If it's not guaranteed that registered memory regions\npersist for the lifetime of the ring, there'll have to be extra\noverhead for every I/O (eg grab the io-uring lock, checking if the mem\nregion is still registered, grab a refcount to that mem region, unlock\nthe ring, do the memcpy to the kaddr, then grab the io-uring lock\nagain, decrement the refcount, and unlock). Or I guess we could add\npinning to a registered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v2 0/9] io_uring: add kernel-managed buffer rings",
          "message_id": "CAJnrk1arKMUjZp0128B6WwhJHi-sxkAFfHYgjDeC=vHjgihmBg@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1arKMUjZp0128B6WwhJHi-sxkAFfHYgjDeC=vHjgihmBg@mail.gmail.com/",
          "date": "2026-02-18T21:45:46Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about inefficient memory allocation in io_region_allocate_pages() by reworking the function to allocate memory in 2MB chunks, attempting compound allocations for each chunk. The author acknowledged that this change is necessary to use kernel-managed ring buffers and will help with TLB performance. A fix is planned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "will help with TLB performance"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Currently, io_region_allocate_pages() tries a single compound allocation\nfor the entire region, and falls back to alloc_pages_bulk_node() if that\nfails.\n\nWhen allocating a large region, trying to do a single compound\nallocation may be unrealistic while allocating page by page may be\ninefficient and cause worse TLB performance.\n\nRework io_region_allocate_pages() to allocate memory in 2MB chunks,\nattempting a compound allocation for each chunk.\n\nReplace IO_REGION_F_SINGLE_REF with IO_REGION_F_COMPOUND_PAGES to\nreflect that the page array may contain tail pages from multiple\ncompound allocations.\n\nCurrently, alloc_pages_bulk_node() fails when the GFP_KERNEL_ACCOUNT gfp\nflag is set. This makes this commit a necessary change in order to use\nkernel-managed ring buffers (which will allocate regions of large\nsizes), at least until that issue is fixed.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/memmap.c | 87 ++++++++++++++++++++++++++++++++++-------------\n 1 file changed, 64 insertions(+), 23 deletions(-)\n\ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 89f56609e50a..6e91960aa8fc 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -15,6 +15,28 @@\n #include \"rsrc.h\"\n #include \"zcrx.h\"\n \n+static void release_compound_pages(struct page **pages, unsigned long nr_pages)\n+{\n+\tstruct page *page;\n+\tunsigned int nr, i = 0;\n+\n+\twhile (nr_pages) {\n+\t\tpage = pages[i];\n+\n+\t\tif (!page || WARN_ON_ONCE(page != compound_head(page)))\n+\t\t\treturn;\n+\n+\t\tnr = compound_nr(page);\n+\t\tput_page(page);\n+\n+\t\tif (nr >= nr_pages)\n+\t\t\treturn;\n+\n+\t\ti += nr;\n+\t\tnr_pages -= nr;\n+\t}\n+}\n+\n static bool io_mem_alloc_compound(struct page **pages, int nr_pages,\n \t\t\t\t  size_t size, gfp_t gfp)\n {\n@@ -84,22 +106,19 @@ enum {\n \tIO_REGION_F_VMAP\t\t\t= 1,\n \t/* memory is provided by user and pinned by the kernel */\n \tIO_REGION_F_USER_PROVIDED\t\t= 2,\n-\t/* only the first page in the array is ref'ed */\n-\tIO_REGION_F_SINGLE_REF\t\t\t= 4,\n+\t/* memory may contain tail pages from compound allocations */\n+\tIO_REGION_F_COMPOUND_PAGES\t\t= 4,\n };\n \n void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n {\n \tif (mr->pages) {\n-\t\tlong nr_refs = mr->nr_pages;\n-\n-\t\tif (mr->flags & IO_REGION_F_SINGLE_REF)\n-\t\t\tnr_refs = 1;\n-\n \t\tif (mr->flags & IO_REGION_F_USER_PROVIDED)\n-\t\t\tunpin_user_pages(mr->pages, nr_refs);\n+\t\t\tunpin_user_pages(mr->pages, mr->nr_pages);\n+\t\telse if (mr->flags & IO_REGION_F_COMPOUND_PAGES)\n+\t\t\trelease_compound_pages(mr->pages, mr->nr_pages);\n \t\telse\n-\t\t\trelease_pages(mr->pages, nr_refs);\n+\t\t\trelease_pages(mr->pages, mr->nr_pages);\n \n \t\tkvfree(mr->pages);\n \t}\n@@ -154,28 +173,50 @@ static int io_region_allocate_pages(struct io_mapped_region *mr,\n \t\t\t\t    unsigned long mmap_offset)\n {\n \tgfp_t gfp = GFP_KERNEL_ACCOUNT | __GFP_ZERO | __GFP_NOWARN;\n-\tsize_t size = io_region_size(mr);\n \tunsigned long nr_allocated;\n-\tstruct page **pages;\n+\tstruct page **pages, **cur_pages;\n+\tunsigned chunk_size, chunk_nr_pages;\n+\tunsigned int pages_left;\n \n \tpages = kvmalloc_array(mr->nr_pages, sizeof(*pages), gfp);\n \tif (!pages)\n \t\treturn -ENOMEM;\n \n-\tif (io_mem_alloc_compound(pages, mr->nr_pages, size, gfp)) {\n-\t\tmr->flags |= IO_REGION_F_SINGLE_REF;\n-\t\tgoto done;\n-\t}\n+\tchunk_size = SZ_2M;\n+\tchunk_nr_pages = chunk_size >> PAGE_SHIFT;\n+\tpages_left = mr->nr_pages;\n+\tcur_pages = pages;\n+\n+\twhile (pages_left) {\n+\t\tunsigned int nr_pages = min(pages_left,\n+\t\t\t\t\t    chunk_nr_pages);\n+\n+\t\tif (io_mem_alloc_compound(cur_pages, nr_pages,\n+\t\t\t\t\t  nr_pages << PAGE_SHIFT, gfp)) {\n+\t\t\tmr->flags |= IO_REGION_F_COMPOUND_PAGES;\n+\t\t\tcur_pages += nr_pages;\n+\t\t\tpages_left -= nr_pages;\n+\t\t\tcontinue;\n+\t\t}\n \n-\tnr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,\n-\t\t\t\t\t     mr->nr_pages, pages);\n-\tif (nr_allocated != mr->nr_pages) {\n-\t\tif (nr_allocated)\n-\t\t\trelease_pages(pages, nr_allocated);\n-\t\tkvfree(pages);\n-\t\treturn -ENOMEM;\n+\t\tnr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,\n+\t\t\t\t\t\t     nr_pages, cur_pages);\n+\t\tif (nr_allocated != nr_pages) {\n+\t\t\tunsigned int total =\n+\t\t\t\t(cur_pages - pages) + nr_allocated;\n+\n+\t\t\tif (mr->flags & IO_REGION_F_COMPOUND_PAGES)\n+\t\t\t\trelease_compound_pages(pages, total);\n+\t\t\telse\n+\t\t\t\trelease_pages(pages, total);\n+\t\t\tkvfree(pages);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tcur_pages += nr_pages;\n+\t\tpages_left -= nr_pages;\n \t}\n-done:\n+\n \treg->mmap_offset = mmap_offset;\n \tmr->pages = pages;\n \treturn 0;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the kernel-managed buffer ring interface, specifically the handling of memory allocation and virtual mappings for the buffers. The author explained that when the caller sets the IOU_PBUF_RING_KERNEL_MANAGED flag, the kernel allocates the memory for the ring and its buffers, and the application must set the buffer size through reg->buf_size. The buffers are recycled by the kernel. When the caller makes a subsequent mmap call, the virtual mapping returned is a contiguous mapping of the buffers. The author did not promise to restructure the code in response to this feedback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for kernel-managed buffer rings, which allow the kernel to\nallocate and manage the backing buffers for a buffer ring, rather than\nrequiring the application to provide and manage them.\n\nInternally, the IOBL_KERNEL_MANAGED flag marks buffer lists as\nkernel-managed for appropriate handling in the I/O path.\n\nAt the uapi level, kernel-managed buffer rings are created through the\npbuf interface with the IOU_PBUF_RING_KERNEL_MANAGED flag set. The\nio_uring_buf_reg struct is modified to allow taking in a buf_size\ninstead of a ring_addr. To create a kernel-managed buffer ring, the\ncaller must set the IOU_PBUF_RING_MMAP flag as well to indicate that the\nkernel will allocate the memory for the ring. When the caller mmaps the\nring, they will get back a virtual mapping to the buffer memory.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h | 14 +++++-\n io_uring/kbuf.c               | 95 +++++++++++++++++++++++++++++------\n io_uring/kbuf.h               |  6 ++-\n 3 files changed, 97 insertions(+), 18 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex 6750c383a2ab..278b56a87745 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -885,15 +885,27 @@ struct io_uring_buf_ring {\n  *\t\t\tuse of it will consume only as much as it needs. This\n  *\t\t\trequires that both the kernel and application keep\n  *\t\t\ttrack of where the current read/recv index is at.\n+ * IOU_PBUF_RING_KERNEL_MANAGED: If set, kernel allocates the memory for the\n+ *\t\t\tring and its buffers. The application must set the\n+ *\t\t\tbuffer size through reg->buf_size. The buffers are\n+ *\t\t\trecycled by the kernel. IOU_PBUF_RING_MMAP must be set\n+ *\t\t\tas well. When the caller makes a subsequent mmap call,\n+ *\t\t\tthe virtual mapping returned is a contiguous mapping of\n+ *\t\t\tthe buffers. IOU_PBUF_RING_INC is not yet supported.\n  */\n enum io_uring_register_pbuf_ring_flags {\n \tIOU_PBUF_RING_MMAP\t= 1,\n \tIOU_PBUF_RING_INC\t= 2,\n+\tIOU_PBUF_RING_KERNEL_MANAGED = 4,\n };\n \n /* argument for IORING_(UN)REGISTER_PBUF_RING */\n struct io_uring_buf_reg {\n-\t__u64\tring_addr;\n+\tunion {\n+\t\t__u64\tring_addr;\n+\t\t/* used if reg->flags & IOU_PBUF_RING_KERNEL_MANAGED */\n+\t\t__u32   buf_size;\n+\t};\n \t__u32\tring_entries;\n \t__u16\tbgid;\n \t__u16\tflags;\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 67d4fe576473..816200e91b1f 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -427,10 +427,13 @@ static int io_remove_buffers_legacy(struct io_ring_ctx *ctx,\n \n static void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)\n {\n-\tif (bl->flags & IOBL_BUF_RING)\n+\tif (bl->flags & IOBL_BUF_RING) {\n \t\tio_free_region(ctx->user, &bl->region);\n-\telse\n+\t\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\t\tkfree(bl->buf_ring);\n+\t} else {\n \t\tio_remove_buffers_legacy(ctx, bl, -1U);\n+\t}\n \n \tkfree(bl);\n }\n@@ -596,6 +599,51 @@ int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags)\n \treturn IOU_COMPLETE;\n }\n \n+static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_buffer_list *bl,\n+\t\t\t       const struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_uring_region_desc rd;\n+\tstruct io_uring_buf_ring *ring;\n+\tunsigned long ring_size;\n+\tvoid *buf_region;\n+\tunsigned int i;\n+\tint ret;\n+\n+\t/* allocate pages for the ring structure */\n+\tring_size = flex_array_size(ring, bufs, reg->ring_entries);\n+\tring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);\n+\tif (!ring)\n+\t\treturn -ENOMEM;\n+\n+\tmemset(&rd, 0, sizeof(rd));\n+\trd.size = (u64)reg->buf_size * reg->ring_entries;\n+\n+\tret = io_create_region(ctx, &bl->region, &rd, 0);\n+\tif (ret) {\n+\t\tkfree(ring);\n+\t\treturn ret;\n+\t}\n+\n+\t/* initialize ring buf entries to point to the buffers */\n+\tbuf_region = io_region_get_ptr(&bl->region);\n+\tfor (i = 0; i < reg->ring_entries; i++) {\n+\t\tstruct io_uring_buf *buf = &ring->bufs[i];\n+\n+\t\tbuf->addr = (u64)(uintptr_t)buf_region;\n+\t\tbuf->len = reg->buf_size;\n+\t\tbuf->bid = i;\n+\n+\t\tbuf_region += reg->buf_size;\n+\t}\n+\tring->tail = reg->ring_entries;\n+\n+\tbl->buf_ring = ring;\n+\tbl->flags |= IOBL_KERNEL_MANAGED;\n+\n+\treturn 0;\n+}\n+\n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n {\n \tstruct io_uring_buf_reg reg;\n@@ -612,7 +660,8 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t\treturn -EFAULT;\n \tif (!mem_is_zero(reg.resv, sizeof(reg.resv)))\n \t\treturn -EINVAL;\n-\tif (reg.flags & ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC))\n+\tif (reg.flags & ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC |\n+\t\t\t  IOU_PBUF_RING_KERNEL_MANAGED))\n \t\treturn -EINVAL;\n \tif (!is_power_of_2(reg.ring_entries))\n \t\treturn -EINVAL;\n@@ -620,6 +669,15 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \tif (reg.ring_entries >= 65536)\n \t\treturn -EINVAL;\n \n+\tif (reg.flags & IOU_PBUF_RING_KERNEL_MANAGED) {\n+\t\tif (!(reg.flags & IOU_PBUF_RING_MMAP))\n+\t\t\treturn -EINVAL;\n+\t\tif (reg.flags & IOU_PBUF_RING_INC)\n+\t\t\treturn -EINVAL;\n+\t\tif (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))\n+\t\t\treturn -EINVAL;\n+\t}\n+\n \tbl = io_buffer_get_list(ctx, reg.bgid);\n \tif (bl) {\n \t\t/* if mapped buffer ring OR classic exists, don't allow */\n@@ -634,17 +692,26 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \n \tmmap_offset = (unsigned long)reg.bgid << IORING_OFF_PBUF_SHIFT;\n \tring_size = flex_array_size(br, bufs, reg.ring_entries);\n-\n \tmemset(&rd, 0, sizeof(rd));\n-\trd.size = PAGE_ALIGN(ring_size);\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP)) {\n-\t\trd.user_addr = reg.ring_addr;\n-\t\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n+\n+\tif (reg.flags & IOU_PBUF_RING_KERNEL_MANAGED) {\n+\t\tret = io_setup_kmbuf_ring(ctx, bl, &reg);\n+\t\tif (ret) {\n+\t\t\tkfree(bl);\n+\t\t\treturn ret;\n+\t\t}\n+\t} else {\n+\t\trd.size = PAGE_ALIGN(ring_size);\n+\t\tif (!(reg.flags & IOU_PBUF_RING_MMAP)) {\n+\t\t\trd.user_addr = reg.ring_addr;\n+\t\t\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n+\t\t}\n+\t\tret = io_create_region(ctx, &bl->region, &rd, mmap_offset);\n+\t\tif (ret)\n+\t\t\tgoto fail;\n+\t\tbl->buf_ring = io_region_get_ptr(&bl->region);\n \t}\n-\tret = io_create_region(ctx, &bl->region, &rd, mmap_offset);\n-\tif (ret)\n-\t\tgoto fail;\n-\tbr = io_region_get_ptr(&bl->region);\n+\tbr = bl->buf_ring;\n \n #ifdef SHM_COLOUR\n \t/*\n@@ -666,15 +733,13 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \tbl->nr_entries = reg.ring_entries;\n \tbl->mask = reg.ring_entries - 1;\n \tbl->flags |= IOBL_BUF_RING;\n-\tbl->buf_ring = br;\n \tif (reg.flags & IOU_PBUF_RING_INC)\n \t\tbl->flags |= IOBL_INC;\n \tret = io_buffer_add_list(ctx, bl, reg.bgid);\n \tif (!ret)\n \t\treturn 0;\n fail:\n-\tio_free_region(ctx->user, &bl->region);\n-\tkfree(bl);\n+\tio_put_bl(ctx, bl);\n \treturn ret;\n }\n \ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex bf15e26520d3..38dd5fe6716e 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -7,9 +7,11 @@\n \n enum {\n \t/* ring mapped provided buffers */\n-\tIOBL_BUF_RING\t= 1,\n+\tIOBL_BUF_RING\t\t= 1,\n \t/* buffers are consumed incrementally rather than always fully */\n-\tIOBL_INC\t= 2,\n+\tIOBL_INC\t\t= 2,\n+\t/* buffers are kernel managed */\n+\tIOBL_KERNEL_MANAGED\t= 4,\n };\n \n struct io_buffer_list {\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about distinguishing between kernel-managed buffer addresses and negative values when error checking, by modifying the io_br_sel struct to separate address and value fields for kernel-managed buffers. The selected kernel-managed buffer will be auto-committed.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "planned a restructuring"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Allow kernel-managed buffers to be selected. This requires modifying the\nio_br_sel struct to separate the fields for address and val, since a\nkernel address cannot be distinguished from a negative val when error\nchecking.\n\nAuto-commit any selected kernel-managed buffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring_types.h |  8 ++++----\n io_uring/kbuf.c                | 16 ++++++++++++----\n 2 files changed, 16 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 3e4a82a6f817..36cc2e0346d9 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -93,13 +93,13 @@ struct io_mapped_region {\n  */\n struct io_br_sel {\n \tstruct io_buffer_list *buf_list;\n-\t/*\n-\t * Some selection parts return the user address, others return an error.\n-\t */\n \tunion {\n+\t\t/* for classic/ring provided buffers */\n \t\tvoid __user *addr;\n-\t\tssize_t val;\n+\t\t/* for kernel-managed buffers */\n+\t\tvoid *kaddr;\n \t};\n+\tssize_t val;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 816200e91b1f..efcc6540f948 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -155,7 +155,8 @@ static int io_provided_buffers_select(struct io_kiocb *req, size_t *len,\n \treturn 1;\n }\n \n-static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n+static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n+\t\t\t     unsigned int issue_flags)\n {\n \t/*\n \t* If we came in unlocked, we have no choice but to consume the\n@@ -170,7 +171,11 @@ static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n \tif (issue_flags & IO_URING_F_UNLOCKED)\n \t\treturn true;\n \n-\t/* uring_cmd commits kbuf upfront, no need to auto-commit */\n+\t/* kernel-managed buffers are auto-committed */\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\treturn true;\n+\n+\t/* multishot uring_cmd commits kbuf upfront, no need to auto-commit */\n \tif (!io_file_can_poll(req) && req->opcode != IORING_OP_URING_CMD)\n \t\treturn true;\n \treturn false;\n@@ -200,9 +205,12 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n-\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n+\telse\n+\t\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n \n-\tif (io_should_commit(req, issue_flags)) {\n+\tif (io_should_commit(req, bl, issue_flags)) {\n \t\tio_kbuf_commit(req, sel.buf_list, *len, 1);\n \t\tsel.buf_list = NULL;\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about userspace unregistering a buffer ring while it is pinned by the kernel, and provided a mechanism for kernel subsystems to safely access buffer ring contents. The author added APIs to pin and unpin buffer rings, preventing userspace from unregistering a buffer ring while it is pinned. This change is necessary for fuse to pin the buffer ring because fuse may need to select a buffer in atomic contexts.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "added mechanism",
                "necessary for fuse"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add kernel APIs to pin and unpin buffer rings, preventing userspace from\nunregistering a buffer ring while it is pinned by the kernel.\n\nThis provides a mechanism for kernel subsystems to safely access buffer\nring contents while ensuring the buffer ring remains valid. A pinned\nbuffer ring cannot be unregistered until explicitly unpinned. On the\nuserspace side, trying to unregister a pinned buffer will return -EBUSY.\n\nThis is a preparatory change for upcoming fuse usage of kernel-managed\nbuffer rings. It is necessary for fuse to pin the buffer ring because\nfuse may need to select a buffer in atomic contexts, which it can only\ndo so by using the underlying buffer list pointer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 17 +++++++++++\n io_uring/kbuf.c              | 55 ++++++++++++++++++++++++++++++++++++\n io_uring/kbuf.h              |  5 ++++\n 3 files changed, 77 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 375fd048c4cb..bd681d8ab1d4 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -84,6 +84,10 @@ struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n \t\t\t\t struct io_br_sel *sel, unsigned int issue_flags);\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **out_bl);\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t    unsigned issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -126,6 +130,19 @@ static inline bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n {\n \treturn true;\n }\n+static inline int io_uring_buf_ring_pin(struct io_uring_cmd *cmd,\n+\t\t\t\t\tunsigned buf_group,\n+\t\t\t\t\tunsigned issue_flags,\n+\t\t\t\t\tstruct io_buffer_list **bl)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n+static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned buf_group,\n+\t\t\t\t\t  unsigned issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex efcc6540f948..1d86ad7803fd 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -9,6 +9,7 @@\n #include <linux/poll.h>\n #include <linux/vmalloc.h>\n #include <linux/io_uring.h>\n+#include <linux/io_uring/cmd.h>\n \n #include <uapi/linux/io_uring.h>\n \n@@ -237,6 +238,58 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \treturn sel;\n }\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **out_bl)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (!bl || !(bl->flags & IOBL_BUF_RING))\n+\t\tgoto err;\n+\n+\tif (unlikely(bl->flags & IOBL_PINNED)) {\n+\t\tret = -EALREADY;\n+\t\tgoto err;\n+\t}\n+\n+\tbl->flags |= IOBL_PINNED;\n+\tret = 0;\n+\t*out_bl = bl;\n+err:\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_pin);\n+\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t       unsigned issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tunsigned int required_flags;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (!bl)\n+\t\tgoto err;\n+\n+\trequired_flags = IOBL_BUF_RING | IOBL_PINNED;\n+\tif ((bl->flags & required_flags) == required_flags) {\n+\t\tbl->flags &= ~IOBL_PINNED;\n+\t\tret = 0;\n+\t}\n+err:\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_unpin);\n+\n /* cap it at a reasonable 256, will be one page even for 4K */\n #define PEEK_MAX_IMPORT\t\t256\n \n@@ -768,6 +821,8 @@ int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t\treturn -ENOENT;\n \tif (!(bl->flags & IOBL_BUF_RING))\n \t\treturn -EINVAL;\n+\tif (bl->flags & IOBL_PINNED)\n+\t\treturn -EBUSY;\n \n \tscoped_guard(mutex, &ctx->mmap_lock)\n \t\txa_erase(&ctx->io_bl_xa, bl->bgid);\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 38dd5fe6716e..006e8a73a117 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -12,6 +12,11 @@ enum {\n \tIOBL_INC\t\t= 2,\n \t/* buffers are kernel managed */\n \tIOBL_KERNEL_MANAGED\t= 4,\n+\t/*\n+\t * buffer ring is pinned and cannot be unregistered by userspace until\n+\t * it has been unpinned\n+\t */\n+\tIOBL_PINNED\t\t= 8,\n };\n \n struct io_buffer_list {\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about returning the id of the selected buffer in io_buffer_select() for kernel-managed buffer rings, agreeing to modify the function to return the buffer id.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed to modify the function"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Return the id of the selected buffer in io_buffer_select(). This is\nneeded for kernel-managed buffer rings to later recycle the selected\nbuffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h   | 2 +-\n include/linux/io_uring_types.h | 2 ++\n io_uring/kbuf.c                | 7 +++++--\n 3 files changed, 8 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex bd681d8ab1d4..31f47cce99f5 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);\n \n /*\n  * Select a buffer from the provided buffer group for multishot uring_cmd.\n- * Returns the selected buffer address and size.\n+ * Returns the selected buffer address, size, and id.\n  */\n struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n \t\t\t\t\t    unsigned buf_group, size_t *len,\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 36cc2e0346d9..5a56bb341337 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -100,6 +100,8 @@ struct io_br_sel {\n \t\tvoid *kaddr;\n \t};\n \tssize_t val;\n+\t/* id of the selected buffer */\n+\tunsigned buf_id;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 1d86ad7803fd..d20221f1b9b2 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -206,6 +206,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n+\tsel.buf_id = req->buf_index;\n \tif (bl->flags & IOBL_KERNEL_MANAGED)\n \t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n \telse\n@@ -229,10 +230,12 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \n \tbl = io_buffer_get_list(ctx, buf_group);\n \tif (likely(bl)) {\n-\t\tif (bl->flags & IOBL_BUF_RING)\n+\t\tif (bl->flags & IOBL_BUF_RING) {\n \t\t\tsel = io_ring_buffer_select(req, len, bl, issue_flags);\n-\t\telse\n+\t\t} else {\n \t\t\tsel.addr = io_provided_buffer_select(req, len, bl);\n+\t\t\tsel.buf_id = req->buf_index;\n+\t\t}\n \t}\n \tio_ring_submit_unlock(req->ctx, issue_flags);\n \treturn sel;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about buffer recycling by adding an interface for buffers to be recycled back into a kernel-managed buffer ring, which will be implemented in the io_uring/kbuf.c file.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add an interface for buffers to be recycled back into a kernel-managed\nbuffer ring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 11 +++++++++\n io_uring/kbuf.c              | 48 ++++++++++++++++++++++++++++++++++++\n 2 files changed, 59 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 31f47cce99f5..5cebcd6d50e6 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -88,6 +88,10 @@ int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t  unsigned issue_flags, struct io_buffer_list **out_bl);\n int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t    unsigned issue_flags);\n+\n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -143,6 +147,13 @@ static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n+\t\t\t\t\t unsigned int buf_group, u64 addr,\n+\t\t\t\t\t unsigned int len, unsigned int bid,\n+\t\t\t\t\t unsigned int issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex d20221f1b9b2..6e4dd1e003f4 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -102,6 +102,54 @@ void io_kbuf_drop_legacy(struct io_kiocb *req)\n \treq->kbuf = NULL;\n }\n \n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags)\n+{\n+\tstruct io_kiocb *req = cmd_to_io_kiocb(cmd);\n+\tstruct io_ring_ctx *ctx = req->ctx;\n+\tstruct io_uring_buf_ring *br;\n+\tstruct io_uring_buf *buf;\n+\tstruct io_buffer_list *bl;\n+\tunsigned int required_flags;\n+\tint ret = -EINVAL;\n+\n+\tif (WARN_ON_ONCE(req->flags & REQ_F_BUFFERS_COMMIT))\n+\t\treturn ret;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\n+\tif (!bl)\n+\t\tgoto err;\n+\n+\trequired_flags = IOBL_BUF_RING | IOBL_KERNEL_MANAGED;\n+\tif (WARN_ON_ONCE((bl->flags & required_flags) != required_flags))\n+\t\tgoto err;\n+\n+\tbr = bl->buf_ring;\n+\n+\tif (WARN_ON_ONCE((__u16)(br->tail - bl->head) >= bl->nr_entries))\n+\t\tgoto err;\n+\n+\tbuf = &br->bufs[(br->tail) & bl->mask];\n+\n+\tbuf->addr = addr;\n+\tbuf->len = len;\n+\tbuf->bid = bid;\n+\n+\treq->flags &= ~REQ_F_BUFFER_RING;\n+\n+\tbr->tail++;\n+\tret = 0;\n+\n+err:\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_kmbuf_recycle);\n+\n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)\n {\n \tstruct io_ring_ctx *ctx = req->ctx;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring_is_kmbuf_ring() function, which returns true if there is a kernel-managed buffer ring at the specified buffer group. The author explained that this function is preparatory for upcoming fuse kernel-managed buffer support and needs to ensure the buffer ring registered by the server is a kernel-managed buffer ring.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "preparatory"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "io_uring_is_kmbuf_ring() returns true if there is a kernel-managed\nbuffer ring at the specified buffer group.\n\nThis is a preparatory patch for upcoming fuse kernel-managed buffer\nsupport, which needs to ensure the buffer ring registered by the server\nis a kernel-managed buffer ring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h |  9 +++++++++\n io_uring/kbuf.c              | 20 ++++++++++++++++++++\n 2 files changed, 29 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 5cebcd6d50e6..dce6a0ce8538 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -92,6 +92,9 @@ int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t   u64 addr, unsigned int len, unsigned int bid,\n \t\t\t   unsigned int issue_flags);\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -154,6 +157,12 @@ static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned int buf_group,\n+\t\t\t\t\t  unsigned int issue_flags)\n+{\n+\treturn false;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 6e4dd1e003f4..bd10c830cd30 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -917,3 +917,23 @@ struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n \t\treturn NULL;\n \treturn &bl->region;\n }\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tbool is_kmbuf_ring = false;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (likely(bl) && (bl->flags & IOBL_KERNEL_MANAGED)) {\n+\t\tWARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING));\n+\t\tis_kmbuf_ring = true;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn is_kmbuf_ring;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_is_kmbuf_ring);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring mutex being held in atomic contexts when selecting a buffer from a kernel-managed bufring, and agreed to export io_ring_buffer_select() as a preparatory patch for fuse io-uring.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Export io_ring_buffer_select() so that it may be used by callers who\npass in a pinned bufring without needing to grab the io_uring mutex.\n\nThis is a preparatory patch that will be needed by fuse io-uring, which\nwill need to select a buffer from a kernel-managed bufring while the\nuring mutex may already be held by in-progress commits, and may need to\nselect a buffer in atomic contexts.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 14 ++++++++++++++\n io_uring/kbuf.c              |  7 ++++---\n 2 files changed, 18 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex dce6a0ce8538..ac8925fa81f6 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -95,6 +95,10 @@ int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \n bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t    unsigned int issue_flags);\n+\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -163,6 +167,16 @@ static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n {\n \treturn false;\n }\n+static inline struct io_br_sel io_ring_buffer_select(struct io_kiocb *req,\n+\t\t\t\t\t\t     size_t *len,\n+\t\t\t\t\t\t     struct io_buffer_list *bl,\n+\t\t\t\t\t\t     unsigned int issue_flags)\n+{\n+\tstruct io_br_sel sel = {\n+\t\t.val = -EOPNOTSUPP,\n+\t};\n+\treturn sel;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex bd10c830cd30..fcc64e4a6a29 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -230,9 +230,9 @@ static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n \treturn false;\n }\n \n-static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n-\t\t\t\t\t      struct io_buffer_list *bl,\n-\t\t\t\t\t      unsigned int issue_flags)\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags)\n {\n \tstruct io_uring_buf_ring *br = bl->buf_ring;\n \t__u16 tail, head = bl->head;\n@@ -266,6 +266,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \t}\n \treturn sel;\n }\n+EXPORT_SYMBOL_GPL(io_ring_buffer_select);\n \n struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \t\t\t\t  unsigned buf_group, unsigned int issue_flags)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about indicating which buffer was selected in the completion queue entry, and responded by adding a flag (IORING_CQE_F_BUFFER) to indicate this, along with encoding the buffer index if a buffer was selected.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged the need for change",
                "provided a clear solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When uring_cmd operations select a buffer, the completion queue entry\nshould indicate which buffer was selected.\n\nSet IORING_CQE_F_BUFFER on the completed entry and encode the buffer\nindex if a buffer was selected.\n\nThis change is needed in order to relay to userspace which selected\nbuffer contains the data.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/uring_cmd.c | 6 +++++-\n 1 file changed, 5 insertions(+), 1 deletion(-)\n\ndiff --git a/io_uring/uring_cmd.c b/io_uring/uring_cmd.c\nindex ee7b49f47cb5..6d38df1a812d 100644\n--- a/io_uring/uring_cmd.c\n+++ b/io_uring/uring_cmd.c\n@@ -151,6 +151,7 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \t\t       unsigned issue_flags, bool is_cqe32)\n {\n \tstruct io_kiocb *req = cmd_to_io_kiocb(ioucmd);\n+\tu32 cflags = 0;\n \n \tif (WARN_ON_ONCE(req->flags & REQ_F_APOLL_MULTISHOT))\n \t\treturn;\n@@ -160,7 +161,10 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \tif (ret < 0)\n \t\treq_set_fail(req);\n \n-\tio_req_set_res(req, ret, 0);\n+\tif (req->flags & (REQ_F_BUFFER_SELECTED | REQ_F_BUFFER_RING))\n+\t\tcflags |= IORING_CQE_F_BUFFER |\n+\t\t\t(req->buf_index << IORING_CQE_BUFFER_SHIFT);\n+\tio_req_set_res(req, ret, cflags);\n \tif (is_cqe32) {\n \t\tif (req->ctx->flags & IORING_SETUP_CQE_MIXED)\n \t\t\treq->cqe.flags |= IORING_CQE_F_32;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "syzbot ci",
              "summary": "The reviewer reported a general protection fault in io_remove_buffers_legacy, indicating a null pointer dereference in the list_del function. The issue was triggered by syzkaller testing and is likely related to incorrect handling of buffer ring lifecycles.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "general protection fault",
                "null-ptr-deref"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "syzbot ci has tested the following series\n\n[v2] io_uring: add kernel-managed buffer rings\nhttps://lore.kernel.org/all/20260218025207.1425553-1-joannelkoong@gmail.com\n* [PATCH v2 1/9] io_uring/memmap: chunk allocations in io_region_allocate_pages()\n* [PATCH v2 2/9] io_uring/kbuf: add support for kernel-managed buffer rings\n* [PATCH v2 3/9] io_uring/kbuf: support kernel-managed buffer rings in buffer selection\n* [PATCH v2 4/9] io_uring/kbuf: add buffer ring pinning/unpinning\n* [PATCH v2 5/9] io_uring/kbuf: return buffer id in buffer selection\n* [PATCH v2 6/9] io_uring/kbuf: add recycling for kernel managed buffer rings\n* [PATCH v2 7/9] io_uring/kbuf: add io_uring_is_kmbuf_ring()\n* [PATCH v2 8/9] io_uring/kbuf: export io_ring_buffer_select()\n* [PATCH v2 9/9] io_uring/cmd: set selected buffer index in __io_uring_cmd_done()\n\nand found the following issue:\ngeneral protection fault in io_remove_buffers_legacy\n\nFull report is available here:\nhttps://ci.syzbot.org/series/ddeaf464-c69b-4166-b0cf-53c9d51e4820\n\n***\n\ngeneral protection fault in io_remove_buffers_legacy\n\ntree:      torvalds\nURL:       https://kernel.googlesource.com/pub/scm/linux/kernel/git/torvalds/linux\nbase:      2961f841b025fb234860bac26dfb7fa7cb0fb122\narch:      amd64\ncompiler:  Debian clang version 21.1.8 (++20251221033036+2078da43e25a-1~exp1~20251221153213.50), Debian LLD 21.1.8\nconfig:    https://ci.syzbot.org/builds/ab5ad5aa-2757-4d66-a2c5-391a8417535d/config\nC repro:   https://ci.syzbot.org/findings/061747e2-36f1-499b-ac34-38cefffbce63/c_repro\nsyz repro: https://ci.syzbot.org/findings/061747e2-36f1-499b-ac34-38cefffbce63/syz_repro\n\nOops: general protection fault, probably for non-canonical address 0xdffffc0000000001: 0000 [#1] SMP KASAN PTI\nKASAN: null-ptr-deref in range [0x0000000000000008-0x000000000000000f]\nCPU: 1 UID: 0 PID: 5967 Comm: syz.0.17 Not tainted syzkaller #0 PREEMPT(full) \nHardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 1.16.2-debian-1.16.2-1 04/01/2014\nRIP: 0010:__list_del_entry_valid_or_report+0x25/0x190 lib/list_debug.c:49\nCode: 90 90 90 90 90 f3 0f 1e fa 41 57 41 56 41 55 41 54 53 48 89 fb 49 bd 00 00 00 00 00 fc ff df 48 83 c7 08 48 89 f8 48 c1 e8 03 <42> 80 3c 28 00 74 05 e8 df 8c 77 fd 4c 8b 7b 08 48 89 d8 48 c1 e8\nRSP: 0018:ffffc900040a7b68 EFLAGS: 00010202\nRAX: 0000000000000001 RBX: 0000000000000000 RCX: 1ffff11035ee2732\nRDX: 1ffff11035ee2730 RSI: 00000000ffffffff RDI: 0000000000000008\nRBP: dffffc0000000000 R08: ffff8881af7139b7 R09: 0000000000000000\nR10: ffff8881af7139a0 R11: ffffed1035ee2737 R12: ffff8881af713980\nR13: dffffc0000000000 R14: 00000000ffffffff R15: 0000000000000000\nFS:  0000555560587500(0000) GS:ffff8882a9466000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 0000200000001000 CR3: 0000000175cd2000 CR4: 00000000000006f0\nCall Trace:\n <TASK>\n __list_del_entry_valid include/linux/list.h:132 [inline]\n __list_del_entry include/linux/list.h:223 [inline]\n list_del include/linux/list.h:237 [inline]\n io_remove_buffers_legacy+0x139/0x310 io_uring/kbuf.c:533\n io_put_bl+0x62/0x120 io_uring/kbuf.c:548\n io_register_pbuf_ring+0x6c0/0x7d0 io_uring/kbuf.c:855\n __io_uring_register io_uring/register.c:838 [inline]\n __do_sys_io_uring_register io_uring/register.c:1024 [inline]\n __se_sys_io_uring_register+0xc3e/0x19a0 io_uring/register.c:1001\n do_syscall_x64 arch/x86/entry/syscall_64.c:63 [inline]\n do_syscall_64+0x14d/0xf80 arch/x86/entry/syscall_64.c:94\n entry_SYSCALL_64_after_hwframe+0x77/0x7f\nRIP: 0033:0x7f056859bf79\nCode: ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 e8 ff ff ff f7 d8 64 89 01 48\nRSP: 002b:00007fffd8dcfaf8 EFLAGS: 00000246 ORIG_RAX: 00000000000001ab\nRAX: ffffffffffffffda RBX: 00007f0568815fa0 RCX: 00007f056859bf79\nRDX: 0000200000000040 RSI: 0000000000000016 RDI: 0000000000000004\nRBP: 00007f05686327e0 R08: 0000000000000000 R09: 0000000000000000\nR10: 0000000000000001 R11: 0000000000000246 R12: 0000000000000000\nR13: 00007f0568815fac R14: 00007f0568815fa0 R15: 00007f0568815fa0\n </TASK>\nModules linked in:\n---[ end trace 0000000000000000 ]---\nRIP: 0010:__list_del_entry_valid_or_report+0x25/0x190 lib/list_debug.c:49\nCode: 90 90 90 90 90 f3 0f 1e fa 41 57 41 56 41 55 41 54 53 48 89 fb 49 bd 00 00 00 00 00 fc ff df 48 83 c7 08 48 89 f8 48 c1 e8 03 <42> 80 3c 28 00 74 05 e8 df 8c 77 fd 4c 8b 7b 08 48 89 d8 48 c1 e8\nRSP: 0018:ffffc900040a7b68 EFLAGS: 00010202\nRAX: 0000000000000001 RBX: 0000000000000000 RCX: 1ffff11035ee2732\nRDX: 1ffff11035ee2730 RSI: 00000000ffffffff RDI: 0000000000000008\nRBP: dffffc0000000000 R08: ffff8881af7139b7 R09: 0000000000000000\nR10: ffff8881af7139a0 R11: ffffed1035ee2737 R12: ffff8881af713980\nR13: dffffc0000000000 R14: 00000000ffffffff R15: 0000000000000000\nFS:  0000555560587500(0000) GS:ffff8882a9466000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00007f25f1e17095 CR3: 0000000175cd2000 CR4: 00000000000006f0\n----------------\nCode disassembly (best guess):\n   0:\t90                   \tnop\n   1:\t90                   \tnop\n   2:\t90                   \tnop\n   3:\t90                   \tnop\n   4:\t90                   \tnop\n   5:\tf3 0f 1e fa          \tendbr64\n   9:\t41 57                \tpush   %r15\n   b:\t41 56                \tpush   %r14\n   d:\t41 55                \tpush   %r13\n   f:\t41 54                \tpush   %r12\n  11:\t53                   \tpush   %rbx\n  12:\t48 89 fb             \tmov    %rdi,%rbx\n  15:\t49 bd 00 00 00 00 00 \tmovabs $0xdffffc0000000000,%r13\n  1c:\tfc ff df\n  1f:\t48 83 c7 08          \tadd    $0x8,%rdi\n  23:\t48 89 f8             \tmov    %rdi,%rax\n  26:\t48 c1 e8 03          \tshr    $0x3,%rax\n* 2a:\t42 80 3c 28 00       \tcmpb   $0x0,(%rax,%r13,1) <-- trapping instruction\n  2f:\t74 05                \tje     0x36\n  31:\te8 df 8c 77 fd       \tcall   0xfd778d15\n  36:\t4c 8b 7b 08          \tmov    0x8(%rbx),%r15\n  3a:\t48 89 d8             \tmov    %rbx,%rax\n  3d:\t48                   \trex.W\n  3e:\tc1                   \t.byte 0xc1\n  3f:\te8                   \t.byte 0xe8\n\n\n***\n\nIf these findings have caused you to resend the series or submit a\nseparate fix, please add the following tag to your commit message:\n  Tested-by: syzbot@syzkaller.appspotmail.com\n\n---\nThis report is generated by a bot. It may contain errors.\nsyzbot ci engineers can be reached at syzkaller@googlegroups.com.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that the patch needs further revision and promised to update v2 with another version",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for further revision",
                "promised to update"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm going to update v2 with another version, as per the conversation\nin this thread [1].\n\nThanks,\nJoanne\n\n[1] https://lore.kernel.org/linux-fsdevel/20260210002852.1394504-1-joannelkoong@gmail.com/T/#t",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v1 03/11] io_uring/kbuf: add support for kernel-managed buffer rings",
          "message_id": "CAJnrk1Y5iTOhj4_RbnR7RJPkr7fFcCdh1gY=3Hm72M91D-SnyQ@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1Y5iTOhj4_RbnR7RJPkr7fFcCdh1gY=3Hm72M91D-SnyQ@mail.gmail.com/",
          "date": "2026-02-18T21:43:50Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the refactoring of io_register_pbuf_ring() logic into generic helpers, explaining that this is a preparatory change for upcoming kernel-managed buffer ring support which will need to reuse some of these helpers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "neutral explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Refactor the logic in io_register_pbuf_ring() into generic helpers:\n- io_copy_and_validate_buf_reg(): Copy out user arg and validate user\n  arg and buffer registration parameters\n- io_alloc_new_buffer_list(): Allocate and initialize a new buffer\n  list for the given buffer group ID\n- io_setup_pbuf_ring(): Sets up the physical buffer ring region and\n  handles memory mapping for provided buffer rings\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport which will need to reuse some of these helpers.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c | 129 +++++++++++++++++++++++++++++++-----------------\n 1 file changed, 85 insertions(+), 44 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 67d4fe576473..850b836f32ee 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -596,55 +596,73 @@ int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags)\n \treturn IOU_COMPLETE;\n }\n \n-int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+static int io_copy_and_validate_buf_reg(const void __user *arg,\n+\t\t\t\t\tstruct io_uring_buf_reg *reg,\n+\t\t\t\t\tunsigned int permitted_flags)\n {\n-\tstruct io_uring_buf_reg reg;\n-\tstruct io_buffer_list *bl;\n-\tstruct io_uring_region_desc rd;\n-\tstruct io_uring_buf_ring *br;\n-\tunsigned long mmap_offset;\n-\tunsigned long ring_size;\n-\tint ret;\n-\n-\tlockdep_assert_held(&ctx->uring_lock);\n-\n-\tif (copy_from_user(&reg, arg, sizeof(reg)))\n+\tif (copy_from_user(reg, arg, sizeof(*reg)))\n \t\treturn -EFAULT;\n-\tif (!mem_is_zero(reg.resv, sizeof(reg.resv)))\n+\n+\tif (!mem_is_zero(reg->resv, sizeof(reg->resv)))\n \t\treturn -EINVAL;\n-\tif (reg.flags & ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC))\n+\tif (reg->flags & ~permitted_flags)\n \t\treturn -EINVAL;\n-\tif (!is_power_of_2(reg.ring_entries))\n+\tif (!is_power_of_2(reg->ring_entries))\n \t\treturn -EINVAL;\n \t/* cannot disambiguate full vs empty due to head/tail size */\n-\tif (reg.ring_entries >= 65536)\n+\tif (reg->ring_entries >= 65536)\n \t\treturn -EINVAL;\n+\treturn 0;\n+}\n \n-\tbl = io_buffer_get_list(ctx, reg.bgid);\n-\tif (bl) {\n+static struct io_buffer_list *\n+io_alloc_new_buffer_list(struct io_ring_ctx *ctx,\n+\t\t\t const struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_buffer_list *list;\n+\n+\tlist = io_buffer_get_list(ctx, reg->bgid);\n+\tif (list) {\n \t\t/* if mapped buffer ring OR classic exists, don't allow */\n-\t\tif (bl->flags & IOBL_BUF_RING || !list_empty(&bl->buf_list))\n-\t\t\treturn -EEXIST;\n-\t\tio_destroy_bl(ctx, bl);\n+\t\tif (list->flags & IOBL_BUF_RING || !list_empty(&list->buf_list))\n+\t\t\treturn ERR_PTR(-EEXIST);\n+\t\tio_destroy_bl(ctx, list);\n \t}\n \n-\tbl = kzalloc(sizeof(*bl), GFP_KERNEL_ACCOUNT);\n-\tif (!bl)\n-\t\treturn -ENOMEM;\n+\tlist = kzalloc(sizeof(*list), GFP_KERNEL_ACCOUNT);\n+\tif (!list)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tlist->nr_entries = reg->ring_entries;\n+\tlist->mask = reg->ring_entries - 1;\n+\tlist->flags = IOBL_BUF_RING;\n+\n+\treturn list;\n+}\n+\n+static int io_setup_pbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t      const struct io_uring_buf_reg *reg,\n+\t\t\t      struct io_buffer_list *bl)\n+{\n+\tstruct io_uring_region_desc rd;\n+\tunsigned long mmap_offset;\n+\tunsigned long ring_size;\n+\tint ret;\n \n-\tmmap_offset = (unsigned long)reg.bgid << IORING_OFF_PBUF_SHIFT;\n-\tring_size = flex_array_size(br, bufs, reg.ring_entries);\n+\tmmap_offset = (unsigned long)reg->bgid << IORING_OFF_PBUF_SHIFT;\n+\tring_size = flex_array_size(bl->buf_ring, bufs, reg->ring_entries);\n \n \tmemset(&rd, 0, sizeof(rd));\n \trd.size = PAGE_ALIGN(ring_size);\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP)) {\n-\t\trd.user_addr = reg.ring_addr;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP)) {\n+\t\trd.user_addr = reg->ring_addr;\n \t\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n \t}\n+\n \tret = io_create_region(ctx, &bl->region, &rd, mmap_offset);\n \tif (ret)\n-\t\tgoto fail;\n-\tbr = io_region_get_ptr(&bl->region);\n+\t\treturn ret;\n+\tbl->buf_ring = io_region_get_ptr(&bl->region);\n \n #ifdef SHM_COLOUR\n \t/*\n@@ -656,25 +674,48 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t * should use IOU_PBUF_RING_MMAP instead, and liburing will handle\n \t * this transparently.\n \t */\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP) &&\n-\t    ((reg.ring_addr | (unsigned long)br) & (SHM_COLOUR - 1))) {\n-\t\tret = -EINVAL;\n-\t\tgoto fail;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP) &&\n+\t    ((reg->ring_addr | (unsigned long)bl->buf_ring) &\n+\t     (SHM_COLOUR - 1))) {\n+\t\tio_free_region(ctx->user, &bl->region);\n+\t\treturn -EINVAL;\n \t}\n #endif\n \n-\tbl->nr_entries = reg.ring_entries;\n-\tbl->mask = reg.ring_entries - 1;\n-\tbl->flags |= IOBL_BUF_RING;\n-\tbl->buf_ring = br;\n-\tif (reg.flags & IOU_PBUF_RING_INC)\n+\tif (reg->flags & IOU_PBUF_RING_INC)\n \t\tbl->flags |= IOBL_INC;\n+\n+\treturn 0;\n+}\n+\n+int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tunsigned int permitted_flags;\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tpermitted_flags = IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC;\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, permitted_flags);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_pbuf_ring(ctx, &reg, bl);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n \tret = io_buffer_add_list(ctx, bl, reg.bgid);\n-\tif (!ret)\n-\t\treturn 0;\n-fail:\n-\tio_free_region(ctx->user, &bl->region);\n-\tkfree(bl);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n \treturn ret;\n }\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the naming of io_unregister_pbuf_ring() to make it more generic and applicable for both provided buffer rings and kernel-managed buffer rings. The author agreed to rename the function as part of preparatory changes for upcoming kernel-managed buffer ring support.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "preparatory change",
                "rename function"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Use the more generic name io_unregister_buf_ring() as this function will\nbe used for unregistering both provided buffer rings and kernel-managed\nbuffer rings.\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c     | 2 +-\n io_uring/kbuf.h     | 2 +-\n io_uring/register.c | 2 +-\n 3 files changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 850b836f32ee..aa9b70b72db4 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -719,7 +719,7 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \treturn ret;\n }\n \n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n {\n \tstruct io_uring_buf_reg reg;\n \tstruct io_buffer_list *bl;\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex bf15e26520d3..40b44f4fdb15 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -74,7 +74,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags);\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 594b1f2ce875..0882cb34f851 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -841,7 +841,7 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-\t\tret = io_unregister_pbuf_ring(ctx, arg);\n+\t\tret = io_unregister_buf_ring(ctx, arg);\n \t\tbreak;\n \tcase IORING_REGISTER_SYNC_CANCEL:\n \t\tret = -EINVAL;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the implementation of kernel-managed buffer rings, explaining that they reuse validation and buffer list allocation helpers from earlier refactoring and introducing new registration opcodes for kernel-managed ring buffers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for kernel-managed buffer rings (kmbuf rings), which allow\nthe kernel to allocate and manage the backing buffers for a buffer\nring, rather than requiring the application to provide and manage them.\n\nThis introduces two new registration opcodes:\n- IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring\n- IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring\n\nThe existing io_uring_buf_reg structure is extended with a union to\nsupport both application-provided buffer rings (pbuf) and kernel-managed\nbuffer rings (kmbuf):\n- For pbuf rings: ring_addr specifies the user-provided ring address\n- For kmbuf rings: buf_size specifies the size of each buffer. buf_size\n  must be non-zero and page-aligned.\n\nThe implementation follows the same pattern as pbuf ring registration,\nreusing the validation and buffer list allocation helpers introduced in\nearlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as\nkernel-managed for appropriate handling in the I/O path.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  15 ++++-\n io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-\n io_uring/kbuf.h               |   7 ++-\n io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++\n io_uring/memmap.h             |   4 ++\n io_uring/register.c           |   7 +++\n 6 files changed, 219 insertions(+), 6 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex fc473af6feb4..a0889c1744bd 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -715,6 +715,10 @@ enum io_uring_register_op {\n \t/* register bpf filtering programs */\n \tIORING_REGISTER_BPF_FILTER\t\t= 37,\n \n+\t/* register/unregister kernel-managed ring buffer group */\n+\tIORING_REGISTER_KMBUF_RING\t\t= 38,\n+\tIORING_UNREGISTER_KMBUF_RING\t\t= 39,\n+\n \t/* this goes last */\n \tIORING_REGISTER_LAST,\n \n@@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {\n \tIOU_PBUF_RING_INC\t= 2,\n };\n \n-/* argument for IORING_(UN)REGISTER_PBUF_RING */\n+/* argument for IORING_(UN)REGISTER_PBUF_RING and\n+ * IORING_(UN)REGISTER_KMBUF_RING\n+ */\n struct io_uring_buf_reg {\n-\t__u64\tring_addr;\n+\tunion {\n+\t\t/* used for pbuf rings */\n+\t\t__u64\tring_addr;\n+\t\t/* used for kmbuf rings */\n+\t\t__u32   buf_size;\n+\t};\n \t__u32\tring_entries;\n \t__u16\tbgid;\n \t__u16\tflags;\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex aa9b70b72db4..9bc36451d083 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -427,10 +427,13 @@ static int io_remove_buffers_legacy(struct io_ring_ctx *ctx,\n \n static void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)\n {\n-\tif (bl->flags & IOBL_BUF_RING)\n+\tif (bl->flags & IOBL_BUF_RING) {\n \t\tio_free_region(ctx->user, &bl->region);\n-\telse\n+\t\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\t\tkfree(bl->buf_ring);\n+\t} else {\n \t\tio_remove_buffers_legacy(ctx, bl, -1U);\n+\t}\n \n \tkfree(bl);\n }\n@@ -779,3 +782,77 @@ struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n \t\treturn NULL;\n \treturn &bl->region;\n }\n+\n+static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_buffer_list *bl,\n+\t\t\t       struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_uring_buf_ring *ring;\n+\tunsigned long ring_size;\n+\tvoid *buf_region;\n+\tunsigned int i;\n+\tint ret;\n+\n+\t/* allocate pages for the ring structure */\n+\tring_size = flex_array_size(ring, bufs, bl->nr_entries);\n+\tring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);\n+\tif (!ring)\n+\t\treturn -ENOMEM;\n+\n+\tret = io_create_region_multi_buf(ctx, &bl->region, bl->nr_entries,\n+\t\t\t\t\t reg->buf_size);\n+\tif (ret) {\n+\t\tkfree(ring);\n+\t\treturn ret;\n+\t}\n+\n+\t/* initialize ring buf entries to point to the buffers */\n+\tbuf_region = bl->region.ptr;\n+\tfor (i = 0; i < bl->nr_entries; i++) {\n+\t\tstruct io_uring_buf *buf = &ring->bufs[i];\n+\n+\t\tbuf->addr = (u64)(uintptr_t)buf_region;\n+\t\tbuf->len = reg->buf_size;\n+\t\tbuf->bid = i;\n+\n+\t\tbuf_region += reg->buf_size;\n+\t}\n+\tring->tail = bl->nr_entries;\n+\n+\tbl->buf_ring = ring;\n+\tbl->flags |= IOBL_KERNEL_MANAGED;\n+\n+\treturn 0;\n+}\n+\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, 0);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tif (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_kmbuf_ring(ctx, bl, &reg);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n+\tret = io_buffer_add_list(ctx, bl, reg.bgid);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n+\treturn ret;\n+}\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 40b44f4fdb15..62c80a1ebf03 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -7,9 +7,11 @@\n \n enum {\n \t/* ring mapped provided buffers */\n-\tIOBL_BUF_RING\t= 1,\n+\tIOBL_BUF_RING\t\t= 1,\n \t/* buffers are consumed incrementally rather than always fully */\n-\tIOBL_INC\t= 2,\n+\tIOBL_INC\t\t= 2,\n+\t/* buffers are kernel managed */\n+\tIOBL_KERNEL_MANAGED\t= 4,\n };\n \n struct io_buffer_list {\n@@ -74,6 +76,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 89f56609e50a..8d37e93c0433 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -15,6 +15,28 @@\n #include \"rsrc.h\"\n #include \"zcrx.h\"\n \n+static void release_multi_buf_pages(struct page **pages, unsigned long nr_pages)\n+{\n+\tstruct page *page;\n+\tunsigned int nr, i = 0;\n+\n+\twhile (nr_pages) {\n+\t\tpage = pages[i];\n+\n+\t\tif (!page || WARN_ON_ONCE(page != compound_head(page)))\n+\t\t\treturn;\n+\n+\t\tnr = compound_nr(page);\n+\t\tput_page(page);\n+\n+\t\tif (WARN_ON_ONCE(nr > nr_pages))\n+\t\t\treturn;\n+\n+\t\ti += nr;\n+\t\tnr_pages -= nr;\n+\t}\n+}\n+\n static bool io_mem_alloc_compound(struct page **pages, int nr_pages,\n \t\t\t\t  size_t size, gfp_t gfp)\n {\n@@ -86,6 +108,8 @@ enum {\n \tIO_REGION_F_USER_PROVIDED\t\t= 2,\n \t/* only the first page in the array is ref'ed */\n \tIO_REGION_F_SINGLE_REF\t\t\t= 4,\n+\t/* pages in the array belong to multiple discrete allocations */\n+\tIO_REGION_F_MULTI_BUF\t\t\t= 8,\n };\n \n void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n@@ -98,6 +122,8 @@ void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n \n \t\tif (mr->flags & IO_REGION_F_USER_PROVIDED)\n \t\t\tunpin_user_pages(mr->pages, nr_refs);\n+\t\telse if (mr->flags & IO_REGION_F_MULTI_BUF)\n+\t\t\trelease_multi_buf_pages(mr->pages, nr_refs);\n \t\telse\n \t\t\trelease_pages(mr->pages, nr_refs);\n \n@@ -149,6 +175,54 @@ static int io_region_pin_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+static int io_region_allocate_pages_multi_buf(struct io_mapped_region *mr,\n+\t\t\t\t\t      unsigned int nr_bufs,\n+\t\t\t\t\t      unsigned int buf_size)\n+{\n+\tgfp_t gfp = GFP_USER | __GFP_ACCOUNT | __GFP_ZERO | __GFP_NOWARN;\n+\tstruct page **pages, **cur_pages;\n+\tunsigned int nr_allocated;\n+\tunsigned int buf_pages;\n+\tunsigned int i;\n+\n+\tif (!PAGE_ALIGNED(buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbuf_pages = buf_size >> PAGE_SHIFT;\n+\n+\tpages = kvmalloc_array(mr->nr_pages, sizeof(*pages), gfp);\n+\tif (!pages)\n+\t\treturn -ENOMEM;\n+\n+\tcur_pages = pages;\n+\n+\tfor (i = 0; i < nr_bufs; i++) {\n+\t\tif (io_mem_alloc_compound(cur_pages, buf_pages, buf_size,\n+\t\t\t\t\t  gfp)) {\n+\t\t\tcur_pages += buf_pages;\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tnr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,\n+\t\t\t\t\t\t     buf_pages, cur_pages);\n+\t\tif (nr_allocated != buf_pages) {\n+\t\t\tunsigned int total =\n+\t\t\t\t(cur_pages - pages) + nr_allocated;\n+\n+\t\t\trelease_multi_buf_pages(pages, total);\n+\t\t\tkvfree(pages);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tcur_pages += buf_pages;\n+\t}\n+\n+\tmr->flags |= IO_REGION_F_MULTI_BUF;\n+\tmr->pages = pages;\n+\n+\treturn 0;\n+}\n+\n static int io_region_allocate_pages(struct io_mapped_region *mr,\n \t\t\t\t    struct io_uring_region_desc *reg,\n \t\t\t\t    unsigned long mmap_offset)\n@@ -181,6 +255,43 @@ static int io_region_allocate_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size)\n+{\n+\tunsigned int nr_pages;\n+\tint ret;\n+\n+\tif (WARN_ON_ONCE(mr->pages || mr->ptr || mr->nr_pages))\n+\t\treturn -EFAULT;\n+\n+\tif (WARN_ON_ONCE(!nr_bufs || !buf_size || !PAGE_ALIGNED(buf_size)))\n+\t\treturn -EINVAL;\n+\n+\tif (check_mul_overflow(buf_size >> PAGE_SHIFT, nr_bufs, &nr_pages))\n+\t\treturn -EINVAL;\n+\n+\tif (ctx->user) {\n+\t\tret = __io_account_mem(ctx->user, nr_pages);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\t}\n+\tmr->nr_pages = nr_pages;\n+\n+\tret = io_region_allocate_pages_multi_buf(mr, nr_bufs, buf_size);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\tret = io_region_init_ptr(mr);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\treturn 0;\n+out_free:\n+\tio_free_region(ctx->user, mr);\n+\treturn ret;\n+}\n+\n int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset)\ndiff --git a/io_uring/memmap.h b/io_uring/memmap.h\nindex f4cfbb6b9a1f..3aa1167462ae 100644\n--- a/io_uring/memmap.h\n+++ b/io_uring/memmap.h\n@@ -22,6 +22,10 @@ int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset);\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size);\n+\n static inline void *io_region_get_ptr(struct io_mapped_region *mr)\n {\n \treturn mr->ptr;\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 0882cb34f851..2db8daaf8fde 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -837,7 +837,14 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\t\tbreak;\n \t\tret = io_register_pbuf_ring(ctx, arg);\n \t\tbreak;\n+\tcase IORING_REGISTER_KMBUF_RING:\n+\t\tret = -EINVAL;\n+\t\tif (!arg || nr_args != 1)\n+\t\t\tbreak;\n+\t\tret = io_register_kmbuf_ring(ctx, arg);\n+\t\tbreak;\n \tcase IORING_UNREGISTER_PBUF_RING:\n+\tcase IORING_UNREGISTER_KMBUF_RING:\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the implementation of kernel-managed buffer rings (kmbuf) and how they interact with mmap, explaining that kmbuf rings use the same pattern as application-provided buffer rings (pbuf) but introduce new constants for encoding the buffer group ID. The author confirmed that userspace can access the kernel-allocated buffers directly through mmap.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "author provided a clear explanation of their implementation",
                "author confirmed that userspace can access kernel-allocated buffers"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for mmapping kernel-managed buffer rings (kmbuf) to\nuserspace, allowing applications to access the kernel-allocated buffers.\n\nSimilar to application-provided buffer rings (pbuf), kmbuf rings use the\nbuffer group ID encoded in the mmap offset to identify which buffer ring\nto map. The implementation follows the same pattern as pbuf rings.\n\nNew mmap offset constants are introduced:\n  - IORING_OFF_KMBUF_RING (0x88000000): Base offset for kmbuf mappings\n  - IORING_OFF_KMBUF_SHIFT (16): Shift value to encode buffer group ID\n\nThe mmap offset encodes the bgid shifted by IORING_OFF_KMBUF_SHIFT.\nThe io_buf_get_region() helper retrieves the appropriate region.\n\nThis allows userspace to mmap the kernel-allocated buffer region and\naccess the buffers directly.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  2 ++\n io_uring/kbuf.c               | 11 +++++++++--\n io_uring/kbuf.h               |  5 +++--\n io_uring/memmap.c             |  5 ++++-\n 4 files changed, 18 insertions(+), 5 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex a0889c1744bd..42a2812c9922 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -545,6 +545,8 @@ struct io_uring_cqe {\n #define IORING_OFF_SQES\t\t\t0x10000000ULL\n #define IORING_OFF_PBUF_RING\t\t0x80000000ULL\n #define IORING_OFF_PBUF_SHIFT\t\t16\n+#define IORING_OFF_KMBUF_RING\t\t0x88000000ULL\n+#define IORING_OFF_KMBUF_SHIFT\t\t16\n #define IORING_OFF_MMAP_MASK\t\t0xf8000000ULL\n \n /*\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9bc36451d083..ccf5b213087b 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -770,16 +770,23 @@ int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg)\n \treturn 0;\n }\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid)\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed)\n {\n \tstruct io_buffer_list *bl;\n+\tbool is_kernel_managed;\n \n \tlockdep_assert_held(&ctx->mmap_lock);\n \n \tbl = xa_load(&ctx->io_bl_xa, bgid);\n \tif (!bl || !(bl->flags & IOBL_BUF_RING))\n \t\treturn NULL;\n+\n+\tis_kernel_managed = !!(bl->flags & IOBL_KERNEL_MANAGED);\n+\tif (is_kernel_managed != kernel_managed)\n+\t\treturn NULL;\n+\n \treturn &bl->region;\n }\n \ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 62c80a1ebf03..11d165888b8e 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -88,8 +88,9 @@ unsigned int __io_put_kbufs(struct io_kiocb *req, struct io_buffer_list *bl,\n bool io_kbuf_commit(struct io_kiocb *req,\n \t\t    struct io_buffer_list *bl, int len, int nr);\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid);\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed);\n \n static inline bool io_kbuf_recycle_ring(struct io_kiocb *req,\n \t\t\t\t\tstruct io_buffer_list *bl)\ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 8d37e93c0433..916315122323 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -356,7 +356,10 @@ static struct io_mapped_region *io_mmap_get_region(struct io_ring_ctx *ctx,\n \t\treturn &ctx->sq_region;\n \tcase IORING_OFF_PBUF_RING:\n \t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_PBUF_SHIFT;\n-\t\treturn io_pbuf_get_region(ctx, id);\n+\t\treturn io_buf_get_region(ctx, id, false);\n+\tcase IORING_OFF_KMBUF_RING:\n+\t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_KMBUF_SHIFT;\n+\t\treturn io_buf_get_region(ctx, id, true);\n \tcase IORING_MAP_OFF_PARAM_REGION:\n \t\treturn &ctx->param_region;\n \tcase IORING_MAP_OFF_ZCRX_REGION:\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about distinguishing between kernel-managed buffer addresses and negative values when error checking, explaining that the io_br_sel struct needs to be modified to separate address and value fields for kernel-managed buffers. The author provided a patch that modifies the io_uring_types.h file and the kbuf.c file to implement this change.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Allow kernel-managed buffers to be selected. This requires modifying the\nio_br_sel struct to separate the fields for address and val, since a\nkernel address cannot be distinguished from a negative val when error\nchecking.\n\nAuto-commit any selected kernel-managed buffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring_types.h |  8 ++++----\n io_uring/kbuf.c                | 16 ++++++++++++----\n 2 files changed, 16 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 3e4a82a6f817..36cc2e0346d9 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -93,13 +93,13 @@ struct io_mapped_region {\n  */\n struct io_br_sel {\n \tstruct io_buffer_list *buf_list;\n-\t/*\n-\t * Some selection parts return the user address, others return an error.\n-\t */\n \tunion {\n+\t\t/* for classic/ring provided buffers */\n \t\tvoid __user *addr;\n-\t\tssize_t val;\n+\t\t/* for kernel-managed buffers */\n+\t\tvoid *kaddr;\n \t};\n+\tssize_t val;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex ccf5b213087b..1e8395270227 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -155,7 +155,8 @@ static int io_provided_buffers_select(struct io_kiocb *req, size_t *len,\n \treturn 1;\n }\n \n-static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n+static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n+\t\t\t     unsigned int issue_flags)\n {\n \t/*\n \t* If we came in unlocked, we have no choice but to consume the\n@@ -170,7 +171,11 @@ static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n \tif (issue_flags & IO_URING_F_UNLOCKED)\n \t\treturn true;\n \n-\t/* uring_cmd commits kbuf upfront, no need to auto-commit */\n+\t/* kernel-managed buffers are auto-committed */\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\treturn true;\n+\n+\t/* multishot uring_cmd commits kbuf upfront, no need to auto-commit */\n \tif (!io_file_can_poll(req) && req->opcode != IORING_OP_URING_CMD)\n \t\treturn true;\n \treturn false;\n@@ -200,9 +205,12 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n-\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n+\telse\n+\t\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n \n-\tif (io_should_commit(req, issue_flags)) {\n+\tif (io_should_commit(req, bl, issue_flags)) {\n \t\tio_kbuf_commit(req, sel.buf_list, *len, 1);\n \t\tsel.buf_list = NULL;\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about userspace unregistering a buffer ring while it is pinned by the kernel, and added APIs to pin and unpin buffer rings, preventing this issue. The pinning mechanism ensures that the buffer ring remains valid for kernel subsystems accessing its contents in atomic contexts.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix",
                "added preparatory change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add kernel APIs to pin and unpin buffer rings, preventing userspace from\nunregistering a buffer ring while it is pinned by the kernel.\n\nThis provides a mechanism for kernel subsystems to safely access buffer\nring contents while ensuring the buffer ring remains valid. A pinned\nbuffer ring cannot be unregistered until explicitly unpinned. On the\nuserspace side, trying to unregister a pinned buffer will return -EBUSY.\n\nThis is a preparatory change for upcoming fuse usage of kernel-managed\nbuffer rings. It is necessary for fuse to pin the buffer ring because\nfuse may need to select a buffer in atomic contexts, which it can only\ndo so by using the underlying buffer list pointer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 17 +++++++++++++\n io_uring/kbuf.c              | 48 ++++++++++++++++++++++++++++++++++++\n io_uring/kbuf.h              |  5 ++++\n 3 files changed, 70 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 375fd048c4cb..702b1903e6ee 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -84,6 +84,10 @@ struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n \t\t\t\t struct io_br_sel *sel, unsigned int issue_flags);\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t    unsigned issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -126,6 +130,19 @@ static inline bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n {\n \treturn true;\n }\n+static inline int io_uring_buf_ring_pin(struct io_uring_cmd *cmd,\n+\t\t\t\t\tunsigned buf_group,\n+\t\t\t\t\tunsigned issue_flags,\n+\t\t\t\t\tstruct io_buffer_list **bl)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n+static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned buf_group,\n+\t\t\t\t\t  unsigned issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 1e8395270227..dee1764ed19f 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -9,6 +9,7 @@\n #include <linux/poll.h>\n #include <linux/vmalloc.h>\n #include <linux/io_uring.h>\n+#include <linux/io_uring/cmd.h>\n \n #include <uapi/linux/io_uring.h>\n \n@@ -237,6 +238,51 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \treturn sel;\n }\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *buffer_list;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbuffer_list = io_buffer_get_list(ctx, buf_group);\n+\tif (buffer_list && (buffer_list->flags & IOBL_BUF_RING)) {\n+\t\tif (unlikely(buffer_list->flags & IOBL_PINNED)) {\n+\t\t\tret = -EALREADY;\n+\t\t} else {\n+\t\t\tbuffer_list->flags |= IOBL_PINNED;\n+\t\t\tret = 0;\n+\t\t\t*bl = buffer_list;\n+\t\t}\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_pin);\n+\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t       unsigned issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (bl && (bl->flags & IOBL_BUF_RING) && (bl->flags & IOBL_PINNED)) {\n+\t\tbl->flags &= ~IOBL_PINNED;\n+\t\tret = 0;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_unpin);\n+\n /* cap it at a reasonable 256, will be one page even for 4K */\n #define PEEK_MAX_IMPORT\t\t256\n \n@@ -747,6 +793,8 @@ int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t\treturn -ENOENT;\n \tif (!(bl->flags & IOBL_BUF_RING))\n \t\treturn -EINVAL;\n+\tif (bl->flags & IOBL_PINNED)\n+\t\treturn -EBUSY;\n \n \tscoped_guard(mutex, &ctx->mmap_lock)\n \t\txa_erase(&ctx->io_bl_xa, bl->bgid);\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 11d165888b8e..781630c2cc10 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -12,6 +12,11 @@ enum {\n \tIOBL_INC\t\t= 2,\n \t/* buffers are kernel managed */\n \tIOBL_KERNEL_MANAGED\t= 4,\n+\t/*\n+\t * buffer ring is pinned and cannot be unregistered by userspace until\n+\t * it has been unpinned\n+\t */\n+\tIOBL_PINNED\t\t= 8,\n };\n \n struct io_buffer_list {\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the lack of an interface for recycling buffers back into kernel-managed buffer rings, added an interface (io_uring_kmbuf_recycle) to recycle buffers and update the io_kiocb flags accordingly.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "addressed_concern",
                "added_feature"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add an interface for buffers to be recycled back into a kernel-managed\nbuffer ring.\n\nThis is a preparatory patch for fuse over io-uring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 11 +++++++++\n io_uring/kbuf.c              | 44 ++++++++++++++++++++++++++++++++++++\n 2 files changed, 55 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 702b1903e6ee..a488e945f883 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -88,6 +88,10 @@ int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t    unsigned issue_flags);\n+\n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -143,6 +147,13 @@ static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n+\t\t\t\t\t unsigned int buf_group, u64 addr,\n+\t\t\t\t\t unsigned int len, unsigned int bid,\n+\t\t\t\t\t unsigned int issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex dee1764ed19f..17b6178be4ce 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -102,6 +102,50 @@ void io_kbuf_drop_legacy(struct io_kiocb *req)\n \treq->kbuf = NULL;\n }\n \n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags)\n+{\n+\tstruct io_kiocb *req = cmd_to_io_kiocb(cmd);\n+\tstruct io_ring_ctx *ctx = req->ctx;\n+\tstruct io_uring_buf_ring *br;\n+\tstruct io_uring_buf *buf;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tif (WARN_ON_ONCE(req->flags & REQ_F_BUFFERS_COMMIT))\n+\t\treturn ret;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\n+\tif (!bl || WARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING)) ||\n+\t    WARN_ON_ONCE(!(bl->flags & IOBL_KERNEL_MANAGED)))\n+\t\tgoto done;\n+\n+\tbr = bl->buf_ring;\n+\n+\tif (WARN_ON_ONCE((br->tail - bl->head) >= bl->nr_entries))\n+\t\tgoto done;\n+\n+\tbuf = &br->bufs[(br->tail) & bl->mask];\n+\n+\tbuf->addr = addr;\n+\tbuf->len = len;\n+\tbuf->bid = bid;\n+\n+\treq->flags &= ~REQ_F_BUFFER_RING;\n+\n+\tbr->tail++;\n+\tret = 0;\n+\n+done:\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_kmbuf_recycle);\n+\n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)\n {\n \tstruct io_ring_ctx *ctx = req->ctx;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring_is_kmbuf_ring() function, which was missing an implementation to check if a buffer ring is kernel-managed. The author added this implementation in the latest patch version, checking for the IOBL_KERNEL_MANAGED flag and verifying that the buffer ring is not empty.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "added implementation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "io_uring_is_kmbuf_ring() returns true if there is a kernel-managed\nbuffer ring at the specified buffer group.\n\nThis is a preparatory patch for upcoming fuse kernel-managed buffer\nsupport, which needs to ensure the buffer ring registered by the server\nis a kernel-managed buffer ring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h |  9 +++++++++\n io_uring/kbuf.c              | 20 ++++++++++++++++++++\n 2 files changed, 29 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex a488e945f883..04a937f6f4d3 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -92,6 +92,9 @@ int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t   u64 addr, unsigned int len, unsigned int bid,\n \t\t\t   unsigned int issue_flags);\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -154,6 +157,12 @@ static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned int buf_group,\n+\t\t\t\t\t  unsigned int issue_flags)\n+{\n+\treturn false;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 17b6178be4ce..797cc2f0a5e9 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -963,3 +963,23 @@ int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \n \treturn ret;\n }\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tbool is_kmbuf_ring = false;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (likely(bl) && (bl->flags & IOBL_KERNEL_MANAGED)) {\n+\t\tWARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING));\n+\t\tis_kmbuf_ring = true;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn is_kmbuf_ring;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_is_kmbuf_ring);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about the io_uring mutex being held by in-progress commits and atomic contexts when selecting a buffer from a kernel-managed bufring. The author agrees that exporting io_ring_buffer_select() is necessary to allow fuse io-uring to select a buffer without needing to grab the io_uring mutex, and has added this export to the patch.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreement",
                "preparatory patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Export io_ring_buffer_select() so that it may be used by callers who\npass in a pinned bufring without needing to grab the io_uring mutex.\n\nThis is a preparatory patch that will be needed by fuse io-uring, which\nwill need to select a buffer from a kernel-managed bufring while the\nuring mutex may already be held by in-progress commits, and may need to\nselect a buffer in atomic contexts.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 14 ++++++++++++++\n io_uring/kbuf.c              |  7 ++++---\n 2 files changed, 18 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 04a937f6f4d3..d4b5943bdeb1 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -95,6 +95,10 @@ int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \n bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t    unsigned int issue_flags);\n+\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -163,6 +167,16 @@ static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n {\n \treturn false;\n }\n+static inline struct io_br_sel io_ring_buffer_select(struct io_kiocb *req,\n+\t\t\t\t\t\t     size_t *len,\n+\t\t\t\t\t\t     struct io_buffer_list *bl,\n+\t\t\t\t\t\t     unsigned int issue_flags)\n+{\n+\tstruct io_br_sel sel = {\n+\t\t.val = -EOPNOTSUPP,\n+\t};\n+\treturn sel;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 797cc2f0a5e9..9a93f10d3214 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -226,9 +226,9 @@ static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n \treturn false;\n }\n \n-static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n-\t\t\t\t\t      struct io_buffer_list *bl,\n-\t\t\t\t\t      unsigned int issue_flags)\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags)\n {\n \tstruct io_uring_buf_ring *br = bl->buf_ring;\n \t__u16 tail, head = bl->head;\n@@ -261,6 +261,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \t}\n \treturn sel;\n }\n+EXPORT_SYMBOL_GPL(io_ring_buffer_select);\n \n struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \t\t\t\t  unsigned buf_group, unsigned int issue_flags)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about returning the id of the selected buffer in io_buffer_select(). They are modifying the function to return the selected buffer address, size, and id by adding a new field 'buf_id' to the struct io_br_sel. The patch includes changes to include/linux/io_uring/cmd.h, include/linux/io_uring_types.h, and io_uring/kbuf.c.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying change",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Return the id of the selected buffer in io_buffer_select(). This is\nneeded for kernel-managed buffer rings to later recycle the selected\nbuffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h   | 2 +-\n include/linux/io_uring_types.h | 2 ++\n io_uring/kbuf.c                | 7 +++++--\n 3 files changed, 8 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex d4b5943bdeb1..94df2bdebe77 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);\n \n /*\n  * Select a buffer from the provided buffer group for multishot uring_cmd.\n- * Returns the selected buffer address and size.\n+ * Returns the selected buffer address, size, and id.\n  */\n struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n \t\t\t\t\t    unsigned buf_group, size_t *len,\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 36cc2e0346d9..5a56bb341337 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -100,6 +100,8 @@ struct io_br_sel {\n \t\tvoid *kaddr;\n \t};\n \tssize_t val;\n+\t/* id of the selected buffer */\n+\tunsigned buf_id;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9a93f10d3214..24c1e34ea23e 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -250,6 +250,7 @@ struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n+\tsel.buf_id = req->buf_index;\n \tif (bl->flags & IOBL_KERNEL_MANAGED)\n \t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n \telse\n@@ -274,10 +275,12 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \n \tbl = io_buffer_get_list(ctx, buf_group);\n \tif (likely(bl)) {\n-\t\tif (bl->flags & IOBL_BUF_RING)\n+\t\tif (bl->flags & IOBL_BUF_RING) {\n \t\t\tsel = io_ring_buffer_select(req, len, bl, issue_flags);\n-\t\telse\n+\t\t} else {\n \t\t\tsel.addr = io_provided_buffer_select(req, len, bl);\n+\t\t\tsel.buf_id = req->buf_index;\n+\t\t}\n \t}\n \tio_ring_submit_unlock(req->ctx, issue_flags);\n \treturn sel;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about indicating which buffer was selected in the completion queue entry, explained that setting IORING_CQE_F_BUFFER and encoding the buffer index is necessary for fuse to relay this information to userspace.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When uring_cmd operations select a buffer, the completion queue entry\nshould indicate which buffer was selected.\n\nSet IORING_CQE_F_BUFFER on the completed entry and encode the buffer\nindex if a buffer was selected.\n\nThis will be needed for fuse, which needs to relay to userspace which\nselected buffer contains the data.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/uring_cmd.c | 6 +++++-\n 1 file changed, 5 insertions(+), 1 deletion(-)\n\ndiff --git a/io_uring/uring_cmd.c b/io_uring/uring_cmd.c\nindex ee7b49f47cb5..6d38df1a812d 100644\n--- a/io_uring/uring_cmd.c\n+++ b/io_uring/uring_cmd.c\n@@ -151,6 +151,7 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \t\t       unsigned issue_flags, bool is_cqe32)\n {\n \tstruct io_kiocb *req = cmd_to_io_kiocb(ioucmd);\n+\tu32 cflags = 0;\n \n \tif (WARN_ON_ONCE(req->flags & REQ_F_APOLL_MULTISHOT))\n \t\treturn;\n@@ -160,7 +161,10 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \tif (ret < 0)\n \t\treq_set_fail(req);\n \n-\tio_req_set_res(req, ret, 0);\n+\tif (req->flags & (REQ_F_BUFFER_SELECTED | REQ_F_BUFFER_RING))\n+\t\tcflags |= IORING_CQE_F_BUFFER |\n+\t\t\t(req->buf_index << IORING_CQE_BUFFER_SHIFT);\n+\tio_req_set_res(req, ret, cflags);\n \tif (is_cqe32) {\n \t\tif (req->ctx->flags & IORING_SETUP_CQE_MIXED)\n \t\t\treq->cqe.flags |= IORING_CQE_F_32;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "reviewer suggested adding a WARN_ON_ONCE() check to prevent int promotion from affecting the calculation of (br->tail - bl->head) >= bl->nr_entries, and noted that while it's not a huge issue for now, some of these WARN_ON_ONCE() instances may be pruned later",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "optimization suggestion"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you want:\n\n\tif (WARN_ON_ONCE((__u16)(br->tail - bl->head) >= bl->nr_entries))\n\nhere to avoid int promotion from messing this up if tail has wrapped.\n\nIn general, across the patches for the WARN_ON_ONCE(), it's not a huge\nissue to have a litter of them for now. Hopefully we can prune some of\nthese down the line, however.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe questioned the necessity of introducing a new field io_uring_cmd_data->kmbuf_idx, suggesting that req->buf_index could be used instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm probably missing something here, but why can't the caller just use\nreq->buf_index for this?\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe requested that Joanne Koong provide a branch with all patches, including user code, for easier cross-referencing and evaluation of the helpers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request",
                "clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Generally looks pretty good - for context, do you have a branch with\nthese patches and the users on top too? Makes it a bit easier for cross\nreferencing, as some of these really do need an exposed user to make a\ngood judgement on the helpers.\n\nI know there's the older series, but I'm assuming the latter patches\nchanged somewhat too, and it'd be nicer to look at a current set rather\nthan go back to the older ones.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested refactoring io_pbuf_get_region() to handle kernel-managed buffer rings by adding a new helper function, io_kbuf_get_region(), and checking the bl->flags in both functions for readability.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "minor nit"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "For this, I think just add another helper - leave io_pbuf_get_region()\nand add a bl->flags & IOBL_KERNEL_MANAGED error check in there, and\nadd a io_kbuf_get_region() or similar and have a !(bl->flags &\nIOBL_KERNEL_MANAGED) error check in that one.\n\nThat's easier to read, and there's little reason to avoid duplicating\nthe xa_load() part.\n\nMinor nit, but imho it's more readable that way.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested using a pointer to struct io_buffer_list instead of a reference to it, and recommended either returning an error pointer or renaming the parameter to indicate its return value.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Probably use the usual struct io_buffer_list *bl here and either use an\nERR_PTR return, or rename the passed on **bl to **blret or something.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Jens Axboe suggested a more efficient way to check for pinned buffer rings by combining two flags into one condition, and also recommended adding an early return statement when the buffer list is empty.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Usually done as:\n\n\tif ((bl->flags & (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))\n\nand maybe then just have an earlier\n\n\tif (!bl)\n\t\tgoto err;",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested that the patch should be modified to avoid exceeding the 80 character limit for io_uring strings, citing a desire to keep code consistent and maintainable.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "to avoid making it way too long. For io_uring, it's fine to exceed 80\nchars where it makes sense.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the code should allow regions to work with user-passed memory, which would enable optimizations such as huge pages, and requested consideration of this.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "If you're creating a region, there should be no reason why it\ncan't work with user passed memory. You're fencing yourself off\noptimisations that are already there like huge pages.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer suggested refactoring io_uring/kbuf: IORING_REGISTER_KMBUF_RING should not allocate buffers, instead it should register a memory region and use that for buffer allocation. They also proposed creating a new flag or internal API to handle this.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Please use io_create_region(), the new function does nothing new\nand only violates abstractions.\n\nProvided buffer rings with kernel addresses could be an interesting\nabstraction, but why is it also responsible for allocating buffers?\nWhat I'd do:\n\n1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.\n2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.\n    Or maybe don't expose it to the user at all and create it from\n    fuse via internal API.\n3. Require the user to register a memory region of appropriate size,\n    see IORING_REGISTER_MEM_REGION, ctx->param_region. Make fuse\n    populating the buffer ring using the memory region.\n\nI wanted to make regions shareable anyway (need it for other purposes),\nI can toss patches for that tomorrow.\n\nA separate question is whether extending buffer rings is the right\napproach as it seems like you're only using it for fuse requests and\nnot for passing buffers to normal requests, but I don't see the\nbig picture here.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the removal of io_create_region_multi_buf() eliminates the need for aligning every buffer, which could result in wasted memory due to 64KB page sizes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested optimization",
                "memory efficiency concern"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "With io_create_region_multi_buf() gone, you shouldn't need\nto align every buffer, that could be a lot of wasted memory\n(thinking about 64KB pages).",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Caleb Mateos",
              "summary": "Reviewer Caleb Mateos noted that the patch's optimization !(~bl->flags & (IOBL_BUF_RING|IOBL_PINNED)) is unnecessary, as modern compilers will automatically perform this optimization and potentially optimize it further.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested change",
                "optimization"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, modern compilers will perform this optimization automatically.\nThey'll even optimize it further to !(~bl->flags &\n(IOBL_BUF_RING|IOBL_PINNED)): https://godbolt.org/z/xGoP4TfhP\n\nBest,\nCaleb",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested that the patch's implementation should follow a more common and readable approach, citing an example where his own version is easier to read than the original.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested improvement",
                "example provided"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure, it's not about that, it's more about the common way of doing it,\nwhich makes it easier to read for people. FWIW, your example is easier\nto read too than the original.\n\n-- \nJens Axboe",
              "reply_to": "Caleb Mateos",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author is asking for clarification on whether kernel-allocated buffers can be optimized further than user-allocated buffers, specifically in the case of huge pages.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Are there any optimizations with user-allocated buffers that wouldn't\nbe possible with kernel-allocated buffers? For huge pages, can't the\nkernel do this as well (eg I see in io_mem_alloc_compound(), it calls\ninto alloc_pages() with order > 0)?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern that using io_create_region() for both single and multi-buffer cases is insufficient, explaining that separate checks are needed between the two functions and different allocation calls.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "There's separate checks needed between io_create_region() and\nio_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag\nchecking) and different allocation calls (eg\nio_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).\nMaybe I'm misinterpreting your comment (or the code), but I'm not\nseeing how this can just use io_create_region().",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong is addressing Pavel Begunkov's feedback about registering buffer rings from userspace, explaining that kernel-managed buffer rings simplify interface and lifecycle management, and guarantee contiguous page allocation. She asks for elaboration on the benefits of allocating buffers from userspace.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "asking_for_clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Conceptually, I think it makes the interface and lifecycle management\nsimpler/cleaner. With registering it from userspace, imo there's\nadditional complications with no tangible benefits, eg it's not\nguaranteed that the memory regions registered for the buffers are the\nsame size, with allocating it from the kernel-side we can guarantee\nthat the pages are allocated physically contiguously, userspace setup\nwith user-allocated buffers is less straightforward, etc. In general,\nI'm just not really seeing what advantages there are in allocating the\nbuffers from userspace. Could you elaborate on that part more?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about pbuf rings needing to support pinning if kmbuf rings are merged into them, explained that pinning is already needed in fuse due to atomic context and recycling buffer issues, and implied that this requirement will still apply even if the patches are restructured.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical concern",
                "provided additional context"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If kmbuf rings are squashed into pbuf rings, then pbuf rings will need\nto support pinning. In fuse, there are some contexts where you can't\ngrab the uring mutex because you're running in atomic context and this\ncan be encountered while recycling the buffer. I originally had a\npatch adding pinning to pbuf rings (to mitigate the overhead of\nregistered buffers lookups) but dropped it when Jens and Caleb didn't\nlike the idea. But for kmbuf rings, pinning will be necessary for\nfuse.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to Pavel Begunkov's question about what constitutes a 'normal request' in the context of io_uring and kernel-managed buffer rings, explaining that for fuse's use case, there are only fuse requests.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "What are 'normal requests'? For fuse's use case, there are only fuse requests.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Jens' concern about accessing buffer IDs from within the io_uring internals by suggesting a helper function or returning the buf ID as part of the io_br_sel struct, indicating a willingness to accommodate reviewer feedback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "willingness to accommodate reviewer feedback",
                "open to alternative solutions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The caller can, but from the caller side they only have access to the\ncmd so they would need to do something like\n\nstruct io_kiocb *req = cmd_to_iocb_kiocb(ent->cmd);\nbuf_id = req->buf_index;\n\nwhich may be kind of ugly with looking inside io-uring internals.\nMaybe a helper here would be nicer, something like\nio_uring_cmd_buf_id() or io_uring_req_buf_id(). It seemed cleaner to\nme to just return the buf id as part of the io_br_sel struct, but I'm\nhappy to do it another way if you have a preference.\n\nThanks,\nJoanne",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged the need for changes in v2, specifically mentioned making changes pointed out by reviewer and addressing discussion with Pavel before submitting a revised patchset.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for changes",
                "v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for reviewing the patches. The branch containing the userside\nchanges on top of these patches is in [1]. I'll make the changes you\npointed out in your other comments as part of v2. Once the discussion\nwith Pavel is resolved / figured out with the changes he wants for v2,\nI'll submit v2.\n\nThanks,\nJoanne\n\n[1] https://github.com/joannekoong/linux/commits/fuse_zero_copy/",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that allocating 1MB in kernel space won't result in a PMD mappable huge page, unlike user space allocation which can register 2MB and reuse the rest for other purposes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change",
                "technical concern"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, there is handful of differences. To name one, 1MB allocation won't\nget you a PMD mappable huge page, while user space can allocate 2MB,\nregister the first 1MB and reuse the rest for other purposes.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested that instead of modifying io_create_region() to be less strict, the caller should filter arguments to ensure only necessary types are passed, specifically advising against passing IORING_MEM_REGION_TYPE_USER if it's not used.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "If io_create_region() is too strict, let's discuss that in\nexamples if there are any, but it's likely not a good idea changing\nthat. If it's too lax, filter arguments in the caller. IOW, don't\npass IORING_MEM_REGION_TYPE_USER if it's not used.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the memmap.c changes in the patch can be dropped because they are essentially replicating the functionality of io_create_region(), and suggested removing them to avoid potential issues such as disabling io_mem_alloc_compound()",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "potential issues"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I saw that and saying that all memmap.c changes can get dropped.\nYou're using it as one big virtually contig kernel memory range then\nchunked into buffers, and that's pretty much what you're getting with\nnormal io_create_region(). I get that you only need it to be\ncontiguous within a single buffer, but that's not what you're doing,\nand it'll be only worse than default io_create_region() e.g.\neffectively disabling any usefulness of io_mem_alloc_compound(),\nand ultimately you don't need to care.\n\nRegions shouldn't know anything about your buffers, how it's\nsubdivided after, etc.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested adding a check for user-provided memory to the io_create_region() call, providing an example of how this could be done by setting rd.user_addr and rd.flags",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "struct io_uring_region_desc rd = {};\ntotal_size = nr_bufs * buf_size;\nrd.size = PAGE_ALIGN(total_size);\nio_create_region(&region, &rd);\n\nAdd something like this for user provided memory:\n\nif (use_user_memory) {\n\trd.user_addr = uaddr;\n\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n}",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer suggests separating ring creation from population on the kernel API level, allowing other users like fuse to create rings and populate them independently.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvement"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think I follow. I'm saying that it might be interesting\nto separate rings from how and with what they're populated on the\nkernel API level, but the fuse kernel module can do the population\nand get exactly same layout as you currently have:\n\nint fuse_create_ring(size_t region_offset /* user space argument */) {\n\tstruct io_mapped_region *mr = get_mem_region(ctx);\n\t// that can take full control of the ring\n\tring = grab_empty_ring(io_uring_ctx);\n\n\tsize = nr_bufs * buf_size;\n\tif (region_offset + size > get_size(mr)) // + other validation\n\t\treturn error;\n\n\tbuf = mr_get_ptr(mr) + offset;\n\tfor (i = 0; i < nr_bufs; i++) {\n\t\tring_push_buffer(ring, buf, buf_size);\n\t\tbuf += buf_size;\n\t}\n}\n\nfuse might not care, but with empty rings other users will get a\nchannel they can use to do IO (e.g. read requests) using their\nkernel addresses in the future.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested that instead of introducing new UAPI and internal changes for kernel-managed buffer rings, the existing pbuf implementation could be used with a flag to differentiate between them. This would simplify the code and reduce duplication.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "suggested alternative approach",
                "flag-based differentiation"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It'd change uapi but not internals, you already piggy back it\non pbuf implementation and differentiate with a flag.\n\nIt could basically be:\n\nif (flags & IOU_PBUF_RING_KM)\n\tbl->flags |= IOBL_KERNEL_MANAGED;\n\nPinning can be gated on that flag as well. Pretty likely uapi\nand internals will be a bit cleaner, but that's not a huge deal,\njust don't see why would you roll out a separate set of uapi\n([un]register, offsets, etc.) when essentially it can be treated\nas the same thing.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that the original patch was pinning the registered buffer table without providing buffer rings, which is a bad idea; instead, the patch could have used a single larger registered buffer and pinned only it, leveraging its existing refcounting mechanism.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IIRC, you was pinning the registered buffer table and not provided\nbuffer rings? Which would indeed be a bad idea. Thinking about it,\nfwiw, instead of creating multiple registered buffers and trying to\nlock the entire table, you could've kept all memory in one larger\nregistered buffer and pinned only it. It's already refcounted, so\nshouldn't have been much of a problem.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed concerns that creating many small regions for kernel-managed buffer rings would lead to unnecessary mmap()s, extra user space management, and wasted space, and also questioned the ring bound memory approach due to potential issues with buffer lifetimes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "expressed concerns"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "To explain why, I don't think that creating many small regions\nis a good direction going forward. In case of kernel allocation,\nit's extra mmap()s, extra user space management, and wasted space.\nFor user provided memory it's over-accounting and extra memory\nfootprint. It'll also give you better lifecycle guarantees, i.e.\nyou won't be able to free buffers while there are requests for the\ncontext. I'm not so sure about ring bound memory, let's say I have\nmy suspicions, and you'd need to be extra careful about buffer\nlifetimes even after a fuse instance dies.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that kernel-managed buffer rings would be particularly useful for operations like read and recv, where the kernel can fill provided buffers without requiring changes to opcode-specific code in kbuf.c.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no specific request or objection"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Any kind of read/recv/etc. that can use provided buffers. It's\nwhere kernel memory filled rings would shine, as you'd be able\nto use them together without changing any opcode specific code.\nI.e. not changes in read request implementation, only kbuf.c\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that any pages mapped to userspace can be allocated in the kernel, allowing for a buffer ring that is only mapped read-only into userspace, enabling zero-copy raids if required by devices.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "likes the design",
                "zero-copy raids"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Any pages mapped to userspace can be allocated in the kernel as well.\n\nAnd I really do like this design, because it means we can have a\nbuffer ring that is only mapped read-only into userspace.  That way\nwe can still do zero-copy raids if the device requires stable pages\nfor checksumming or raid.  I was going to implement this as soon\nas this series lands upstream.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong addressed Pavel Begunkov's concern that io_uring/kbuf should use io_create_region(), explaining that it fails due to excessive memory allocation and instead uses io_region_allocate_pages_multi_buf() to bypass this issue, with no indication of a fix planned for the original approach.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "author explains reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When I originally implemented it, I had it use\nio_region_allocate_pages() but this fails because it's allocating way\ntoo much memory at once. For fuse's use case, each buffer is usually\nat least 1 MB if not more. Allocating the memory one buffer a time in\nio_region_allocate_pages_multi_buf() bypasses the allocation errors I\nwas seeing. That's the main reason I don't think this can just use\nio_create_region().",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarified her understanding of Pavel's feedback, confirming that she and Christoph thought the user should own buffer allocation, but now believes a different kernel-allocated approach is being proposed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question",
                "request for confirmation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oh okay, from your first message I (and I think christoph too) thought\nwhat you were saying is that the user should be responsible for\nallocating the buffers with complete ownership over them, and then\njust pass those allocated to the kernel to use. But what you're saying\nis that just use a different way for getting the kernel to allocate\nthe buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am\nI reading this correctly?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed Pavel Begunkov's concern that combining kernel-managed buffer rings (kmbufs) and regular pbufs into a single API would make the pbuf API overly complex. The author explained that having separate APIs for pbufs, kmbufs, and regular pbufs clarifies their different expectations and behaviors.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged feedback",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "imo, it looked cleaner as a separate api because it has different\nexpectations and behaviors and squashing kmbuf into the pbuf api makes\nthe pbuf api needlessly more complex. Though I guess from the\nuserspace pov, liburing could have a wrapper that takes care of\nsetting up the pbuf details for kernel-managed pbufs. But in my head,\nhaving pbufs vs. kmbufs makes it clearer what each one does vs regular\npbufs vs. pbufs that are kernel-managed.\n\nEspecially with now having kmbufs go through the ioring mem region\ninterface, it makes things more confusing imo if they're combined, eg\npbufs that are kernel-managed are created empty and then populated\nfrom the kernel side by whatever subsystem is using them. Right now\nthere's only one mem region supported per ring, but in the future if\nthere's the possibility that multiple mem regions can be registered\n(eg if userspace doesn't know upfront what mem region length they'll\nneed), then we should also probably add in a region id param for the\nregistration arg, which if kmbuf rings go through the pbuf ring\nregistration api, is not possible to do.\n\nBut I'm happy to combine the interfaces and go with your suggestion.\nI'll make this change for v2 unless someone else objects.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that they previously pinned the wrong data structure (registered buffer table) instead of the intended one (pbuf ring), but it's unclear if this will be fixed in a future version.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgment of mistake",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah, you're right I misremembered and the objections / patch I\ndropped was pinning the registered buffer table, not the pbuf ring",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing Pavel Begunkov's concern about sparse buffers populated by the kernel and their registration/unregistration. The author suggests that if those buffers are automatically pinned, users would need to unregister them individually instead of using IORING_UNREGISTER_BUFFERS, which could be annoying for them. The author notes that they dropped this feature from the fuse code because it didn't seem to make a significant performance difference.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hmm, I'm not sure this idea would work for sparse buffers populated by\nthe kernel, unless those are automatically pinned too but then from\nthe user POV for unregistration they'd need to unregister buffers\nindividually instead of just calling IORING_UNREGISTER_BUFFERS but it\nmight be annoying for them to now need to know which buffers are\npinned vs not. When i benchmarked the fuse code with vs without pinned\nregistered buffers, it didn't seem to make much of a difference\nperformance-wise thankfully, so I just dropped it.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing concerns about allocating individual buffers separately by the kernel, explaining that having separate allocations would not necessarily reduce mmap calls or userspace management, and may even improve physical contiguity of the buffer region.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explaining reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "To clarify, is this in reply to why the individual buffers shouldn't\nbe allocated separately by the kernel?\nI added a comment about this above in the discussion about\nio_region_allocate_pages_multi_buf(), and if the memory allocation\nissue I was seeing is bypassable and the region can be allocated all\nat once, I'm happy to make that change. With having the allocation be\nseparate buffers though, I'm not sure I agree that there are extra\nmmaps / userspace management. All the pages across the buffers are\nvmapped together and the userspace just needs to do 1 mmap call for\nthem. On the userspace side, I don't think there's more management\nsince the mmapped address represents the range across all the buffers.\nI'm not seeing how there's wasted space either since the only\nrequirement is that the buffer size is page aligned. I think also\nthere's a higher chance of the entire buffer region being physically\ncontiguous if each buffer is allocated separately vs. all the buffers\nare allocated as 1 region. I don't feel strongly about this either way\nand I'm happy to allocate the entire region at once if that's\npossible.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author asks for clarification on reviewer's concerns about over-accounting and extra memory footprint in kernel-managed buffer rings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question",
                "request for explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Just out of curiosity, could you elaborate on the over-accounting and\nextra memory footprint? I was under the impression it would be the\nsame since the accounting gets adjusted by the total bytes allocated?\nFor the extra memory footprint, is the extra footprint from the\nmetadata to describe each buffer region, or are you referring to\nsomething else?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the API interface and kernel buffer allocation by proposing to modify the PBUF_RING API, register memory regions through IORING_REGISTER_MEM_REGION, and add APIs for subsystems to populate kernel-managed buffer rings.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "proposed changes for v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for your input on the series. To iterate / sum up, these are\nchanges for v2 I'll be making:\n- api-wise from userspace/liburing: get rid of KMBUF_RING api\ninterface and have users go through PBUF_RING api instead with a flag\nindicating the ring is kernel-managed\n- have kernel buffer allocation go through IORING_REGISTER_MEM_REGION\ninstead, which means when the pbuf ring is created and the\nkernel-managed flag is set, the ring will be empty. The memory region\nwill need to be registered before the mmap call to the ring fd.\n- add apis for subsystems to populate a kernel-managed buffer ring\nwith addresses from the registered mem region\n\nDoes this align with your understanding of the conversation as well or\nis there anything I'm missing?\n\nAnd Christoph, do these changes for v2 work for your use case as well?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer Christoph Hellwig expressed a need for kernel-controlled buffer allocation, guaranteeing user processes can only read the memory and not write to it, and requested to piggyback on the existing work.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm arguing exactly against this.  For my use case I need a setup\nwhere the kernel controls the allocation fully and guarantees user\nprocesses can only read the memory but never write to it.  I'd love\nto be able to piggy back than onto your work.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the current pow2 round up for memory allocation will waste memory, as 1MB allocations will never become 2MB huge pages, and also questioned the handling of 1GB huge pages, suggesting users could make better placement decisions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "waste",
                "question"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "pow2 round ups will waste memory. 1MB allocations will never\nbecome 2MB huge pages. And there is a separate question of\n1GB huge pages. The user can be smarter about all placement\ndecisions.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer suggests that user-provided memory should be an optional feature for pbuf rings, and recommends adding fields in the io_uring uapi to accommodate this",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "optional feature",
                "add fields"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That's an interesting case. To be clear, user provided memory is\nan optional feature for pbuf rings / regions / etc., and I think\nthe io_uring uapi should leave fields for the feature. However, I\nhave nothing against fuse refusing to bind to buffer rings it\ndoesn't like.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested adding a new registration flag to IORING_REGISTER_MEM_REGION for read-only allocations, and making bounce avoidance optional or rejecting binding fuse to unsupported setups during init.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request for clarification",
                "no clear objection"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IORING_REGISTER_MEM_REGION supports both types of allocations. It can\nhave a new registration flag for read-only, and then you either make\nthe bounce avoidance optional or reject binding fuse to unsupported\nsetups during init. Any arguments against that? I need to go over\nJoanne's reply, but I don't see any contradiction in principal with\nyour use case.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarified a concern about how kernel-managed buffer rings interact with user-space initiated kbuf ring setup, explaining that if the kernel initiates allocation, it's semantically similar to IORING_REGISTER_MEM_REGION but with earlier buffer allocation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying a concern",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"control the allocation fully\" do you mean for your use case, the\nallocation/setup isn't triggered by userspace but is initiated by the\nkernel (eg user never explicitly registers any kbuf ring, the kernel\njust uses the kbuf ring data structure internally and users can read\nthe buffer contents)? If userspace initiates the setup of the kbuf\nring, going through IORING_REGISTER_MEM_REGION would be semantically\nthe same, except the buffer allocation by the kernel now happens\nbefore the ring is created and then later populated into the ring.\nuserspace would still need to make an mmap call to the region and the\nkernel could enforce that as read-only. But if userspace doesn't\ninitiate the setup, then going through IORING_REGISTER_MEM_REGION gets\nuglier.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged a potential over-engineering in the current design, suggesting an alternative approach where a straightforward kmbuf ring goes through the pbuf interface and future interfaces for pbuf rings can be added to go through IORING_REGISTERED_MEM_REGIONS.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "overkill",
                "over-engineered"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "So i guess the flow would have to be:\na) user calls io_uring_register_region(&ring, &mem_region_reg) with\nmem_region_reg.region_uptr's size field set to the total buffer size\n(and mem_region_reg.flags read-only bit set if needed)\n     kernel allocates region\nb) user calls mmap() to get the address of the region. If read-only\nbit was set, it gets a read-only address\nc) user calls io_uring_register_buf_ring(&ring, &buf_reg, flags) with\nbuf_reg.flags |= IOU_PBUF_RING_KERNEL_MANAGED\n     kernel creates an empty kernel-managed ring. None of the buffers\nare populated\nd) user tells X subsystem to populate the ring starting from offset Z\nin the registered mem region\ne) on the kernel side, the subsystem populates the ring starting from\noffset Z, filling it up using the buf_size and ring_entries values\nthat the user registered the ring with in c)\n\nTo be completely honest, the more I look at this the more this feels\nlike overkill / over-engineered to me. I get that now the user can do\nthe PMD optimization, but does that actually lead to noticeable\nperformance benefits? It seems especially confusing with them going\nthrough the same pbuf ring interface but having totally different\nexpectations.\n\nWhat about adding a straightforward kmbuf ring that goes through the\npbuf interface (eg the design in this patchset) and then in the future\nadding an interface for pbuf rings (both kernel-managed and\nnon-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if\nusers end up needing/wanting to have their rings populated that way?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer noted that if an application is concerned about TLB pressure, it can simply round up the buffer size to a multiple of PTE levels, making the proposed optimization unnecessary.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested change",
                "suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure.  But if the application cares that much about TLB pressure\nI'd just round up to nice multtiple of PTE levels.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig questioned the meaning of 'pbuf' in the patch description, expressing confusion about how it relates to io_uring_register_buffers*, which always takes user-provided buffers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "lack of clarity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Can you clarify what you mean with 'pbuf'?  The only fixed buffer API I\nknow is io_uring_register_buffers* which always takes user provided\nbuffers, so I have a hard time parsing what you're saying there.  But\nthat might just be sign that I'm no expert in io_uring APIs, and that\nweb searches have degraded to the point of not being very useful\nanymore.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "reviewer expressed confusion over the purpose of IORING_REGISTER_MEM_REGION, as it appears to be related to cqs (completion queues) but its name and documentation do not clearly convey this",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "lack of clarity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IORING_REGISTER_MEM_REGION seems to be all about cqs from both your\ncommit message and the public documentation.  I'm confused.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "reviewer noted that the patch does not address their specific use case of block and file system I/O",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "My use case is not about fuse, but good old block and file system\nI/O.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that io_uring_register_buffers() currently only pins memory, allowing applications or other processes to modify it, which can cause issues for file systems and storage devices that need to verify checksums or rebuild data from parity. He suggested implementing a version of io_uring_register_buffers where the kernel provides buffers and maps them into the application's address space read-only.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The idea is that the application tells the kernel that it wants to use\na fixed buffer pool for reads.  Right now the application does this\nusing io_uring_register_buffers().  The problem with that is that\nio_uring_register_buffers ends up just doing a pin of the memory,\nbut the application or, in case of shared memory, someone else could\nstill modify the memory.  If the underlying file system or storage\ndevice needs verify checksums, or worse rebuild data from parity\n(or uncompress), it needs to ensure that the memory it is operating\non can't be modified by someone else.\n\nSo I've been thinking of a version of io_uring_register_buffers where\nthe buffers are not provided by the application, but instead by the\nkernel and mapped into the application address space read-only for\na while, and I thought I could implement this on top of your series,\nbut I have to admit I haven't really looked into the details all\nthat much.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "reviewer noted that the PMD mapping is less relevant than previously thought, and mentioned that both AMD and ARM architectures have optimizations for contiguous PTEs",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "neutral comment",
                "technical discussion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes.  The PMD mapping also is not that relevant.  Both AMD (implicit)\nand ARM (explicit) have optimizations for contiguous PTEs that are\nalmost as valuable.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer questioned the necessity of introducing kernel-managed buffer rings for io_uring, suggesting that fuse can contain all its needs within itself and reusing pbuf ring code as an internal memory allocator. They also pointed out inflexibilities in the current design, such as requiring a new io_uring uapi and binding buffer memory to ring entries.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "infelicities",
                "inflexibilities"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Registered, aka fixed, buffers are the ones you pass to\nIORING_OP_[READ,WRITE]_FIXED and some other requests. It's normally\ncreated by io_uring_register_buffers*() / IORING_REGISTER_BUFFERS*\nwith user memory, but there are special cases when it's installed\ninternally by other kernel components, e.g. ublk.\nThis series has nothing to do with them, and relevant parts of\nthe discussion here don't mention them either.\n\nProvided buffer rings, a.k.a pbuf rings, IORING_REGISTER_PBUF_RING\nis a kernel-user shared ring. The entries are user buffers\n{uaddr, size}. The user space adds entries, the kernel (io_uring\nrequests) consumes them and issues I/O using the user addresses.\nE.g. you can issue a IORING_OP_RECV request (+IOSQE_BUFFER_SELECT)\nand it'll grab a buffer from the ring instead of using sqe->addr.\n\npbuf rings, IORING_REGISTER_MEM_REGION, completion/submission\nqueues and all other kernel-user rings/etc. are internally based\non so called regions. All of them support both user allocated\nmemory and kernel allocations + mmap.\n\nThis series essentially creates provided buffer rings, where\n1. the ring now contains kernel addresses\n2. the ring itself is in-kernel only and not shared with user space\n3. it also allocates kernel buffers (as a region), populates the ring\n    with them, and allows mapping the buffers into the user space.\n\nFuse is doing both adding (kernel) buffers to the ring and consuming\nthem. At which point it's not clear:\n\n1. Why it even needs io_uring provided buffer rings, it can be all\n    contained in fuse. Maybe it's trying to reuse pbuf ring code as\n    basically an internal memory allocator, but then why expose buffer\n    rings as an io_uring uapi instead of keeping it internally.\n\n    That's also why I mentioned whether those buffers are supposed to\n    be used with other types of io_uring requests like recv, etc.\n\n2. Why making io_uring to allocate payload memory. The answer to which\n    is probably to reuse the region api with mmap and so on. And why\n    payload buffers are inseparably created together with the ring\n    and via a new io_uring uapi.\n\n    And yes, I believe in the current form it's inflexible, it requires\n    a new io_uring uapi. It requires the number of buffers to match\n    the number of ring entries, which are related but not the same\n    thing. You can't easily add more memory as it's bound to the ring\n    object. The buffer memory won't even have same lifetime as the\n    ring object -- allow using that km buffer ring with recv requests\n    and highly likely I'll most likely give you a way to crash the\n    kernel.\n\nBut hey, I'm tired. I don't have any beef here and am only trying\nto make it a bit cleaner and flexible for fuse in the first place\nwithout even questioning the I/O path. If everyone believes\neverything is right, just ask Jens to merge it.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer expressed concern that the current use of kernel-managed buffer rings is not suitable for huge payload buffers and suggested respinning patches to place SQ/CQ onto a different area of memory.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "concern about suitability"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Think of it as an area of memory for kernel-user communication. Used\nfor syscall parameters passing to avoid copy_from_user, but I added\nit for a bunch of use cases. We'll hopefully get support at some\npoint for passing request arguments like struct iovec. BPF patches\nuse it for communication. I need to respin patches placing SQ/CQ onto\nit (avoid some memory waste).\n\nTbh, I never meant it nor io_uring regions to be used for huge\npayload buffers, but this series already uses regions for that.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed confusion about how the kernel-managed buffer rings can work without a kernel component returning buffers into the ring, pointing out that io_uring does not currently support this.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "confusion",
                "lack of clear understanding"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Then I'm confused. Take a look at the other reply, this series is\nabout buffer rings with kernel memory, it can't work without a kernel\ncomponent returning buffers into the ring, and io_uring doesn't do\nthat. But maybe you're thinking about adding some more elaborate API.\n\nIIUC, Joanne also wants to add support for fuse installing registered\nbuffers, which would allow zero-copy, but those got split out of\nthis series.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the patch introduces a potential issue where the kernel-managed buffer ring's metadata is not properly handled, and suggested working around it by wrapping the affected code in a loop.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential issue",
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Let's fix that then. For now, just work it around by wrapping\ninto a loop.\n\nBtw, I thought you're going to use it for metadata like some\nfuse headers and payloads would be zero copied by installing\nit as registered buffers.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested moving ring population into fuse, disentangling memory allocation from ring creation in the io_uring uapi, and using IORING_REGISTER_MEM_REGION to achieve this without extra uapi",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The main point is disentangling memory allocation from ring\ncreation in the io_uring uapi, and moving ring population\ninto fuse instead of doing it at creation. And it'll still be\npopulated by the kernel (fuse), user space doesn't have access\nto the ring. IORING_REGISTER_MEM_REGION is just the easiest way\nto achieve that without any extra uapi.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the differences between io_uring and kbuf are mainly due to special region path and embedded buffer allocations, but is open to making a separate opcode if the submitter prefers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "open-minded",
                "non-confrontational"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It appeared to me that they're different because of special\nregion path and embedded buffer allocations, and otherwise\ndifferences would be minimal. But if you think it's still\nbetter to be made as a separate opcode, I'm not opposing it,\ngo for it.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that the current implementation does not provide a clear control path for looking up buffer rings from io_uring, and suggested adding a new command to handle this case. They also proposed passing necessary parameters through this command, including region ID and buffer ring ID.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Not having patches using the functionality is inconvenient. How\nfuse looks up the buffer ring from io_uring? I could imagine you\nhave some control path io-uring command:\n\ncase FUSE_CMD_BIND_BUFFER_RING:\n\treturn bind_queue(params);\n\nThen you can pass all necessary parameters to it, pseudo code:\n\nstruct fuse_bind_kmbuf_ring_params {\n\tregion_id;\n\tbuf_ring_id;\n\t...\n};\n\nbind_queue(cmd, struct fuse_bind_kmbuf_ring_params *p)\n{\n\tregion = io_uring_get_region(cmd, p->region_id);\n\t// get exclusive access:\n\tbuf_ring = io_uring_get_buf_ring(cmd, p->buf_ring_id);\n\n\tif (!validate_buf_ring(buf_ring))\n\t\treturn NOTSUPPORTED;\n\n\tio_uring_pin(buf_ring);\n\tfuse_populate_buf_ring(buf_ring, region, ...);\n}\n\nDoes that match expectations? I don't think you even need\nthe ring part exposed as an io_uring uapi, tbh, as it\nstays completely in fuse and doesn't meaningfully interact\nwith the rest of io_uring.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov pointed out that the patch could use IORING_REGISTER_MEM_REGION instead of a separate registration for buffer regions, and noted that this is a separate issue from binding buffers to the ring.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That was about an argument for using IORING_REGISTER_MEM_REGION\ninstead a separate region. And it's separate from whether\nbuffers should be bound to the ring.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that when allocating huge pages for non-pow2 buffer sizes, the kernel may allocate a larger huge page than needed, potentially wasting memory.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential memory waste"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I shouldn't affect you much since you have such large buffers,\nbut imagine the total allocation size is not being pow2, and\nthe kernel allocating it as a single folio. E.g. 3 buffers,\n0.5 MB each, total = 1.5MB, and the kernel allocates a 2MB\nhuge page.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the patch series does not address registered buffers and suggested separating kernel-managed buffer rings from io_uring, arguing that reusing buffer allocation would introduce unnecessary complexity.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "separation of concerns"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There is nothing about registered buffers in this series. And even\nif you try to reuse buffer allocation out of it, it'll come with\na circular buffer you'll have no need for. And I'm pretty much\narguing about separating those for io_uring.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested reusing internal regions for allocations and mmap() operations, wrapping them in a registered buffer, and making vmap'ing optional as it's not needed.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "suggested improvement",
                "optional vmap"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, the easiest solution is to internally reuse regions for\nallocations and mmap()'ing and wrap it into a registered buffer.\nIt just need to make vmap'ing optional as it won't be needed.\n\n-- \nPavel Begunkov",
              "reply_to": "",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that the io_uring uapi should not be tied to fuse-specific requirements, such as uniform buffer sizes, ring size matching number of buffers, and kernel-allocated buffers. He questioned why these constraints are necessary and suggested considering alternative use cases where memory is allocated elsewhere.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "strong opinion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, it's mainly about not keeping payload buffers and rings in the same\nobject from the io_uring uapi perspective.\n\n1. If it's an io_uring uapi, it shouldn't be fuse specific or with\na bunch of use case specific expectations attached. Why does it\nrequire all buffers to be uniform in size? Why does it require\nthe ring size to match the number of buffers? Why does it require\nbuffers to be allocated by io_uring in the first place? Maybe some\nsubsystem got memory from somewhere else and wants to do use it\nwith io_uring. Why does it need to know the total size at creation,\nand what would you do if you want to add more memory at runtime\nwhile using the same ring?\n\n2. If it's meant to be fuse specific and _not_ used with other requests\nlike recv/read/etc., then what's the point of having it as an io_uring\nuapi? Which also adds additional trouble like the once you're solving\nwith pinning.\n\nIf it's supposed to be used with other requests, then buffers and\nrings will have different in-kernel lifetime expectations imposed\nby io_uring, so having them together won't even help with\nmanagement.\n\nI have a strong opinion about the memmap.c change. For the\nrest, if you believe it's fine, just send it out and let Jens\ndecide.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed confusion about the distinction between kernel-managed buffer rings and user-visible buffer rings, questioning what expectations are different apart from one being in-kernel with kernel addresses and the other user visible with user addresses.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "questioning"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It's predicated on separating buffers from rings, see above,\nand assuming that I'm not sure what expectations are different\napart from one being in-kernel with kernel addresses and the\nother user visible with user addresses.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern about the efficiency of buffer allocation by explaining that a circular buffer will allow for shared buffers across entries, reducing memory waste and allocation needs.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think the circular buffer will be useful for Christoph's use case in\nthe same way it'll be useful for fuse's. The read payload could be\ndifferently sized across requests, so it's a lot of wasted space to\nhave to allocate a buffer large enough to support the max-size request\nper entry in the io_ring. With using a circular buffer, buffers have a\nway to be shared across entries, which means we can significantly\nreduce how much memory needs to be allocated.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that Christoph Hellwig's use case for read-only buffers aligns with the benefits of kernel-managed buffer rings, and sees no need to modify the patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a similar use case",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "(resending because I hit reply instead of reply-all)\n\nI think we have the exact same use case, except your buffers need to\nbe read-only. I think your use case benefits from the same memory wins\nwe'll get with incremental buffer consumption, which is the primary\nreason fuse is using a bufring instead of fixed buffers.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Christoph Hellwig's concern about making mmap calls on kernel-managed buffer rings read-only, explained how to achieve this by passing a read-only flag from userspace and checking it in the kernel, and offered to add a patch to the series if needed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "offer_to_add_patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think you can and it'll be very easy to do so. All that would be\nneeded is to pass in a read-only flag from the userspace side when it\nregisters the bufring, and then when userspace makes the mmap call to\nthe bufring, the kernel checks if that read-only flag is set on the\nbufring and if so returns a read-only mapping. I'm happy to add that\npatch to this series if that would make things easier for you. The\nio_uring_register_buffers() api registers fixed buffers (which have to\nbe user-allocated memory) so you would need to go through the\nio_uring_register_buf_ring() api once kmbufs are squashed into the\npbuf interface.\n\nWith going through IORING_MEM_REGION, this would work for your use\ncase as well. The user would have to register the mem region with\nio_uring_register_region() and pass in a read-only flag, and then the\nkernel will allocate the memory region. Then userspace would mmap the\nmemory region and on the kernel side, it would set the mapping to be\nread-only. When the kmbufring then gets registered, the buffers in it\nwill be empty. The filesystem will then have to populate the buffers\nin it from the mem region that was previously registered.\n\nThanks,\nJoanne",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Bernd Schubert",
              "summary": "Reviewer Bernd Schubert expressed skepticism about the kernel-managed buffer rings feature, questioning its purpose and suggesting that sharing buffers across entries would only reduce ring size without providing any benefits.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Dunno, what we actually want is requests of multiple sizes. Sharing\nbuffers across entries sounds like just reducing the ring size - I\npersonally don't see the point here.\n\n\nThanks,\nBernd",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarified that kernel-managed buffer rings allow concurrent access to different regions of a shared buffer across multiple io_uring entries, addressing concerns about buffer sharing.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"sharing buffers across entries\" what I mean is different regions\nof the buffer can now be used concurrently by multiple entries.\n\nThanks,\nJoanne",
              "reply_to": "Bernd Schubert",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed Pavel's concern about fuse needing kernel-managed buffer rings to control when buffers get recycled back into the ring, explaining that this is necessary for passing data between the kernel and the server in fuse's use case.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a specific requirement",
                "explained technical reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The most important part and the whole reason fuse needs the buffer\nring to be kernel-managed is because the kernel needs to control when\nbuffers get recycled back into the ring. For fuse's use case, the\nbuffer is used for passing data between the kernel and the server. We\ncan't have the server recycle the buffer because the server writes\nback data to the kernel in that buffer when it submits the sqe. After\nfuse receives the sqe and reads the reply from the server, it then\nneeds to recycle that buffer back into the ring so it can be reused\nfor a future cqe (eg sending a future request).",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel's concern about the use case of kernel-managed buffer rings, explaining that they are used for other io-uring operations in addition to reading and writing from/to a locally-backed file.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On the userspace/server side, it uses the buffers for other io-uring\noperations (eg reading or writing the contents from/to a\nlocally-backed file).",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed Pavel's concern about adding non-trivial complexity by using a registered memory region, explaining that it allows optimizations but may not be beneficial for most kmbuf use cases. The author feels that offering both simple and advanced kernel-managed pbufs is the best approach.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "lack of understanding",
                "non-trivial complexity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "My main motivation for this is simplicity. I see (and thanks for\nexplaining) that using a registered mem region allows the use of some\noptimizations (the only one I know of right now is the PMD one you\nmentioned but maybe there's more I'm missing) that could be useful for\nsome workloads, but I don't think (and this could just be my lack of\nunderstanding of what more optimizations there are) most use cases of\nkmbufs benefit from those optimizations, so to me it feels like we're\nadding non-trivial complexity for no noticeable benefit.\n\nI feel like we get the best of both worlds by letting users have both:\nthe simple kernel-managed pbuf where the kernel allocates the buffers\nand the buffers are tied to the lifecycle of the ring, and the more\nadvanced kernel-managed pbuf where buffers are tied to a registered\nmemory region that the subsystem is responsible for later populating\nthe ring with.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the user interface and behavior of kernel-managed buffer rings (kmbufs) versus process buffer rings (pbufs), agreeing that separating them into different types of ring buffers is cleaner, but also willing to revisit this decision in v2.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a valid concern",
                "willingness to reconsider"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "imo it felt cleaner to have a new uapi for it because kmbufs and pbufs\nhave different expectations and behaviors (eg pbufs only work with\nuser-provided buffers and requires userspace to populate the ring\nbefore using it, whereas for kmbufs the kernel allocates the buffers\nand populates it for you; pbufs require userspace to recycle back the\nbuffer, whereas for kmbufs the kernel is the one in control of\nrecycling) and from the user pov it seemed confusing to have kmbufs as\npart of the pbuf ring uapi, instead of separating it out as a\ndifferent type of ringbuffer with a different expectation and\nbehavior. I was trying to make the point that combining the interface\nif we go with IORING_MEM_REGION gets even more confusing because now\npbufs that are kernel-managed are also empty at initialization and\nonly can point to areas inside a registered mem region and the\nresponsibility of populating it is now on whatever subsystem is using\nit.\n\nI still have this opinion but I also think in general, you likely know\nbetter than I do what kind of io-uring uapi is best for io-uring's\nusers. For v2 I'll have kmbufs go through the pbuf uapi.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong is addressing a concern about the purpose and usage of ring entries without associated buffers, explaining that it can be easily fixed by passing in the number of buffers from the uapi for kernel-managed pbuf rings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm not really seeing what the purpose of having a ring entry with no\nbuffer associated with it is. In the existing code for non-kernel\nmanaged pbuf rings, there's the same tie between reg->ring_entries\nbeing used as the marker for how many buffers the ring supports. But\nif the number of buffers should be different than the number of ring\nentries, this can be easily fixed by passing in the number of buffers\nfrom the uapi for kernel-managed pbuf rings.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that kernel-managed buffer rings require upfront allocation of memory for the lifetime of the ring, and explained that this may be challenging to determine in certain scenarios.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a challenge",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "To play devil's advocate, we also can't easily add more memory to the\nmem region once it's been registered. I think there's also a worse\npenalty where the user needs to know upfront how much memory to\nallocate for the mem region for the lifetime of the ring, which imo\nmay be hard to do (eg if a kernel-managed buf ring only needs to be\nregistered for some code paths and not others, the mem region\nregistration would still have to allocate the memory a potential kbuf\nring would use).",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern that the buffer memory and ring object have different lifetimes by explaining that the buffers are freed when the ring is freed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm a bit confused by this part. The buffer memory does have the same\nlifetime as the ring object, no? The buffers only get freed when the\nring itself is freed.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author is addressing Pavel Begunkov's feedback on whether kernel-managed buffer rings should support both a simple interface and a more complex one that goes through registered memory regions, and she is open to making the change if necessary.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "openness to revision",
                "acknowledgment of reviewer's concerns"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I appreciate you looking at this and giving your feedback and insight.\nThank you for doing so. I don't want to merge in something you're\nunhappy with.\n\nAre you open to having support for both a simple kernel-managed pbuf\ninterface and later on if/when the need arises, a kernel-managed pbuf\ninterface that goes through a registered memory region? If the answer\nis no, then I'll make the change to have kmbufs go through the\nregistered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that buffer rings are not suitable for storage read/write operations because they immediately bind to a buffer, whereas other types of requests like recv allow io_uring to first poll the socket and then take a buffer from the ring. This leads to two issues: the inability to specify where data lands in storage rw requests, and the lack of a mechanism to return buffers back into the kernel private ring.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Provided buffer rings are not useful for storage read/write requests\nbecause they bind to a buffer right away, that's in contrast to some\nrecv request, where io_uring will first poll the socket to confirm\nthe data is there, and only then take a buffer from the buffer ring\nand copy into it. With storage rw it makes more sense to specify\nthe buffer directly gain control over where exactly data lands\nIOW, instead of the usual \"read data into a given pointer\" request\nsemantics like what read(2) gives you, buffer rings are rather\n\"read data somewhere and return a pointer to where you placed it\".\n\nAnother problem is that someone needs to return buffers back into\nthe buffer ring, and it's a kernel private ring. For this patchset\nit's assumed the fuse driver is going to be doing that, but there\nis no one for normal rw requests.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested replacing the current kernel-managed buffer ring implementation with a simpler approach based on IORING_MEM_REGION, which provides buffers/memory without extra semantics.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes. You only need buffers, and it'll be better to base on sth that\ngives you buffers/memory without extra semantics, i.e.\nIORING_MEM_REGION. Or it can be a standalone registered buffer\nextension, likely reusing regions internally. That might even yield\na finer API.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer asked whether kernel-managed buffer rings can be used with other requests, specifically IORING_OP_RECV with IOSQE_BUFFER_SELECT",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification_request"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Oops, typo. I was asking whether the buffer rings (not buffers) are\nsupposed to be used with other requests. E.g. submitting a\nIORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying\nyour kernel-managed buffer ring.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer has two main concerns: first, that buffers should not be inseparable from buffer rings in the io_uring user API; second, that there should be an option to allow user memory for buffer creation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no objection",
                "trivial implementation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There are two separate arguments. The first is about not making buffers\ninseparable from buffer rings in the io_uring user API. Whether it's\nIORING_REGISTER_MEM_REGION or something else is not that important.\nI have no objection if it's a part of fuse instead though, e.g. if\nfuse binds two objects together when you register it with fuse, or even\nif fuse create a buffer ring internally (assuming it doesn't indirectly\nleak into io_uring uapi).\n\nAnd the second was about optionally allowing user memory for buffer\ncreation as you're reusing the region abstraction. You can find pros\nand cons for both modes, and funnily enough, SQ/CQ were first kernel\nallocated and then people asked for backing it by user memory, and IIRC\nit was in the reverse order for pbuf rings.\n\nImplementing this is trivial as well, you just need to pass an argument\nwhile creating a region. All new region users use struct\nio_uring_region_desc for uapi and forward it to io_create_region()\nwithout caring if it's user or kernel allocated memory.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov questioned the necessity of tying kernel-managed buffer rings to io_uring, suggesting it might be simpler to implement in fuse or as an implementation detail within io_uring",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The stress is on why it's an _io_uring_ API. It doesn't matter to me\nwhether it's a separate opcode or not. Currently, buffer rings don't give\nyou anything that can't be pure fuse, and it might be simpler to have\nit implemented in fuse than binding to some io_uring object. Or it could\ncreate buffer rings internally to reuse code but it doesn't become an\nio_uring uapi but rather implementation detail. And that predicates on\nwhether km rings are intended to be used with other / non-fuse requests.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed concerns that the kernel-managed buffer ring feature is not reusable for all io_uring users and suggested a middle ground where km rings can be registered together with memory, but without a notion of a buffer.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I believe the source of disagreement is that you're thinking\nabout how it's going to look like for fuse specifically, and I\nbelieve you that it'll be nicer for the fuse use case. However,\non the other hand it's an io_uring uapi, and if it is an io_uring\nuapi, we need reusable blocks that are not specific to particular\nusers.\n\nIf it km rings has to stay an io_uring uapi, I guess a middle\nground would be to allow registering km rings together with memory,\nbut make it a pure region without a notion of a buffer, and let\nfuse to chunk it. Later, we can make payload memory allocation\noptional.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov pointed out that the patch introduces a fuse-specific API, which is masquerading as a generic io_uring API, and noted that various parts of the code make assumptions based on this incorrect assumption.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "assumptions in different places"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right, intentionally so, because otherwise it's a fuse uapi that\npretends to be a generic io_uring uapi but it's not because of\nall assumptions in different places.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that the current implementation of kernel-managed buffer rings only specifies the ring depth but not the allocated memory, which could lead to inconsistencies between the expected and actual memory usage.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "inconsistency",
                "potential issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Not really, it tells the buffer ring depth but says nothing about\nhow much memory user space allocated and how it's pushed. It's a\nreasonable default but they could be different. For example, if you\nexpect adding more memory at runtime, you might create the buffer\nring a bit larger. Or when server processing takes a while and you\ncan't recycle until it finishes, you might have more buffers than\nyou need ring entries. Or you might might decide to split buffers\nand as you mentioned incremental consumption, which is an entire\nseparate topic because it doesn't do de-fragmentation and you'd\nneed to have it in fuse, just like user space does with pbufs.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested that instead of passing the number of buffers to io_uring, the kernel should create a large chunk of memory and let fuse manage it by splitting it into smaller chunks",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "My entire point is that we're making lots of assumptions for io_uring\nuapi, and if it's moved to fuse because it knows better what it\nneeds, it should be a win.\n\nIOW, it sounds better if instead of passing the number of buffers to\nio_uring, you just ask it to create a large chunk of memory, and then\nfuse chunks it up and puts into the ring.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov agreed that additional memory is needed but suggested it does not necessarily have to be added through IORING_REGISTER_MEM_REGION specifically.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "agreed",
                "suggested"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I agree, and you'd need something new in either case to add more\nmemory, and it doesn't need to be IORING_REGISTER_MEM_REGION\nspecifically.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that unregistering a kernel-managed buffer ring does not guarantee the absence of in-flight requests using buffers from the ring, and suggested synchronizing with all other io_uring requests to address this issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "synchronization",
                "inflight requests"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Unregistering a buffer ring doesn't guarantee that there are no\ninflight requests that are still using buffers that came out of\nthe buffer ring. The fuse driver can wait/terminate its requests\nbefore unregisteration, but allow userspace issued IORING_OP_RECV\nto use this km buffer ring, and you'll need to somehow synchronise\nwith all other io_uring requests.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that a fix is needed and promised to modify v2 in response to reviewer feedback.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for modification",
                "promised to revise"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sorry, I submitted v2 last night thinking the conversation on this\nthread had died. After reading through your reply, I'll modify v2.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that kernel-managed buffer rings are intended for use with other io-uring requests, confirming their purpose aligns with reviewer's expectations.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "aligned_with_expectations"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes the buffer rings are intended to be used with other io-uring\nrequests. The ideal scenario is that the user can then do the\nequivalent of IORING_OP_READ/WRITE_FIXED operations on the\nkernel-managed buffers and avoid the per-i/o page pinning overhead\ncosts.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong addressed Pavel Begunkov's concern about the design of kernel-managed buffer rings, agreeing that having buffers owned by the ring and tied to its lifetime is more generically useful. She proposed a new design where the user creates a memory region for the io-uring, maps it, and passes an offset into the region to the subsystem, which then creates a locally managed buffer ring and adds buffers from the mem region. This design matches her original intention but requires further clarification on whether userspace can unregister the memory region while it's being used.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "further_revision_needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree 100%. The api we add should be what's best for io-uring, not fuse.\n\nFor the majority of use cases, it seemed to me that having the buffers\nseparated from the buffer rings didn't yield perceptible benefits but\nadded complexity and more restrictions like having to statically know\nup front how big the mem region needs to be across the lifetime of the\nio-uring for anything the io-uring might use the mem region for. It\nseems more generically useful as a concept to have the buffers owned\nby the ring and tied to the lifetime of the ring. I like how with this\ndesign everything is self-contained and multiple subsystems can use it\nwithout having to reimplement functionality locally in the subsystem.\nOn the other hand, I see your point about how it might be something\nusers want in the future if they want complete control over which\nparts of the mem region get used as the backing buffers to do stuff\nlike PMD optimizations.\n\nI think this is a matter of opinion/preference and I think in general\nfor anything io-uring related, yours should take precedence.\n\nWith it going through a mem region, I don't think it should even go\nthrough the \"pbuf ring\" interface then if it's not going to specify\nthe number of entries and buffer sizes upfront, if support is added\nfor io-uring normal requests (eg IORING_OP_READ/WRITE) to use the\nbacking pages from a memory region and if we're able to guarantee that\nthe registered memory region will never be able to be unregistered by\nthe user. I think if we repurpose the\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n};\n\nfields in the struct io_uring_sqe to\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n  __u64 offset; /* offset into registered mem region */\n};\n\nand add some IOSQE_ flag to indicate it should find the pages from the\nregistered mem region, then that should work for normal requests.\nWhere on the kernel side, it looks up the associated pages stored in\nthe io_mapped_region's pages array for the offset passed in.\n\nRight now there's only a uapi to register a memory region and none to\nunregister one. Is it guaranteed that io-uring will never add\nsomething in the future that will let userspace unregister the memory\nregion or at least unregister it while it's being used (eg if we add\nfuture refcounting to it to track active uses of it)?\n\nIf so, then end-to-end, with it going through the mem region, it would\nbe something like:\n* user creates a mem region for the io-uring\n* user mmaps the mem region\n* user passes in offset into region, length of each buffer, and number\nof entries in the ring to the subsystem\n* subsystem creates a locally managed bufring and adds buffers to that\nring from the mem region\n* on the cqe side, it sends the buffer id of the registered mem region\nthrough the same \"IORING_CQE_F_BUFFER |  (buf_id <<\nIORING_CQE_BUFFER_SHIFT)\" mechanism\n\nDoes this design match what you had in mind / prefer?\n\nI think the above works for Christoph's use case too (as his and my\nuse case are the same) but if not, please let me know.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov questioned whether a server or user space program can issue I/O requests that consume buffers/entries from kernel-managed buffer rings without involving fuse kernel code, and asked for clarification on the expected use case.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "clarification needed",
                "use case unclear"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You mention OP_READ_FIXED and below agreed not exposing km rings\nan io_uring uapi, which makes me believe we're still talking about\ndifferent things.\n\nCorrect me if I'm wrong. Currently, only fuse cmds use the buffer\nring itself, I'm not talking about buffer, i.e. fuse cmds consume\nentries from the ring (!!! that's the part I'm interested in), then\nprocess them and tell the server \"this offset in the region has user\ndata to process or should be populated with data\".\n\nNaturally, the server should be able to use the buffers to issue\nsome I/O and process it in other ways, whether it's a normal\nOP_READ to which you pass the user space address (you can since\nit's mmap()'ed by the server) or something else is important but\na separate question than the one I'm trying to understand.\n\nSo I'm asking whether you expect that a server or other user space\nprogram should be able to issue a READ_OP_RECV, READ_OP_READ or any\nother similar request, which would consume buffers/entries from the\nkm ring without any fuse kernel code involved? Do you have some\nuse case for that in mind?\n\nUnderstanding that is the key in deciding whether km rings should\nbe exposed as io_uring uapi or not, regardless of where buffers\nto populate the ring come from.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov suggested reusing registered buffers instead of introducing a new mechanism for kernel-managed buffer rings, citing efficiency and similarity to zero-copy internally registered buffers as benefits.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested reuse of existing mechanism"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "So you already can do all that using the mmap()'ed region user\npointer, and you just want it to be more efficient, right?\nFor that let's just reuse registered buffers, we don't need a\nnew mechanism that needs to be propagated to all request types.\nAnd registered buffer are already optimised for I/O in a bunch\nof ways. And as a bonus, it'll be similar to the zero-copy\ninternally registered buffers if you still plan to add them.\n\nThe simplest way to do that is to create a registered buffer out\nof the mmap'ed region pointer. Pseudo code:\n\n// mmap'ed if it's kernel allocated.\n{region_ptr, region_size} = create_region();\n\nstruct iovec iov;\niov.iov_base = region_ptr;\niov.iov_len = region_size;\nio_uring_register_buffers(ring, &iov, 1);\n\n// later instead of this:\nptr = region_ptr + off;\nio_uring_prep_read(sqe, fd, ptr, ...);\n\n// you use registered buffers as usual:\nio_uring_prep_read_fixed(sqe, fd, off, regbuf_idx, ...);\n\n\nIIRC the registration would fail because it doesn't allow file\nbacked pages, but it should be fine if we know it's io_uring\nregion memory, so that would need to be patched.\n\nThere might be a bunch of other ways you can do that like\ncreate a kernel allocated registered buffer like what Cristoph\nwants, and then register it as a region. Or allow creating\nregistered buffers out of a region. etc.\n\nI wanted to unify registered buffers and regions internally\nat some point, but then drifted away from active io_uring core\ninfrastructure development, so I guess that could've been useful.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that kernel-managed buffer rings would require handling page references and/or pinning regions, which could be complex and unnecessary if registered buffers are used instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Let's talk about it when it's needed or something changes, but if\nyou do registered buffers instead as per above, they'll be holding\npage references and or have to pin the region in some other way.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested adding a liburing helper to handle mmap'ing for the fuse server, eliminating its need to directly manage memory mapping.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, we should just add a liburing helper, so that fuse server\ndoesn't need to deal with mmap'ing.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer expressed conditional approval, requesting confirmation that the patch allows for all desired fast path optimizations.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "conditional approval",
                "request for confirmation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That's sounds clean to me _if_ it allows you to achieve all\n(fast path) optimisations you want to have. I hope it does?\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing Pavel Begunkov's question about whether kernel-managed buffer rings are fuse-specific, and responds that while they may be useful for fuse servers with certain characteristics, the concept is not exclusive to fuse and could benefit other subsystems/users.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a use case",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for clarifying your question. Yes, this would be a useful\noptimization in the future for fuse servers with certain workload\ncharacteristics (eg network-backed servers with high concurrency and\nunpredictable latencies). I don't think the concept of kmbufrings is\nexclusively fuse-specific though (for example, Christoph's use case\nbeing a recent instance); I think other subsystems/users that'll use\nkmbuf rings would also generically find it useful to have the option\nof READ_OP_RECV/READ_OP_READ operating directly on the ring.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing concerns about the added complexity and convoluted interface introduced by tying kernel-managed buffer rings to existing concepts, specifically buffer IDs. They express confusion about the benefits of this design and suggest that memory regions should be decoupled from other structures, implying that native support for using memory regions in io-uring requests would simplify things.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I feel like this design makes the interface more convoluted and now\nmuddies different concepts together by adding new complexity /\nrelationships between them whereas they were otherwise cleanly\nisolated. Maybe I'm just not seeing/understanding the overarching\nvision for why conceptually it makes sense for them to be tied\ntogether besides as a mechanism to tell io-uring requests where to\ncopy from by reusing what exists for fixed buffer ids. There's more\ncomplexity now on the kernel side (eg having to detect if the buffer\npassed in is kernel-allocated to know whether to pin the pages /\ncharge it against the user's RLIMIT_MEMLOCK limit) but I'm not\nunderstanding what we gain from it. I got the sense from your previous\ncomments that memory regions are the de facto way to go and should be\ndecoupled from other structures, so if that's the case, why doesn't it\nmake sense for io-uring to add native support for using memory regions\nfor io-uring requests? I feel like from the userspace side it makes\nthings more confusing with this extra layer of indirection that now\nhas to go through a fixed buffer.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern that the kernel cannot guarantee the caller will register the memory region as a fixed buffer, explaining that this would introduce extra overhead for every I/O operation and proposing two possible solutions: adding pinning to registered memory regions or implementing additional locking mechanisms.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a problem",
                "proposed potential fixes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't think we can guarantee that the caller will register the\nmemory region as a fixed buffer (eg if it doesn't need/want to use the\nbuffer for normal io-uring requests). On the kernel side, the internal\nbuffer entry uses the kaddr of the registered memory region buffer for\nany memcpys. If it's not guaranteed that registered memory regions\npersist for the lifetime of the ring, there'll have to be extra\noverhead for every I/O (eg grab the io-uring lock, checking if the mem\nregion is still registered, grab a refcount to that mem region, unlock\nthe ring, do the memcpy to the kaddr, then grab the io-uring lock\nagain, decrement the refcount, and unlock). Or I guess we could add\npinning to a registered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Johannes Weiner",
      "primary_email": "hannes@cmpxchg.org",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joshua Hahn",
      "primary_email": "joshua.hahnjy@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "JP Kobryn",
      "primary_email": "inwardvessel@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3] mm: move pgscan, pgsteal, pgrefill to node stats",
          "message_id": "20260218222652.108411-1-jp.kobryn@linux.dev",
          "url": "https://lore.kernel.org/all/20260218222652.108411-1-jp.kobryn@linux.dev/",
          "date": "2026-02-18T22:27:32Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch moves the pgscan, pgsteal, and pgrefill counters from vm_event_item to node_stat_item, allowing for per-node reclaim visibility in /proc/zoneinfo. The counters are still aggregated across all nodes in /proc/vmstat but with a different ordering. Memcg accounting is preserved, and the virtio_balloon driver is updated to use global_node_page_state() to fetch the counters.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Vlastimil (SUSE)",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-19",
              "analysis_source": "heuristic"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2] mm: move pgscan and pgsteal to node stats",
          "message_id": "20260218032941.225439-1-jp.kobryn@linux.dev",
          "url": "https://lore.kernel.org/all/20260218032941.225439-1-jp.kobryn@linux.dev/",
          "date": "2026-02-18T03:30:17Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch moves the pgscan and pgsteal counters from global vm_event_item to node_stat_item, allowing for per-node reclaim visibility in /proc/zoneinfo. The counters are still visible in /proc/vmstat but now aggregated across all nodes. Memcg accounting is preserved, and the virtio_balloon driver is updated to use the new global_node_page_state() function to fetch the counters.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Michael Tsirkin",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-18",
              "analysis_source": "heuristic"
            },
            {
              "author": "Michal Hocko",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-18",
              "analysis_source": "heuristic"
            },
            {
              "author": "Vlastimil (SUSE)",
              "summary": "Reviewer noted that PGREFILL is the only counter not updated to be per-node, suggesting it should also be changed for consistency.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "inconsistency",
                "request for change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hm so that leaves PGREFILL (scanned in the active list) the odd one out,\nright? Not being per-node and gated on !cgroup_reclaim() for global stats.\nShould we change it too for full consistency?",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "JP (Meta) (author)",
              "summary": "Author agreed to add counters for the active list side and suggested including PGDEACTIVATE for completeness.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "suggested"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm fine with adding coverage for the active list side as well. For\ncompleteness, I could also include PGDEACTIVATE.",
              "reply_to": "Vlastimil (SUSE)",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "JP (Meta) (author)",
              "summary": "Author acknowledged a concern about the patch affecting global counters accumulation, agreed to restructure in v2 by excluding PGDEACTIVATE from the change and promising to send a new version (v3) that includes PGREFILL.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Actually, I see PGDEACTIVATE is not gated so I'll leave that one out.\nI'll send v3 and include PGREFILL.",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/2] improve per-node allocation and reclaim visibility",
          "message_id": "20260212045109.255391-1-inwardvessel@gmail.com",
          "url": "https://lore.kernel.org/all/20260212045109.255391-1-inwardvessel@gmail.com/",
          "date": "2026-02-12T04:51:30Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-12",
          "patch_summary": "This patch series improves visibility into per-node memory allocation and reclaim by adding new node statistics to track allocations on a per-policy basis and changing the way reclaim-related statistics are tracked, allowing for easier diagnosis of NUMA imbalance scenarios.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "JP Kobryn (author)",
              "summary": "The author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It would be useful to see a breakdown of allocations to understand which\nNUMA policies are driving them. For example, when investigating memory\npressure, having policy-specific counts could show that allocations were\nbound to the affected node (via MPOL_BIND).\n\nAdd per-policy page allocation counters as new node stat items. These\ncounters can provide correlation between a mempolicy and pressure on a\ngiven node.\n\nSigned-off-by: JP Kobryn <inwardvessel@gmail.com>\nSuggested-by: Johannes Weiner <hannes@cmpxchg.org>\n---\n include/linux/mmzone.h |  9 +++++++++\n mm/mempolicy.c         | 30 ++++++++++++++++++++++++++++--\n mm/vmstat.c            |  9 +++++++++\n 3 files changed, 46 insertions(+), 2 deletions(-)\n\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex fc5d6c88d2f0..762609d5f0af 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -255,6 +255,15 @@ enum node_stat_item {\n \tPGDEMOTE_DIRECT,\n \tPGDEMOTE_KHUGEPAGED,\n \tPGDEMOTE_PROACTIVE,\n+#ifdef CONFIG_NUMA\n+\tPGALLOC_MPOL_DEFAULT,\n+\tPGALLOC_MPOL_PREFERRED,\n+\tPGALLOC_MPOL_BIND,\n+\tPGALLOC_MPOL_INTERLEAVE,\n+\tPGALLOC_MPOL_LOCAL,\n+\tPGALLOC_MPOL_PREFERRED_MANY,\n+\tPGALLOC_MPOL_WEIGHTED_INTERLEAVE,\n+#endif\n #ifdef CONFIG_HUGETLB_PAGE\n \tNR_HUGETLB,\n #endif\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 68a98ba57882..3c64784af761 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -217,6 +217,21 @@ static void reduce_interleave_weights(unsigned int *bw, u8 *new_iw)\n \t\tnew_iw[nid] /= iw_gcd;\n }\n \n+#define CHECK_MPOL_NODE_STAT_OFFSET(mpol) \\\n+\tBUILD_BUG_ON(PGALLOC_##mpol - mpol != PGALLOC_MPOL_DEFAULT)\n+\n+static enum node_stat_item mpol_node_stat(unsigned short mode)\n+{\n+\tCHECK_MPOL_NODE_STAT_OFFSET(MPOL_PREFERRED);\n+\tCHECK_MPOL_NODE_STAT_OFFSET(MPOL_BIND);\n+\tCHECK_MPOL_NODE_STAT_OFFSET(MPOL_INTERLEAVE);\n+\tCHECK_MPOL_NODE_STAT_OFFSET(MPOL_LOCAL);\n+\tCHECK_MPOL_NODE_STAT_OFFSET(MPOL_PREFERRED_MANY);\n+\tCHECK_MPOL_NODE_STAT_OFFSET(MPOL_WEIGHTED_INTERLEAVE);\n+\n+\treturn PGALLOC_MPOL_DEFAULT + mode;\n+}\n+\n int mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)\n {\n \tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;\n@@ -2446,8 +2461,14 @@ static struct page *alloc_pages_mpol(gfp_t gfp, unsigned int order,\n \n \tnodemask = policy_nodemask(gfp, pol, ilx, &nid);\n \n-\tif (pol->mode == MPOL_PREFERRED_MANY)\n-\t\treturn alloc_pages_preferred_many(gfp, order, nid, nodemask);\n+\tif (pol->mode == MPOL_PREFERRED_MANY) {\n+\t\tpage = alloc_pages_preferred_many(gfp, order, nid, nodemask);\n+\t\tif (page)\n+\t\t\t__mod_node_page_state(page_pgdat(page),\n+\t\t\t\t\tmpol_node_stat(MPOL_PREFERRED_MANY), 1 << order);\n+\n+\t\treturn page;\n+\t}\n \n \tif (IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) &&\n \t    /* filter \"hugepage\" allocation, unless from alloc_pages() */\n@@ -2472,6 +2493,9 @@ static struct page *alloc_pages_mpol(gfp_t gfp, unsigned int order,\n \t\t\tpage = __alloc_frozen_pages_noprof(\n \t\t\t\tgfp | __GFP_THISNODE | __GFP_NORETRY, order,\n \t\t\t\tnid, NULL);\n+\t\t\tif (page)\n+\t\t\t\t__mod_node_page_state(page_pgdat(page),\n+\t\t\t\t\t\tmpol_node_stat(pol->mode), 1 << order);\n \t\t\tif (page || !(gfp & __GFP_DIRECT_RECLAIM))\n \t\t\t\treturn page;\n \t\t\t/*\n@@ -2484,6 +2508,8 @@ static struct page *alloc_pages_mpol(gfp_t gfp, unsigned int order,\n \t}\n \n \tpage = __alloc_frozen_pages_noprof(gfp, order, nid, nodemask);\n+\tif (page)\n+\t\t__mod_node_page_state(page_pgdat(page), mpol_node_stat(pol->mode), 1 << order);\n \n \tif (unlikely(pol->mode == MPOL_INTERLEAVE ||\n \t\t     pol->mode == MPOL_WEIGHTED_INTERLEAVE) && page) {\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 65de88cdf40e..74e0ddde1e93 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -1291,6 +1291,15 @@ const char * const vmstat_text[] = {\n \t[I(PGDEMOTE_DIRECT)]\t\t\t= \"pgdemote_direct\",\n \t[I(PGDEMOTE_KHUGEPAGED)]\t\t= \"pgdemote_khugepaged\",\n \t[I(PGDEMOTE_PROACTIVE)]\t\t\t= \"pgdemote_proactive\",\n+#ifdef CONFIG_NUMA\n+\t[I(PGALLOC_MPOL_DEFAULT)]\t\t= \"pgalloc_mpol_default\",\n+\t[I(PGALLOC_MPOL_PREFERRED)]\t\t= \"pgalloc_mpol_preferred\",\n+\t[I(PGALLOC_MPOL_BIND)]\t\t\t= \"pgalloc_mpol_bind\",\n+\t[I(PGALLOC_MPOL_INTERLEAVE)]\t\t= \"pgalloc_mpol_interleave\",\n+\t[I(PGALLOC_MPOL_LOCAL)]\t\t\t= \"pgalloc_mpol_local\",\n+\t[I(PGALLOC_MPOL_PREFERRED_MANY)]\t= \"pgalloc_mpol_preferred_many\",\n+\t[I(PGALLOC_MPOL_WEIGHTED_INTERLEAVE)]\t= \"pgalloc_mpol_weighted_interleave\",\n+#endif\n #ifdef CONFIG_HUGETLB_PAGE\n \t[I(NR_HUGETLB)]\t\t\t\t= \"nr_hugetlb\",\n #endif\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "JP Kobryn (author)",
              "summary": "The author is addressing a concern about reclaim visibility, specifically that the patch should provide per-node reclaim statistics. The author agrees to change pgscan and pgsteal stats from global vm_event_item's to node_stat_item's, which will now track these stats on a per-memcg basis.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It would be useful to narrow down reclaim to specific nodes.\n\nProvide per-node reclaim visibility by changing the pgscan and pgsteal\nstats from global vm_event_item's to node_stat_item's. Note this change has\nthe side effect of now tracking these stats on a per-memcg basis.\n\nSigned-off-by: JP Kobryn <inwardvessel@gmail.com>\nSuggested-by: Johannes Weiner <hannes@cmpxchg.org>\n---\n drivers/virtio/virtio_balloon.c |  8 ++++----\n include/linux/mmzone.h          | 12 +++++++++++\n include/linux/vm_event_item.h   | 12 -----------\n mm/memcontrol.c                 | 36 ++++++++++++++++++---------------\n mm/vmscan.c                     | 32 +++++++++++------------------\n mm/vmstat.c                     | 24 +++++++++++-----------\n 6 files changed, 60 insertions(+), 64 deletions(-)\n\ndiff --git a/drivers/virtio/virtio_balloon.c b/drivers/virtio/virtio_balloon.c\nindex 74fe59f5a78c..1341d9d1a2a1 100644\n--- a/drivers/virtio/virtio_balloon.c\n+++ b/drivers/virtio/virtio_balloon.c\n@@ -374,13 +374,13 @@ static inline unsigned int update_balloon_vm_stats(struct virtio_balloon *vb)\n \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_ALLOC_STALL, stall);\n \n \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_ASYNC_SCAN,\n-\t\t    pages_to_bytes(events[PGSCAN_KSWAPD]));\n+\t\t    pages_to_bytes(global_node_page_state(PGSCAN_KSWAPD)));\n \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_DIRECT_SCAN,\n-\t\t    pages_to_bytes(events[PGSCAN_DIRECT]));\n+\t\t    pages_to_bytes(global_node_page_state(PGSCAN_DIRECT)));\n \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_ASYNC_RECLAIM,\n-\t\t    pages_to_bytes(events[PGSTEAL_KSWAPD]));\n+\t\t    pages_to_bytes(global_node_page_state(PGSTEAL_KSWAPD)));\n \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_DIRECT_RECLAIM,\n-\t\t    pages_to_bytes(events[PGSTEAL_DIRECT]));\n+\t\t    pages_to_bytes(global_node_page_state(PGSTEAL_DIRECT)));\n \n #ifdef CONFIG_HUGETLB_PAGE\n \tupdate_stat(vb, idx++, VIRTIO_BALLOON_S_HTLB_PGALLOC,\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 762609d5f0af..fc39c107a4b5 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -255,6 +255,18 @@ enum node_stat_item {\n \tPGDEMOTE_DIRECT,\n \tPGDEMOTE_KHUGEPAGED,\n \tPGDEMOTE_PROACTIVE,\n+\tPGSTEAL_KSWAPD,\n+\tPGSTEAL_DIRECT,\n+\tPGSTEAL_KHUGEPAGED,\n+\tPGSTEAL_PROACTIVE,\n+\tPGSTEAL_ANON,\n+\tPGSTEAL_FILE,\n+\tPGSCAN_KSWAPD,\n+\tPGSCAN_DIRECT,\n+\tPGSCAN_KHUGEPAGED,\n+\tPGSCAN_PROACTIVE,\n+\tPGSCAN_ANON,\n+\tPGSCAN_FILE,\n #ifdef CONFIG_NUMA\n \tPGALLOC_MPOL_DEFAULT,\n \tPGALLOC_MPOL_PREFERRED,\ndiff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\nindex 92f80b4d69a6..6f1787680658 100644\n--- a/include/linux/vm_event_item.h\n+++ b/include/linux/vm_event_item.h\n@@ -40,19 +40,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n \t\tPGLAZYFREED,\n \t\tPGREFILL,\n \t\tPGREUSE,\n-\t\tPGSTEAL_KSWAPD,\n-\t\tPGSTEAL_DIRECT,\n-\t\tPGSTEAL_KHUGEPAGED,\n-\t\tPGSTEAL_PROACTIVE,\n-\t\tPGSCAN_KSWAPD,\n-\t\tPGSCAN_DIRECT,\n-\t\tPGSCAN_KHUGEPAGED,\n-\t\tPGSCAN_PROACTIVE,\n \t\tPGSCAN_DIRECT_THROTTLE,\n-\t\tPGSCAN_ANON,\n-\t\tPGSCAN_FILE,\n-\t\tPGSTEAL_ANON,\n-\t\tPGSTEAL_FILE,\n #ifdef CONFIG_NUMA\n \t\tPGSCAN_ZONE_RECLAIM_SUCCESS,\n \t\tPGSCAN_ZONE_RECLAIM_FAILED,\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 86f43b7e5f71..bde0b6536be6 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -328,6 +328,18 @@ static const unsigned int memcg_node_stat_items[] = {\n \tPGDEMOTE_DIRECT,\n \tPGDEMOTE_KHUGEPAGED,\n \tPGDEMOTE_PROACTIVE,\n+\tPGSTEAL_KSWAPD,\n+\tPGSTEAL_DIRECT,\n+\tPGSTEAL_KHUGEPAGED,\n+\tPGSTEAL_PROACTIVE,\n+\tPGSTEAL_ANON,\n+\tPGSTEAL_FILE,\n+\tPGSCAN_KSWAPD,\n+\tPGSCAN_DIRECT,\n+\tPGSCAN_KHUGEPAGED,\n+\tPGSCAN_PROACTIVE,\n+\tPGSCAN_ANON,\n+\tPGSCAN_FILE,\n #ifdef CONFIG_HUGETLB_PAGE\n \tNR_HUGETLB,\n #endif\n@@ -441,14 +453,6 @@ static const unsigned int memcg_vm_event_stat[] = {\n #endif\n \tPSWPIN,\n \tPSWPOUT,\n-\tPGSCAN_KSWAPD,\n-\tPGSCAN_DIRECT,\n-\tPGSCAN_KHUGEPAGED,\n-\tPGSCAN_PROACTIVE,\n-\tPGSTEAL_KSWAPD,\n-\tPGSTEAL_DIRECT,\n-\tPGSTEAL_KHUGEPAGED,\n-\tPGSTEAL_PROACTIVE,\n \tPGFAULT,\n \tPGMAJFAULT,\n \tPGREFILL,\n@@ -1496,15 +1500,15 @@ static void memcg_stat_format(struct mem_cgroup *memcg, struct seq_buf *s)\n \n \t/* Accumulated memory events */\n \tseq_buf_printf(s, \"pgscan %lu\\n\",\n-\t\t       memcg_events(memcg, PGSCAN_KSWAPD) +\n-\t\t       memcg_events(memcg, PGSCAN_DIRECT) +\n-\t\t       memcg_events(memcg, PGSCAN_PROACTIVE) +\n-\t\t       memcg_events(memcg, PGSCAN_KHUGEPAGED));\n+\t\t       memcg_page_state(memcg, PGSCAN_KSWAPD) +\n+\t\t       memcg_page_state(memcg, PGSCAN_DIRECT) +\n+\t\t       memcg_page_state(memcg, PGSCAN_PROACTIVE) +\n+\t\t       memcg_page_state(memcg, PGSCAN_KHUGEPAGED));\n \tseq_buf_printf(s, \"pgsteal %lu\\n\",\n-\t\t       memcg_events(memcg, PGSTEAL_KSWAPD) +\n-\t\t       memcg_events(memcg, PGSTEAL_DIRECT) +\n-\t\t       memcg_events(memcg, PGSTEAL_PROACTIVE) +\n-\t\t       memcg_events(memcg, PGSTEAL_KHUGEPAGED));\n+\t\t       memcg_page_state(memcg, PGSTEAL_KSWAPD) +\n+\t\t       memcg_page_state(memcg, PGSTEAL_DIRECT) +\n+\t\t       memcg_page_state(memcg, PGSTEAL_PROACTIVE) +\n+\t\t       memcg_page_state(memcg, PGSTEAL_KHUGEPAGED));\n \n \tfor (i = 0; i < ARRAY_SIZE(memcg_vm_event_stat); i++) {\n #ifdef CONFIG_MEMCG_V1\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 614ccf39fe3f..16a0f21e3ea1 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -1977,7 +1977,7 @@ static unsigned long shrink_inactive_list(unsigned long nr_to_scan,\n \tunsigned long nr_taken;\n \tstruct reclaim_stat stat;\n \tbool file = is_file_lru(lru);\n-\tenum vm_event_item item;\n+\tenum node_stat_item item;\n \tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n \tbool stalled = false;\n \n@@ -2003,10 +2003,8 @@ static unsigned long shrink_inactive_list(unsigned long nr_to_scan,\n \n \t__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, nr_taken);\n \titem = PGSCAN_KSWAPD + reclaimer_offset(sc);\n-\tif (!cgroup_reclaim(sc))\n-\t\t__count_vm_events(item, nr_scanned);\n-\tcount_memcg_events(lruvec_memcg(lruvec), item, nr_scanned);\n-\t__count_vm_events(PGSCAN_ANON + file, nr_scanned);\n+\tmod_lruvec_state(lruvec, item, nr_scanned);\n+\tmod_lruvec_state(lruvec, PGSCAN_ANON + file, nr_scanned);\n \n \tspin_unlock_irq(&lruvec->lru_lock);\n \n@@ -2023,10 +2021,8 @@ static unsigned long shrink_inactive_list(unsigned long nr_to_scan,\n \t\t\t\t\tstat.nr_demoted);\n \t__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);\n \titem = PGSTEAL_KSWAPD + reclaimer_offset(sc);\n-\tif (!cgroup_reclaim(sc))\n-\t\t__count_vm_events(item, nr_reclaimed);\n-\tcount_memcg_events(lruvec_memcg(lruvec), item, nr_reclaimed);\n-\t__count_vm_events(PGSTEAL_ANON + file, nr_reclaimed);\n+\tmod_lruvec_state(lruvec, item, nr_reclaimed);\n+\tmod_lruvec_state(lruvec, PGSTEAL_ANON + file, nr_reclaimed);\n \n \tlru_note_cost_unlock_irq(lruvec, file, stat.nr_pageout,\n \t\t\t\t\tnr_scanned - nr_reclaimed);\n@@ -4536,7 +4532,7 @@ static int scan_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n {\n \tint i;\n \tint gen;\n-\tenum vm_event_item item;\n+\tenum node_stat_item item;\n \tint sorted = 0;\n \tint scanned = 0;\n \tint isolated = 0;\n@@ -4595,13 +4591,11 @@ static int scan_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n \t}\n \n \titem = PGSCAN_KSWAPD + reclaimer_offset(sc);\n-\tif (!cgroup_reclaim(sc)) {\n-\t\t__count_vm_events(item, isolated);\n+\tif (!cgroup_reclaim(sc))\n \t\t__count_vm_events(PGREFILL, sorted);\n-\t}\n-\tcount_memcg_events(memcg, item, isolated);\n+\tmod_lruvec_state(lruvec, item, isolated);\n \tcount_memcg_events(memcg, PGREFILL, sorted);\n-\t__count_vm_events(PGSCAN_ANON + type, isolated);\n+\tmod_lruvec_state(lruvec, PGSCAN_ANON + type, isolated);\n \ttrace_mm_vmscan_lru_isolate(sc->reclaim_idx, sc->order, scan_batch,\n \t\t\t\tscanned, skipped, isolated,\n \t\t\t\ttype ? LRU_INACTIVE_FILE : LRU_INACTIVE_ANON);\n@@ -4686,7 +4680,7 @@ static int evict_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n \tLIST_HEAD(clean);\n \tstruct folio *folio;\n \tstruct folio *next;\n-\tenum vm_event_item item;\n+\tenum node_stat_item item;\n \tstruct reclaim_stat stat;\n \tstruct lru_gen_mm_walk *walk;\n \tbool skip_retry = false;\n@@ -4750,10 +4744,8 @@ static int evict_folios(unsigned long nr_to_scan, struct lruvec *lruvec,\n \t\t\t\t\tstat.nr_demoted);\n \n \titem = PGSTEAL_KSWAPD + reclaimer_offset(sc);\n-\tif (!cgroup_reclaim(sc))\n-\t\t__count_vm_events(item, reclaimed);\n-\tcount_memcg_events(memcg, item, reclaimed);\n-\t__count_vm_events(PGSTEAL_ANON + type, reclaimed);\n+\tmod_lruvec_state(lruvec, item, reclaimed);\n+\tmod_lruvec_state(lruvec, PGSTEAL_ANON + type, reclaimed);\n \n \tspin_unlock_irq(&lruvec->lru_lock);\n \ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 74e0ddde1e93..e4b259989d58 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -1291,6 +1291,18 @@ const char * const vmstat_text[] = {\n \t[I(PGDEMOTE_DIRECT)]\t\t\t= \"pgdemote_direct\",\n \t[I(PGDEMOTE_KHUGEPAGED)]\t\t= \"pgdemote_khugepaged\",\n \t[I(PGDEMOTE_PROACTIVE)]\t\t\t= \"pgdemote_proactive\",\n+\t[I(PGSTEAL_KSWAPD)]\t\t\t= \"pgsteal_kswapd\",\n+\t[I(PGSTEAL_DIRECT)]\t\t\t= \"pgsteal_direct\",\n+\t[I(PGSTEAL_KHUGEPAGED)]\t\t\t= \"pgsteal_khugepaged\",\n+\t[I(PGSTEAL_PROACTIVE)]\t\t\t= \"pgsteal_proactive\",\n+\t[I(PGSTEAL_ANON)]\t\t\t= \"pgsteal_anon\",\n+\t[I(PGSTEAL_FILE)]\t\t\t= \"pgsteal_file\",\n+\t[I(PGSCAN_KSWAPD)]\t\t\t= \"pgscan_kswapd\",\n+\t[I(PGSCAN_DIRECT)]\t\t\t= \"pgscan_direct\",\n+\t[I(PGSCAN_KHUGEPAGED)]\t\t\t= \"pgscan_khugepaged\",\n+\t[I(PGSCAN_PROACTIVE)]\t\t\t= \"pgscan_proactive\",\n+\t[I(PGSCAN_ANON)]\t\t\t= \"pgscan_anon\",\n+\t[I(PGSCAN_FILE)]\t\t\t= \"pgscan_file\",\n #ifdef CONFIG_NUMA\n \t[I(PGALLOC_MPOL_DEFAULT)]\t\t= \"pgalloc_mpol_default\",\n \t[I(PGALLOC_MPOL_PREFERRED)]\t\t= \"pgalloc_mpol_preferred\",\n@@ -1344,19 +1356,7 @@ const char * const vmstat_text[] = {\n \n \t[I(PGREFILL)]\t\t\t\t= \"pgrefill\",\n \t[I(PGREUSE)]\t\t\t\t= \"pgreuse\",\n-\t[I(PGSTEAL_KSWAPD)]\t\t\t= \"pgsteal_kswapd\",\n-\t[I(PGSTEAL_DIRECT)]\t\t\t= \"pgsteal_direct\",\n-\t[I(PGSTEAL_KHUGEPAGED)]\t\t\t= \"pgsteal_khugepaged\",\n-\t[I(PGSTEAL_PROACTIVE)]\t\t\t= \"pgsteal_proactive\",\n-\t[I(PGSCAN_KSWAPD)]\t\t\t= \"pgscan_kswapd\",\n-\t[I(PGSCAN_DIRECT)]\t\t\t= \"pgscan_direct\",\n-\t[I(PGSCAN_KHUGEPAGED)]\t\t\t= \"pgscan_khugepaged\",\n-\t[I(PGSCAN_PROACTIVE)]\t\t\t= \"pgscan_proactive\",\n \t[I(PGSCAN_DIRECT_THROTTLE)]\t\t= \"pgscan_direct_throttle\",\n-\t[I(PGSCAN_ANON)]\t\t\t= \"pgscan_anon\",\n-\t[I(PGSCAN_FILE)]\t\t\t= \"pgscan_file\",\n-\t[I(PGSTEAL_ANON)]\t\t\t= \"pgsteal_anon\",\n-\t[I(PGSTEAL_FILE)]\t\t\t= \"pgsteal_file\",\n \n #ifdef CONFIG_NUMA\n \t[I(PGSCAN_ZONE_RECLAIM_SUCCESS)]\t= \"zone_reclaim_success\",\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Michal Hocko",
              "summary": "reviewer noted that the changelog could be more clear about the actual changes, specifically mentioning that /proc/vmstat will have slightly different counters ordering and per-node stats will be displayed in /proc/zoneinfo",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The changelog could have been more clear about the actual changes as\nthis is not overly clear for untrained eyes. The most important parts\nare that /proc/vmstat will preserve reclaim stats with slightly\ndifferent counters ordering (shouldn't break userspace much^W), per-node\nstats will be now newly displayed in /proc/zoneinfo - this is presumably\nthe primary motivation to have a better insight of per-node reclaim\nactivity, and memcg stats will now show their share of the global memory\nreclaim.\n\nHave I missed anything?",
              "reply_to": "JP Kobryn",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Michal Hocko",
              "summary": "reviewer requested clarification on the intended usage of new counters",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "lack of clear explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Could you be more specific how exactly do you plan to use those\ncounters?\n\n-- \nMichal Hocko\nSUSE Labs",
              "reply_to": "JP Kobryn",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt noted that the use of __mod_node_page_state() is incorrect because it requires preempt disable or IRQ disable, but this code path does not take those precautions, and suggested replacing it with mod_node_page_state()",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Here and two places below, please use mod_node_page_state() instead of\n__mod_node_page_state() as __foo() requires preempt disable or if the\ngiven stat can be updated in IRQ, then IRQ disable. This code path does\nnot do either of that.",
              "reply_to": "JP Kobryn",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Vlastimil Babka",
              "summary": "Reviewer questioned whether adding counters per policy type is sufficient, noting that numa_{hit,miss,etc.} counters might be insufficient and suggesting extension to capture missing details.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Are the numa_{hit,miss,etc.} counters insufficient? Could they be extended\nin a way that would capture any missing important details? A counter per\npolicy type seems exhaustive, but then on one hand it might be not important\nto distinguish beetween some of them, and on the other hand it doesn't track\nthe nodemask anyway.",
              "reply_to": "JP Kobryn",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "syzbot ci",
              "summary": "The reviewer reported a WARNING in __mod_node_page_state due to an apparent lock ordering violation, where the per-vswap spinlock is acquired while holding the folio lock, and requested that the lock be dropped before calling try_to_unmap().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "WARNING",
                "lock ordering violation"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "syzbot ci has tested the following series\n\n[v1] improve per-node allocation and reclaim visibility\nhttps://lore.kernel.org/all/20260212045109.255391-1-inwardvessel@gmail.com\n* [PATCH 1/2] mm/mempolicy: track page allocations per mempolicy\n* [PATCH 2/2] mm: move pgscan and pgsteal to node stats\n\nand found the following issue:\nWARNING in __mod_node_page_state\n\nFull report is available here:\nhttps://ci.syzbot.org/series/4ec12ede-3298-43a3-ab6b-79d47759672e\n\n***\n\nWARNING in __mod_node_page_state\n\ntree:      mm-new\nURL:       https://kernel.googlesource.com/pub/scm/linux/kernel/git/akpm/mm.git\nbase:      72a46cdd4ef13690beb8c5a2f6a2023fd7ef2eb4\narch:      amd64\ncompiler:  Debian clang version 21.1.8 (++20251221033036+2078da43e25a-1~exp1~20251221153213.50), Debian LLD 21.1.8\nconfig:    https://ci.syzbot.org/builds/0f678e4c-a4ba-4f17-8ed7-8ae99e56a463/config\n\n------------[ cut here ]------------\nIS_ENABLED(CONFIG_PREEMPT_COUNT) && __lockdep_enabled && (preempt_count() == 0 && this_cpu_read(hardirqs_enabled))\nWARNING: mm/vmstat.c:396 at __mod_node_page_state+0x126/0x170, CPU#0: kthreadd/2\nModules linked in:\nCPU: 0 UID: 0 PID: 2 Comm: kthreadd Not tainted syzkaller #0 PREEMPT(full) \nHardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 1.16.2-debian-1.16.2-1 04/01/2014\nRIP: 0010:__mod_node_page_state+0x126/0x170\nCode: 5c 41 5d 41 5e 41 5f 5d c3 cc cc cc cc cc 48 89 df 4c 89 e6 44 89 fa e8 68 00 00 00 31 db eb cc 90 0f 0b 90 e9 3e ff ff ff 90 <0f> 0b 90 eb 80 48 c7 c7 e0 c6 64 8e 4c 89 f6 e8 66 3c d3 02 e9 28\nRSP: 0000:ffffc900000773d0 EFLAGS: 00010202\nRAX: 0000000000000001 RBX: 0000000000000001 RCX: 0000000000000000\nRDX: 0000000000000001 RSI: 000000000000003d RDI: ffff88815fffb380\nRBP: dffffc0000000000 R08: ffffffff8fef2977 R09: 1ffffffff1fde52e\nR10: dffffc0000000000 R11: fffffbfff1fde52f R12: ffff88815fffb380\nR13: ffffffff92f50f00 R14: 000000000000003d R15: 000000000000003d\nFS:  0000000000000000(0000) GS:ffff88818e0f0000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: ffff88823ffff000 CR3: 000000000e346000 CR4: 00000000000006f0\nCall Trace:\n <TASK>\n alloc_pages_mpol+0x407/0x740\n alloc_pages_noprof+0xa8/0x190\n get_free_pages_noprof+0xf/0x80\n __kasan_populate_vmalloc+0x38/0x1d0\n alloc_vmap_area+0xd21/0x1460\n __get_vm_area_node+0x1f8/0x300\n __vmalloc_node_range_noprof+0x372/0x1730\n __vmalloc_node_noprof+0xc2/0x100\n dup_task_struct+0x228/0x9a0\n copy_process+0x508/0x3980\n kernel_clone+0x248/0x870\n kernel_thread+0x13f/0x1b0\n kthreadd+0x4f9/0x6f0\n ret_from_fork+0x51b/0xa40\n ret_from_fork_asm+0x1a/0x30\n </TASK>\n\n\n***\n\nIf these findings have caused you to resend the series or submit a\nseparate fix, please add the following tag to your commit message:\n  Tested-by: syzbot@syzkaller.appspotmail.com\n\n---\nThis report is generated by a bot. It may contain errors.\nsyzbot ci engineers can be reached at syzkaller@googlegroups.com.",
              "reply_to": "JP Kobryn",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "JP Kobryn (author)",
              "summary": "Author acknowledged that the patch's new node stats are not only visible in /proc/zoneinfo but also in /sys/devices/system/node/nodeN/vmstat, agreed to add this information to the changelog in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a need for clarification",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "That's accurate. Plus aside from reading /proc/zoneinfo they will also\nbe in /sys/devices/system/node/nodeN/vmstat. I see I could have been\nmore explicit about this. Let me make additions to the changelog in v2.\nThanks for taking a look.",
              "reply_to": "Michal Hocko",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "JP Kobryn (author)",
              "summary": "Author acknowledged that the current patch does not address reclaim visibility, but confirmed that Patch 2 already provides a way to identify affected nodes and agreed that correlating pressure with mempolicy is necessary.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a limitation",
                "agreed on next steps"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes. Patch 2 allows us to find which nodes are undergoing reclaim. Once\nwe identify the affected node(s), the new mpol counters (this patch)\nallow us correlate the pressure to the mempolicy driving it.",
              "reply_to": "Michal Hocko",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "JP Kobryn (author)",
              "summary": "Author acknowledged the need to address the issue of swapoff path dropping per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged the need for a fix",
                "agreed to restructure in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks, I also see syzbot flagged this as well. I can make this change\nin v2.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "JP Kobryn (author)",
              "summary": "Author acknowledged that patch 2 provides visibility into affected nodes, and explained that extending numa_* counters would require more permutations to account for policy-specific stats.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical consideration",
                "asked for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The two patches of the series should complement each other. When\ninvestigating memory pressure, we could identify the affected nodes\n(patch 2). Then we can cross-reference the policy-specific stats to find\nany correlation (this patch).\n\nI think extending numa_* counters would call for more permutations to\naccount for the numa stat per policy. I think distinguishing between\nMPOL_DEFAULT and MPOL_BIND is meaningful, for example. Am I\nunderstanding your question?",
              "reply_to": "Vlastimil Babka",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Matthew Wilcox",
              "summary": "reviewer pointed out that the 'from' line in the patch header must be the first line, specifying the author's name and affiliation, to ensure correct attribution in the changelog",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "request"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "^^^^^^^^^\n\nThe ``from`` line must be the very first line in the message body,\nand has the form:\n\n        From: Patch Author <author@example.com>\n\nThe ``from`` line specifies who will be credited as the author of the\npatch in the permanent changelog.  If the ``from`` line is missing,\nthen the ``From:`` line from the email header will be used to determine\nthe patch author in the changelog.\n\nThe author may indicate their affiliation or the sponsor of the work\nby adding the name of an organization to the ``from`` and ``SoB`` lines,\ne.g.:\n\n        From: Patch Author (Company) <author@example.com>\n\n\nI do this with ~/.gitconfig\n\n[user]\n        name = Matthew Wilcox (Oracle)\n        email = willy@infradead.org\n\nand it goes into the From and Signed-off-by lines correctly when\ngenerating patches.",
              "reply_to": "JP Kobryn",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Vlastimil Babka",
              "summary": "reviewer questioned the level of detail in always-on counters, suggesting they should be limited to what's known to be useful and not include everything that could potentially be needed",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Are there other useful examples or would it be enough to add e.g. a\nnuma_bind counter to the numa_hit/miss/etc?\nWhat I'm trying to say the level of detail you are trying to add to the\nalways-on counters seems like more suitable for tracepoints. The counters\nshould be limited to what's known to be useful and not \"everything we are\nable to track and possibly could need one day\".",
              "reply_to": "JP Kobryn",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "JP (Meta) (author)",
              "summary": "Author acknowledged that numa stats may not be suitable for tracking policy-driven allocations, and is seeking feedback on their suitability.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "author seeks clarification",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Aside from bind, it's worth emphasizing that with default policy\ntracking we could see if the local node is the source of pressure. In\nthe interleave case, we would be able to see if the loads are being\nbalanced or, in the weighted case, being distributed properly.\n\nOn extending the numa stats instead, I looked into this some more. I'm\nnot sure if they're a good fit. They seem more about whether the\nallocator succeeded at placement rather than which policy drove the\nallocation. Thoughts?",
              "reply_to": "Vlastimil Babka",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "JP (Meta) (author)",
              "summary": "Author acknowledged the need for historical stats collection in triage scenarios, explained why attaching tracepoints at the time of the incident would be too late, and implied that the current patch is sufficient for per-node allocation visibility.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a limitation",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "In a triage scenario, having the stats collected up to the time of the\nreported issue would be better. We make use of the tool called below[0].\nIt periodically samples the system and allows us to view the\nhistorical state prior to the issue. If we started at the time of the\nincident and attached tracepoints it would be too late.\n\nThe triage workflow would look like this:\n1) Pressure/OOMs reported while system-wide memory is free.\n2) Check per-node pgscan/pgsteal stats (provided by patch 2) to narrow\ndown node(s) under pressure.\n3) Check per-policy allocation counters (this patch) on that node to\nfind what policy was driving it.\n\n[0] https://github.com/facebookincubator/below",
              "reply_to": "Vlastimil Babka",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Michal Hocko",
              "summary": "reviewer expressed concern that adding counters for per-node allocation and reclaim visibility may be difficult to remove once implemented, citing the precedent of dropping some counters in the past but noting it's still not trivial",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I would appreciate somehow more specificity. You are adding counters\nthat are not really easy to drop once they are in. Sure we have\nprecedence of dropping some counters in the past so this is not as hard\nas usual userspace APIs but still...\n\nHow exactly do you tolerate mempolicy allocations to specific nodes?\nWhile MPOL_MBIND is quite straightforward others are less so.\n-- \nMichal Hocko\nSUSE Labs",
              "reply_to": "JP Kobryn",
              "message_date": "2026-02-16",
              "analysis_source": "llm"
            },
            {
              "author": "JP (Meta) (author)",
              "summary": "Author addressed a concern that the patch does not account for per-policy allocations, explaining that it uses page_pgdat(page) in __mod_node_page_state() to attribute stats to the node where pages actually land.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged feedback",
                "provided explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The design does account for this regardless of the policy. In the call\nto __mod_node_page_state(), I'm using page_pgdat(page) so the stat is\nattributed to the node where the page actually landed.",
              "reply_to": "Michal Hocko",
              "message_date": "2026-02-16",
              "analysis_source": "llm"
            },
            {
              "author": "Michal Hocko",
              "summary": "reviewer questioned the usefulness of tracking per-policy and per-node memory pressure, noting that it's unclear how to determine which policy or part of the nodemask is causing pressure on a particular node",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested clarification",
                "pointed out potential misaccounting"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That much is clear[*]. The consumer side of things is not really clear to\nme. How do you know which policy or part of the nodemask of that policy\nis the source of the memory pressure on a particular node? In other\nwords how much is the data actually useful except for a single node\nmempolicy (i.e. MBIND).\n\n[*] btw. I believe you misaccount MPOL_LOCAL because you attribute the\ntarget node even when the allocation is from a remote node from the\n\"local\" POV.\n-- \nMichal Hocko\nSUSE Labs",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-16",
              "analysis_source": "llm"
            },
            {
              "author": "JP (Meta) (author)",
              "summary": "Author addressed Michal Hocko's concern about the effectiveness of the patch in diagnosing NUMA imbalance scenarios by explaining how the per-policy allocation counters can be used to identify the policy driving pressure on a specific node, and proposed a workflow for troubleshooting.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Other than the bind policy, having the interleave (and weighted) stats\nwould allow us to see the effective distribution of the policy. Pressure\ncould be linked to a user configured weight scheme. I would think it\ncould also help with confirming expected distributions.\n\nYou brought up the node mask so with the preferred policy, I think this\nis a good one for using the counters as well. Once we're at the point\nwhere we know the node(s) under pressure and then see significant\npreferred allocs accounted for, we could search the numa_maps that have\n\"prefer:<node>\" to find the tasks targeting the affected nodes.\n\nI mentioned this on another thread in this series but I'll include here\nas well and expand some more. For any given policy, the workflow would\nbe:\n1) Pressure/OOMs reported while system-wide memory is free.\n2) Check per-node pgscan/pgsteal stats (provided by patch 2) to narrow\ndown node(s) under pressure. They become available in\n/sys/devices/system/node/nodeN/vmstat.\n3) Check per-policy allocation counters (this patch) on that node to\nfind what policy was driving it. Same readout at nodeN/vmstat.\n4) Now use /proc/*/numa_maps to identify tasks using the policy.",
              "reply_to": "Michal Hocko",
              "message_date": "2026-02-16",
              "analysis_source": "llm"
            },
            {
              "author": "JP (Meta) (author)",
              "summary": "Author acknowledged a concern about accounting for fallback cases in node pressure investigation, explaining that these allocations can be considered acceptable noise.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "explained the reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It's a good point. The accounting as a result of fallback cases\nshouldn't detract from an investigation though. We're interested in the\nnode(s) under pressure so the relatively few fallback allocations would\nland on nodes that are not under pressure and could be viewed as\nacceptable noise.",
              "reply_to": "Michal Hocko",
              "message_date": "2026-02-16",
              "analysis_source": "llm"
            },
            {
              "author": "Michal Hocko",
              "summary": "reviewer noted that the patch does not provide a way to distinguish between requested node and actual node used, making it impossible to determine whether memory pressure is caused by fallbacks or mempolicy configurations",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "long term maintainability"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This is really confusing. You simply have no means to tell the\ndifference between the requested node and the real node used so you\ncannot really say whether the memory pressure is because of fallbacks or\nyour mempolicy configurations. That means that you cannot tell the\ndifference between the source of the pressure and victim of that\npressure. \n\nI am not saying these scheme doesn't work in your particular setup but I\ndo not see this is long term maintainable thing. It is just too easy to\nget misleading numbers. If we want/need to track mempolicy allocations\nbetter than what existing numa_* counters offer then this needs to be\nthought through I believe.\n\nI do not think we should add these counters in this form. \n-- \nMichal Hocko\nSUSE Labs",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "JP (Meta) (author)",
              "summary": "Author considered an alternative approach to address Michal Hocko's concern about determining the correct node for tracking allocations, suggesting excluding fallback cases and comparing the allocated page's node against the requested node or node mask.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "considered_an_alternative_approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "What if I excluded the fallback cases? I could get the actual node from\nthe allocated page and compare against the requested node or node mask.",
              "reply_to": "Michal Hocko",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Michal Hocko",
              "summary": "Reviewer Michal Hocko requested that per-node reclaim stats be sent separately, and asked for a clear definition of the semantic for each mempolicy before implementing tracking for it.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think it would make sense to send the per-node reclaim stats\nseparately as there doesn't seem to be any dispute about that.\n\nFor mempolicy stats try to define semantic for each mempolicy first.\nWhat exactly do you miss from existing numa_*?\nDo you want to count number of requests/successes. Do you want to track\nfailures? In what kind of granularity (track fallback nodes)?\n\n-- \nMichal Hocko\nSUSE Labs",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "kernel robot",
              "summary": "The reviewer, kernel robot, noticed that the function __mod_node_page_state is being called in a RIP (return instruction prefix) state, indicating a potential lock ordering issue. This was observed on a specific commit and test case, where the function is being called while holding the folio lock, which may lead to a deadlock with reclaim paths.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential lock ordering issue",
                "deadlock"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hello,\n\nkernel test robot noticed \"RIP:__mod_node_page_state\" on:\n\ncommit: 4b5f69459c0988d3b292aceb74633e04eea84c7f (\"[PATCH 1/2] mm/mempolicy: track page allocations per mempolicy\")\nurl: https://github.com/intel-lab-lkp/linux/commits/JP-Kobryn/mm-mempolicy-track-page-allocations-per-mempolicy/20260212-142941\nbase: https://git.kernel.org/cgit/linux/kernel/git/akpm/mm.git mm-everything\npatch link: https://lore.kernel.org/all/20260212045109.255391-2-inwardvessel@gmail.com/\npatch subject: [PATCH 1/2] mm/mempolicy: track page allocations per mempolicy\n\nin testcase: boot\n\nconfig: x86_64-randconfig-007-20250327\ncompiler: gcc-14\ntest machine: qemu-system-x86_64 -enable-kvm -cpu SandyBridge -smp 2 -m 32G\n\n(please refer to attached dmesg/kmsg for entire log/backtrace)\n\n\n+------------------------------------------------------------------+------------+------------+\n|                                                                  | 5cbf93e36f | 4b5f69459c |\n+------------------------------------------------------------------+------------+------------+\n| boot_successes                                                   | 244        | 0          |\n| boot_failures                                                    | 0          | 244        |\n| RIP:__mod_node_page_state                                        | 0          | 244        |\n| BUG:using__this_cpu_read()in_preemptible                         | 0          | 244        |\n| BUG:using__this_cpu_write()in_preemptible[#]code:kthreadd        | 0          | 244        |\n| BUG:using__this_cpu_write()in_preemptible[#]code:swapper         | 0          | 187        |\n| BUG:using__this_cpu_write()in_preemptible[#]code:kdevtmpfs       | 0          | 79         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:kworker/u8      | 0          | 229        |\n| BUG:using__this_cpu_write()in_preemptible[#]code:udevd           | 0          | 62         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:tail            | 0          | 21         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:syslogd         | 0          | 54         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:klogd           | 0          | 113        |\n| BUG:using__this_cpu_write()in_preemptible[#]code:sleep           | 0          | 98         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:post-run        | 0          | 39         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:rsync           | 0          | 9          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:modprobe        | 0          | 6          |\n| BUG:using__this_cpu_write()in_preemptible[#]code                 | 0          | 32         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:udevadm         | 0          | 78         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd         | 0          | 39         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:(udev-worker)   | 0          | 53         |\n| RIP:rep_movs_alternative                                         | 0          | 5          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:cat             | 0          | 7          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:sed             | 0          | 98         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-udevd   | 0          | 19         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-journal | 0          | 54         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-random  | 0          | 4          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:journalctl      | 0          | 8          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:start_getty     | 0          | 4          |\n| RIP:__put_user_4                                                 | 0          | 24         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:wget            | 0          | 82         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:run-lkp         | 0          | 32         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:boot-#-yocto-i3 | 0          | 24         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:one-shot-monito | 0          | 4          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:vmstat          | 0          | 29         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:rs:main_Q:Reg   | 0          | 9          |\n| RIP:rep_stos_alternative                                         | 0          | 11         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:lkp-setup-rootf | 0          | 21         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:stty            | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:tee             | 0          | 7          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-rc-loca | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:(exec-inner)    | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:groupadd        | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:(sd-exec-strv)  | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:rc              | 0          | 14         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:getty           | 0          | 18         |\n| BUG:using__this_cpu_write()in_preemptible[#]code:boot-#-debian   | 0          | 4          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:monitor         | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-tmpfile | 0          | 6          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:lscpu           | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:dirname         | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-sysuser | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:(d-sysctl)      | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:mount           | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:ls              | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:pgrep           | 0          | 4          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:grep            | 0          | 8          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:S77lkp-bootstra | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:date            | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-sysctl  | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:find            | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:sshd            | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-system  | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-sysv-ge | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-hiberna | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:journal-offline | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:sysctl          | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:init            | 0          | 7          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:mkdir           | 0          | 6          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:mountpoint      | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-logind  | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:dmesg           | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-ssh-gen | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:cp              | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:wakeup          | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:dpkg-deb        | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:dpkg            | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:(modprobe)      | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:sync            | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-update  | 0          | 4          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:kmod            | 0          | 1          |\n| RIP:strncpy_from_user                                            | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:sm-notify       | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-remount | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:blkmapd         | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:mkfifo          | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:ln              | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:sh              | 0          | 5          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:bootlogd        | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:run-test        | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:S07bootlogd     | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:hwclock.sh      | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:(sd-mkdcreds)   | 0          | 1          |\n| RIP:filldir64                                                    | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:chmod           | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:ps              | 0          | 3          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:which           | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:ip              | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:start-stop-daem | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:S20syslog       | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-gpt-aut | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-debug-g | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:(rpcbind)       | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:seq             | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-run-gen | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:wait            | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:addgroup        | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:rm              | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:in:imklog       | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:basename        | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:touch           | 0          | 1          |\n| RIP:ia32_setup_frame                                             | 0          | 2          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:no-stdout-monit | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:systemd-tpm#-ge | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:(mount)         | 0          | 1          |\n| BUG:using__this_cpu_write()in_preemptible[#]code:ldconfig        | 0          | 1          |\n+------------------------------------------------------------------+------------+------------+\n\nIf you fix the issue in a separate patch/commit (i.e. not just a new version of\nthe same patch/commit), kindly add following tags\n| Reported-by: kernel test robot <oliver.sang@intel.com>\n| Closes: https://lore.kernel.org/oe-lkp/202602181136.f66ba888-lkp@intel.com\n\n\n\n[    0.624787][    T2] ------------[ cut here ]------------\n[    0.625191][    T2] WARNING: mm/vmstat.c:396 at __mod_node_page_state+0x88/0x1c0, CPU#0: kthreadd/2\n[    0.625887][    T2] Modules linked in:\n[    0.626070][    T2] CPU: 0 UID: 0 PID: 2 Comm: kthreadd Tainted: G                T   6.19.0-rc6-00596-g4b5f69459c09 #1 PREEMPT(lazy)  a55f7fce8adbfb8e52612c1f0ea71f4db1a1df23\n[    0.626084][    T2] Tainted: [T]=RANDSTRUCT\n[    0.626402][    T2] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.16.3-debian-1.16.3-2 04/01/2014\n[    0.627150][    T2] RIP: 0010:__mod_node_page_state (mm/vmstat.c:396 (discriminator 34))\n[    0.627592][    T2] Code: 8b 05 88 b9 73 02 48 c7 c7 d8 b0 b4 83 85 c0 89 45 d0 40 0f 95 c6 31 c9 31 d2 40 0f b6 f6 e8 3f 96 e4 ff 8b 45 d0 85 c0 74 1b <0f> 0b be 01 00 00 00 eb 14 31 c9 31 d2 31 f6 48 c7 c7 d8 b0 b4 83\nAll code\n========\n   0:\t8b 05 88 b9 73 02    \tmov    0x273b988(%rip),%eax        # 0x273b98e\n   6:\t48 c7 c7 d8 b0 b4 83 \tmov    $0xffffffff83b4b0d8,%rdi\n   d:\t85 c0                \ttest   %eax,%eax\n   f:\t89 45 d0             \tmov    %eax,-0x30(%rbp)\n  12:\t40 0f 95 c6          \tsetne  %sil\n  16:\t31 c9                \txor    %ecx,%ecx\n  18:\t31 d2                \txor    %edx,%edx\n  1a:\t40 0f b6 f6          \tmovzbl %sil,%esi\n  1e:\te8 3f 96 e4 ff       \tcall   0xffffffffffe49662\n  23:\t8b 45 d0             \tmov    -0x30(%rbp),%eax\n  26:\t85 c0                \ttest   %eax,%eax\n  28:\t74 1b                \tje     0x45\n  2a:*\t0f 0b                \tud2\t\t<-- trapping instruction\n  2c:\tbe 01 00 00 00       \tmov    $0x1,%esi\n  31:\teb 14                \tjmp    0x47\n  33:\t31 c9                \txor    %ecx,%ecx\n  35:\t31 d2                \txor    %edx,%edx\n  37:\t31 f6                \txor    %esi,%esi\n  39:\t48 c7 c7 d8 b0 b4 83 \tmov    $0xffffffff83b4b0d8,%rdi\n\nCode starting with the faulting instruction\n===========================================\n   0:\t0f 0b                \tud2\n   2:\tbe 01 00 00 00       \tmov    $0x1,%esi\n   7:\teb 14                \tjmp    0x1d\n   9:\t31 c9                \txor    %ecx,%ecx\n   b:\t31 d2                \txor    %edx,%edx\n   d:\t31 f6                \txor    %esi,%esi\n   f:\t48 c7 c7 d8 b0 b4 83 \tmov    $0xffffffff83b4b0d8,%rdi\n[    0.629418][    T2] RSP: 0000:ffff88810039fa20 EFLAGS: 00010202\n[    0.629869][    T2] RAX: 0000000000000001 RBX: 0000000000000002 RCX: 0000000000000000\n[    0.630445][    T2] RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000\n[    0.631089][    T2] RBP: ffff88810039fa50 R08: 0000000000000000 R09: 0000000000000000\n[    0.631671][    T2] R10: 0000000000000000 R11: 0000000000000000 R12: ffff88883ffe02c0\n[    0.632247][    T2] R13: ffffffff83f18971 R14: ffffffff83f18940 R15: 0000000000000030\n[    0.632746][    T2] FS:  0000000000000000(0000) GS:ffff88889bd1c000(0000) knlGS:0000000000000000\n[    0.633394][    T2] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[    0.633875][    T2] CR2: ffff88883ffff000 CR3: 000000000343d000 CR4: 00000000000406b0\n[    0.634478][    T2] Call Trace:\n[    0.634723][    T2]  <TASK>\n[    0.634951][    T2]  alloc_pages_mpol (mm/mempolicy.c:2513 (discriminator 1))\n[    0.635326][    T2]  alloc_frozen_pages_noprof (mm/mempolicy.c:2584)\n[    0.635746][    T2]  allocate_slab (mm/slub.c:3075 (discriminator 2) mm/slub.c:3248 (discriminator 2))\n[    0.636086][    T2]  new_slab (mm/slub.c:3304)\n[    0.636394][    T2]  ___slab_alloc (mm/slub.c:4657)\n[    0.636749][    T2]  ? dup_task_struct (kernel/fork.c:184 (discriminator 2) kernel/fork.c:915 (discriminator 2))\n[    0.637114][    T2]  __slab_alloc+0x8a/0x180\n[    0.637519][    T2]  slab_alloc_node+0x189/0x340\n[    0.637919][    T2]  ? dup_task_struct (kernel/fork.c:184 (discriminator 2) kernel/fork.c:915 (discriminator 2))\n[    0.638285][    T2]  kmem_cache_alloc_node_noprof (mm/slub.c:5317 (discriminator 1))\n[    0.638710][    T2]  dup_task_struct (kernel/fork.c:184 (discriminator 2) kernel/fork.c:915 (discriminator 2))\n[    0.639058][    T2]  ? ftrace_likely_update (arch/x86/include/asm/smap.h:90 kernel/trace/trace_branch.c:223)\n[    0.639416][    T2]  copy_process (kernel/fork.c:2052 (discriminator 1))\n[    0.639773][    T2]  kernel_clone (include/linux/random.h:26 kernel/fork.c:2652)\n[    0.640115][    T2]  ? kthread_fetch_affinity (kernel/kthread.c:412)\n[    0.640552][    T2]  kernel_thread (kernel/fork.c:2713)\n[    0.640892][    T2]  ? kthread_fetch_affinity (kernel/kthread.c:412)\n[    0.641310][    T2]  kthreadd (kernel/kthread.c:486 kernel/kthread.c:844)\n[    0.641621][    T2]  ? kthreadd (kernel/kthread.c:830 (discriminator 5))\n[    0.641938][    T2]  ? kthread_is_per_cpu (kernel/kthread.c:816)\n[    0.642316][    T2]  ret_from_fork (arch/x86/kernel/process.c:164)\n[    0.642657][    T2]  ? kthread_is_per_cpu (kernel/kthread.c:816)\n[    0.642744][    T2]  ? kthread_is_per_cpu (kernel/kthread.c:816)\n[    0.643127][    T2]  ret_from_fork_asm (arch/x86/entry/entry_64.S:256)\n[    0.643502][    T2]  </TASK>\n[    0.643755][    T2] irq event stamp: 393\n[    0.644054][    T2] hardirqs last  enabled at (401): __up_console_sem (arch/x86/include/asm/irqflags.h:42 arch/x86/include/asm/irqflags.h:119 arch/x86/include/asm/irqflags.h:159 kernel/printk/printk.c:345)\n[    0.644730][    T2] hardirqs last disabled at (408): __up_console_sem (kernel/printk/printk.c:343 (discriminator 3))\n[    0.645406][    T2] softirqs last  enabled at (54): handle_softirqs (kernel/softirq.c:469 (discriminator 1) kernel/softirq.c:650 (discriminator 1))\n[    0.646077][    T2] softirqs last disabled at (49): __irq_exit_rcu (kernel/softirq.c:657 kernel/softirq.c:496 kernel/softirq.c:723)\n[    0.646741][    T2] ---[ end trace 0000000000000000 ]---\n\n\nThe kernel config and materials to reproduce are available at:\nhttps://download.01.org/0day-ci/archive/20260218/202602181136.f66ba888-lkp@intel.com\n\n\n\n-- \n0-DAY CI Kernel Test Service\nhttps://github.com/intel/lkp-tests/wiki",
              "reply_to": "JP Kobryn",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v2] mm: move pgscan and pgsteal to node stats",
          "message_id": "2397f4cd-367b-421c-b4f0-9c023f6cd546@linux.dev",
          "url": "https://lore.kernel.org/all/2397f4cd-367b-421c-b4f0-9c023f6cd546@linux.dev/",
          "date": "2026-02-18T19:53:31Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Michael Tsirkin",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-18",
              "analysis_source": "heuristic"
            },
            {
              "author": "Michal Hocko",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-18",
              "analysis_source": "heuristic"
            },
            {
              "author": "Vlastimil (SUSE)",
              "summary": "Reviewer noted that PGREFILL is the only counter not being updated to be per-node, and suggested making it consistent by also updating its behavior to be per-node and gated on !cgroup_reclaim() for global stats.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "inconsistency",
                "requested change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hm so that leaves PGREFILL (scanned in the active list) the odd one out,\nright? Not being per-node and gated on !cgroup_reclaim() for global stats.\nShould we change it too for full consistency?",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "JP (Meta) (author)",
              "summary": "Author agreed to add counters for the active list side and suggested including PGDEACTIVATE for completeness.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed_to_add_counters",
                "suggested_inclusion_of_PGDEACTIVATE"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm fine with adding coverage for the active list side as well. For\ncompleteness, I could also include PGDEACTIVATE.",
              "reply_to": "Vlastimil (SUSE)",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "JP (Meta) (author)",
              "summary": "Author acknowledged a mistake in the original patch where PGDEACTIVATE was not gated, agreed to restructure in v3 by including PGREFILL.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a mistake",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Actually, I see PGDEACTIVATE is not gated so I'll leave that one out.\nI'll send v3 and include PGREFILL.",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v2] mm: move pgscan and pgsteal to node stats",
          "message_id": "fbcd6770-f555-443c-b5f2-fe5e73722118@linux.dev",
          "url": "https://lore.kernel.org/all/fbcd6770-f555-443c-b5f2-fe5e73722118@linux.dev/",
          "date": "2026-02-18T18:02:41Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Michael Tsirkin",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-18",
              "analysis_source": "heuristic"
            },
            {
              "author": "Michal Hocko",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-18",
              "analysis_source": "heuristic"
            },
            {
              "author": "Vlastimil (SUSE)",
              "summary": "Reviewer noted that PGREFILL counter is inconsistent with the rest, as it's not per-node and doesn't follow the same logic for global statistics, and suggested making it consistent by changing its behavior to be per-node and gated on !cgroup_reclaim()",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "inconsistency",
                "requested change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hm so that leaves PGREFILL (scanned in the active list) the odd one out,\nright? Not being per-node and gated on !cgroup_reclaim() for global stats.\nShould we change it too for full consistency?",
              "reply_to": "JP (Meta)",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "JP (Meta) (author)",
              "summary": "Author agreed to add counters for the active list side and suggested including PGDEACTIVATE for completeness.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed_to_add_counters",
                "suggested_inclusion_of_PGDEACTIVATE"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm fine with adding coverage for the active list side as well. For\ncompleteness, I could also include PGDEACTIVATE.",
              "reply_to": "Vlastimil (SUSE)",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "JP (Meta) (author)",
              "summary": "Author acknowledged a mistake in the original patch, specifically excluding PGDEACTIVATE from being gated, and plans to fix it in v3 by including PGREFILL.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a mistake",
                "plans to fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Actually, I see PGDEACTIVATE is not gated so I'll leave that one out.\nI'll send v3 and include PGREFILL.",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Kiryl Shutsemau",
      "primary_email": "kas@kernel.org",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Leo Martins",
      "primary_email": "loemra.dev@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Mark Harmstone",
      "primary_email": "mark@harmstone.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix chunk map leaks in btrfs_map_block()",
          "message_id": "20260218143334.25014-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260218143334.25014-1-mark@harmstone.com/",
          "date": "2026-02-18T14:33:40Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch fixes two early returns in btrfs_map_block() that could lead to chunk map leaks, ensuring the chunk map is properly put after getting it.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Recommended adding Fixes tags to identify the commits that introduced the leaks, and suggested splitting the patch into two separate patches for easier backporting of the second leak.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Wed, Feb 18, 2026 at 2:33PM Mark Harmstone <mark@harmstone.com> wrote:\n>\n> Fix the two early returns in btrfs_map_block() so that we can no longer\n> fail to put the chunk map after getting it.\n>\n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Reported-by: Chris Mason <clm@fb.com>\n\nSo being a bug fix, this is the type of patch that should have a Fixes tag.\n\nThe commit that introduced the first leak is not in any released\nkernel, so no stable releases are affected.\nHowever it's still useful to have the Fixes tag here, because in case\nsomeone decides to backport the offending commit, either upstream or\ndownstream, there are scripts in place to check if there are newer\ncommits that fix a bug in the former commit and therefore must be\nbackported as well.\n\nBut the second leak was introduced in a much older commit, from 2024,\nand should be backported.\nSe comments inline below.\n\nAlso, since this was reported publicly, please add a Link tag pointing\nto the URL with the review.\n\n> ---\n>  fs/btrfs/volumes.c | 8 +++++---\n>  1 file changed, 5 insertions(+), 3 deletions(-)\n>\n> diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\n> index 83e2834ea273..a1f0fccd552c 100644\n> --- a/fs/btrfs/volumes.c\n> +++ b/fs/btrfs/volumes.c\n> @@ -7082,7 +7082,7 @@ int btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n>\n>                 ret = btrfs_translate_remap(fs_info, &new_logical, length);\n>                 if (ret)\n> -                       return ret;\n> +                       goto out;\n\nFor this hunk:\n\nFixes: 18ba64992871 (\"btrfs: redirect I/O for remapped block groups\")\n\n>\n>                 if (new_logical != logical) {\n>                         btrfs_free_chunk_map(map);\n> @@ -7096,8 +7096,10 @@ int btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n>         }\n>\n>         num_copies = btrfs_chunk_map_num_copies(map);\n> -       if (io_geom.mirror_num > num_copies)\n> -               return -EINVAL;\n> +       if (io_geom.mirror_num > num_copies) {\n> +               ret = -EINVAL;\n> +               goto out;\n> +       }\n\nFor this hunk:\n\nFixes: 0ae653fbec2b (\"btrfs: reduce chunk_map lookups in btrfs_map_block()\")\n\nI would suggest splitting this into 2 different patches, each one with\nthe respective Fixes tag so that the second leak can be backported\nmore easily.\n\n>\n>         map_offset = logical - map->start;\n>         io_geom.raid56_full_stripe_start = (u64)-1;\n> --\n> 2.52.0\n>\n>\n",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix unlikely in btrfs_insert_one_raid_extent()",
          "message_id": "20260218130006.9563-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260218130006.9563-1-mark@harmstone.com/",
          "date": "2026-02-18T13:00:13Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch fixes an unlikely annotation in btrfs_insert_one_raid_extent() by correcting the placement of the exclamation point, which was incorrectly indicating that allocation failure is expected.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Raised concern that the Fixes tag is not suitable for this patch, as it's not a bug fix or severe performance regression. Requested removal of the tag.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_REVISION"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Wed, Feb 18, 2026 at 1:01PM Mark Harmstone <mark@harmstone.com> wrote:\n>\n> Fix the unlikely added to btrfs_insert_one_raid_extent() by commit\n> a929904c: the exclamation point is in the wrong place, so we are telling\n> the compiler that allocation failure is actually expected.\n>\n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: a929904cf73b (\"btrfs: add unlikely annotations to branches leading to transaction abort\")\n\nWe use the Fixes tag for things that must be backported, which are\naither bug fixes or rather severe performance regressions (i.e. things\nthat affect users), and this doesn't fit into these categories.\n\nOtherwise:\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\n\n> ---\n>  fs/btrfs/raid-stripe-tree.c | 2 +-\n>  1 file changed, 1 insertion(+), 1 deletion(-)\n>\n> diff --git a/fs/btrfs/raid-stripe-tree.c b/fs/btrfs/raid-stripe-tree.c\n> index 2987cb7c686e..638c4ad572c9 100644\n> --- a/fs/btrfs/raid-stripe-tree.c\n> +++ b/fs/btrfs/raid-stripe-tree.c\n> @@ -300,7 +300,7 @@ int btrfs_insert_one_raid_extent(struct btrfs_trans_handle *trans,\n>         int ret;\n>\n>         stripe_extent = kzalloc(item_size, GFP_NOFS);\n> -       if (!unlikely(stripe_extent)) {\n> +       if (unlikely(!stripe_extent)) {\n>                 btrfs_abort_transaction(trans, -ENOMEM);\n>                 btrfs_end_transaction(trans);\n>                 return -ENOMEM;\n> --\n> 2.52.0\n>\n>\n",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone",
              "summary": "Agreed to remove the Fixes tag and push an updated version of the patch.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On 18/02/2026 2.08 pm, Filipe Manana wrote:\n> On Wed, Feb 18, 2026 at 1:01\\u202fPM Mark Harmstone <mark@harmstone.com> wrote:\n>>\n>> Fix the unlikely added to btrfs_insert_one_raid_extent() by commit\n>> a929904c: the exclamation point is in the wrong place, so we are telling\n>> the compiler that allocation failure is actually expected.\n>>\n>> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n>> Fixes: a929904cf73b (\"btrfs: add unlikely annotations to branches leading to transaction abort\")\n> \n> We use the Fixes tag for things that must be backported, which are\n> aither bug fixes or rather severe performance regressions (i.e. things\n> that affect users), and this doesn't fit into these categories.\n\nOkay - thanks Filipe, I'll push it with the Fixes line taken out.\n\n> Otherwise:\n> \n> Reviewed-by: Filipe Manana <fdmanana@suse.com>\n> \n> \n>> ---\n>>   fs/btrfs/raid-stripe-tree.c | 2 +-\n>>   1 file changed, 1 insertion(+), 1 deletion(-)\n>>\n>> diff --git a/fs/btrfs/raid-stripe-tree.c b/fs/btrfs/raid-stripe-tree.c\n>> index 2987cb7c686e..638c4ad572c9 100644\n>> --- a/fs/btrfs/raid-stripe-tree.c\n>> +++ b/fs/btrfs/raid-stripe-tree.c\n>> @@ -300,7 +300,7 @@ int btrfs_insert_one_raid_extent(struct btrfs_trans_handle *trans,\n>>          int ret;\n>>\n>>          stripe_extent = kzalloc(item_size, GFP_NOFS);\n>> -       if (!unlikely(stripe_extent)) {\n>> +       if (unlikely(!stripe_extent)) {\n>>                  btrfs_abort_transaction(trans, -ENOMEM);\n>>                  btrfs_end_transaction(trans);\n>>                  return -ENOMEM;\n>> --\n>> 2.52.0\n>>\n>>\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: print correct subvol num if can't delete because of active swapfile",
          "message_id": "20260218120322.327-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260218120322.327-1-mark@harmstone.com/",
          "date": "2026-02-18T12:03:29Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch fixes an error message in btrfs_delete_subvolume() to correctly print the number of the target subvolume when it cannot be deleted due to an active swapfile.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Approved the patch with a Reviewed-by tag, indicating it looks good to them.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Wed, Feb 18, 2026 at 12:03PM Mark Harmstone <mark@harmstone.com> wrote:\n>\n> Fix the error message in btrfs_delete_subvolume() if we can't delete a\n> subvolume because it has an active swapfile: we were printing the number\n> of the parent rather than the target.\n>\n> Fixes: 60021bd754c6 (\"btrfs: prevent subvol with swapfile from being deleted\")\n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\nLooks good, thanks.\n\n> ---\n>  fs/btrfs/inode.c | 2 +-\n>  1 file changed, 1 insertion(+), 1 deletion(-)\n>\n> diff --git a/fs/btrfs/inode.c b/fs/btrfs/inode.c\n> index 4523b689711d..233d91556fe4 100644\n> --- a/fs/btrfs/inode.c\n> +++ b/fs/btrfs/inode.c\n> @@ -4804,7 +4804,7 @@ int btrfs_delete_subvolume(struct btrfs_inode *dir, struct dentry *dentry)\n>                 spin_unlock(&dest->root_item_lock);\n>                 btrfs_warn(fs_info,\n>                            \"attempt to delete subvolume %llu with active swapfile\",\n> -                          btrfs_root_id(root));\n> +                          btrfs_root_id(dest));\n>                 ret = -EPERM;\n>                 goto out_up_write;\n>         }\n> --\n> 2.52.0\n>\n>\n",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Qu Wenruo",
              "summary": "Approved the patch with a Reviewed-by tag, indicating it looks good to them.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\n 2026/2/18 22:33, Mark Harmstone :\n> Fix the error message in btrfs_delete_subvolume() if we can't delete a\n> subvolume because it has an active swapfile: we were printing the number\n> of the parent rather than the target.\n> \n> Fixes: 60021bd754c6 (\"btrfs: prevent subvol with swapfile from being deleted\")\n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\n\nThanks,\nQu\n\n> ---\n>   fs/btrfs/inode.c | 2 +-\n>   1 file changed, 1 insertion(+), 1 deletion(-)\n> \n> diff --git a/fs/btrfs/inode.c b/fs/btrfs/inode.c\n> index 4523b689711d..233d91556fe4 100644\n> --- a/fs/btrfs/inode.c\n> +++ b/fs/btrfs/inode.c\n> @@ -4804,7 +4804,7 @@ int btrfs_delete_subvolume(struct btrfs_inode *dir, struct dentry *dentry)\n>   \t\tspin_unlock(&dest->root_item_lock);\n>   \t\tbtrfs_warn(fs_info,\n>   \t\t\t   \"attempt to delete subvolume %llu with active swapfile\",\n> -\t\t\t   btrfs_root_id(root));\n> +\t\t\t   btrfs_root_id(dest));\n>   \t\tret = -EPERM;\n>   \t\tgoto out_up_write;\n>   \t}\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix error messages in btrfs_check_features()",
          "message_id": "20260218111346.31243-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260218111346.31243-1-mark@harmstone.com/",
          "date": "2026-02-18T11:14:00Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch fixes a regression introduced by commit d7f67ac9 in the btrfs_check_features() function, which incorrectly printed all unsupported incompat and compat_ro flags instead of just the unrecognized ones.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Qu Wenruo",
              "summary": "Gave a Reviewed-by tag without providing any specific feedback or suggestions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\n\\u5728 2026/2/18 21:43, Mark Harmstone \\u5199\\u9053:\n> Commit d7f67ac9 introduced a regression when it comes to handling\n> unsupported incompat or compat_ro flags. Beforehand we only printed the\n> flags that we didn't recognize, afterwards we printed them all, which is\n> less useful. Fix the error handling so it behaves like it used to.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: d7f67ac9a928 (\"btrfs: relax block-group-tree feature dependency checks\")\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\n\nThanks,\nQu\n\n> ---\n>   fs/btrfs/disk-io.c | 6 +++---\n>   1 file changed, 3 insertions(+), 3 deletions(-)\n> \n> diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c\n> index f39008591631..7478d1c50cca 100644\n> --- a/fs/btrfs/disk-io.c\n> +++ b/fs/btrfs/disk-io.c\n> @@ -3176,7 +3176,7 @@ int btrfs_check_features(struct btrfs_fs_info *fs_info, bool is_rw_mount)\n>   \tif (incompat & ~BTRFS_FEATURE_INCOMPAT_SUPP) {\n>   \t\tbtrfs_err(fs_info,\n>   \t\t\"cannot mount because of unknown incompat features (0x%llx)\",\n> -\t\t    incompat);\n> +\t\t    incompat & ~BTRFS_FEATURE_INCOMPAT_SUPP);\n>   \t\treturn -EINVAL;\n>   \t}\n>   \n> @@ -3208,7 +3208,7 @@ int btrfs_check_features(struct btrfs_fs_info *fs_info, bool is_rw_mount)\n>   \tif (compat_ro_unsupp && is_rw_mount) {\n>   \t\tbtrfs_err(fs_info,\n>   \t\"cannot mount read-write because of unknown compat_ro features (0x%llx)\",\n> -\t\t       compat_ro);\n> +\t\t       compat_ro_unsupp);\n>   \t\treturn -EINVAL;\n>   \t}\n>   \n> @@ -3221,7 +3221,7 @@ int btrfs_check_features(struct btrfs_fs_info *fs_info, bool is_rw_mount)\n>   \t    !btrfs_test_opt(fs_info, NOLOGREPLAY)) {\n>   \t\tbtrfs_err(fs_info,\n>   \"cannot replay dirty log with unsupported compat_ro features (0x%llx), try rescue=nologreplay\",\n> -\t\t\t  compat_ro);\n> +\t\t\t  compat_ro_unsupp);\n>   \t\treturn -EINVAL;\n>   \t}\n>   \n\n\n",
              "reply_to": "",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix warning in scrub_verify_one_metadata()",
          "message_id": "20260217190238.22006-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260217190238.22006-1-mark@harmstone.com/",
          "date": "2026-02-17T19:02:42Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-17",
          "patch_summary": "This patch fixes a warning in the btrfs scrub code by updating the comparison to use metadata_uuid instead of fsid, matching the change made in commit b471965f.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Qu Wenruo",
              "summary": "Approved the patch and added a Reviewed-by tag.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "APPROVED"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\n 2026/2/18 05:32, Mark Harmstone :\n> Commit b471965f fixed the comparison in scrub_verify_one_metadata to use\n> metadata_uuid rather than fsid, but left the warning as it was. Fix it\n> so it matches what we're doing.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: b471965fdb2d (\"btrfs: fix replace/scrub failure with metadata_uuid\")\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\n\nThanks,\nQu\n\n> ---\n>   fs/btrfs/scrub.c | 2 +-\n>   1 file changed, 1 insertion(+), 1 deletion(-)\n> \n> diff --git a/fs/btrfs/scrub.c b/fs/btrfs/scrub.c\n> index 2a64e2d50ced..052d83feea9a 100644\n> --- a/fs/btrfs/scrub.c\n> +++ b/fs/btrfs/scrub.c\n> @@ -744,7 +744,7 @@ static void scrub_verify_one_metadata(struct scrub_stripe *stripe, int sector_nr\n>   \t\tbtrfs_warn_rl(fs_info,\n>   \t      \"scrub: tree block %llu mirror %u has bad fsid, has %pU want %pU\",\n>   \t\t\t      logical, stripe->mirror_num,\n> -\t\t\t      header->fsid, fs_info->fs_devices->fsid);\n> +\t\t\t      header->fsid, fs_info->fs_devices->metadata_uuid);\n>   \t\treturn;\n>   \t}\n>   \tif (memcmp(header->chunk_tree_uuid, fs_info->chunk_tree_uuid,\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix error message in btrfs_validate_super()",
          "message_id": "20260217185335.21013-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260217185335.21013-1-mark@harmstone.com/",
          "date": "2026-02-17T18:54:05Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-17",
          "patch_summary": "The patch fixes an error message in btrfs_validate_super() by updating the format string to match the new u64 constant used for superblock offset calculations.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Qu Wenruo",
              "summary": "Applied the patch and added a formal Reviewed-by tag.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\n 2026/2/18 05:23, Mark Harmstone :\n> Fix the superblock offset mismatch error message in\n> btrfs_validate_super(): we changed it so that it considers all the\n> superblocks, but the message still assumes we're only looking at the\n> first one.\n> \n> The change from %u to %llu is because we're changing from a constant to\n> a u64.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 069ec957c35e (\"btrfs: Refactor btrfs_check_super_valid\")\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\n\nThanks,\nQu\n\n> ---\n>   fs/btrfs/disk-io.c | 4 ++--\n>   1 file changed, 2 insertions(+), 2 deletions(-)\n> \n> diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c\n> index 600287ac8eb7..f39008591631 100644\n> --- a/fs/btrfs/disk-io.c\n> +++ b/fs/btrfs/disk-io.c\n> @@ -2533,8 +2533,8 @@ int btrfs_validate_super(const struct btrfs_fs_info *fs_info,\n>   \n>   \tif (unlikely(mirror_num >= 0 &&\n>   \t\t     btrfs_super_bytenr(sb) != btrfs_sb_offset(mirror_num))) {\n> -\t\tbtrfs_err(fs_info, \"super offset mismatch %llu != %u\",\n> -\t\t\t  btrfs_super_bytenr(sb), BTRFS_SUPER_INFO_OFFSET);\n> +\t\tbtrfs_err(fs_info, \"super offset mismatch %llu != %llu\",\n> +\t\t\t  btrfs_super_bytenr(sb), btrfs_sb_offset(mirror_num));\n>   \t\tret = -EINVAL;\n>   \t}\n>   \n\n\n",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix error message in check_extent_data_ref()",
          "message_id": "20260217180933.15805-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260217180933.15805-1-mark@harmstone.com/",
          "date": "2026-02-17T18:09:42Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-17",
          "patch_summary": "This patch fixes a copy-paste error in the btrfs tree-checker, correcting an error message to print the correct object ID instead of 'root'. The fix is applied to the check_extent_data_ref() function.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Qu Wenruo",
              "summary": "Approved the patch, noting that it fixes a copy-paste error in the tree-checker's error message. The fix is correct and should be applied.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\n 2026/2/18 04:39, Mark Harmstone :\n> Fix a copy-paste error in check_extent_data_ref(): we're printing root\n> as in the message above, we should be printing objectid.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: f333a3c7e832 (\"btrfs: tree-checker: validate dref root and objectid\")\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\n\nThanks,\nQu\n\n> ---\n>   fs/btrfs/tree-checker.c | 2 +-\n>   1 file changed, 1 insertion(+), 1 deletion(-)\n> \n> diff --git a/fs/btrfs/tree-checker.c b/fs/btrfs/tree-checker.c\n> index 9774779f060b..ac4c4573ee39 100644\n> --- a/fs/btrfs/tree-checker.c\n> +++ b/fs/btrfs/tree-checker.c\n> @@ -1740,7 +1740,7 @@ static int check_extent_data_ref(struct extent_buffer *leaf,\n>   \t\t\t     objectid > BTRFS_LAST_FREE_OBJECTID)) {\n>   \t\t\textent_err(leaf, slot,\n>   \t\t\t\t   \"invalid extent data backref objectid value %llu\",\n> -\t\t\t\t   root);\n> +\t\t\t\t   objectid);\n>   \t\t\treturn -EUCLEAN;\n>   \t\t}\n>   \t\tif (unlikely(!IS_ALIGNED(offset, leaf->fs_info->sectorsize))) {\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix incorrect error message in check_dev_extent_item()",
          "message_id": "20260217103419.19609-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260217103419.19609-1-mark@harmstone.com/",
          "date": "2026-02-17T10:40:48Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-17",
          "patch_summary": "This patch fixes an incorrect error message in the btrfs tree-checker's check_dev_extent_item() function, which occurs when an overlapping stripe is encountered. The fix involves changing the comparison of objectid and offset to use the correct field (offset) for prev_key.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Boris Burkov",
              "summary": "Approved the patch with a Reviewed-by tag.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "APPROVED"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Tue, Feb 17, 2026 at 10:34:08AM +0000, Mark Harmstone wrote:\n> Fix the error message in check_dev_extent_item(), when an overlapping\n> stripe is encountered. For dev extents, objectid is the disk number and\n> offset the physical address, so prev_key->objectid should actually be\n> prev_key->offset.\n> \n> (I can't take any credit for this one - this was discovered by Chris and\n> his friend Claude.)\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Suggested-by: Chris Mason <clm@fb.com>\n\nReviewed-by: Boris Burkov <boris@bur.io>\n\n> Fixes: 008e2512dc56 (\"btrfs: tree-checker: add dev extent item checks\")\n> ---\n>  fs/btrfs/tree-checker.c | 2 +-\n>  1 file changed, 1 insertion(+), 1 deletion(-)\n> \n> diff --git a/fs/btrfs/tree-checker.c b/fs/btrfs/tree-checker.c\n> index 452394b34d01..9774779f060b 100644\n> --- a/fs/btrfs/tree-checker.c\n> +++ b/fs/btrfs/tree-checker.c\n> @@ -1921,7 +1921,7 @@ static int check_dev_extent_item(const struct extent_buffer *leaf,\n>  \t\tif (unlikely(prev_key->offset + prev_len > key->offset)) {\n>  \t\t\tgeneric_err(leaf, slot,\n>  \t\t\"dev extent overlap, prev offset %llu len %llu current offset %llu\",\n> -\t\t\t\t    prev_key->objectid, prev_len, key->offset);\n> +\t\t\t\t    prev_key->offset, prev_len, key->offset);\n>  \t\t\treturn -EUCLEAN;\n>  \t\t}\n>  \t}\n> -- \n> 2.52.0\n> \n",
              "reply_to": "",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Qu Wenruo",
              "summary": "Approved the patch with a Reviewed-by tag.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "APPROVED"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\n 2026/2/17 21:04, Mark Harmstone :\n> Fix the error message in check_dev_extent_item(), when an overlapping\n> stripe is encountered. For dev extents, objectid is the disk number and\n> offset the physical address, so prev_key->objectid should actually be\n> prev_key->offset.\n> \n> (I can't take any credit for this one - this was discovered by Chris and\n> his friend Claude.)\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Suggested-by: Chris Mason <clm@fb.com>\n> Fixes: 008e2512dc56 (\"btrfs: tree-checker: add dev extent item checks\")\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\n\nThanks,\nQu\n\n> ---\n>   fs/btrfs/tree-checker.c | 2 +-\n>   1 file changed, 1 insertion(+), 1 deletion(-)\n> \n> diff --git a/fs/btrfs/tree-checker.c b/fs/btrfs/tree-checker.c\n> index 452394b34d01..9774779f060b 100644\n> --- a/fs/btrfs/tree-checker.c\n> +++ b/fs/btrfs/tree-checker.c\n> @@ -1921,7 +1921,7 @@ static int check_dev_extent_item(const struct extent_buffer *leaf,\n>   \t\tif (unlikely(prev_key->offset + prev_len > key->offset)) {\n>   \t\t\tgeneric_err(leaf, slot,\n>   \t\t\"dev extent overlap, prev offset %llu len %llu current offset %llu\",\n> -\t\t\t\t    prev_key->objectid, prev_len, key->offset);\n> +\t\t\t\t    prev_key->offset, prev_len, key->offset);\n>   \t\t\treturn -EUCLEAN;\n>   \t\t}\n>   \t}\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] btrfs: fix chunk map leaks in btrfs_map_block()",
          "message_id": "408293e5-75f8-461b-9d0e-65ff95027a0b@harmstone.com",
          "url": "https://lore.kernel.org/all/408293e5-75f8-461b-9d0e-65ff95027a0b@harmstone.com/",
          "date": "2026-02-18T15:55:21Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Patch fixes two early returns in btrfs_map_block() to prevent chunk map leaks, and adds Fixes tags for the commits that introduced these issues.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Suggested adding Fixes tags to identify the commits that introduced the leaks, and proposed splitting the patch into two separate patches for easier backporting of the second leak.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "WAITING_FOR_REVIEW"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Wed, Feb 18, 2026 at 2:33PM Mark Harmstone <mark@harmstone.com> wrote:\n>\n> Fix the two early returns in btrfs_map_block() so that we can no longer\n> fail to put the chunk map after getting it.\n>\n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Reported-by: Chris Mason <clm@fb.com>\n\nSo being a bug fix, this is the type of patch that should have a Fixes tag.\n\nThe commit that introduced the first leak is not in any released\nkernel, so no stable releases are affected.\nHowever it's still useful to have the Fixes tag here, because in case\nsomeone decides to backport the offending commit, either upstream or\ndownstream, there are scripts in place to check if there are newer\ncommits that fix a bug in the former commit and therefore must be\nbackported as well.\n\nBut the second leak was introduced in a much older commit, from 2024,\nand should be backported.\nSe comments inline below.\n\nAlso, since this was reported publicly, please add a Link tag pointing\nto the URL with the review.\n\n> ---\n>  fs/btrfs/volumes.c | 8 +++++---\n>  1 file changed, 5 insertions(+), 3 deletions(-)\n>\n> diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\n> index 83e2834ea273..a1f0fccd552c 100644\n> --- a/fs/btrfs/volumes.c\n> +++ b/fs/btrfs/volumes.c\n> @@ -7082,7 +7082,7 @@ int btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n>\n>                 ret = btrfs_translate_remap(fs_info, &new_logical, length);\n>                 if (ret)\n> -                       return ret;\n> +                       goto out;\n\nFor this hunk:\n\nFixes: 18ba64992871 (\"btrfs: redirect I/O for remapped block groups\")\n\n>\n>                 if (new_logical != logical) {\n>                         btrfs_free_chunk_map(map);\n> @@ -7096,8 +7096,10 @@ int btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n>         }\n>\n>         num_copies = btrfs_chunk_map_num_copies(map);\n> -       if (io_geom.mirror_num > num_copies)\n> -               return -EINVAL;\n> +       if (io_geom.mirror_num > num_copies) {\n> +               ret = -EINVAL;\n> +               goto out;\n> +       }\n\nFor this hunk:\n\nFixes: 0ae653fbec2b (\"btrfs: reduce chunk_map lookups in btrfs_map_block()\")\n\nI would suggest splitting this into 2 different patches, each one with\nthe respective Fixes tag so that the second leak can be backported\nmore easily.\n\n>\n>         map_offset = logical - map->start;\n>         io_geom.raid56_full_stripe_start = (u64)-1;\n> --\n> 2.52.0\n>\n>\n",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v8 09/17] btrfs: redirect I/O for remapped block groups",
          "message_id": "8369b0f4-f7b7-40ad-8a72-4d7fafd88a84@harmstone.com",
          "url": "https://lore.kernel.org/all/8369b0f4-f7b7-40ad-8a72-4d7fafd88a84@harmstone.com/",
          "date": "2026-02-18T14:29:07Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author addressed the reviewer's concern about adding an incompat flag for the new remap-tree feature, and provided the necessary constants and definitions to support it. The author added a new incompat flag (BTRFS_FEATURE_INCOMPAT_REMAP_TREE) and updated various files to include this flag.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "added an incompat flag",
                "provided necessary constants"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add an incompat flag for the new remap-tree feature, and the constants\nand definitions needed to support it.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/accessors.h            |  4 ++++\n fs/btrfs/locking.c              |  1 +\n fs/btrfs/sysfs.c                |  2 ++\n fs/btrfs/tree-checker.c         |  6 ++----\n fs/btrfs/tree-checker.h         |  5 +++++\n fs/btrfs/volumes.c              |  1 +\n include/uapi/linux/btrfs.h      |  1 +\n include/uapi/linux/btrfs_tree.h | 17 +++++++++++++++++\n 8 files changed, 33 insertions(+), 4 deletions(-)\n\ndiff --git a/fs/btrfs/accessors.h b/fs/btrfs/accessors.h\nindex 78721412951c..09cdd6bfddf5 100644\n--- a/fs/btrfs/accessors.h\n+++ b/fs/btrfs/accessors.h\n@@ -1010,6 +1010,10 @@ BTRFS_SETGET_STACK_FUNCS(stack_verity_descriptor_encryption,\n BTRFS_SETGET_STACK_FUNCS(stack_verity_descriptor_size,\n \t\t\t struct btrfs_verity_descriptor_item, size, 64);\n \n+BTRFS_SETGET_FUNCS(remap_address, struct btrfs_remap_item, address, 64);\n+BTRFS_SETGET_STACK_FUNCS(stack_remap_address, struct btrfs_remap_item,\n+\t\t\t address, 64);\n+\n /* Cast into the data area of the leaf. */\n #define btrfs_item_ptr(leaf, slot, type)\t\t\t\t\\\n \t((type *)(btrfs_item_nr_offset(leaf, 0) + btrfs_item_offset(leaf, slot)))\ndiff --git a/fs/btrfs/locking.c b/fs/btrfs/locking.c\nindex 0035851d72b0..e3df5ca0b552 100644\n--- a/fs/btrfs/locking.c\n+++ b/fs/btrfs/locking.c\n@@ -73,6 +73,7 @@ static struct btrfs_lockdep_keyset {\n \t{ .id = BTRFS_FREE_SPACE_TREE_OBJECTID,\tDEFINE_NAME(\"free-space\") },\n \t{ .id = BTRFS_BLOCK_GROUP_TREE_OBJECTID, DEFINE_NAME(\"block-group\") },\n \t{ .id = BTRFS_RAID_STRIPE_TREE_OBJECTID, DEFINE_NAME(\"raid-stripe\") },\n+\t{ .id = BTRFS_REMAP_TREE_OBJECTID,      DEFINE_NAME(\"remap\") },\n \t{ .id = 0,\t\t\t\tDEFINE_NAME(\"tree\")\t},\n };\n \ndiff --git a/fs/btrfs/sysfs.c b/fs/btrfs/sysfs.c\nindex f0974f4c0ae4..0e2ed8072443 100644\n--- a/fs/btrfs/sysfs.c\n+++ b/fs/btrfs/sysfs.c\n@@ -291,6 +291,7 @@ BTRFS_FEAT_ATTR_COMPAT_RO(free_space_tree, FREE_SPACE_TREE);\n BTRFS_FEAT_ATTR_COMPAT_RO(block_group_tree, BLOCK_GROUP_TREE);\n BTRFS_FEAT_ATTR_INCOMPAT(raid1c34, RAID1C34);\n BTRFS_FEAT_ATTR_INCOMPAT(simple_quota, SIMPLE_QUOTA);\n+BTRFS_FEAT_ATTR_INCOMPAT(remap_tree, REMAP_TREE);\n #ifdef CONFIG_BLK_DEV_ZONED\n BTRFS_FEAT_ATTR_INCOMPAT(zoned, ZONED);\n #endif\n@@ -331,6 +332,7 @@ static struct attribute *btrfs_supported_feature_attrs[] = {\n #ifdef CONFIG_BTRFS_EXPERIMENTAL\n \tBTRFS_FEAT_ATTR_PTR(extent_tree_v2),\n \tBTRFS_FEAT_ATTR_PTR(raid_stripe_tree),\n+\tBTRFS_FEAT_ATTR_PTR(remap_tree),\n #endif\n #ifdef CONFIG_FS_VERITY\n \tBTRFS_FEAT_ATTR_PTR(verity),\ndiff --git a/fs/btrfs/tree-checker.c b/fs/btrfs/tree-checker.c\nindex c21c21adf61e..aedc208a95b8 100644\n--- a/fs/btrfs/tree-checker.c\n+++ b/fs/btrfs/tree-checker.c\n@@ -913,12 +913,10 @@ int btrfs_check_chunk_valid(const struct btrfs_fs_info *fs_info,\n \t\t\t  length, btrfs_stripe_nr_to_offset(U32_MAX));\n \t\treturn -EUCLEAN;\n \t}\n-\tif (unlikely(type & ~(BTRFS_BLOCK_GROUP_TYPE_MASK |\n-\t\t\t      BTRFS_BLOCK_GROUP_PROFILE_MASK))) {\n+\tif (unlikely(type & ~BTRFS_BLOCK_GROUP_VALID)) {\n \t\tchunk_err(fs_info, leaf, chunk, logical,\n \t\t\t  \"unrecognized chunk type: 0x%llx\",\n-\t\t\t  ~(BTRFS_BLOCK_GROUP_TYPE_MASK |\n-\t\t\t    BTRFS_BLOCK_GROUP_PROFILE_MASK) & type);\n+\t\t\t  type & ~BTRFS_BLOCK_GROUP_VALID);\n \t\treturn -EUCLEAN;\n \t}\n \ndiff --git a/fs/btrfs/tree-checker.h b/fs/btrfs/tree-checker.h\nindex eb201f4ec3c7..833e2fd989eb 100644\n--- a/fs/btrfs/tree-checker.h\n+++ b/fs/btrfs/tree-checker.h\n@@ -57,6 +57,11 @@ enum btrfs_tree_block_status {\n \tBTRFS_TREE_BLOCK_WRITTEN_NOT_SET,\n };\n \n+\n+#define BTRFS_BLOCK_GROUP_VALID\t(BTRFS_BLOCK_GROUP_TYPE_MASK | \\\n+\t\t\t\t BTRFS_BLOCK_GROUP_PROFILE_MASK | \\\n+\t\t\t\t BTRFS_BLOCK_GROUP_REMAPPED)\n+\n /*\n  * Exported simply for btrfs-progs which wants to have the\n  * btrfs_tree_block_status return codes.\ndiff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\nindex ce0535c0264d..1134474926ff 100644\n--- a/fs/btrfs/volumes.c\n+++ b/fs/btrfs/volumes.c\n@@ -231,6 +231,7 @@ void btrfs_describe_block_groups(u64 bg_flags, char *buf, u32 size_buf)\n \tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_DATA, \"data\");\n \tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_SYSTEM, \"system\");\n \tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_METADATA, \"metadata\");\n+\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_REMAPPED, \"remapped\");\n \n \tDESCRIBE_FLAG(BTRFS_AVAIL_ALLOC_BIT_SINGLE, \"single\");\n \tfor (i = 0; i < BTRFS_NR_RAID_TYPES; i++)\ndiff --git a/include/uapi/linux/btrfs.h b/include/uapi/linux/btrfs.h\nindex e8fd92789423..9165154a274d 100644\n--- a/include/uapi/linux/btrfs.h\n+++ b/include/uapi/linux/btrfs.h\n@@ -336,6 +336,7 @@ struct btrfs_ioctl_fs_info_args {\n #define BTRFS_FEATURE_INCOMPAT_EXTENT_TREE_V2\t(1ULL << 13)\n #define BTRFS_FEATURE_INCOMPAT_RAID_STRIPE_TREE\t(1ULL << 14)\n #define BTRFS_FEATURE_INCOMPAT_SIMPLE_QUOTA\t(1ULL << 16)\n+#define BTRFS_FEATURE_INCOMPAT_REMAP_TREE\t(1ULL << 17)\n \n struct btrfs_ioctl_feature_flags {\n \t__u64 compat_flags;\ndiff --git a/include/uapi/linux/btrfs_tree.h b/include/uapi/linux/btrfs_tree.h\nindex fc29d273845d..f011d34cb699 100644\n--- a/include/uapi/linux/btrfs_tree.h\n+++ b/include/uapi/linux/btrfs_tree.h\n@@ -76,6 +76,9 @@\n /* Tracks RAID stripes in block groups. */\n #define BTRFS_RAID_STRIPE_TREE_OBJECTID 12ULL\n \n+/* Holds details of remapped addresses after relocation. */\n+#define BTRFS_REMAP_TREE_OBJECTID 13ULL\n+\n /* device stats in the device tree */\n #define BTRFS_DEV_STATS_OBJECTID 0ULL\n \n@@ -282,6 +285,10 @@\n \n #define BTRFS_RAID_STRIPE_KEY\t230\n \n+#define BTRFS_IDENTITY_REMAP_KEY \t234\n+#define BTRFS_REMAP_KEY\t\t \t235\n+#define BTRFS_REMAP_BACKREF_KEY\t \t236\n+\n /*\n  * Records the overall state of the qgroups.\n  * There's only one instance of this key present,\n@@ -1161,6 +1168,7 @@ struct btrfs_dev_replace_item {\n #define BTRFS_BLOCK_GROUP_RAID6         (1ULL << 8)\n #define BTRFS_BLOCK_GROUP_RAID1C3       (1ULL << 9)\n #define BTRFS_BLOCK_GROUP_RAID1C4       (1ULL << 10)\n+#define BTRFS_BLOCK_GROUP_REMAPPED      (1ULL << 11)\n #define BTRFS_BLOCK_GROUP_RESERVED\t(BTRFS_AVAIL_ALLOC_BIT_SINGLE | \\\n \t\t\t\t\t BTRFS_SPACE_INFO_GLOBAL_RSV)\n \n@@ -1323,4 +1331,13 @@ struct btrfs_verity_descriptor_item {\n \t__u8 encryption;\n } __attribute__ ((__packed__));\n \n+/*\n+ * For a range identified by a BTRFS_REMAP_KEY item in the remap tree, gives\n+ * the address that the start of the range will get remapped to.  This\n+ * structure is also shared by BTRFS_REMAP_BACKREF_KEY.\n+ */\n+struct btrfs_remap_item {\n+\t__le64 address;\n+} __attribute__ ((__packed__));\n+\n #endif /* _BTRFS_CTREE_H_ */\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author addressed a concern about the remap tree's storage requirements and explained that it needs to be stored in a separate chunk type due to space limitations in the system chunk, which would require copying the chunk item into the superblock.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "explained reasoning",
                "acknowledged concern"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a new METADATA_REMAP chunk type, which is a metadata chunk that holds the\nremap tree.\n\nThis is needed for bootstrapping purposes: the remap tree can't itself\nbe remapped, and must be relocated the existing way, by COWing every\nleaf. The remap tree can't go in the SYSTEM chunk as space there is\nlimited, because a copy of the chunk item gets placed in the superblock.\n\nThe changes in fs/btrfs/volumes.h are because we're adding a new block\ngroup type bit after the profile bits, and so can no longer rely on the\nconst_ilog2 trick.\n\nThe sizing to 32MB per chunk, matching the SYSTEM chunk, is an estimate\nhere, we can adjust it later if it proves to be too big or too small.\nThis works out to be ~500,000 remap items, which for a 4KB block size\ncovers ~2GB of remapped data in the worst case and ~500TB in the best case.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/block-rsv.c            |  9 +++++++++\n fs/btrfs/block-rsv.h            |  1 +\n fs/btrfs/disk-io.c              |  1 +\n fs/btrfs/fs.h                   |  2 ++\n fs/btrfs/space-info.c           | 13 ++++++++++++-\n fs/btrfs/sysfs.c                |  2 ++\n fs/btrfs/tree-checker.c         | 13 +++++++++++--\n fs/btrfs/volumes.c              |  3 +++\n fs/btrfs/volumes.h              | 10 +++++++++-\n include/uapi/linux/btrfs_tree.h |  4 +++-\n 10 files changed, 53 insertions(+), 5 deletions(-)\n\ndiff --git a/fs/btrfs/block-rsv.c b/fs/btrfs/block-rsv.c\nindex 96cf7a162987..2781abf18f26 100644\n--- a/fs/btrfs/block-rsv.c\n+++ b/fs/btrfs/block-rsv.c\n@@ -419,6 +419,9 @@ void btrfs_init_root_block_rsv(struct btrfs_root *root)\n \tcase BTRFS_TREE_LOG_OBJECTID:\n \t\troot->block_rsv = &fs_info->treelog_rsv;\n \t\tbreak;\n+\tcase BTRFS_REMAP_TREE_OBJECTID:\n+\t\troot->block_rsv = &fs_info->remap_block_rsv;\n+\t\tbreak;\n \tdefault:\n \t\troot->block_rsv = NULL;\n \t\tbreak;\n@@ -432,6 +435,10 @@ void btrfs_init_global_block_rsv(struct btrfs_fs_info *fs_info)\n \tspace_info = btrfs_find_space_info(fs_info, BTRFS_BLOCK_GROUP_SYSTEM);\n \tfs_info->chunk_block_rsv.space_info = space_info;\n \n+\tspace_info = btrfs_find_space_info(fs_info,\n+\t\t\t\t\t   BTRFS_BLOCK_GROUP_METADATA_REMAP);\n+\tfs_info->remap_block_rsv.space_info = space_info;\n+\n \tspace_info = btrfs_find_space_info(fs_info, BTRFS_BLOCK_GROUP_METADATA);\n \tfs_info->global_block_rsv.space_info = space_info;\n \tfs_info->trans_block_rsv.space_info = space_info;\n@@ -458,6 +465,8 @@ void btrfs_release_global_block_rsv(struct btrfs_fs_info *fs_info)\n \tWARN_ON(fs_info->trans_block_rsv.reserved > 0);\n \tWARN_ON(fs_info->chunk_block_rsv.size > 0);\n \tWARN_ON(fs_info->chunk_block_rsv.reserved > 0);\n+\tWARN_ON(fs_info->remap_block_rsv.size > 0);\n+\tWARN_ON(fs_info->remap_block_rsv.reserved > 0);\n \tWARN_ON(fs_info->delayed_block_rsv.size > 0);\n \tWARN_ON(fs_info->delayed_block_rsv.reserved > 0);\n \tWARN_ON(fs_info->delayed_refs_rsv.reserved > 0);\ndiff --git a/fs/btrfs/block-rsv.h b/fs/btrfs/block-rsv.h\nindex 79ae9d05cd91..8359fb96bc3c 100644\n--- a/fs/btrfs/block-rsv.h\n+++ b/fs/btrfs/block-rsv.h\n@@ -22,6 +22,7 @@ enum btrfs_rsv_type {\n \tBTRFS_BLOCK_RSV_DELALLOC,\n \tBTRFS_BLOCK_RSV_TRANS,\n \tBTRFS_BLOCK_RSV_CHUNK,\n+\tBTRFS_BLOCK_RSV_REMAP,\n \tBTRFS_BLOCK_RSV_DELOPS,\n \tBTRFS_BLOCK_RSV_DELREFS,\n \tBTRFS_BLOCK_RSV_TREELOG,\ndiff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c\nindex cecb81d0f9e0..cbfb7127b528 100644\n--- a/fs/btrfs/disk-io.c\n+++ b/fs/btrfs/disk-io.c\n@@ -2773,6 +2773,7 @@ void btrfs_init_fs_info(struct btrfs_fs_info *fs_info)\n \t\t\t     BTRFS_BLOCK_RSV_GLOBAL);\n \tbtrfs_init_block_rsv(&fs_info->trans_block_rsv, BTRFS_BLOCK_RSV_TRANS);\n \tbtrfs_init_block_rsv(&fs_info->chunk_block_rsv, BTRFS_BLOCK_RSV_CHUNK);\n+\tbtrfs_init_block_rsv(&fs_info->remap_block_rsv, BTRFS_BLOCK_RSV_REMAP);\n \tbtrfs_init_block_rsv(&fs_info->treelog_rsv, BTRFS_BLOCK_RSV_TREELOG);\n \tbtrfs_init_block_rsv(&fs_info->empty_block_rsv, BTRFS_BLOCK_RSV_EMPTY);\n \tbtrfs_init_block_rsv(&fs_info->delayed_block_rsv,\ndiff --git a/fs/btrfs/fs.h b/fs/btrfs/fs.h\nindex 0dc851b9c51b..46c4f1dcec47 100644\n--- a/fs/btrfs/fs.h\n+++ b/fs/btrfs/fs.h\n@@ -501,6 +501,8 @@ struct btrfs_fs_info {\n \tstruct btrfs_block_rsv trans_block_rsv;\n \t/* Block reservation for chunk tree */\n \tstruct btrfs_block_rsv chunk_block_rsv;\n+\t/* Block reservation for remap tree. */\n+\tstruct btrfs_block_rsv remap_block_rsv;\n \t/* Block reservation for delayed operations */\n \tstruct btrfs_block_rsv delayed_block_rsv;\n \t/* Block reservation for delayed refs */\ndiff --git a/fs/btrfs/space-info.c b/fs/btrfs/space-info.c\nindex 7b7b7255f7d8..badebe6e0b34 100644\n--- a/fs/btrfs/space-info.c\n+++ b/fs/btrfs/space-info.c\n@@ -215,7 +215,7 @@ static u64 calc_chunk_size(const struct btrfs_fs_info *fs_info, u64 flags)\n \n \tif (flags & BTRFS_BLOCK_GROUP_DATA)\n \t\treturn BTRFS_MAX_DATA_CHUNK_SIZE;\n-\telse if (flags & BTRFS_BLOCK_GROUP_SYSTEM)\n+\telse if (flags & (BTRFS_BLOCK_GROUP_SYSTEM | BTRFS_BLOCK_GROUP_METADATA_REMAP))\n \t\treturn SZ_32M;\n \n \t/* Handle BTRFS_BLOCK_GROUP_METADATA */\n@@ -344,6 +344,8 @@ int btrfs_init_space_info(struct btrfs_fs_info *fs_info)\n \tif (mixed) {\n \t\tflags = BTRFS_BLOCK_GROUP_METADATA | BTRFS_BLOCK_GROUP_DATA;\n \t\tret = create_space_info(fs_info, flags);\n+\t\tif (ret)\n+\t\t\tgoto out;\n \t} else {\n \t\tflags = BTRFS_BLOCK_GROUP_METADATA;\n \t\tret = create_space_info(fs_info, flags);\n@@ -352,7 +354,15 @@ int btrfs_init_space_info(struct btrfs_fs_info *fs_info)\n \n \t\tflags = BTRFS_BLOCK_GROUP_DATA;\n \t\tret = create_space_info(fs_info, flags);\n+\t\tif (ret)\n+\t\t\tgoto out;\n+\t}\n+\n+\tif (features & BTRFS_FEATURE_INCOMPAT_REMAP_TREE) {\n+\t\tflags = BTRFS_BLOCK_GROUP_METADATA_REMAP;\n+\t\tret = create_space_info(fs_info, flags);\n \t}\n+\n out:\n \treturn ret;\n }\n@@ -607,6 +617,7 @@ static void dump_global_block_rsv(struct btrfs_fs_info *fs_info)\n \tDUMP_BLOCK_RSV(fs_info, global_block_rsv);\n \tDUMP_BLOCK_RSV(fs_info, trans_block_rsv);\n \tDUMP_BLOCK_RSV(fs_info, chunk_block_rsv);\n+\tDUMP_BLOCK_RSV(fs_info, remap_block_rsv);\n \tDUMP_BLOCK_RSV(fs_info, delayed_block_rsv);\n \tDUMP_BLOCK_RSV(fs_info, delayed_refs_rsv);\n }\ndiff --git a/fs/btrfs/sysfs.c b/fs/btrfs/sysfs.c\nindex 0e2ed8072443..0213a3c44628 100644\n--- a/fs/btrfs/sysfs.c\n+++ b/fs/btrfs/sysfs.c\n@@ -1972,6 +1972,8 @@ static const char *alloc_name(struct btrfs_space_info *space_info)\n \tcase BTRFS_BLOCK_GROUP_SYSTEM:\n \t\tASSERT(space_info->subgroup_id == BTRFS_SUB_GROUP_PRIMARY);\n \t\treturn \"system\";\n+\tcase BTRFS_BLOCK_GROUP_METADATA_REMAP:\n+\t\treturn \"metadata-remap\";\n \tdefault:\n \t\tWARN_ON(1);\n \t\treturn \"invalid-combination\";\ndiff --git a/fs/btrfs/tree-checker.c b/fs/btrfs/tree-checker.c\nindex aedc208a95b8..a6c158cd8fcd 100644\n--- a/fs/btrfs/tree-checker.c\n+++ b/fs/btrfs/tree-checker.c\n@@ -748,17 +748,26 @@ static int check_block_group_item(struct extent_buffer *leaf,\n \t\treturn -EUCLEAN;\n \t}\n \n+\tif (unlikely(flags & BTRFS_BLOCK_GROUP_METADATA_REMAP &&\n+\t\t     !btrfs_fs_incompat(fs_info, REMAP_TREE))) {\n+\t\tblock_group_err(leaf, slot,\n+\"invalid flags, have 0x%llx (METADATA_REMAP flag set) but no remap-tree incompat flag\",\n+\t\t\t\tflags);\n+\t\treturn -EUCLEAN;\n+\t}\n+\n \ttype = flags & BTRFS_BLOCK_GROUP_TYPE_MASK;\n \tif (unlikely(type != BTRFS_BLOCK_GROUP_DATA &&\n \t\t     type != BTRFS_BLOCK_GROUP_METADATA &&\n \t\t     type != BTRFS_BLOCK_GROUP_SYSTEM &&\n+\t\t     type != BTRFS_BLOCK_GROUP_METADATA_REMAP &&\n \t\t     type != (BTRFS_BLOCK_GROUP_METADATA |\n \t\t\t      BTRFS_BLOCK_GROUP_DATA))) {\n \t\tblock_group_err(leaf, slot,\n-\"invalid type, have 0x%llx (%lu bits set) expect either 0x%llx, 0x%llx, 0x%llx or 0x%llx\",\n+\"invalid type, have 0x%llx (%lu bits set) expect either 0x%llx, 0x%llx, 0x%llx, 0x%llx or 0x%llx\",\n \t\t\ttype, hweight64(type),\n \t\t\tBTRFS_BLOCK_GROUP_DATA, BTRFS_BLOCK_GROUP_METADATA,\n-\t\t\tBTRFS_BLOCK_GROUP_SYSTEM,\n+\t\t\tBTRFS_BLOCK_GROUP_SYSTEM, BTRFS_BLOCK_GROUP_METADATA_REMAP,\n \t\t\tBTRFS_BLOCK_GROUP_METADATA | BTRFS_BLOCK_GROUP_DATA);\n \t\treturn -EUCLEAN;\n \t}\ndiff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\nindex 1134474926ff..07d42ba38d7d 100644\n--- a/fs/btrfs/volumes.c\n+++ b/fs/btrfs/volumes.c\n@@ -231,6 +231,9 @@ void btrfs_describe_block_groups(u64 bg_flags, char *buf, u32 size_buf)\n \tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_DATA, \"data\");\n \tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_SYSTEM, \"system\");\n \tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_METADATA, \"metadata\");\n+\t/* Block groups containing the remap tree. */\n+\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_METADATA_REMAP, \"metadata-remap\");\n+\t/* Block group that has been remapped. */\n \tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_REMAPPED, \"remapped\");\n \n \tDESCRIBE_FLAG(BTRFS_AVAIL_ALLOC_BIT_SINGLE, \"single\");\ndiff --git a/fs/btrfs/volumes.h b/fs/btrfs/volumes.h\nindex 34b854c1a303..4117fabb248b 100644\n--- a/fs/btrfs/volumes.h\n+++ b/fs/btrfs/volumes.h\n@@ -58,7 +58,6 @@ static_assert(ilog2(BTRFS_STRIPE_LEN) == BTRFS_STRIPE_LEN_SHIFT);\n  */\n static_assert(const_ffs(BTRFS_BLOCK_GROUP_RAID0) <\n \t      const_ffs(BTRFS_BLOCK_GROUP_PROFILE_MASK & ~BTRFS_BLOCK_GROUP_RAID0));\n-static_assert(ilog2(BTRFS_BLOCK_GROUP_RAID0) > ilog2(BTRFS_BLOCK_GROUP_TYPE_MASK));\n \n /* ilog2() can handle both constants and variables */\n #define BTRFS_BG_FLAG_TO_INDEX(profile)\t\t\t\t\t\\\n@@ -80,6 +79,15 @@ enum btrfs_raid_types {\n \tBTRFS_NR_RAID_TYPES\n };\n \n+static_assert(BTRFS_RAID_RAID0 == 1);\n+static_assert(BTRFS_RAID_RAID1 == 2);\n+static_assert(BTRFS_RAID_DUP == 3);\n+static_assert(BTRFS_RAID_RAID10 == 4);\n+static_assert(BTRFS_RAID_RAID5 == 5);\n+static_assert(BTRFS_RAID_RAID6 == 6);\n+static_assert(BTRFS_RAID_RAID1C3 == 7);\n+static_assert(BTRFS_RAID_RAID1C4 == 8);\n+\n /*\n  * Use sequence counter to get consistent device stat data on\n  * 32-bit processors.\ndiff --git a/include/uapi/linux/btrfs_tree.h b/include/uapi/linux/btrfs_tree.h\nindex f011d34cb699..76578426671c 100644\n--- a/include/uapi/linux/btrfs_tree.h\n+++ b/include/uapi/linux/btrfs_tree.h\n@@ -1169,12 +1169,14 @@ struct btrfs_dev_replace_item {\n #define BTRFS_BLOCK_GROUP_RAID1C3       (1ULL << 9)\n #define BTRFS_BLOCK_GROUP_RAID1C4       (1ULL << 10)\n #define BTRFS_BLOCK_GROUP_REMAPPED      (1ULL << 11)\n+#define BTRFS_BLOCK_GROUP_METADATA_REMAP (1ULL << 12)\n #define BTRFS_BLOCK_GROUP_RESERVED\t(BTRFS_AVAIL_ALLOC_BIT_SINGLE | \\\n \t\t\t\t\t BTRFS_SPACE_INFO_GLOBAL_RSV)\n \n #define BTRFS_BLOCK_GROUP_TYPE_MASK\t(BTRFS_BLOCK_GROUP_DATA |    \\\n \t\t\t\t\t BTRFS_BLOCK_GROUP_SYSTEM |  \\\n-\t\t\t\t\t BTRFS_BLOCK_GROUP_METADATA)\n+\t\t\t\t\t BTRFS_BLOCK_GROUP_METADATA | \\\n+\t\t\t\t\t BTRFS_BLOCK_GROUP_METADATA_REMAP)\n \n #define BTRFS_BLOCK_GROUP_PROFILE_MASK\t(BTRFS_BLOCK_GROUP_RAID0 |   \\\n \t\t\t\t\t BTRFS_BLOCK_GROUP_RAID1 |   \\\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author is addressing a concern about the patch series being too large and complex to review in one go, stating that they have broken it down into smaller patches and provided links to previous versions for context.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "provided additional context",
                "acknowledged complexity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This is version 8 of the patch series for the new logical remapping tree\nfeature - see the previous cover letters for more information including\nthe rationale:\n\n* RFC: https://lore.kernel.org/all/20250515163641.3449017-1-maharmstone@fb.com/\n* Version 1: https://lore.kernel.org/all/20250605162345.2561026-1-maharmstone@fb.com/\n* Version 2: https://lore.kernel.org/all/20250813143509.31073-1-mark@harmstone.com/\n* Version 3: https://lore.kernel.org/all/20251009112814.13942-1-mark@harmstone.com/\n* Version 4: https://lore.kernel.org/all/20251024181227.32228-1-mark@harmstone.com/\n* Version 5: https://lore.kernel.org/all/20251110171511.20900-1-mark@harmstone.com/\n* Version 6: https://lore.kernel.org/all/20251114184745.9304-1-mark@harmstone.com/\n* Version 7: https://lore.kernel.org/all/20251124185335.16556-1-mark@harmstone.com/\n\nChanges since version 7:\n* renamed struct btrfs_remap to struct btrfs_remap_item\n* renamed BTRFS_BLOCK_GROUP_FLAGS_REMAP to BTRFS_BLOCK_GROUP_FLAGS_METADATA_REMAP\n* added unlikelies\n* renamed new commit_* fields in struct btrfs_block_group to last_*, and added\n  new patch renaming existing commit_used to last_used to match\n* merged do_copy() into copy_remapped_data()\n* initialized on-stack struct btrfs_remap_items\n* fixed comments\n* added other minor changes as suggested by David Sterba\n\nMark Harmstone (17):\n  btrfs: add definitions and constants for remap-tree\n  btrfs: add METADATA_REMAP chunk type\n  btrfs: allow remapped chunks to have zero stripes\n  btrfs: remove remapped block groups from the free-space tree\n  btrfs: don't add metadata items for the remap tree to the extent tree\n  btrfs: rename struct btrfs_block_group field commit_used to last_used\n  btrfs: add extended version of struct block_group_item\n  btrfs: allow mounting filesystems with remap-tree incompat flag\n  btrfs: redirect I/O for remapped block groups\n  btrfs: handle deletions from remapped block group\n  btrfs: handle setting up relocation of block group with remap-tree\n  btrfs: move existing remaps before relocating block group\n  btrfs: replace identity remaps with actual remaps when doing\n    relocations\n  btrfs: add do_remap param to btrfs_discard_extent()\n  btrfs: allow balancing remap tree\n  btrfs: handle discarding fully-remapped block groups\n  btrfs: populate fully_remapped_bgs_list on mount\n\n fs/btrfs/Kconfig                |    2 +\n fs/btrfs/accessors.h            |   30 +\n fs/btrfs/bio.c                  |    3 +-\n fs/btrfs/bio.h                  |    3 +\n fs/btrfs/block-group.c          |  323 ++++--\n fs/btrfs/block-group.h          |   29 +-\n fs/btrfs/block-rsv.c            |    9 +\n fs/btrfs/block-rsv.h            |    1 +\n fs/btrfs/discard.c              |   57 +-\n fs/btrfs/disk-io.c              |  130 ++-\n fs/btrfs/extent-tree.c          |  151 ++-\n fs/btrfs/extent-tree.h          |    4 +-\n fs/btrfs/free-space-cache.c     |   59 +-\n fs/btrfs/free-space-cache.h     |    1 +\n fs/btrfs/free-space-tree.c      |    4 +-\n fs/btrfs/free-space-tree.h      |    5 +-\n fs/btrfs/fs.h                   |   10 +-\n fs/btrfs/inode.c                |    2 +-\n fs/btrfs/locking.c              |    1 +\n fs/btrfs/relocation.c           | 1885 +++++++++++++++++++++++++++++--\n fs/btrfs/relocation.h           |   18 +\n fs/btrfs/space-info.c           |   22 +-\n fs/btrfs/sysfs.c                |    4 +\n fs/btrfs/transaction.c          |    7 +\n fs/btrfs/tree-checker.c         |   94 +-\n fs/btrfs/tree-checker.h         |    5 +\n fs/btrfs/volumes.c              |  355 +++++-\n fs/btrfs/volumes.h              |   18 +-\n include/uapi/linux/btrfs.h      |    1 +\n include/uapi/linux/btrfs_tree.h |   34 +-\n 30 files changed, 2991 insertions(+), 276 deletions(-)\n\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author addressed a concern about the tree-checker not allowing for remapped chunks by adding a new function to validate stripe counts and modifying the check_leaf_chunk_item() function to use this new function.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "author acknowledged a fix is needed",
                "author confirmed the issue is resolved"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a chunk has been fully remapped, we are going to set its\nnum_stripes to 0, as it will no longer represent a physical location on\ndisk.\n\nChange tree-checker to allow for this, and fix read_one_chunk() to avoid\na divide by zero.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/tree-checker.c | 65 ++++++++++++++++++++++++++++-------------\n fs/btrfs/volumes.c      |  7 ++++-\n 2 files changed, 51 insertions(+), 21 deletions(-)\n\ndiff --git a/fs/btrfs/tree-checker.c b/fs/btrfs/tree-checker.c\nindex a6c158cd8fcd..4e390d6517a3 100644\n--- a/fs/btrfs/tree-checker.c\n+++ b/fs/btrfs/tree-checker.c\n@@ -816,6 +816,41 @@ static void chunk_err(const struct btrfs_fs_info *fs_info,\n \tva_end(args);\n }\n \n+static bool valid_stripe_count(u64 profile, u16 num_stripes,\n+\t\t\t       u16 sub_stripes)\n+{\n+\tswitch (profile) {\n+\tcase BTRFS_BLOCK_GROUP_RAID0:\n+\t\treturn true;\n+\tcase BTRFS_BLOCK_GROUP_RAID10:\n+\t\treturn sub_stripes ==\n+\t\t\tbtrfs_raid_array[BTRFS_RAID_RAID10].sub_stripes;\n+\tcase BTRFS_BLOCK_GROUP_RAID1:\n+\t\treturn num_stripes ==\n+\t\t\tbtrfs_raid_array[BTRFS_RAID_RAID1].devs_min;\n+\tcase BTRFS_BLOCK_GROUP_RAID1C3:\n+\t\treturn num_stripes ==\n+\t\t\tbtrfs_raid_array[BTRFS_RAID_RAID1C3].devs_min;\n+\tcase BTRFS_BLOCK_GROUP_RAID1C4:\n+\t\treturn num_stripes ==\n+\t\t\tbtrfs_raid_array[BTRFS_RAID_RAID1C4].devs_min;\n+\tcase BTRFS_BLOCK_GROUP_RAID5:\n+\t\treturn num_stripes >=\n+\t\t\tbtrfs_raid_array[BTRFS_RAID_RAID5].devs_min;\n+\tcase BTRFS_BLOCK_GROUP_RAID6:\n+\t\treturn num_stripes >=\n+\t\t\tbtrfs_raid_array[BTRFS_RAID_RAID6].devs_min;\n+\tcase BTRFS_BLOCK_GROUP_DUP:\n+\t\treturn num_stripes ==\n+\t\t\tbtrfs_raid_array[BTRFS_RAID_DUP].dev_stripes;\n+\tcase 0: /* SINGLE */\n+\t\treturn num_stripes ==\n+\t\t\tbtrfs_raid_array[BTRFS_RAID_SINGLE].dev_stripes;\n+\tdefault:\n+\t\tBUG();\n+\t}\n+}\n+\n /*\n  * The common chunk check which could also work on super block sys chunk array.\n  *\n@@ -839,6 +874,7 @@ int btrfs_check_chunk_valid(const struct btrfs_fs_info *fs_info,\n \tu64 features;\n \tu32 chunk_sector_size;\n \tbool mixed = false;\n+\tbool remapped;\n \tint raid_index;\n \tint nparity;\n \tint ncopies;\n@@ -862,12 +898,14 @@ int btrfs_check_chunk_valid(const struct btrfs_fs_info *fs_info,\n \tncopies = btrfs_raid_array[raid_index].ncopies;\n \tnparity = btrfs_raid_array[raid_index].nparity;\n \n-\tif (unlikely(!num_stripes)) {\n+\tremapped = type & BTRFS_BLOCK_GROUP_REMAPPED;\n+\n+\tif (unlikely(!remapped && !num_stripes)) {\n \t\tchunk_err(fs_info, leaf, chunk, logical,\n \t\t\t  \"invalid chunk num_stripes, have %u\", num_stripes);\n \t\treturn -EUCLEAN;\n \t}\n-\tif (unlikely(num_stripes < ncopies)) {\n+\tif (unlikely(num_stripes != 0 && num_stripes < ncopies)) {\n \t\tchunk_err(fs_info, leaf, chunk, logical,\n \t\t\t  \"invalid chunk num_stripes < ncopies, have %u < %d\",\n \t\t\t  num_stripes, ncopies);\n@@ -965,22 +1003,9 @@ int btrfs_check_chunk_valid(const struct btrfs_fs_info *fs_info,\n \t\t}\n \t}\n \n-\tif (unlikely((type & BTRFS_BLOCK_GROUP_RAID10 &&\n-\t\t      sub_stripes != btrfs_raid_array[BTRFS_RAID_RAID10].sub_stripes) ||\n-\t\t     (type & BTRFS_BLOCK_GROUP_RAID1 &&\n-\t\t      num_stripes != btrfs_raid_array[BTRFS_RAID_RAID1].devs_min) ||\n-\t\t     (type & BTRFS_BLOCK_GROUP_RAID1C3 &&\n-\t\t      num_stripes != btrfs_raid_array[BTRFS_RAID_RAID1C3].devs_min) ||\n-\t\t     (type & BTRFS_BLOCK_GROUP_RAID1C4 &&\n-\t\t      num_stripes != btrfs_raid_array[BTRFS_RAID_RAID1C4].devs_min) ||\n-\t\t     (type & BTRFS_BLOCK_GROUP_RAID5 &&\n-\t\t      num_stripes < btrfs_raid_array[BTRFS_RAID_RAID5].devs_min) ||\n-\t\t     (type & BTRFS_BLOCK_GROUP_RAID6 &&\n-\t\t      num_stripes < btrfs_raid_array[BTRFS_RAID_RAID6].devs_min) ||\n-\t\t     (type & BTRFS_BLOCK_GROUP_DUP &&\n-\t\t      num_stripes != btrfs_raid_array[BTRFS_RAID_DUP].dev_stripes) ||\n-\t\t     ((type & BTRFS_BLOCK_GROUP_PROFILE_MASK) == 0 &&\n-\t\t      num_stripes != btrfs_raid_array[BTRFS_RAID_SINGLE].dev_stripes))) {\n+\tif (!remapped &&\n+\t    !valid_stripe_count(type & BTRFS_BLOCK_GROUP_PROFILE_MASK,\n+\t\t\t\tnum_stripes, sub_stripes)) {\n \t\tchunk_err(fs_info, leaf, chunk, logical,\n \t\t\t\"invalid num_stripes:sub_stripes %u:%u for profile %llu\",\n \t\t\tnum_stripes, sub_stripes,\n@@ -1004,11 +1029,11 @@ static int check_leaf_chunk_item(struct extent_buffer *leaf,\n \tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n \tint num_stripes;\n \n-\tif (unlikely(btrfs_item_size(leaf, slot) < sizeof(struct btrfs_chunk))) {\n+\tif (unlikely(btrfs_item_size(leaf, slot) < offsetof(struct btrfs_chunk, stripe))) {\n \t\tchunk_err(fs_info, leaf, chunk, key->offset,\n \t\t\t\"invalid chunk item size: have %u expect [%zu, %u)\",\n \t\t\tbtrfs_item_size(leaf, slot),\n-\t\t\tsizeof(struct btrfs_chunk),\n+\t\t\toffsetof(struct btrfs_chunk, stripe),\n \t\t\tBTRFS_LEAF_DATA_SIZE(fs_info));\n \t\treturn -EUCLEAN;\n \t}\ndiff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\nindex 07d42ba38d7d..070efac46a81 100644\n--- a/fs/btrfs/volumes.c\n+++ b/fs/btrfs/volumes.c\n@@ -7045,7 +7045,12 @@ static int read_one_chunk(struct btrfs_key *key, struct extent_buffer *leaf,\n \t */\n \tmap->sub_stripes = btrfs_raid_array[index].sub_stripes;\n \tmap->verified_stripes = 0;\n-\tmap->stripe_size = btrfs_calc_stripe_length(map);\n+\n+\tif (num_stripes > 0)\n+\t\tmap->stripe_size = btrfs_calc_stripe_length(map);\n+\telse\n+\t\tmap->stripe_size = 0;\n+\n \tfor (i = 0; i < num_stripes; i++) {\n \t\tmap->stripes[i].physical =\n \t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author addressed a concern about the need to track remapped block groups in the free-space tree by introducing a new struct btrfs_block_group_item_v2 that includes fields for remap_bytes and identity_remap_count, which are used to indicate if a block group needs to be relocated or has been fully remapped. The author confirmed that this change is necessary and will help with relocation.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged concern",
                "confirmed necessity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a struct btrfs_block_group_item_v2, which is used in the block group\ntree if the remap-tree incompat flag is set.\n\nThis adds two new fields to the block group item: `remap_bytes` and\n`identity_remap_count`.\n\n`remap_bytes` records the amount of data that's physically within this\nblock group, but nominally in another, remapped block group. This is\nnecessary because this data will need to be moved first if this block\ngroup is itself relocated. If `remap_bytes` > 0, this is an indicator to\nthe relocation thread that it will need to search the remap-tree for\nbackrefs. A block group must also have `remap_bytes` == 0 before it can\nbe dropped.\n\n`identity_remap_count` records how many identity remap items are located\nin the remap tree for this block group. When relocation is begun for\nthis block group, this is set to the number of holes in the free-space\ntree for this range. As identity remaps are converted into actual remaps\nby the relocation process, this number is decreased. Once it reaches 0,\neither because of relocation or because extents have been deleted, the\nblock group has been fully remapped and its chunk's device extents are\nremoved.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/accessors.h            |  20 +++++++\n fs/btrfs/block-group.c          | 100 ++++++++++++++++++++++++--------\n fs/btrfs/block-group.h          |  14 ++++-\n fs/btrfs/discard.c              |   2 +-\n fs/btrfs/tree-checker.c         |  10 +++-\n include/uapi/linux/btrfs_tree.h |   8 +++\n 6 files changed, 126 insertions(+), 28 deletions(-)\n\ndiff --git a/fs/btrfs/accessors.h b/fs/btrfs/accessors.h\nindex 09cdd6bfddf5..9797f9e8d4e5 100644\n--- a/fs/btrfs/accessors.h\n+++ b/fs/btrfs/accessors.h\n@@ -240,6 +240,26 @@ BTRFS_SETGET_FUNCS(block_group_flags, struct btrfs_block_group_item, flags, 64);\n BTRFS_SETGET_STACK_FUNCS(stack_block_group_flags,\n \t\t\tstruct btrfs_block_group_item, flags, 64);\n \n+/* struct btrfs_block_group_item_v2 */\n+BTRFS_SETGET_STACK_FUNCS(stack_block_group_v2_used, struct btrfs_block_group_item_v2,\n+\t\t\t used, 64);\n+BTRFS_SETGET_FUNCS(block_group_v2_used, struct btrfs_block_group_item_v2, used, 64);\n+BTRFS_SETGET_STACK_FUNCS(stack_block_group_v2_chunk_objectid,\n+\t\t\t struct btrfs_block_group_item_v2, chunk_objectid, 64);\n+BTRFS_SETGET_FUNCS(block_group_v2_chunk_objectid,\n+\t\t   struct btrfs_block_group_item_v2, chunk_objectid, 64);\n+BTRFS_SETGET_STACK_FUNCS(stack_block_group_v2_flags,\n+\t\t\t struct btrfs_block_group_item_v2, flags, 64);\n+BTRFS_SETGET_FUNCS(block_group_v2_flags, struct btrfs_block_group_item_v2, flags, 64);\n+BTRFS_SETGET_STACK_FUNCS(stack_block_group_v2_remap_bytes,\n+\t\t\t struct btrfs_block_group_item_v2, remap_bytes, 64);\n+BTRFS_SETGET_FUNCS(block_group_v2_remap_bytes, struct btrfs_block_group_item_v2,\n+\t\t   remap_bytes, 64);\n+BTRFS_SETGET_STACK_FUNCS(stack_block_group_v2_identity_remap_count,\n+\t\t\t struct btrfs_block_group_item_v2, identity_remap_count, 32);\n+BTRFS_SETGET_FUNCS(block_group_v2_identity_remap_count, struct btrfs_block_group_item_v2,\n+\t\t   identity_remap_count, 32);\n+\n /* struct btrfs_free_space_info */\n BTRFS_SETGET_FUNCS(free_space_extent_count, struct btrfs_free_space_info,\n \t\t   extent_count, 32);\ndiff --git a/fs/btrfs/block-group.c b/fs/btrfs/block-group.c\nindex 822c5306a7a4..4962d17a175e 100644\n--- a/fs/btrfs/block-group.c\n+++ b/fs/btrfs/block-group.c\n@@ -2372,7 +2372,7 @@ static int check_chunk_block_group_mappings(struct btrfs_fs_info *fs_info)\n }\n \n static int read_one_block_group(struct btrfs_fs_info *info,\n-\t\t\t\tstruct btrfs_block_group_item *bgi,\n+\t\t\t\tstruct btrfs_block_group_item_v2 *bgi,\n \t\t\t\tconst struct btrfs_key *key,\n \t\t\t\tint need_clear)\n {\n@@ -2387,11 +2387,16 @@ static int read_one_block_group(struct btrfs_fs_info *info,\n \t\treturn -ENOMEM;\n \n \tcache->length = key->offset;\n-\tcache->used = btrfs_stack_block_group_used(bgi);\n+\tcache->used = btrfs_stack_block_group_v2_used(bgi);\n \tcache->last_used = cache->used;\n-\tcache->flags = btrfs_stack_block_group_flags(bgi);\n-\tcache->global_root_id = btrfs_stack_block_group_chunk_objectid(bgi);\n+\tcache->flags = btrfs_stack_block_group_v2_flags(bgi);\n+\tcache->global_root_id = btrfs_stack_block_group_v2_chunk_objectid(bgi);\n \tcache->space_info = btrfs_find_space_info(info, cache->flags);\n+\tcache->remap_bytes = btrfs_stack_block_group_v2_remap_bytes(bgi);\n+\tcache->last_remap_bytes = cache->remap_bytes;\n+\tcache->identity_remap_count =\n+\t\tbtrfs_stack_block_group_v2_identity_remap_count(bgi);\n+\tcache->last_identity_remap_count = cache->identity_remap_count;\n \n \tbtrfs_set_free_space_tree_thresholds(cache);\n \n@@ -2456,7 +2461,7 @@ static int read_one_block_group(struct btrfs_fs_info *info,\n \t} else if (cache->length == cache->used) {\n \t\tcache->cached = BTRFS_CACHE_FINISHED;\n \t\tbtrfs_free_excluded_extents(cache);\n-\t} else if (cache->used == 0) {\n+\t} else if (cache->used == 0 && cache->remap_bytes == 0) {\n \t\tcache->cached = BTRFS_CACHE_FINISHED;\n \t\tret = btrfs_add_new_free_space(cache, cache->start,\n \t\t\t\t\t       cache->start + cache->length, NULL);\n@@ -2476,7 +2481,7 @@ static int read_one_block_group(struct btrfs_fs_info *info,\n \n \tset_avail_alloc_bits(info, cache->flags);\n \tif (btrfs_chunk_writeable(info, cache->start)) {\n-\t\tif (cache->used == 0) {\n+\t\tif (cache->used == 0 && cache->remap_bytes == 0) {\n \t\t\tASSERT(list_empty(&cache->bg_list));\n \t\t\tif (btrfs_test_opt(info, DISCARD_ASYNC))\n \t\t\t\tbtrfs_discard_queue_work(&info->discard_ctl, cache);\n@@ -2580,9 +2585,10 @@ int btrfs_read_block_groups(struct btrfs_fs_info *info)\n \t\tneed_clear = 1;\n \n \twhile (1) {\n-\t\tstruct btrfs_block_group_item bgi;\n+\t\tstruct btrfs_block_group_item_v2 bgi;\n \t\tstruct extent_buffer *leaf;\n \t\tint slot;\n+\t\tsize_t size;\n \n \t\tret = find_first_block_group(info, path, &key);\n \t\tif (ret > 0)\n@@ -2593,8 +2599,16 @@ int btrfs_read_block_groups(struct btrfs_fs_info *info)\n \t\tleaf = path->nodes[0];\n \t\tslot = path->slots[0];\n \n+\t\tif (btrfs_fs_incompat(info, REMAP_TREE)) {\n+\t\t\tsize = sizeof(struct btrfs_block_group_item_v2);\n+\t\t} else {\n+\t\t\tsize = sizeof(struct btrfs_block_group_item);\n+\t\t\tbtrfs_set_stack_block_group_v2_remap_bytes(&bgi, 0);\n+\t\t\tbtrfs_set_stack_block_group_v2_identity_remap_count(&bgi, 0);\n+\t\t}\n+\n \t\tread_extent_buffer(leaf, &bgi, btrfs_item_ptr_offset(leaf, slot),\n-\t\t\t\t   sizeof(bgi));\n+\t\t\t\t   size);\n \n \t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n \t\tbtrfs_release_path(path);\n@@ -2664,25 +2678,38 @@ static int insert_block_group_item(struct btrfs_trans_handle *trans,\n \t\t\t\t   struct btrfs_block_group *block_group)\n {\n \tstruct btrfs_fs_info *fs_info = trans->fs_info;\n-\tstruct btrfs_block_group_item bgi;\n+\tstruct btrfs_block_group_item_v2 bgi;\n \tstruct btrfs_root *root = btrfs_block_group_root(fs_info);\n \tstruct btrfs_key key;\n \tu64 old_last_used;\n+\tsize_t size;\n \tint ret;\n \n \tspin_lock(&block_group->lock);\n-\tbtrfs_set_stack_block_group_used(&bgi, block_group->used);\n-\tbtrfs_set_stack_block_group_chunk_objectid(&bgi,\n-\t\t\t\t\t\t   block_group->global_root_id);\n-\tbtrfs_set_stack_block_group_flags(&bgi, block_group->flags);\n+\tbtrfs_set_stack_block_group_v2_used(&bgi, block_group->used);\n+\tbtrfs_set_stack_block_group_v2_chunk_objectid(&bgi,\n+\t\t\t\t\t\t      block_group->global_root_id);\n+\tbtrfs_set_stack_block_group_v2_flags(&bgi, block_group->flags);\n+\tbtrfs_set_stack_block_group_v2_remap_bytes(&bgi,\n+\t\t\t\t\t\t   block_group->remap_bytes);\n+\tbtrfs_set_stack_block_group_v2_identity_remap_count(&bgi,\n+\t\t\t\t\tblock_group->identity_remap_count);\n \told_last_used = block_group->last_used;\n \tblock_group->last_used = block_group->used;\n+\tblock_group->last_remap_bytes = block_group->remap_bytes;\n+\tblock_group->last_identity_remap_count =\n+\t\tblock_group->identity_remap_count;\n \tkey.objectid = block_group->start;\n \tkey.type = BTRFS_BLOCK_GROUP_ITEM_KEY;\n \tkey.offset = block_group->length;\n \tspin_unlock(&block_group->lock);\n \n-\tret = btrfs_insert_item(trans, root, &key, &bgi, sizeof(bgi));\n+\tif (btrfs_fs_incompat(fs_info, REMAP_TREE))\n+\t\tsize = sizeof(struct btrfs_block_group_item_v2);\n+\telse\n+\t\tsize = sizeof(struct btrfs_block_group_item);\n+\n+\tret = btrfs_insert_item(trans, root, &key, &bgi, size);\n \tif (ret < 0) {\n \t\tspin_lock(&block_group->lock);\n \t\tblock_group->last_used = old_last_used;\n@@ -3137,10 +3164,12 @@ static int update_block_group_item(struct btrfs_trans_handle *trans,\n \tstruct btrfs_root *root = btrfs_block_group_root(fs_info);\n \tunsigned long bi;\n \tstruct extent_buffer *leaf;\n-\tstruct btrfs_block_group_item bgi;\n+\tstruct btrfs_block_group_item_v2 bgi;\n \tstruct btrfs_key key;\n-\tu64 old_last_used;\n-\tu64 used;\n+\tu64 old_last_used, old_last_remap_bytes;\n+\tu32 old_last_identity_remap_count;\n+\tu64 used, remap_bytes;\n+\tu32 identity_remap_count;\n \n \t/*\n \t * Block group items update can be triggered out of commit transaction\n@@ -3150,13 +3179,21 @@ static int update_block_group_item(struct btrfs_trans_handle *trans,\n \t */\n \tspin_lock(&cache->lock);\n \told_last_used = cache->last_used;\n+\told_last_remap_bytes = cache->last_remap_bytes;\n+\told_last_identity_remap_count = cache->last_identity_remap_count;\n \tused = cache->used;\n-\t/* No change in used bytes, can safely skip it. */\n-\tif (cache->last_used == used) {\n+\tremap_bytes = cache->remap_bytes;\n+\tidentity_remap_count = cache->identity_remap_count;\n+\t/* No change in values, can safely skip it. */\n+\tif (cache->last_used == used &&\n+\t    cache->last_remap_bytes == remap_bytes &&\n+\t    cache->last_identity_remap_count == identity_remap_count) {\n \t\tspin_unlock(&cache->lock);\n \t\treturn 0;\n \t}\n \tcache->last_used = used;\n+\tcache->last_remap_bytes = remap_bytes;\n+\tcache->last_identity_remap_count = identity_remap_count;\n \tspin_unlock(&cache->lock);\n \n \tkey.objectid = cache->start;\n@@ -3172,11 +3209,23 @@ static int update_block_group_item(struct btrfs_trans_handle *trans,\n \n \tleaf = path->nodes[0];\n \tbi = btrfs_item_ptr_offset(leaf, path->slots[0]);\n-\tbtrfs_set_stack_block_group_used(&bgi, used);\n-\tbtrfs_set_stack_block_group_chunk_objectid(&bgi,\n-\t\t\t\t\t\t   cache->global_root_id);\n-\tbtrfs_set_stack_block_group_flags(&bgi, cache->flags);\n-\twrite_extent_buffer(leaf, &bgi, bi, sizeof(bgi));\n+\tbtrfs_set_stack_block_group_v2_used(&bgi, used);\n+\tbtrfs_set_stack_block_group_v2_chunk_objectid(&bgi,\n+\t\t\t\t\t\t      cache->global_root_id);\n+\tbtrfs_set_stack_block_group_v2_flags(&bgi, cache->flags);\n+\n+\tif (btrfs_fs_incompat(fs_info, REMAP_TREE)) {\n+\t\tbtrfs_set_stack_block_group_v2_remap_bytes(&bgi,\n+\t\t\t\t\t\t\t   cache->remap_bytes);\n+\t\tbtrfs_set_stack_block_group_v2_identity_remap_count(&bgi,\n+\t\t\t\t\t\tcache->identity_remap_count);\n+\t\twrite_extent_buffer(leaf, &bgi, bi,\n+\t\t\t\t    sizeof(struct btrfs_block_group_item_v2));\n+\t} else {\n+\t\twrite_extent_buffer(leaf, &bgi, bi,\n+\t\t\t\t    sizeof(struct btrfs_block_group_item));\n+\t}\n+\n fail:\n \tbtrfs_release_path(path);\n \t/*\n@@ -3191,6 +3240,9 @@ static int update_block_group_item(struct btrfs_trans_handle *trans,\n \tif (ret < 0 && ret != -ENOENT) {\n \t\tspin_lock(&cache->lock);\n \t\tcache->last_used = old_last_used;\n+\t\tcache->last_remap_bytes = old_last_remap_bytes;\n+\t\tcache->last_identity_remap_count =\n+\t\t\told_last_identity_remap_count;\n \t\tspin_unlock(&cache->lock);\n \t}\n \treturn ret;\ndiff --git a/fs/btrfs/block-group.h b/fs/btrfs/block-group.h\nindex 01401e9959c1..4cee3448ded3 100644\n--- a/fs/btrfs/block-group.h\n+++ b/fs/btrfs/block-group.h\n@@ -129,6 +129,8 @@ struct btrfs_block_group {\n \tu64 flags;\n \tu64 cache_generation;\n \tu64 global_root_id;\n+\tu64 remap_bytes;\n+\tu32 identity_remap_count;\n \n \t/*\n \t * The last committed used bytes of this block group, if the above @used\n@@ -136,6 +138,15 @@ struct btrfs_block_group {\n \t * group item of this block group.\n \t */\n \tu64 last_used;\n+\t/*\n+\t * The last committed remap_bytes value of this block group.\n+\t */\n+\tu64 last_remap_bytes;\n+\t/*\n+\t * The last commited identity_remap_count value of this block group.\n+\t */\n+\tu32 last_identity_remap_count;\n+\n \t/*\n \t * If the free space extent count exceeds this number, convert the block\n \t * group to bitmaps.\n@@ -282,7 +293,8 @@ static inline bool btrfs_is_block_group_used(const struct btrfs_block_group *bg)\n {\n \tlockdep_assert_held(&bg->lock);\n \n-\treturn (bg->used > 0 || bg->reserved > 0 || bg->pinned > 0);\n+\treturn (bg->used > 0 || bg->reserved > 0 || bg->pinned > 0 ||\n+\t\tbg->remap_bytes > 0);\n }\n \n static inline bool btrfs_is_block_group_data_only(const struct btrfs_block_group *block_group)\ndiff --git a/fs/btrfs/discard.c b/fs/btrfs/discard.c\nindex 89fe85778115..ee5f5b2788e1 100644\n--- a/fs/btrfs/discard.c\n+++ b/fs/btrfs/discard.c\n@@ -373,7 +373,7 @@ void btrfs_discard_queue_work(struct btrfs_discard_ctl *discard_ctl,\n \tif (!block_group || !btrfs_test_opt(block_group->fs_info, DISCARD_ASYNC))\n \t\treturn;\n \n-\tif (block_group->used == 0)\n+\tif (block_group->used == 0 && block_group->remap_bytes == 0)\n \t\tadd_to_discard_unused_list(discard_ctl, block_group);\n \telse\n \t\tadd_to_discard_list(discard_ctl, block_group);\ndiff --git a/fs/btrfs/tree-checker.c b/fs/btrfs/tree-checker.c\nindex 4e390d6517a3..d524fd4c3898 100644\n--- a/fs/btrfs/tree-checker.c\n+++ b/fs/btrfs/tree-checker.c\n@@ -688,6 +688,7 @@ static int check_block_group_item(struct extent_buffer *leaf,\n \tu64 chunk_objectid;\n \tu64 flags;\n \tu64 type;\n+\tsize_t exp_size;\n \n \t/*\n \t * Here we don't really care about alignment since extent allocator can\n@@ -699,10 +700,15 @@ static int check_block_group_item(struct extent_buffer *leaf,\n \t\treturn -EUCLEAN;\n \t}\n \n-\tif (unlikely(item_size != sizeof(bgi))) {\n+\tif (btrfs_fs_incompat(fs_info, REMAP_TREE))\n+\t\texp_size = sizeof(struct btrfs_block_group_item_v2);\n+\telse\n+\t\texp_size = sizeof(struct btrfs_block_group_item);\n+\n+\tif (unlikely(item_size != exp_size)) {\n \t\tblock_group_err(leaf, slot,\n \t\t\t\"invalid item size, have %u expect %zu\",\n-\t\t\t\titem_size, sizeof(bgi));\n+\t\t\t\titem_size, exp_size);\n \t\treturn -EUCLEAN;\n \t}\n \ndiff --git a/include/uapi/linux/btrfs_tree.h b/include/uapi/linux/btrfs_tree.h\nindex 76578426671c..86820a9644e8 100644\n--- a/include/uapi/linux/btrfs_tree.h\n+++ b/include/uapi/linux/btrfs_tree.h\n@@ -1229,6 +1229,14 @@ struct btrfs_block_group_item {\n \t__le64 flags;\n } __attribute__ ((__packed__));\n \n+struct btrfs_block_group_item_v2 {\n+\t__le64 used;\n+\t__le64 chunk_objectid;\n+\t__le64 flags;\n+\t__le64 remap_bytes;\n+\t__le32 identity_remap_count;\n+} __attribute__ ((__packed__));\n+\n struct btrfs_free_space_info {\n \t__le32 extent_count;\n \t__le32 flags;\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author is addressing a concern about I/O redirection for remapped block groups, specifically how to handle the case where the block group has been relocated due to remapping. The author proposes calling btrfs_translate_remap() in this scenario, which searches the remap tree and returns a new address if necessary. This approach will be used in place of the existing address, ensuring that I/O operations are redirected correctly even after remapping.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Change btrfs_map_block() so that if the block group has the REMAPPED\nflag set, we call btrfs_translate_remap() to obtain a new address.\n\nbtrfs_translate_remap() searches the remap tree for a range\ncorresponding to the logical address passed to btrfs_map_block(). If it\nis within an identity remap, this part of the block group hasn't yet\nbeen relocated, and so we use the existing address.\n\nIf it is within an actual remap, we subtract the start of the remap\nrange and add the address of its destination, contained in the item's\npayload.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/relocation.c | 54 +++++++++++++++++++++++++++++++++++++++++++\n fs/btrfs/relocation.h |  2 ++\n fs/btrfs/volumes.c    | 19 +++++++++++++++\n 3 files changed, 75 insertions(+)\n\ndiff --git a/fs/btrfs/relocation.c b/fs/btrfs/relocation.c\nindex 310b7d817a27..525f45c668f6 100644\n--- a/fs/btrfs/relocation.c\n+++ b/fs/btrfs/relocation.c\n@@ -3859,6 +3859,60 @@ static const char *stage_to_string(enum reloc_stage stage)\n \treturn \"unknown\";\n }\n \n+int btrfs_translate_remap(struct btrfs_fs_info *fs_info, u64 *logical,\n+\t\t\t  u64 *length)\n+{\n+\tint ret;\n+\tstruct btrfs_key key, found_key;\n+\tstruct extent_buffer *leaf;\n+\tstruct btrfs_remap_item *remap;\n+\tBTRFS_PATH_AUTO_FREE(path);\n+\n+\tpath = btrfs_alloc_path();\n+\tif (!path)\n+\t\treturn -ENOMEM;\n+\n+\tkey.objectid = *logical;\n+\tkey.type = (u8)-1;\n+\tkey.offset = (u64)-1;\n+\n+\tret = btrfs_search_slot(NULL, fs_info->remap_root, &key, path,\n+\t\t\t\t0, 0);\n+\tif (ret < 0)\n+\t\treturn ret;\n+\n+\tleaf = path->nodes[0];\n+\n+\tif (path->slots[0] == 0)\n+\t\treturn -ENOENT;\n+\n+\tpath->slots[0]--;\n+\n+\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n+\n+\tif (found_key.type != BTRFS_REMAP_KEY &&\n+\t    found_key.type != BTRFS_IDENTITY_REMAP_KEY) {\n+\t\treturn -ENOENT;\n+\t}\n+\n+\tif (found_key.objectid > *logical ||\n+\t    found_key.objectid + found_key.offset <= *logical) {\n+\t\treturn -ENOENT;\n+\t}\n+\n+\tif (*logical + *length > found_key.objectid + found_key.offset)\n+\t\t*length = found_key.objectid + found_key.offset - *logical;\n+\n+\tif (found_key.type == BTRFS_IDENTITY_REMAP_KEY)\n+\t\treturn 0;\n+\n+\tremap = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_remap_item);\n+\n+\t*logical += btrfs_remap_address(leaf, remap) - found_key.objectid;\n+\n+\treturn 0;\n+}\n+\n /*\n  * function to relocate all extents in a block group.\n  */\ndiff --git a/fs/btrfs/relocation.h b/fs/btrfs/relocation.h\nindex 5c36b3f84b57..b2ba83966650 100644\n--- a/fs/btrfs/relocation.h\n+++ b/fs/btrfs/relocation.h\n@@ -31,5 +31,7 @@ int btrfs_should_cancel_balance(const struct btrfs_fs_info *fs_info);\n struct btrfs_root *find_reloc_root(struct btrfs_fs_info *fs_info, u64 bytenr);\n bool btrfs_should_ignore_reloc_root(const struct btrfs_root *root);\n u64 btrfs_get_reloc_bg_bytenr(const struct btrfs_fs_info *fs_info);\n+int btrfs_translate_remap(struct btrfs_fs_info *fs_info, u64 *logical,\n+\t\t\t  u64 *length);\n \n #endif\ndiff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\nindex d6060e0e2144..557ce56df800 100644\n--- a/fs/btrfs/volumes.c\n+++ b/fs/btrfs/volumes.c\n@@ -6584,6 +6584,25 @@ int btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n \tif (IS_ERR(map))\n \t\treturn PTR_ERR(map);\n \n+\tif (map->type & BTRFS_BLOCK_GROUP_REMAPPED) {\n+\t\tu64 new_logical = logical;\n+\n+\t\tret = btrfs_translate_remap(fs_info, &new_logical, length);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\n+\t\tif (new_logical != logical) {\n+\t\t\tbtrfs_free_chunk_map(map);\n+\n+\t\t\tmap = btrfs_get_chunk_map(fs_info, new_logical,\n+\t\t\t\t\t\t  *length);\n+\t\t\tif (IS_ERR(map))\n+\t\t\t\treturn PTR_ERR(map);\n+\n+\t\t\tlogical = new_logical;\n+\t\t}\n+\t}\n+\n \tnum_copies = btrfs_chunk_map_num_copies(map);\n \tif (io_geom.mirror_num > num_copies)\n \t\treturn -EINVAL;\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author addressed a concern about the remap-tree feature being incompatible with other features like no-holes and block-group-tree. The author explained that these features were made dependencies to reduce testing complexity, but acknowledged that mixed-bg and zoned may be incompatible with remap-tree and are blocked for further testing.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a potential issue",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If we encounter a filesystem with the remap-tree incompat flag set,\nvaldiate its compatibility with the other flags, and load the remap tree\nusing the values that have been added to the superblock.\n\nThe remap-tree feature depends on the free space tere, but no-holes and\nblock-group-tree have been made dependencies to reduce the testing\nmatrix. Similarly I'm not aware of any reason why mixed-bg and zoned would be\nincompatible with remap-tree, but this is blocked for the time being\nuntil it can be fully tested.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/Kconfig                |   2 +\n fs/btrfs/accessors.h            |   6 ++\n fs/btrfs/disk-io.c              | 111 ++++++++++++++++++++++++++++----\n fs/btrfs/extent-tree.c          |   2 +\n fs/btrfs/fs.h                   |   4 +-\n fs/btrfs/transaction.c          |   7 ++\n include/uapi/linux/btrfs_tree.h |   5 +-\n 7 files changed, 122 insertions(+), 15 deletions(-)\n\ndiff --git a/fs/btrfs/Kconfig b/fs/btrfs/Kconfig\nindex bf7feff2fe44..ee2fdcb49719 100644\n--- a/fs/btrfs/Kconfig\n+++ b/fs/btrfs/Kconfig\n@@ -115,4 +115,6 @@ config BTRFS_EXPERIMENTAL\n \n \t  - large folio support\n \n+\t  - remap-tree - logical address remapping tree\n+\n \t  If unsure, say N.\ndiff --git a/fs/btrfs/accessors.h b/fs/btrfs/accessors.h\nindex 9797f9e8d4e5..8938357fcb40 100644\n--- a/fs/btrfs/accessors.h\n+++ b/fs/btrfs/accessors.h\n@@ -883,6 +883,12 @@ BTRFS_SETGET_STACK_FUNCS(super_uuid_tree_generation, struct btrfs_super_block,\n \t\t\t uuid_tree_generation, 64);\n BTRFS_SETGET_STACK_FUNCS(super_nr_global_roots, struct btrfs_super_block,\n \t\t\t nr_global_roots, 64);\n+BTRFS_SETGET_STACK_FUNCS(super_remap_root, struct btrfs_super_block,\n+\t\t\t remap_root, 64);\n+BTRFS_SETGET_STACK_FUNCS(super_remap_root_generation, struct btrfs_super_block,\n+\t\t\t remap_root_generation, 64);\n+BTRFS_SETGET_STACK_FUNCS(super_remap_root_level, struct btrfs_super_block,\n+\t\t\t remap_root_level, 8);\n \n /* struct btrfs_file_extent_item */\n BTRFS_SETGET_STACK_FUNCS(stack_file_extent_type, struct btrfs_file_extent_item,\ndiff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c\nindex c36367f9017f..b03654ee91f5 100644\n--- a/fs/btrfs/disk-io.c\n+++ b/fs/btrfs/disk-io.c\n@@ -1158,6 +1158,8 @@ static struct btrfs_root *btrfs_get_global_root(struct btrfs_fs_info *fs_info,\n \t\treturn btrfs_grab_root(btrfs_global_root(fs_info, &key));\n \tcase BTRFS_RAID_STRIPE_TREE_OBJECTID:\n \t\treturn btrfs_grab_root(fs_info->stripe_root);\n+\tcase BTRFS_REMAP_TREE_OBJECTID:\n+\t\treturn btrfs_grab_root(fs_info->remap_root);\n \tdefault:\n \t\treturn NULL;\n \t}\n@@ -1248,6 +1250,7 @@ void btrfs_free_fs_info(struct btrfs_fs_info *fs_info)\n \tbtrfs_put_root(fs_info->data_reloc_root);\n \tbtrfs_put_root(fs_info->block_group_root);\n \tbtrfs_put_root(fs_info->stripe_root);\n+\tbtrfs_put_root(fs_info->remap_root);\n \tbtrfs_check_leaked_roots(fs_info);\n \tbtrfs_extent_buffer_leak_debug_check(fs_info);\n \tkfree(fs_info->super_copy);\n@@ -1800,6 +1803,7 @@ static void free_root_pointers(struct btrfs_fs_info *info, bool free_chunk_root)\n \tfree_root_extent_buffers(info->data_reloc_root);\n \tfree_root_extent_buffers(info->block_group_root);\n \tfree_root_extent_buffers(info->stripe_root);\n+\tfree_root_extent_buffers(info->remap_root);\n \tif (free_chunk_root)\n \t\tfree_root_extent_buffers(info->chunk_root);\n }\n@@ -2213,21 +2217,49 @@ static int btrfs_read_roots(struct btrfs_fs_info *fs_info)\n \tif (ret)\n \t\tgoto out;\n \n-\t/*\n-\t * This tree can share blocks with some other fs tree during relocation\n-\t * and we need a proper setup by btrfs_get_fs_root\n-\t */\n-\troot = btrfs_get_fs_root(tree_root->fs_info,\n-\t\t\t\t BTRFS_DATA_RELOC_TREE_OBJECTID, true);\n-\tif (IS_ERR(root)) {\n-\t\tif (!btrfs_test_opt(fs_info, IGNOREBADROOTS)) {\n-\t\t\tlocation.objectid = BTRFS_DATA_RELOC_TREE_OBJECTID;\n-\t\t\tret = PTR_ERR(root);\n-\t\t\tgoto out;\n+\tif (btrfs_fs_incompat(fs_info, REMAP_TREE)) {\n+\t\t/*\n+\t\t * The remap_root has already been loaded in\n+\t\t * load_important_roots().\n+\t\t */\n+\t\troot = fs_info->remap_root;\n+\n+\t\tset_bit(BTRFS_ROOT_TRACK_DIRTY, &root->state);\n+\n+\t\troot->root_key.objectid = BTRFS_REMAP_TREE_OBJECTID;\n+\t\troot->root_key.type = BTRFS_ROOT_ITEM_KEY;\n+\t\troot->root_key.offset = 0;\n+\n+\t\t/* Check that data reloc tree doesn't also exist. */\n+\t\tlocation.objectid = BTRFS_DATA_RELOC_TREE_OBJECTID;\n+\t\troot = btrfs_read_tree_root(fs_info->tree_root, &location);\n+\t\tif (!IS_ERR(root)) {\n+\t\t\tbtrfs_err(fs_info,\n+\t\t\t   \"data reloc tree exists when remap-tree enabled\");\n+\t\t\tbtrfs_put_root(root);\n+\t\t\treturn -EIO;\n+\t\t} else if (PTR_ERR(root) != -ENOENT) {\n+\t\t\tbtrfs_warn(fs_info,\n+\t\t\t   \"error %ld when checking for data reloc tree\",\n+\t\t\t\t   PTR_ERR(root));\n \t\t}\n \t} else {\n-\t\tset_bit(BTRFS_ROOT_TRACK_DIRTY, &root->state);\n-\t\tfs_info->data_reloc_root = root;\n+\t\t/*\n+\t\t * This tree can share blocks with some other fs tree during\n+\t\t * relocation and we need a proper setup by btrfs_get_fs_root().\n+\t\t */\n+\t\troot = btrfs_get_fs_root(tree_root->fs_info,\n+\t\t\t\t\t BTRFS_DATA_RELOC_TREE_OBJECTID, true);\n+\t\tif (IS_ERR(root)) {\n+\t\t\tif (!btrfs_test_opt(fs_info, IGNOREBADROOTS)) {\n+\t\t\t\tlocation.objectid = BTRFS_DATA_RELOC_TREE_OBJECTID;\n+\t\t\t\tret = PTR_ERR(root);\n+\t\t\t\tgoto out;\n+\t\t\t}\n+\t\t} else {\n+\t\t\tset_bit(BTRFS_ROOT_TRACK_DIRTY, &root->state);\n+\t\t\tfs_info->data_reloc_root = root;\n+\t\t}\n \t}\n \n \tlocation.objectid = BTRFS_QUOTA_TREE_OBJECTID;\n@@ -2467,6 +2499,36 @@ int btrfs_validate_super(const struct btrfs_fs_info *fs_info,\n \t\tret = -EINVAL;\n \t}\n \n+\tif (btrfs_fs_incompat(fs_info, REMAP_TREE)) {\n+\t\t/*\n+\t\t * Reduce test matrix for remap tree by requiring block-group-tree\n+\t\t * and no-holes. Free-space-tree is a hard requirement.\n+\t\t */\n+\t\tif (!btrfs_fs_compat_ro(fs_info, FREE_SPACE_TREE_VALID) ||\n+\t\t    !btrfs_fs_incompat(fs_info, NO_HOLES) ||\n+\t\t    !btrfs_fs_compat_ro(fs_info, BLOCK_GROUP_TREE)) {\n+\t\t\tbtrfs_err(fs_info,\n+\"remap-tree feature requires free-space-tree, no-holes, and block-group-tree\");\n+\t\t\tret = -EINVAL;\n+\t\t}\n+\n+\t\tif (btrfs_fs_incompat(fs_info, MIXED_GROUPS)) {\n+\t\t\tbtrfs_err(fs_info, \"remap-tree not supported with mixed-bg\");\n+\t\t\tret = -EINVAL;\n+\t\t}\n+\n+\t\tif (btrfs_fs_incompat(fs_info, ZONED)) {\n+\t\t\tbtrfs_err(fs_info, \"remap-tree not supported with zoned devices\");\n+\t\t\tret = -EINVAL;\n+\t\t}\n+\n+\t\tif (sectorsize > PAGE_SIZE) {\n+\t\t\tbtrfs_err(fs_info,\n+\t\t\t\t  \"remap-tree not supported when block size > page size\");\n+\t\t\tret = -EINVAL;\n+\t\t}\n+\t}\n+\n \t/*\n \t * Hint to catch really bogus numbers, bitflips or so, more exact checks are\n \t * done later\n@@ -2625,6 +2687,18 @@ static int load_important_roots(struct btrfs_fs_info *fs_info)\n \t\tbtrfs_warn(fs_info, \"couldn't read tree root\");\n \t\treturn ret;\n \t}\n+\n+\tif (btrfs_fs_incompat(fs_info, REMAP_TREE)) {\n+\t\tbytenr = btrfs_super_remap_root(sb);\n+\t\tgen = btrfs_super_remap_root_generation(sb);\n+\t\tlevel = btrfs_super_remap_root_level(sb);\n+\t\tret = load_super_root(fs_info->remap_root, bytenr, gen, level);\n+\t\tif (ret) {\n+\t\t\tbtrfs_warn(fs_info, \"couldn't read remap root\");\n+\t\t\treturn ret;\n+\t\t}\n+\t}\n+\n \treturn 0;\n }\n \n@@ -3245,6 +3319,7 @@ int __cold open_ctree(struct super_block *sb, struct btrfs_fs_devices *fs_device\n \tstruct btrfs_fs_info *fs_info = btrfs_sb(sb);\n \tstruct btrfs_root *tree_root;\n \tstruct btrfs_root *chunk_root;\n+\tstruct btrfs_root *remap_root;\n \tint ret;\n \tint level;\n \n@@ -3375,6 +3450,16 @@ int __cold open_ctree(struct super_block *sb, struct btrfs_fs_devices *fs_device\n \tif (ret < 0)\n \t\tgoto fail_alloc;\n \n+\tif (btrfs_super_incompat_flags(disk_super) & BTRFS_FEATURE_INCOMPAT_REMAP_TREE) {\n+\t\tremap_root = btrfs_alloc_root(fs_info, BTRFS_REMAP_TREE_OBJECTID,\n+\t\t\t\t\t      GFP_KERNEL);\n+\t\tfs_info->remap_root = remap_root;\n+\t\tif (!remap_root) {\n+\t\t\tret = -ENOMEM;\n+\t\t\tgoto fail_alloc;\n+\t\t}\n+\t}\n+\n \t/*\n \t * At this point our mount options are validated, if we set ->max_inline\n \t * to something non-standard make sure we truncate it to sectorsize.\ndiff --git a/fs/btrfs/extent-tree.c b/fs/btrfs/extent-tree.c\nindex 43473a6d91d7..3868a295be62 100644\n--- a/fs/btrfs/extent-tree.c\n+++ b/fs/btrfs/extent-tree.c\n@@ -2589,6 +2589,8 @@ static u64 get_alloc_profile_by_root(struct btrfs_root *root, int data)\n \t\tflags = BTRFS_BLOCK_GROUP_DATA;\n \telse if (root == fs_info->chunk_root)\n \t\tflags = BTRFS_BLOCK_GROUP_SYSTEM;\n+\telse if (root == fs_info->remap_root)\n+\t\tflags = BTRFS_BLOCK_GROUP_METADATA_REMAP;\n \telse\n \t\tflags = BTRFS_BLOCK_GROUP_METADATA;\n \ndiff --git a/fs/btrfs/fs.h b/fs/btrfs/fs.h\nindex 46c4f1dcec47..af11f2ce310a 100644\n--- a/fs/btrfs/fs.h\n+++ b/fs/btrfs/fs.h\n@@ -307,7 +307,8 @@ enum {\n #define BTRFS_FEATURE_INCOMPAT_SUPP\t\t\\\n \t(BTRFS_FEATURE_INCOMPAT_SUPP_STABLE |\t\\\n \t BTRFS_FEATURE_INCOMPAT_RAID_STRIPE_TREE | \\\n-\t BTRFS_FEATURE_INCOMPAT_EXTENT_TREE_V2)\n+\t BTRFS_FEATURE_INCOMPAT_EXTENT_TREE_V2 | \\\n+\t BTRFS_FEATURE_INCOMPAT_REMAP_TREE)\n \n #else\n \n@@ -467,6 +468,7 @@ struct btrfs_fs_info {\n \tstruct btrfs_root *data_reloc_root;\n \tstruct btrfs_root *block_group_root;\n \tstruct btrfs_root *stripe_root;\n+\tstruct btrfs_root *remap_root;\n \n \t/* The log root tree is a directory of all the other log roots */\n \tstruct btrfs_root *log_root_tree;\ndiff --git a/fs/btrfs/transaction.c b/fs/btrfs/transaction.c\nindex e2f993b1783f..f4cc9e1a1b93 100644\n--- a/fs/btrfs/transaction.c\n+++ b/fs/btrfs/transaction.c\n@@ -1967,6 +1967,13 @@ static void update_super_roots(struct btrfs_fs_info *fs_info)\n \t\tsuper->cache_generation = 0;\n \tif (test_bit(BTRFS_FS_UPDATE_UUID_TREE_GEN, &fs_info->flags))\n \t\tsuper->uuid_tree_generation = root_item->generation;\n+\n+\tif (btrfs_fs_incompat(fs_info, REMAP_TREE)) {\n+\t\troot_item = &fs_info->remap_root->root_item;\n+\t\tsuper->remap_root = root_item->bytenr;\n+\t\tsuper->remap_root_generation = root_item->generation;\n+\t\tsuper->remap_root_level = root_item->level;\n+\t}\n }\n \n int btrfs_transaction_blocked(struct btrfs_fs_info *info)\ndiff --git a/include/uapi/linux/btrfs_tree.h b/include/uapi/linux/btrfs_tree.h\nindex 86820a9644e8..f7843e6bb978 100644\n--- a/include/uapi/linux/btrfs_tree.h\n+++ b/include/uapi/linux/btrfs_tree.h\n@@ -721,9 +721,12 @@ struct btrfs_super_block {\n \t__u8 metadata_uuid[BTRFS_FSID_SIZE];\n \n \t__u64 nr_global_roots;\n+\t__le64 remap_root;\n+\t__le64 remap_root_generation;\n+\t__u8 remap_root_level;\n \n \t/* Future expansion */\n-\t__le64 reserved[27];\n+\t__u8 reserved[199];\n \t__u8 sys_chunk_array[BTRFS_SYSTEM_CHUNK_ARRAY_SIZE];\n \tstruct btrfs_root_backup super_roots[BTRFS_NUM_BACKUP_ROOTS];\n \n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author is addressing a concern about clarity and consistency in field naming conventions by renaming commit_used to last_used in struct btrfs_block_group, explaining that the original name was not immediately clear as indicating 'flags as of the last commit'.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "clarification",
                "consistency"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Rename the field commit_used in struct btrfs_block_group to last_used,\nfor clarity and consistency with the similar fields we're about to add.\nIt's not obvious that commit_flags means \"flags as of the last commit\"\nrather than \"flags related to a commit\".\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\n---\n fs/btrfs/block-group.c | 24 ++++++++++++------------\n fs/btrfs/block-group.h |  4 ++--\n 2 files changed, 14 insertions(+), 14 deletions(-)\n\ndiff --git a/fs/btrfs/block-group.c b/fs/btrfs/block-group.c\nindex 39e2db630bce..822c5306a7a4 100644\n--- a/fs/btrfs/block-group.c\n+++ b/fs/btrfs/block-group.c\n@@ -2388,7 +2388,7 @@ static int read_one_block_group(struct btrfs_fs_info *info,\n \n \tcache->length = key->offset;\n \tcache->used = btrfs_stack_block_group_used(bgi);\n-\tcache->commit_used = cache->used;\n+\tcache->last_used = cache->used;\n \tcache->flags = btrfs_stack_block_group_flags(bgi);\n \tcache->global_root_id = btrfs_stack_block_group_chunk_objectid(bgi);\n \tcache->space_info = btrfs_find_space_info(info, cache->flags);\n@@ -2667,7 +2667,7 @@ static int insert_block_group_item(struct btrfs_trans_handle *trans,\n \tstruct btrfs_block_group_item bgi;\n \tstruct btrfs_root *root = btrfs_block_group_root(fs_info);\n \tstruct btrfs_key key;\n-\tu64 old_commit_used;\n+\tu64 old_last_used;\n \tint ret;\n \n \tspin_lock(&block_group->lock);\n@@ -2675,8 +2675,8 @@ static int insert_block_group_item(struct btrfs_trans_handle *trans,\n \tbtrfs_set_stack_block_group_chunk_objectid(&bgi,\n \t\t\t\t\t\t   block_group->global_root_id);\n \tbtrfs_set_stack_block_group_flags(&bgi, block_group->flags);\n-\told_commit_used = block_group->commit_used;\n-\tblock_group->commit_used = block_group->used;\n+\told_last_used = block_group->last_used;\n+\tblock_group->last_used = block_group->used;\n \tkey.objectid = block_group->start;\n \tkey.type = BTRFS_BLOCK_GROUP_ITEM_KEY;\n \tkey.offset = block_group->length;\n@@ -2685,7 +2685,7 @@ static int insert_block_group_item(struct btrfs_trans_handle *trans,\n \tret = btrfs_insert_item(trans, root, &key, &bgi, sizeof(bgi));\n \tif (ret < 0) {\n \t\tspin_lock(&block_group->lock);\n-\t\tblock_group->commit_used = old_commit_used;\n+\t\tblock_group->last_used = old_last_used;\n \t\tspin_unlock(&block_group->lock);\n \t}\n \n@@ -3139,7 +3139,7 @@ static int update_block_group_item(struct btrfs_trans_handle *trans,\n \tstruct extent_buffer *leaf;\n \tstruct btrfs_block_group_item bgi;\n \tstruct btrfs_key key;\n-\tu64 old_commit_used;\n+\tu64 old_last_used;\n \tu64 used;\n \n \t/*\n@@ -3149,14 +3149,14 @@ static int update_block_group_item(struct btrfs_trans_handle *trans,\n \t * may be changed.\n \t */\n \tspin_lock(&cache->lock);\n-\told_commit_used = cache->commit_used;\n+\told_last_used = cache->last_used;\n \tused = cache->used;\n \t/* No change in used bytes, can safely skip it. */\n-\tif (cache->commit_used == used) {\n+\tif (cache->last_used == used) {\n \t\tspin_unlock(&cache->lock);\n \t\treturn 0;\n \t}\n-\tcache->commit_used = used;\n+\tcache->last_used = used;\n \tspin_unlock(&cache->lock);\n \n \tkey.objectid = cache->start;\n@@ -3180,17 +3180,17 @@ static int update_block_group_item(struct btrfs_trans_handle *trans,\n fail:\n \tbtrfs_release_path(path);\n \t/*\n-\t * We didn't update the block group item, need to revert commit_used\n+\t * We didn't update the block group item, need to revert last_used\n \t * unless the block group item didn't exist yet - this is to prevent a\n \t * race with a concurrent insertion of the block group item, with\n \t * insert_block_group_item(), that happened just after we attempted to\n-\t * update. In that case we would reset commit_used to 0 just after the\n+\t * update. In that case we would reset last_used to 0 just after the\n \t * insertion set it to a value greater than 0 - if the block group later\n \t * becomes with 0 used bytes, we would incorrectly skip its update.\n \t */\n \tif (ret < 0 && ret != -ENOENT) {\n \t\tspin_lock(&cache->lock);\n-\t\tcache->commit_used = old_commit_used;\n+\t\tcache->last_used = old_last_used;\n \t\tspin_unlock(&cache->lock);\n \t}\n \treturn ret;\ndiff --git a/fs/btrfs/block-group.h b/fs/btrfs/block-group.h\nindex 5f933455118c..01401e9959c1 100644\n--- a/fs/btrfs/block-group.h\n+++ b/fs/btrfs/block-group.h\n@@ -132,10 +132,10 @@ struct btrfs_block_group {\n \n \t/*\n \t * The last committed used bytes of this block group, if the above @used\n-\t * is still the same as @commit_used, we don't need to update block\n+\t * is still the same as @last_used, we don't need to update block\n \t * group item of this block group.\n \t */\n-\tu64 commit_used;\n+\tu64 last_used;\n \t/*\n \t * If the free space extent count exceeds this number, convert the block\n \t * group to bitmaps.\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author addressed a concern about the equivalence of extent removal and free-space tree walking in remapped block groups, explaining that they are no longer equivalent due to address remapping. The author added a do_remap parameter to btrfs_discard_extent() and btrfs_map_discard(), which will be used to determine whether the address needs to be remapped before discarding.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "btrfs_discard_extent() can be called either when an extent is removed\nor from walking the free-space tree. With a remapped block group these\ntwo things are no longer equivalent: the extent's addresses are\nremapped, while the free-space tree exclusively uses underlying\naddresses.\n\nAdd a do_remap parameter to btrfs_discard_extent() and\nbtrfs_map_discard(), saying whether or not the address needs to be run\nthrough the remap tree first.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/extent-tree.c      | 11 +++++++----\n fs/btrfs/extent-tree.h      |  2 +-\n fs/btrfs/free-space-cache.c |  2 +-\n fs/btrfs/inode.c            |  2 +-\n fs/btrfs/volumes.c          | 23 +++++++++++++++++++++--\n fs/btrfs/volumes.h          |  2 +-\n 6 files changed, 32 insertions(+), 10 deletions(-)\n\ndiff --git a/fs/btrfs/extent-tree.c b/fs/btrfs/extent-tree.c\nindex 70020ba8ef92..9d68f3fa4fa9 100644\n--- a/fs/btrfs/extent-tree.c\n+++ b/fs/btrfs/extent-tree.c\n@@ -1381,7 +1381,7 @@ static int do_discard_extent(struct btrfs_discard_stripe *stripe, u64 *bytes)\n }\n \n int btrfs_discard_extent(struct btrfs_fs_info *fs_info, u64 bytenr,\n-\t\t\t u64 num_bytes, u64 *actual_bytes)\n+\t\t\t u64 num_bytes, u64 *actual_bytes, bool do_remap)\n {\n \tint ret = 0;\n \tu64 discarded_bytes = 0;\n@@ -1399,7 +1399,8 @@ int btrfs_discard_extent(struct btrfs_fs_info *fs_info, u64 bytenr,\n \t\tint i;\n \n \t\tnum_bytes = end - cur;\n-\t\tstripes = btrfs_map_discard(fs_info, cur, &num_bytes, &num_stripes);\n+\t\tstripes = btrfs_map_discard(fs_info, cur, &num_bytes,\n+\t\t\t\t\t    &num_stripes, do_remap);\n \t\tif (IS_ERR(stripes)) {\n \t\t\tret = PTR_ERR(stripes);\n \t\t\tif (ret == -EOPNOTSUPP)\n@@ -2932,7 +2933,8 @@ int btrfs_finish_extent_commit(struct btrfs_trans_handle *trans)\n \n \t\tif (btrfs_test_opt(fs_info, DISCARD_SYNC))\n \t\t\tret = btrfs_discard_extent(fs_info, start,\n-\t\t\t\t\t\t   end + 1 - start, NULL);\n+\t\t\t\t\t\t   end + 1 - start, NULL,\n+\t\t\t\t\t\t   true);\n \n \t\tnext_state = btrfs_next_extent_state(unpin, cached_state);\n \t\tbtrfs_clear_extent_dirty(unpin, start, end, &cached_state);\n@@ -2990,7 +2992,8 @@ int btrfs_finish_extent_commit(struct btrfs_trans_handle *trans)\n \t\tret = -EROFS;\n \t\tif (!TRANS_ABORTED(trans))\n \t\t\tret = btrfs_discard_extent(fs_info, block_group->start,\n-\t\t\t\t\t\t   block_group->length, NULL);\n+\t\t\t\t\t\t   block_group->length, NULL,\n+\t\t\t\t\t\t   true);\n \n \t\t/*\n \t\t * Not strictly necessary to lock, as the block_group should be\ndiff --git a/fs/btrfs/extent-tree.h b/fs/btrfs/extent-tree.h\nindex d7b6aeb63656..ff330d4896d6 100644\n--- a/fs/btrfs/extent-tree.h\n+++ b/fs/btrfs/extent-tree.h\n@@ -161,7 +161,7 @@ int btrfs_drop_subtree(struct btrfs_trans_handle *trans,\n \t\t\tstruct extent_buffer *parent);\n void btrfs_error_unpin_extent_range(struct btrfs_fs_info *fs_info, u64 start, u64 end);\n int btrfs_discard_extent(struct btrfs_fs_info *fs_info, u64 bytenr,\n-\t\t\t u64 num_bytes, u64 *actual_bytes);\n+\t\t\t u64 num_bytes, u64 *actual_bytes, bool do_remap);\n int btrfs_trim_fs(struct btrfs_fs_info *fs_info, struct fstrim_range *range);\n void btrfs_handle_fully_remapped_bgs(struct btrfs_fs_info *fs_info);\n int btrfs_complete_bg_remapping(struct btrfs_block_group *bg);\ndiff --git a/fs/btrfs/free-space-cache.c b/fs/btrfs/free-space-cache.c\nindex 8d4db3d57cf7..17e79ee3e021 100644\n--- a/fs/btrfs/free-space-cache.c\n+++ b/fs/btrfs/free-space-cache.c\n@@ -3677,7 +3677,7 @@ static int do_trimming(struct btrfs_block_group *block_group,\n \t}\n \tspin_unlock(&space_info->lock);\n \n-\tret = btrfs_discard_extent(fs_info, start, bytes, &trimmed);\n+\tret = btrfs_discard_extent(fs_info, start, bytes, &trimmed, false);\n \tif (!ret) {\n \t\t*total_trimmed += trimmed;\n \t\ttrim_state = BTRFS_TRIM_STATE_TRIMMED;\ndiff --git a/fs/btrfs/inode.c b/fs/btrfs/inode.c\nindex 247b373bf5cf..3915e08d252c 100644\n--- a/fs/btrfs/inode.c\n+++ b/fs/btrfs/inode.c\n@@ -3370,7 +3370,7 @@ int btrfs_finish_one_ordered(struct btrfs_ordered_extent *ordered_extent)\n \t\t\t\tbtrfs_discard_extent(fs_info,\n \t\t\t\t\t\tordered_extent->disk_bytenr,\n \t\t\t\t\t\tordered_extent->disk_num_bytes,\n-\t\t\t\t\t\tNULL);\n+\t\t\t\t\t\tNULL, true);\n \t\t\tbtrfs_free_reserved_extent(fs_info,\n \t\t\t\t\tordered_extent->disk_bytenr,\n \t\t\t\t\tordered_extent->disk_num_bytes, true);\ndiff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\nindex caffee6527b2..b0aef4d489e7 100644\n--- a/fs/btrfs/volumes.c\n+++ b/fs/btrfs/volumes.c\n@@ -3425,7 +3425,7 @@ static int btrfs_relocate_chunk_finish(struct btrfs_fs_info *fs_info,\n \t */\n \tif (btrfs_is_zoned(fs_info)) {\n \t\tret = btrfs_discard_extent(fs_info, bg->start, length,\n-\t\t\t\t\t   NULL);\n+\t\t\t\t\t   NULL, true);\n \t\tif (ret)\n \t\t\tbtrfs_info(fs_info,\n \t\t\t\t   \"failed to reset zone %llu after relocation\",\n@@ -6111,7 +6111,7 @@ void btrfs_put_bioc(struct btrfs_io_context *bioc)\n  */\n struct btrfs_discard_stripe *btrfs_map_discard(struct btrfs_fs_info *fs_info,\n \t\t\t\t\t       u64 logical, u64 *length_ret,\n-\t\t\t\t\t       u32 *num_stripes)\n+\t\t\t\t\t       u32 *num_stripes, bool do_remap)\n {\n \tstruct btrfs_chunk_map *map;\n \tstruct btrfs_discard_stripe *stripes;\n@@ -6135,6 +6135,25 @@ struct btrfs_discard_stripe *btrfs_map_discard(struct btrfs_fs_info *fs_info,\n \tif (IS_ERR(map))\n \t\treturn ERR_CAST(map);\n \n+\tif (do_remap && map->type & BTRFS_BLOCK_GROUP_REMAPPED) {\n+\t\tu64 new_logical = logical;\n+\n+\t\tret = btrfs_translate_remap(fs_info, &new_logical, &length);\n+\t\tif (ret)\n+\t\t\tgoto out_free_map;\n+\n+\t\tif (new_logical != logical) {\n+\t\t\tbtrfs_free_chunk_map(map);\n+\n+\t\t\tmap = btrfs_get_chunk_map(fs_info, new_logical,\n+\t\t\t\t\t\t  length);\n+\t\t\tif (IS_ERR(map))\n+\t\t\t\treturn ERR_CAST(map);\n+\n+\t\t\tlogical = new_logical;\n+\t\t}\n+\t}\n+\n \t/* we don't discard raid56 yet */\n \tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n \t\tret = -EOPNOTSUPP;\ndiff --git a/fs/btrfs/volumes.h b/fs/btrfs/volumes.h\nindex ccf0a459180d..505a50689fb0 100644\n--- a/fs/btrfs/volumes.h\n+++ b/fs/btrfs/volumes.h\n@@ -732,7 +732,7 @@ int btrfs_map_repair_block(struct btrfs_fs_info *fs_info,\n \t\t\t   u32 length, int mirror_num);\n struct btrfs_discard_stripe *btrfs_map_discard(struct btrfs_fs_info *fs_info,\n \t\t\t\t\t       u64 logical, u64 *length_ret,\n-\t\t\t\t\t       u32 *num_stripes);\n+\t\t\t\t\t       u32 *num_stripes, bool do_remap);\n int btrfs_read_sys_array(struct btrfs_fs_info *fs_info);\n int btrfs_read_chunk_tree(struct btrfs_fs_info *fs_info);\n struct btrfs_block_group *btrfs_create_chunk(struct btrfs_trans_handle *trans,\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "Author addressed a concern about the remap tree relocation process by introducing new functions to handle the actual work of doing a relocation using the remap tree, including finding and removing identity remaps and adding new remap entries.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "new functions introduced",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a function do_remap_tree_reloc(), which does the actual work of\ndoing a relocation using the remap tree.\n\nIn a loop we call do_remap_reloc_trans(), which searches for the first\nidentity remap for the block group. We call btrfs_reserve_extent() to\nfind space elsewhere for it, and read the data into memory and write it\nto the new location. We then carve out the identity remap and replace it\nwith an actual remap, which points to the new location in which to look.\n\nOnce the last identity remap has been removed we call\nlast_identity_remap_gone(), which, as with deletions, removes the\nchunk's stripes and device extents.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/relocation.c | 337 ++++++++++++++++++++++++++++++++++++++++++\n 1 file changed, 337 insertions(+)\n\ndiff --git a/fs/btrfs/relocation.c b/fs/btrfs/relocation.c\nindex 82f0e15f0f84..20cf0f7fd401 100644\n--- a/fs/btrfs/relocation.c\n+++ b/fs/btrfs/relocation.c\n@@ -4633,6 +4633,61 @@ static int create_remap_tree_entries(struct btrfs_trans_handle *trans,\n \treturn ret;\n }\n \n+static int find_next_identity_remap(struct btrfs_trans_handle *trans,\n+\t\t\t\t    struct btrfs_path *path, u64 bg_end,\n+\t\t\t\t    u64 last_start, u64 *start,\n+\t\t\t\t    u64 *length)\n+{\n+\tint ret;\n+\tstruct btrfs_key key, found_key;\n+\tstruct btrfs_root *remap_root = trans->fs_info->remap_root;\n+\tstruct extent_buffer *leaf;\n+\n+\tkey.objectid = last_start;\n+\tkey.type = BTRFS_IDENTITY_REMAP_KEY;\n+\tkey.offset = 0;\n+\n+\tret = btrfs_search_slot(trans, remap_root, &key, path, 0, 0);\n+\tif (ret < 0)\n+\t\tgoto out;\n+\n+\tleaf = path->nodes[0];\n+\twhile (true) {\n+\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n+\t\t\tret = btrfs_next_leaf(remap_root, path);\n+\n+\t\t\tif (ret != 0) {\n+\t\t\t\tif (ret == 1)\n+\t\t\t\t\tret = -ENOENT;\n+\t\t\t\tgoto out;\n+\t\t\t}\n+\n+\t\t\tleaf = path->nodes[0];\n+\t\t}\n+\n+\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n+\n+\t\tif (found_key.objectid >= bg_end) {\n+\t\t\tret = -ENOENT;\n+\t\t\tgoto out;\n+\t\t}\n+\n+\t\tif (found_key.type == BTRFS_IDENTITY_REMAP_KEY) {\n+\t\t\t*start = found_key.objectid;\n+\t\t\t*length = found_key.offset;\n+\t\t\tret = 0;\n+\t\t\tgoto out;\n+\t\t}\n+\n+\t\tpath->slots[0]++;\n+\t}\n+\n+out:\n+\tbtrfs_release_path(path);\n+\n+\treturn ret;\n+}\n+\n static int remove_chunk_stripes(struct btrfs_trans_handle *trans,\n \t\t\t\tstruct btrfs_chunk_map *chunk_map,\n \t\t\t\tstruct btrfs_path *path)\n@@ -4779,6 +4834,96 @@ static void adjust_identity_remap_count(struct btrfs_trans_handle *trans,\n \t\tbtrfs_mark_bg_fully_remapped(bg, trans);\n }\n \n+static int add_remap_entry(struct btrfs_trans_handle *trans,\n+\t\t\t   struct btrfs_path *path,\n+\t\t\t   struct btrfs_block_group *src_bg, u64 old_addr,\n+\t\t\t   u64 new_addr, u64 length)\n+{\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tstruct btrfs_key key, new_key;\n+\tint ret;\n+\tint identity_count_delta = 0;\n+\n+\tkey.objectid = old_addr;\n+\tkey.type = (u8)-1;\n+\tkey.offset = (u64)-1;\n+\n+\tret = btrfs_search_slot(trans, fs_info->remap_root, &key, path, -1, 1);\n+\tif (ret < 0)\n+\t\tgoto end;\n+\n+\tif (path->slots[0] == 0) {\n+\t\tret = -ENOENT;\n+\t\tgoto end;\n+\t}\n+\n+\tpath->slots[0]--;\n+\n+\tbtrfs_item_key_to_cpu(path->nodes[0], &key, path->slots[0]);\n+\n+\tif (key.type != BTRFS_IDENTITY_REMAP_KEY ||\n+\t    key.objectid > old_addr ||\n+\t    key.objectid + key.offset <= old_addr) {\n+\t\tret = -ENOENT;\n+\t\tgoto end;\n+\t}\n+\n+\t/* Shorten or delete identity mapping entry. */\n+\n+\tif (key.objectid == old_addr) {\n+\t\tret = btrfs_del_item(trans, fs_info->remap_root, path);\n+\t\tif (ret)\n+\t\t\tgoto end;\n+\n+\t\tidentity_count_delta--;\n+\t} else {\n+\t\tnew_key.objectid = key.objectid;\n+\t\tnew_key.type = BTRFS_IDENTITY_REMAP_KEY;\n+\t\tnew_key.offset = old_addr - key.objectid;\n+\n+\t\tbtrfs_set_item_key_safe(trans, path, &new_key);\n+\t}\n+\n+\tbtrfs_release_path(path);\n+\n+\t/* Create new remap entry. */\n+\n+\tret = add_remap_item(trans, path, new_addr, length, old_addr);\n+\tif (ret)\n+\t\tgoto end;\n+\n+\t/* Add entry for remainder of identity mapping, if necessary. */\n+\n+\tif (key.objectid + key.offset != old_addr + length) {\n+\t\tnew_key.objectid = old_addr + length;\n+\t\tnew_key.type = BTRFS_IDENTITY_REMAP_KEY;\n+\t\tnew_key.offset = key.objectid + key.offset - old_addr - length;\n+\n+\t\tret = btrfs_insert_empty_item(trans, fs_info->remap_root,\n+\t\t\t\t\t      path, &new_key, 0);\n+\t\tif (ret)\n+\t\t\tgoto end;\n+\n+\t\tbtrfs_release_path(path);\n+\n+\t\tidentity_count_delta++;\n+\t}\n+\n+\t/* Add backref. */\n+\n+\tret = add_remap_backref_item(trans, path, new_addr, length, old_addr);\n+\tif (ret)\n+\t\tgoto end;\n+\n+\tif (identity_count_delta != 0)\n+\t\tadjust_identity_remap_count(trans, src_bg, identity_count_delta);\n+\n+end:\n+\tbtrfs_release_path(path);\n+\n+\treturn ret;\n+}\n+\n static int mark_chunk_remapped(struct btrfs_trans_handle *trans,\n \t\t\t       struct btrfs_path *path, uint64_t start)\n {\n@@ -4828,6 +4973,190 @@ static int mark_chunk_remapped(struct btrfs_trans_handle *trans,\n \treturn ret;\n }\n \n+static int do_remap_reloc_trans(struct btrfs_fs_info *fs_info,\n+\t\t\t\tstruct btrfs_block_group *src_bg,\n+\t\t\t\tstruct btrfs_path *path, u64 *last_start)\n+{\n+\tstruct btrfs_trans_handle *trans;\n+\tstruct btrfs_root *extent_root;\n+\tstruct btrfs_key ins;\n+\tstruct btrfs_block_group *dest_bg = NULL;\n+\tu64 start, remap_length, length, new_addr, min_size;\n+\tint ret;\n+\tbool no_more = false;\n+\tbool is_data = src_bg->flags & BTRFS_BLOCK_GROUP_DATA;\n+\tbool made_reservation = false, bg_needs_free_space;\n+\tstruct btrfs_space_info *sinfo = src_bg->space_info;\n+\n+\textent_root = btrfs_extent_root(fs_info, src_bg->start);\n+\n+\ttrans = btrfs_start_transaction(extent_root, 0);\n+\tif (IS_ERR(trans))\n+\t\treturn PTR_ERR(trans);\n+\n+\tmutex_lock(&fs_info->remap_mutex);\n+\n+\tret = find_next_identity_remap(trans, path, src_bg->start + src_bg->length,\n+\t\t\t\t       *last_start, &start, &remap_length);\n+\tif (ret == -ENOENT) {\n+\t\tno_more = true;\n+\t\tgoto next;\n+\t} else if (ret) {\n+\t\tmutex_unlock(&fs_info->remap_mutex);\n+\t\tbtrfs_end_transaction(trans);\n+\t\treturn ret;\n+\t}\n+\n+\t/* Try to reserve enough space for block. */\n+\n+\tspin_lock(&sinfo->lock);\n+\tbtrfs_space_info_update_bytes_may_use(sinfo, remap_length);\n+\tspin_unlock(&sinfo->lock);\n+\n+\tif (is_data)\n+\t\tmin_size = fs_info->sectorsize;\n+\telse\n+\t\tmin_size = fs_info->nodesize;\n+\n+\t/*\n+\t * We're using btrfs_reserve_extent() to allocate a contiguous\n+\t * logical address range, but this will become a remap item rather than\n+\t * an extent in the extent tree.\n+\t *\n+\t * Short allocations are fine: it means that we chop off the beginning\n+\t * of the identity remap that we're processing, and will tackle the\n+\t * rest of it the next time round.\n+\t */\n+\tret = btrfs_reserve_extent(fs_info->fs_root, remap_length,\n+\t\t\t\t   remap_length, min_size,\n+\t\t\t\t   0, 0, &ins, is_data, false);\n+\tif (ret) {\n+\t\tspin_lock(&sinfo->lock);\n+\t\tbtrfs_space_info_update_bytes_may_use(sinfo, -remap_length);\n+\t\tspin_unlock(&sinfo->lock);\n+\n+\t\tmutex_unlock(&fs_info->remap_mutex);\n+\t\tbtrfs_end_transaction(trans);\n+\t\treturn ret;\n+\t}\n+\n+\tmade_reservation = true;\n+\n+\tnew_addr = ins.objectid;\n+\tlength = ins.offset;\n+\n+\tif (!is_data && !IS_ALIGNED(length, fs_info->nodesize)) {\n+\t\tu64 new_length = ALIGN_DOWN(length, fs_info->nodesize);\n+\n+\t\tbtrfs_free_reserved_extent(fs_info, new_addr + new_length,\n+\t\t\t\t\t   length - new_length, 0);\n+\n+\t\tlength = new_length;\n+\t}\n+\n+\tdest_bg = btrfs_lookup_block_group(fs_info, new_addr);\n+\n+\tmutex_lock(&dest_bg->free_space_lock);\n+\tbg_needs_free_space = test_bit(BLOCK_GROUP_FLAG_NEEDS_FREE_SPACE,\n+\t\t\t\t       &dest_bg->runtime_flags);\n+\tmutex_unlock(&dest_bg->free_space_lock);\n+\n+\tif (bg_needs_free_space) {\n+\t\tret = btrfs_add_block_group_free_space(trans, dest_bg);\n+\t\tif (ret)\n+\t\t\tgoto fail;\n+\t}\n+\n+\tret = copy_remapped_data(fs_info, start, new_addr, length);\n+\tif (ret)\n+\t\tgoto fail;\n+\n+\tret = btrfs_remove_from_free_space_tree(trans, new_addr, length);\n+\tif (ret)\n+\t\tgoto fail;\n+\n+\tret = add_remap_entry(trans, path, src_bg, start, new_addr, length);\n+\tif (ret) {\n+\t\tbtrfs_add_to_free_space_tree(trans, new_addr, length);\n+\t\tgoto fail;\n+\t}\n+\n+\tadjust_block_group_remap_bytes(trans, dest_bg, length);\n+\tbtrfs_free_reserved_bytes(dest_bg, length, 0);\n+\n+\tspin_lock(&sinfo->lock);\n+\tsinfo->bytes_readonly += length;\n+\tspin_unlock(&sinfo->lock);\n+\n+next:\n+\tif (dest_bg)\n+\t\tbtrfs_put_block_group(dest_bg);\n+\n+\tif (made_reservation)\n+\t\tbtrfs_dec_block_group_reservations(fs_info, new_addr);\n+\n+\tmutex_unlock(&fs_info->remap_mutex);\n+\n+\tif (src_bg->identity_remap_count == 0) {\n+\t\tbool mark_fully_remapped = false;\n+\n+\t\tspin_lock(&src_bg->lock);\n+\n+\t\tif (!test_bit(BLOCK_GROUP_FLAG_FULLY_REMAPPED, &src_bg->runtime_flags)) {\n+\t\t\tmark_fully_remapped = true;\n+\t\t\tset_bit(BLOCK_GROUP_FLAG_FULLY_REMAPPED,\n+\t\t\t\t&src_bg->runtime_flags);\n+\t\t}\n+\n+\t\tspin_unlock(&src_bg->lock);\n+\n+\t\tif (mark_fully_remapped)\n+\t\t\tbtrfs_mark_bg_fully_remapped(src_bg, trans);\n+\t}\n+\n+\tret = btrfs_end_transaction(trans);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tif (no_more)\n+\t\treturn 1;\n+\n+\t*last_start = start;\n+\n+\treturn 0;\n+\n+fail:\n+\tif (dest_bg)\n+\t\tbtrfs_put_block_group(dest_bg);\n+\n+\tbtrfs_free_reserved_extent(fs_info, new_addr, length, 0);\n+\n+\tmutex_unlock(&fs_info->remap_mutex);\n+\tbtrfs_end_transaction(trans);\n+\n+\treturn ret;\n+}\n+\n+static int do_remap_reloc(struct btrfs_fs_info *fs_info,\n+\t\t\t  struct btrfs_path *path, struct btrfs_block_group *bg)\n+{\n+\tu64 last_start;\n+\tint ret;\n+\n+\tlast_start = bg->start;\n+\n+\twhile (true) {\n+\t\tret = do_remap_reloc_trans(fs_info, bg, path, &last_start);\n+\t\tif (ret) {\n+\t\t\tif (ret == 1)\n+\t\t\t\tret = 0;\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+\n+\treturn ret;\n+}\n+\n int btrfs_translate_remap(struct btrfs_fs_info *fs_info, u64 *logical,\n \t\t\t  u64 *length)\n {\n@@ -5121,6 +5450,14 @@ int btrfs_relocate_block_group(struct btrfs_fs_info *fs_info, u64 group_start,\n \t\t}\n \n \t\tret = start_block_group_remapping(fs_info, path, bg);\n+\t\tif (ret)\n+\t\t\tgoto out;\n+\n+\t\tret = do_remap_reloc(fs_info, path, rc->block_group);\n+\t\tif (ret)\n+\t\t\tgoto out;\n+\n+\t\tbtrfs_delete_unused_bgs(fs_info);\n \t} else {\n \t\tret = do_nonremap_reloc(fs_info, verbose, rc);\n \t}\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author addressed a concern about corruption when the remap tree is relocated, explaining that it would cause a level-0 block to have metadata from a different level in the extent tree. They proposed reworking the remap tree code to operate via delayed refs or removing its entries from the extent tree altogether, which would also reduce write amplification.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "There is the following potential problem with the remap tree and delayed refs:\n\n* Remapped extent freed in a delayed ref, which removes an entry from the\n  remap tree\n* Remap tree now small enough to fit in a single leaf\n* Corruption as we now have a level-0 block with a level-1 metadata item\n  in the extent tree\n\nOne solution to this would be to rework the remap tree code so that it operates\nvia delayed refs. But as we're hoping to remove cow-only metadata items in the\nfuture anyway, change things so that the remap tree doesn't have any entries in\nthe extent tree. This also has the benefit of reducing write amplification.\n\nWe also make it so that the clear_cache mount option is a no-op, as with the\nextent tree v2, as the free-space tree can no longer be recreated from the\nextent tree.\n\nFinally disable relocating the remap tree itself, which is added back in\na later patch. As it is we would get corruption as the traditional\nrelocation method walks the extent tree, and we're removing its metadata\nitems.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/disk-io.c     |  3 +++\n fs/btrfs/extent-tree.c | 31 ++++++++++++++++++++++++++++++-\n fs/btrfs/volumes.c     |  3 +++\n 3 files changed, 36 insertions(+), 1 deletion(-)\n\ndiff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c\nindex cbfb7127b528..c36367f9017f 100644\n--- a/fs/btrfs/disk-io.c\n+++ b/fs/btrfs/disk-io.c\n@@ -3007,6 +3007,9 @@ int btrfs_start_pre_rw_mount(struct btrfs_fs_info *fs_info)\n \t\tif (btrfs_fs_incompat(fs_info, EXTENT_TREE_V2))\n \t\t\tbtrfs_warn(fs_info,\n \t\t\t\t   \"'clear_cache' option is ignored with extent tree v2\");\n+\t\telse if (btrfs_fs_incompat(fs_info, REMAP_TREE))\n+\t\t\tbtrfs_warn(fs_info,\n+\t\t\t\t   \"'clear_cache' option is ignored with remap tree\");\n \t\telse\n \t\t\trebuild_free_space_tree = true;\n \t} else if (btrfs_fs_compat_ro(fs_info, FREE_SPACE_TREE) &&\ndiff --git a/fs/btrfs/extent-tree.c b/fs/btrfs/extent-tree.c\nindex 1dcd69fe97ed..43473a6d91d7 100644\n--- a/fs/btrfs/extent-tree.c\n+++ b/fs/btrfs/extent-tree.c\n@@ -1553,6 +1553,28 @@ static void free_head_ref_squota_rsv(struct btrfs_fs_info *fs_info,\n \t\t\t\t  BTRFS_QGROUP_RSV_DATA);\n }\n \n+static int drop_remap_tree_ref(struct btrfs_trans_handle *trans,\n+\t\t\t       const struct btrfs_delayed_ref_node *node)\n+{\n+\tu64 bytenr = node->bytenr;\n+\tu64 num_bytes = node->num_bytes;\n+\tint ret;\n+\n+\tret = btrfs_add_to_free_space_tree(trans, bytenr, num_bytes);\n+\tif (unlikely(ret)) {\n+\t\tbtrfs_abort_transaction(trans, ret);\n+\t\treturn ret;\n+\t}\n+\n+\tret = btrfs_update_block_group(trans, bytenr, num_bytes, false);\n+\tif (unlikely(ret)) {\n+\t\tbtrfs_abort_transaction(trans, ret);\n+\t\treturn ret;\n+\t}\n+\n+\treturn 0;\n+}\n+\n static int run_delayed_data_ref(struct btrfs_trans_handle *trans,\n \t\t\t\tstruct btrfs_delayed_ref_head *href,\n \t\t\t\tconst struct btrfs_delayed_ref_node *node,\n@@ -1747,7 +1769,10 @@ static int run_delayed_tree_ref(struct btrfs_trans_handle *trans,\n \t} else if (node->action == BTRFS_ADD_DELAYED_REF) {\n \t\tret = __btrfs_inc_extent_ref(trans, node, extent_op);\n \t} else if (node->action == BTRFS_DROP_DELAYED_REF) {\n-\t\tret = __btrfs_free_extent(trans, href, node, extent_op);\n+\t\tif (node->ref_root == BTRFS_REMAP_TREE_OBJECTID)\n+\t\t\tret = drop_remap_tree_ref(trans, node);\n+\t\telse\n+\t\t\tret = __btrfs_free_extent(trans, href, node, extent_op);\n \t} else {\n \t\tBUG();\n \t}\n@@ -4886,6 +4911,9 @@ static int alloc_reserved_tree_block(struct btrfs_trans_handle *trans,\n \tint level = btrfs_delayed_ref_owner(node);\n \tbool skinny_metadata = btrfs_fs_incompat(fs_info, SKINNY_METADATA);\n \n+\tif (unlikely(node->ref_root == BTRFS_REMAP_TREE_OBJECTID))\n+\t\tgoto skip;\n+\n \textent_key.objectid = node->bytenr;\n \tif (skinny_metadata) {\n \t\t/* The owner of a tree block is the level. */\n@@ -4938,6 +4966,7 @@ static int alloc_reserved_tree_block(struct btrfs_trans_handle *trans,\n \n \tbtrfs_free_path(path);\n \n+skip:\n \treturn alloc_reserved_extent(trans, node->bytenr, fs_info->nodesize);\n }\n \ndiff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\nindex 070efac46a81..d6060e0e2144 100644\n--- a/fs/btrfs/volumes.c\n+++ b/fs/btrfs/volumes.c\n@@ -3970,6 +3970,9 @@ static bool should_balance_chunk(struct extent_buffer *leaf, struct btrfs_chunk\n \tstruct btrfs_balance_args *bargs = NULL;\n \tu64 chunk_type = btrfs_chunk_type(leaf, chunk);\n \n+\tif (chunk_type & BTRFS_BLOCK_GROUP_METADATA_REMAP)\n+\t\treturn false;\n+\n \t/* type filter */\n \tif (!((chunk_type & BTRFS_BLOCK_GROUP_TYPE_MASK) &\n \t      (bctl->flags & BTRFS_BALANCE_TYPE_MASK))) {\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author addressed a concern about removing free space for remapped block groups, explained that btrfs_remove_bg_from_sinfo() is called only if the REMAPPED flag is not set, and confirmed that this is the intended behavior.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "confirmed intention"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Handle the case where we free an extent from a block group that has the\nREMAPPED flag set. Because the remap tree is orthogonal to the extent\ntree, for data this may be within any number of identity remaps or\nactual remaps. If we're freeing a metadata node, this will be wholly\ninside one or the other.\n\nbtrfs_remove_extent_from_remap_tree() searches the remap tree for the\nremaps that cover the range in question, then calls\nremove_range_from_remap_tree() for each one, to punch a hole in the\nremap and adjust the free-space tree.\n\nFor an identity remap, remove_range_from_remap_tree() will adjust the\nblock group's `identity_remap_count` if this changes. If it reaches\nzero we mark the block group as fully remapped.\n\nFor an identity remap, remove_range_from_remap_tree() will adjust the\nblock group's `identity_remap_count` if this changes. If it reaches\nzero we mark the block group as fully remapped.\n\nFully remapped block groups have their chunk stripes removed and their\ndevice extents freed, which makes the disk space available again to the\nchunk allocator. This happens asynchronously: in the cleaner thread for\nsync discard and nodiscard, and (in a later patch) in the discard worker\nfor async discard.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/block-group.c |  98 ++++++---\n fs/btrfs/block-group.h |   4 +\n fs/btrfs/disk-io.c     |   6 +\n fs/btrfs/extent-tree.c |  98 ++++++++-\n fs/btrfs/extent-tree.h |   2 +\n fs/btrfs/fs.h          |   4 +-\n fs/btrfs/relocation.c  | 453 +++++++++++++++++++++++++++++++++++++++++\n fs/btrfs/relocation.h  |   5 +\n fs/btrfs/volumes.c     |  57 ++++--\n fs/btrfs/volumes.h     |   6 +\n 10 files changed, 678 insertions(+), 55 deletions(-)\n\ndiff --git a/fs/btrfs/block-group.c b/fs/btrfs/block-group.c\nindex 4962d17a175e..0143b0290a72 100644\n--- a/fs/btrfs/block-group.c\n+++ b/fs/btrfs/block-group.c\n@@ -1067,6 +1067,29 @@ static int remove_block_group_item(struct btrfs_trans_handle *trans,\n \treturn btrfs_del_item(trans, root, path);\n }\n \n+void btrfs_remove_bg_from_sinfo(struct btrfs_block_group *bg)\n+{\n+\tint factor = btrfs_bg_type_to_factor(bg->flags);\n+\n+\tspin_lock(&bg->space_info->lock);\n+\n+\tif (btrfs_test_opt(bg->fs_info, ENOSPC_DEBUG)) {\n+\t\tWARN_ON(bg->space_info->total_bytes < bg->length);\n+\t\tWARN_ON(bg->space_info->bytes_readonly\n+\t\t\t< bg->length - bg->zone_unusable);\n+\t\tWARN_ON(bg->space_info->bytes_zone_unusable\n+\t\t\t< bg->zone_unusable);\n+\t\tWARN_ON(bg->space_info->disk_total < bg->length * factor);\n+\t}\n+\tbg->space_info->total_bytes -= bg->length;\n+\tbg->space_info->bytes_readonly -= (bg->length - bg->zone_unusable);\n+\tbtrfs_space_info_update_bytes_zone_unusable(bg->space_info,\n+\t\t\t\t\t\t    -bg->zone_unusable);\n+\tbg->space_info->disk_total -= bg->length * factor;\n+\n+\tspin_unlock(&bg->space_info->lock);\n+}\n+\n int btrfs_remove_block_group(struct btrfs_trans_handle *trans,\n \t\t\t     struct btrfs_chunk_map *map)\n {\n@@ -1078,7 +1101,6 @@ int btrfs_remove_block_group(struct btrfs_trans_handle *trans,\n \tstruct kobject *kobj = NULL;\n \tint ret;\n \tint index;\n-\tint factor;\n \tstruct btrfs_caching_control *caching_ctl = NULL;\n \tbool remove_map;\n \tbool remove_rsv = false;\n@@ -1087,7 +1109,7 @@ int btrfs_remove_block_group(struct btrfs_trans_handle *trans,\n \tif (!block_group)\n \t\treturn -ENOENT;\n \n-\tBUG_ON(!block_group->ro);\n+\tBUG_ON(!block_group->ro && !(block_group->flags & BTRFS_BLOCK_GROUP_REMAPPED));\n \n \ttrace_btrfs_remove_block_group(block_group);\n \t/*\n@@ -1099,7 +1121,6 @@ int btrfs_remove_block_group(struct btrfs_trans_handle *trans,\n \t\t\t\t  block_group->length);\n \n \tindex = btrfs_bg_flags_to_raid_index(block_group->flags);\n-\tfactor = btrfs_bg_type_to_factor(block_group->flags);\n \n \t/* make sure this block group isn't part of an allocation cluster */\n \tcluster = &fs_info->data_alloc_cluster;\n@@ -1223,26 +1244,11 @@ int btrfs_remove_block_group(struct btrfs_trans_handle *trans,\n \n \tspin_lock(&block_group->space_info->lock);\n \tlist_del_init(&block_group->ro_list);\n-\n-\tif (btrfs_test_opt(fs_info, ENOSPC_DEBUG)) {\n-\t\tWARN_ON(block_group->space_info->total_bytes\n-\t\t\t< block_group->length);\n-\t\tWARN_ON(block_group->space_info->bytes_readonly\n-\t\t\t< block_group->length - block_group->zone_unusable);\n-\t\tWARN_ON(block_group->space_info->bytes_zone_unusable\n-\t\t\t< block_group->zone_unusable);\n-\t\tWARN_ON(block_group->space_info->disk_total\n-\t\t\t< block_group->length * factor);\n-\t}\n-\tblock_group->space_info->total_bytes -= block_group->length;\n-\tblock_group->space_info->bytes_readonly -=\n-\t\t(block_group->length - block_group->zone_unusable);\n-\tbtrfs_space_info_update_bytes_zone_unusable(block_group->space_info,\n-\t\t\t\t\t\t    -block_group->zone_unusable);\n-\tblock_group->space_info->disk_total -= block_group->length * factor;\n-\n \tspin_unlock(&block_group->space_info->lock);\n \n+\tif (!(block_group->flags & BTRFS_BLOCK_GROUP_REMAPPED))\n+\t\tbtrfs_remove_bg_from_sinfo(block_group);\n+\n \t/*\n \t * Remove the free space for the block group from the free space tree\n \t * and the block group's item from the extent tree before marking the\n@@ -1576,8 +1582,10 @@ void btrfs_delete_unused_bgs(struct btrfs_fs_info *fs_info)\n \n \t\tspin_lock(&space_info->lock);\n \t\tspin_lock(&block_group->lock);\n-\t\tif (btrfs_is_block_group_used(block_group) || block_group->ro ||\n-\t\t    list_is_singular(&block_group->list)) {\n+\t\tif (btrfs_is_block_group_used(block_group) ||\n+\t\t    (block_group->ro && !(block_group->flags & BTRFS_BLOCK_GROUP_REMAPPED)) ||\n+\t\t    list_is_singular(&block_group->list) ||\n+\t\t    test_bit(BLOCK_GROUP_FLAG_FULLY_REMAPPED, &block_group->runtime_flags)) {\n \t\t\t/*\n \t\t\t * We want to bail if we made new allocations or have\n \t\t\t * outstanding allocations in this block group.  We do\n@@ -1618,9 +1626,10 @@ void btrfs_delete_unused_bgs(struct btrfs_fs_info *fs_info)\n \t\t * needing to allocate extents from the block group.\n \t\t */\n \t\tused = btrfs_space_info_used(space_info, true);\n-\t\tif ((space_info->total_bytes - block_group->length < used &&\n-\t\t     block_group->zone_unusable < block_group->length) ||\n-\t\t    has_unwritten_metadata(block_group)) {\n+\t\tif (((space_info->total_bytes - block_group->length < used &&\n+\t\t      block_group->zone_unusable < block_group->length) ||\n+\t\t     has_unwritten_metadata(block_group)) &&\n+\t\t    !(block_group->flags & BTRFS_BLOCK_GROUP_REMAPPED)) {\n \t\t\t/*\n \t\t\t * Add a reference for the list, compensate for the ref\n \t\t\t * drop under the \"next\" label for the\n@@ -1785,6 +1794,12 @@ void btrfs_mark_bg_unused(struct btrfs_block_group *bg)\n \t\tbtrfs_get_block_group(bg);\n \t\ttrace_btrfs_add_unused_block_group(bg);\n \t\tlist_add_tail(&bg->bg_list, &fs_info->unused_bgs);\n+\t} else if (bg->flags & BTRFS_BLOCK_GROUP_REMAPPED &&\n+\t\t   bg->identity_remap_count == 0) {\n+\t\t/*\n+\t\t * Leave fully remapped block groups on the\n+\t\t * fully_remapped_bgs list.\n+\t\t */\n \t} else if (!test_bit(BLOCK_GROUP_FLAG_NEW, &bg->runtime_flags)) {\n \t\t/* Pull out the block group from the reclaim_bgs list. */\n \t\ttrace_btrfs_add_unused_block_group(bg);\n@@ -4594,6 +4609,14 @@ int btrfs_free_block_groups(struct btrfs_fs_info *info)\n \t\tlist_del_init(&block_group->bg_list);\n \t\tbtrfs_put_block_group(block_group);\n \t}\n+\n+\twhile (!list_empty(&info->fully_remapped_bgs)) {\n+\t\tblock_group = list_first_entry(&info->fully_remapped_bgs,\n+\t\t\t\t\t       struct btrfs_block_group,\n+\t\t\t\t\t       bg_list);\n+\t\tlist_del_init(&block_group->bg_list);\n+\t\tbtrfs_put_block_group(block_group);\n+\t}\n \tspin_unlock(&info->unused_bgs_lock);\n \n \tspin_lock(&info->zone_active_bgs_lock);\n@@ -4781,3 +4804,26 @@ bool btrfs_block_group_should_use_size_class(const struct btrfs_block_group *bg)\n \t\treturn false;\n \treturn true;\n }\n+\n+void btrfs_mark_bg_fully_remapped(struct btrfs_block_group *bg,\n+\t\t\t\t  struct btrfs_trans_handle *trans)\n+{\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\n+\tspin_lock(&fs_info->unused_bgs_lock);\n+\n+\t/*\n+\t * The block group might already be on the unused_bgs list, remove it\n+\t * if it is. It'll get readded after the async discard worker finishes,\n+\t * or in btrfs_handle_fully_remapped_bgs() if we're not using async\n+\t * discard.\n+\t */\n+\tif (!list_empty(&bg->bg_list))\n+\t\tlist_del(&bg->bg_list);\n+\telse\n+\t\tbtrfs_get_block_group(bg);\n+\n+\tlist_add_tail(&bg->bg_list, &fs_info->fully_remapped_bgs);\n+\n+\tspin_unlock(&fs_info->unused_bgs_lock);\n+}\ndiff --git a/fs/btrfs/block-group.h b/fs/btrfs/block-group.h\nindex 4cee3448ded3..436d51a707a9 100644\n--- a/fs/btrfs/block-group.h\n+++ b/fs/btrfs/block-group.h\n@@ -92,6 +92,7 @@ enum btrfs_block_group_flags {\n \t * transaction.\n \t */\n \tBLOCK_GROUP_FLAG_NEW,\n+\tBLOCK_GROUP_FLAG_FULLY_REMAPPED,\n };\n \n enum btrfs_caching_type {\n@@ -336,6 +337,7 @@ int btrfs_add_new_free_space(struct btrfs_block_group *block_group,\n struct btrfs_trans_handle *btrfs_start_trans_remove_block_group(\n \t\t\t\tstruct btrfs_fs_info *fs_info,\n \t\t\t\tconst u64 chunk_offset);\n+void btrfs_remove_bg_from_sinfo(struct btrfs_block_group *bg);\n int btrfs_remove_block_group(struct btrfs_trans_handle *trans,\n \t\t\t     struct btrfs_chunk_map *map);\n void btrfs_delete_unused_bgs(struct btrfs_fs_info *fs_info);\n@@ -407,5 +409,7 @@ int btrfs_use_block_group_size_class(struct btrfs_block_group *bg,\n \t\t\t\t     enum btrfs_block_group_size_class size_class,\n \t\t\t\t     bool force_wrong_size_class);\n bool btrfs_block_group_should_use_size_class(const struct btrfs_block_group *bg);\n+void btrfs_mark_bg_fully_remapped(struct btrfs_block_group *bg,\n+\t\t\t\t  struct btrfs_trans_handle *trans);\n \n #endif /* BTRFS_BLOCK_GROUP_H */\ndiff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c\nindex b03654ee91f5..ba500e3bf0d8 100644\n--- a/fs/btrfs/disk-io.c\n+++ b/fs/btrfs/disk-io.c\n@@ -1495,6 +1495,10 @@ static int cleaner_kthread(void *arg)\n \t\t */\n \t\tbtrfs_run_defrag_inodes(fs_info);\n \n+\t\tif (btrfs_fs_incompat(fs_info, REMAP_TREE) &&\n+\t\t    !btrfs_test_opt(fs_info, DISCARD_ASYNC))\n+\t\t\tbtrfs_handle_fully_remapped_bgs(fs_info);\n+\n \t\t/*\n \t\t * Acquires fs_info->reclaim_bgs_lock to avoid racing\n \t\t * with relocation (btrfs_relocate_chunk) and relocation\n@@ -2835,6 +2839,7 @@ void btrfs_init_fs_info(struct btrfs_fs_info *fs_info)\n \tINIT_LIST_HEAD(&fs_info->tree_mod_seq_list);\n \tINIT_LIST_HEAD(&fs_info->unused_bgs);\n \tINIT_LIST_HEAD(&fs_info->reclaim_bgs);\n+\tINIT_LIST_HEAD(&fs_info->fully_remapped_bgs);\n \tINIT_LIST_HEAD(&fs_info->zone_active_bgs);\n #ifdef CONFIG_BTRFS_DEBUG\n \tINIT_LIST_HEAD(&fs_info->allocated_roots);\n@@ -2890,6 +2895,7 @@ void btrfs_init_fs_info(struct btrfs_fs_info *fs_info)\n \tmutex_init(&fs_info->chunk_mutex);\n \tmutex_init(&fs_info->transaction_kthread_mutex);\n \tmutex_init(&fs_info->cleaner_mutex);\n+\tmutex_init(&fs_info->remap_mutex);\n \tmutex_init(&fs_info->ro_block_group_mutex);\n \tinit_rwsem(&fs_info->commit_root_sem);\n \tinit_rwsem(&fs_info->cleanup_work_sem);\ndiff --git a/fs/btrfs/extent-tree.c b/fs/btrfs/extent-tree.c\nindex 3868a295be62..fef85ade017c 100644\n--- a/fs/btrfs/extent-tree.c\n+++ b/fs/btrfs/extent-tree.c\n@@ -41,6 +41,7 @@\n #include \"tree-checker.h\"\n #include \"raid-stripe-tree.h\"\n #include \"delayed-inode.h\"\n+#include \"relocation.h\"\n \n #undef SCRAMBLE_DELAYED_REFS\n \n@@ -2844,6 +2845,73 @@ static int unpin_extent_range(struct btrfs_fs_info *fs_info,\n \treturn 0;\n }\n \n+/*\n+ * Complete the remapping of a block group by removing its chunk stripes and\n+ * device extents, and adding it to the unused list if there's no longer any\n+ * extents nominally within it.\n+ */\n+int btrfs_complete_bg_remapping(struct btrfs_block_group *bg)\n+{\n+\tstruct btrfs_fs_info *fs_info = bg->fs_info;\n+\tstruct btrfs_chunk_map *map;\n+\tint ret;\n+\n+\tmap = btrfs_get_chunk_map(fs_info, bg->start, 1);\n+\tif (IS_ERR(map))\n+\t\treturn PTR_ERR(map);\n+\n+\tret = btrfs_last_identity_remap_gone(map, bg);\n+\tif (ret) {\n+\t\tbtrfs_free_chunk_map(map);\n+\t\treturn ret;\n+\t}\n+\n+\t/*\n+\t * Set num_stripes to 0, so that btrfs_remove_dev_extents()\n+\t * won't run a second time.\n+\t */\n+\tmap->num_stripes = 0;\n+\n+\tbtrfs_free_chunk_map(map);\n+\n+\tif (bg->used == 0) {\n+\t\tspin_lock(&fs_info->unused_bgs_lock);\n+\t\tif (!list_empty(&bg->bg_list)) {\n+\t\t\tlist_del_init(&bg->bg_list);\n+\t\t\tbtrfs_put_block_group(bg);\n+\t\t}\n+\t\tspin_unlock(&fs_info->unused_bgs_lock);\n+\n+\t\tbtrfs_mark_bg_unused(bg);\n+\t}\n+\n+\treturn 0;\n+}\n+\n+void btrfs_handle_fully_remapped_bgs(struct btrfs_fs_info *fs_info)\n+{\n+\tstruct btrfs_block_group *bg;\n+\tint ret;\n+\n+\tspin_lock(&fs_info->unused_bgs_lock);\n+\twhile (!list_empty(&fs_info->fully_remapped_bgs)) {\n+\t\tbg = list_first_entry(&fs_info->fully_remapped_bgs,\n+\t\t\t\t      struct btrfs_block_group, bg_list);\n+\t\tlist_del_init(&bg->bg_list);\n+\t\tspin_unlock(&fs_info->unused_bgs_lock);\n+\n+\t\tret = btrfs_complete_bg_remapping(bg);\n+\t\tif (ret) {\n+\t\t\tbtrfs_put_block_group(bg);\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tbtrfs_put_block_group(bg);\n+\t\tspin_lock(&fs_info->unused_bgs_lock);\n+\t}\n+\tspin_unlock(&fs_info->unused_bgs_lock);\n+}\n+\n int btrfs_finish_extent_commit(struct btrfs_trans_handle *trans)\n {\n \tstruct btrfs_fs_info *fs_info = trans->fs_info;\n@@ -2996,11 +3064,23 @@ u64 btrfs_get_extent_owner_root(struct btrfs_fs_info *fs_info,\n }\n \n static int do_free_extent_accounting(struct btrfs_trans_handle *trans,\n-\t\t\t\t     u64 bytenr, struct btrfs_squota_delta *delta)\n+\t\t\t\t     u64 bytenr, struct btrfs_squota_delta *delta,\n+\t\t\t\t     struct btrfs_path *path)\n {\n \tint ret;\n+\tbool remapped = false;\n \tu64 num_bytes = delta->num_bytes;\n \n+\t/* Returns 1 on success and 0 on no-op. */\n+\tret = btrfs_remove_extent_from_remap_tree(trans, path, bytenr,\n+\t\t\t\t\t\t  num_bytes);\n+\tif (unlikely(ret < 0)) {\n+\t\tbtrfs_abort_transaction(trans, ret);\n+\t\treturn ret;\n+\t} else if (ret == 1) {\n+\t\tremapped = true;\n+\t}\n+\n \tif (delta->is_data) {\n \t\tstruct btrfs_root *csum_root;\n \n@@ -3024,10 +3104,16 @@ static int do_free_extent_accounting(struct btrfs_trans_handle *trans,\n \t\treturn ret;\n \t}\n \n-\tret = btrfs_add_to_free_space_tree(trans, bytenr, num_bytes);\n-\tif (unlikely(ret)) {\n-\t\tbtrfs_abort_transaction(trans, ret);\n-\t\treturn ret;\n+\t/*\n+\t * If remapped, FST has already been taken care of in\n+\t * remove_range_from_remap_tree().\n+\t */\n+\tif (!remapped) {\n+\t\tret = btrfs_add_to_free_space_tree(trans, bytenr, num_bytes);\n+\t\tif (unlikely(ret)) {\n+\t\t\tbtrfs_abort_transaction(trans, ret);\n+\t\t\treturn ret;\n+\t\t}\n \t}\n \n \tret = btrfs_update_block_group(trans, bytenr, num_bytes, false);\n@@ -3386,7 +3472,7 @@ static int __btrfs_free_extent(struct btrfs_trans_handle *trans,\n \t\t}\n \t\tbtrfs_release_path(path);\n \n-\t\tret = do_free_extent_accounting(trans, bytenr, &delta);\n+\t\tret = do_free_extent_accounting(trans, bytenr, &delta, path);\n \t}\n \tbtrfs_release_path(path);\n \ndiff --git a/fs/btrfs/extent-tree.h b/fs/btrfs/extent-tree.h\nindex 71bb8109c969..d7b6aeb63656 100644\n--- a/fs/btrfs/extent-tree.h\n+++ b/fs/btrfs/extent-tree.h\n@@ -163,5 +163,7 @@ void btrfs_error_unpin_extent_range(struct btrfs_fs_info *fs_info, u64 start, u6\n int btrfs_discard_extent(struct btrfs_fs_info *fs_info, u64 bytenr,\n \t\t\t u64 num_bytes, u64 *actual_bytes);\n int btrfs_trim_fs(struct btrfs_fs_info *fs_info, struct fstrim_range *range);\n+void btrfs_handle_fully_remapped_bgs(struct btrfs_fs_info *fs_info);\n+int btrfs_complete_bg_remapping(struct btrfs_block_group *bg);\n \n #endif\ndiff --git a/fs/btrfs/fs.h b/fs/btrfs/fs.h\nindex af11f2ce310a..b59bda3f8e62 100644\n--- a/fs/btrfs/fs.h\n+++ b/fs/btrfs/fs.h\n@@ -579,6 +579,7 @@ struct btrfs_fs_info {\n \tstruct mutex transaction_kthread_mutex;\n \tstruct mutex cleaner_mutex;\n \tstruct mutex chunk_mutex;\n+\tstruct mutex remap_mutex;\n \n \t/*\n \t * This is taken to make sure we don't set block groups ro after the\n@@ -832,10 +833,11 @@ struct btrfs_fs_info {\n \tstruct list_head reclaim_bgs;\n \tint bg_reclaim_threshold;\n \n-\t/* Protects the lists unused_bgs and reclaim_bgs. */\n+\t/* Protects the lists unused_bgs, reclaim_bgs, and fully_remapped_bgs. */\n \tspinlock_t unused_bgs_lock;\n \t/* Protected by unused_bgs_lock. */\n \tstruct list_head unused_bgs;\n+\tstruct list_head fully_remapped_bgs;\n \tstruct mutex unused_bg_unpin_mutex;\n \t/* Protect block groups that are going to be deleted */\n \tstruct mutex reclaim_bgs_lock;\ndiff --git a/fs/btrfs/relocation.c b/fs/btrfs/relocation.c\nindex 525f45c668f6..e47234d5a156 100644\n--- a/fs/btrfs/relocation.c\n+++ b/fs/btrfs/relocation.c\n@@ -37,6 +37,7 @@\n #include \"super.h\"\n #include \"tree-checker.h\"\n #include \"raid-stripe-tree.h\"\n+#include \"free-space-tree.h\"\n \n /*\n  * Relocation overview\n@@ -3859,6 +3860,184 @@ static const char *stage_to_string(enum reloc_stage stage)\n \treturn \"unknown\";\n }\n \n+static void adjust_block_group_remap_bytes(struct btrfs_trans_handle *trans,\n+\t\t\t\t\t   struct btrfs_block_group *bg,\n+\t\t\t\t\t   s64 diff)\n+{\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tbool bg_already_dirty = true, mark_unused = false;\n+\n+\tspin_lock(&bg->lock);\n+\n+\tbg->remap_bytes += diff;\n+\n+\tif (bg->used == 0 && bg->remap_bytes == 0)\n+\t\tmark_unused = true;\n+\n+\tspin_unlock(&bg->lock);\n+\n+\tif (mark_unused)\n+\t\tbtrfs_mark_bg_unused(bg);\n+\n+\tspin_lock(&trans->transaction->dirty_bgs_lock);\n+\tif (list_empty(&bg->dirty_list)) {\n+\t\tlist_add_tail(&bg->dirty_list, &trans->transaction->dirty_bgs);\n+\t\tbg_already_dirty = false;\n+\t\tbtrfs_get_block_group(bg);\n+\t}\n+\tspin_unlock(&trans->transaction->dirty_bgs_lock);\n+\n+\t/* Modified block groups are accounted for in the delayed_refs_rsv. */\n+\tif (!bg_already_dirty)\n+\t\tbtrfs_inc_delayed_refs_rsv_bg_updates(fs_info);\n+}\n+\n+static int remove_chunk_stripes(struct btrfs_trans_handle *trans,\n+\t\t\t\tstruct btrfs_chunk_map *chunk_map,\n+\t\t\t\tstruct btrfs_path *path)\n+{\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tstruct btrfs_key key;\n+\tstruct extent_buffer *leaf;\n+\tstruct btrfs_chunk *chunk;\n+\tint ret;\n+\n+\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n+\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n+\tkey.offset = chunk_map->start;\n+\n+\tbtrfs_reserve_chunk_metadata(trans, false);\n+\n+\tret = btrfs_search_slot(trans, fs_info->chunk_root, &key, path,\n+\t\t\t\t0, 1);\n+\tif (ret) {\n+\t\tif (ret == 1) {\n+\t\t\tbtrfs_release_path(path);\n+\t\t\tret = -ENOENT;\n+\t\t}\n+\t\tbtrfs_trans_release_chunk_metadata(trans);\n+\t\treturn ret;\n+\t}\n+\n+\tleaf = path->nodes[0];\n+\n+\tchunk = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_chunk);\n+\tbtrfs_set_chunk_num_stripes(leaf, chunk, 0);\n+\tbtrfs_set_chunk_sub_stripes(leaf, chunk, 0);\n+\n+\tbtrfs_truncate_item(trans, path, offsetof(struct btrfs_chunk, stripe),\n+\t\t\t    1);\n+\n+\tbtrfs_mark_buffer_dirty(trans, leaf);\n+\n+\tbtrfs_release_path(path);\n+\tbtrfs_trans_release_chunk_metadata(trans);\n+\n+\treturn 0;\n+}\n+\n+int btrfs_last_identity_remap_gone(struct btrfs_chunk_map *chunk_map,\n+\t\t\t\t   struct btrfs_block_group *bg)\n+{\n+\tstruct btrfs_fs_info *fs_info = bg->fs_info;\n+\tstruct btrfs_trans_handle *trans;\n+\tint ret;\n+\tunsigned int num_items;\n+\tBTRFS_PATH_AUTO_FREE(path);\n+\n+\tpath = btrfs_alloc_path();\n+\tif (!path)\n+\t\treturn -ENOMEM;\n+\n+\t/*\n+\t * One item for each entry we're removing in the dev extents tree, and\n+\t * another for each device. DUP chunks are all on one device,\n+\t * everything else has one device per stripe.\n+\t */\n+\tif (bg->flags & BTRFS_BLOCK_GROUP_DUP)\n+\t\tnum_items = chunk_map->num_stripes + 1;\n+\telse\n+\t\tnum_items = 2 * chunk_map->num_stripes;\n+\n+\ttrans = btrfs_start_transaction_fallback_global_rsv(fs_info->tree_root,\n+\t\t\t\t\t\t\t    num_items);\n+\tif (IS_ERR(trans))\n+\t\treturn PTR_ERR(trans);\n+\n+\tret = btrfs_remove_dev_extents(trans, chunk_map);\n+\tif (unlikely(ret)) {\n+\t\tbtrfs_abort_transaction(trans, ret);\n+\t\treturn ret;\n+\t}\n+\n+\tmutex_lock(&trans->fs_info->chunk_mutex);\n+\n+\tfor (unsigned int i = 0; i < chunk_map->num_stripes; i++) {\n+\t\tret = btrfs_update_device(trans, chunk_map->stripes[i].dev);\n+\t\tif (unlikely(ret)) {\n+\t\t\tmutex_unlock(&trans->fs_info->chunk_mutex);\n+\t\t\tbtrfs_abort_transaction(trans, ret);\n+\t\t\treturn ret;\n+\t\t}\n+\t}\n+\n+\tmutex_unlock(&trans->fs_info->chunk_mutex);\n+\n+\twrite_lock(&trans->fs_info->mapping_tree_lock);\n+\tbtrfs_chunk_map_device_clear_bits(chunk_map, CHUNK_ALLOCATED);\n+\twrite_unlock(&trans->fs_info->mapping_tree_lock);\n+\n+\tbtrfs_remove_bg_from_sinfo(bg);\n+\n+\tret = remove_chunk_stripes(trans, chunk_map, path);\n+\tif (unlikely(ret)) {\n+\t\tbtrfs_abort_transaction(trans, ret);\n+\t\treturn ret;\n+\t}\n+\n+\tret = btrfs_commit_transaction(trans);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\treturn 0;\n+}\n+\n+static void adjust_identity_remap_count(struct btrfs_trans_handle *trans,\n+\t\t\t\t        struct btrfs_block_group *bg, int delta)\n+{\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tbool bg_already_dirty = true, mark_fully_remapped = false;\n+\n+\tWARN_ON(delta < 0 && -delta > bg->identity_remap_count);\n+\n+\tspin_lock(&bg->lock);\n+\n+\tbg->identity_remap_count += delta;\n+\n+\tif (bg->identity_remap_count == 0 &&\n+\t    !test_bit(BLOCK_GROUP_FLAG_FULLY_REMAPPED, &bg->runtime_flags)) {\n+\t\tset_bit(BLOCK_GROUP_FLAG_FULLY_REMAPPED, &bg->runtime_flags);\n+\t\tmark_fully_remapped = true;\n+\t}\n+\n+\tspin_unlock(&bg->lock);\n+\n+\tspin_lock(&trans->transaction->dirty_bgs_lock);\n+\tif (list_empty(&bg->dirty_list)) {\n+\t\tlist_add_tail(&bg->dirty_list, &trans->transaction->dirty_bgs);\n+\t\tbg_already_dirty = false;\n+\t\tbtrfs_get_block_group(bg);\n+\t}\n+\tspin_unlock(&trans->transaction->dirty_bgs_lock);\n+\n+\t/* Modified block groups are accounted for in the delayed_refs_rsv. */\n+\tif (!bg_already_dirty)\n+\t\tbtrfs_inc_delayed_refs_rsv_bg_updates(fs_info);\n+\n+\tif (mark_fully_remapped)\n+\t\tbtrfs_mark_bg_fully_remapped(bg, trans);\n+}\n+\n int btrfs_translate_remap(struct btrfs_fs_info *fs_info, u64 *logical,\n \t\t\t  u64 *length)\n {\n@@ -4467,3 +4646,277 @@ u64 btrfs_get_reloc_bg_bytenr(const struct btrfs_fs_info *fs_info)\n \t\tlogical = fs_info->reloc_ctl->block_group->start;\n \treturn logical;\n }\n+\n+static int insert_remap_item(struct btrfs_trans_handle *trans,\n+\t\t\t     struct btrfs_path *path, u64 old_addr, u64 length,\n+\t\t\t     u64 new_addr)\n+{\n+\tint ret;\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tstruct btrfs_key key;\n+\tstruct btrfs_remap_item remap = { 0 };\n+\n+\tif (old_addr == new_addr) {\n+\t\t/* Add new identity remap item. */\n+\n+\t\tkey.objectid = old_addr;\n+\t\tkey.type = BTRFS_IDENTITY_REMAP_KEY;\n+\t\tkey.offset = length;\n+\n+\t\tret = btrfs_insert_empty_item(trans, fs_info->remap_root,\n+\t\t\t\t\t      path, &key, 0);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\t} else {\n+\t\t/* Add new remap item. */\n+\n+\t\tkey.objectid = old_addr;\n+\t\tkey.type = BTRFS_REMAP_KEY;\n+\t\tkey.offset = length;\n+\n+\t\tret = btrfs_insert_empty_item(trans, fs_info->remap_root,\n+\t\t\t\t\t      path, &key,\n+\t\t\t\t\t      sizeof(struct btrfs_remap_item));\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\n+\t\tbtrfs_set_stack_remap_address(&remap, new_addr);\n+\n+\t\twrite_extent_buffer(path->nodes[0], &remap,\n+\t\t\tbtrfs_item_ptr_offset(path->nodes[0], path->slots[0]),\n+\t\t\tsizeof(struct btrfs_remap_item));\n+\n+\t\tbtrfs_release_path(path);\n+\n+\t\t/* Add new backref item. */\n+\n+\t\tkey.objectid = new_addr;\n+\t\tkey.type = BTRFS_REMAP_BACKREF_KEY;\n+\t\tkey.offset = length;\n+\n+\t\tret = btrfs_insert_empty_item(trans, fs_info->remap_root,\n+\t\t\t\t\t      path, &key,\n+\t\t\t\t\t      sizeof(struct btrfs_remap_item));\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\n+\t\tbtrfs_set_stack_remap_address(&remap, old_addr);\n+\n+\t\twrite_extent_buffer(path->nodes[0], &remap,\n+\t\t\tbtrfs_item_ptr_offset(path->nodes[0], path->slots[0]),\n+\t\t\tsizeof(struct btrfs_remap_item));\n+\t}\n+\n+\tbtrfs_release_path(path);\n+\n+\treturn 0;\n+}\n+\n+/*\n+ * Punch a hole in the remap item or identity remap item pointed to by path,\n+ * for the range [hole_start, hole_start + hole_length).\n+ */\n+static int remove_range_from_remap_tree(struct btrfs_trans_handle *trans,\n+\t\t\t\t\tstruct btrfs_path *path,\n+\t\t\t\t\tstruct btrfs_block_group *bg,\n+\t\t\t\t\tu64 hole_start, u64 hole_length)\n+{\n+\tint ret;\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tstruct extent_buffer *leaf = path->nodes[0];\n+\tstruct btrfs_key key;\n+\tu64 hole_end, new_addr, remap_start, remap_length, remap_end;\n+\tu64 overlap_length;\n+\tbool is_identity_remap;\n+\tint identity_count_delta = 0;\n+\n+\thole_end = hole_start + hole_length;\n+\n+\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n+\n+\tis_identity_remap = key.type == BTRFS_IDENTITY_REMAP_KEY;\n+\n+\tremap_start = key.objectid;\n+\tremap_length = key.offset;\n+\n+\tremap_end = remap_start + remap_length;\n+\n+\tif (is_identity_remap) {\n+\t\tnew_addr = remap_start;\n+\t} else {\n+\t\tstruct btrfs_remap_item *remap_ptr;\n+\n+\t\tremap_ptr = btrfs_item_ptr(leaf, path->slots[0],\n+\t\t\t\t\t   struct btrfs_remap_item);\n+\t\tnew_addr = btrfs_remap_address(leaf, remap_ptr);\n+\t}\n+\n+\t/* Delete old item. */\n+\n+\tret = btrfs_del_item(trans, fs_info->remap_root, path);\n+\n+\tbtrfs_release_path(path);\n+\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tif (is_identity_remap) {\n+\t\tidentity_count_delta = -1;\n+\t} else {\n+\t\t/* Remove backref. */\n+\n+\t\tkey.objectid = new_addr;\n+\t\tkey.type = BTRFS_REMAP_BACKREF_KEY;\n+\t\tkey.offset = remap_length;\n+\n+\t\tret = btrfs_search_slot(trans, fs_info->remap_root,\n+\t\t\t\t\t&key, path, -1, 1);\n+\t\tif (ret) {\n+\t\t\tif (ret == 1) {\n+\t\t\t\tbtrfs_release_path(path);\n+\t\t\t\tret = -ENOENT;\n+\t\t\t}\n+\t\t\treturn ret;\n+\t\t}\n+\n+\t\tret = btrfs_del_item(trans, fs_info->remap_root, path);\n+\n+\t\tbtrfs_release_path(path);\n+\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\t}\n+\n+\t/* If hole_start > remap_start, re-add the start of the remap item. */\n+\tif (hole_start > remap_start) {\n+\t\tret = insert_remap_item(trans, path, remap_start,\n+\t\t\t\t\thole_start - remap_start, new_addr);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\n+\t\tif (is_identity_remap)\n+\t\t\tidentity_count_delta++;\n+\t}\n+\n+\t/* If hole_end < remap_end, re-add the end of the remap item. */\n+\tif (hole_end < remap_end) {\n+\t\tret = insert_remap_item(trans, path, hole_end,\n+\t\t\t\tremap_end - hole_end,\n+\t\t\t\thole_end - remap_start + new_addr);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\n+\t\tif (is_identity_remap)\n+\t\t\tidentity_count_delta++;\n+\t}\n+\n+\tif (identity_count_delta != 0)\n+\t\tadjust_identity_remap_count(trans, bg, identity_count_delta);\n+\n+\toverlap_length = min_t(u64, hole_end, remap_end) -\n+\t\t\t max_t(u64, hole_start, remap_start);\n+\n+\tif (!is_identity_remap) {\n+\t\tstruct btrfs_block_group *dest_bg;\n+\n+\t\tdest_bg = btrfs_lookup_block_group(fs_info, new_addr);\n+\n+\t\tadjust_block_group_remap_bytes(trans, dest_bg, -overlap_length);\n+\n+\t\tbtrfs_put_block_group(dest_bg);\n+\n+\t\tret = btrfs_add_to_free_space_tree(trans,\n+\t\t\t\t\t     hole_start - remap_start + new_addr,\n+\t\t\t\t\t     overlap_length);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\t}\n+\n+\tret = overlap_length;\n+\n+\treturn ret;\n+}\n+\n+/*\n+ * Returns 1 if remove_range_from_remap_tree() has been called successfully,\n+ * 0 if block group wasn't remapped, and a negative number on error.\n+ */\n+int btrfs_remove_extent_from_remap_tree(struct btrfs_trans_handle *trans,\n+\t\t\t\t\tstruct btrfs_path *path,\n+\t\t\t\t\tu64 bytenr, u64 num_bytes)\n+{\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tstruct btrfs_key key, found_key;\n+\tstruct extent_buffer *leaf;\n+\tstruct btrfs_block_group *bg;\n+\tint ret, length;\n+\n+\tif (!(btrfs_super_incompat_flags(fs_info->super_copy) &\n+\t      BTRFS_FEATURE_INCOMPAT_REMAP_TREE))\n+\t\treturn 0;\n+\n+\tbg = btrfs_lookup_block_group(fs_info, bytenr);\n+\tif (!bg)\n+\t\treturn 0;\n+\n+\tmutex_lock(&fs_info->remap_mutex);\n+\n+\tif (!(bg->flags & BTRFS_BLOCK_GROUP_REMAPPED)) {\n+\t\tmutex_unlock(&fs_info->remap_mutex);\n+\t\tbtrfs_put_block_group(bg);\n+\t\treturn 0;\n+\t}\n+\n+\tdo {\n+\t\tkey.objectid = bytenr;\n+\t\tkey.type = (u8)-1;\n+\t\tkey.offset = (u64)-1;\n+\n+\t\tret = btrfs_search_slot(trans, fs_info->remap_root, &key, path,\n+\t\t\t\t\t-1, 1);\n+\t\tif (ret < 0)\n+\t\t\tgoto end;\n+\n+\t\tleaf = path->nodes[0];\n+\n+\t\tif (path->slots[0] == 0) {\n+\t\t\tret = -ENOENT;\n+\t\t\tgoto end;\n+\t\t}\n+\n+\t\tpath->slots[0]--;\n+\n+\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n+\n+\t\tif (found_key.type != BTRFS_IDENTITY_REMAP_KEY &&\n+\t\t    found_key.type != BTRFS_REMAP_KEY) {\n+\t\t\tret = -ENOENT;\n+\t\t\tgoto end;\n+\t\t}\n+\n+\t\tif (bytenr < found_key.objectid ||\n+\t\t    bytenr >= found_key.objectid + found_key.offset) {\n+\t\t\tret = -ENOENT;\n+\t\t\tgoto end;\n+\t\t}\n+\n+\t\tlength = remove_range_from_remap_tree(trans, path, bg, bytenr,\n+\t\t\t\t\t\t      num_bytes);\n+\t\tif (length < 0) {\n+\t\t\tret = length;\n+\t\t\tgoto end;\n+\t\t}\n+\n+\t\tbytenr += length;\n+\t\tnum_bytes -= length;\n+\t} while (num_bytes > 0);\n+\n+\tret = 1;\n+\n+end:\n+\tmutex_unlock(&fs_info->remap_mutex);\n+\n+\tbtrfs_put_block_group(bg);\n+\tbtrfs_release_path(path);\n+\treturn ret;\n+}\ndiff --git a/fs/btrfs/relocation.h b/fs/btrfs/relocation.h\nindex b2ba83966650..0f4874f815db 100644\n--- a/fs/btrfs/relocation.h\n+++ b/fs/btrfs/relocation.h\n@@ -33,5 +33,10 @@ bool btrfs_should_ignore_reloc_root(const struct btrfs_root *root);\n u64 btrfs_get_reloc_bg_bytenr(const struct btrfs_fs_info *fs_info);\n int btrfs_translate_remap(struct btrfs_fs_info *fs_info, u64 *logical,\n \t\t\t  u64 *length);\n+int btrfs_remove_extent_from_remap_tree(struct btrfs_trans_handle *trans,\n+\t\t\t\t\tstruct btrfs_path *path,\n+\t\t\t\t\tu64 bytenr, u64 num_bytes);\n+int btrfs_last_identity_remap_gone(struct btrfs_chunk_map *chunk_map,\n+\t\t\t\t   struct btrfs_block_group *bg);\n \n #endif\ndiff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\nindex 557ce56df800..46c5acc96725 100644\n--- a/fs/btrfs/volumes.c\n+++ b/fs/btrfs/volumes.c\n@@ -2923,8 +2923,8 @@ int btrfs_init_new_device(struct btrfs_fs_info *fs_info, const char *device_path\n \treturn ret;\n }\n \n-static noinline int btrfs_update_device(struct btrfs_trans_handle *trans,\n-\t\t\t\t\tstruct btrfs_device *device)\n+int btrfs_update_device(struct btrfs_trans_handle *trans,\n+\t\t\tstruct btrfs_device *device)\n {\n \tint ret;\n \tBTRFS_PATH_AUTO_FREE(path);\n@@ -3222,25 +3222,13 @@ static int remove_chunk_item(struct btrfs_trans_handle *trans,\n \treturn btrfs_free_chunk(trans, chunk_offset);\n }\n \n-int btrfs_remove_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n+int btrfs_remove_dev_extents(struct btrfs_trans_handle *trans,\n+\t\t\t     struct btrfs_chunk_map *map)\n {\n \tstruct btrfs_fs_info *fs_info = trans->fs_info;\n-\tstruct btrfs_chunk_map *map;\n+\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n \tu64 dev_extent_len = 0;\n \tint i, ret = 0;\n-\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n-\n-\tmap = btrfs_get_chunk_map(fs_info, chunk_offset, 1);\n-\tif (IS_ERR(map)) {\n-\t\t/*\n-\t\t * This is a logic error, but we don't want to just rely on the\n-\t\t * user having built with ASSERT enabled, so if ASSERT doesn't\n-\t\t * do anything we still error out.\n-\t\t */\n-\t\tDEBUG_WARN(\"errr %ld reading chunk map at offset %llu\",\n-\t\t\t   PTR_ERR(map), chunk_offset);\n-\t\treturn PTR_ERR(map);\n-\t}\n \n \t/*\n \t * First delete the device extent items from the devices btree.\n@@ -3261,7 +3249,7 @@ int btrfs_remove_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n \t\tif (unlikely(ret)) {\n \t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n \t\t\tbtrfs_abort_transaction(trans, ret);\n-\t\t\tgoto out;\n+\t\t\treturn ret;\n \t\t}\n \n \t\tif (device->bytes_used > 0) {\n@@ -3281,6 +3269,31 @@ int btrfs_remove_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n \t}\n \tmutex_unlock(&fs_devices->device_list_mutex);\n \n+\treturn 0;\n+}\n+\n+int btrfs_remove_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n+{\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tstruct btrfs_chunk_map *map;\n+\tint ret;\n+\n+\tmap = btrfs_get_chunk_map(fs_info, chunk_offset, 1);\n+\tif (IS_ERR(map)) {\n+\t\t/*\n+\t\t * This is a logic error, but we don't want to just rely on the\n+\t\t * user having built with ASSERT enabled, so if ASSERT doesn't\n+\t\t * do anything we still error out.\n+\t\t */\n+\t\tDEBUG_WARN(\"errr %ld reading chunk map at offset %llu\",\n+\t\t\t   PTR_ERR(map), chunk_offset);\n+\t\treturn PTR_ERR(map);\n+\t}\n+\n+\tret = btrfs_remove_dev_extents(trans, map);\n+\tif (ret)\n+\t\tgoto out;\n+\n \t/*\n \t * We acquire fs_info->chunk_mutex for 2 reasons:\n \t *\n@@ -5417,7 +5430,7 @@ static void chunk_map_device_set_bits(struct btrfs_chunk_map *map, unsigned int\n \t}\n }\n \n-static void chunk_map_device_clear_bits(struct btrfs_chunk_map *map, unsigned int bits)\n+void btrfs_chunk_map_device_clear_bits(struct btrfs_chunk_map *map, unsigned int bits)\n {\n \tfor (int i = 0; i < map->num_stripes; i++) {\n \t\tstruct btrfs_io_stripe *stripe = &map->stripes[i];\n@@ -5434,7 +5447,7 @@ void btrfs_remove_chunk_map(struct btrfs_fs_info *fs_info, struct btrfs_chunk_ma\n \twrite_lock(&fs_info->mapping_tree_lock);\n \trb_erase_cached(&map->rb_node, &fs_info->mapping_tree);\n \tRB_CLEAR_NODE(&map->rb_node);\n-\tchunk_map_device_clear_bits(map, CHUNK_ALLOCATED);\n+\tbtrfs_chunk_map_device_clear_bits(map, CHUNK_ALLOCATED);\n \twrite_unlock(&fs_info->mapping_tree_lock);\n \n \t/* Once for the tree reference. */\n@@ -5470,7 +5483,7 @@ int btrfs_add_chunk_map(struct btrfs_fs_info *fs_info, struct btrfs_chunk_map *m\n \t\treturn -EEXIST;\n \t}\n \tchunk_map_device_set_bits(map, CHUNK_ALLOCATED);\n-\tchunk_map_device_clear_bits(map, CHUNK_TRIMMED);\n+\tbtrfs_chunk_map_device_clear_bits(map, CHUNK_TRIMMED);\n \twrite_unlock(&fs_info->mapping_tree_lock);\n \n \treturn 0;\n@@ -5826,7 +5839,7 @@ void btrfs_mapping_tree_free(struct btrfs_fs_info *fs_info)\n \t\tmap = rb_entry(node, struct btrfs_chunk_map, rb_node);\n \t\trb_erase_cached(&map->rb_node, &fs_info->mapping_tree);\n \t\tRB_CLEAR_NODE(&map->rb_node);\n-\t\tchunk_map_device_clear_bits(map, CHUNK_ALLOCATED);\n+\t\tbtrfs_chunk_map_device_clear_bits(map, CHUNK_ALLOCATED);\n \t\t/* Once for the tree ref. */\n \t\tbtrfs_free_chunk_map(map);\n \t\tcond_resched_rwlock_write(&fs_info->mapping_tree_lock);\ndiff --git a/fs/btrfs/volumes.h b/fs/btrfs/volumes.h\nindex 4117fabb248b..ccf0a459180d 100644\n--- a/fs/btrfs/volumes.h\n+++ b/fs/btrfs/volumes.h\n@@ -794,6 +794,8 @@ u64 btrfs_calc_stripe_length(const struct btrfs_chunk_map *map);\n int btrfs_nr_parity_stripes(u64 type);\n int btrfs_chunk_alloc_add_chunk_item(struct btrfs_trans_handle *trans,\n \t\t\t\t     struct btrfs_block_group *bg);\n+int btrfs_remove_dev_extents(struct btrfs_trans_handle *trans,\n+\t\t\t     struct btrfs_chunk_map *map);\n int btrfs_remove_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset);\n \n #ifdef CONFIG_BTRFS_FS_RUN_SANITY_TESTS\n@@ -905,6 +907,10 @@ bool btrfs_repair_one_zone(struct btrfs_fs_info *fs_info, u64 logical);\n \n bool btrfs_pinned_by_swapfile(struct btrfs_fs_info *fs_info, void *ptr);\n const u8 *btrfs_sb_fsid_ptr(const struct btrfs_super_block *sb);\n+int btrfs_update_device(struct btrfs_trans_handle *trans,\n+\t\t\tstruct btrfs_device *device);\n+void btrfs_chunk_map_device_clear_bits(struct btrfs_chunk_map *map,\n+\t\t\t\t       unsigned int bits);\n \n #ifdef CONFIG_BTRFS_FS_RUN_SANITY_TESTS\n struct btrfs_io_context *alloc_btrfs_io_context(struct btrfs_fs_info *fs_info,\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author addressed a concern about the potential for an unbounded chain of remaps, explaining that relocating one block group at a time and setting the REMAPPED flag on it prevents this issue. They also introduced new code to handle I/O from copy_remapped_data(), which involves searching the remap tree for any remap backrefs within the range of `remap_bytes`.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a potential issue",
                "provided an explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If when relocating a block group we find that `remap_bytes` > 0 in its\nblock group item, that means that it has been the destination block\ngroup for another that has been remapped.\n\nWe need to seach the remap tree for any remap backrefs within this\nrange, and move the data to a third block group. This is because\notherwise btrfs_translate_remap() could end up following an unbounded\nchain of remaps, which would only get worse over time.\n\nWe only relocate one block group at a time, so `remap_bytes` will only\never go down while we are doing this. Once we're finished we set the\nREMAPPED flag on the block group, which will permanently prevent any\nother data from being moved to within it.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/bio.c         |   3 +-\n fs/btrfs/bio.h         |   3 +\n fs/btrfs/extent-tree.c |   6 +-\n fs/btrfs/relocation.c  | 481 +++++++++++++++++++++++++++++++++++++++++\n 4 files changed, 490 insertions(+), 3 deletions(-)\n\ndiff --git a/fs/btrfs/bio.c b/fs/btrfs/bio.c\nindex a12446aa0fbf..1be042c0d521 100644\n--- a/fs/btrfs/bio.c\n+++ b/fs/btrfs/bio.c\n@@ -826,7 +826,8 @@ static bool btrfs_submit_chunk(struct btrfs_bio *bbio, int mirror_num)\n \t\t */\n \t\tif (!(inode->flags & BTRFS_INODE_NODATASUM) &&\n \t\t    !test_bit(BTRFS_FS_STATE_NO_DATA_CSUMS, &fs_info->fs_state) &&\n-\t\t    !btrfs_is_data_reloc_root(inode->root)) {\n+\t\t    !btrfs_is_data_reloc_root(inode->root) &&\n+\t\t    !bbio->is_remap) {\n \t\t\tif (should_async_write(bbio) &&\n \t\t\t    btrfs_wq_submit_bio(bbio, bioc, &smap, mirror_num))\n \t\t\t\tgoto done;\ndiff --git a/fs/btrfs/bio.h b/fs/btrfs/bio.h\nindex 157cdfa2f78a..303ed6c7103d 100644\n--- a/fs/btrfs/bio.h\n+++ b/fs/btrfs/bio.h\n@@ -90,6 +90,9 @@ struct btrfs_bio {\n \t */\n \tbool is_scrub:1;\n \n+\t/* Whether the bio is coming from copy_remapped_data_io(). */\n+\tbool is_remap:1;\n+\n \t/* Whether the csum generation for data write is async. */\n \tbool async_csum:1;\n \ndiff --git a/fs/btrfs/extent-tree.c b/fs/btrfs/extent-tree.c\nindex fef85ade017c..70020ba8ef92 100644\n--- a/fs/btrfs/extent-tree.c\n+++ b/fs/btrfs/extent-tree.c\n@@ -4557,7 +4557,8 @@ static noinline int find_free_extent(struct btrfs_root *root,\n \t\t    block_group->cached != BTRFS_CACHE_NO) {\n \t\t\tdown_read(&space_info->groups_sem);\n \t\t\tif (list_empty(&block_group->list) ||\n-\t\t\t    block_group->ro) {\n+\t\t\t    block_group->ro ||\n+\t\t\t    block_group->flags & BTRFS_BLOCK_GROUP_REMAPPED) {\n \t\t\t\t/*\n \t\t\t\t * someone is removing this block group,\n \t\t\t\t * we can't jump into the have_block_group\n@@ -4591,7 +4592,8 @@ static noinline int find_free_extent(struct btrfs_root *root,\n \n \t\tffe_ctl->hinted = false;\n \t\t/* If the block group is read-only, we can skip it entirely. */\n-\t\tif (unlikely(block_group->ro)) {\n+\t\tif (unlikely(block_group->ro ||\n+\t\t\t     block_group->flags & BTRFS_BLOCK_GROUP_REMAPPED)) {\n \t\t\tif (ffe_ctl->for_treelog)\n \t\t\t\tbtrfs_clear_treelog_bg(block_group);\n \t\t\tif (ffe_ctl->for_data_reloc)\ndiff --git a/fs/btrfs/relocation.c b/fs/btrfs/relocation.c\nindex 143eede52be0..82f0e15f0f84 100644\n--- a/fs/btrfs/relocation.c\n+++ b/fs/btrfs/relocation.c\n@@ -3976,6 +3976,481 @@ static void adjust_block_group_remap_bytes(struct btrfs_trans_handle *trans,\n \t\tbtrfs_inc_delayed_refs_rsv_bg_updates(fs_info);\n }\n \n+/* Private structure for I/O from copy_remapped_data().  */\n+struct reloc_io_private {\n+\tstruct completion done;\n+\trefcount_t pending_refs;\n+\tblk_status_t status;\n+};\n+\n+static void reloc_endio(struct btrfs_bio *bbio)\n+{\n+\tstruct reloc_io_private *priv = bbio->private;\n+\n+\tif (bbio->bio.bi_status)\n+\t\tWRITE_ONCE(priv->status, bbio->bio.bi_status);\n+\n+\tif (refcount_dec_and_test(&priv->pending_refs))\n+\t\tcomplete(&priv->done);\n+\n+\tbio_put(&bbio->bio);\n+}\n+\n+static int copy_remapped_data_io(struct btrfs_fs_info *fs_info,\n+\t\t\t\t struct reloc_io_private *priv,\n+\t\t\t\t struct page **pages, u64 addr, u64 length,\n+\t\t\t\t blk_opf_t op)\n+{\n+\tstruct btrfs_bio *bbio;\n+\tunsigned int i = 0;\n+\n+\tinit_completion(&priv->done);\n+\trefcount_set(&priv->pending_refs, 1);\n+\tpriv->status = 0;\n+\n+\tbbio = btrfs_bio_alloc(BIO_MAX_VECS, op, BTRFS_I(fs_info->btree_inode),\n+\t\t\t       addr, reloc_endio, priv);\n+\tbbio->bio.bi_iter.bi_sector = addr >> SECTOR_SHIFT;\n+\tbbio->is_remap = true;\n+\n+\tdo {\n+\t\tsize_t bytes = min_t(u64, length, PAGE_SIZE);\n+\n+\t\tif (bio_add_page(&bbio->bio, pages[i], bytes, 0) < bytes) {\n+\t\t\trefcount_inc(&priv->pending_refs);\n+\t\t\tbtrfs_submit_bbio(bbio, 0);\n+\n+\t\t\tbbio = btrfs_bio_alloc(BIO_MAX_VECS, op,\n+\t\t\t\t\t       BTRFS_I(fs_info->btree_inode),\n+\t\t\t\t\t       addr, reloc_endio, priv);\n+\t\t\tbbio->bio.bi_iter.bi_sector = addr >> SECTOR_SHIFT;\n+\t\t\tbbio->is_remap = true;\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\ti++;\n+\t\taddr += bytes;\n+\t\tlength -= bytes;\n+\t} while (length);\n+\n+\trefcount_inc(&priv->pending_refs);\n+\tbtrfs_submit_bbio(bbio, 0);\n+\n+\tif (!refcount_dec_and_test(&priv->pending_refs))\n+\t\twait_for_completion_io(&priv->done);\n+\n+\treturn blk_status_to_errno(READ_ONCE(priv->status));\n+}\n+\n+static int copy_remapped_data(struct btrfs_fs_info *fs_info, u64 old_addr,\n+\t\t\t      u64 new_addr, u64 length)\n+{\n+\tint ret;\n+\tu64 copy_len = min_t(u64, length, SZ_1M);\n+\tstruct page **pages;\n+\tstruct reloc_io_private priv;\n+\tunsigned int nr_pages = DIV_ROUND_UP(length, PAGE_SIZE);\n+\n+\tpages = kcalloc(nr_pages, sizeof(struct page *), GFP_NOFS);\n+\tif (!pages)\n+\t\treturn -ENOMEM;\n+\n+\tret = btrfs_alloc_page_array(nr_pages, pages, 0);\n+\tif (ret) {\n+\t\tret = -ENOMEM;\n+\t\tgoto end;\n+\t}\n+\n+\t/* Copy 1MB at a time, to avoid using too much memory. */\n+\n+\tdo {\n+\t\tu64 to_copy = min_t(u64, length, copy_len);\n+\n+\t\t/* Limit to one bio. */\n+\t\tto_copy = min_t(u64, to_copy, BIO_MAX_VECS << PAGE_SHIFT);\n+\n+\t\tret = copy_remapped_data_io(fs_info, &priv, pages, old_addr,\n+\t\t\t\t\t    to_copy, REQ_OP_READ);\n+\t\tif (ret)\n+\t\t\tgoto end;\n+\n+\t\tret = copy_remapped_data_io(fs_info, &priv, pages, new_addr,\n+\t\t\t\t\t    to_copy, REQ_OP_WRITE);\n+\t\tif (ret)\n+\t\t\tgoto end;\n+\n+\t\tif (to_copy == length)\n+\t\t\tbreak;\n+\n+\t\told_addr += to_copy;\n+\t\tnew_addr += to_copy;\n+\t\tlength -= to_copy;\n+\t} while (true);\n+\n+\tret = 0;\n+end:\n+\tfor (unsigned int i = 0; i < nr_pages; i++) {\n+\t\tif (pages[i])\n+\t\t\t__free_page(pages[i]);\n+\t}\n+\tkfree(pages);\n+\n+\treturn ret;\n+}\n+\n+static int add_remap_item(struct btrfs_trans_handle *trans,\n+\t\t\t  struct btrfs_path *path, u64 new_addr, u64 length,\n+\t\t\t  u64 old_addr)\n+{\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tstruct btrfs_remap_item remap = { 0 };\n+\tstruct btrfs_key key;\n+\tstruct extent_buffer *leaf;\n+\tint ret;\n+\n+\tkey.objectid = old_addr;\n+\tkey.type = BTRFS_REMAP_KEY;\n+\tkey.offset = length;\n+\n+\tret = btrfs_insert_empty_item(trans, fs_info->remap_root, path,\n+\t\t\t\t      &key, sizeof(struct btrfs_remap_item));\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tleaf = path->nodes[0];\n+\n+\tbtrfs_set_stack_remap_address(&remap, new_addr);\n+\n+\twrite_extent_buffer(leaf, &remap,\n+\t\t\t    btrfs_item_ptr_offset(leaf, path->slots[0]),\n+\t\t\t    sizeof(struct btrfs_remap_item));\n+\n+\tbtrfs_release_path(path);\n+\n+\treturn 0;\n+}\n+\n+static int add_remap_backref_item(struct btrfs_trans_handle *trans,\n+\t\t\t\t  struct btrfs_path *path, u64 new_addr,\n+\t\t\t\t  u64 length, u64 old_addr)\n+{\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tstruct btrfs_remap_item remap = { 0 };\n+\tstruct btrfs_key key;\n+\tstruct extent_buffer *leaf;\n+\tint ret;\n+\n+\tkey.objectid = new_addr;\n+\tkey.type = BTRFS_REMAP_BACKREF_KEY;\n+\tkey.offset = length;\n+\n+\tret = btrfs_insert_empty_item(trans, fs_info->remap_root,\n+\t\t\t\t      path, &key,\n+\t\t\t\t      sizeof(struct btrfs_remap_item));\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tleaf = path->nodes[0];\n+\n+\tbtrfs_set_stack_remap_address(&remap, old_addr);\n+\n+\twrite_extent_buffer(leaf, &remap,\n+\t\t\t    btrfs_item_ptr_offset(leaf, path->slots[0]),\n+\t\t\t    sizeof(struct btrfs_remap_item));\n+\n+\tbtrfs_release_path(path);\n+\n+\treturn 0;\n+}\n+\n+static int move_existing_remap(struct btrfs_fs_info *fs_info,\n+\t\t\t       struct btrfs_path *path,\n+\t\t\t       struct btrfs_block_group *bg, u64 new_addr,\n+\t\t\t       u64 length, u64 old_addr)\n+{\n+\tstruct btrfs_trans_handle *trans;\n+\tstruct extent_buffer *leaf;\n+\tstruct btrfs_remap_item *remap_ptr;\n+\tstruct btrfs_remap_item remap = { 0 };\n+\tstruct btrfs_key key, ins;\n+\tu64 dest_addr, dest_length, min_size;\n+\tstruct btrfs_block_group *dest_bg;\n+\tint ret;\n+\tconst bool is_data = bg->flags & BTRFS_BLOCK_GROUP_DATA;\n+\tstruct btrfs_space_info *sinfo = bg->space_info;\n+\tbool mutex_taken = false, bg_needs_free_space;\n+\n+\tspin_lock(&sinfo->lock);\n+\tbtrfs_space_info_update_bytes_may_use(sinfo, length);\n+\tspin_unlock(&sinfo->lock);\n+\n+\tif (is_data)\n+\t\tmin_size = fs_info->sectorsize;\n+\telse\n+\t\tmin_size = fs_info->nodesize;\n+\n+\tret = btrfs_reserve_extent(fs_info->fs_root, length, length, min_size,\n+\t\t\t\t   0, 0, &ins, is_data, false);\n+\tif (unlikely(ret)) {\n+\t\tspin_lock(&sinfo->lock);\n+\t\tbtrfs_space_info_update_bytes_may_use(sinfo, -length);\n+\t\tspin_unlock(&sinfo->lock);\n+\t\treturn ret;\n+\t}\n+\n+\tdest_addr = ins.objectid;\n+\tdest_length = ins.offset;\n+\n+\tif (!is_data && !IS_ALIGNED(dest_length, fs_info->nodesize)) {\n+\t\tu64 new_length = ALIGN_DOWN(dest_length, fs_info->nodesize);\n+\n+\t\tbtrfs_free_reserved_extent(fs_info, dest_addr + new_length,\n+\t\t\t\t\t   dest_length - new_length, 0);\n+\n+\t\tdest_length = new_length;\n+\t}\n+\n+\ttrans = btrfs_join_transaction(fs_info->remap_root);\n+\tif (IS_ERR(trans)) {\n+\t\tret = PTR_ERR(trans);\n+\t\ttrans = NULL;\n+\t\tgoto end;\n+\t}\n+\n+\tmutex_lock(&fs_info->remap_mutex);\n+\tmutex_taken = true;\n+\n+\t/* Find old remap entry. */\n+\n+\tkey.objectid = old_addr;\n+\tkey.type = BTRFS_REMAP_KEY;\n+\tkey.offset = length;\n+\n+\tret = btrfs_search_slot(trans, fs_info->remap_root, &key,\n+\t\t\t\tpath, 0, 1);\n+\tif (ret == 1) {\n+\t\t/*\n+\t\t * Not a problem if the remap entry wasn't found: that means\n+\t\t * that another transaction has deallocated the data.\n+\t\t * move_existing_remaps() loops until the BG contains no\n+\t\t * remaps, so we can just return 0 in this case.\n+\t\t */\n+\t\tbtrfs_release_path(path);\n+\t\tret = 0;\n+\t\tgoto end;\n+\t} else if (unlikely(ret)) {\n+\t\tgoto end;\n+\t}\n+\n+\tret = copy_remapped_data(fs_info, new_addr, dest_addr, dest_length);\n+\tif (unlikely(ret))\n+\t\tgoto end;\n+\n+\t/* Change data of old remap entry. */\n+\n+\tleaf = path->nodes[0];\n+\n+\tremap_ptr = btrfs_item_ptr(leaf, path->slots[0],\n+\t\t\t\t   struct btrfs_remap_item);\n+\tbtrfs_set_remap_address(leaf, remap_ptr, dest_addr);\n+\n+\tbtrfs_mark_buffer_dirty(trans, leaf);\n+\n+\tif (dest_length != length) {\n+\t\tkey.offset = dest_length;\n+\t\tbtrfs_set_item_key_safe(trans, path, &key);\n+\t}\n+\n+\tbtrfs_release_path(path);\n+\n+\tif (dest_length != length) {\n+\t\t/* Add remap item for remainder. */\n+\n+\t\tret = add_remap_item(trans, path, new_addr + dest_length,\n+\t\t\t\t     length - dest_length,\n+\t\t\t\t     old_addr + dest_length);\n+\t\tif (unlikely(ret))\n+\t\t\tgoto end;\n+\t}\n+\n+\t/* Change or remove old backref. */\n+\n+\tkey.objectid = new_addr;\n+\tkey.type = BTRFS_REMAP_BACKREF_KEY;\n+\tkey.offset = length;\n+\n+\tret = btrfs_search_slot(trans, fs_info->remap_root, &key,\n+\t\t\t\tpath, -1, 1);\n+\tif (unlikely(ret)) {\n+\t\tif (ret == 1) {\n+\t\t\tbtrfs_release_path(path);\n+\t\t\tret = -ENOENT;\n+\t\t}\n+\t\tgoto end;\n+\t}\n+\n+\tleaf = path->nodes[0];\n+\n+\tif (dest_length == length) {\n+\t\tret = btrfs_del_item(trans, fs_info->remap_root, path);\n+\t\tif (unlikely(ret)) {\n+\t\t\tbtrfs_release_path(path);\n+\t\t\tgoto end;\n+\t\t}\n+\t} else {\n+\t\tkey.objectid += dest_length;\n+\t\tkey.offset -= dest_length;\n+\t\tbtrfs_set_item_key_safe(trans, path, &key);\n+\n+\t\tbtrfs_set_stack_remap_address(&remap, old_addr + dest_length);\n+\n+\t\twrite_extent_buffer(leaf, &remap,\n+\t\t\t\t    btrfs_item_ptr_offset(leaf, path->slots[0]),\n+\t\t\t\t    sizeof(struct btrfs_remap_item));\n+\t}\n+\n+\tbtrfs_release_path(path);\n+\n+\t/* Add new backref. */\n+\n+\tret = add_remap_backref_item(trans, path, dest_addr, dest_length,\n+\t\t\t\t     old_addr);\n+\tif (unlikely(ret))\n+\t\tgoto end;\n+\n+\tadjust_block_group_remap_bytes(trans, bg, -dest_length);\n+\n+\tret = btrfs_add_to_free_space_tree(trans, new_addr, dest_length);\n+\tif (unlikely(ret))\n+\t\tgoto end;\n+\n+\tdest_bg = btrfs_lookup_block_group(fs_info, dest_addr);\n+\n+\tadjust_block_group_remap_bytes(trans, dest_bg, dest_length);\n+\n+\tmutex_lock(&dest_bg->free_space_lock);\n+\tbg_needs_free_space = test_bit(BLOCK_GROUP_FLAG_NEEDS_FREE_SPACE,\n+\t\t\t\t       &dest_bg->runtime_flags);\n+\tmutex_unlock(&dest_bg->free_space_lock);\n+\tbtrfs_put_block_group(dest_bg);\n+\n+\tif (bg_needs_free_space) {\n+\t\tret = btrfs_add_block_group_free_space(trans, dest_bg);\n+\t\tif (unlikely(ret))\n+\t\t\tgoto end;\n+\t}\n+\n+\tret = btrfs_remove_from_free_space_tree(trans, dest_addr, dest_length);\n+\tif (unlikely(ret)) {\n+\t\tbtrfs_remove_from_free_space_tree(trans, new_addr,\n+\t\t\t\t\t\t  dest_length);\n+\t\tgoto end;\n+\t}\n+\n+\tret = 0;\n+\n+end:\n+\tif (mutex_taken)\n+\t\tmutex_unlock(&fs_info->remap_mutex);\n+\n+\tbtrfs_dec_block_group_reservations(fs_info, dest_addr);\n+\n+\tif (unlikely(ret)) {\n+\t\tbtrfs_free_reserved_extent(fs_info, dest_addr, dest_length, 0);\n+\n+\t\tif (trans) {\n+\t\t\tbtrfs_abort_transaction(trans, ret);\n+\t\t\tbtrfs_end_transaction(trans);\n+\t\t}\n+\t} else {\n+\t\tdest_bg = btrfs_lookup_block_group(fs_info, dest_addr);\n+\t\tbtrfs_free_reserved_bytes(dest_bg, dest_length, 0);\n+\t\tbtrfs_put_block_group(dest_bg);\n+\n+\t\tret = btrfs_commit_transaction(trans);\n+\t}\n+\n+\treturn ret;\n+}\n+\n+static int move_existing_remaps(struct btrfs_fs_info *fs_info,\n+\t\t\t\tstruct btrfs_block_group *bg,\n+\t\t\t\tstruct btrfs_path *path)\n+{\n+\tint ret;\n+\tstruct btrfs_key key;\n+\tstruct extent_buffer *leaf;\n+\tstruct btrfs_remap_item *remap;\n+\tu64 old_addr;\n+\n+\t/* Look for backrefs in remap tree. */\n+\n+\twhile (bg->remap_bytes > 0) {\n+\t\tkey.objectid = bg->start;\n+\t\tkey.type = BTRFS_REMAP_BACKREF_KEY;\n+\t\tkey.offset = 0;\n+\n+\t\tret = btrfs_search_slot(NULL, fs_info->remap_root, &key, path,\n+\t\t\t\t\t0, 0);\n+\t\tif (ret < 0)\n+\t\t\treturn ret;\n+\n+\t\tleaf = path->nodes[0];\n+\n+\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n+\t\t\tret = btrfs_next_leaf(fs_info->remap_root, path);\n+\t\t\tif (ret < 0) {\n+\t\t\t\tbtrfs_release_path(path);\n+\t\t\t\treturn ret;\n+\t\t\t}\n+\n+\t\t\tif (ret) {\n+\t\t\t\tbtrfs_release_path(path);\n+\t\t\t\tbreak;\n+\t\t\t}\n+\n+\t\t\tleaf = path->nodes[0];\n+\t\t}\n+\n+\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n+\n+\t\tif (key.type != BTRFS_REMAP_BACKREF_KEY) {\n+\t\t\tpath->slots[0]++;\n+\n+\t\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n+\t\t\t\tret = btrfs_next_leaf(fs_info->remap_root, path);\n+\t\t\t\tif (ret < 0) {\n+\t\t\t\t\tbtrfs_release_path(path);\n+\t\t\t\t\treturn ret;\n+\t\t\t\t}\n+\n+\t\t\t\tif (ret) {\n+\t\t\t\t\tbtrfs_release_path(path);\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\n+\t\t\t\tleaf = path->nodes[0];\n+\t\t\t}\n+\t\t}\n+\n+\t\tremap = btrfs_item_ptr(leaf, path->slots[0],\n+\t\t\t\t       struct btrfs_remap_item);\n+\n+\t\told_addr = btrfs_remap_address(leaf, remap);\n+\n+\t\tbtrfs_release_path(path);\n+\n+\t\tret = move_existing_remap(fs_info, path, bg, key.objectid,\n+\t\t\t\t\t  key.offset, old_addr);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\t}\n+\n+\tASSERT(bg->remap_bytes == 0);\n+\n+\treturn 0;\n+}\n+\n static int create_remap_tree_entries(struct btrfs_trans_handle *trans,\n \t\t\t\t     struct btrfs_path *path,\n \t\t\t\t     struct btrfs_block_group *bg)\n@@ -4639,6 +5114,12 @@ int btrfs_relocate_block_group(struct btrfs_fs_info *fs_info, u64 group_start,\n \tWARN_ON(ret && ret != -EAGAIN);\n \n \tif (should_relocate_using_remap_tree(bg)) {\n+\t\tif (bg->remap_bytes != 0) {\n+\t\t\tret = move_existing_remaps(fs_info, bg, path);\n+\t\t\tif (ret)\n+\t\t\t\tgoto out;\n+\t\t}\n+\n \t\tret = start_block_group_remapping(fs_info, path, bg);\n \t} else {\n \t\tret = do_nonremap_reloc(fs_info, verbose, rc);\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author addressed a concern about the free-space tree being updated for remapped block groups, explaining that they are intentionally skipped in this patch and will be handled separately in a later patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Handle the preliminary work for relocating a block group in a filesystem\nwith the remap-tree flag set.\n\nIf the block group is SYSTEM btrfs_relocate_block_group() proceeds as it\ndoes already, as bootstrapping issues mean that these block groups have\nto be processed the existing way. Similarly with METADATA_REMAP blocks, which\nare dealt with in a later patch.\n\nOtherwise we walk the free-space tree for the block group in question,\nrecording any holes. These get converted into identity remaps and placed\nin the remap tree, and the block group's REMAPPED flag is set. From now\non no new allocations are possible within this block group, and any I/O\nto it will be funnelled through btrfs_translate_remap(). We store the\nnumber of identity remaps in `identity_remap_count`, so that we know\nwhen we've removed the last one and the block group is fully remapped.\n\nThe change in btrfs_read_roots() is because data relocations no longer\nrely on the data reloc tree as a hidden subvolume in which to do\nsnapshots.\n\n(Thanks to Sun YangKai for his suggestions.)\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/block-group.c     |   6 +-\n fs/btrfs/block-group.h     |   4 +\n fs/btrfs/free-space-tree.c |   4 +-\n fs/btrfs/free-space-tree.h |   5 +-\n fs/btrfs/relocation.c      | 516 +++++++++++++++++++++++++++++++++----\n fs/btrfs/relocation.h      |  11 +\n fs/btrfs/space-info.c      |   9 +-\n fs/btrfs/volumes.c         |  89 ++++---\n 8 files changed, 551 insertions(+), 93 deletions(-)\n\ndiff --git a/fs/btrfs/block-group.c b/fs/btrfs/block-group.c\nindex 0143b0290a72..2b3fd80a690f 100644\n--- a/fs/btrfs/block-group.c\n+++ b/fs/btrfs/block-group.c\n@@ -2405,6 +2405,7 @@ static int read_one_block_group(struct btrfs_fs_info *info,\n \tcache->used = btrfs_stack_block_group_v2_used(bgi);\n \tcache->last_used = cache->used;\n \tcache->flags = btrfs_stack_block_group_v2_flags(bgi);\n+\tcache->last_flags = cache->flags;\n \tcache->global_root_id = btrfs_stack_block_group_v2_chunk_objectid(bgi);\n \tcache->space_info = btrfs_find_space_info(info, cache->flags);\n \tcache->remap_bytes = btrfs_stack_block_group_v2_remap_bytes(bgi);\n@@ -2714,6 +2715,7 @@ static int insert_block_group_item(struct btrfs_trans_handle *trans,\n \tblock_group->last_remap_bytes = block_group->remap_bytes;\n \tblock_group->last_identity_remap_count =\n \t\tblock_group->identity_remap_count;\n+\tblock_group->last_flags = block_group->flags;\n \tkey.objectid = block_group->start;\n \tkey.type = BTRFS_BLOCK_GROUP_ITEM_KEY;\n \tkey.offset = block_group->length;\n@@ -3202,13 +3204,15 @@ static int update_block_group_item(struct btrfs_trans_handle *trans,\n \t/* No change in values, can safely skip it. */\n \tif (cache->last_used == used &&\n \t    cache->last_remap_bytes == remap_bytes &&\n-\t    cache->last_identity_remap_count == identity_remap_count) {\n+\t    cache->last_identity_remap_count == identity_remap_count &&\n+\t    cache->last_flags == cache->flags) {\n \t\tspin_unlock(&cache->lock);\n \t\treturn 0;\n \t}\n \tcache->last_used = used;\n \tcache->last_remap_bytes = remap_bytes;\n \tcache->last_identity_remap_count = identity_remap_count;\n+\tcache->last_flags = cache->flags;\n \tspin_unlock(&cache->lock);\n \n \tkey.objectid = cache->start;\ndiff --git a/fs/btrfs/block-group.h b/fs/btrfs/block-group.h\nindex 436d51a707a9..3e8c3d424481 100644\n--- a/fs/btrfs/block-group.h\n+++ b/fs/btrfs/block-group.h\n@@ -147,6 +147,10 @@ struct btrfs_block_group {\n \t * The last commited identity_remap_count value of this block group.\n \t */\n \tu32 last_identity_remap_count;\n+\t/*\n+\t * The last committed flags value for this block group.\n+\t */\n+\tu64 last_flags;\n \n \t/*\n \t * If the free space extent count exceeds this number, convert the block\ndiff --git a/fs/btrfs/free-space-tree.c b/fs/btrfs/free-space-tree.c\nindex ac092898130f..96d52c031977 100644\n--- a/fs/btrfs/free-space-tree.c\n+++ b/fs/btrfs/free-space-tree.c\n@@ -21,8 +21,7 @@ static int __add_block_group_free_space(struct btrfs_trans_handle *trans,\n \t\t\t\t\tstruct btrfs_block_group *block_group,\n \t\t\t\t\tstruct btrfs_path *path);\n \n-static struct btrfs_root *btrfs_free_space_root(\n-\t\t\t\tstruct btrfs_block_group *block_group)\n+struct btrfs_root *btrfs_free_space_root(struct btrfs_block_group *block_group)\n {\n \tstruct btrfs_key key = {\n \t\t.objectid = BTRFS_FREE_SPACE_TREE_OBJECTID,\n@@ -93,7 +92,6 @@ static int add_new_free_space_info(struct btrfs_trans_handle *trans,\n \treturn 0;\n }\n \n-EXPORT_FOR_TESTS\n struct btrfs_free_space_info *btrfs_search_free_space_info(\n \t\tstruct btrfs_trans_handle *trans,\n \t\tstruct btrfs_block_group *block_group,\ndiff --git a/fs/btrfs/free-space-tree.h b/fs/btrfs/free-space-tree.h\nindex ca04fc7cf29e..709730e36888 100644\n--- a/fs/btrfs/free-space-tree.h\n+++ b/fs/btrfs/free-space-tree.h\n@@ -36,12 +36,13 @@ int btrfs_add_to_free_space_tree(struct btrfs_trans_handle *trans,\n int btrfs_remove_from_free_space_tree(struct btrfs_trans_handle *trans,\n \t\t\t\t      u64 start, u64 size);\n int btrfs_delete_orphan_free_space_entries(struct btrfs_fs_info *fs_info);\n-\n-#ifdef CONFIG_BTRFS_FS_RUN_SANITY_TESTS\n struct btrfs_free_space_info *\n btrfs_search_free_space_info(struct btrfs_trans_handle *trans,\n \t\t\t     struct btrfs_block_group *block_group,\n \t\t\t     struct btrfs_path *path, int cow);\n+struct btrfs_root *btrfs_free_space_root(struct btrfs_block_group *block_group);\n+\n+#ifdef CONFIG_BTRFS_FS_RUN_SANITY_TESTS\n int __btrfs_add_to_free_space_tree(struct btrfs_trans_handle *trans,\n \t\t\t\t   struct btrfs_block_group *block_group,\n \t\t\t\t   struct btrfs_path *path, u64 start, u64 size);\ndiff --git a/fs/btrfs/relocation.c b/fs/btrfs/relocation.c\nindex e47234d5a156..143eede52be0 100644\n--- a/fs/btrfs/relocation.c\n+++ b/fs/btrfs/relocation.c\n@@ -3616,7 +3616,7 @@ static noinline_for_stack int relocate_block_group(struct reloc_control *rc)\n \t\tbtrfs_btree_balance_dirty(fs_info);\n \t}\n \n-\tif (!err) {\n+\tif (!err && !btrfs_fs_incompat(fs_info, REMAP_TREE)) {\n \t\tret = relocate_file_extent_cluster(rc);\n \t\tif (ret < 0)\n \t\t\terr = ret;\n@@ -3860,6 +3860,90 @@ static const char *stage_to_string(enum reloc_stage stage)\n \treturn \"unknown\";\n }\n \n+static int add_remap_tree_entries(struct btrfs_trans_handle *trans,\n+\t\t\t\t  struct btrfs_path *path,\n+\t\t\t\t  struct btrfs_key *entries,\n+\t\t\t\t  unsigned int num_entries)\n+{\n+\tint ret;\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tstruct btrfs_item_batch batch;\n+\tu32 *data_sizes;\n+\tu32 max_items;\n+\n+\tmax_items = BTRFS_LEAF_DATA_SIZE(trans->fs_info) / sizeof(struct btrfs_item);\n+\n+\tdata_sizes = kzalloc(sizeof(u32) * min_t(u32, num_entries, max_items),\n+\t\t\t     GFP_NOFS);\n+\tif (!data_sizes)\n+\t\treturn -ENOMEM;\n+\n+\twhile (true) {\n+\t\tbatch.keys = entries;\n+\t\tbatch.data_sizes = data_sizes;\n+\t\tbatch.total_data_size = 0;\n+\t\tbatch.nr = min_t(u32, num_entries, max_items);\n+\n+\t\tret = btrfs_insert_empty_items(trans, fs_info->remap_root, path,\n+\t\t\t\t\t       &batch);\n+\t\tbtrfs_release_path(path);\n+\n+\t\tif (num_entries <= max_items)\n+\t\t\tbreak;\n+\n+\t\tnum_entries -= max_items;\n+\t\tentries += max_items;\n+\t}\n+\n+\tkfree(data_sizes);\n+\n+\treturn ret;\n+}\n+\n+struct space_run {\n+\tu64 start;\n+\tu64 end;\n+};\n+\n+static void parse_bitmap(u64 block_size, const unsigned long *bitmap,\n+\t\t\t unsigned long size, u64 address,\n+\t\t\t struct space_run *space_runs,\n+\t\t\t unsigned int *num_space_runs)\n+{\n+\tunsigned long pos, end;\n+\tu64 run_start, run_length;\n+\n+\tpos = find_first_bit(bitmap, size);\n+\n+\tif (pos == size)\n+\t\treturn;\n+\n+\twhile (true) {\n+\t\tend = find_next_zero_bit(bitmap, size, pos);\n+\n+\t\trun_start = address + (pos * block_size);\n+\t\trun_length = (end - pos) * block_size;\n+\n+\t\tif (*num_space_runs != 0 &&\n+\t\t    space_runs[*num_space_runs - 1].end == run_start) {\n+\t\t\tspace_runs[*num_space_runs - 1].end += run_length;\n+\t\t} else {\n+\t\t\tspace_runs[*num_space_runs].start = run_start;\n+\t\t\tspace_runs[*num_space_runs].end = run_start + run_length;\n+\n+\t\t\t(*num_space_runs)++;\n+\t\t}\n+\n+\t\tif (end == size)\n+\t\t\tbreak;\n+\n+\t\tpos = find_next_bit(bitmap, size, end + 1);\n+\n+\t\tif (pos == size)\n+\t\t\tbreak;\n+\t}\n+}\n+\n static void adjust_block_group_remap_bytes(struct btrfs_trans_handle *trans,\n \t\t\t\t\t   struct btrfs_block_group *bg,\n \t\t\t\t\t   s64 diff)\n@@ -3892,6 +3976,188 @@ static void adjust_block_group_remap_bytes(struct btrfs_trans_handle *trans,\n \t\tbtrfs_inc_delayed_refs_rsv_bg_updates(fs_info);\n }\n \n+static int create_remap_tree_entries(struct btrfs_trans_handle *trans,\n+\t\t\t\t     struct btrfs_path *path,\n+\t\t\t\t     struct btrfs_block_group *bg)\n+{\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tstruct btrfs_free_space_info *fsi;\n+\tstruct btrfs_key key, found_key;\n+\tstruct extent_buffer *leaf;\n+\tstruct btrfs_root *space_root;\n+\tu32 extent_count;\n+\tstruct space_run *space_runs = NULL;\n+\tunsigned int num_space_runs = 0;\n+\tstruct btrfs_key *entries = NULL;\n+\tunsigned int max_entries, num_entries;\n+\tint ret;\n+\n+\tmutex_lock(&bg->free_space_lock);\n+\n+\tif (test_bit(BLOCK_GROUP_FLAG_NEEDS_FREE_SPACE, &bg->runtime_flags)) {\n+\t\tmutex_unlock(&bg->free_space_lock);\n+\n+\t\tret = btrfs_add_block_group_free_space(trans, bg);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\n+\t\tmutex_lock(&bg->free_space_lock);\n+\t}\n+\n+\tfsi = btrfs_search_free_space_info(trans, bg, path, 0);\n+\tif (IS_ERR(fsi)) {\n+\t\tmutex_unlock(&bg->free_space_lock);\n+\t\treturn PTR_ERR(fsi);\n+\t}\n+\n+\textent_count = btrfs_free_space_extent_count(path->nodes[0], fsi);\n+\n+\tbtrfs_release_path(path);\n+\n+\tspace_runs = kmalloc(sizeof(*space_runs) * extent_count, GFP_NOFS);\n+\tif (!space_runs) {\n+\t\tmutex_unlock(&bg->free_space_lock);\n+\t\treturn -ENOMEM;\n+\t}\n+\n+\tkey.objectid = bg->start;\n+\tkey.type = 0;\n+\tkey.offset = 0;\n+\n+\tspace_root = btrfs_free_space_root(bg);\n+\n+\tret = btrfs_search_slot(trans, space_root, &key, path, 0, 0);\n+\tif (ret < 0) {\n+\t\tmutex_unlock(&bg->free_space_lock);\n+\t\tgoto out;\n+\t}\n+\n+\tret = 0;\n+\n+\twhile (true) {\n+\t\tleaf = path->nodes[0];\n+\n+\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n+\n+\t\tif (found_key.objectid >= bg->start + bg->length)\n+\t\t\tbreak;\n+\n+\t\tif (found_key.type == BTRFS_FREE_SPACE_EXTENT_KEY) {\n+\t\t\tif (num_space_runs != 0 &&\n+\t\t\t    space_runs[num_space_runs - 1].end == found_key.objectid) {\n+\t\t\t\tspace_runs[num_space_runs - 1].end =\n+\t\t\t\t\tfound_key.objectid + found_key.offset;\n+\t\t\t} else {\n+\t\t\t\tASSERT(num_space_runs < extent_count);\n+\n+\t\t\t\tspace_runs[num_space_runs].start = found_key.objectid;\n+\t\t\t\tspace_runs[num_space_runs].end =\n+\t\t\t\t\tfound_key.objectid + found_key.offset;\n+\n+\t\t\t\tnum_space_runs++;\n+\t\t\t}\n+\t\t} else if (found_key.type == BTRFS_FREE_SPACE_BITMAP_KEY) {\n+\t\t\tvoid *bitmap;\n+\t\t\tunsigned long offset;\n+\t\t\tu32 data_size;\n+\n+\t\t\toffset = btrfs_item_ptr_offset(leaf, path->slots[0]);\n+\t\t\tdata_size = btrfs_item_size(leaf, path->slots[0]);\n+\n+\t\t\tif (data_size != 0) {\n+\t\t\t\tbitmap = kmalloc(data_size, GFP_NOFS);\n+\t\t\t\tif (!bitmap) {\n+\t\t\t\t\tmutex_unlock(&bg->free_space_lock);\n+\t\t\t\t\tret = -ENOMEM;\n+\t\t\t\t\tgoto out;\n+\t\t\t\t}\n+\n+\t\t\t\tread_extent_buffer(leaf, bitmap, offset,\n+\t\t\t\t\t\t   data_size);\n+\n+\t\t\t\tparse_bitmap(fs_info->sectorsize, bitmap,\n+\t\t\t\t\t     data_size * BITS_PER_BYTE,\n+\t\t\t\t\t     found_key.objectid, space_runs,\n+\t\t\t\t\t     &num_space_runs);\n+\n+\t\t\t\tASSERT(num_space_runs <= extent_count);\n+\n+\t\t\t\tkfree(bitmap);\n+\t\t\t}\n+\t\t}\n+\n+\t\tpath->slots[0]++;\n+\n+\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n+\t\t\tret = btrfs_next_leaf(space_root, path);\n+\t\t\tif (ret != 0) {\n+\t\t\t\tif (ret == 1)\n+\t\t\t\t\tret = 0;\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tleaf = path->nodes[0];\n+\t\t}\n+\t}\n+\n+\tbtrfs_release_path(path);\n+\n+\tmutex_unlock(&bg->free_space_lock);\n+\n+\tmax_entries = extent_count + 2;\n+\tentries = kmalloc(sizeof(*entries) * max_entries, GFP_NOFS);\n+\tif (!entries) {\n+\t\tret = -ENOMEM;\n+\t\tgoto out;\n+\t}\n+\n+\tnum_entries = 0;\n+\n+\tif (num_space_runs == 0) {\n+\t\tentries[num_entries].objectid = bg->start;\n+\t\tentries[num_entries].type = BTRFS_IDENTITY_REMAP_KEY;\n+\t\tentries[num_entries].offset = bg->length;\n+\t\tnum_entries++;\n+\t} else {\n+\t\tif (space_runs[0].start > bg->start) {\n+\t\t\tentries[num_entries].objectid = bg->start;\n+\t\t\tentries[num_entries].type = BTRFS_IDENTITY_REMAP_KEY;\n+\t\t\tentries[num_entries].offset =\n+\t\t\t\tspace_runs[0].start - bg->start;\n+\t\t\tnum_entries++;\n+\t\t}\n+\n+\t\tfor (unsigned int i = 1; i < num_space_runs; i++) {\n+\t\t\tentries[num_entries].objectid = space_runs[i - 1].end;\n+\t\t\tentries[num_entries].type = BTRFS_IDENTITY_REMAP_KEY;\n+\t\t\tentries[num_entries].offset =\n+\t\t\t\tspace_runs[i].start - space_runs[i - 1].end;\n+\t\t\tnum_entries++;\n+\t\t}\n+\n+\t\tif (space_runs[num_space_runs - 1].end < bg->start + bg->length) {\n+\t\t\tentries[num_entries].objectid =\n+\t\t\t\tspace_runs[num_space_runs - 1].end;\n+\t\t\tentries[num_entries].type = BTRFS_IDENTITY_REMAP_KEY;\n+\t\t\tentries[num_entries].offset =\n+\t\t\t\tbg->start + bg->length - space_runs[num_space_runs - 1].end;\n+\t\t\tnum_entries++;\n+\t\t}\n+\n+\t\tif (num_entries == 0)\n+\t\t\tgoto out;\n+\t}\n+\n+\tbg->identity_remap_count = num_entries;\n+\n+\tret = add_remap_tree_entries(trans, path, entries, num_entries);\n+\n+out:\n+\tkfree(entries);\n+\tkfree(space_runs);\n+\n+\treturn ret;\n+}\n+\n static int remove_chunk_stripes(struct btrfs_trans_handle *trans,\n \t\t\t\tstruct btrfs_chunk_map *chunk_map,\n \t\t\t\tstruct btrfs_path *path)\n@@ -4038,6 +4304,55 @@ static void adjust_identity_remap_count(struct btrfs_trans_handle *trans,\n \t\tbtrfs_mark_bg_fully_remapped(bg, trans);\n }\n \n+static int mark_chunk_remapped(struct btrfs_trans_handle *trans,\n+\t\t\t       struct btrfs_path *path, uint64_t start)\n+{\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tstruct btrfs_chunk_map *chunk_map;\n+\tstruct btrfs_key key;\n+\tu64 type;\n+\tint ret;\n+\tstruct extent_buffer *leaf;\n+\tstruct btrfs_chunk *chunk;\n+\n+\tread_lock(&fs_info->mapping_tree_lock);\n+\n+\tchunk_map = btrfs_find_chunk_map_nolock(fs_info, start, 1);\n+\tif (!chunk_map) {\n+\t\tread_unlock(&fs_info->mapping_tree_lock);\n+\t\treturn -ENOENT;\n+\t}\n+\n+\tchunk_map->type |= BTRFS_BLOCK_GROUP_REMAPPED;\n+\ttype = chunk_map->type;\n+\n+\tread_unlock(&fs_info->mapping_tree_lock);\n+\n+\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n+\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n+\tkey.offset = start;\n+\n+\tret = btrfs_search_slot(trans, fs_info->chunk_root, &key, path,\n+\t\t\t\t0, 1);\n+\tif (ret == 1) {\n+\t\tret = -ENOENT;\n+\t\tgoto end;\n+\t} else if (ret < 0)\n+\t\tgoto end;\n+\n+\tleaf = path->nodes[0];\n+\n+\tchunk = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_chunk);\n+\tbtrfs_set_chunk_type(leaf, chunk, type);\n+\tbtrfs_mark_buffer_dirty(trans, leaf);\n+\n+\tret = 0;\n+end:\n+\tbtrfs_free_chunk_map(chunk_map);\n+\tbtrfs_release_path(path);\n+\treturn ret;\n+}\n+\n int btrfs_translate_remap(struct btrfs_fs_info *fs_info, u64 *logical,\n \t\t\t  u64 *length)\n {\n@@ -4092,6 +4407,136 @@ int btrfs_translate_remap(struct btrfs_fs_info *fs_info, u64 *logical,\n \treturn 0;\n }\n \n+static int start_block_group_remapping(struct btrfs_fs_info *fs_info,\n+\t\t\t\t       struct btrfs_path *path,\n+\t\t\t\t       struct btrfs_block_group *bg)\n+{\n+\tstruct btrfs_trans_handle *trans;\n+\tbool bg_already_dirty = true;\n+\tint ret, ret2;\n+\n+\tret = btrfs_cache_block_group(bg, true);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\ttrans = btrfs_start_transaction(fs_info->remap_root, 0);\n+\tif (IS_ERR(trans))\n+\t\treturn PTR_ERR(trans);\n+\n+\t/* We need to run delayed refs, to make sure FST is up to date. */\n+\tret = btrfs_run_delayed_refs(trans, U64_MAX);\n+\tif (ret) {\n+\t\tbtrfs_end_transaction(trans);\n+\t\treturn ret;\n+\t}\n+\n+\tmutex_lock(&fs_info->remap_mutex);\n+\n+\tif (bg->flags & BTRFS_BLOCK_GROUP_REMAPPED) {\n+\t\tret = 0;\n+\t\tgoto end;\n+\t}\n+\n+\tret = create_remap_tree_entries(trans, path, bg);\n+\tif (unlikely(ret)) {\n+\t\tbtrfs_abort_transaction(trans, ret);\n+\t\tgoto end;\n+\t}\n+\n+\tspin_lock(&bg->lock);\n+\tbg->flags |= BTRFS_BLOCK_GROUP_REMAPPED;\n+\tspin_unlock(&bg->lock);\n+\n+\tspin_lock(&trans->transaction->dirty_bgs_lock);\n+\tif (list_empty(&bg->dirty_list)) {\n+\t\tlist_add_tail(&bg->dirty_list,\n+\t\t\t      &trans->transaction->dirty_bgs);\n+\t\tbg_already_dirty = false;\n+\t\tbtrfs_get_block_group(bg);\n+\t}\n+\tspin_unlock(&trans->transaction->dirty_bgs_lock);\n+\n+\t/* Modified block groups are accounted for in the delayed_refs_rsv. */\n+\tif (!bg_already_dirty)\n+\t\tbtrfs_inc_delayed_refs_rsv_bg_updates(fs_info);\n+\n+\tret = mark_chunk_remapped(trans, path, bg->start);\n+\tif (unlikely(ret)) {\n+\t\tbtrfs_abort_transaction(trans, ret);\n+\t\tgoto end;\n+\t}\n+\n+\tret = btrfs_remove_block_group_free_space(trans, bg);\n+\tif (unlikely(ret)) {\n+\t\tbtrfs_abort_transaction(trans, ret);\n+\t\tgoto end;\n+\t}\n+\n+\tbtrfs_remove_free_space_cache(bg);\n+\n+end:\n+\tmutex_unlock(&fs_info->remap_mutex);\n+\n+\tret2 = btrfs_end_transaction(trans);\n+\tif (!ret)\n+\t\tret = ret2;\n+\n+\treturn ret;\n+}\n+\n+static int do_nonremap_reloc(struct btrfs_fs_info *fs_info, bool verbose,\n+\t\t\t     struct reloc_control *rc)\n+{\n+\tint ret;\n+\n+\twhile (1) {\n+\t\tenum reloc_stage finishes_stage;\n+\n+\t\tmutex_lock(&fs_info->cleaner_mutex);\n+\t\tret = relocate_block_group(rc);\n+\t\tmutex_unlock(&fs_info->cleaner_mutex);\n+\n+\t\tfinishes_stage = rc->stage;\n+\t\t/*\n+\t\t * We may have gotten ENOSPC after we already dirtied some\n+\t\t * extents.  If writeout happens while we're relocating a\n+\t\t * different block group we could end up hitting the\n+\t\t * BUG_ON(rc->stage == UPDATE_DATA_PTRS) in\n+\t\t * btrfs_reloc_cow_block.  Make sure we write everything out\n+\t\t * properly so we don't trip over this problem, and then break\n+\t\t * out of the loop if we hit an error.\n+\t\t */\n+\t\tif (rc->stage == MOVE_DATA_EXTENTS && rc->found_file_extent) {\n+\t\t\tint wb_ret;\n+\n+\t\t\twb_ret = btrfs_wait_ordered_range(BTRFS_I(rc->data_inode),\n+\t\t\t\t\t\t\t\t0, (u64)-1);\n+\t\t\tif (wb_ret && ret == 0)\n+\t\t\t\tret = wb_ret;\n+\t\t\tinvalidate_mapping_pages(rc->data_inode->i_mapping,\n+\t\t\t\t\t\t\t0, -1);\n+\t\t\trc->stage = UPDATE_DATA_PTRS;\n+\t\t}\n+\n+\t\tif (ret < 0)\n+\t\t\treturn ret;\n+\n+\t\tif (rc->extents_found == 0)\n+\t\t\tbreak;\n+\n+\t\tif (verbose)\n+\t\t\tbtrfs_info(fs_info, \"found %llu extents, stage: %s\",\n+\t\t\t\t   rc->extents_found,\n+\t\t\t\t   stage_to_string(finishes_stage));\n+\t}\n+\n+\tWARN_ON(rc->block_group->pinned > 0);\n+\tWARN_ON(rc->block_group->reserved > 0);\n+\tWARN_ON(rc->block_group->used > 0);\n+\n+\treturn 0;\n+}\n+\n /*\n  * function to relocate all extents in a block group.\n  */\n@@ -4102,7 +4547,7 @@ int btrfs_relocate_block_group(struct btrfs_fs_info *fs_info, u64 group_start,\n \tstruct btrfs_root *extent_root = btrfs_extent_root(fs_info, group_start);\n \tstruct reloc_control *rc;\n \tstruct inode *inode;\n-\tstruct btrfs_path *path;\n+\tstruct btrfs_path *path = NULL;\n \tint ret;\n \tbool bg_is_ro = false;\n \n@@ -4164,7 +4609,7 @@ int btrfs_relocate_block_group(struct btrfs_fs_info *fs_info, u64 group_start,\n \t}\n \n \tinode = lookup_free_space_inode(rc->block_group, path);\n-\tbtrfs_free_path(path);\n+\tbtrfs_release_path(path);\n \n \tif (!IS_ERR(inode))\n \t\tret = delete_block_group_cache(rc->block_group, inode, 0);\n@@ -4174,11 +4619,13 @@ int btrfs_relocate_block_group(struct btrfs_fs_info *fs_info, u64 group_start,\n \tif (ret && ret != -ENOENT)\n \t\tgoto out;\n \n-\trc->data_inode = create_reloc_inode(rc->block_group);\n-\tif (IS_ERR(rc->data_inode)) {\n-\t\tret = PTR_ERR(rc->data_inode);\n-\t\trc->data_inode = NULL;\n-\t\tgoto out;\n+\tif (!btrfs_fs_incompat(fs_info, REMAP_TREE)) {\n+\t\trc->data_inode = create_reloc_inode(rc->block_group);\n+\t\tif (IS_ERR(rc->data_inode)) {\n+\t\t\tret = PTR_ERR(rc->data_inode);\n+\t\t\trc->data_inode = NULL;\n+\t\t\tgoto out;\n+\t\t}\n \t}\n \n \tif (verbose)\n@@ -4191,54 +4638,17 @@ int btrfs_relocate_block_group(struct btrfs_fs_info *fs_info, u64 group_start,\n \tret = btrfs_zone_finish(rc->block_group);\n \tWARN_ON(ret && ret != -EAGAIN);\n \n-\twhile (1) {\n-\t\tenum reloc_stage finishes_stage;\n-\n-\t\tmutex_lock(&fs_info->cleaner_mutex);\n-\t\tret = relocate_block_group(rc);\n-\t\tmutex_unlock(&fs_info->cleaner_mutex);\n-\n-\t\tfinishes_stage = rc->stage;\n-\t\t/*\n-\t\t * We may have gotten ENOSPC after we already dirtied some\n-\t\t * extents.  If writeout happens while we're relocating a\n-\t\t * different block group we could end up hitting the\n-\t\t * BUG_ON(rc->stage == UPDATE_DATA_PTRS) in\n-\t\t * btrfs_reloc_cow_block.  Make sure we write everything out\n-\t\t * properly so we don't trip over this problem, and then break\n-\t\t * out of the loop if we hit an error.\n-\t\t */\n-\t\tif (rc->stage == MOVE_DATA_EXTENTS && rc->found_file_extent) {\n-\t\t\tint wb_ret;\n-\n-\t\t\twb_ret = btrfs_wait_ordered_range(BTRFS_I(rc->data_inode), 0,\n-\t\t\t\t\t\t\t  (u64)-1);\n-\t\t\tif (wb_ret && ret == 0)\n-\t\t\t\tret = wb_ret;\n-\t\t\tinvalidate_mapping_pages(rc->data_inode->i_mapping,\n-\t\t\t\t\t\t 0, -1);\n-\t\t\trc->stage = UPDATE_DATA_PTRS;\n-\t\t}\n-\n-\t\tif (ret < 0)\n-\t\t\tgoto out;\n-\n-\t\tif (rc->extents_found == 0)\n-\t\t\tbreak;\n-\n-\t\tif (verbose)\n-\t\t\tbtrfs_info(fs_info, \"found %llu extents, stage: %s\",\n-\t\t\t\t   rc->extents_found,\n-\t\t\t\t   stage_to_string(finishes_stage));\n+\tif (should_relocate_using_remap_tree(bg)) {\n+\t\tret = start_block_group_remapping(fs_info, path, bg);\n+\t} else {\n+\t\tret = do_nonremap_reloc(fs_info, verbose, rc);\n \t}\n-\n-\tWARN_ON(rc->block_group->pinned > 0);\n-\tWARN_ON(rc->block_group->reserved > 0);\n-\tWARN_ON(rc->block_group->used > 0);\n out:\n \tif (ret && bg_is_ro)\n \t\tbtrfs_dec_block_group_ro(rc->block_group);\n-\tiput(rc->data_inode);\n+\tif (!btrfs_fs_incompat(fs_info, REMAP_TREE))\n+\t\tiput(rc->data_inode);\n+\tbtrfs_free_path(path);\n \treloc_chunk_end(fs_info);\n out_put_bg:\n \tbtrfs_put_block_group(bg);\n@@ -4432,7 +4842,7 @@ int btrfs_recover_relocation(struct btrfs_fs_info *fs_info)\n \n \tbtrfs_free_path(path);\n \n-\tif (ret == 0) {\n+\tif (ret == 0 && !btrfs_fs_incompat(fs_info, REMAP_TREE)) {\n \t\t/* cleanup orphan inode in data relocation tree */\n \t\tfs_root = btrfs_grab_root(fs_info->data_reloc_root);\n \t\tASSERT(fs_root);\ndiff --git a/fs/btrfs/relocation.h b/fs/btrfs/relocation.h\nindex 0f4874f815db..40d0a67f6f07 100644\n--- a/fs/btrfs/relocation.h\n+++ b/fs/btrfs/relocation.h\n@@ -12,6 +12,17 @@ struct btrfs_trans_handle;\n struct btrfs_ordered_extent;\n struct btrfs_pending_snapshot;\n \n+static inline bool should_relocate_using_remap_tree(struct btrfs_block_group *bg)\n+{\n+\tif (!btrfs_fs_incompat(bg->fs_info, REMAP_TREE))\n+\t\treturn false;\n+\n+\tif (bg->flags & (BTRFS_BLOCK_GROUP_SYSTEM | BTRFS_BLOCK_GROUP_METADATA_REMAP))\n+\t\treturn false;\n+\n+\treturn true;\n+}\n+\n int btrfs_relocate_block_group(struct btrfs_fs_info *fs_info, u64 group_start,\n \t\t\t       bool verbose);\n int btrfs_init_reloc_root(struct btrfs_trans_handle *trans, struct btrfs_root *root);\ndiff --git a/fs/btrfs/space-info.c b/fs/btrfs/space-info.c\nindex badebe6e0b34..45c4815b3854 100644\n--- a/fs/btrfs/space-info.c\n+++ b/fs/btrfs/space-info.c\n@@ -376,8 +376,13 @@ void btrfs_add_bg_to_space_info(struct btrfs_fs_info *info,\n \tfactor = btrfs_bg_type_to_factor(block_group->flags);\n \n \tspin_lock(&space_info->lock);\n-\tspace_info->total_bytes += block_group->length;\n-\tspace_info->disk_total += block_group->length * factor;\n+\n+\tif (!(block_group->flags & BTRFS_BLOCK_GROUP_REMAPPED) ||\n+\t    block_group->identity_remap_count != 0) {\n+\t\tspace_info->total_bytes += block_group->length;\n+\t\tspace_info->disk_total += block_group->length * factor;\n+\t}\n+\n \tspace_info->bytes_used += block_group->used;\n \tspace_info->disk_used += block_group->used * factor;\n \tspace_info->bytes_readonly += block_group->bytes_super;\ndiff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\nindex 46c5acc96725..caffee6527b2 100644\n--- a/fs/btrfs/volumes.c\n+++ b/fs/btrfs/volumes.c\n@@ -3405,15 +3405,55 @@ int btrfs_remove_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n \treturn ret;\n }\n \n-int btrfs_relocate_chunk(struct btrfs_fs_info *fs_info, u64 chunk_offset,\n-\t\t\t bool verbose)\n+static int btrfs_relocate_chunk_finish(struct btrfs_fs_info *fs_info,\n+\t\t\t\t       struct btrfs_block_group *bg)\n {\n \tstruct btrfs_root *root = fs_info->chunk_root;\n \tstruct btrfs_trans_handle *trans;\n-\tstruct btrfs_block_group *block_group;\n \tu64 length;\n \tint ret;\n \n+\tbtrfs_discard_cancel_work(&fs_info->discard_ctl, bg);\n+\tlength = bg->length;\n+\tbtrfs_put_block_group(bg);\n+\n+\t/*\n+\t * On a zoned file system, discard the whole block group, this will\n+\t * trigger a REQ_OP_ZONE_RESET operation on the device zone. If\n+\t * resetting the zone fails, don't treat it as a fatal problem from the\n+\t * filesystem's point of view.\n+\t */\n+\tif (btrfs_is_zoned(fs_info)) {\n+\t\tret = btrfs_discard_extent(fs_info, bg->start, length,\n+\t\t\t\t\t   NULL);\n+\t\tif (ret)\n+\t\t\tbtrfs_info(fs_info,\n+\t\t\t\t   \"failed to reset zone %llu after relocation\",\n+\t\t\t\t   bg->start);\n+\t}\n+\n+\ttrans = btrfs_start_trans_remove_block_group(root->fs_info, bg->start);\n+\tif (IS_ERR(trans)) {\n+\t\tret = PTR_ERR(trans);\n+\t\tbtrfs_handle_fs_error(root->fs_info, ret, NULL);\n+\t\treturn ret;\n+\t}\n+\n+\t/*\n+\t * Step two, delete the device extents and the chunk tree entries.\n+\t */\n+\tret = btrfs_remove_chunk(trans, bg->start);\n+\tbtrfs_end_transaction(trans);\n+\n+\treturn ret;\n+}\n+\n+int btrfs_relocate_chunk(struct btrfs_fs_info *fs_info, u64 chunk_offset,\n+\t\t\t bool verbose)\n+{\n+\tstruct btrfs_block_group *block_group;\n+\tint ret;\n+\n \tif (btrfs_fs_incompat(fs_info, EXTENT_TREE_V2)) {\n \t\tbtrfs_err(fs_info,\n \t\t\t  \"relocate: not supported on extent tree v2 yet\");\n@@ -3451,38 +3491,15 @@ int btrfs_relocate_chunk(struct btrfs_fs_info *fs_info, u64 chunk_offset,\n \tblock_group = btrfs_lookup_block_group(fs_info, chunk_offset);\n \tif (!block_group)\n \t\treturn -ENOENT;\n-\tbtrfs_discard_cancel_work(&fs_info->discard_ctl, block_group);\n-\tlength = block_group->length;\n-\tbtrfs_put_block_group(block_group);\n-\n-\t/*\n-\t * On a zoned file system, discard the whole block group, this will\n-\t * trigger a REQ_OP_ZONE_RESET operation on the device zone. If\n-\t * resetting the zone fails, don't treat it as a fatal problem from the\n-\t * filesystem's point of view.\n-\t */\n-\tif (btrfs_is_zoned(fs_info)) {\n-\t\tret = btrfs_discard_extent(fs_info, chunk_offset, length, NULL);\n-\t\tif (ret)\n-\t\t\tbtrfs_info(fs_info,\n-\t\t\t\t\"failed to reset zone %llu after relocation\",\n-\t\t\t\tchunk_offset);\n-\t}\n \n-\ttrans = btrfs_start_trans_remove_block_group(root->fs_info,\n-\t\t\t\t\t\t     chunk_offset);\n-\tif (IS_ERR(trans)) {\n-\t\tret = PTR_ERR(trans);\n-\t\tbtrfs_handle_fs_error(root->fs_info, ret, NULL);\n-\t\treturn ret;\n+\tif (should_relocate_using_remap_tree(block_group)) {\n+\t\t/* If we're relocating using the remap tree we're now done. */\n+\t\tbtrfs_put_block_group(block_group);\n+\t\tret = 0;\n+\t} else {\n+\t\tret = btrfs_relocate_chunk_finish(fs_info, block_group);\n \t}\n \n-\t/*\n-\t * step two, delete the device extents and the\n-\t * chunk tree entries\n-\t */\n-\tret = btrfs_remove_chunk(trans, chunk_offset);\n-\tbtrfs_end_transaction(trans);\n \treturn ret;\n }\n \n@@ -4155,6 +4172,14 @@ static int __btrfs_balance(struct btrfs_fs_info *fs_info)\n \t\tchunk = btrfs_item_ptr(leaf, slot, struct btrfs_chunk);\n \t\tchunk_type = btrfs_chunk_type(leaf, chunk);\n \n+\t\t/* Check if chunk has already been fully relocated. */\n+\t\tif (chunk_type & BTRFS_BLOCK_GROUP_REMAPPED &&\n+\t\t    btrfs_chunk_num_stripes(leaf, chunk) == 0) {\n+\t\t\tbtrfs_release_path(path);\n+\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n+\t\t\tgoto loop;\n+\t\t}\n+\n \t\tif (!counting) {\n \t\t\tspin_lock(&fs_info->balance_lock);\n \t\t\tbctl->stat.considered++;\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author addressed the concern that block groups which are fully remapped but still have pending stripe removals would be lost on unmount, by adding a function to populate a list of such block groups and add them to the discard queue when the first transaction is committed. The author confirmed this addresses the issue.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "confirmed issue resolved"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a function btrfs_populate_fully_remapped_bgs_list() which gets\ncalled on mount, which looks for fully remapped block groups\n(i.e. identity_remap_count == 0) which haven't yet had their chunk\nstripes and device extents removed.\n\nThis happens when a filesystem is unmounted while async discard has not\nyet finished, as otherwise the data range occupied by the chunk stripes\nwould be permanently unusable.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/block-group.c      | 79 +++++++++++++++++++++++++++++++++++++\n fs/btrfs/block-group.h      |  2 +\n fs/btrfs/disk-io.c          |  9 +++++\n fs/btrfs/free-space-cache.c | 18 +++++++++\n fs/btrfs/relocation.c       |  4 ++\n 5 files changed, 112 insertions(+)\n\ndiff --git a/fs/btrfs/block-group.c b/fs/btrfs/block-group.c\nindex 47454c22d6f4..1f5101f40b8c 100644\n--- a/fs/btrfs/block-group.c\n+++ b/fs/btrfs/block-group.c\n@@ -4815,6 +4815,11 @@ void btrfs_mark_bg_fully_remapped(struct btrfs_block_group *bg,\n \tstruct btrfs_fs_info *fs_info = trans->fs_info;\n \n \tif (btrfs_test_opt(fs_info, DISCARD_ASYNC)) {\n+\t\tspin_lock(&bg->lock);\n+\t\tset_bit(BLOCK_GROUP_FLAG_STRIPE_REMOVAL_PENDING,\n+\t\t\t&bg->runtime_flags);\n+\t\tspin_unlock(&bg->lock);\n+\n \t\tbtrfs_discard_queue_work(&fs_info->discard_ctl, bg);\n \t} else {\n \t\tspin_lock(&fs_info->unused_bgs_lock);\n@@ -4834,3 +4839,77 @@ void btrfs_mark_bg_fully_remapped(struct btrfs_block_group *bg,\n \t\tspin_unlock(&fs_info->unused_bgs_lock);\n \t}\n }\n+\n+/*\n+ * Compare the block group and chunk trees, and find any fully-remapped block\n+ * groups which haven't yet had their chunk stripes and device extents removed,\n+ * and put them on the fully_remapped_bgs list so this gets done.\n+ *\n+ * This happens when a block group becomes fully remapped, i.e. its last\n+ * identity mapping is removed, and the volume is unmounted before async\n+ * discard has finished. It's important this gets done as until it is the\n+ * chunk's stripes are dead space.\n+ */\n+int btrfs_populate_fully_remapped_bgs_list(struct btrfs_fs_info *fs_info)\n+{\n+\tstruct rb_node *node_bg, *node_chunk;\n+\n+\tnode_bg = rb_first_cached(&fs_info->block_group_cache_tree);\n+\tnode_chunk = rb_first_cached(&fs_info->mapping_tree);\n+\n+\twhile (node_bg && node_chunk) {\n+\t\tstruct btrfs_block_group *bg;\n+\t\tstruct btrfs_chunk_map *map;\n+\n+\t\tbg = rb_entry(node_bg, struct btrfs_block_group, cache_node);\n+\t\tmap = rb_entry(node_chunk, struct btrfs_chunk_map, rb_node);\n+\n+\t\tASSERT(bg->start == map->start);\n+\n+\t\tif (!(bg->flags & BTRFS_BLOCK_GROUP_REMAPPED))\n+\t\t\tgoto next;\n+\n+\t\tif (bg->identity_remap_count != 0)\n+\t\t\tgoto next;\n+\n+\t\tif (map->num_stripes == 0)\n+\t\t\tgoto next;\n+\n+\t\tspin_lock(&fs_info->unused_bgs_lock);\n+\n+\t\tif (list_empty(&bg->bg_list)) {\n+\t\t\tbtrfs_get_block_group(bg);\n+\t\t\tlist_add_tail(&bg->bg_list,\n+\t\t\t\t      &fs_info->fully_remapped_bgs);\n+\t\t} else {\n+\t\t\tlist_move_tail(&bg->bg_list,\n+\t\t\t\t       &fs_info->fully_remapped_bgs);\n+\t\t}\n+\n+\t\tspin_unlock(&fs_info->unused_bgs_lock);\n+\n+\t\t/*\n+\t\t * Ideally we'd want to call btrfs_discard_queue_work() here,\n+\t\t * but it'd do nothing as the discard worker hasn't been\n+\t\t * started yet.\n+\t\t *\n+\t\t * The block group will get added to the discard list when\n+\t\t * btrfs_handle_fully_remapped_bgs() gets called, when we\n+\t\t * commit the first transaction.\n+\t\t */\n+\t\tif (btrfs_test_opt(fs_info, DISCARD_ASYNC)) {\n+\t\t\tspin_lock(&bg->lock);\n+\t\t\tset_bit(BLOCK_GROUP_FLAG_STRIPE_REMOVAL_PENDING,\n+\t\t\t\t&bg->runtime_flags);\n+\t\t\tspin_unlock(&bg->lock);\n+\t\t}\n+\n+next:\n+\t\tnode_bg = rb_next(node_bg);\n+\t\tnode_chunk = rb_next(node_chunk);\n+\t}\n+\n+\tASSERT(!node_bg && !node_chunk);\n+\n+\treturn 0;\n+}\ndiff --git a/fs/btrfs/block-group.h b/fs/btrfs/block-group.h\nindex 3117cebf02f5..ccca6ee517a9 100644\n--- a/fs/btrfs/block-group.h\n+++ b/fs/btrfs/block-group.h\n@@ -94,6 +94,7 @@ enum btrfs_block_group_flags {\n \t */\n \tBLOCK_GROUP_FLAG_NEW,\n \tBLOCK_GROUP_FLAG_FULLY_REMAPPED,\n+\tBLOCK_GROUP_FLAG_STRIPE_REMOVAL_PENDING,\n };\n \n enum btrfs_caching_type {\n@@ -416,5 +417,6 @@ int btrfs_use_block_group_size_class(struct btrfs_block_group *bg,\n bool btrfs_block_group_should_use_size_class(const struct btrfs_block_group *bg);\n void btrfs_mark_bg_fully_remapped(struct btrfs_block_group *bg,\n \t\t\t\t  struct btrfs_trans_handle *trans);\n+int btrfs_populate_fully_remapped_bgs_list(struct btrfs_fs_info *fs_info);\n \n #endif /* BTRFS_BLOCK_GROUP_H */\ndiff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c\nindex ba500e3bf0d8..0491b799148f 100644\n--- a/fs/btrfs/disk-io.c\n+++ b/fs/btrfs/disk-io.c\n@@ -3613,6 +3613,15 @@ int __cold open_ctree(struct super_block *sb, struct btrfs_fs_devices *fs_device\n \t\tgoto fail_sysfs;\n \t}\n \n+\tif (btrfs_fs_incompat(fs_info, REMAP_TREE)) {\n+\t\tret = btrfs_populate_fully_remapped_bgs_list(fs_info);\n+\t\tif (ret) {\n+\t\t\tbtrfs_err(fs_info,\n+\t\t\t\"failed to populate fully_remapped_bgs list: %d\", ret);\n+\t\t\tgoto fail_sysfs;\n+\t\t}\n+\t}\n+\n \tbtrfs_zoned_reserve_data_reloc_bg(fs_info);\n \tbtrfs_free_zone_cache(fs_info);\n \ndiff --git a/fs/btrfs/free-space-cache.c b/fs/btrfs/free-space-cache.c\nindex e15fa8567f7c..7f7744a78de2 100644\n--- a/fs/btrfs/free-space-cache.c\n+++ b/fs/btrfs/free-space-cache.c\n@@ -3068,6 +3068,7 @@ bool btrfs_is_free_space_trimmed(struct btrfs_block_group *block_group)\n \tbool ret = true;\n \n \tif (block_group->flags & BTRFS_BLOCK_GROUP_REMAPPED &&\n+\t    !test_bit(BLOCK_GROUP_FLAG_STRIPE_REMOVAL_PENDING, &block_group->runtime_flags) &&\n \t    block_group->identity_remap_count == 0) {\n \t\treturn true;\n \t}\n@@ -3849,6 +3850,23 @@ void btrfs_trim_fully_remapped_block_group(struct btrfs_block_group *bg)\n \tconst u64 max_discard_size = READ_ONCE(discard_ctl->max_discard_size);\n \tu64 end = btrfs_block_group_end(bg);\n \n+\tif (!test_bit(BLOCK_GROUP_FLAG_STRIPE_REMOVAL_PENDING, &bg->runtime_flags)) {\n+\t\tbg->discard_cursor = end;\n+\n+\t\tif (bg->used == 0) {\n+\t\t\tspin_lock(&fs_info->unused_bgs_lock);\n+\t\t\tif (!list_empty(&bg->bg_list)) {\n+\t\t\t\tlist_del_init(&bg->bg_list);\n+\t\t\t\tbtrfs_put_block_group(bg);\n+\t\t\t}\n+\t\t\tspin_unlock(&fs_info->unused_bgs_lock);\n+\n+\t\t\tbtrfs_mark_bg_unused(bg);\n+\t\t}\n+\n+\t\treturn;\n+\t}\n+\n \tbytes = end - bg->discard_cursor;\n \n \tif (max_discard_size &&\ndiff --git a/fs/btrfs/relocation.c b/fs/btrfs/relocation.c\nindex 20cf0f7fd401..c3f1b7828179 100644\n--- a/fs/btrfs/relocation.c\n+++ b/fs/btrfs/relocation.c\n@@ -4785,6 +4785,10 @@ int btrfs_last_identity_remap_gone(struct btrfs_chunk_map *chunk_map,\n \n \tbtrfs_remove_bg_from_sinfo(bg);\n \n+\tspin_lock(&bg->lock);\n+\tclear_bit(BLOCK_GROUP_FLAG_STRIPE_REMOVAL_PENDING, &bg->runtime_flags);\n+\tspin_unlock(&bg->lock);\n+\n \tret = remove_chunk_stripes(trans, chunk_map, path);\n \tif (unlikely(ret)) {\n \t\tbtrfs_abort_transaction(trans, ret);\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author addressed a concern about balancing the METADATA_REMAP chunk, explaining that it's a special case and cannot be balanced like other chunks. They described a new approach to handle this situation by marking block groups as readonly and COWing every leaf of the remap tree.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Balancing the METADATA_REMAP chunk, i.e. the chunk in which the remap tree\nlives, is a special case.\n\nWe can't use the remap tree itself for this, as then we'd have no way to\nboostrap it on mount. And we can't use the pre-remap tree code for this\nas it relies on walking the extent tree, and we're not creating backrefs\nfor METADATA_REMAP chunks.\n\nSo instead, if a balance would relocate any METADATA_REMAP block groups, mark\nthose block groups as readonly and COW every leaf of the remap tree.\n\nThere's more sophisticated ways of doing this, such as only COWing nodes\nwithin a block group that's to be relocated, but they're fiddly and with\nlots of edge cases. Plus it's not anticipated that a) the number of\nMETADATA_REMAP chunks is going to be particularly large, or b) that users will\nwant to only relocate some of these chunks - the main use case here is\nto unbreak RAID conversion and device removal.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/volumes.c | 159 +++++++++++++++++++++++++++++++++++++++++++--\n 1 file changed, 155 insertions(+), 4 deletions(-)\n\ndiff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c\nindex b0aef4d489e7..96a3c0752f91 100644\n--- a/fs/btrfs/volumes.c\n+++ b/fs/btrfs/volumes.c\n@@ -4000,8 +4000,11 @@ static bool should_balance_chunk(struct extent_buffer *leaf, struct btrfs_chunk\n \tstruct btrfs_balance_args *bargs = NULL;\n \tu64 chunk_type = btrfs_chunk_type(leaf, chunk);\n \n-\tif (chunk_type & BTRFS_BLOCK_GROUP_METADATA_REMAP)\n-\t\treturn false;\n+\t/* Treat METADATA_REMAP chunks as METADATA. */\n+\tif (chunk_type & BTRFS_BLOCK_GROUP_METADATA_REMAP) {\n+\t\tchunk_type &= ~BTRFS_BLOCK_GROUP_METADATA_REMAP;\n+\t\tchunk_type |= BTRFS_BLOCK_GROUP_METADATA;\n+\t}\n \n \t/* type filter */\n \tif (!((chunk_type & BTRFS_BLOCK_GROUP_TYPE_MASK) &\n@@ -4084,6 +4087,113 @@ static bool should_balance_chunk(struct extent_buffer *leaf, struct btrfs_chunk\n \treturn true;\n }\n \n+struct remap_chunk_info {\n+\tstruct list_head list;\n+\tu64 offset;\n+\tstruct btrfs_block_group *bg;\n+\tbool made_ro;\n+};\n+\n+static int cow_remap_tree(struct btrfs_trans_handle *trans,\n+\t\t\t  struct btrfs_path *path)\n+{\n+\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n+\tstruct btrfs_key key = { 0 };\n+\tint ret;\n+\n+\tret = btrfs_search_slot(trans, fs_info->remap_root, &key, path, 0, 1);\n+\tif (ret < 0)\n+\t\treturn ret;\n+\n+\twhile (true) {\n+\t\tret = btrfs_next_leaf(fs_info->remap_root, path);\n+\t\tif (ret < 0) {\n+\t\t\treturn ret;\n+\t\t} else if (ret > 0) {\n+\t\t\tret = 0;\n+\t\t\tbreak;\n+\t\t}\n+\n+\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key, path->slots[0]);\n+\n+\t\tbtrfs_release_path(path);\n+\n+\t\tret = btrfs_search_slot(trans, fs_info->remap_root, &key, path,\n+\t\t\t\t\t0, 1);\n+\t\tif (ret < 0)\n+\t\t\tbreak;\n+\t}\n+\n+\treturn ret;\n+}\n+\n+static int balance_remap_chunks(struct btrfs_fs_info *fs_info,\n+\t\t\t\tstruct btrfs_path *path,\n+\t\t\t\tstruct list_head *chunks)\n+{\n+\tstruct remap_chunk_info *rci, *tmp;\n+\tstruct btrfs_trans_handle *trans;\n+\tint ret;\n+\n+\tlist_for_each_entry_safe(rci, tmp, chunks, list) {\n+\t\trci->bg = btrfs_lookup_block_group(fs_info, rci->offset);\n+\t\tif (!rci->bg) {\n+\t\t\tlist_del(&rci->list);\n+\t\t\tkfree(rci);\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tret = btrfs_inc_block_group_ro(rci->bg, false);\n+\t\tif (ret)\n+\t\t\tgoto end;\n+\n+\t\trci->made_ro = true;\n+\t}\n+\n+\tif (list_empty(chunks))\n+\t\treturn 0;\n+\n+\ttrans = btrfs_start_transaction(fs_info->remap_root, 0);\n+\tif (IS_ERR(trans)) {\n+\t\tret = PTR_ERR(trans);\n+\t\tgoto end;\n+\t}\n+\n+\tmutex_lock(&fs_info->remap_mutex);\n+\n+\tret = cow_remap_tree(trans, path);\n+\n+\tmutex_unlock(&fs_info->remap_mutex);\n+\n+\tbtrfs_release_path(path);\n+\n+\tbtrfs_commit_transaction(trans);\n+\n+end:\n+\twhile (!list_empty(chunks)) {\n+\t\tbool is_unused;\n+\n+\t\trci = list_first_entry(chunks, struct remap_chunk_info, list);\n+\n+\t\tspin_lock(&rci->bg->lock);\n+\t\tis_unused = !btrfs_is_block_group_used(rci->bg);\n+\t\tspin_unlock(&rci->bg->lock);\n+\n+\t\tif (is_unused)\n+\t\t\tbtrfs_mark_bg_unused(rci->bg);\n+\n+\t\tif (rci->made_ro)\n+\t\t\tbtrfs_dec_block_group_ro(rci->bg);\n+\n+\t\tbtrfs_put_block_group(rci->bg);\n+\n+\t\tlist_del(&rci->list);\n+\t\tkfree(rci);\n+\t}\n+\n+\treturn ret;\n+}\n+\n static int __btrfs_balance(struct btrfs_fs_info *fs_info)\n {\n \tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n@@ -4106,6 +4216,9 @@ static int __btrfs_balance(struct btrfs_fs_info *fs_info)\n \tu32 count_meta = 0;\n \tu32 count_sys = 0;\n \tint chunk_reserved = 0;\n+\tstruct remap_chunk_info *rci;\n+\tunsigned int num_remap_chunks = 0;\n+\tLIST_HEAD(remap_chunks);\n \n \tpath = btrfs_alloc_path();\n \tif (!path) {\n@@ -4204,7 +4317,8 @@ static int __btrfs_balance(struct btrfs_fs_info *fs_info)\n \t\t\t\tcount_data++;\n \t\t\telse if (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM)\n \t\t\t\tcount_sys++;\n-\t\t\telse if (chunk_type & BTRFS_BLOCK_GROUP_METADATA)\n+\t\t\telse if (chunk_type & (BTRFS_BLOCK_GROUP_METADATA |\n+\t\t\t\t\t       BTRFS_BLOCK_GROUP_METADATA_REMAP))\n \t\t\t\tcount_meta++;\n \n \t\t\tgoto loop;\n@@ -4224,6 +4338,30 @@ static int __btrfs_balance(struct btrfs_fs_info *fs_info)\n \t\t\tgoto loop;\n \t\t}\n \n+\t\t/*\n+\t\t * Balancing METADATA_REMAP chunks takes place separately - add\n+\t\t * the details to a list so it can be processed later.\n+\t\t */\n+\t\tif (chunk_type & BTRFS_BLOCK_GROUP_METADATA_REMAP) {\n+\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n+\n+\t\t\trci = kmalloc(sizeof(struct remap_chunk_info),\n+\t\t\t\t      GFP_NOFS);\n+\t\t\tif (!rci) {\n+\t\t\t\tret = -ENOMEM;\n+\t\t\t\tgoto error;\n+\t\t\t}\n+\n+\t\t\trci->offset = found_key.offset;\n+\t\t\trci->bg = NULL;\n+\t\t\trci->made_ro = false;\n+\t\t\tlist_add_tail(&rci->list, &remap_chunks);\n+\n+\t\t\tnum_remap_chunks++;\n+\n+\t\t\tgoto loop;\n+\t\t}\n+\n \t\tif (!chunk_reserved) {\n \t\t\t/*\n \t\t\t * We may be relocating the only data chunk we have,\n@@ -4263,11 +4401,24 @@ static int __btrfs_balance(struct btrfs_fs_info *fs_info)\n \t\tkey.offset = found_key.offset - 1;\n \t}\n \n+\tbtrfs_release_path(path);\n+\n \tif (counting) {\n-\t\tbtrfs_release_path(path);\n \t\tcounting = false;\n \t\tgoto again;\n \t}\n+\n+\tif (!list_empty(&remap_chunks)) {\n+\t\tret = balance_remap_chunks(fs_info, path, &remap_chunks);\n+\t\tif (ret == -ENOSPC)\n+\t\t\tenospc_errors++;\n+\n+\t\tif (!ret) {\n+\t\t\tspin_lock(&fs_info->balance_lock);\n+\t\t\tbctl->stat.completed += num_remap_chunks;\n+\t\t\tspin_unlock(&fs_info->balance_lock);\n+\t\t}\n+\t}\n error:\n \tif (enospc_errors) {\n \t\tbtrfs_info(fs_info, \"%d enospc errors during balance\",\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "The author addressed a concern about discard operations on fully-remapped block groups, explaining that normal discard iteration won't work and proposing two solutions for sync and async discard: calling btrfs_discard_extent() after the last identity remap is removed in sync mode, and adding a new function btrfs_trim_fully_remapped_block_group() to handle async discard. The author agreed to restructure the code in v2.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Discard normally works by iterating over the free-space entries of a\nblock group. This doesn't work for fully-remapped block groups, as we\nremoved their free-space entries when we started relocation.\n\nFor sync discard, call btrfs_discard_extent() when we commit the\ntransaction in which the last identity remap was removed.\n\nFor async discard, add a new function btrfs_trim_fully_remapped_block_group()\nto be called by the discard worker, which iterates over the block\ngroup's range using the normal async discard rules. Once we reach the\nend, remove the chunk's stripes and device extents to get back its free\nspace.\n\nSigned-off-by: Mark Harmstone <mark@harmstone.com>\nReviewed-by: Boris Burkov <boris@bur.io>\n---\n fs/btrfs/block-group.c      | 29 ++++++++++---------\n fs/btrfs/block-group.h      |  1 +\n fs/btrfs/discard.c          | 57 ++++++++++++++++++++++++++++++++-----\n fs/btrfs/extent-tree.c      |  3 ++\n fs/btrfs/free-space-cache.c | 36 +++++++++++++++++++++++\n fs/btrfs/free-space-cache.h |  1 +\n 6 files changed, 107 insertions(+), 20 deletions(-)\n\ndiff --git a/fs/btrfs/block-group.c b/fs/btrfs/block-group.c\nindex 2b3fd80a690f..47454c22d6f4 100644\n--- a/fs/btrfs/block-group.c\n+++ b/fs/btrfs/block-group.c\n@@ -4814,20 +4814,23 @@ void btrfs_mark_bg_fully_remapped(struct btrfs_block_group *bg,\n {\n \tstruct btrfs_fs_info *fs_info = trans->fs_info;\n \n-\tspin_lock(&fs_info->unused_bgs_lock);\n+\tif (btrfs_test_opt(fs_info, DISCARD_ASYNC)) {\n+\t\tbtrfs_discard_queue_work(&fs_info->discard_ctl, bg);\n+\t} else {\n+\t\tspin_lock(&fs_info->unused_bgs_lock);\n \n-\t/*\n-\t * The block group might already be on the unused_bgs list, remove it\n-\t * if it is. It'll get readded after the async discard worker finishes,\n-\t * or in btrfs_handle_fully_remapped_bgs() if we're not using async\n-\t * discard.\n-\t */\n-\tif (!list_empty(&bg->bg_list))\n-\t\tlist_del(&bg->bg_list);\n-\telse\n-\t\tbtrfs_get_block_group(bg);\n+\t\t/*\n+\t\t * The block group might already be on the unused_bgs list,\n+\t\t * remove it if it is. It'll get readded after\n+\t\t * btrfs_handle_fully_remapped_bgs() finishes.\n+\t\t */\n+\t\tif (!list_empty(&bg->bg_list))\n+\t\t\tlist_del(&bg->bg_list);\n+\t\telse\n+\t\t\tbtrfs_get_block_group(bg);\n \n-\tlist_add_tail(&bg->bg_list, &fs_info->fully_remapped_bgs);\n+\t\tlist_add_tail(&bg->bg_list, &fs_info->fully_remapped_bgs);\n \n-\tspin_unlock(&fs_info->unused_bgs_lock);\n+\t\tspin_unlock(&fs_info->unused_bgs_lock);\n+\t}\n }\ndiff --git a/fs/btrfs/block-group.h b/fs/btrfs/block-group.h\nindex 3e8c3d424481..3117cebf02f5 100644\n--- a/fs/btrfs/block-group.h\n+++ b/fs/btrfs/block-group.h\n@@ -49,6 +49,7 @@ enum btrfs_discard_state {\n \tBTRFS_DISCARD_EXTENTS,\n \tBTRFS_DISCARD_BITMAPS,\n \tBTRFS_DISCARD_RESET_CURSOR,\n+\tBTRFS_DISCARD_FULLY_REMAPPED,\n };\n \n /*\ndiff --git a/fs/btrfs/discard.c b/fs/btrfs/discard.c\nindex ee5f5b2788e1..a3d7b7752518 100644\n--- a/fs/btrfs/discard.c\n+++ b/fs/btrfs/discard.c\n@@ -215,6 +215,27 @@ static struct btrfs_block_group *find_next_block_group(\n \treturn ret_block_group;\n }\n \n+/*\n+ * Returns whether a block group is empty.\n+ *\n+ * @bg: block group of interest\n+ *\n+ * \"Empty\" here means that there are no extents physically located within the\n+ * device extents corresponding to this block group.\n+ *\n+ * For a remapped block group, this means that all of its identity remaps have\n+ * been removed. For a non-remapped block group, this means that no extents\n+ * have an address within its range, and that nothing has been remapped to be\n+ * within it.\n+ */\n+static bool block_group_is_empty(struct btrfs_block_group *bg)\n+{\n+\tif (bg->flags & BTRFS_BLOCK_GROUP_REMAPPED)\n+\t\treturn bg->identity_remap_count == 0;\n+\telse\n+\t\treturn bg->used == 0 && bg->remap_bytes == 0;\n+}\n+\n /*\n  * Look up next block group and set it for use.\n  *\n@@ -241,8 +262,10 @@ static struct btrfs_block_group *peek_discard_list(\n \tblock_group = find_next_block_group(discard_ctl, now);\n \n \tif (block_group && now >= block_group->discard_eligible_time) {\n+\t\tbool empty = block_group_is_empty(block_group);\n+\n \t\tif (block_group->discard_index == BTRFS_DISCARD_INDEX_UNUSED &&\n-\t\t    block_group->used != 0) {\n+\t\t    !empty) {\n \t\t\tif (btrfs_is_block_group_data_only(block_group)) {\n \t\t\t\t__add_to_discard_list(discard_ctl, block_group);\n \t\t\t\t/*\n@@ -267,7 +290,15 @@ static struct btrfs_block_group *peek_discard_list(\n \t\t}\n \t\tif (block_group->discard_state == BTRFS_DISCARD_RESET_CURSOR) {\n \t\t\tblock_group->discard_cursor = block_group->start;\n-\t\t\tblock_group->discard_state = BTRFS_DISCARD_EXTENTS;\n+\n+\t\t\tif (block_group->flags & BTRFS_BLOCK_GROUP_REMAPPED &&\n+\t\t\t    empty) {\n+\t\t\t\tblock_group->discard_state =\n+\t\t\t\t\tBTRFS_DISCARD_FULLY_REMAPPED;\n+\t\t\t} else {\n+\t\t\t\tblock_group->discard_state =\n+\t\t\t\t\tBTRFS_DISCARD_EXTENTS;\n+\t\t\t}\n \t\t}\n \t}\n \tif (block_group) {\n@@ -373,7 +404,7 @@ void btrfs_discard_queue_work(struct btrfs_discard_ctl *discard_ctl,\n \tif (!block_group || !btrfs_test_opt(block_group->fs_info, DISCARD_ASYNC))\n \t\treturn;\n \n-\tif (block_group->used == 0 && block_group->remap_bytes == 0)\n+\tif (block_group_is_empty(block_group))\n \t\tadd_to_discard_unused_list(discard_ctl, block_group);\n \telse\n \t\tadd_to_discard_list(discard_ctl, block_group);\n@@ -470,7 +501,7 @@ static void btrfs_finish_discard_pass(struct btrfs_discard_ctl *discard_ctl,\n {\n \tremove_from_discard_list(discard_ctl, block_group);\n \n-\tif (block_group->used == 0) {\n+\tif (block_group_is_empty(block_group)) {\n \t\tif (btrfs_is_free_space_trimmed(block_group))\n \t\t\tbtrfs_mark_bg_unused(block_group);\n \t\telse\n@@ -524,7 +555,8 @@ static void btrfs_discard_workfn(struct work_struct *work)\n \t/* Perform discarding */\n \tminlen = discard_minlen[discard_index];\n \n-\tif (discard_state == BTRFS_DISCARD_BITMAPS) {\n+\tswitch (discard_state) {\n+\tcase BTRFS_DISCARD_BITMAPS: {\n \t\tu64 maxlen = 0;\n \n \t\t/*\n@@ -541,17 +573,28 @@ static void btrfs_discard_workfn(struct work_struct *work)\n \t\t\t\t       btrfs_block_group_end(block_group),\n \t\t\t\t       minlen, maxlen, true);\n \t\tdiscard_ctl->discard_bitmap_bytes += trimmed;\n-\t} else {\n+\n+\t\tbreak;\n+\t}\n+\n+\tcase BTRFS_DISCARD_FULLY_REMAPPED:\n+\t\tbtrfs_trim_fully_remapped_block_group(block_group);\n+\t\tbreak;\n+\n+\tdefault:\n \t\tbtrfs_trim_block_group_extents(block_group, &trimmed,\n \t\t\t\t       block_group->discard_cursor,\n \t\t\t\t       btrfs_block_group_end(block_group),\n \t\t\t\t       minlen, true);\n \t\tdiscard_ctl->discard_extent_bytes += trimmed;\n+\n+\t\tbreak;\n \t}\n \n \t/* Determine next steps for a block_group */\n \tif (block_group->discard_cursor >= btrfs_block_group_end(block_group)) {\n-\t\tif (discard_state == BTRFS_DISCARD_BITMAPS) {\n+\t\tif (discard_state == BTRFS_DISCARD_BITMAPS ||\n+\t\t    discard_state == BTRFS_DISCARD_FULLY_REMAPPED) {\n \t\t\tbtrfs_finish_discard_pass(discard_ctl, block_group);\n \t\t} else {\n \t\t\tblock_group->discard_cursor = block_group->start;\ndiff --git a/fs/btrfs/extent-tree.c b/fs/btrfs/extent-tree.c\nindex 9d68f3fa4fa9..7d010d480f7c 100644\n--- a/fs/btrfs/extent-tree.c\n+++ b/fs/btrfs/extent-tree.c\n@@ -2901,6 +2901,9 @@ void btrfs_handle_fully_remapped_bgs(struct btrfs_fs_info *fs_info)\n \t\tlist_del_init(&bg->bg_list);\n \t\tspin_unlock(&fs_info->unused_bgs_lock);\n \n+\t\tbtrfs_discard_extent(fs_info, bg->start, bg->length,\n+\t\t\t\t     NULL, false);\n+\n \t\tret = btrfs_complete_bg_remapping(bg);\n \t\tif (ret) {\n \t\t\tbtrfs_put_block_group(bg);\ndiff --git a/fs/btrfs/free-space-cache.c b/fs/btrfs/free-space-cache.c\nindex 17e79ee3e021..e15fa8567f7c 100644\n--- a/fs/btrfs/free-space-cache.c\n+++ b/fs/btrfs/free-space-cache.c\n@@ -29,6 +29,7 @@\n #include \"file-item.h\"\n #include \"file.h\"\n #include \"super.h\"\n+#include \"relocation.h\"\n \n #define BITS_PER_BITMAP\t\t(PAGE_SIZE * 8UL)\n #define MAX_CACHE_BYTES_PER_GIG\tSZ_64K\n@@ -3066,6 +3067,11 @@ bool btrfs_is_free_space_trimmed(struct btrfs_block_group *block_group)\n \tstruct rb_node *node;\n \tbool ret = true;\n \n+\tif (block_group->flags & BTRFS_BLOCK_GROUP_REMAPPED &&\n+\t    block_group->identity_remap_count == 0) {\n+\t\treturn true;\n+\t}\n+\n \tspin_lock(&ctl->tree_lock);\n \tnode = rb_first(&ctl->free_space_offset);\n \n@@ -3834,6 +3840,36 @@ static int trim_no_bitmap(struct btrfs_block_group *block_group,\n \treturn ret;\n }\n \n+void btrfs_trim_fully_remapped_block_group(struct btrfs_block_group *bg)\n+{\n+\tstruct btrfs_fs_info *fs_info = bg->fs_info;\n+\tstruct btrfs_discard_ctl *discard_ctl = &fs_info->discard_ctl;\n+\tint ret = 0;\n+\tu64 bytes, trimmed;\n+\tconst u64 max_discard_size = READ_ONCE(discard_ctl->max_discard_size);\n+\tu64 end = btrfs_block_group_end(bg);\n+\n+\tbytes = end - bg->discard_cursor;\n+\n+\tif (max_discard_size &&\n+\t\tbytes >= (max_discard_size +\n+\t\t\tBTRFS_ASYNC_DISCARD_MIN_FILTER)) {\n+\t\tbytes = max_discard_size;\n+\t}\n+\n+\tret = btrfs_discard_extent(fs_info, bg->discard_cursor, bytes, &trimmed,\n+\t\t\t\t   false);\n+\tif (ret)\n+\t\treturn;\n+\n+\tbg->discard_cursor += trimmed;\n+\n+\tif (bg->discard_cursor < end)\n+\t\treturn;\n+\n+\tbtrfs_complete_bg_remapping(bg);\n+}\n+\n /*\n  * If we break out of trimming a bitmap prematurely, we should reset the\n  * trimming bit.  In a rather contrived case, it's possible to race here so\ndiff --git a/fs/btrfs/free-space-cache.h b/fs/btrfs/free-space-cache.h\nindex 9f1dbfdee8ca..33fc3b245648 100644\n--- a/fs/btrfs/free-space-cache.h\n+++ b/fs/btrfs/free-space-cache.h\n@@ -166,6 +166,7 @@ int btrfs_trim_block_group_extents(struct btrfs_block_group *block_group,\n int btrfs_trim_block_group_bitmaps(struct btrfs_block_group *block_group,\n \t\t\t\t   u64 *trimmed, u64 start, u64 end, u64 minlen,\n \t\t\t\t   u64 maxlen, bool async);\n+void btrfs_trim_fully_remapped_block_group(struct btrfs_block_group *bg);\n \n bool btrfs_free_space_cache_v1_active(struct btrfs_fs_info *fs_info);\n int btrfs_set_free_space_cache_v1_active(struct btrfs_fs_info *fs_info, bool active);\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-01-07",
              "analysis_source": "llm"
            },
            {
              "author": "David Sterba",
              "summary": "Reviewer David Sterba pointed out that many coding style issues were present in the patch, which he addressed by making changes to fix these issues, noting that since this is a large amount of new code, maintaining consistency in coding style until it's updated again is crucial.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "reviewer focused on minor issues rather than technical correctness"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Patches have been added to for-next. There were many coding style issues\nwhich I've tried to fix. As this is a lot of new code it'll get updated\nanyway, I realized that for this kind of initial batch the coding\nstyle is quite important as we'd have to stick with until some random\nchange touches it. Please have a look for the differences. Thanks.",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-21",
              "analysis_source": "llm"
            },
            {
              "author": "Filipe Manana",
              "summary": "Reviewer Filipe Manana expressed concern that the patch lacks test cases, specifically mentioning that he did not see any test cases submitted for the new feature.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "lack of test cases",
                "critical code"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This is a huge amount of code and quite critical.\nShouldn't we have test cases in fstests to exercise this feature?\nI didn't see any test cases submitted.",
              "reply_to": "David Sterba",
              "message_date": "2026-01-23",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "Author acknowledged that setting the incompat flag is a no-op if the incompat flag isn't set, and confirmed that fstests will be added before removing it from EXPERIMENTAL.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "confirmed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It is, but it's a no-op if the incompat flag set isn't set.\n\nThere will be fstests for this before I propose taking it out of \nEXPERIMENTAL.",
              "reply_to": "Filipe Manana",
              "message_date": "2026-01-23",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Mason",
              "summary": "Reviewer Chris Mason noted that the patch's AI review prompts flagged a potential issue, which he deemed correct and didn't elaborate further.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no specific technical concern raised"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi everyone,\n\nI ran my AI patch review prompts on linux-next, and this one was flagged.   As\nwe add more btrfs specifics we'll probably find some other fun, but this one\nseems right to me:",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-25",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Mason",
              "summary": "Reviewer noted that if btrfs_inc_block_group_ro() fails, the remaining entries in the list still have rci->bg set to NULL, potentially causing issues during cleanup.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential bug",
                "cleanup issue"
              ],
              "has_inline_review": true,
              "tags_given": [],
              "raw_body": "If btrfs_inc_block_group_ro() fails here, the remaining entries in the\nlist still have rci->bg set to NULL (from the initialization in\n__btrfs_balance()). The goto jumps to the cleanup loop below.",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-25",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Mason",
              "summary": "Reviewer Chris Mason noted that when the loop exits early via goto end, remaining rci entries in the list have a NULL bg pointer, which can cause a NULL pointer dereference when trying to acquire the lock",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential null pointer dereference"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "^^^^^^^^\n\nCan this dereference a NULL pointer? When the loop above exits early\nvia goto end, the remaining rci entries in the list have rci->bg == NULL.\nThe cleanup loop then calls spin_lock(&rci->bg->lock) on these entries,\nwhich would crash the kernel.",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-25",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Mason",
              "summary": "Reviewer Chris Mason noted that in case of a partial failure during balance_remap_chunks(), the block groups (bg) for unprocessed entries would be initialized to NULL, which could lead to issues when the cleanup loop runs.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "partial failure",
                "cleanup loop"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "^^^^\n\nThis is where rci->bg is initialized to NULL. If balance_remap_chunks()\nfails partway through processing the list, these unprocessed entries\nstill have bg == NULL when the cleanup loop runs.\n\n[ ... ]",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-25",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Mason",
              "summary": "Reviewer Chris Mason noted that the patch does not handle the case where a remapped block group is being removed, and requested that the code be updated to check for the REMAPPED flag before calling btrfs_remove_block_group_free_space()",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi everyone,\n\nOne more AI review from linux-next, this looks valid to me:",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-25",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Mason",
              "summary": "Reviewer Chris Mason noted a potential use-after-free issue in the code, where btrfs_put_block_group() is called before accessing the freed block group's free_space_lock mutex in btrfs_add_block_group_free_space(). He suggested moving the btrfs_put_block_group() call after the conditional block that uses dest_bg.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential use-after-free issue"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "^^^^^^^\n\nIs there a use-after-free here? btrfs_put_block_group() is called above,\nwhich can free the block group if the refcount drops to zero. Then\nbtrfs_add_block_group_free_space() dereferences dest_bg by calling\nmutex_lock(&block_group->free_space_lock).\n\nPerhaps the btrfs_put_block_group() call should be moved after the\nconditional block that uses dest_bg?",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-25",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Mason",
              "summary": "Reviewer Chris Mason noted that when incrementing the slot due to a key type mismatch, the code reads the remap item from the new slot but passes objectid and offset values from the old slot to move_existing_remap(), potentially causing mismatched address and length values to be passed.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential bug",
                "requested change"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "When the slot is incremented here because the key type does not match,\nshould the key be re-read from the new slot? The code below reads\nthe remap item from path->slots[0] (the incremented slot), but then\npasses key.objectid and key.offset (from the old slot) to\nmove_existing_remap().\n\nIf the key at the new slot has different objectid/offset values than\nthe previous slot, this would pass mismatched address and length values\nto move_existing_remap().",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-25",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Mason",
              "summary": "Reviewer Chris Mason noted that the patch does not handle the case where a remapped block group is being removed, and requested that the code be updated to skip btrfs_remove_block_group_free_space() when the REMAPPED flag is set.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi everyone,\n\nAnother AI review from my recent linux-next run:",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-25",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Mason",
              "summary": "Reviewer Chris Mason noted that the patch may leak the transaction handle due to btrfs_abort_transaction() marking it as aborted but not ending it, and suggested checking __btrfs_abort_transaction()'s comment for cleanup details.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "leak",
                "transaction handle"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Can this leak the transaction handle? btrfs_abort_transaction() marks\nthe transaction as aborted but does not end it. __btrfs_abort_transaction()\nhas a comment explaining cleanup happens in btrfs_end_transaction().",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-25",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Mason",
              "summary": "Reviewer Chris Mason noted that btrfs_lookup_block_group() can return NULL, and since every other caller checks for this before using the result, it's likely necessary to add a similar check here to prevent a potential crash in adjust_block_group_remap_bytes().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential crash",
                "NULL pointer"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "btrfs_lookup_block_group() can return NULL if no block group is found\nfor the address. Every other caller checks for NULL before using the\nresult. Is a NULL check needed here before passing dest_bg to\nadjust_block_group_remap_bytes()? That function does spin_lock(&bg->lock)\nwhich would crash on a NULL pointer.",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-25",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Mason",
              "summary": "Chris Mason noted that the commit message contains a duplicated paragraph about identity remaps, which is not an error but rather a redundant piece of information.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no bug reported",
                "request for correction not made"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "[ ... ]\n\nThis is not a bug, but the commit message appears to have a duplicated\nparagraph about identity remaps:\n\n    For an identity remap, remove_range_from_remap_tree() will adjust the\n    block group's `identity_remap_count` if this changes. If it reaches\n    zero we mark the block group as fully remapped.\n\n    For an identity remap, remove_range_from_remap_tree() will adjust the\n    block group's `identity_remap_count` if this changes. If it reaches\n    zero we mark the block group as fully remapped.",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-25",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Mason",
              "summary": "Reviewer Chris Mason noted that the patch does not handle the case where a remapped block group is being removed, and requested that the code be updated to check for this condition before calling btrfs_remove_block_group_free_space().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi everyone,\n\nAnother AI review from linux-next runs:",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-25",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Mason",
              "summary": "Reviewer Chris Mason noted that if btrfs_translate_remap() returns an error, the chunk map may not be properly released due to missing call to btrfs_free_chunk_map(), and suggested adding a goto out statement to ensure proper cleanup.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "leak",
                "missing cleanup"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "^^^^^^^^^^\n\nWhen btrfs_translate_remap() returns an error, does this leak the chunk\nmap? The map was obtained via btrfs_get_chunk_map() which increments a\nrefcount, and btrfs_get_chunk_map() documents that callers are\nresponsible for dropping the reference. The other error paths in this\nfunction use goto out, which calls btrfs_free_chunk_map(map).",
              "reply_to": "Mark Harmstone",
              "message_date": "2026-01-25",
              "analysis_source": "llm"
            },
            {
              "author": "Mark Harmstone (author)",
              "summary": "Author acknowledged that two issues were introduced by copying code from lower down in the file and agreed to fix both in a future patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged mistake",
                "agreed to fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Claude's right, and I made this mistake because I was copying the \n\"return -EINVAL\" a few lines lower down, which also leaks. I'll patch \nthem both.\n\nOn 25/01/2026 12.57 pm, Chris Mason wrote:",
              "reply_to": "Chris Mason",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Nhat Pham",
      "primary_email": "nphamcs@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Rik van Riel",
      "primary_email": "riel@surriel.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Shakeel Butt",
      "primary_email": "shakeel.butt@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH] arm64: remove HAVE_CMPXCHG_LOCAL",
          "message_id": "aZY30X9bpsV-nPsm@linux.dev",
          "url": "https://lore.kernel.org/all/aZY30X9bpsV-nPsm@linux.dev/",
          "date": "2026-02-18T22:08:07Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dev Jain",
              "summary": "Reviewer noted that the performance improvement is due to preempt_disable()/enable() in this_cpu_* macros, not LL/SC/LSE, and suggested keeping the code but removing the HAVE_CMPXCHG_LOCAL config selection.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Thanks. This concurs with my investigation on [1]. The problem\nisn't really LL/SC/LSE but preempt_disable()/enable() in\nthis_cpu_* [1, 2].\n\nI think you should only remove the selection of the config,\nbut keep the code? We may want to switch this on again if\nthe real issue gets solved.\n\n[1] https://lore.kernel.org/all/5a6782f3-d758-4d9c-975b-5ae4b5d80d4e@arm.com/\n[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/",
              "reply_to": "Jisheng Zhang",
              "message_date": "2026-02-16",
              "analysis_source": "llm"
            },
            {
              "author": "Will Deacon",
              "summary": "The reviewer, Will Deacon, expressed concerns that the patch's approach to removing HAVE_CMPXCHG_LOCAL is system-dependent and may not be suitable for all systems, particularly those that struggle with atomic operations.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "system-dependency",
                "micro-optimization"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That is _entirely_ dependent on the system, so this isn't the right\napproach. I also don't think it's something we particularly want to\nmicro-optimise to accomodate systems that suck at atomics.\n\nWill",
              "reply_to": "Jisheng Zhang",
              "message_date": "2026-02-16",
              "analysis_source": "llm"
            },
            {
              "author": "Dev Jain",
              "summary": "Reviewer Dev Jain suspects that the performance regression is caused by preempt_disable() in _pcp_protect_return, not atomics, and suggests testing this hypothesis on other hardware.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Will,\n\nAs I mention in the other email, the suspect is not the atomics, but\npreempt_disable(). On Apple M3, the regression reported in [1] resolves\nby removing preempt_disable/enable in _pcp_protect_return. To prove\nthis another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the\nregression worsened, indicating that at least on Apple M3 the\natomics are faster.\n\nIt may help to confirm this hypothesis on other hardware - perhaps\nJisheng can test with this change on his hardware and confirm\nwhether he gets the same performance improvement.\n\nBy coincidence, Yang Shi has been discussing the this_cpu_* overhead\nat [2].\n\n[1] https://lore.kernel.org/all/1052a452-9ba3-4da7-be47-7d27d27b3d1d@arm.com/\n[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/",
              "reply_to": "Will Deacon",
              "message_date": "2026-02-16",
              "analysis_source": "llm"
            },
            {
              "author": "Catalin Marinas",
              "summary": "Reviewer Catalin Marinas suggested replacing preempt disabling with local_irq_save() in the arm64 code to still use LSE atomics, citing a potential issue where another CPU can access and modify the current CPU's variable via per_cpu_ptr(), making the generic cmpxchg not atomic.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Then why don't we replace the preempt disabling with local_irq_save()\nin the arm64 code and still use the LSE atomics?\n\nIIUC (lots of macro indirection), the generic cmpxchg is not atomic, so\nanother CPU is allowed to mess this up if it accesses current CPU's\nvariable via per_cpu_ptr().\n\n-- \nCatalin",
              "reply_to": "Dev Jain",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Will Deacon",
              "summary": "Reviewer Will Deacon suggested that instead of removing HAVE_CMPXCHG_LOCAL, the patch should focus on optimizing preempt_disable() as it is used in many other places.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Even better, work on making preempt_disable() faster as it's used in many\nother places. Of course, if people want to hack the .config, they could\nalso change the preemption mode...\n\nWill",
              "reply_to": "Catalin Marinas",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Catalin Marinas",
              "summary": "Reviewer Catalin Marinas noted that preempt_enable_notrace() can lead to unconditional calls to __schedule(), and suggested a simple change to the preempt_schedule_notrace() function to avoid this issue, or alternatively changing the preemption model to make the macros no-ops.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, that would be good. It's the preempt_enable_notrace() path that\nends up calling preempt_schedule_notrace() -> __schedule() pretty much\nunconditionally. Not sure what would go wrong but some simple change\nlike this (can be done at a higher in the preempt macros to even avoid\ngetting here):\n\ndiff --git a/kernel/sched/core.c b/kernel/sched/core.c\nindex 854984967fe2..d9a5d6438303 100644\n--- a/kernel/sched/core.c\n+++ b/kernel/sched/core.c\n@@ -7119,7 +7119,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n \tif (likely(!preemptible()))\n \t\treturn;\n \n-\tdo {\n+\twhile (need_resched()) {\n \t\t/*\n \t\t * Because the function tracer can trace preempt_count_sub()\n \t\t * and it also uses preempt_enable/disable_notrace(), if\n@@ -7146,7 +7146,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n \n \t\tpreempt_latency_stop(1);\n \t\tpreempt_enable_no_resched_notrace();\n-\t} while (need_resched());\n+\t}\n }\n EXPORT_SYMBOL_GPL(preempt_schedule_notrace);\n \n\nOf course, changing the preemption model solves this by making the\nmacros no-ops but I assume people want to keep preemption on.\n\n-- \nCatalin",
              "reply_to": "Will Deacon",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph (Ampere)",
              "summary": "Reviewer noted that the performance of cmpxchg varies by platform and kernel config, and suggested that preempt_enable/disable overhead is not incurred in production systems due to PREEMPT_VOLUNTARY or distro kernels not enabling PREEMPT_FULL.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested_changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yang Shi is on vacation but we have a patchset that removes\npreempt_enable/disable from this_cpu operations on ARM64.\n\nThe performance of cmpxchg varies by platform in use and with the kernel\nconfig. The measurements that I did 2 years ago indicated that the cmpxchg\nuse with Ampere processors did not cause a regression.\n\nNote that distro kernels often do not enable PREEMPT_FULL and therefore\npreempt_disable/enable overhead is not incurred in production systems.\n\nPREEMPT_VOLUNTARY does not use preemption for this_cpu ops.",
              "reply_to": "Dev Jain",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "K Nayak",
              "summary": "Reviewer K Nayak noted that the patch removes HAVE_CMPXCHG_LOCAL, but this change is not accompanied by a corresponding update to the cmpxchg_relaxed implementation in arch/arm64/include/asm/percpu.h.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "missing update",
                "incomplete"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hello Catalin,\n\nOn 2/17/2026 10:18 PM, Catalin Marinas wrote:",
              "reply_to": "Catalin Marinas",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "K Nayak",
              "summary": "Reviewer K Nayak pointed out that the patch description's claim that the generic disable/enable irq this_cpu_cmpxchg implementation is faster than LL/SC or lse implementation on arm64 is incorrect, as the preempt_count_dec_and_test() check before calling __schedule() makes the arm64 implementation similar to x86.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "What do you mean by unconditionally? We always check\n__preempt_count_dec_and_test() before calling into __schedule().\n\nOn x86, We use MSB of preempt_count to indicate a resched and\nset_preempt_need_resched() would just clear this MSB.\n\nIf the preempt_count() turns 0, we immediately go into schedule\nor  or the next preempt_enable() -> __preempt_count_dec_and_test()\nwould see the entire preempt_count being clear and will call into\nschedule.\n\nThe arm64 implementation seems to be doing something similar too\nwith a separate \"ti->preempt.need_resched\" bit which is part of\nthe \"ti->preempt_count\"'s union so it isn't really unconditional.",
              "reply_to": "Catalin Marinas",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "K Nayak",
              "summary": "The reviewer pointed out that the patch is essentially redundant because need_resched() state has already been communicated by __preempt_count_dec_and_test(), making the additional check unnecessary.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "redundancy",
                "unnecessary"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Essentially you are simply checking it twice now on entry since\nneed_resched() state would have already been communicated by\n__preempt_count_dec_and_test().",
              "reply_to": "Catalin Marinas",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Catalin Marinas",
              "summary": "Reviewer Catalin Marinas pointed out that the patch does not handle the case where HAVE_CMPXCHG_LOCAL is removed from other architectures, potentially causing a build failure due to missing definitions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "build failure",
                "missing definitions"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Prateek,\n\nOn Wed, Feb 18, 2026 at 09:31:19AM +0530, K Prateek Nayak wrote:",
              "reply_to": "K Nayak",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Catalin Marinas",
              "summary": "Reviewer noted that the additional pointer chase and preempt_count update in this_cpu_cmpxchg_1() are the main causes of overhead, and suggested that significant performance improvements would require a large overhaul or alternative approaches like per-CPU page tables or in-kernel restartable sequences.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Ah, yes, you are right. I got the polarity of need_resched in\nthread_info wrong (we should have named it no_need_to_resched).\n\nSo in the common case, the overhead is caused by the additional\npointer chase and preempt_count update, on top of the cpu offset read.\nNot sure we can squeeze any more cycles out of these without some\nlarge overhaul like:\n\nhttps://git.kernel.org/mark/c/84ee5f23f93d4a650e828f831da9ed29c54623c5\n\nor Yang's per-CPU page tables. Well, there are more ideas like in-kernel\nrestartable sequences but they move the overhead elsewhere.\n\nThanks.\n\n-- \nCatalin",
              "reply_to": "K Nayak",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that mod_node_page_state() can be called in NMI context, which may lead to issues with the generic disable/enable irq implementation used by this patch, and requested further consideration of NMI safety.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NMI",
                "irq"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Please note that mod_node_page_state() can be called in NMI context and\ngeneric disable/enable irq are not safe against NMIs (newer arm arch supports\nNMI).",
              "reply_to": "Jisheng Zhang",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Jisheng Zhang (author)",
              "summary": "Author asked for clarification on whether reviewer thinks cmpxchg_local is better than generic disable/enable irq version on new arm64 systems, indicating no clear resolution or agreement yet.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Will,\n\nI read this as an implication that the cmpxchg_local version is better\nthan generic disable/enable irq version on the newer arm64 systems. Is my\nunderstanding correct?",
              "reply_to": "Dev Jain",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Jisheng Zhang (author)",
              "summary": "Author acknowledged that removing preempt_disable/enable from _pcp_protect_return improves performance, but the HAVE_CMPXCHG_LOCAL version is still slower than the generic implementation on specific platforms.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a performance issue",
                "agreed to further investigation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Dev,\n\nThanks for the hints. I tried to remove the preempt_disable/enable from\n_pcp_protect_return, it improves, but the HAVE_CMPXCHG_LOCAL version is\nstill worse than generic disable/enable irq version on CA55 and CA73.",
              "reply_to": "Dev Jain",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Jisheng Zhang (author)",
              "summary": "Author acknowledged that arm, powerpc, and mips architectures rely on the generic disable/enable irq version of cmpxchg, implying they may not be safe in NMI context, but did not commit to revising the patch or addressing this issue.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a potential issue",
                "did not commit to revising the patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "hmm, interesting...\n\nfgrep HAVE_NMI arch/*/Kconfig\nthen\nfgrep HAVE_CMPXCHG_LOCAL arch/*/Kconfig\n\nshows that only x86, arm64, s390 and loongarch are safe, while arm,\npowerpc and mips enable HAVE_NMI but missing HAVE_CMPXCHG_LOCAL, so\nthey rely on generic generic disable/enable irq version, so you imply\nthat these three arch are not safe considering mod_node_page_state()\nin NMI context.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that vmstat updates may require NMI-safe cmpxchg operations, similar to memcg stats, and questioned whether adding complexity for arm, powerpc, and mips architectures is necessary.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "added complexity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes it seems like it. For memcg stats, we use ARCH_HAVE_NMI_SAFE_CMPXCHG and\nARCH_HAS_NMI_SAFE_THIS_CPU_OPS config options to correctly handle the updates\nfrom NMI context. Maybe we need something similar for vmstat as well.\n\nSo arm, powerpc and mips does not have ARCH_HAS_NMI_SAFE_THIS_CPU_OPS but\npowerpc does have ARCH_HAVE_NMI_SAFE_CMPXCHG and arm has\nit for CPU_V7, CPU_V7M & CPU_V6K models.\n\nI wonder if we need to add complexity for these archs.",
              "reply_to": "Jisheng Zhang",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [linux-next20260216]Warnings at mm/page_counter.c:60 at page_counter_cancel+0x110/0x134, CPU#24: kworker/24:3/1074770",
          "message_id": "aZYyNtI-4yS4BFXX@linux.dev",
          "url": "https://lore.kernel.org/all/aZYyNtI-4yS4BFXX@linux.dev/",
          "date": "2026-02-18T21:43:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Carlos Maiolino",
              "summary": "Reviewer questioned the relevance of the issue to xfs and suggested not sharing it with the linux-mm mailing list.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning relevance",
                "suggesting not sharing"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This does not seem related to xfs at all, I'm not Cc'ing linux-mm\nbecause Vlastimil is already Cc'ed so, he knows better than me if this\nis worth sharing with linux-mm.",
              "reply_to": "Venkat Bagalkote",
              "message_date": "2026-02-17",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed uncertainty about whether two specific configuration options are related to a page_counter underflow warning and a subsequent crash.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "not sure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I found the following two config options interesting. Not sure if it is related\nto this warning (and the other crash).",
              "reply_to": "Vlastimil Babka",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Usama Arif",
      "primary_email": "usama.arif@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    }
  ]
}