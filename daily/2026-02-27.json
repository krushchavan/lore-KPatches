{
  "date": "2026-02-27",
  "report_file": "2026-02-27_ollama_llama3.1-8b.html",
  "status": "in_progress",
  "last_updated": "2026-02-28 07:38 UTC",
  "llm_backends": [
    [
      "ollama",
      "llama3.1:8b"
    ]
  ],
  "generation_time_seconds": 0.0,
  "developer_reports": [
    {
      "name": "Alexandre Ghiti",
      "primary_email": "alexghiti@rivosinc.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Boris Burkov",
      "primary_email": "boris@bur.io",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Dmitry Ilvokhin",
      "primary_email": "d@ilvokhin.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v4 5/5] mm: add tracepoints for zone lock",
          "message_id": "ae145fe890f028409f727b4921904b547346fa0b.1772206930.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/ae145fe890f028409f727b4921904b547346fa0b.1772206930.git.d@ilvokhin.com/",
          "date": "2026-02-27T16:01:22Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper to check if tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This patch introduces three new tracepoints: zone_lock_start_locking, zone_lock_acquire_returned, and zone_lock_released.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Steven Rostedt",
              "summary": "reviewer noted that the zone lock tracepoints do not handle the case where CONFIG_TRACING is disabled, and suggested adding a check for this condition to avoid unnecessary function calls\n\nThe reviewer questioned the use of a boolean 'success' parameter in __zone_lock_trace_acquire_returned(), suggesting that it should always be set to true and proposing a simplified version.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "CONFIG_TRACING",
                "unnecessary function calls",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, 27 Feb 2026 16:00:27 +0000\nDmitry Ilvokhin <d@ilvokhin.com> wrote:\n\n---\n\nWhy the \"success\" variable and not just:\n\n\t__zone_lock_trace_acquire_returned(zone, true);\n\n ?",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-27",
              "message_id": "20260227144649.3dbff742@gandalf.local.home",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v4 3/5] mm: convert compaction to zone lock wrappers",
          "message_id": "3a09e46f52cf9f709b0725bc2b648cc5212843b2.1772206930.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/3a09e46f52cf9f709b0725bc2b648cc5212843b2.1772206930.git.d@ilvokhin.com/",
          "date": "2026-02-27T16:01:21Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper to check if tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This patch introduces three new tracepoints: zone_lock_start_locking, zone_lock_acquire_returned, and zone_lock_released.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Steven Rostedt",
              "summary": "reviewer noted that the zone lock tracepoints do not handle the case where CONFIG_TRACING is disabled, and suggested adding a check for this condition to avoid unnecessary function calls\n\nThe reviewer questioned the use of a boolean 'success' parameter in __zone_lock_trace_acquire_returned(), suggesting that it should always be set to true and proposing a simplified version.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "CONFIG_TRACING",
                "unnecessary function calls",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, 27 Feb 2026 16:00:27 +0000\nDmitry Ilvokhin <d@ilvokhin.com> wrote:\n\n---\n\nWhy the \"success\" variable and not just:\n\n\t__zone_lock_trace_acquire_returned(zone, true);\n\n ?",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v4 4/5] mm: rename zone->lock to zone->_lock",
          "message_id": "d61500c5784c64e971f4d328c57639303c475f81.1772206930.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/d61500c5784c64e971f4d328c57639303c475f81.1772206930.git.d@ilvokhin.com/",
          "date": "2026-02-27T16:01:21Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper to check if tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This patch introduces three new tracepoints: zone_lock_start_locking, zone_lock_acquire_returned, and zone_lock_released.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Steven Rostedt",
              "summary": "reviewer noted that the zone lock tracepoints do not handle the case where CONFIG_TRACING is disabled, and suggested adding a check for this condition to avoid unnecessary function calls\n\nThe reviewer questioned the use of a boolean 'success' parameter in __zone_lock_trace_acquire_returned(), suggesting that it should always be set to true and proposing a simplified version.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "CONFIG_TRACING",
                "unnecessary function calls",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, 27 Feb 2026 16:00:27 +0000\nDmitry Ilvokhin <d@ilvokhin.com> wrote:\n\n---\n\nWhy the \"success\" variable and not just:\n\n\t__zone_lock_trace_acquire_returned(zone, true);\n\n ?",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v4 2/5] mm: convert zone lock users to wrappers",
          "message_id": "d26a43ebed2f0f1edb9cfe4fbed16dd31c7a069c.1772206930.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/d26a43ebed2f0f1edb9cfe4fbed16dd31c7a069c.1772206930.git.d@ilvokhin.com/",
          "date": "2026-02-27T16:01:21Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper to check if tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This patch introduces three new tracepoints: zone_lock_start_locking, zone_lock_acquire_returned, and zone_lock_released.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Steven Rostedt",
              "summary": "reviewer noted that the zone lock tracepoints do not handle the case where CONFIG_TRACING is disabled, and suggested adding a check for this condition to avoid unnecessary function calls\n\nThe reviewer questioned the use of a boolean 'success' parameter in __zone_lock_trace_acquire_returned(), suggesting that it should always be set to true and proposing a simplified version.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "CONFIG_TRACING",
                "unnecessary function calls",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, 27 Feb 2026 16:00:27 +0000\nDmitry Ilvokhin <d@ilvokhin.com> wrote:\n\n---\n\nWhy the \"success\" variable and not just:\n\n\t__zone_lock_trace_acquire_returned(zone, true);\n\n ?",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v4 1/5] mm: introduce zone lock wrappers",
          "message_id": "849dee9c47df1e6fba97c9933af0d5a08b8e15d3.1772206930.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/849dee9c47df1e6fba97c9933af0d5a08b8e15d3.1772206930.git.d@ilvokhin.com/",
          "date": "2026-02-27T16:01:20Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper to check if tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This patch introduces three new tracepoints: zone_lock_start_locking, zone_lock_acquire_returned, and zone_lock_released.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Steven Rostedt",
              "summary": "reviewer noted that the zone lock tracepoints do not handle the case where CONFIG_TRACING is disabled, and suggested adding a check for this condition to avoid unnecessary function calls\n\nThe reviewer questioned the use of a boolean 'success' parameter in __zone_lock_trace_acquire_returned(), suggesting that it should always be set to true and proposing a simplified version.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "CONFIG_TRACING",
                "unnecessary function calls",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, 27 Feb 2026 16:00:27 +0000\nDmitry Ilvokhin <d@ilvokhin.com> wrote:\n\n---\n\nWhy the \"success\" variable and not just:\n\n\t__zone_lock_trace_acquire_returned(zone, true);\n\n ?",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v4 0/5] mm: zone lock tracepoint instrumentation",
          "message_id": "cover.1772206930.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/cover.1772206930.git.d@ilvokhin.com/",
          "date": "2026-02-27T16:01:19Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper to check if tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This patch introduces three new tracepoints: zone_lock_start_locking, zone_lock_acquire_returned, and zone_lock_released.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Steven Rostedt",
              "summary": "reviewer noted that the zone lock tracepoints do not handle the case where CONFIG_TRACING is disabled, and suggested adding a check for this condition to avoid unnecessary function calls\n\nThe reviewer questioned the use of a boolean 'success' parameter in __zone_lock_trace_acquire_returned(), suggesting that it should always be set to true and proposing a simplified version.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "CONFIG_TRACING",
                "unnecessary function calls",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Fri, 27 Feb 2026 16:00:27 +0000\nDmitry Ilvokhin <d@ilvokhin.com> wrote:\n\n---\n\nWhy the \"success\" variable and not just:\n\n\t__zone_lock_trace_acquire_returned(zone, true);\n\n ?",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 5/5] mm: add tracepoints for zone lock",
          "message_id": "378089dd269249d3d7981fe10eb8b49ad551d353.1772129168.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/378089dd269249d3d7981fe10eb8b49ad551d353.1772129168.git.d@ilvokhin.com/",
          "date": "2026-02-26T18:26:59Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-26",
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 3/5] mm: convert compaction to zone lock wrappers",
          "message_id": "01729baf359e4c6612aead53f1fcb644f782d1de.1772129168.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/01729baf359e4c6612aead53f1fcb644f782d1de.1772129168.git.d@ilvokhin.com/",
          "date": "2026-02-26T18:26:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-26",
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 4/5] mm: rename zone->lock to zone->_lock",
          "message_id": "1221b8e7fa9f5694f3c4e411f01581b5aba9bc63.1772129168.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/1221b8e7fa9f5694f3c4e411f01581b5aba9bc63.1772129168.git.d@ilvokhin.com/",
          "date": "2026-02-26T18:26:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-26",
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 1/5] mm: introduce zone lock wrappers",
          "message_id": "5bcc39cd3a227944d0fbe75ff86cdac92b38d4ca.1772129168.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/5bcc39cd3a227944d0fbe75ff86cdac92b38d4ca.1772129168.git.d@ilvokhin.com/",
          "date": "2026-02-26T18:26:56Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-26",
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 0/5] mm: zone lock tracepoint instrumentation",
          "message_id": "cover.1772129168.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/cover.1772129168.git.d@ilvokhin.com/",
          "date": "2026-02-26T18:26:56Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-26",
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 2/5] mm: convert zone lock users to wrappers",
          "message_id": "e5324d64361f86d930d940a5b49235f7996efe53.1772129168.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/e5324d64361f86d930d940a5b49235f7996efe53.1772129168.git.d@ilvokhin.com/",
          "date": "2026-02-26T18:26:56Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-26",
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v3 3/5] mm: convert compaction to zone lock wrappers",
          "message_id": "aaGuQFaCij5hvw4N@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aaGuQFaCij5hvw4N@shell.ilvokhin.com/",
          "date": "2026-02-27T14:46:28Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch author acknowledged a bug and promised to fix it in the next version.",
          "analysis_source": "llm",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v2] x86/irq: Optimize interrupts decimals printing",
          "message_id": "aaGkUXrxzClRCqPy@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aaGkUXrxzClRCqPy@shell.ilvokhin.com/",
          "date": "2026-02-27T14:04:07Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v3 1/5] mm: introduce zone lock wrappers",
          "message_id": "aaGWuPxfAMeHSKL1@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aaGWuPxfAMeHSKL1@shell.ilvokhin.com/",
          "date": "2026-02-27T13:06:03Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch author acknowledged a bug and promised to fix it in the next version.",
          "analysis_source": "llm",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v3 5/5] mm: add tracepoints for zone lock",
          "message_id": "aaGQZOk0_afa8SOk@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aaGQZOk0_afa8SOk@shell.ilvokhin.com/",
          "date": "2026-02-27T12:39:06Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch author acknowledged a bug and promised to fix it in the next version.",
          "analysis_source": "llm",
          "review_comments": []
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Gregory Price",
      "primary_email": "gourry@gourry.net",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v2 3/3] cxl: Move pci generic code",
          "message_id": "aaIjq03lnrD_4tsS@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aaIjq03lnrD_4tsS@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-27T23:07:28Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Gregory suggested using the provided patch as an add-on to existing code, allowing for easier debugging if issues arise.",
          "analysis_source": "llm",
          "review_comments": []
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Jeff Layton",
      "primary_email": "jlayton@kernel.org",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 61/61] vfs: update core format strings for u64 i_ino",
          "message_id": "20260226-iino-u64-v1-61-ccceff366db9@kernel.org",
          "url": "https://lore.kernel.org/all/20260226-iino-u64-v1-61-ccceff366db9@kernel.org/",
          "date": "2026-02-26T16:09:05Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-26",
          "patch_summary": "This patch updates the format strings in various Linux kernel files to accommodate the change from unsigned long to u64 for inode numbers (i_ino). The changes affect the pipe, dcache, fserror, and eventpoll modules, updating print statements to use %llu/%llx instead of %lu/%lx for printing inode numbers. This ensures that inode numbers are printed correctly as 64-bit values.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx and 0UL literal to 0ULL in various files, but did not provide any specific feedback or concerns",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Update format strings from %lu/%lx to %llu/%llx and 0UL literal to\n0ULL in pipe, dcache, fserror, and eventpoll, now that i_ino is u64\ninstead of unsigned long.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n fs/dcache.c    | 4 ++--\n fs/eventpoll.c | 2 +-\n fs/fserror.c   | 2 +-\n fs/pipe.c      | 2 +-\n 4 files changed, 5 insertions(+), 5 deletions(-)\n\ndiff --git a/fs/dcache.c b/fs/dcache.c\nindex 24f4f3acaa8cffd6f98124eec38c1a92d6c9fd8e..9e8425ecd88955c72027d21591b1d12c87e7e8aa 100644\n--- a/fs/dcache.c\n+++ b/fs/dcache.c\n@@ -1637,11 +1637,11 @@ static enum d_walk_ret umount_check(void *_data, struct dentry *dentry)\n \tif (dentry == _data && dentry->d_lockref.count == 1)\n \t\treturn D_WALK_CONTINUE;\n \n-\tWARN(1, \"BUG: Dentry %p{i=%lx,n=%pd} \"\n+\tWARN(1, \"BUG: Dentry %p{i=%llx,n=%pd} \"\n \t\t\t\" still in use (%d) [unmount of %s %s]\\n\",\n \t\t       dentry,\n \t\t       dentry->d_inode ?\n-\t\t       dentry->d_inode->i_ino : 0UL,\n+\t\t       dentry->d_inode->i_ino : 0ULL,\n \t\t       dentry,\n \t\t       dentry->d_lockref.count,\n \t\t       dentry->d_sb->s_type->name,\ndiff --git a/fs/eventpoll.c b/fs/eventpoll.c\nindex 5714e900567c499739bb205f43bb6bf73f7ebe54..4ccd4d2e31adf571f939d2e777123e40302e565f 100644\n--- a/fs/eventpoll.c\n+++ b/fs/eventpoll.c\n@@ -1080,7 +1080,7 @@ static void ep_show_fdinfo(struct seq_file *m, struct file *f)\n \t\tstruct inode *inode = file_inode(epi->ffd.file);\n \n \t\tseq_printf(m, \"tfd: %8d events: %8x data: %16llx \"\n-\t\t\t   \" pos:%lli ino:%lx sdev:%x\\n\",\n+\t\t\t   \" pos:%lli ino:%llx sdev:%x\\n\",\n \t\t\t   epi->ffd.fd, epi->event.events,\n \t\t\t   (long long)epi->event.data,\n \t\t\t   (long long)epi->ffd.file->f_pos,\ndiff --git a/fs/fserror.c b/fs/fserror.c\nindex 06ca86adab9b769dfb72ec58b9e51627abee5152..1e4d11fd9562fd158a23b64ca60e9b7e01719cb8 100644\n--- a/fs/fserror.c\n+++ b/fs/fserror.c\n@@ -176,7 +176,7 @@ void fserror_report(struct super_block *sb, struct inode *inode,\n lost:\n \tif (inode)\n \t\tpr_err_ratelimited(\n- \"%s: lost file I/O error report for ino %lu type %u pos 0x%llx len 0x%llx error %d\",\n+ \"%s: lost file I/O error report for ino %llu type %u pos 0x%llx len 0x%llx error %d\",\n \t\t       sb->s_id, inode->i_ino, type, pos, len, error);\n \telse\n \t\tpr_err_ratelimited(\ndiff --git a/fs/pipe.c b/fs/pipe.c\nindex b44a756c0b4165edc2801b2290bf35480245d7a6..9841648c9cf3e8e569cf6ba5c792624fe92396f5 100644\n--- a/fs/pipe.c\n+++ b/fs/pipe.c\n@@ -873,7 +873,7 @@ static struct vfsmount *pipe_mnt __ro_after_init;\n  */\n static char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n {\n-\treturn dynamic_dname(buffer, buflen, \"pipe:[%lu]\",\n+\treturn dynamic_dname(buffer, buflen, \"pipe:[%llu]\",\n \t\t\t\td_inode(dentry)->i_ino);\n }\n \n\n-- \n2.53.0\n\n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On Thu, Feb 26, 2026 at 10:56:03AM -0500, Jeff Layton wrote:\n> Update format strings from %lu/%lx to %llu/%llx and 0UL literal to\n> 0ULL in pipe, dcache, fserror, and eventpoll, now that i_ino is u64\n> instead of unsigned long.\n> \n> Signed-off-by: Jeff Layton <jlayton@kernel.org>\n\nAcked-by: \"Darrick J. Wong\" <djwong@kernel.org>\n\n--D\n\n> ---\n>  fs/dcache.c    | 4 ++--\n>  fs/eventpoll.c | 2 +-\n>  fs/fserror.c   | 2 +-\n>  fs/pipe.c      | 2 +-\n>  4 files changed, 5 insertions(+), 5 deletions(-)\n> \n> diff --git a/fs/dcache.c b/fs/dcache.c\n> index 24f4f3acaa8cffd6f98124eec38c1a92d6c9fd8e..9e8425ecd88955c72027d21591b1d12c87e7e8aa 100644\n> --- a/fs/dcache.c\n> +++ b/fs/dcache.c\n> @@ -1637,11 +1637,11 @@ static enum d_walk_ret umount_check(void *_data, struct dentry *dentry)\n>  \tif (dentry == _data && dentry->d_lockref.count == 1)\n>  \t\treturn D_WALK_CONTINUE;\n>  \n> -\tWARN(1, \"BUG: Dentry %p{i=%lx,n=%pd} \"\n> +\tWARN(1, \"BUG: Dentry %p{i=%llx,n=%pd} \"\n>  \t\t\t\" still in use (%d) [unmount of %s %s]\\n\",\n>  \t\t       dentry,\n>  \t\t       dentry->d_inode ?\n> -\t\t       dentry->d_inode->i_ino : 0UL,\n> +\t\t       dentry->d_inode->i_ino : 0ULL,\n>  \t\t       dentry,\n>  \t\t       dentry->d_lockref.count,\n>  \t\t       dentry->d_sb->s_type->name,\n> diff --git a/fs/eventpoll.c b/fs/eventpoll.c\n> index 5714e900567c499739bb205f43bb6bf73f7ebe54..4ccd4d2e31adf571f939d2e777123e40302e565f 100644\n> --- a/fs/eventpoll.c\n> +++ b/fs/eventpoll.c\n> @@ -1080,7 +1080,7 @@ static void ep_show_fdinfo(struct seq_file *m, struct file *f)\n>  \t\tstruct inode *inode = file_inode(epi->ffd.file);\n>  \n>  \t\tseq_printf(m, \"tfd: %8d events: %8x data: %16llx \"\n> -\t\t\t   \" pos:%lli ino:%lx sdev:%x\\n\",\n> +\t\t\t   \" pos:%lli ino:%llx sdev:%x\\n\",\n>  \t\t\t   epi->ffd.fd, epi->event.events,\n>  \t\t\t   (long long)epi->event.data,\n>  \t\t\t   (long long)epi->ffd.file->f_pos,\n> diff --git a/fs/fserror.c b/fs/fserror.c\n> index 06ca86adab9b769dfb72ec58b9e51627abee5152..1e4d11fd9562fd158a23b64ca60e9b7e01719cb8 100644\n> --- a/fs/fserror.c\n> +++ b/fs/fserror.c\n> @@ -176,7 +176,7 @@ void fserror_report(struct super_block *sb, struct inode *inode,\n>  lost:\n>  \tif (inode)\n>  \t\tpr_err_ratelimited(\n> - \"%s: lost file I/O error report for ino %lu type %u pos 0x%llx len 0x%llx error %d\",\n> + \"%s: lost file I/O error report for ino %llu type %u pos 0x%llx len 0x%llx error %d\",\n>  \t\t       sb->s_id, inode->i_ino, type, pos, len, error);\n>  \telse\n>  \t\tpr_err_ratelimited(\n> diff --git a/fs/pipe.c b/fs/pipe.c\n> index b44a756c0b4165edc2801b2290bf35480245d7a6..9841648c9cf3e8e569cf6ba5c792624fe92396f5 100644\n> --- a/fs/pipe.c\n> +++ b/fs/pipe.c\n> @@ -873,7 +873,7 @@ static struct vfsmount *pipe_mnt __ro_after_init;\n>   */\n>  static char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n>  {\n> -\treturn dynamic_dname(buffer, buflen, \"pipe:[%lu]\",\n> +\treturn dynamic_dname(buffer, buflen, \"pipe:[%llu]\",\n>  \t\t\t\td_inode(dentry)->i_ino);\n>  }\n>  \n> \n> -- \n> 2.53.0\n> \n> \n\n\n---\n\nOn Thu, Feb 26, 2026 at 10:56:03AM -0500, Jeff Layton wrote:\n> Update format strings from %lu/%lx to %llu/%llx and 0UL literal to\n> 0ULL in pipe, dcache, fserror, and eventpoll, now that i_ino is u64\n> instead of unsigned long.\n> \n> Signed-off-by: Jeff Layton <jlayton@kernel.org>\n\nAcked-by: \"Darrick J. Wong\" <djwong@kernel.org>\n\n--D\n\n> ---\n>  fs/dcache.c    | 4 ++--\n>  fs/eventpoll.c | 2 +-\n>  fs/fserror.c   | 2 +-\n>  fs/pipe.c      | 2 +-\n>  4 files changed, 5 insertions(+), 5 deletions(-)\n> \n> diff --git a/fs/dcache.c b/fs/dcache.c\n> index 24f4f3acaa8cffd6f98124eec38c1a92d6c9fd8e..9e8425ecd88955c72027d21591b1d12c87e7e8aa 100644\n> --- a/fs/dcache.c\n> +++ b/fs/dcache.c\n> @@ -1637,11 +1637,11 @@ static enum d_walk_ret umount_check(void *_data, struct dentry *dentry)\n>  \tif (dentry == _data && dentry->d_lockref.count == 1)\n>  \t\treturn D_WALK_CONTINUE;\n>  \n> -\tWARN(1, \"BUG: Dentry %p{i=%lx,n=%pd} \"\n> +\tWARN(1, \"BUG: Dentry %p{i=%llx,n=%pd} \"\n>  \t\t\t\" still in use (%d) [unmount of %s %s]\\n\",\n>  \t\t       dentry,\n>  \t\t       dentry->d_inode ?\n> -\t\t       dentry->d_inode->i_ino : 0UL,\n> +\t\t       dentry->d_inode->i_ino : 0ULL,\n>  \t\t       dentry,\n>  \t\t       dentry->d_lockref.count,\n>  \t\t       dentry->d_sb->s_type->name,\n> diff --git a/fs/eventpoll.c b/fs/eventpoll.c\n> index 5714e900567c499739bb205f43bb6bf73f7ebe54..4ccd4d2e31adf571f939d2e777123e40302e565f 100644\n> --- a/fs/eventpoll.c\n> +++ b/fs/eventpoll.c\n> @@ -1080,7 +1080,7 @@ static void ep_show_fdinfo(struct seq_file *m, struct file *f)\n>  \t\tstruct inode *inode = file_inode(epi->ffd.file);\n>  \n>  \t\tseq_printf(m, \"tfd: %8d events: %8x data: %16llx \"\n> -\t\t\t   \" pos:%lli ino:%lx sdev:%x\\n\",\n> +\t\t\t   \" pos:%lli ino:%llx sdev:%x\\n\",\n>  \t\t\t   epi->ffd.fd, epi->event.events,\n>  \t\t\t   (long long)epi->event.data,\n>  \t\t\t   (long long)epi->ffd.file->f_pos,\n> diff --git a/fs/fserror.c b/fs/fserror.c\n> index 06ca86adab9b769dfb72ec58b9e51627abee5152..1e4d11fd9562fd158a23b64ca60e9b7e01719cb8 100644\n> --- a/fs/fserror.c\n> +++ b/fs/fserror.c\n> @@ -176,7 +176,7 @@ void fserror_report(struct super_block *sb, struct inode *inode,\n>  lost:\n>  \tif (inode)\n>  \t\tpr_err_ratelimited(\n> - \"%s: lost file I/O error report for ino %lu type %u pos 0x%llx len 0x%llx error %d\",\n> + \"%s: lost file I/O error report for ino %llu type %u pos 0x%llx len 0x%llx error %d\",\n>  \t\t       sb->s_id, inode->i_ino, type, pos, len, error);\n>  \telse\n>  \t\tpr_err_ratelimited(\n> diff --git a/fs/pipe.c b/fs/pipe.c\n> index b44a756c0b4165edc2801b2290bf35480245d7a6..9841648c9cf3e8e569cf6ba5c792624fe92396f5 100644\n> --- a/fs/pipe.c\n> +++ b/fs/pipe.c\n> @@ -873,7 +873,7 @@ static struct vfsmount *pipe_mnt __ro_after_init;\n>   */\n>  static char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n>  {\n> -\treturn dynamic_dname(buffer, buflen, \"pipe:[%lu]\",\n> +\treturn dynamic_dname(buffer, buflen, \"pipe:[%llu]\",\n>  \t\t\t\td_inode(dentry)->i_ino);\n>  }\n>  \n> \n> -- \n> 2.53.0\n> \n> \n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/\n",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Mathieu Desnoyers",
              "summary": "Reviewer noted that applying this patch as a fixup at the end of the series would break bisectability, making it harder to identify the exact commit causing issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Doing this as a fixup at the end of the series breaks bissectability.\n\nThanks,\n\nMathieu\n\n-- \nMathieu Desnoyers\nEfficiOS Inc.\nhttps://www.efficios.com",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx, but did not update the literal 0UL to 0ULL in all affected files.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 03/61] trace: update VFS-layer trace events for u64 i_ino",
          "message_id": "4481598d13941191af0369bf204fe577d33f33bb.camel@kernel.org",
          "url": "https://lore.kernel.org/all/4481598d13941191af0369bf204fe577d33f33bb.camel@kernel.org/",
          "date": "2026-02-27T21:05:54Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch updates VFS-layer trace events to use 64-bit fields for inode numbers (i_ino) instead of the current unsigned long type. This change is necessary because the i_ino field will be converted to a u64 in future patches, and existing tracepoints need to accommodate this change. The update aims to improve reliability and consistency across different machine architectures, including 32-bit systems.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Jeff Layton (author)",
              "summary": "Author acknowledged reviewer's concern about reordering tracepoint fields for better packing and agreed to look into it, potentially addressing the issue in a future version of the patch.\n\nThe author is addressing a concern about tracepoint field sizes and packing, explaining that extending them to 64-bit fields is preferred over using the new kino_t typedef. The author acknowledges that reordering the tracepoint fields for better packing has material consequences and will look at it.\n\nAuthor acknowledged the need to reorder tracepoint fields for better packing, which has material consequences, and agreed to address this in a future patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged",
                "agreed",
                "acknowledged a concern",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ok, I'll look at that. Given the number of places that need it though I\nmay do it in a separate patch.\n\n---\n\nNo, ino_t isn't. That's part of the ABI and has to remain unsigned\nlong. The point of this series is to make inode->i_ino a u64. Any event\nholding an ino_t today is going to need a 64-bit field to fully\ndescribe it.\n\nAnd to be clear, this should make things better for 32-bit boxes in the\nlong run. Once this change is done, i_ino should be a reliable source\nof info regardless of machine's word size.\n\nFor the tracepoints, I think it's best to just extend them to 64-bit\nfields outright rather than using the new (temporary) kino_t typedef\nthat I'm adding.\n\n---\n\nThanks for the review! I'll definitely look at reordering the\ntracepoint fields for better packing since that has material\nconsequences.\n-- \nJeff Layton <jlayton@kernel.org>\n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "",
              "message_date": "2026-02-27",
              "message_id": "4481598d13941191af0369bf204fe577d33f33bb.camel@kernel.org",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v5 1/1] NFSD: move accumulated callback ops to per-net namespace",
          "message_id": "7ce6c46bef16d20f6e8ae8da1576b1a765cea1ca.camel@kernel.org",
          "url": "https://lore.kernel.org/all/7ce6c46bef16d20f6e8ae8da1576b1a765cea1ca.camel@kernel.org/",
          "date": "2026-02-27T19:45:42Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch moves accumulated callback operations for NFSD (Network File System Daemon) to a per-net namespace, improving the interface for userland applications and making it more efficient by avoiding file-based interfaces.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Dai Ngo",
              "summary": "Reviewer noted that the patch only moves statistics from a global location to a per-net namespace, and does not add any new objects under /proc as previously suggested by Jeff Layton.\n\nReviewer Dai Ngo noted that while netlink is a cleaner interface, it requires userland utility to retrieve and display data, and suggested preserving /proc/net/rpc/nfsd output for developers who need quick access\n\nReviewer suggested using netlink for accessing NFS stats, recommending replication of existing /proc/net/rpc/nfsd stats before adding new ones and implementing a fallback mechanism in userland",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change",
                "requested preservation of existing functionality",
                "reviewer's comment is neutral as it only provides guidance without expressing a clear opinion or requesting changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This patch does not add any new object under /proc. It moves existing\nstatistic from global to per-net-namespace.\n\n---\n\nYes, netlink is a much cleaner interface but it requires userland\nutility written to retrieve and display the data in a user-friendly\nformat for end users or administrators. I think the output of\nat /proc/net/rpc/nfsd' is still useful for developers who want a\nquick look of what's going in the back channel.\n\n---\n\nI will look into this.\n\nThanks,\n-Dai",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-27",
              "message_id": "c04bd15f-0744-4ff4-92b9-4847e08e0174@oracle.com",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 00/61] vfs: change inode->i_ino from unsigned long to u64",
          "message_id": "1b38afe8ff4ba5880835d919c42e094d77a9d5ce.camel@kernel.org",
          "url": "https://lore.kernel.org/all/1b38afe8ff4ba5880835d919c42e094d77a9d5ce.camel@kernel.org/",
          "date": "2026-02-27T19:35:26Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch updates VFS-layer trace events to use 64-bit fields for inode numbers (i_ino) instead of the current unsigned long type. This change is necessary because the i_ino field will be converted to a u64 in future patches, and existing tracepoints need to accommodate this change. The update aims to improve reliability and consistency across different machine architectures, including 32-bit systems.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton (author)",
              "summary": "Author acknowledged reviewer's concern about reordering tracepoint fields for better packing and agreed to look into it, potentially addressing the issue in a future version of the patch.\n\nThe author is addressing a concern about tracepoint field sizes and packing, explaining that extending them to 64-bit fields is preferred over using the new kino_t typedef. The author acknowledges that reordering the tracepoint fields for better packing has material consequences and will look at it.\n\nAuthor acknowledged the need to reorder tracepoint fields for better packing, which has material consequences, and agreed to address this in a future patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged",
                "agreed",
                "acknowledged a concern",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ok, I'll look at that. Given the number of places that need it though I\nmay do it in a separate patch.\n\n---\n\nNo, ino_t isn't. That's part of the ABI and has to remain unsigned\nlong. The point of this series is to make inode->i_ino a u64. Any event\nholding an ino_t today is going to need a 64-bit field to fully\ndescribe it.\n\nAnd to be clear, this should make things better for 32-bit boxes in the\nlong run. Once this change is done, i_ino should be a reliable source\nof info regardless of machine's word size.\n\nFor the tracepoints, I think it's best to just extend them to 64-bit\nfields outright rather than using the new (temporary) kino_t typedef\nthat I'm adding.\n\n---\n\nThanks for the review! I'll definitely look at reordering the\ntracepoint fields for better packing since that has material\nconsequences.\n-- \nJeff Layton <jlayton@kernel.org>\n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 00/61] vfs: change inode->i_ino from unsigned long to u64",
          "message_id": "4a462d40899698586c110add96ce3fab6ddac30b.camel@kernel.org",
          "url": "https://lore.kernel.org/all/4a462d40899698586c110add96ce3fab6ddac30b.camel@kernel.org/",
          "date": "2026-02-27T17:19:20Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch updates VFS-layer trace events to use 64-bit fields for inode numbers (i_ino) instead of the current unsigned long type. This change is necessary because the i_ino field will be converted to a u64 in future patches, and existing tracepoints need to accommodate this change. The update aims to improve reliability and consistency across different machine architectures, including 32-bit systems.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton (author)",
              "summary": "Author acknowledged reviewer's concern about reordering tracepoint fields for better packing and agreed to look into it, potentially addressing the issue in a future version of the patch.\n\nThe author is addressing a concern about tracepoint field sizes and packing, explaining that extending them to 64-bit fields is preferred over using the new kino_t typedef. The author acknowledges that reordering the tracepoint fields for better packing has material consequences and will look at it.\n\nAuthor acknowledged the need to reorder tracepoint fields for better packing, which has material consequences, and agreed to address this in a future patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged",
                "agreed",
                "acknowledged a concern",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ok, I'll look at that. Given the number of places that need it though I\nmay do it in a separate patch.\n\n---\n\nNo, ino_t isn't. That's part of the ABI and has to remain unsigned\nlong. The point of this series is to make inode->i_ino a u64. Any event\nholding an ino_t today is going to need a 64-bit field to fully\ndescribe it.\n\nAnd to be clear, this should make things better for 32-bit boxes in the\nlong run. Once this change is done, i_ino should be a reliable source\nof info regardless of machine's word size.\n\nFor the tracepoints, I think it's best to just extend them to 64-bit\nfields outright rather than using the new (temporary) kino_t typedef\nthat I'm adding.\n\n---\n\nThanks for the review! I'll definitely look at reordering the\ntracepoint fields for better packing since that has material\nconsequences.\n-- \nJeff Layton <jlayton@kernel.org>\n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 00/61] vfs: change inode->i_ino from unsigned long to u64",
          "message_id": "2185009115e4c8efcb1c94866db4efec4fbcccbf.camel@kernel.org",
          "url": "https://lore.kernel.org/all/2185009115e4c8efcb1c94866db4efec4fbcccbf.camel@kernel.org/",
          "date": "2026-02-27T11:52:41Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch updates VFS-layer trace events to use 64-bit fields for inode numbers (i_ino) instead of the current unsigned long type. This change is necessary because the i_ino field will be converted to a u64 in future patches, and existing tracepoints need to accommodate this change. The update aims to improve reliability and consistency across different machine architectures, including 32-bit systems.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton (author)",
              "summary": "Author acknowledged reviewer's concern about reordering tracepoint fields for better packing and agreed to look into it, potentially addressing the issue in a future version of the patch.\n\nThe author is addressing a concern about tracepoint field sizes and packing, explaining that extending them to 64-bit fields is preferred over using the new kino_t typedef. The author acknowledges that reordering the tracepoint fields for better packing has material consequences and will look at it.\n\nAuthor acknowledged the need to reorder tracepoint fields for better packing, which has material consequences, and agreed to address this in a future patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged",
                "agreed",
                "acknowledged a concern",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Ok, I'll look at that. Given the number of places that need it though I\nmay do it in a separate patch.\n\n---\n\nNo, ino_t isn't. That's part of the ABI and has to remain unsigned\nlong. The point of this series is to make inode->i_ino a u64. Any event\nholding an ino_t today is going to need a 64-bit field to fully\ndescribe it.\n\nAnd to be clear, this should make things better for 32-bit boxes in the\nlong run. Once this change is done, i_ino should be a reliable source\nof info regardless of machine's word size.\n\nFor the tracepoints, I think it's best to just extend them to 64-bit\nfields outright rather than using the new (temporary) kino_t typedef\nthat I'm adding.\n\n---\n\nThanks for the review! I'll definitely look at reordering the\ntracepoint fields for better packing since that has material\nconsequences.\n-- \nJeff Layton <jlayton@kernel.org>\n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joanne Koong",
      "primary_email": "joannelkoong@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: Re: [PATCH v6 3/3] fuse: add an implementation of open+getattr",
          "message_id": "CAJnrk1ZQN6vGog2p_CsOh=C=O_jg6qHgXA0s4dKsgNbZycN2Cg@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1ZQN6vGog2p_CsOh=C=O_jg6qHgXA0s4dKsgNbZycN2Cg@mail.gmail.com/",
          "date": "2026-02-27T18:07:33Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Joanne suggested a more efficient way to handle compound open+getattr by checking if the attribute valid time is before the current time, and only sending the compound operation if true.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joanne Koong",
              "summary": "Identified a potential issue with the current implementation and suggested an alternative approach to improve efficiency.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: Re: [PATCH v6 3/3] fuse: add an implementation of open+getattr",
          "message_id": "CAJnrk1ZiKyi4jVN=mP2N-27nmcf929jsN7u6LhzdYePiEzJWaA@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1ZiKyi4jVN=mP2N-27nmcf929jsN7u6LhzdYePiEzJWaA@mail.gmail.com/",
          "date": "2026-02-27T17:51:58Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Joanne suggested a more efficient way to handle compound open+getattr by checking if the attribute valid time is before the current time, and only sending the compound operation if true.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joanne Koong",
              "summary": "Identified a potential issue with the current implementation and suggested an alternative approach to improve efficiency.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Fri, Feb 27, 2026 at 9:51AM Joanne Koong <joannelkoong@gmail.com> wrote:\n>\n> On Thu, Feb 26, 2026 at 11:48PM Horst Birthelmer <horst@birthelmer.de> wrote:\n> >\n> > On Thu, Feb 26, 2026 at 11:12:00AM -0800, Joanne Koong wrote:\n> > > On Thu, Feb 26, 2026 at 8:43AM Horst Birthelmer <horst@birthelmer.com> wrote:\n> > > >\n> > > > From: Horst Birthelmer <hbirthelmer@ddn.com>\n> > > >\n> > > > The discussion about compound commands in fuse was\n> > > > started over an argument to add a new operation that\n> > > > will open a file and return its attributes in the same operation.\n> > > >\n> > > > Here is a demonstration of that use case with compound commands.\n> > > >\n> > > > Signed-off-by: Horst Birthelmer <hbirthelmer@ddn.com>\n> > > > ---\n> > > >  fs/fuse/file.c   | 111 +++++++++++++++++++++++++++++++++++++++++++++++--------\n> > > >  fs/fuse/fuse_i.h |   4 +-\n> > > >  fs/fuse/ioctl.c  |   2 +-\n> > > >  3 files changed, 99 insertions(+), 18 deletions(-)\n> > > >\n> > > > diff --git a/fs/fuse/file.c b/fs/fuse/file.c\n> > > > index a408a9668abbb361e2c1e386ebab9dfcb0a7a573..daa95a640c311fc393241bdf727e00a2bc714f35 100644\n> > > > --- a/fs/fuse/file.c\n> > > > +++ b/fs/fuse/file.c\n> > > >  struct fuse_file *fuse_file_open(struct fuse_mount *fm, u64 nodeid,\n> > > > -                                unsigned int open_flags, bool isdir)\n> > > > +                               struct inode *inode,\n> > >\n> > > As I understand it, now every open() is a opengetattr() (except for\n> > > the ioctl path) but is this the desired behavior? for example if there\n> > > was a previous FUSE_LOOKUP that was just done, doesn't this mean\n> > > there's no getattr that's needed since the lookup refreshed the attrs?\n> > > or if the server has reasonable entry_valid and attr_valid timeouts,\n> > > multiple opens() of the same file would only need to send FUSE_OPEN\n> > > and not the FUSE_GETATTR, no?\n> >\n> > So your concern is, that we send too many requests?\n> > If the fuse server implwments the compound that is not the case.\n> >\n>\n> My concern is that we're adding unnecessary overhead for every open\n> when in most cases, the attributes are already uptodate. I don't think\n> we can assume that the server always has attributes locally cached, so\n> imo the extra getattr is nontrivial (eg might require having to\n> stat()).\n\nLooking at where the attribute valid time gets set... it looks like\nthis gets stored in fi->i_time (as per\nfuse_change_attributes_common()), so maybe it's better to only send\nthe compound open+getattr if time_before64(fi->i_time,\nget_jiffies_64()) is true, otherwise only the open is needed. This\ndoesn't solve the O_APPEND data corruption bug seen in [1] but imo\nthis would be a more preferable way of doing it.\n\nThanks,\nJoanne\n\n[1] https://lore.kernel.org/linux-fsdevel/20240813212149.1909627-1-joannelkoong@gmail.com/\n\n>\n> Thanks,\n> Joanne\n\n",
              "reply_to": "",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v1 03/11] io_uring/kbuf: add support for kernel-managed buffer rings",
          "message_id": "CAJnrk1YoaHnCmuwQra0XwOxf0aC_PQGby-DT1y_p=YRzotiE-w@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1YoaHnCmuwQra0XwOxf0aC_PQGby-DT1y_p=YRzotiE-w@mail.gmail.com/",
          "date": "2026-02-27T01:12:14Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch adds support for kernel-managed buffer rings in io_uring, but raises concerns about physical contiguity and allocation methods.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joanne Koong",
              "summary": "Raised concerns about physical contiguity and allocation methods for kernel-managed buffer rings, suggesting that per-buffer contiguity is necessary for efficient DMA paths. Also questioned the use of io_mem_alloc_compound() and alloc_pages_bulk_node().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "CONTROVERSIAL"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Responded to Joanne's concerns by suggesting extending the uapi for regions and using THP. Also emphasized the importance of not working around capabilities and mm policies.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On 2/27/26 01:12, Joanne Koong wrote:\n...\n>>> Regions shouldn't know anything about your buffers, how it's\n>>> subdivided after, etc.\n> \n> I still think the memory for the buffers should be tied to the ring\n> itself and allocated physically contiguously per buffer. Per-buffer\n> contiguity will enable the most efficient DMA path for servers to send\n> read/write data to local storage or the network. If the buffers for\n> the bufring have to be allocated as one single memory region, the\n> io_mem_alloc_compound() call will fail for this large allocation size.\n> Even if io_mem_alloc_compound() did succeed, this is a waste as the\n> buffer pool as an entity doesn't need to be physically contiguous,\n> just the individual buffers themselves. For fuse, the server\n> configures what buffer pool size it wants to use, depending on what\n> queue depth and max request size it needs. So for most use cases, at\n> least for high-performance servers, allocation will have to fall back\n> to alloc_pages_bulk_node(), which doesn't allocate contiguously. You\n> mentioned in an earlier comment that this \"only violates abstractions\"\n> - which abstractions does this break? The pre-existing behavior\n> already defaults to allocating pages non-contiguously if the mem\n> region can't be allocated fully contiguously.\n\nRegions has uapi (see struct io_uring_region_desc) so that users\ncan operate with them in a unified manner. If you want regions to\nbe allocated in some special way, just extend it.\n\n> Going through registered buffers doesn't help either. Fuse servers can\n> be unprivileged and it's not guaranteed that there are enough huge\n> pages reserved or that another process hasn't taken them or that the\n> server has privileges to pre-reserve pages for the allocation. Also\n\nThere is THP these days. And FWIW, we should be vigilant about not\nusing io_uring to work around capabilities and mm policies. If user\ncan't do it, io_uring shouldn't either. It's also all accounted\nagainst mlock, if the limit is not high enough, you won't be able\nto use this feature at all.\n\n> the 2 MB granularity is inflexible while 1 GB is too much.\n\n-- \nPavel Begunkov\n\n\n",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Johannes Weiner",
      "primary_email": "hannes@cmpxchg.org",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH] memcg: fix slab accounting in refill_obj_stock() trylock path",
          "message_id": "aaGTVWumz4jYEx9L@cmpxchg.org",
          "url": "https://lore.kernel.org/all/aaGTVWumz4jYEx9L@cmpxchg.org/",
          "date": "2026-02-27T12:51:38Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": []
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joshua Hahn",
      "primary_email": "joshua.hahnjy@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/8] mm/zswap, zsmalloc: Per-memcg-lruvec zswap accounting",
          "message_id": "20260226192936.3190275-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260226192936.3190275-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-26T19:29:40Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-26",
          "patch_summary": "This patch series introduces per-memcg-lruvec zswap accounting by adding a new array of objcg pointers to the zpdesc structure, allowing for accurate tracking of memory usage by zswap and reducing NR_ZSWAP. The changes also move memcg charges to the zsmalloc layer, which is the only user of zswap at this time.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "Raised concerns about the increased size of struct zpdesc and the potential impact on performance. Suggested exploring alternative solutions to achieve the desired accounting functionality without incurring significant overhead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "PERFORMANCE_CONCERN"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 8/8] mm/vmstat, memcontrol: Track ZSWAP_B, ZSWAPPED_B per-memcg-lruvec",
          "message_id": "20260227194538.928770-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260227194538.928770-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-27T19:45:41Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Patch author suggests making memcg charging happen unconditionally, checking for objcg presence within charging functions.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joshua Hahn",
              "summary": "Provided an alternative solution to the charging issue, making it unconditional and checking for objcg presence within charging functions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 5/8] mm/zsmalloc,zswap: Redirect zswap_entry->obcg to zpdesc",
          "message_id": "20260227191012.144117-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260227191012.144117-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-27T19:10:15Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Patch author suggests making memcg charging happen unconditionally, checking for objcg presence within charging functions.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joshua Hahn",
              "summary": "Provided an alternative solution to the charging issue, making it unconditional and checking for objcg presence within charging functions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "JP Kobryn",
      "primary_email": "inwardvessel@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Kiryl Shutsemau",
      "primary_email": "kas@kernel.org",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCHv7 18/18] mm/slab: Use compound_head() in page_slab()",
          "message_id": "20260227194302.274384-19-kas@kernel.org",
          "url": "https://lore.kernel.org/all/20260227194302.274384-19-kas@kernel.org/",
          "date": "2026-02-27T19:43:51Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series refactors the slab allocator to use compound_head() in page_slab(), replacing duplicated code and improving maintainability.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand (Arm)",
              "summary": "Acked-by: David Hildenbrand (Arm) <david@kernel.org>",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Vlastimil Babka",
              "summary": "No explicit review or comment from Vlastimil Babka.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCHv7 00/17] mm: Eliminate fake head pages from vmemmap optimization",
          "message_id": "20260202155634.650837-1-kas@kernel.org",
          "url": "https://lore.kernel.org/all/20260202155634.650837-1-kas@kernel.org/",
          "date": "2026-02-27T19:31:03Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series removes 'fake head pages' from the HugeTLB vmemmap optimization by changing how tail pages encode their relationship to the head page, simplifying compound_head() and page_ref_add_unless(). The new approach uses a mask-based encoding for architectures where sizeof(struct page) is a power of 2, allowing shared read-only tail pages across huge pages on a NUMA node. This reduces complexity and overhead in the hot path, but testing has shown either no change or only slight performance improvement.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author is addressing a concern about passing down the head and tail page indices, instead proposing to pass the tail and head pages directly along with their order in preparation for changing how the head position is encoded in the tail page.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "preparation",
                "change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n\nThis is preparation for adding the vmemmap_tails array to struct\npglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: David Hildenbrand (Red Hat) <david@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\nAcked-by: Muchun Song <muchun.song@linux.dev>\n---\n include/linux/mm.h     | 31 -------------------------------\n include/linux/mmzone.h | 31 +++++++++++++++++++++++++++++++\n 2 files changed, 31 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex f8a8fd47399c..8d5fa655fea4 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -27,7 +27,6 @@\n #include <linux/page-flags.h>\n #include <linux/page_ref.h>\n #include <linux/overflow.h>\n-#include <linux/sizes.h>\n #include <linux/sched.h>\n #include <linux/pgtable.h>\n #include <linux/kasan.h>\n@@ -2477,36 +2476,6 @@ static inline unsigned long folio_nr_pages(const struct folio *folio)\n \treturn folio_large_nr_pages(folio);\n }\n \n-#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n-/*\n- * We don't expect any folios that exceed buddy sizes (and consequently\n- * memory sections).\n- */\n-#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n-#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n-/*\n- * Only pages within a single memory section are guaranteed to be\n- * contiguous. By limiting folios to a single memory section, all folio\n- * pages are guaranteed to be contiguous.\n- */\n-#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n-#elif defined(CONFIG_HUGETLB_PAGE)\n-/*\n- * There is no real limit on the folio size. We limit them to the maximum we\n- * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n- * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n- */\n-#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n-#else\n-/*\n- * Without hugetlb, gigantic folios that are bigger than a single PUD are\n- * currently impossible.\n- */\n-#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n-#endif\n-\n-#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n-\n /*\n  * compound_nr() returns the number of pages in this potentially compound\n  * page.  compound_nr() can be called on a tail page, and is defined to\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 3e51190a55e4..be8ce40b5638 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -23,6 +23,7 @@\n #include <linux/page-flags.h>\n #include <linux/local_lock.h>\n #include <linux/zswap.h>\n+#include <linux/sizes.h>\n #include <asm/page.h>\n \n /* Free memory management - zoned buddy allocator.  */\n@@ -61,6 +62,36 @@\n  */\n #define PAGE_ALLOC_COSTLY_ORDER 3\n \n+#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n+/*\n+ * We don't expect any folios that exceed buddy sizes (and consequently\n+ * memory sections).\n+ */\n+#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n+#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n+/*\n+ * Only pages within a single memory section are guaranteed to be\n+ * contiguous. By limiting folios to a single memory section, all folio\n+ * pages are guaranteed to be contiguous.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n+#elif defined(CONFIG_HUGETLB_PAGE)\n+/*\n+ * There is no real limit on the folio size. We limit them to the maximum we\n+ * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n+ * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n+ */\n+#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n+#else\n+/*\n+ * Without hugetlb, gigantic folios that are bigger than a single PUD are\n+ * currently impossible.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n+#endif\n+\n+#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n+\n enum migratetype {\n \tMIGRATE_UNMOVABLE,\n \tMIGRATE_MOVABLE,\n-- \n2.51.2\n\n---\n\nInstead of passing down the head page and tail page index, pass the tail\nand head pages directly, as well as the order of the compound page.\n\nThis is a preparation for changing how the head position is encoded in\nthe tail page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h |  4 +++-\n mm/hugetlb.c               |  8 +++++---\n mm/internal.h              | 12 ++++++------\n mm/mm_init.c               |  2 +-\n mm/page_alloc.c            |  2 +-\n 5 files changed, 16 insertions(+), 12 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex f7a0e4af0c73..8a3694369e15 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page, struct page *head)\n+static __always_inline void set_compound_head(struct page *page,\n+\t\t\t\t\t      const struct page *head,\n+\t\t\t\t\t      unsigned int order)\n {\n \tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n }\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 6e855a32de3d..54ba7cd05a86 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -3168,6 +3168,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)\n \n /* Initialize [start_page:end_page_number] tail struct pages of a hugepage */\n static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n+\t\t\t\t\tstruct hstate *h,\n \t\t\t\t\tunsigned long start_page_number,\n \t\t\t\t\tunsigned long end_page_number)\n {\n@@ -3176,6 +3177,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \tstruct page *page = folio_page(folio, start_page_number);\n \tunsigned long head_pfn = folio_pfn(folio);\n \tunsigned long pfn, end_pfn = head_pfn + end_page_number;\n+\tunsigned int order = huge_page_order(h);\n \n \t/*\n \t * As we marked all tail pages with memblock_reserved_mark_noinit(),\n@@ -3183,7 +3185,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \t */\n \tfor (pfn = head_pfn + start_page_number; pfn < end_pfn; page++, pfn++) {\n \t\t__init_single_page(page, pfn, zone, nid);\n-\t\tprep_compound_tail((struct page *)folio, pfn - head_pfn);\n+\t\tprep_compound_tail(page, &folio->page, order);\n \t\tset_page_count(page, 0);\n \t}\n }\n@@ -3203,7 +3205,7 @@ static void __init hugetlb_folio_init_vmemmap(struct folio *folio,\n \t__folio_set_head(folio);\n \tret = folio_ref_freeze(folio, 1);\n \tVM_BUG_ON(!ret);\n-\thugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages);\n+\thugetlb_folio_init_tail_vmemmap(folio, h, 1, nr_pages);\n \tprep_compound_head(&folio->page, huge_page_order(h));\n }\n \n@@ -3260,7 +3262,7 @@ static void __init prep_and_add_bootmem_folios(struct hstate *h,\n \t\t\t * time as this is early in boot and there should\n \t\t\t * be no contention.\n \t\t\t */\n-\t\t\thugetlb_folio_init_tail_vmemmap(folio,\n+\t\t\thugetlb_folio_init_tail_vmemmap(folio, h,\n \t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_PAGES,\n \t\t\t\t\tpages_per_huge_page(h));\n \t\t}\ndiff --git a/mm/internal.h b/mm/internal.h\nindex d67e8bb75734..037ddcda25ff 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n }\n \n-static inline void prep_compound_tail(struct page *head, int tail_idx)\n+static inline void prep_compound_tail(struct page *tail,\n+\t\t\t\t      const struct page *head,\n+\t\t\t\t      unsigned int order)\n {\n-\tstruct page *p = head + tail_idx;\n-\n-\tp->mapping = TAIL_MAPPING;\n-\tset_compound_head(p, head);\n-\tset_page_private(p, 0);\n+\ttail->mapping = TAIL_MAPPING;\n+\tset_compound_head(tail, head, order);\n+\tset_page_private(tail, 0);\n }\n \n void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..ba50f4c4337b 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -1099,7 +1099,7 @@ static void __ref memmap_init_compound(struct page *head,\n \t\tstruct page *page = pfn_to_page(pfn);\n \n \t\t__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);\n-\t\tprep_compound_tail(head, pfn - head_pfn);\n+\t\tprep_compound_tail(page, head, order);\n \t\tset_page_count(page, 0);\n \t}\n \tprep_compound_head(head, order);\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..00c7ea958767 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -744,7 +744,7 @@ void prep_compound_page(struct page *page, unsigned int order)\n \n \t__SetPageHead(page);\n \tfor (i = 1; i < nr_pages; i++)\n-\t\tprep_compound_tail(page, i);\n+\t\tprep_compound_tail(page + i, page, order);\n \n \tprep_compound_head(page, order);\n }\n-- \n2.51.2\n\n---\n\nThe 'compound_head' field in the 'struct page' encodes whether the page\nis a tail and where to locate the head page. Bit 0 is set if the page is\na tail, and the remaining bits in the field point to the head page.\n\nAs preparation for changing how the field encodes information about the\nhead page, rename the field to 'compound_info'.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n .../admin-guide/kdump/vmcoreinfo.rst          |  2 +-\n Documentation/mm/vmemmap_dedup.rst            |  6 +++---\n include/linux/mm_types.h                      | 20 +++++++++----------\n include/linux/page-flags.h                    | 18 ++++++++---------\n include/linux/types.h                         |  2 +-\n kernel/vmcore_info.c                          |  2 +-\n mm/page_alloc.c                               |  2 +-\n mm/slab.h                                     |  2 +-\n mm/util.c                                     |  2 +-\n 9 files changed, 28 insertions(+), 28 deletions(-)\n\ndiff --git a/Documentation/admin-guide/kdump/vmcoreinfo.rst b/Documentation/admin-guide/kdump/vmcoreinfo.rst\nindex 404a15f6782c..7663c610fe90 100644\n--- a/Documentation/admin-guide/kdump/vmcoreinfo.rst\n+++ b/Documentation/admin-guide/kdump/vmcoreinfo.rst\n@@ -141,7 +141,7 @@ nodemask_t\n The size of a nodemask_t type. Used to compute the number of online\n nodes.\n \n-(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_head)\n+(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_info)\n ----------------------------------------------------------------------------------\n \n User-space tools compute their values based on the offset of these\ndiff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst\nindex b4a55b6569fa..1863d88d2dcb 100644\n--- a/Documentation/mm/vmemmap_dedup.rst\n+++ b/Documentation/mm/vmemmap_dedup.rst\n@@ -24,7 +24,7 @@ For each base page, there is a corresponding ``struct page``.\n Within the HugeTLB subsystem, only the first 4 ``struct page`` are used to\n contain unique information about a HugeTLB page. ``__NR_USED_SUBPAGE`` provides\n this upper limit. The only 'useful' information in the remaining ``struct page``\n-is the compound_head field, and this field is the same for all tail pages.\n+is the compound_info field, and this field is the same for all tail pages.\n \n By removing redundant ``struct page`` for HugeTLB pages, memory can be returned\n to the buddy allocator for other uses.\n@@ -124,10 +124,10 @@ Here is how things look before optimization::\n  |           |\n  +-----------+\n \n-The value of page->compound_head is the same for all tail pages. The first\n+The value of page->compound_info is the same for all tail pages. The first\n page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4\n ``struct page`` necessary to describe the HugeTLB. The only use of the remaining\n-pages of ``struct page`` (page 1 to page 7) is to point to page->compound_head.\n+pages of ``struct page`` (page 1 to page 7) is to point to page->compound_info.\n Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``\n will be used for each HugeTLB page. This will allow us to free the remaining\n 7 pages to the buddy allocator.\ndiff --git a/include/linux/mm_types.h b/include/linux/mm_types.h\nindex 3cc8ae722886..7bc82a2b889f 100644\n--- a/include/linux/mm_types.h\n+++ b/include/linux/mm_types.h\n@@ -126,14 +126,14 @@ struct page {\n \t\t\tatomic_long_t pp_ref_count;\n \t\t};\n \t\tstruct {\t/* Tail pages of compound page */\n-\t\t\tunsigned long compound_head;\t/* Bit zero is set */\n+\t\t\tunsigned long compound_info;\t/* Bit zero is set */\n \t\t};\n \t\tstruct {\t/* ZONE_DEVICE pages */\n \t\t\t/*\n-\t\t\t * The first word is used for compound_head or folio\n+\t\t\t * The first word is used for compound_info or folio\n \t\t\t * pgmap\n \t\t\t */\n-\t\t\tvoid *_unused_pgmap_compound_head;\n+\t\t\tvoid *_unused_pgmap_compound_info;\n \t\t\tvoid *zone_device_data;\n \t\t\t/*\n \t\t\t * ZONE_DEVICE private pages are counted as being\n@@ -409,7 +409,7 @@ struct folio {\n \t/* private: avoid cluttering the output */\n \t\t\t\t/* For the Unevictable \"LRU list\" slot */\n \t\t\t\tstruct {\n-\t\t\t\t\t/* Avoid compound_head */\n+\t\t\t\t\t/* Avoid compound_info */\n \t\t\t\t\tvoid *__filler;\n \t/* public: */\n \t\t\t\t\tunsigned int mlock_count;\n@@ -510,7 +510,7 @@ struct folio {\n FOLIO_MATCH(flags, flags);\n FOLIO_MATCH(lru, lru);\n FOLIO_MATCH(mapping, mapping);\n-FOLIO_MATCH(compound_head, lru);\n+FOLIO_MATCH(compound_info, lru);\n FOLIO_MATCH(__folio_index, index);\n FOLIO_MATCH(private, private);\n FOLIO_MATCH(_mapcount, _mapcount);\n@@ -529,7 +529,7 @@ FOLIO_MATCH(_last_cpupid, _last_cpupid);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + sizeof(struct page))\n FOLIO_MATCH(flags, _flags_1);\n-FOLIO_MATCH(compound_head, _head_1);\n+FOLIO_MATCH(compound_info, _head_1);\n FOLIO_MATCH(_mapcount, _mapcount_1);\n FOLIO_MATCH(_refcount, _refcount_1);\n #undef FOLIO_MATCH\n@@ -537,13 +537,13 @@ FOLIO_MATCH(_refcount, _refcount_1);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 2 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_2);\n-FOLIO_MATCH(compound_head, _head_2);\n+FOLIO_MATCH(compound_info, _head_2);\n #undef FOLIO_MATCH\n #define FOLIO_MATCH(pg, fl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 3 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_3);\n-FOLIO_MATCH(compound_head, _head_3);\n+FOLIO_MATCH(compound_info, _head_3);\n #undef FOLIO_MATCH\n \n /**\n@@ -609,8 +609,8 @@ struct ptdesc {\n #define TABLE_MATCH(pg, pt)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))\n TABLE_MATCH(flags, pt_flags);\n-TABLE_MATCH(compound_head, pt_list);\n-TABLE_MATCH(compound_head, _pt_pad_1);\n+TABLE_MATCH(compound_info, pt_list);\n+TABLE_MATCH(compound_info, _pt_pad_1);\n TABLE_MATCH(mapping, __page_mapping);\n TABLE_MATCH(__folio_index, pt_index);\n TABLE_MATCH(rcu_head, pt_rcu_head);\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 8a3694369e15..aa46d49e82f7 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -213,7 +213,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n-\t * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)\n+\t * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)\n \t * cold cacheline in some cases.\n \t */\n \tif (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &&\n@@ -223,7 +223,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_head);\n+\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n \n \t\tif (likely(head & 1))\n \t\t\treturn (const struct page *)(head - 1);\n@@ -281,7 +281,7 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_head);\n+\tunsigned long head = READ_ONCE(page->compound_info);\n \n \tif (unlikely(head & 1))\n \t\treturn head - 1;\n@@ -320,13 +320,13 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n static __always_inline int PageTail(const struct page *page)\n {\n-\treturn READ_ONCE(page->compound_head) & 1 || page_is_fake_head(page);\n+\treturn READ_ONCE(page->compound_info) & 1 || page_is_fake_head(page);\n }\n \n static __always_inline int PageCompound(const struct page *page)\n {\n \treturn test_bit(PG_head, &page->flags.f) ||\n-\t       READ_ONCE(page->compound_head) & 1;\n+\t       READ_ONCE(page->compound_info) & 1;\n }\n \n #define\tPAGE_POISON_PATTERN\t-1l\n@@ -348,7 +348,7 @@ static const unsigned long *const_folio_flags(const struct folio *folio,\n {\n \tconst struct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -357,7 +357,7 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)\n {\n \tstruct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -869,12 +869,12 @@ static __always_inline void set_compound_head(struct page *page,\n \t\t\t\t\t      const struct page *head,\n \t\t\t\t\t      unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n+\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\n {\n-\tWRITE_ONCE(page->compound_head, 0);\n+\tWRITE_ONCE(page->compound_info, 0);\n }\n \n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\ndiff --git a/include/linux/types.h b/include/linux/types.h\nindex f69be881369f..604697abf151 100644\n--- a/include/linux/types.h\n+++ b/include/linux/types.h\n@@ -234,7 +234,7 @@ struct ustat {\n  *\n  * This guarantee is important for few reasons:\n  *  - future call_rcu_lazy() will make use of lower bits in the pointer;\n- *  - the structure shares storage space in struct page with @compound_head,\n+ *  - the structure shares storage space in struct page with @compound_info,\n  *    which encode PageTail() in bit 0. The guarantee is needed to avoid\n  *    false-positive PageTail().\n  */\ndiff --git a/kernel/vmcore_info.c b/kernel/vmcore_info.c\nindex 46198580373a..0a46df3e3db9 100644\n--- a/kernel/vmcore_info.c\n+++ b/kernel/vmcore_info.c\n@@ -198,7 +198,7 @@ static int __init crash_save_vmcoreinfo_init(void)\n \tVMCOREINFO_OFFSET(page, lru);\n \tVMCOREINFO_OFFSET(page, _mapcount);\n \tVMCOREINFO_OFFSET(page, private);\n-\tVMCOREINFO_OFFSET(page, compound_head);\n+\tVMCOREINFO_OFFSET(page, compound_info);\n \tVMCOREINFO_OFFSET(pglist_data, node_zones);\n \tVMCOREINFO_OFFSET(pglist_data, nr_zones);\n #ifdef CONFIG_FLATMEM\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 00c7ea958767..cb7375eb1713 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -731,7 +731,7 @@ static inline bool pcp_allowed_order(unsigned int order)\n  * The first PAGE_SIZE page is called the \"head page\" and have PG_head set.\n  *\n  * The remaining PAGE_SIZE pages are called \"tail pages\". PageTail() is encoded\n- * in bit 0 of page->compound_head. The rest of bits is pointer to head page.\n+ * in bit 0 of page->compound_info. The rest of bits is pointer to head page.\n  *\n  * The first tail page's ->compound_order holds the order of allocation.\n  * This usage means that zero-order pages may not be compound.\ndiff --git a/mm/slab.h b/mm/slab.h\nindex e767aa7e91b0..8a2a9c6c697b 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -100,7 +100,7 @@ struct slab {\n #define SLAB_MATCH(pg, sl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))\n SLAB_MATCH(flags, flags);\n-SLAB_MATCH(compound_head, slab_cache);\t/* Ensure bit 0 is clear */\n+SLAB_MATCH(compound_info, slab_cache);\t/* Ensure bit 0 is clear */\n SLAB_MATCH(_refcount, __page_refcount);\n #ifdef CONFIG_MEMCG\n SLAB_MATCH(memcg_data, obj_exts);\ndiff --git a/mm/util.c b/mm/util.c\nindex b05ab6f97e11..3ebcb9e6035c 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1247,7 +1247,7 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_head;\n+\thead = ps->page_snapshot.compound_info;\n \tif ((head & 1) == 0) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n-- \n2.51.2\n\n---\n\nMove set_compound_head() and clear_compound_head() to be adjacent to the\ncompound_head() function in page-flags.h.\n\nThese functions encode and decode the same compound_info field, so\nkeeping them together makes it easier to verify their logic is\nconsistent, especially when the encoding changes.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h | 24 ++++++++++++------------\n 1 file changed, 12 insertions(+), 12 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex aa46d49e82f7..d14a17ffb55b 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -290,6 +290,18 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n \n+static __always_inline void set_compound_head(struct page *page,\n+\t\t\t\t\t      const struct page *head,\n+\t\t\t\t\t      unsigned int order)\n+{\n+\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n+}\n+\n+static __always_inline void clear_compound_head(struct page *page)\n+{\n+\tWRITE_ONCE(page->compound_info, 0);\n+}\n+\n /**\n  * page_folio - Converts from page to folio.\n  * @p: The page.\n@@ -865,18 +877,6 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page,\n-\t\t\t\t\t      const struct page *head,\n-\t\t\t\t\t      unsigned int order)\n-{\n-\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n-}\n-\n-static __always_inline void clear_compound_head(struct page *page)\n-{\n-\tWRITE_ONCE(page->compound_info, 0);\n-}\n-\n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n static inline void ClearPageCompound(struct page *page)\n {\n-- \n2.51.2\n\n---\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_NR_PAGES.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/riscv/mm/init.c | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c\nindex 21d534824624..c555b9a4fdce 100644\n--- a/arch/riscv/mm/init.c\n+++ b/arch/riscv/mm/init.c\n@@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;\n EXPORT_SYMBOL(phys_ram_base);\n \n #ifdef CONFIG_SPARSEMEM_VMEMMAP\n-#define VMEMMAP_ADDR_ALIGN\t(1ULL << SECTION_SIZE_BITS)\n+#define VMEMMAP_ADDR_ALIGN\tmax(1ULL << SECTION_SIZE_BITS, \\\n+\t\t\t\t    MAX_FOLIO_NR_PAGES * sizeof(struct page))\n \n unsigned long vmemmap_start_pfn __ro_after_init;\n EXPORT_SYMBOL(vmemmap_start_pfn);\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-02-02",
              "message_id": "20260202155634.650837-2-kas@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author addressed a concern about the alignment of struct pages in HugeTLB vmemmap optimization, explained that for cases where sizeof(struct page) is power-of-2, they plan to change the encoding of compound_info to store a mask that can be applied to the virtual address of the tail page to access the head page. This modification will allow all tail pages of the same order to have identical 'compound_info', regardless of the compound page they are associated with, paving the way for eliminating fake heads.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_NR_PAGES.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/loongarch/include/asm/pgtable.h | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\nindex c33b3bcb733e..f9416acb9156 100644\n--- a/arch/loongarch/include/asm/pgtable.h\n+++ b/arch/loongarch/include/asm/pgtable.h\n@@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n #endif\n \n-#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n+#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n+#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n #define VMEMMAP_END\t((unsigned long)vmemmap + VMEMMAP_SIZE - 1)\n \n #define KFENCE_AREA_START\t(VMEMMAP_END + 1)\n-- \n2.51.2\n\n---\n\nFor tail pages, the kernel uses the 'compound_info' field to get to the\nhead page. The bit 0 of the field indicates whether the page is a\ntail page, and if set, the remaining bits represent a pointer to the\nhead page.\n\nFor cases when size of struct page is power-of-2, change the encoding of\ncompound_info to store a mask that can be applied to the virtual address\nof the tail page in order to access the head page. It is possible\nbecause struct page of the head page is naturally aligned with regards\nto order of the page.\n\nThe significant impact of this modification is that all tail pages of\nthe same order will now have identical 'compound_info', regardless of\nthe compound page they are associated with. This paves the way for\neliminating fake heads.\n\nThe HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\napplied when the sizeof(struct page) is power-of-2. Having identical\ntail pages allows the same page to be mapped into the vmemmap of all\npages, maintaining memory savings without fake heads.\n\nIf sizeof(struct page) is not power-of-2, there is no functional\nchanges.\n\nLimit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\na difference. The approach with mask would work in the wider set of\nconditions, but it requires validating that struct pages are naturally\naligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n mm/slab.h                  | 16 ++++++--\n mm/util.c                  | 16 ++++++--\n 3 files changed, 97 insertions(+), 16 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex d14a17ffb55b..8f2c7fbc739b 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -198,6 +198,29 @@ enum pageflags {\n \n #ifndef __GENERATING_BOUNDS_H\n \n+/*\n+ * For tail pages, if the size of struct page is power-of-2 ->compound_info\n+ * encodes the mask that converts the address of the tail page address to\n+ * the head page address.\n+ *\n+ * Otherwise, ->compound_info has direct pointer to head pages.\n+ */\n+static __always_inline bool compound_info_has_mask(void)\n+{\n+\t/*\n+\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n+\t * makes a difference.\n+\t *\n+\t * The approach with mask would work in the wider set of conditions,\n+\t * but it requires validating that struct pages are naturally aligned\n+\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n+\t */\n+\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n+\t\treturn false;\n+\n+\treturn is_power_of_2(sizeof(struct page));\n+}\n+\n #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n \n@@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n \t\treturn page;\n \n+\t/* Fake heads only exists if compound_info_has_mask() is true */\n+\tif (!compound_info_has_mask())\n+\t\treturn page;\n+\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n@@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n+\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n \n-\t\tif (likely(head & 1))\n-\t\t\treturn (const struct page *)(head - 1);\n+\t\t/* See set_compound_head() */\n+\t\tif (likely(info & 1)) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\treturn (const struct page *)(p & info);\n+\t\t}\n \t}\n \treturn page;\n }\n@@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_info);\n+\tunsigned long info = READ_ONCE(page->compound_info);\n \n-\tif (unlikely(head & 1))\n-\t\treturn head - 1;\n-\treturn (unsigned long)page_fixed_fake_head(page);\n+\t/* Bit 0 encodes PageTail() */\n+\tif (!(info & 1))\n+\t\treturn (unsigned long)page_fixed_fake_head(page);\n+\n+\t/*\n+\t * If compound_info_has_mask() is false, the rest of compound_info is\n+\t * the pointer to the head page.\n+\t */\n+\tif (!compound_info_has_mask())\n+\t\treturn info - 1;\n+\n+\t/*\n+\t * If compoun_info_has_mask() is true the rest of the info encodes\n+\t * the mask that converts the address of the tail page to the head page.\n+\t *\n+\t * No need to clear bit 0 in the mask as 'page' always has it clear.\n+\t */\n+\treturn (unsigned long)page & info;\n }\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n@@ -294,7 +340,26 @@ static __always_inline void set_compound_head(struct page *page,\n \t\t\t\t\t      const struct page *head,\n \t\t\t\t\t      unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n+\tunsigned int shift;\n+\tunsigned long mask;\n+\n+\tif (!compound_info_has_mask()) {\n+\t\tWRITE_ONCE(page->compound_info, (unsigned long)head | 1);\n+\t\treturn;\n+\t}\n+\n+\t/*\n+\t * If the size of struct page is power-of-2, bits [shift:0] of the\n+\t * virtual address of compound head are zero.\n+\t *\n+\t * Calculate mask that can be applied to the virtual address of\n+\t * the tail page to get address of the head page.\n+\t */\n+\tshift = order + order_base_2(sizeof(struct page));\n+\tmask = GENMASK(BITS_PER_LONG - 1, shift);\n+\n+\t/* Bit 0 encodes PageTail() */\n+\tWRITE_ONCE(page->compound_info, mask | 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\ndiff --git a/mm/slab.h b/mm/slab.h\nindex 8a2a9c6c697b..f68c3ac8126f 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -137,11 +137,19 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist\n  */\n static inline struct slab *page_slab(const struct page *page)\n {\n-\tunsigned long head;\n+\tunsigned long info;\n+\n+\tinfo = READ_ONCE(page->compound_info);\n+\tif (info & 1) {\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\t\t\tpage = (struct page *)(p & info);\n+\t\t} else {\n+\t\t\tpage = (struct page *)(info - 1);\n+\t\t}\n+\t}\n \n-\thead = READ_ONCE(page->compound_head);\n-\tif (head & 1)\n-\t\tpage = (struct page *)(head - 1);\n \tif (data_race(page->page_type >> 24) != PGTY_slab)\n \t\tpage = NULL;\n \ndiff --git a/mm/util.c b/mm/util.c\nindex 3ebcb9e6035c..20dccf2881d7 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1237,7 +1237,7 @@ static void set_ps_flags(struct page_snapshot *ps, const struct folio *folio,\n  */\n void snapshot_page(struct page_snapshot *ps, const struct page *page)\n {\n-\tunsigned long head, nr_pages = 1;\n+\tunsigned long info, nr_pages = 1;\n \tstruct folio *foliop;\n \tint loops = 5;\n \n@@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_info;\n-\tif ((head & 1) == 0) {\n+\tinfo = ps->page_snapshot.compound_info;\n+\tif (!(info & 1)) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n \t\tif (!folio_test_large(foliop)) {\n@@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n \t\t}\n \t\tfoliop = (struct folio *)page;\n \t} else {\n-\t\tfoliop = (struct folio *)(head - 1);\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\tfoliop = (struct folio *)(p & info);\n+\t\t} else {\n+\t\t\tfoliop = (struct folio *)(info - 1);\n+\t\t}\n+\n \t\tps->idx = folio_page_idx(foliop, page);\n \t}\n \n-- \n2.51.2\n\n---\n\nWith the upcoming changes to HVO, a single page of tail struct pages\nwill be shared across all huge pages of the same order on a node. Since\nhuge pages on the same node may belong to different zones, the zone\ninformation stored in shared tail page flags would be incorrect.\n\nAlways fetch zone information from the head page, which has unique and\ncorrect zone flags for each compound page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/mmzone.h | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex be8ce40b5638..192143b5cdc0 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n \n static inline enum zone_type page_zonenum(const struct page *page)\n {\n+\tpage = compound_head(page);\n \treturn memdesc_zonenum(page->flags);\n }\n \n-- \n2.51.2\n\n---\n\nIf page->compound_info encodes a mask, it is expected that vmemmap to be\nnaturally aligned to the maximum folio size.\n\nAdd a VM_BUG_ON() to check the alignment.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n mm/sparse.c | 7 +++++++\n 1 file changed, 7 insertions(+)\n\ndiff --git a/mm/sparse.c b/mm/sparse.c\nindex b5b2b6f7041b..6c9b62607f3f 100644\n--- a/mm/sparse.c\n+++ b/mm/sparse.c\n@@ -600,6 +600,13 @@ void __init sparse_init(void)\n \tBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n \tmemblocks_present();\n \n+\tif (compound_info_has_mask()) {\n+\t\tunsigned long alignment;\n+\n+\t\talignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);\n+\t\tVM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), alignment));\n+\t}\n+\n \tpnum_begin = first_present_section_nr();\n \tnid_begin = sparse_early_nid(__nr_to_section(pnum_begin));\n \n-- \n2.51.2\n\n---\n\nThis series removes \"fake head pages\" from the HugeTLB vmemmap\noptimization (HVO) by changing how tail pages encode their relationship\nto the head page.\n\nIt simplifies compound_head() and page_ref_add_unless(). Both are in the\nhot path.\n\nBackground\n==========\n\nHVO reduces memory overhead by freeing vmemmap pages for HugeTLB pages\nand remapping the freed virtual addresses to a single physical page.\nPreviously, all tail page vmemmap entries were remapped to the first\nvmemmap page (containing the head struct page), creating \"fake heads\" -\ntail pages that appear to have PG_head set when accessed through the\ndeduplicated vmemmap.\n\nThis required special handling in compound_head() to detect and work\naround fake heads, adding complexity and overhead to a very hot path.\n\nNew Approach\n============\n\nFor architectures/configs where sizeof(struct page) is a power of 2 (the\ncommon case), this series changes how position of the head page is encoded\nin the tail pages.\n\nInstead of storing a pointer to the head page, the ->compound_info\n(renamed from ->compound_head) now stores a mask.\n\nThe mask can be applied to any tail page's virtual address to compute\nthe head page address. Critically, all tail pages of the same order now\nhave identical compound_info values, regardless of which compound page\nthey belong to.\n\nThe key insight is that all tail pages of the same order now have\nidentical compound_info values, regardless of which compound page they\nbelong to. This allows a single page of tail struct pages to be shared\nacross all huge pages of the same order on a NUMA node.\n\nBenefits\n========\n\n1. Simplified compound_head(): No fake head detection needed, can be\n   implemented in a branchless manner.\n\n2. Simplified page_ref_add_unless(): RCU protection removed since there's\n   no race with fake head remapping.\n\n3. Cleaner architecture: The shared tail pages are truly read-only and\n   contain valid tail page metadata.\n\nIf sizeof(struct page) is not power-of-2, there are no functional changes.\nHVO is not supported in this configuration.\n\nI had hoped to see performance improvement, but my testing thus far has\nshown either no change or only a slight improvement within the noise.\n\nSeries Organization\n===================\n\nPatch 1: Preparation - move MAX_FOLIO_ORDER to mmzone.h\nPatches 2-4: Refactoring - interface changes, field rename, code movement\nPatches 5-6: Arch fixes - align vmemmap for riscv and LoongArch\nPatch 7: Core change - new mask-based compound_head() encoding\nPatch 8: Correctness fix - page_zonenum() must use head page\nPatch 9: Add memmap alignment check for compound_info_has_mask()\nPatch 10: Refactor vmemmap_walk for new design\nPatch 11: Eliminate fake heads with shared tail pages\nPatches 12-15: Cleanup - remove fake head infrastructure\nPatch 16: Documentation update\nPatch 17: Get rid of opencoded compound_head() in page_slab()\n\nChanges in v6:\n==============\n  - Simplify memmap alignment check in mm/sparse.c: use VM_BUG_ON()\n    (Muchun)\n\n  - Store struct page pointers in vmemmap_tails[] instead of PFNs.\n    (Muchun)\n\n  - Fix build error on powerpc due to negative NR_VMEMMAP_TAILS.\n\nChanges in v5:\n==============\n  - Rebased to mm-everything-2026-01-27-04-35\n\n  - Add arch-specific patches to align vmemmap to maximal folio size\n    for riscv and LoongArch architectures.\n\n  - Strengthen the memmap alignment check in mm/sparse.c: use BUG()\n    for CONFIG_DEBUG_VM, WARN() otherwise. (Muchun)\n\n  - Use cmpxchg() instead of hugetlb_lock to update vmemmap_tails\n    array. (Muchun)\n\n  - Update page_slab().\n\nChanges in v4:\n==============\n  - Fix build issues due to linux/mmzone.h <-> linux/pgtable.h\n    dependency loop by avoiding including linux/pgtable.h into\n    linux/mmzone.h\n\n  - Rework vmemmap_remap_alloc() interface. (Muchun)\n\n  - Use &folio->page instead of folio address for optimization\n    target. (Muchun)\n\nChanges in v3:\n==============\n  - Fixed error recovery path in vmemmap_remap_free() to pass correct start\n    address for TLB flush. (Muchun)\n\n  - Wrapped the mask-based compound_info encoding within CONFIG_SPARSEMEM_VMEMMAP\n    check via compound_info_has_mask(). For other memory models, alignment\n    guarantees are harder to verify. (Muchun)\n\n  - Updated vmemmap_dedup.rst documentation wording: changed \"vmemmap_tail\n    shared for the struct hstate\" to \"A single, per-node page frame shared\n    among all hugepages of the same size\". (Muchun)\n\n  - Fixed build error with MAX_FOLIO_ORDER expanding to undefined PUD_ORDER\n    in certain configurations. (kernel test robot)\n\nChanges in v2:\n==============\n\n- Handle boot-allocated huge pages correctly. (Frank)\n\n- Changed from per-hstate vmemmap_tail to per-node vmemmap_tails[] array\n  in pglist_data. (Muchun)\n\n- Added spin_lock(&hugetlb_lock) protection in vmemmap_get_tail() to fix\n  a race condition where two threads could both allocate tail pages.\n  The losing thread now properly frees its allocated page. (Usama)\n\n- Add warning if memmap is not aligned to MAX_FOLIO_SIZE, which is\n  required for the mask approach. (Muchun)\n\n- Make page_zonenum() use head page - correctness fix since shared\n  tail pages cannot have valid zone information. (Muchun)\n\n- Added 'const' qualifier to head parameter in set_compound_head() and\n  prep_compound_tail(). (Usama)\n\n- Updated commit messages.\n\nKiryl Shutsemau (17):\n  mm: Move MAX_FOLIO_ORDER definition to mmzone.h\n  mm: Change the interface of prep_compound_tail()\n  mm: Rename the 'compound_head' field in the 'struct page' to\n    'compound_info'\n  mm: Move set/clear_compound_head() next to compound_head()\n  riscv/mm: Align vmemmap to maximal folio size\n  LoongArch/mm: Align vmemmap to maximal folio size\n  mm: Rework compound_head() for power-of-2 sizeof(struct page)\n  mm: Make page_zonenum() use head page\n  mm/sparse: Check memmap alignment for compound_info_has_mask()\n  mm/hugetlb: Refactor code around vmemmap_walk\n  mm/hugetlb: Remove fake head pages\n  mm: Drop fake head checks\n  hugetlb: Remove VMEMMAP_SYNCHRONIZE_RCU\n  mm/hugetlb: Remove hugetlb_optimize_vmemmap_key static key\n  mm: Remove the branch from compound_head()\n  hugetlb: Update vmemmap_dedup.rst\n  mm/slab: Use compound_head() in page_slab()\n\n .../admin-guide/kdump/vmcoreinfo.rst          |   2 +-\n Documentation/mm/vmemmap_dedup.rst            |  62 ++--\n arch/loongarch/include/asm/pgtable.h          |   3 +-\n arch/riscv/mm/init.c                          |   3 +-\n include/linux/mm.h                            |  31 --\n include/linux/mm_types.h                      |  20 +-\n include/linux/mmzone.h                        |  47 +++\n include/linux/page-flags.h                    | 167 +++++-----\n include/linux/page_ref.h                      |   8 +-\n include/linux/types.h                         |   2 +-\n kernel/vmcore_info.c                          |   2 +-\n mm/hugetlb.c                                  |   8 +-\n mm/hugetlb_vmemmap.c                          | 288 ++++++++----------\n mm/internal.h                                 |  12 +-\n mm/mm_init.c                                  |   2 +-\n mm/page_alloc.c                               |   4 +-\n mm/slab.h                                     |   8 +-\n mm/sparse-vmemmap.c                           |  43 ++-\n mm/sparse.c                                   |   7 +\n mm/util.c                                     |  16 +-\n 20 files changed, 363 insertions(+), 372 deletions(-)\n\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nMove MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n\nThis is preparation for adding the vmemmap_tails array to struct\npglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: David Hildenbrand (Red Hat) <david@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\nAcked-by: Muchun Song <muchun.song@linux.dev>\n---\n include/linux/mm.h     | 31 -------------------------------\n include/linux/mmzone.h | 31 +++++++++++++++++++++++++++++++\n 2 files changed, 31 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex f8a8fd47399c..8d5fa655fea4 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -27,7 +27,6 @@\n #include <linux/page-flags.h>\n #include <linux/page_ref.h>\n #include <linux/overflow.h>\n-#include <linux/sizes.h>\n #include <linux/sched.h>\n #include <linux/pgtable.h>\n #include <linux/kasan.h>\n@@ -2477,36 +2476,6 @@ static inline unsigned long folio_nr_pages(const struct folio *folio)\n \treturn folio_large_nr_pages(folio);\n }\n \n-#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n-/*\n- * We don't expect any folios that exceed buddy sizes (and consequently\n- * memory sections).\n- */\n-#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n-#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n-/*\n- * Only pages within a single memory section are guaranteed to be\n- * contiguous. By limiting folios to a single memory section, all folio\n- * pages are guaranteed to be contiguous.\n- */\n-#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n-#elif defined(CONFIG_HUGETLB_PAGE)\n-/*\n- * There is no real limit on the folio size. We limit them to the maximum we\n- * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n- * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n- */\n-#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n-#else\n-/*\n- * Without hugetlb, gigantic folios that are bigger than a single PUD are\n- * currently impossible.\n- */\n-#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n-#endif\n-\n-#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n-\n /*\n  * compound_nr() returns the number of pages in this potentially compound\n  * page.  compound_nr() can be called on a tail page, and is defined to\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 3e51190a55e4..be8ce40b5638 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -23,6 +23,7 @@\n #include <linux/page-flags.h>\n #include <linux/local_lock.h>\n #include <linux/zswap.h>\n+#include <linux/sizes.h>\n #include <asm/page.h>\n \n /* Free memory management - zoned buddy allocator.  */\n@@ -61,6 +62,36 @@\n  */\n #define PAGE_ALLOC_COSTLY_ORDER 3\n \n+#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n+/*\n+ * We don't expect any folios that exceed buddy sizes (and consequently\n+ * memory sections).\n+ */\n+#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n+#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n+/*\n+ * Only pages within a single memory section are guaranteed to be\n+ * contiguous. By limiting folios to a single memory section, all folio\n+ * pages are guaranteed to be contiguous.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n+#elif defined(CONFIG_HUGETLB_PAGE)\n+/*\n+ * There is no real limit on the folio size. We limit them to the maximum we\n+ * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n+ * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n+ */\n+#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n+#else\n+/*\n+ * Without hugetlb, gigantic folios that are bigger than a single PUD are\n+ * currently impossible.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n+#endif\n+\n+#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n+\n enum migratetype {\n \tMIGRATE_UNMOVABLE,\n \tMIGRATE_MOVABLE,\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-02",
              "message_id": "20260202155634.650837-7-kas@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author addressed a concern about the consistency of set_compound_head() and clear_compound_head() logic by moving these functions adjacent to compound_head() in page-flags.h, making it easier to verify their logic is consistent.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "improved code organization"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Move set_compound_head() and clear_compound_head() to be adjacent to the\ncompound_head() function in page-flags.h.\n\nThese functions encode and decode the same compound_info field, so\nkeeping them together makes it easier to verify their logic is\nconsistent, especially when the encoding changes.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h | 24 ++++++++++++------------\n 1 file changed, 12 insertions(+), 12 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex aa46d49e82f7..d14a17ffb55b 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -290,6 +290,18 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n \n+static __always_inline void set_compound_head(struct page *page,\n+\t\t\t\t\t      const struct page *head,\n+\t\t\t\t\t      unsigned int order)\n+{\n+\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n+}\n+\n+static __always_inline void clear_compound_head(struct page *page)\n+{\n+\tWRITE_ONCE(page->compound_info, 0);\n+}\n+\n /**\n  * page_folio - Converts from page to folio.\n  * @p: The page.\n@@ -865,18 +877,6 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page,\n-\t\t\t\t\t      const struct page *head,\n-\t\t\t\t\t      unsigned int order)\n-{\n-\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n-}\n-\n-static __always_inline void clear_compound_head(struct page *page)\n-{\n-\tWRITE_ONCE(page->compound_info, 0);\n-}\n-\n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n static inline void ClearPageCompound(struct page *page)\n {\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nInstead of passing down the head page and tail page index, pass the tail\nand head pages directly, as well as the order of the compound page.\n\nThis is a preparation for changing how the head position is encoded in\nthe tail page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h |  4 +++-\n mm/hugetlb.c               |  8 +++++---\n mm/internal.h              | 12 ++++++------\n mm/mm_init.c               |  2 +-\n mm/page_alloc.c            |  2 +-\n 5 files changed, 16 insertions(+), 12 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex f7a0e4af0c73..8a3694369e15 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page, struct page *head)\n+static __always_inline void set_compound_head(struct page *page,\n+\t\t\t\t\t      const struct page *head,\n+\t\t\t\t\t      unsigned int order)\n {\n \tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n }\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 6e855a32de3d..54ba7cd05a86 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -3168,6 +3168,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)\n \n /* Initialize [start_page:end_page_number] tail struct pages of a hugepage */\n static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n+\t\t\t\t\tstruct hstate *h,\n \t\t\t\t\tunsigned long start_page_number,\n \t\t\t\t\tunsigned long end_page_number)\n {\n@@ -3176,6 +3177,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \tstruct page *page = folio_page(folio, start_page_number);\n \tunsigned long head_pfn = folio_pfn(folio);\n \tunsigned long pfn, end_pfn = head_pfn + end_page_number;\n+\tunsigned int order = huge_page_order(h);\n \n \t/*\n \t * As we marked all tail pages with memblock_reserved_mark_noinit(),\n@@ -3183,7 +3185,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \t */\n \tfor (pfn = head_pfn + start_page_number; pfn < end_pfn; page++, pfn++) {\n \t\t__init_single_page(page, pfn, zone, nid);\n-\t\tprep_compound_tail((struct page *)folio, pfn - head_pfn);\n+\t\tprep_compound_tail(page, &folio->page, order);\n \t\tset_page_count(page, 0);\n \t}\n }\n@@ -3203,7 +3205,7 @@ static void __init hugetlb_folio_init_vmemmap(struct folio *folio,\n \t__folio_set_head(folio);\n \tret = folio_ref_freeze(folio, 1);\n \tVM_BUG_ON(!ret);\n-\thugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages);\n+\thugetlb_folio_init_tail_vmemmap(folio, h, 1, nr_pages);\n \tprep_compound_head(&folio->page, huge_page_order(h));\n }\n \n@@ -3260,7 +3262,7 @@ static void __init prep_and_add_bootmem_folios(struct hstate *h,\n \t\t\t * time as this is early in boot and there should\n \t\t\t * be no contention.\n \t\t\t */\n-\t\t\thugetlb_folio_init_tail_vmemmap(folio,\n+\t\t\thugetlb_folio_init_tail_vmemmap(folio, h,\n \t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_PAGES,\n \t\t\t\t\tpages_per_huge_page(h));\n \t\t}\ndiff --git a/mm/internal.h b/mm/internal.h\nindex d67e8bb75734..037ddcda25ff 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n }\n \n-static inline void prep_compound_tail(struct page *head, int tail_idx)\n+static inline void prep_compound_tail(struct page *tail,\n+\t\t\t\t      const struct page *head,\n+\t\t\t\t      unsigned int order)\n {\n-\tstruct page *p = head + tail_idx;\n-\n-\tp->mapping = TAIL_MAPPING;\n-\tset_compound_head(p, head);\n-\tset_page_private(p, 0);\n+\ttail->mapping = TAIL_MAPPING;\n+\tset_compound_head(tail, head, order);\n+\tset_page_private(tail, 0);\n }\n \n void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..ba50f4c4337b 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -1099,7 +1099,7 @@ static void __ref memmap_init_compound(struct page *head,\n \t\tstruct page *page = pfn_to_page(pfn);\n \n \t\t__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);\n-\t\tprep_compound_tail(head, pfn - head_pfn);\n+\t\tprep_compound_tail(page, head, order);\n \t\tset_page_count(page, 0);\n \t}\n \tprep_compound_head(head, order);\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..00c7ea958767 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -744,7 +744,7 @@ void prep_compound_page(struct page *page, unsigned int order)\n \n \t__SetPageHead(page);\n \tfor (i = 1; i < nr_pages; i++)\n-\t\tprep_compound_tail(page, i);\n+\t\tprep_compound_tail(page + i, page, order);\n \n \tprep_compound_head(page, order);\n }\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nThe 'compound_head' field in the 'struct page' encodes whether the page\nis a tail and where to locate the head page. Bit 0 is set if the page is\na tail, and the remaining bits in the field point to the head page.\n\nAs preparation for changing how the field encodes information about the\nhead page, rename the field to 'compound_info'.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n .../admin-guide/kdump/vmcoreinfo.rst          |  2 +-\n Documentation/mm/vmemmap_dedup.rst            |  6 +++---\n include/linux/mm_types.h                      | 20 +++++++++----------\n include/linux/page-flags.h                    | 18 ++++++++---------\n include/linux/types.h                         |  2 +-\n kernel/vmcore_info.c                          |  2 +-\n mm/page_alloc.c                               |  2 +-\n mm/slab.h                                     |  2 +-\n mm/util.c                                     |  2 +-\n 9 files changed, 28 insertions(+), 28 deletions(-)\n\ndiff --git a/Documentation/admin-guide/kdump/vmcoreinfo.rst b/Documentation/admin-guide/kdump/vmcoreinfo.rst\nindex 404a15f6782c..7663c610fe90 100644\n--- a/Documentation/admin-guide/kdump/vmcoreinfo.rst\n+++ b/Documentation/admin-guide/kdump/vmcoreinfo.rst\n@@ -141,7 +141,7 @@ nodemask_t\n The size of a nodemask_t type. Used to compute the number of online\n nodes.\n \n-(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_head)\n+(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_info)\n ----------------------------------------------------------------------------------\n \n User-space tools compute their values based on the offset of these\ndiff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst\nindex b4a55b6569fa..1863d88d2dcb 100644\n--- a/Documentation/mm/vmemmap_dedup.rst\n+++ b/Documentation/mm/vmemmap_dedup.rst\n@@ -24,7 +24,7 @@ For each base page, there is a corresponding ``struct page``.\n Within the HugeTLB subsystem, only the first 4 ``struct page`` are used to\n contain unique information about a HugeTLB page. ``__NR_USED_SUBPAGE`` provides\n this upper limit. The only 'useful' information in the remaining ``struct page``\n-is the compound_head field, and this field is the same for all tail pages.\n+is the compound_info field, and this field is the same for all tail pages.\n \n By removing redundant ``struct page`` for HugeTLB pages, memory can be returned\n to the buddy allocator for other uses.\n@@ -124,10 +124,10 @@ Here is how things look before optimization::\n  |           |\n  +-----------+\n \n-The value of page->compound_head is the same for all tail pages. The first\n+The value of page->compound_info is the same for all tail pages. The first\n page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4\n ``struct page`` necessary to describe the HugeTLB. The only use of the remaining\n-pages of ``struct page`` (page 1 to page 7) is to point to page->compound_head.\n+pages of ``struct page`` (page 1 to page 7) is to point to page->compound_info.\n Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``\n will be used for each HugeTLB page. This will allow us to free the remaining\n 7 pages to the buddy allocator.\ndiff --git a/include/linux/mm_types.h b/include/linux/mm_types.h\nindex 3cc8ae722886..7bc82a2b889f 100644\n--- a/include/linux/mm_types.h\n+++ b/include/linux/mm_types.h\n@@ -126,14 +126,14 @@ struct page {\n \t\t\tatomic_long_t pp_ref_count;\n \t\t};\n \t\tstruct {\t/* Tail pages of compound page */\n-\t\t\tunsigned long compound_head;\t/* Bit zero is set */\n+\t\t\tunsigned long compound_info;\t/* Bit zero is set */\n \t\t};\n \t\tstruct {\t/* ZONE_DEVICE pages */\n \t\t\t/*\n-\t\t\t * The first word is used for compound_head or folio\n+\t\t\t * The first word is used for compound_info or folio\n \t\t\t * pgmap\n \t\t\t */\n-\t\t\tvoid *_unused_pgmap_compound_head;\n+\t\t\tvoid *_unused_pgmap_compound_info;\n \t\t\tvoid *zone_device_data;\n \t\t\t/*\n \t\t\t * ZONE_DEVICE private pages are counted as being\n@@ -409,7 +409,7 @@ struct folio {\n \t/* private: avoid cluttering the output */\n \t\t\t\t/* For the Unevictable \"LRU list\" slot */\n \t\t\t\tstruct {\n-\t\t\t\t\t/* Avoid compound_head */\n+\t\t\t\t\t/* Avoid compound_info */\n \t\t\t\t\tvoid *__filler;\n \t/* public: */\n \t\t\t\t\tunsigned int mlock_count;\n@@ -510,7 +510,7 @@ struct folio {\n FOLIO_MATCH(flags, flags);\n FOLIO_MATCH(lru, lru);\n FOLIO_MATCH(mapping, mapping);\n-FOLIO_MATCH(compound_head, lru);\n+FOLIO_MATCH(compound_info, lru);\n FOLIO_MATCH(__folio_index, index);\n FOLIO_MATCH(private, private);\n FOLIO_MATCH(_mapcount, _mapcount);\n@@ -529,7 +529,7 @@ FOLIO_MATCH(_last_cpupid, _last_cpupid);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + sizeof(struct page))\n FOLIO_MATCH(flags, _flags_1);\n-FOLIO_MATCH(compound_head, _head_1);\n+FOLIO_MATCH(compound_info, _head_1);\n FOLIO_MATCH(_mapcount, _mapcount_1);\n FOLIO_MATCH(_refcount, _refcount_1);\n #undef FOLIO_MATCH\n@@ -537,13 +537,13 @@ FOLIO_MATCH(_refcount, _refcount_1);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 2 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_2);\n-FOLIO_MATCH(compound_head, _head_2);\n+FOLIO_MATCH(compound_info, _head_2);\n #undef FOLIO_MATCH\n #define FOLIO_MATCH(pg, fl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 3 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_3);\n-FOLIO_MATCH(compound_head, _head_3);\n+FOLIO_MATCH(compound_info, _head_3);\n #undef FOLIO_MATCH\n \n /**\n@@ -609,8 +609,8 @@ struct ptdesc {\n #define TABLE_MATCH(pg, pt)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))\n TABLE_MATCH(flags, pt_flags);\n-TABLE_MATCH(compound_head, pt_list);\n-TABLE_MATCH(compound_head, _pt_pad_1);\n+TABLE_MATCH(compound_info, pt_list);\n+TABLE_MATCH(compound_info, _pt_pad_1);\n TABLE_MATCH(mapping, __page_mapping);\n TABLE_MATCH(__folio_index, pt_index);\n TABLE_MATCH(rcu_head, pt_rcu_head);\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 8a3694369e15..aa46d49e82f7 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -213,7 +213,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n-\t * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)\n+\t * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)\n \t * cold cacheline in some cases.\n \t */\n \tif (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &&\n@@ -223,7 +223,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_head);\n+\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n \n \t\tif (likely(head & 1))\n \t\t\treturn (const struct page *)(head - 1);\n@@ -281,7 +281,7 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_head);\n+\tunsigned long head = READ_ONCE(page->compound_info);\n \n \tif (unlikely(head & 1))\n \t\treturn head - 1;\n@@ -320,13 +320,13 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n static __always_inline int PageTail(const struct page *page)\n {\n-\treturn READ_ONCE(page->compound_head) & 1 || page_is_fake_head(page);\n+\treturn READ_ONCE(page->compound_info) & 1 || page_is_fake_head(page);\n }\n \n static __always_inline int PageCompound(const struct page *page)\n {\n \treturn test_bit(PG_head, &page->flags.f) ||\n-\t       READ_ONCE(page->compound_head) & 1;\n+\t       READ_ONCE(page->compound_info) & 1;\n }\n \n #define\tPAGE_POISON_PATTERN\t-1l\n@@ -348,7 +348,7 @@ static const unsigned long *const_folio_flags(const struct folio *folio,\n {\n \tconst struct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -357,7 +357,7 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)\n {\n \tstruct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -869,12 +869,12 @@ static __always_inline void set_compound_head(struct page *page,\n \t\t\t\t\t      const struct page *head,\n \t\t\t\t\t      unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n+\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\n {\n-\tWRITE_ONCE(page->compound_head, 0);\n+\tWRITE_ONCE(page->compound_info, 0);\n }\n \n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\ndiff --git a/include/linux/types.h b/include/linux/types.h\nindex f69be881369f..604697abf151 100644\n--- a/include/linux/types.h\n+++ b/include/linux/types.h\n@@ -234,7 +234,7 @@ struct ustat {\n  *\n  * This guarantee is important for few reasons:\n  *  - future call_rcu_lazy() will make use of lower bits in the pointer;\n- *  - the structure shares storage space in struct page with @compound_head,\n+ *  - the structure shares storage space in struct page with @compound_info,\n  *    which encode PageTail() in bit 0. The guarantee is needed to avoid\n  *    false-positive PageTail().\n  */\ndiff --git a/kernel/vmcore_info.c b/kernel/vmcore_info.c\nindex 46198580373a..0a46df3e3db9 100644\n--- a/kernel/vmcore_info.c\n+++ b/kernel/vmcore_info.c\n@@ -198,7 +198,7 @@ static int __init crash_save_vmcoreinfo_init(void)\n \tVMCOREINFO_OFFSET(page, lru);\n \tVMCOREINFO_OFFSET(page, _mapcount);\n \tVMCOREINFO_OFFSET(page, private);\n-\tVMCOREINFO_OFFSET(page, compound_head);\n+\tVMCOREINFO_OFFSET(page, compound_info);\n \tVMCOREINFO_OFFSET(pglist_data, node_zones);\n \tVMCOREINFO_OFFSET(pglist_data, nr_zones);\n #ifdef CONFIG_FLATMEM\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 00c7ea958767..cb7375eb1713 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -731,7 +731,7 @@ static inline bool pcp_allowed_order(unsigned int order)\n  * The first PAGE_SIZE page is called the \"head page\" and have PG_head set.\n  *\n  * The remaining PAGE_SIZE pages are called \"tail pages\". PageTail() is encoded\n- * in bit 0 of page->compound_head. The rest of bits is pointer to head page.\n+ * in bit 0 of page->compound_info. The rest of bits is pointer to head page.\n  *\n  * The first tail page's ->compound_order holds the order of allocation.\n  * This usage means that zero-order pages may not be compound.\ndiff --git a/mm/slab.h b/mm/slab.h\nindex e767aa7e91b0..8a2a9c6c697b 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -100,7 +100,7 @@ struct slab {\n #define SLAB_MATCH(pg, sl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))\n SLAB_MATCH(flags, flags);\n-SLAB_MATCH(compound_head, slab_cache);\t/* Ensure bit 0 is clear */\n+SLAB_MATCH(compound_info, slab_cache);\t/* Ensure bit 0 is clear */\n SLAB_MATCH(_refcount, __page_refcount);\n #ifdef CONFIG_MEMCG\n SLAB_MATCH(memcg_data, obj_exts);\ndiff --git a/mm/util.c b/mm/util.c\nindex b05ab6f97e11..3ebcb9e6035c 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1247,7 +1247,7 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_head;\n+\thead = ps->page_snapshot.compound_info;\n \tif ((head & 1) == 0) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_NR_PAGES.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/riscv/mm/init.c | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c\nindex 21d534824624..c555b9a4fdce 100644\n--- a/arch/riscv/mm/init.c\n+++ b/arch/riscv/mm/init.c\n@@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;\n EXPORT_SYMBOL(phys_ram_base);\n \n #ifdef CONFIG_SPARSEMEM_VMEMMAP\n-#define VMEMMAP_ADDR_ALIGN\t(1ULL << SECTION_SIZE_BITS)\n+#define VMEMMAP_ADDR_ALIGN\tmax(1ULL << SECTION_SIZE_BITS, \\\n+\t\t\t\t    MAX_FOLIO_NR_PAGES * sizeof(struct page))\n \n unsigned long vmemmap_start_pfn __ro_after_init;\n EXPORT_SYMBOL(vmemmap_start_pfn);\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_NR_PAGES.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/loongarch/include/asm/pgtable.h | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\nindex c33b3bcb733e..f9416acb9156 100644\n--- a/arch/loongarch/include/asm/pgtable.h\n+++ b/arch/loongarch/include/asm/pgtable.h\n@@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n #endif\n \n-#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n+#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n+#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n #define VMEMMAP_END\t((unsigned long)vmemmap + VMEMMAP_SIZE - 1)\n \n #define KFENCE_AREA_START\t(VMEMMAP_END + 1)\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-02",
              "message_id": "20260202155634.650837-5-kas@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author addressed a concern about the mask-based compound_info encoding being too restrictive, agreeing that it should be limited to HugeTLB vmemmap optimization (HVO) where it makes a difference. They also acknowledged that validating struct pages are naturally aligned for all orders up to MAX_FOLIO_ORDER can be tricky and would require additional validation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a limitation",
                "agreed with a restriction"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "For tail pages, the kernel uses the 'compound_info' field to get to the\nhead page. The bit 0 of the field indicates whether the page is a\ntail page, and if set, the remaining bits represent a pointer to the\nhead page.\n\nFor cases when size of struct page is power-of-2, change the encoding of\ncompound_info to store a mask that can be applied to the virtual address\nof the tail page in order to access the head page. It is possible\nbecause struct page of the head page is naturally aligned with regards\nto order of the page.\n\nThe significant impact of this modification is that all tail pages of\nthe same order will now have identical 'compound_info', regardless of\nthe compound page they are associated with. This paves the way for\neliminating fake heads.\n\nThe HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\napplied when the sizeof(struct page) is power-of-2. Having identical\ntail pages allows the same page to be mapped into the vmemmap of all\npages, maintaining memory savings without fake heads.\n\nIf sizeof(struct page) is not power-of-2, there is no functional\nchanges.\n\nLimit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\na difference. The approach with mask would work in the wider set of\nconditions, but it requires validating that struct pages are naturally\naligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n mm/slab.h                  | 16 ++++++--\n mm/util.c                  | 16 ++++++--\n 3 files changed, 97 insertions(+), 16 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex d14a17ffb55b..8f2c7fbc739b 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -198,6 +198,29 @@ enum pageflags {\n \n #ifndef __GENERATING_BOUNDS_H\n \n+/*\n+ * For tail pages, if the size of struct page is power-of-2 ->compound_info\n+ * encodes the mask that converts the address of the tail page address to\n+ * the head page address.\n+ *\n+ * Otherwise, ->compound_info has direct pointer to head pages.\n+ */\n+static __always_inline bool compound_info_has_mask(void)\n+{\n+\t/*\n+\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n+\t * makes a difference.\n+\t *\n+\t * The approach with mask would work in the wider set of conditions,\n+\t * but it requires validating that struct pages are naturally aligned\n+\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n+\t */\n+\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n+\t\treturn false;\n+\n+\treturn is_power_of_2(sizeof(struct page));\n+}\n+\n #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n \n@@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n \t\treturn page;\n \n+\t/* Fake heads only exists if compound_info_has_mask() is true */\n+\tif (!compound_info_has_mask())\n+\t\treturn page;\n+\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n@@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n+\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n \n-\t\tif (likely(head & 1))\n-\t\t\treturn (const struct page *)(head - 1);\n+\t\t/* See set_compound_head() */\n+\t\tif (likely(info & 1)) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\treturn (const struct page *)(p & info);\n+\t\t}\n \t}\n \treturn page;\n }\n@@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_info);\n+\tunsigned long info = READ_ONCE(page->compound_info);\n \n-\tif (unlikely(head & 1))\n-\t\treturn head - 1;\n-\treturn (unsigned long)page_fixed_fake_head(page);\n+\t/* Bit 0 encodes PageTail() */\n+\tif (!(info & 1))\n+\t\treturn (unsigned long)page_fixed_fake_head(page);\n+\n+\t/*\n+\t * If compound_info_has_mask() is false, the rest of compound_info is\n+\t * the pointer to the head page.\n+\t */\n+\tif (!compound_info_has_mask())\n+\t\treturn info - 1;\n+\n+\t/*\n+\t * If compoun_info_has_mask() is true the rest of the info encodes\n+\t * the mask that converts the address of the tail page to the head page.\n+\t *\n+\t * No need to clear bit 0 in the mask as 'page' always has it clear.\n+\t */\n+\treturn (unsigned long)page & info;\n }\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n@@ -294,7 +340,26 @@ static __always_inline void set_compound_head(struct page *page,\n \t\t\t\t\t      const struct page *head,\n \t\t\t\t\t      unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n+\tunsigned int shift;\n+\tunsigned long mask;\n+\n+\tif (!compound_info_has_mask()) {\n+\t\tWRITE_ONCE(page->compound_info, (unsigned long)head | 1);\n+\t\treturn;\n+\t}\n+\n+\t/*\n+\t * If the size of struct page is power-of-2, bits [shift:0] of the\n+\t * virtual address of compound head are zero.\n+\t *\n+\t * Calculate mask that can be applied to the virtual address of\n+\t * the tail page to get address of the head page.\n+\t */\n+\tshift = order + order_base_2(sizeof(struct page));\n+\tmask = GENMASK(BITS_PER_LONG - 1, shift);\n+\n+\t/* Bit 0 encodes PageTail() */\n+\tWRITE_ONCE(page->compound_info, mask | 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\ndiff --git a/mm/slab.h b/mm/slab.h\nindex 8a2a9c6c697b..f68c3ac8126f 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -137,11 +137,19 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist\n  */\n static inline struct slab *page_slab(const struct page *page)\n {\n-\tunsigned long head;\n+\tunsigned long info;\n+\n+\tinfo = READ_ONCE(page->compound_info);\n+\tif (info & 1) {\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\t\t\tpage = (struct page *)(p & info);\n+\t\t} else {\n+\t\t\tpage = (struct page *)(info - 1);\n+\t\t}\n+\t}\n \n-\thead = READ_ONCE(page->compound_head);\n-\tif (head & 1)\n-\t\tpage = (struct page *)(head - 1);\n \tif (data_race(page->page_type >> 24) != PGTY_slab)\n \t\tpage = NULL;\n \ndiff --git a/mm/util.c b/mm/util.c\nindex 3ebcb9e6035c..20dccf2881d7 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1237,7 +1237,7 @@ static void set_ps_flags(struct page_snapshot *ps, const struct folio *folio,\n  */\n void snapshot_page(struct page_snapshot *ps, const struct page *page)\n {\n-\tunsigned long head, nr_pages = 1;\n+\tunsigned long info, nr_pages = 1;\n \tstruct folio *foliop;\n \tint loops = 5;\n \n@@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_info;\n-\tif ((head & 1) == 0) {\n+\tinfo = ps->page_snapshot.compound_info;\n+\tif (!(info & 1)) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n \t\tif (!folio_test_large(foliop)) {\n@@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n \t\t}\n \t\tfoliop = (struct folio *)page;\n \t} else {\n-\t\tfoliop = (struct folio *)(head - 1);\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\tfoliop = (struct folio *)(p & info);\n+\t\t} else {\n+\t\t\tfoliop = (struct folio *)(info - 1);\n+\t\t}\n+\n \t\tps->idx = folio_page_idx(foliop, page);\n \t}\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nWith the upcoming changes to HVO, a single page of tail struct pages\nwill be shared across all huge pages of the same order on a node. Since\nhuge pages on the same node may belong to different zones, the zone\ninformation stored in shared tail page flags would be incorrect.\n\nAlways fetch zone information from the head page, which has unique and\ncorrect zone flags for each compound page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/mmzone.h | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex be8ce40b5638..192143b5cdc0 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n \n static inline enum zone_type page_zonenum(const struct page *page)\n {\n+\tpage = compound_head(page);\n \treturn memdesc_zonenum(page->flags);\n }\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nIf page->compound_info encodes a mask, it is expected that vmemmap to be\nnaturally aligned to the maximum folio size.\n\nAdd a VM_BUG_ON() to check the alignment.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n mm/sparse.c | 7 +++++++\n 1 file changed, 7 insertions(+)\n\ndiff --git a/mm/sparse.c b/mm/sparse.c\nindex b5b2b6f7041b..6c9b62607f3f 100644\n--- a/mm/sparse.c\n+++ b/mm/sparse.c\n@@ -600,6 +600,13 @@ void __init sparse_init(void)\n \tBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n \tmemblocks_present();\n \n+\tif (compound_info_has_mask()) {\n+\t\tunsigned long alignment;\n+\n+\t\talignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);\n+\t\tVM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), alignment));\n+\t}\n+\n \tpnum_begin = first_present_section_nr();\n \tnid_begin = sparse_early_nid(__nr_to_section(pnum_begin));\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Wed, Feb 04, 2026 at 05:14:12PM +0100, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > Instead of passing down the head page and tail page index, pass the tail\n> > and head pages directly, as well as the order of the compound page.\n> > \n> > This is a preparation for changing how the head position is encoded in\n> > the tail page.\n> > \n> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> > Reviewed-by: Zi Yan <ziy@nvidia.com>\n> > ---\n> >   include/linux/page-flags.h |  4 +++-\n> >   mm/hugetlb.c               |  8 +++++---\n> >   mm/internal.h              | 12 ++++++------\n> >   mm/mm_init.c               |  2 +-\n> >   mm/page_alloc.c            |  2 +-\n> >   5 files changed, 16 insertions(+), 12 deletions(-)\n> > \n> > diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> > index f7a0e4af0c73..8a3694369e15 100644\n> > --- a/include/linux/page-flags.h\n> > +++ b/include/linux/page-flags.h\n> > @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n> >   \treturn folio_test_head(folio);\n> >   }\n> > -static __always_inline void set_compound_head(struct page *page, struct page *head)\n> > +static __always_inline void set_compound_head(struct page *page,\n> > +\t\t\t\t\t      const struct page *head,\n> > +\t\t\t\t\t      unsigned int order)\n> \n> Two tab indents please on second+ parameter list whenever you touch code.\n\nDo we have this coding style preference written down somewhere?\n\n-tip tree wants the opposite. Documentation/process/maintainer-tip.rst:\n\n\tWhen splitting function declarations or function calls, then please align\n\tthe first argument in the second line with the first argument in the first\n\tline::\n\nI want the editor to do The Right Thing\\u2122 without my brain involvement.\nHaving different coding styles in different corners of the kernel makes\nit hard.\n\n> \n> >   {\n> >   \tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n> >   }\n> > diff --git a/mm/hugetlb.c b/mm/hugetlb.c\n> > index 6e855a32de3d..54ba7cd05a86 100644\n> \n> \n> [...]\n> \n> > diff --git a/mm/internal.h b/mm/internal.h\n> > index d67e8bb75734..037ddcda25ff 100644\n> > --- a/mm/internal.h\n> > +++ b/mm/internal.h\n> > @@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n> >   \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n> >   }\n> > -static inline void prep_compound_tail(struct page *head, int tail_idx)\n> > +static inline void prep_compound_tail(struct page *tail,\n> \n> Just wondering whether we should call this \"struct page *page\" for\n> consistency with set_compound_head().\n> \n> Or alternatively, call it also \"tail\" in set_compound_head().\n\nI will take the alternative path :)\n\n> \n> > +\t\t\t\t      const struct page *head,\n> > +\t\t\t\t      unsigned int order)\n> \n> Two tab indent, then this fits into two lines in total.\n> \n> >   {\n> > -\tstruct page *p = head + tail_idx;\n> > -\n> > -\tp->mapping = TAIL_MAPPING;\n> > -\tset_compound_head(p, head);\n> > -\tset_page_private(p, 0);\n> > +\ttail->mapping = TAIL_MAPPING;\n> > +\tset_compound_head(tail, head, order);\n> > +\tset_page_private(tail, 0);\n> >   }\n> Only nits, in general LGTM\n> \n> Acked-by: David Hildenbrand (arm) <david@kernel.org>\n\nThanks!\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Wed, Feb 04, 2026 at 05:14:12PM +0100, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > Instead of passing down the head page and tail page index, pass the tail\n> > and head pages directly, as well as the order of the compound page.\n> > \n> > This is a preparation for changing how the head position is encoded in\n> > the tail page.\n> > \n> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> > Reviewed-by: Zi Yan <ziy@nvidia.com>\n> > ---\n> >   include/linux/page-flags.h |  4 +++-\n> >   mm/hugetlb.c               |  8 +++++---\n> >   mm/internal.h              | 12 ++++++------\n> >   mm/mm_init.c               |  2 +-\n> >   mm/page_alloc.c            |  2 +-\n> >   5 files changed, 16 insertions(+), 12 deletions(-)\n> > \n> > diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> > index f7a0e4af0c73..8a3694369e15 100644\n> > --- a/include/linux/page-flags.h\n> > +++ b/include/linux/page-flags.h\n> > @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n> >   \treturn folio_test_head(folio);\n> >   }\n> > -static __always_inline void set_compound_head(struct page *page, struct page *head)\n> > +static __always_inline void set_compound_head(struct page *page,\n> > +\t\t\t\t\t      const struct page *head,\n> > +\t\t\t\t\t      unsigned int order)\n> \n> Two tab indents please on second+ parameter list whenever you touch code.\n\nDo we have this coding style preference written down somewhere?\n\n-tip tree wants the opposite. Documentation/process/maintainer-tip.rst:\n\n\tWhen splitting function declarations or function calls, then please align\n\tthe first argument in the second line with the first argument in the first\n\tline::\n\nI want the editor to do The Right Thing without my brain involvement.\nHaving different coding styles in different corners of the kernel makes\nit hard.\n\n> \n> >   {\n> >   \tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n> >   }\n> > diff --git a/mm/hugetlb.c b/mm/hugetlb.c\n> > index 6e855a32de3d..54ba7cd05a86 100644\n> \n> \n> [...]\n> \n> > diff --git a/mm/internal.h b/mm/internal.h\n> > index d67e8bb75734..037ddcda25ff 100644\n> > --- a/mm/internal.h\n> > +++ b/mm/internal.h\n> > @@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n> >   \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n> >   }\n> > -static inline void prep_compound_tail(struct page *head, int tail_idx)\n> > +static inline void prep_compound_tail(struct page *tail,\n> \n> Just wondering whether we should call this \"struct page *page\" for\n> consistency with set_compound_head().\n> \n> Or alternatively, call it also \"tail\" in set_compound_head().\n\nI will take the alternative path :)\n\n> \n> > +\t\t\t\t      const struct page *head,\n> > +\t\t\t\t      unsigned int order)\n> \n> Two tab indent, then this fits into two lines in total.\n> \n> >   {\n> > -\tstruct page *p = head + tail_idx;\n> > -\n> > -\tp->mapping = TAIL_MAPPING;\n> > -\tset_compound_head(p, head);\n> > -\tset_page_private(p, 0);\n> > +\ttail->mapping = TAIL_MAPPING;\n> > +\tset_compound_head(tail, head, order);\n> > +\tset_page_private(tail, 0);\n> >   }\n> Only nits, in general LGTM\n> \n> Acked-by: David Hildenbrand (arm) <david@kernel.org>\n\nThanks!\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Thu, Feb 05, 2026 at 01:56:36PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/4/26 17:56, David Hildenbrand (arm) wrote:\n> > On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > > The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> > > struct pages of the head page to be naturally aligned with regard to the\n> > > folio size.\n> > > \n> > > Align vmemmap to MAX_FOLIO_NR_PAGES.\n> > > \n> > > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > > ---\n> > >  arch/loongarch/include/asm/pgtable.h | 3 ++-\n> > >  1 file changed, 2 insertions(+), 1 deletion(-)\n> > > \n> > > diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/\n> > > include/asm/pgtable.h\n> > > index c33b3bcb733e..f9416acb9156 100644\n> > > --- a/arch/loongarch/include/asm/pgtable.h\n> > > +++ b/arch/loongarch/include/asm/pgtable.h\n> > > @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE /\n> > > sizeof(unsigned long)];\n> > >  min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE\n> > > * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE -\n> > > KFENCE_AREA_SIZE)\n> > >  #endif\n> > > -#define vmemmap ((struct page *)((VMALLOC_END + PMD_SIZE) &\n> > > PMD_MASK))\n> > > +#define VMEMMAP_ALIGN max(PMD_SIZE, MAX_FOLIO_NR_PAGES *\n> > > sizeof(struct page))\n> > > +#define vmemmap ((struct page *)(ALIGN(VMALLOC_END,\n> > > VMEMMAP_ALIGN)))\n> > \n> > \n> > Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just\n> > black magic here\n> > and the description of the situation is wrong.\n> > \n> > Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct\n> > page)\" into the core and call it\n> > \n> > #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n> > page))\n> > \n> > But then special case it base on (a) HVO being configured in an (b) HVO\n> > being possible\n> > \n> > #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n> > /* A very helpful comment explaining the situation. */\n> > #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n> > page))\n> > #else\n> > #define MAX_FOLIO_VMEMMAP_ALIGN 0\n> > #endif\n> > \n> > Something like that.\n> > \n> \n> Thinking about this ...\n> \n> the vmemmap start is always struct-page-aligned. Otherwise we'd be in\n> trouble already.\n> \n> Isn't it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?\n> \n> Let's assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for\n> simplicity.\n> \n> vmemmap start would be multiples of 512 (0x0010000000).\n> \n> 512, 1024, 1536, 2048 ...\n> \n> Assume we have an 256-pages folio at 1536+256 = 0x111000000\n\ns/0x/0b/, but okay.\n\n> Assume we have the last page of that folio (0x011111111111), we would just\n> get to the start of that folio by AND-ing with ~(256-1).\n> \n> Which case am I ignoring?\n\nIIUC, you are ignoring the actual size of struct page. It is not 1 byte :P\n\nThe last page of this 256-page folio is at 1536+256 + (64 * 255) which\nis 0b100011011000000. There's no mask that you can AND that gets you to\n0b11100000000.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Thu, Feb 05, 2026 at 01:56:36PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/4/26 17:56, David Hildenbrand (arm) wrote:\n> > On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > > The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> > > struct pages of the head page to be naturally aligned with regard to the\n> > > folio size.\n> > > \n> > > Align vmemmap to MAX_FOLIO_NR_PAGES.\n> > > \n> > > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > > ---\n> > >  arch/loongarch/include/asm/pgtable.h | 3 ++-\n> > >  1 file changed, 2 insertions(+), 1 deletion(-)\n> > > \n> > > diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/\n> > > include/asm/pgtable.h\n> > > index c33b3bcb733e..f9416acb9156 100644\n> > > --- a/arch/loongarch/include/asm/pgtable.h\n> > > +++ b/arch/loongarch/include/asm/pgtable.h\n> > > @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE /\n> > > sizeof(unsigned long)];\n> > >  min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE\n> > > * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE -\n> > > KFENCE_AREA_SIZE)\n> > >  #endif\n> > > -#define vmemmap ((struct page *)((VMALLOC_END + PMD_SIZE) &\n> > > PMD_MASK))\n> > > +#define VMEMMAP_ALIGN max(PMD_SIZE, MAX_FOLIO_NR_PAGES *\n> > > sizeof(struct page))\n> > > +#define vmemmap ((struct page *)(ALIGN(VMALLOC_END,\n> > > VMEMMAP_ALIGN)))\n> > \n> > \n> > Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just\n> > black magic here\n> > and the description of the situation is wrong.\n> > \n> > Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct\n> > page)\" into the core and call it\n> > \n> > #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n> > page))\n> > \n> > But then special case it base on (a) HVO being configured in an (b) HVO\n> > being possible\n> > \n> > #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n> > /* A very helpful comment explaining the situation. */\n> > #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n> > page))\n> > #else\n> > #define MAX_FOLIO_VMEMMAP_ALIGN 0\n> > #endif\n> > \n> > Something like that.\n> > \n> \n> Thinking about this ...\n> \n> the vmemmap start is always struct-page-aligned. Otherwise we'd be in\n> trouble already.\n> \n> Isn't it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?\n> \n> Let's assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for\n> simplicity.\n> \n> vmemmap start would be multiples of 512 (0x0010000000).\n> \n> 512, 1024, 1536, 2048 ...\n> \n> Assume we have an 256-pages folio at 1536+256 = 0x111000000\n\ns/0x/0b/, but okay.\n\n> Assume we have the last page of that folio (0x011111111111), we would just\n> get to the start of that folio by AND-ing with ~(256-1).\n> \n> Which case am I ignoring?\n\nIIUC, you are ignoring the actual size of struct page. It is not 1 byte :P\n\nThe last page of this 256-page folio is at 1536+256 + (64 * 255) which\nis 0b100011011000000. There's no mask that you can AND that gets you to\n0b11100000000.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-02",
              "message_id": "20260202155634.650837-8-kas@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "Author acknowledged that the calculation for MAX_FOLIO_NR_PAGES * sizeof(struct page) is incorrect and can result in a non-power-of-2 value, which would disable HVO even if it's configured. He suggested using roundup_pow_of_two(sizeof(struct page)) instead, but also mentioned that avoiding sizeof(struct page) altogether might not be possible.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical issue",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Wed, Feb 04, 2026 at 05:50:23PM +0100, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> > struct pages of the head page to be naturally aligned with regard to the\n> > folio size.\n> > \n> > Align vmemmap to MAX_FOLIO_NR_PAGES.\n> \n> I think neither that statement nor the one in the patch description is\n> correct?\n> \n> \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is neither the maximum folio size\n> nor MAX_FOLIO_NR_PAGES.\n> \n> It's the size of the memmap that a large folio could span at maximum.\n> \n> \n> Assuming we have a 16 GiB folio, the calculation would give us\n> \n> \t4194304 * sizeof(struct page)\n> \n> Which could be something like (assuming 80 bytes)\n> \n> \t335544320\n> \n> -> not even a power of 2, weird? (for HVO you wouldn't care as HVO would be\n> disabled, but that aliment is super weird?)\n> \n> \n> Assuming 64 bytes, it would be a power of two (as 64 is a power of two).\n> \n> \t268435456 (1<< 28)\n> \n> \n> Which makes me wonder whether there is a way to avoid sizeof(struct page)\n> here completely.\n\nI don't think we can. See the other thread.\n\nWhat about using roundup_pow_of_two(sizeof(struct page)) here.\n\n> Or limit the alignment to the case where HVO is actually active and\n> sizeof(struct page) makes any sense?\n\nThe annoying part of HVO is that it is unknown at compile-time if it\nwill be used. You can compile kernel with HVO that will no be activated\ndue to non-power-of-2 sizeof(struct page) because of a debug config option.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Wed, Feb 04, 2026 at 05:50:23PM +0100, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> > struct pages of the head page to be naturally aligned with regard to the\n> > folio size.\n> > \n> > Align vmemmap to MAX_FOLIO_NR_PAGES.\n> \n> I think neither that statement nor the one in the patch description is\n> correct?\n> \n> \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is neither the maximum folio size\n> nor MAX_FOLIO_NR_PAGES.\n> \n> It's the size of the memmap that a large folio could span at maximum.\n> \n> \n> Assuming we have a 16 GiB folio, the calculation would give us\n> \n> \t4194304 * sizeof(struct page)\n> \n> Which could be something like (assuming 80 bytes)\n> \n> \t335544320\n> \n> -> not even a power of 2, weird? (for HVO you wouldn't care as HVO would be\n> disabled, but that aliment is super weird?)\n> \n> \n> Assuming 64 bytes, it would be a power of two (as 64 is a power of two).\n> \n> \t268435456 (1<< 28)\n> \n> \n> Which makes me wonder whether there is a way to avoid sizeof(struct page)\n> here completely.\n\nI don't think we can. See the other thread.\n\nWhat about using roundup_pow_of_two(sizeof(struct page)) here.\n\n> Or limit the alignment to the case where HVO is actually active and\n> sizeof(struct page) makes any sense?\n\nThe annoying part of HVO is that it is unknown at compile-time if it\nwill be used. You can compile kernel with HVO that will no be activated\ndue to non-power-of-2 sizeof(struct page) because of a debug config option.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Wed, Feb 04, 2026 at 05:56:45PM +0100, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> > struct pages of the head page to be naturally aligned with regard to the\n> > folio size.\n> > \n> > Align vmemmap to MAX_FOLIO_NR_PAGES.\n> > \n> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > ---\n> >   arch/loongarch/include/asm/pgtable.h | 3 ++-\n> >   1 file changed, 2 insertions(+), 1 deletion(-)\n> > \n> > diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\n> > index c33b3bcb733e..f9416acb9156 100644\n> > --- a/arch/loongarch/include/asm/pgtable.h\n> > +++ b/arch/loongarch/include/asm/pgtable.h\n> > @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n> >   \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n> >   #endif\n> > -#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n> > +#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n> > +#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n> \n> \n> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just black magic here\n> and the description of the situation is wrong.\n> \n> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" into the core and call it\n> \n> #define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n> \n> But then special case it base on (a) HVO being configured in an (b) HVO being possible\n> \n> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n\nThis would require some kind of asm-offsets.c/bounds.c magic to pull the\nstruct page size condition to the preprocessor level.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Wed, Feb 04, 2026 at 05:56:45PM +0100, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> > struct pages of the head page to be naturally aligned with regard to the\n> > folio size.\n> > \n> > Align vmemmap to MAX_FOLIO_NR_PAGES.\n> > \n> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > ---\n> >   arch/loongarch/include/asm/pgtable.h | 3 ++-\n> >   1 file changed, 2 insertions(+), 1 deletion(-)\n> > \n> > diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\n> > index c33b3bcb733e..f9416acb9156 100644\n> > --- a/arch/loongarch/include/asm/pgtable.h\n> > +++ b/arch/loongarch/include/asm/pgtable.h\n> > @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n> >   \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n> >   #endif\n> > -#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n> > +#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n> > +#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n> \n> \n> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just black magic here\n> and the description of the situation is wrong.\n> \n> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" into the core and call it\n> \n> #define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n> \n> But then special case it base on (a) HVO being configured in an (b) HVO being possible\n> \n> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n\nThis would require some kind of asm-offsets.c/bounds.c magic to pull the\nstruct page size condition to the preprocessor level.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > With the upcoming changes to HVO, a single page of tail struct pages\n> > will be shared across all huge pages of the same order on a node. Since\n> > huge pages on the same node may belong to different zones, the zone\n> > information stored in shared tail page flags would be incorrect.\n> > \n> > Always fetch zone information from the head page, which has unique and\n> > correct zone flags for each compound page.\n> > \n> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > Acked-by: Zi Yan <ziy@nvidia.com>\n> > ---\n> >   include/linux/mmzone.h | 1 +\n> >   1 file changed, 1 insertion(+)\n> > \n> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> > index be8ce40b5638..192143b5cdc0 100644\n> > --- a/include/linux/mmzone.h\n> > +++ b/include/linux/mmzone.h\n> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> >   static inline enum zone_type page_zonenum(const struct page *page)\n> >   {\n> > +\tpage = compound_head(page);\n> >   \treturn memdesc_zonenum(page->flags);\n> \n> We end up calling page_zonenum() without holding a reference.\n> \n> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> see concurrent page freeing etc.\n> \n> However, this change implies that we now perform a compound page lookup for\n> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> including for pageblock access and page freeing].\n> \n> That's a nasty compromise for making HVO better? :)\n> \n> We should likely limit that special casing to kernels that really rquire it\n> (HVO).\n\nI will add compound_info_has_mask() check.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > With the upcoming changes to HVO, a single page of tail struct pages\n> > will be shared across all huge pages of the same order on a node. Since\n> > huge pages on the same node may belong to different zones, the zone\n> > information stored in shared tail page flags would be incorrect.\n> > \n> > Always fetch zone information from the head page, which has unique and\n> > correct zone flags for each compound page.\n> > \n> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > Acked-by: Zi Yan <ziy@nvidia.com>\n> > ---\n> >   include/linux/mmzone.h | 1 +\n> >   1 file changed, 1 insertion(+)\n> > \n> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> > index be8ce40b5638..192143b5cdc0 100644\n> > --- a/include/linux/mmzone.h\n> > +++ b/include/linux/mmzone.h\n> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> >   static inline enum zone_type page_zonenum(const struct page *page)\n> >   {\n> > +\tpage = compound_head(page);\n> >   \treturn memdesc_zonenum(page->flags);\n> \n> We end up calling page_zonenum() without holding a reference.\n> \n> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> see concurrent page freeing etc.\n> \n> However, this change implies that we now perform a compound page lookup for\n> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> including for pageblock access and page freeing].\n> \n> That's a nasty compromise for making HVO better? :)\n> \n> We should likely limit that special casing to kernels that really rquire it\n> (HVO).\n\nI will add compound_info_has_mask() check.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:\n> On 2/9/26 12:52, Kiryl Shutsemau wrote:\n> > On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> >> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> >> > With the upcoming changes to HVO, a single page of tail struct pages\n> >> > will be shared across all huge pages of the same order on a node. Since\n> >> > huge pages on the same node may belong to different zones, the zone\n> >> > information stored in shared tail page flags would be incorrect.\n> >> > \n> >> > Always fetch zone information from the head page, which has unique and\n> >> > correct zone flags for each compound page.\n> >> > \n> >> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> >> > Acked-by: Zi Yan <ziy@nvidia.com>\n> >> > ---\n> >> >   include/linux/mmzone.h | 1 +\n> >> >   1 file changed, 1 insertion(+)\n> >> > \n> >> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> >> > index be8ce40b5638..192143b5cdc0 100644\n> >> > --- a/include/linux/mmzone.h\n> >> > +++ b/include/linux/mmzone.h\n> >> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> >> >   static inline enum zone_type page_zonenum(const struct page *page)\n> >> >   {\n> >> > +\tpage = compound_head(page);\n> >> >   \treturn memdesc_zonenum(page->flags);\n> >> \n> >> We end up calling page_zonenum() without holding a reference.\n> >> \n> >> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> >> see concurrent page freeing etc.\n> >> \n> >> However, this change implies that we now perform a compound page lookup for\n> >> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> >> including for pageblock access and page freeing].\n> >> \n> >> That's a nasty compromise for making HVO better? :)\n> >> \n> >> We should likely limit that special casing to kernels that really rquire it\n> >> (HVO).\n> > \n> > I will add compound_info_has_mask() check.\n> \n> Not thrilled by this indeed. Would it be a problem to have the shared tail\n> pages per node+zone instead of just per node?\n\nI thought it would be overkill. It likely is going to be unused for most\nnodes. But sure, move it to per-zone.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:\n> On 2/9/26 12:52, Kiryl Shutsemau wrote:\n> > On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> >> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> >> > With the upcoming changes to HVO, a single page of tail struct pages\n> >> > will be shared across all huge pages of the same order on a node. Since\n> >> > huge pages on the same node may belong to different zones, the zone\n> >> > information stored in shared tail page flags would be incorrect.\n> >> > \n> >> > Always fetch zone information from the head page, which has unique and\n> >> > correct zone flags for each compound page.\n> >> > \n> >> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> >> > Acked-by: Zi Yan <ziy@nvidia.com>\n> >> > ---\n> >> >   include/linux/mmzone.h | 1 +\n> >> >   1 file changed, 1 insertion(+)\n> >> > \n> >> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> >> > index be8ce40b5638..192143b5cdc0 100644\n> >> > --- a/include/linux/mmzone.h\n> >> > +++ b/include/linux/mmzone.h\n> >> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> >> >   static inline enum zone_type page_zonenum(const struct page *page)\n> >> >   {\n> >> > +\tpage = compound_head(page);\n> >> >   \treturn memdesc_zonenum(page->flags);\n> >> \n> >> We end up calling page_zonenum() without holding a reference.\n> >> \n> >> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> >> see concurrent page freeing etc.\n> >> \n> >> However, this change implies that we now perform a compound page lookup for\n> >> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> >> including for pageblock access and page freeing].\n> >> \n> >> That's a nasty compromise for making HVO better? :)\n> >> \n> >> We should likely limit that special casing to kernels that really rquire it\n> >> (HVO).\n> > \n> > I will add compound_info_has_mask() check.\n> \n> Not thrilled by this indeed. Would it be a problem to have the shared tail\n> pages per node+zone instead of just per node?\n\nI thought it would be overkill. It likely is going to be unused for most\nnodes. But sure, move it to per-zone.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Mon, Feb 16, 2026 at 11:30:22AM +0000, Kiryl Shutsemau wrote:\n> On Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:\n> > On 2/9/26 12:52, Kiryl Shutsemau wrote:\n> > > On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> > >> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > >> > With the upcoming changes to HVO, a single page of tail struct pages\n> > >> > will be shared across all huge pages of the same order on a node. Since\n> > >> > huge pages on the same node may belong to different zones, the zone\n> > >> > information stored in shared tail page flags would be incorrect.\n> > >> > \n> > >> > Always fetch zone information from the head page, which has unique and\n> > >> > correct zone flags for each compound page.\n> > >> > \n> > >> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > >> > Acked-by: Zi Yan <ziy@nvidia.com>\n> > >> > ---\n> > >> >   include/linux/mmzone.h | 1 +\n> > >> >   1 file changed, 1 insertion(+)\n> > >> > \n> > >> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> > >> > index be8ce40b5638..192143b5cdc0 100644\n> > >> > --- a/include/linux/mmzone.h\n> > >> > +++ b/include/linux/mmzone.h\n> > >> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> > >> >   static inline enum zone_type page_zonenum(const struct page *page)\n> > >> >   {\n> > >> > +\tpage = compound_head(page);\n> > >> >   \treturn memdesc_zonenum(page->flags);\n> > >> \n> > >> We end up calling page_zonenum() without holding a reference.\n> > >> \n> > >> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> > >> see concurrent page freeing etc.\n> > >> \n> > >> However, this change implies that we now perform a compound page lookup for\n> > >> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> > >> including for pageblock access and page freeing].\n> > >> \n> > >> That's a nasty compromise for making HVO better? :)\n> > >> \n> > >> We should likely limit that special casing to kernels that really rquire it\n> > >> (HVO).\n> > > \n> > > I will add compound_info_has_mask() check.\n> > \n> > Not thrilled by this indeed. Would it be a problem to have the shared tail\n> > pages per node+zone instead of just per node?\n> \n> I thought it would be overkill. It likely is going to be unused for most\n> nodes. But sure, move it to per-zone.\n\nI gave it a try, but stumbled on a problem.\n\nWe need to know the zone in hugetlb_vmemmap_init_early(), but zones are\nnot yet defined.\n\nhugetlb_vmemmap_init_early() is called from within sparse_init(), but\nspan of zones is defined in free_area_init() after sparse_init().\n\nAny ideas, how get past this? :/\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Mon, Feb 16, 2026 at 11:30:22AM +0000, Kiryl Shutsemau wrote:\n> On Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:\n> > On 2/9/26 12:52, Kiryl Shutsemau wrote:\n> > > On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> > >> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > >> > With the upcoming changes to HVO, a single page of tail struct pages\n> > >> > will be shared across all huge pages of the same order on a node. Since\n> > >> > huge pages on the same node may belong to different zones, the zone\n> > >> > information stored in shared tail page flags would be incorrect.\n> > >> > \n> > >> > Always fetch zone information from the head page, which has unique and\n> > >> > correct zone flags for each compound page.\n> > >> > \n> > >> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > >> > Acked-by: Zi Yan <ziy@nvidia.com>\n> > >> > ---\n> > >> >   include/linux/mmzone.h | 1 +\n> > >> >   1 file changed, 1 insertion(+)\n> > >> > \n> > >> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> > >> > index be8ce40b5638..192143b5cdc0 100644\n> > >> > --- a/include/linux/mmzone.h\n> > >> > +++ b/include/linux/mmzone.h\n> > >> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> > >> >   static inline enum zone_type page_zonenum(const struct page *page)\n> > >> >   {\n> > >> > +\tpage = compound_head(page);\n> > >> >   \treturn memdesc_zonenum(page->flags);\n> > >> \n> > >> We end up calling page_zonenum() without holding a reference.\n> > >> \n> > >> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> > >> see concurrent page freeing etc.\n> > >> \n> > >> However, this change implies that we now perform a compound page lookup for\n> > >> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> > >> including for pageblock access and page freeing].\n> > >> \n> > >> That's a nasty compromise for making HVO better? :)\n> > >> \n> > >> We should likely limit that special casing to kernels that really rquire it\n> > >> (HVO).\n> > > \n> > > I will add compound_info_has_mask() check.\n> > \n> > Not thrilled by this indeed. Would it be a problem to have the shared tail\n> > pages per node+zone instead of just per node?\n> \n> I thought it would be overkill. It likely is going to be unused for most\n> nodes. But sure, move it to per-zone.\n\nI gave it a try, but stumbled on a problem.\n\nWe need to know the zone in hugetlb_vmemmap_init_early(), but zones are\nnot yet defined.\n\nhugetlb_vmemmap_init_early() is called from within sparse_init(), but\nspan of zones is defined in free_area_init() after sparse_init().\n\nAny ideas, how get past this? :/\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-05",
              "message_id": "aYSe0TAIzxJ9i1Wy@thinkstation",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author is addressing a concern about passing down the head page and tail page index, which was deemed too complex. The author instead proposes to pass the tail and head pages directly, along with the compound page order, as a preparation for changing how the head position is encoded in the tail page.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "preparation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Kiryl Shutsemau <kas@kernel.org>\n\nMove MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n\nThis is preparation for adding the vmemmap_tails array to struct\nzone, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: David Hildenbrand (Red Hat) <david@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\nAcked-by: Muchun Song <muchun.song@linux.dev>\nAcked-by: Usama Arif <usamaarif642@gmail.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/mm.h     | 31 -------------------------------\n include/linux/mmzone.h | 31 +++++++++++++++++++++++++++++++\n 2 files changed, 31 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex 5be3d8a8f806..7f4dbbb9d783 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -27,7 +27,6 @@\n #include <linux/page-flags.h>\n #include <linux/page_ref.h>\n #include <linux/overflow.h>\n-#include <linux/sizes.h>\n #include <linux/sched.h>\n #include <linux/pgtable.h>\n #include <linux/kasan.h>\n@@ -2479,36 +2478,6 @@ static inline unsigned long folio_nr_pages(const struct folio *folio)\n \treturn folio_large_nr_pages(folio);\n }\n \n-#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n-/*\n- * We don't expect any folios that exceed buddy sizes (and consequently\n- * memory sections).\n- */\n-#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n-#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n-/*\n- * Only pages within a single memory section are guaranteed to be\n- * contiguous. By limiting folios to a single memory section, all folio\n- * pages are guaranteed to be contiguous.\n- */\n-#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n-#elif defined(CONFIG_HUGETLB_PAGE)\n-/*\n- * There is no real limit on the folio size. We limit them to the maximum we\n- * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n- * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n- */\n-#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n-#else\n-/*\n- * Without hugetlb, gigantic folios that are bigger than a single PUD are\n- * currently impossible.\n- */\n-#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n-#endif\n-\n-#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n-\n /*\n  * compound_nr() returns the number of pages in this potentially compound\n  * page.  compound_nr() can be called on a tail page, and is defined to\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 3e51190a55e4..be8ce40b5638 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -23,6 +23,7 @@\n #include <linux/page-flags.h>\n #include <linux/local_lock.h>\n #include <linux/zswap.h>\n+#include <linux/sizes.h>\n #include <asm/page.h>\n \n /* Free memory management - zoned buddy allocator.  */\n@@ -61,6 +62,36 @@\n  */\n #define PAGE_ALLOC_COSTLY_ORDER 3\n \n+#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n+/*\n+ * We don't expect any folios that exceed buddy sizes (and consequently\n+ * memory sections).\n+ */\n+#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n+#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n+/*\n+ * Only pages within a single memory section are guaranteed to be\n+ * contiguous. By limiting folios to a single memory section, all folio\n+ * pages are guaranteed to be contiguous.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n+#elif defined(CONFIG_HUGETLB_PAGE)\n+/*\n+ * There is no real limit on the folio size. We limit them to the maximum we\n+ * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n+ * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n+ */\n+#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n+#else\n+/*\n+ * Without hugetlb, gigantic folios that are bigger than a single PUD are\n+ * currently impossible.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n+#endif\n+\n+#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n+\n enum migratetype {\n \tMIGRATE_UNMOVABLE,\n \tMIGRATE_MOVABLE,\n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nInstead of passing down the head page and tail page index, pass the tail\nand head pages directly, as well as the order of the compound page.\n\nThis is a preparation for changing how the head position is encoded in\nthe tail page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/page-flags.h |  5 +++--\n mm/hugetlb.c               |  8 +++++---\n mm/internal.h              | 11 +++++------\n mm/mm_init.c               |  2 +-\n mm/page_alloc.c            |  2 +-\n 5 files changed, 15 insertions(+), 13 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex f7a0e4af0c73..5e7687ccccf8 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -865,9 +865,10 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page, struct page *head)\n+static __always_inline void set_compound_head(struct page *tail,\n+\t\tconst struct page *head, unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n+\tWRITE_ONCE(tail->compound_head, (unsigned long)head + 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 0beb6e22bc26..fc55f22c9e41 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -3168,6 +3168,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)\n \n /* Initialize [start_page:end_page_number] tail struct pages of a hugepage */\n static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n+\t\t\t\t\tstruct hstate *h,\n \t\t\t\t\tunsigned long start_page_number,\n \t\t\t\t\tunsigned long end_page_number)\n {\n@@ -3176,6 +3177,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \tstruct page *page = folio_page(folio, start_page_number);\n \tunsigned long head_pfn = folio_pfn(folio);\n \tunsigned long pfn, end_pfn = head_pfn + end_page_number;\n+\tunsigned int order = huge_page_order(h);\n \n \t/*\n \t * As we marked all tail pages with memblock_reserved_mark_noinit(),\n@@ -3183,7 +3185,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \t */\n \tfor (pfn = head_pfn + start_page_number; pfn < end_pfn; page++, pfn++) {\n \t\t__init_single_page(page, pfn, zone, nid);\n-\t\tprep_compound_tail((struct page *)folio, pfn - head_pfn);\n+\t\tprep_compound_tail(page, &folio->page, order);\n \t\tset_page_count(page, 0);\n \t}\n }\n@@ -3203,7 +3205,7 @@ static void __init hugetlb_folio_init_vmemmap(struct folio *folio,\n \t__folio_set_head(folio);\n \tret = folio_ref_freeze(folio, 1);\n \tVM_BUG_ON(!ret);\n-\thugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages);\n+\thugetlb_folio_init_tail_vmemmap(folio, h, 1, nr_pages);\n \tprep_compound_head(&folio->page, huge_page_order(h));\n }\n \n@@ -3260,7 +3262,7 @@ static void __init prep_and_add_bootmem_folios(struct hstate *h,\n \t\t\t * time as this is early in boot and there should\n \t\t\t * be no contention.\n \t\t\t */\n-\t\t\thugetlb_folio_init_tail_vmemmap(folio,\n+\t\t\thugetlb_folio_init_tail_vmemmap(folio, h,\n \t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_PAGES,\n \t\t\t\t\tpages_per_huge_page(h));\n \t\t}\ndiff --git a/mm/internal.h b/mm/internal.h\nindex cb0af847d7d9..c76122f22294 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -878,13 +878,12 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n }\n \n-static inline void prep_compound_tail(struct page *head, int tail_idx)\n+static inline void prep_compound_tail(struct page *tail,\n+\t\tconst struct page *head, unsigned int order)\n {\n-\tstruct page *p = head + tail_idx;\n-\n-\tp->mapping = TAIL_MAPPING;\n-\tset_compound_head(p, head);\n-\tset_page_private(p, 0);\n+\ttail->mapping = TAIL_MAPPING;\n+\tset_compound_head(tail, head, order);\n+\tset_page_private(tail, 0);\n }\n \n void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 61d983d23f55..0a12a9be0bcc 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -1099,7 +1099,7 @@ static void __ref memmap_init_compound(struct page *head,\n \t\tstruct page *page = pfn_to_page(pfn);\n \n \t\t__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);\n-\t\tprep_compound_tail(head, pfn - head_pfn);\n+\t\tprep_compound_tail(page, head, order);\n \t\tset_page_count(page, 0);\n \t}\n \tprep_compound_head(head, order);\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex fcc32737f451..aa657e4a99e8 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -744,7 +744,7 @@ void prep_compound_page(struct page *page, unsigned int order)\n \n \t__SetPageHead(page);\n \tfor (i = 1; i < nr_pages; i++)\n-\t\tprep_compound_tail(page, i);\n+\t\tprep_compound_tail(page + i, page, order);\n \n \tprep_compound_head(page, order);\n }\n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nThe 'compound_head' field in the 'struct page' encodes whether the page\nis a tail and where to locate the head page. Bit 0 is set if the page is\na tail, and the remaining bits in the field point to the head page.\n\nAs preparation for changing how the field encodes information about the\nhead page, rename the field to 'compound_info'.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n .../admin-guide/kdump/vmcoreinfo.rst          |  2 +-\n Documentation/mm/vmemmap_dedup.rst            |  6 +++---\n include/linux/mm_types.h                      | 20 +++++++++----------\n include/linux/page-flags.h                    | 18 ++++++++---------\n include/linux/types.h                         |  2 +-\n kernel/vmcore_info.c                          |  2 +-\n mm/page_alloc.c                               |  2 +-\n mm/slab.h                                     |  2 +-\n mm/util.c                                     |  2 +-\n 9 files changed, 28 insertions(+), 28 deletions(-)\n\ndiff --git a/Documentation/admin-guide/kdump/vmcoreinfo.rst b/Documentation/admin-guide/kdump/vmcoreinfo.rst\nindex 404a15f6782c..7663c610fe90 100644\n--- a/Documentation/admin-guide/kdump/vmcoreinfo.rst\n+++ b/Documentation/admin-guide/kdump/vmcoreinfo.rst\n@@ -141,7 +141,7 @@ nodemask_t\n The size of a nodemask_t type. Used to compute the number of online\n nodes.\n \n-(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_head)\n+(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_info)\n ----------------------------------------------------------------------------------\n \n User-space tools compute their values based on the offset of these\ndiff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst\nindex b4a55b6569fa..1863d88d2dcb 100644\n--- a/Documentation/mm/vmemmap_dedup.rst\n+++ b/Documentation/mm/vmemmap_dedup.rst\n@@ -24,7 +24,7 @@ For each base page, there is a corresponding ``struct page``.\n Within the HugeTLB subsystem, only the first 4 ``struct page`` are used to\n contain unique information about a HugeTLB page. ``__NR_USED_SUBPAGE`` provides\n this upper limit. The only 'useful' information in the remaining ``struct page``\n-is the compound_head field, and this field is the same for all tail pages.\n+is the compound_info field, and this field is the same for all tail pages.\n \n By removing redundant ``struct page`` for HugeTLB pages, memory can be returned\n to the buddy allocator for other uses.\n@@ -124,10 +124,10 @@ Here is how things look before optimization::\n  |           |\n  +-----------+\n \n-The value of page->compound_head is the same for all tail pages. The first\n+The value of page->compound_info is the same for all tail pages. The first\n page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4\n ``struct page`` necessary to describe the HugeTLB. The only use of the remaining\n-pages of ``struct page`` (page 1 to page 7) is to point to page->compound_head.\n+pages of ``struct page`` (page 1 to page 7) is to point to page->compound_info.\n Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``\n will be used for each HugeTLB page. This will allow us to free the remaining\n 7 pages to the buddy allocator.\ndiff --git a/include/linux/mm_types.h b/include/linux/mm_types.h\nindex 3cc8ae722886..7bc82a2b889f 100644\n--- a/include/linux/mm_types.h\n+++ b/include/linux/mm_types.h\n@@ -126,14 +126,14 @@ struct page {\n \t\t\tatomic_long_t pp_ref_count;\n \t\t};\n \t\tstruct {\t/* Tail pages of compound page */\n-\t\t\tunsigned long compound_head;\t/* Bit zero is set */\n+\t\t\tunsigned long compound_info;\t/* Bit zero is set */\n \t\t};\n \t\tstruct {\t/* ZONE_DEVICE pages */\n \t\t\t/*\n-\t\t\t * The first word is used for compound_head or folio\n+\t\t\t * The first word is used for compound_info or folio\n \t\t\t * pgmap\n \t\t\t */\n-\t\t\tvoid *_unused_pgmap_compound_head;\n+\t\t\tvoid *_unused_pgmap_compound_info;\n \t\t\tvoid *zone_device_data;\n \t\t\t/*\n \t\t\t * ZONE_DEVICE private pages are counted as being\n@@ -409,7 +409,7 @@ struct folio {\n \t/* private: avoid cluttering the output */\n \t\t\t\t/* For the Unevictable \"LRU list\" slot */\n \t\t\t\tstruct {\n-\t\t\t\t\t/* Avoid compound_head */\n+\t\t\t\t\t/* Avoid compound_info */\n \t\t\t\t\tvoid *__filler;\n \t/* public: */\n \t\t\t\t\tunsigned int mlock_count;\n@@ -510,7 +510,7 @@ struct folio {\n FOLIO_MATCH(flags, flags);\n FOLIO_MATCH(lru, lru);\n FOLIO_MATCH(mapping, mapping);\n-FOLIO_MATCH(compound_head, lru);\n+FOLIO_MATCH(compound_info, lru);\n FOLIO_MATCH(__folio_index, index);\n FOLIO_MATCH(private, private);\n FOLIO_MATCH(_mapcount, _mapcount);\n@@ -529,7 +529,7 @@ FOLIO_MATCH(_last_cpupid, _last_cpupid);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + sizeof(struct page))\n FOLIO_MATCH(flags, _flags_1);\n-FOLIO_MATCH(compound_head, _head_1);\n+FOLIO_MATCH(compound_info, _head_1);\n FOLIO_MATCH(_mapcount, _mapcount_1);\n FOLIO_MATCH(_refcount, _refcount_1);\n #undef FOLIO_MATCH\n@@ -537,13 +537,13 @@ FOLIO_MATCH(_refcount, _refcount_1);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 2 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_2);\n-FOLIO_MATCH(compound_head, _head_2);\n+FOLIO_MATCH(compound_info, _head_2);\n #undef FOLIO_MATCH\n #define FOLIO_MATCH(pg, fl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 3 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_3);\n-FOLIO_MATCH(compound_head, _head_3);\n+FOLIO_MATCH(compound_info, _head_3);\n #undef FOLIO_MATCH\n \n /**\n@@ -609,8 +609,8 @@ struct ptdesc {\n #define TABLE_MATCH(pg, pt)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))\n TABLE_MATCH(flags, pt_flags);\n-TABLE_MATCH(compound_head, pt_list);\n-TABLE_MATCH(compound_head, _pt_pad_1);\n+TABLE_MATCH(compound_info, pt_list);\n+TABLE_MATCH(compound_info, _pt_pad_1);\n TABLE_MATCH(mapping, __page_mapping);\n TABLE_MATCH(__folio_index, pt_index);\n TABLE_MATCH(rcu_head, pt_rcu_head);\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 5e7687ccccf8..70c4e43f2d9a 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -213,7 +213,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n-\t * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)\n+\t * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)\n \t * cold cacheline in some cases.\n \t */\n \tif (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &&\n@@ -223,7 +223,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_head);\n+\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n \n \t\tif (likely(head & 1))\n \t\t\treturn (const struct page *)(head - 1);\n@@ -281,7 +281,7 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_head);\n+\tunsigned long head = READ_ONCE(page->compound_info);\n \n \tif (unlikely(head & 1))\n \t\treturn head - 1;\n@@ -320,13 +320,13 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n static __always_inline int PageTail(const struct page *page)\n {\n-\treturn READ_ONCE(page->compound_head) & 1 || page_is_fake_head(page);\n+\treturn READ_ONCE(page->compound_info) & 1 || page_is_fake_head(page);\n }\n \n static __always_inline int PageCompound(const struct page *page)\n {\n \treturn test_bit(PG_head, &page->flags.f) ||\n-\t       READ_ONCE(page->compound_head) & 1;\n+\t       READ_ONCE(page->compound_info) & 1;\n }\n \n #define\tPAGE_POISON_PATTERN\t-1l\n@@ -348,7 +348,7 @@ static const unsigned long *const_folio_flags(const struct folio *folio,\n {\n \tconst struct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -357,7 +357,7 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)\n {\n \tstruct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -868,12 +868,12 @@ static inline bool folio_test_large(const struct folio *folio)\n static __always_inline void set_compound_head(struct page *tail,\n \t\tconst struct page *head, unsigned int order)\n {\n-\tWRITE_ONCE(tail->compound_head, (unsigned long)head + 1);\n+\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\n {\n-\tWRITE_ONCE(page->compound_head, 0);\n+\tWRITE_ONCE(page->compound_info, 0);\n }\n \n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\ndiff --git a/include/linux/types.h b/include/linux/types.h\nindex 7e71d260763c..608050dbca6a 100644\n--- a/include/linux/types.h\n+++ b/include/linux/types.h\n@@ -239,7 +239,7 @@ struct ustat {\n  *\n  * This guarantee is important for few reasons:\n  *  - future call_rcu_lazy() will make use of lower bits in the pointer;\n- *  - the structure shares storage space in struct page with @compound_head,\n+ *  - the structure shares storage space in struct page with @compound_info,\n  *    which encode PageTail() in bit 0. The guarantee is needed to avoid\n  *    false-positive PageTail().\n  */\ndiff --git a/kernel/vmcore_info.c b/kernel/vmcore_info.c\nindex 8d82913223a1..94e4ef75b1b2 100644\n--- a/kernel/vmcore_info.c\n+++ b/kernel/vmcore_info.c\n@@ -198,7 +198,7 @@ static int __init crash_save_vmcoreinfo_init(void)\n \tVMCOREINFO_OFFSET(page, lru);\n \tVMCOREINFO_OFFSET(page, _mapcount);\n \tVMCOREINFO_OFFSET(page, private);\n-\tVMCOREINFO_OFFSET(page, compound_head);\n+\tVMCOREINFO_OFFSET(page, compound_info);\n \tVMCOREINFO_OFFSET(pglist_data, node_zones);\n \tVMCOREINFO_OFFSET(pglist_data, nr_zones);\n #ifdef CONFIG_FLATMEM\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex aa657e4a99e8..e83f67fbbf07 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -731,7 +731,7 @@ static inline bool pcp_allowed_order(unsigned int order)\n  * The first PAGE_SIZE page is called the \"head page\" and have PG_head set.\n  *\n  * The remaining PAGE_SIZE pages are called \"tail pages\". PageTail() is encoded\n- * in bit 0 of page->compound_head. The rest of bits is pointer to head page.\n+ * in bit 0 of page->compound_info. The rest of bits is pointer to head page.\n  *\n  * The first tail page's ->compound_order holds the order of allocation.\n  * This usage means that zero-order pages may not be compound.\ndiff --git a/mm/slab.h b/mm/slab.h\nindex 71c7261bf822..62dfa50c1f01 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -94,7 +94,7 @@ struct slab {\n #define SLAB_MATCH(pg, sl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))\n SLAB_MATCH(flags, flags);\n-SLAB_MATCH(compound_head, slab_cache);\t/* Ensure bit 0 is clear */\n+SLAB_MATCH(compound_info, slab_cache);\t/* Ensure bit 0 is clear */\n SLAB_MATCH(_refcount, __page_refcount);\n #ifdef CONFIG_MEMCG\n SLAB_MATCH(memcg_data, obj_exts);\ndiff --git a/mm/util.c b/mm/util.c\nindex b05ab6f97e11..3ebcb9e6035c 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1247,7 +1247,7 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_head;\n+\thead = ps->page_snapshot.compound_info;\n \tif ((head & 1) == 0) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nMove set_compound_head() and clear_compound_head() to be adjacent to the\ncompound_head() function in page-flags.h.\n\nThese functions encode and decode the same compound_info field, so\nkeeping them together makes it easier to verify their logic is\nconsistent, especially when the encoding changes.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/page-flags.h | 22 +++++++++++-----------\n 1 file changed, 11 insertions(+), 11 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 70c4e43f2d9a..42bf8ed02a29 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -290,6 +290,17 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n \n+static __always_inline void set_compound_head(struct page *tail,\n+\t\tconst struct page *head, unsigned int order)\n+{\n+\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n+}\n+\n+static __always_inline void clear_compound_head(struct page *page)\n+{\n+\tWRITE_ONCE(page->compound_info, 0);\n+}\n+\n /**\n  * page_folio - Converts from page to folio.\n  * @p: The page.\n@@ -865,17 +876,6 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *tail,\n-\t\tconst struct page *head, unsigned int order)\n-{\n-\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n-}\n-\n-static __always_inline void clear_compound_head(struct page *page)\n-{\n-\tWRITE_ONCE(page->compound_info, 0);\n-}\n-\n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n static inline void ClearPageCompound(struct page *page)\n {\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-02-27",
              "message_id": "20260227193030.272078-1-kas@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author addressed a concern about aligning vmemmap to the newly introduced MAX_FOLIO_VMEMMAP_ALIGN, explaining that it's required for HugeTLB Vmemmap Optimization (HVO) and providing patch updates for riscv and loongarch architectures. The author also described how changing the encoding of compound_info from a pointer to a mask will allow identical tail pages to be mapped into the vmemmap of all pages, eliminating fake heads.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "addressed_concern",
                "provided_explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Kiryl Shutsemau <kas@kernel.org>\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to the newly introduced MAX_FOLIO_VMEMMAP_ALIGN.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/riscv/mm/init.c   |  3 ++-\n include/linux/mmzone.h | 11 +++++++++++\n 2 files changed, 13 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c\nindex 811e03786c56..e8fb2239a0b5 100644\n--- a/arch/riscv/mm/init.c\n+++ b/arch/riscv/mm/init.c\n@@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;\n EXPORT_SYMBOL(phys_ram_base);\n \n #ifdef CONFIG_SPARSEMEM_VMEMMAP\n-#define VMEMMAP_ADDR_ALIGN\t(1ULL << SECTION_SIZE_BITS)\n+#define VMEMMAP_ADDR_ALIGN\tmax(1ULL << SECTION_SIZE_BITS, \\\n+\t\t\t\t    MAX_FOLIO_VMEMMAP_ALIGN)\n \n unsigned long vmemmap_start_pfn __ro_after_init;\n EXPORT_SYMBOL(vmemmap_start_pfn);\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex be8ce40b5638..492a5be1090f 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -92,6 +92,17 @@\n \n #define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n \n+/*\n+ * HugeTLB Vmemmap Optimization (HVO) requires struct pages of the head page to\n+ * be naturally aligned with regard to the folio size.\n+ *\n+ * HVO which is only active if the size of struct page is a power of 2.\n+ */\n+#define MAX_FOLIO_VMEMMAP_ALIGN \\\n+\t(IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP) && \\\n+\t is_power_of_2(sizeof(struct page)) ? \\\n+\t MAX_FOLIO_NR_PAGES * sizeof(struct page) : 0)\n+\n enum migratetype {\n \tMIGRATE_UNMOVABLE,\n \tMIGRATE_MOVABLE,\n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_VMEMMAP_ALIGN.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/loongarch/include/asm/pgtable.h | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\nindex c33b3bcb733e..ea6c09eed2e7 100644\n--- a/arch/loongarch/include/asm/pgtable.h\n+++ b/arch/loongarch/include/asm/pgtable.h\n@@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n #endif\n \n-#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n+#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_VMEMMAP_ALIGN)\n+#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n #define VMEMMAP_END\t((unsigned long)vmemmap + VMEMMAP_SIZE - 1)\n \n #define KFENCE_AREA_START\t(VMEMMAP_END + 1)\n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nFor tail pages, the kernel uses the 'compound_info' field to get to the\nhead page. The bit 0 of the field indicates whether the page is a\ntail page, and if set, the remaining bits represent a pointer to the\nhead page.\n\nFor cases when size of struct page is power-of-2, change the encoding of\ncompound_info to store a mask that can be applied to the virtual address\nof the tail page in order to access the head page. It is possible\nbecause struct page of the head page is naturally aligned with regards\nto order of the page.\n\nThe significant impact of this modification is that all tail pages of\nthe same order will now have identical 'compound_info', regardless of\nthe compound page they are associated with. This paves the way for\neliminating fake heads.\n\nThe HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\napplied when the sizeof(struct page) is power-of-2. Having identical\ntail pages allows the same page to be mapped into the vmemmap of all\npages, maintaining memory savings without fake heads.\n\nIf sizeof(struct page) is not power-of-2, there is no functional\nchanges.\n\nLimit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\na difference. The approach with mask would work in the wider set of\nconditions, but it requires validating that struct pages are naturally\naligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (Arm) <david@kernel.org>\nAcked-by: Usama Arif <usamaarif642@gmail.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n mm/slab.h                  | 16 ++++++--\n mm/util.c                  | 16 ++++++--\n 3 files changed, 97 insertions(+), 16 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 42bf8ed02a29..01970bd38bff 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -198,6 +198,29 @@ enum pageflags {\n \n #ifndef __GENERATING_BOUNDS_H\n \n+/*\n+ * For tail pages, if the size of struct page is power-of-2 ->compound_info\n+ * encodes the mask that converts the address of the tail page address to\n+ * the head page address.\n+ *\n+ * Otherwise, ->compound_info has direct pointer to head pages.\n+ */\n+static __always_inline bool compound_info_has_mask(void)\n+{\n+\t/*\n+\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n+\t * makes a difference.\n+\t *\n+\t * The approach with mask would work in the wider set of conditions,\n+\t * but it requires validating that struct pages are naturally aligned\n+\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n+\t */\n+\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n+\t\treturn false;\n+\n+\treturn is_power_of_2(sizeof(struct page));\n+}\n+\n #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n \n@@ -207,6 +230,10 @@ DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n  */\n static __always_inline const struct page *page_fixed_fake_head(const struct page *page)\n {\n+\t/* Fake heads only exists if compound_info_has_mask() is true */\n+\tif (!compound_info_has_mask())\n+\t\treturn page;\n+\n \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n \t\treturn page;\n \n@@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n+\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n \n-\t\tif (likely(head & 1))\n-\t\t\treturn (const struct page *)(head - 1);\n+\t\t/* See set_compound_head() */\n+\t\tif (likely(info & 1)) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\treturn (const struct page *)(p & info);\n+\t\t}\n \t}\n \treturn page;\n }\n@@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_info);\n+\tunsigned long info = READ_ONCE(page->compound_info);\n \n-\tif (unlikely(head & 1))\n-\t\treturn head - 1;\n-\treturn (unsigned long)page_fixed_fake_head(page);\n+\t/* Bit 0 encodes PageTail() */\n+\tif (!(info & 1))\n+\t\treturn (unsigned long)page_fixed_fake_head(page);\n+\n+\t/*\n+\t * If compound_info_has_mask() is false, the rest of compound_info is\n+\t * the pointer to the head page.\n+\t */\n+\tif (!compound_info_has_mask())\n+\t\treturn info - 1;\n+\n+\t/*\n+\t * If compound_info_has_mask() is true the rest of the info encodes\n+\t * the mask that converts the address of the tail page to the head page.\n+\t *\n+\t * No need to clear bit 0 in the mask as 'page' always has it clear.\n+\t */\n+\treturn (unsigned long)page & info;\n }\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n@@ -293,7 +339,26 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n static __always_inline void set_compound_head(struct page *tail,\n \t\tconst struct page *head, unsigned int order)\n {\n-\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n+\tunsigned int shift;\n+\tunsigned long mask;\n+\n+\tif (!compound_info_has_mask()) {\n+\t\tWRITE_ONCE(tail->compound_info, (unsigned long)head | 1);\n+\t\treturn;\n+\t}\n+\n+\t/*\n+\t * If the size of struct page is power-of-2, bits [shift:0] of the\n+\t * virtual address of compound head are zero.\n+\t *\n+\t * Calculate mask that can be applied to the virtual address of\n+\t * the tail page to get address of the head page.\n+\t */\n+\tshift = order + order_base_2(sizeof(struct page));\n+\tmask = GENMASK(BITS_PER_LONG - 1, shift);\n+\n+\t/* Bit 0 encodes PageTail() */\n+\tWRITE_ONCE(tail->compound_info, mask | 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\ndiff --git a/mm/slab.h b/mm/slab.h\nindex 62dfa50c1f01..1a1b3758df05 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -131,11 +131,19 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist\n  */\n static inline struct slab *page_slab(const struct page *page)\n {\n-\tunsigned long head;\n+\tunsigned long info;\n+\n+\tinfo = READ_ONCE(page->compound_info);\n+\tif (info & 1) {\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\t\t\tpage = (struct page *)(p & info);\n+\t\t} else {\n+\t\t\tpage = (struct page *)(info - 1);\n+\t\t}\n+\t}\n \n-\thead = READ_ONCE(page->compound_head);\n-\tif (head & 1)\n-\t\tpage = (struct page *)(head - 1);\n \tif (data_race(page->page_type >> 24) != PGTY_slab)\n \t\tpage = NULL;\n \ndiff --git a/mm/util.c b/mm/util.c\nindex 3ebcb9e6035c..20dccf2881d7 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1237,7 +1237,7 @@ static void set_ps_flags(struct page_snapshot *ps, const struct folio *folio,\n  */\n void snapshot_page(struct page_snapshot *ps, const struct page *page)\n {\n-\tunsigned long head, nr_pages = 1;\n+\tunsigned long info, nr_pages = 1;\n \tstruct folio *foliop;\n \tint loops = 5;\n \n@@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_info;\n-\tif ((head & 1) == 0) {\n+\tinfo = ps->page_snapshot.compound_info;\n+\tif (!(info & 1)) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n \t\tif (!folio_test_large(foliop)) {\n@@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n \t\t}\n \t\tfoliop = (struct folio *)page;\n \t} else {\n-\t\tfoliop = (struct folio *)(head - 1);\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\tfoliop = (struct folio *)(p & info);\n+\t\t} else {\n+\t\t\tfoliop = (struct folio *)(info - 1);\n+\t\t}\n+\n \t\tps->idx = folio_page_idx(foliop, page);\n \t}\n \n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nIf page->compound_info encodes a mask, it is expected that vmemmap to be\nnaturally aligned to the maximum folio size.\n\nAdd a VM_WARN_ON_ONCE() to check the alignment.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n mm/sparse.c | 5 +++++\n 1 file changed, 5 insertions(+)\n\ndiff --git a/mm/sparse.c b/mm/sparse.c\nindex b5b2b6f7041b..dfabe554adf8 100644\n--- a/mm/sparse.c\n+++ b/mm/sparse.c\n@@ -600,6 +600,11 @@ void __init sparse_init(void)\n \tBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n \tmemblocks_present();\n \n+\tif (compound_info_has_mask()) {\n+\t\tVM_WARN_ON_ONCE(!IS_ALIGNED((unsigned long) pfn_to_page(0),\n+\t\t\t\t    MAX_FOLIO_VMEMMAP_ALIGN));\n+\t}\n+\n \tpnum_begin = first_present_section_nr();\n \tnid_begin = sparse_early_nid(__nr_to_section(pnum_begin));\n \n-- \n2.51.2\n\n---\n\nCurrently, the vmemmap for bootmem-allocated gigantic pages is populated\nearly in hugetlb_vmemmap_init_early(). However, the zone information is\nonly available after zones are initialized. If it is later discovered\nthat a page spans multiple zones, the HVO mapping must be undone and\nreplaced with a normal mapping using vmemmap_undo_hvo().\n\nDefer the actual vmemmap population to hugetlb_vmemmap_init_late(). At\nthis stage, zones are already initialized, so it can be checked if the\npage is valid for HVO before deciding how to populate the vmemmap.\n\nThis allows us to remove vmemmap_undo_hvo() and the complex logic\nrequired to rollback HVO mappings.\n\nIn hugetlb_vmemmap_init_late(), if HVO population fails or if the zones\nare invalid, fall back to a normal vmemmap population.\n\nPostponing population until hugetlb_vmemmap_init_late() also makes zone\ninformation available from within vmemmap_populate_hvo().\n\nSigned-off-by: Kiryl Shutsemau (Meta) <kas@kernel.org>\n---\n include/linux/mm.h   |  2 --\n mm/hugetlb_vmemmap.c | 37 +++++++++++++++----------------\n mm/sparse-vmemmap.c  | 53 --------------------------------------------\n 3 files changed, 18 insertions(+), 74 deletions(-)\n\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex 7f4dbbb9d783..0e2d45008ff4 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -4484,8 +4484,6 @@ int vmemmap_populate(unsigned long start, unsigned long end, int node,\n \t\tstruct vmem_altmap *altmap);\n int vmemmap_populate_hvo(unsigned long start, unsigned long end, int node,\n \t\t\t unsigned long headsize);\n-int vmemmap_undo_hvo(unsigned long start, unsigned long end, int node,\n-\t\t     unsigned long headsize);\n void vmemmap_wrprotect_hvo(unsigned long start, unsigned long end, int node,\n \t\t\t  unsigned long headsize);\n void vmemmap_populate_print_last(void);\ndiff --git a/mm/hugetlb_vmemmap.c b/mm/hugetlb_vmemmap.c\nindex a9280259e12a..935ec5829be9 100644\n--- a/mm/hugetlb_vmemmap.c\n+++ b/mm/hugetlb_vmemmap.c\n@@ -790,7 +790,6 @@ void __init hugetlb_vmemmap_init_early(int nid)\n {\n \tunsigned long psize, paddr, section_size;\n \tunsigned long ns, i, pnum, pfn, nr_pages;\n-\tunsigned long start, end;\n \tstruct huge_bootmem_page *m = NULL;\n \tvoid *map;\n \n@@ -808,14 +807,6 @@ void __init hugetlb_vmemmap_init_early(int nid)\n \t\tpaddr = virt_to_phys(m);\n \t\tpfn = PHYS_PFN(paddr);\n \t\tmap = pfn_to_page(pfn);\n-\t\tstart = (unsigned long)map;\n-\t\tend = start + nr_pages * sizeof(struct page);\n-\n-\t\tif (vmemmap_populate_hvo(start, end, nid,\n-\t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_SIZE) < 0)\n-\t\t\tcontinue;\n-\n-\t\tmemmap_boot_pages_add(HUGETLB_VMEMMAP_RESERVE_SIZE / PAGE_SIZE);\n \n \t\tpnum = pfn_to_section_nr(pfn);\n \t\tns = psize / section_size;\n@@ -850,28 +841,36 @@ void __init hugetlb_vmemmap_init_late(int nid)\n \t\th = m->hstate;\n \t\tpfn = PHYS_PFN(phys);\n \t\tnr_pages = pages_per_huge_page(h);\n+\t\tmap = pfn_to_page(pfn);\n+\t\tstart = (unsigned long)map;\n+\t\tend = start + nr_pages * sizeof(struct page);\n \n \t\tif (!hugetlb_bootmem_page_zones_valid(nid, m)) {\n \t\t\t/*\n \t\t\t * Oops, the hugetlb page spans multiple zones.\n-\t\t\t * Remove it from the list, and undo HVO.\n+\t\t\t * Remove it from the list, and populate it normally.\n \t\t\t */\n \t\t\tlist_del(&m->list);\n \n-\t\t\tmap = pfn_to_page(pfn);\n-\n-\t\t\tstart = (unsigned long)map;\n-\t\t\tend = start + nr_pages * sizeof(struct page);\n-\n-\t\t\tvmemmap_undo_hvo(start, end, nid,\n-\t\t\t\t\t HUGETLB_VMEMMAP_RESERVE_SIZE);\n-\t\t\tnr_mmap = end - start - HUGETLB_VMEMMAP_RESERVE_SIZE;\n+\t\t\tvmemmap_populate(start, end, nid, NULL);\n+\t\t\tnr_mmap = end - start;\n \t\t\tmemmap_boot_pages_add(DIV_ROUND_UP(nr_mmap, PAGE_SIZE));\n \n \t\t\tmemblock_phys_free(phys, huge_page_size(h));\n \t\t\tcontinue;\n-\t\t} else\n+\t\t}\n+\n+\t\tif (vmemmap_populate_hvo(start, end, nid,\n+\t\t\t\t\t HUGETLB_VMEMMAP_RESERVE_SIZE) < 0) {\n+\t\t\t/* Fallback if HVO population fails */\n+\t\t\tvmemmap_populate(start, end, nid, NULL);\n+\t\t\tnr_mmap = end - start;\n+\t\t} else {\n \t\t\tm->flags |= HUGE_BOOTMEM_ZONES_VALID;\n+\t\t\tnr_mmap = HUGETLB_VMEMMAP_RESERVE_SIZE;\n+\t\t}\n+\n+\t\tmemmap_boot_pages_add(DIV_ROUND_UP(nr_mmap, PAGE_SIZE));\n \t}\n }\n #endif\ndiff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c\nindex 37522d6cb398..032a81450838 100644\n--- a/mm/sparse-vmemmap.c\n+++ b/mm/sparse-vmemmap.c\n@@ -302,59 +302,6 @@ int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,\n \treturn vmemmap_populate_range(start, end, node, altmap, -1, 0);\n }\n \n-/*\n- * Undo populate_hvo, and replace it with a normal base page mapping.\n- * Used in memory init in case a HVO mapping needs to be undone.\n- *\n- * This can happen when it is discovered that a memblock allocated\n- * hugetlb page spans multiple zones, which can only be verified\n- * after zones have been initialized.\n- *\n- * We know that:\n- * 1) The first @headsize / PAGE_SIZE vmemmap pages were individually\n- *    allocated through memblock, and mapped.\n- *\n- * 2) The rest of the vmemmap pages are mirrors of the last head page.\n- */\n-int __meminit vmemmap_undo_hvo(unsigned long addr, unsigned long end,\n-\t\t\t\t      int node, unsigned long headsize)\n-{\n-\tunsigned long maddr, pfn;\n-\tpte_t *pte;\n-\tint headpages;\n-\n-\t/*\n-\t * Should only be called early in boot, so nothing will\n-\t * be accessing these page structures.\n-\t */\n-\tWARN_ON(!early_boot_irqs_disabled);\n-\n-\theadpages = headsize >> PAGE_SHIFT;\n-\n-\t/*\n-\t * Clear mirrored mappings for tail page structs.\n-\t */\n-\tfor (maddr = addr + headsize; maddr < end; maddr += PAGE_SIZE) {\n-\t\tpte = virt_to_kpte(maddr);\n-\t\tpte_clear(&init_mm, maddr, pte);\n-\t}\n-\n-\t/*\n-\t * Clear and free mappings for head page and first tail page\n-\t * structs.\n-\t */\n-\tfor (maddr = addr; headpages-- > 0; maddr += PAGE_SIZE) {\n-\t\tpte = virt_to_kpte(maddr);\n-\t\tpfn = pte_pfn(ptep_get(pte));\n-\t\tpte_clear(&init_mm, maddr, pte);\n-\t\tmemblock_phys_free(PFN_PHYS(pfn), PAGE_SIZE);\n-\t}\n-\n-\tflush_tlb_kernel_range(addr, end);\n-\n-\treturn vmemmap_populate(addr, end, node, NULL);\n-}\n-\n /*\n  * Write protect the mirrored tail page structs for HVO. This will be\n  * called from the hugetlb code when gathering and initializing the\n-- \n2.51.2",
              "reply_to": "",
              "message_date": "2026-02-27",
              "message_id": "20260227193030.272078-5-kas@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author addressed a concern about zone information being correct for shared tail pages, explaining that in v7, these pages are allocated per-zone and the vmemmap population is deferred until zones are initialized.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This series removes \"fake head pages\" from the HugeTLB vmemmap\noptimization (HVO) by changing how tail pages encode their relationship\nto the head page.\n\nIt simplifies compound_head() and page_ref_add_unless(). Both are in the\nhot path.\n\nBackground\n==========\n\nHVO reduces memory overhead by freeing vmemmap pages for HugeTLB pages\nand remapping the freed virtual addresses to a single physical page.\nPreviously, all tail page vmemmap entries were remapped to the first\nvmemmap page (containing the head struct page), creating \"fake heads\" -\ntail pages that appear to have PG_head set when accessed through the\ndeduplicated vmemmap.\n\nThis required special handling in compound_head() to detect and work\naround fake heads, adding complexity and overhead to a very hot path.\n\nNew Approach\n============\n\nFor architectures/configs where sizeof(struct page) is a power of 2 (the\ncommon case), this series changes how position of the head page is encoded\nin the tail pages.\n\nInstead of storing a pointer to the head page, the ->compound_info\n(renamed from ->compound_head) now stores a mask.\n\nThe mask can be applied to any tail page's virtual address to compute\nthe head page address. Critically, all tail pages of the same order now\nhave identical compound_info values, regardless of which compound page\nthey belong to.\n\nThe key insight is that all tail pages of the same order now have\nidentical compound_info values, regardless of which compound page they\nbelong to.\n\nIn v7, these shared tail pages are allocated per-zone. This ensures \nthat zone information (stored in page->flags) is correct even for \nshared tail pages, removing the need for the special-casing in \npage_zonenum() proposed in earlier versions.\n\nTo support per-zone shared pages for boot-allocated gigantic pages, \nthe vmemmap population is deferred until zones are initialized. This \nsimplifies the logic significantly and allows the removal of \nvmemmap_undo_hvo().\n\nBenefits\n========\n\n1. Simplified compound_head(): No fake head detection needed, can be\n   implemented in a branchless manner.\n\n2. Simplified page_ref_add_unless(): RCU protection removed since there's\n   no race with fake head remapping.\n\n3. Cleaner architecture: The shared tail pages are truly read-only and\n   contain valid tail page metadata.\n\nIf sizeof(struct page) is not power-of-2, there are no functional changes.\nHVO is not supported in this configuration.\n\nI had hoped to see performance improvement, but my testing thus far has\nshown either no change or only a slight improvement within the noise.\n\nSeries Organization\n===================\n\nPatch 1: Move MAX_FOLIO_ORDER definition to mmzone.h.\nPatches 2-4: Refactoring of field names and interfaces.\nPatches 5-6: Architecture alignment for LoongArch and RISC-V.\nPatch 7: Mask-based compound_head() implementation.\nPatch 8: Add memmap alignment checks.\nPatch 9: Branchless compound_head() optimization.\nPatch 10: Defer vmemmap population for bootmem hugepages.\nPatch 11: Refactor vmemmap_walk.\nPatch 12: x86 vDSO build fix.\nPatch 13: Eliminate fake heads with per-zone shared tail pages.\nPatches 14-16: Cleanup of fake head infrastructure.\nPatch 17: Documentation update.\nPatch 18: Use compound_head() in page_slab().\n\nChanges in v7:\n==============\n\n  - Move vmemmap_tails from per-node to per-zone. This ensures tail\n    pages have correct zone information.\n\n  - Defer vmemmap population for boot-allocated huge pages to \n    hugetlb_vmemmap_init_late(). This makes zone information available \n    during population and allows removing vmemmap_undo_hvo().\n\n  - Undefine CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP for x86 vdso32 to \n    fix build issues.\n\n  - Remove the patch that modified page_zonenum(), as per-zone \n    shared pages make it unnecessary.\n\nChanges in v6:\n==============\n  - Simplify memmap alignment check in mm/sparse.c: use VM_BUG_ON()\n    (Muchun)\n\n  - Store struct page pointers in vmemmap_tails[] instead of PFNs.\n    (Muchun)\n\n  - Fix build error on powerpc due to negative NR_VMEMMAP_TAILS.\n\nChanges in v5:\n==============\n  - Rebased to mm-everything-2026-01-27-04-35\n\n  - Add arch-specific patches to align vmemmap to maximal folio size\n    for riscv and LoongArch architectures.\n\n  - Strengthen the memmap alignment check in mm/sparse.c: use BUG()\n    for CONFIG_DEBUG_VM, WARN() otherwise. (Muchun)\n\n  - Use cmpxchg() instead of hugetlb_lock to update vmemmap_tails\n    array. (Muchun)\n\n  - Update page_slab().\n\nChanges in v4:\n==============\n  - Fix build issues due to linux/mmzone.h <-> linux/pgtable.h\n    dependency loop by avoiding including linux/pgtable.h into\n    linux/mmzone.h\n\n  - Rework vmemmap_remap_alloc() interface. (Muchun)\n\n  - Use &folio->page instead of folio address for optimization\n    target. (Muchun)\n\nChanges in v3:\n==============\n  - Fixed error recovery path in vmemmap_remap_free() to pass correct start\n    address for TLB flush. (Muchun)\n\n  - Wrapped the mask-based compound_info encoding within CONFIG_SPARSEMEM_VMEMMAP\n    check via compound_info_has_mask(). For other memory models, alignment\n    guarantees are harder to verify. (Muchun)\n\n  - Updated vmemmap_dedup.rst documentation wording: changed \"vmemmap_tail\n    shared for the struct hstate\" to \"A single, per-node page frame shared\n    among all hugepages of the same size\". (Muchun)\n\n  - Fixed build error with MAX_FOLIO_ORDER expanding to undefined PUD_ORDER\n    in certain configurations. (kernel test robot)\n\nChanges in v2:\n==============\n\n- Handle boot-allocated huge pages correctly. (Frank)\n\n- Changed from per-hstate vmemmap_tail to per-node vmemmap_tails[] array\n  in pglist_data. (Muchun)\n\n- Added spin_lock(&hugetlb_lock) protection in vmemmap_get_tail() to fix\n  a race condition where two threads could both allocate tail pages.\n  The losing thread now properly frees its allocated page. (Usama)\n\n- Add warning if memmap is not aligned to MAX_FOLIO_SIZE, which is\n  required for the mask approach. (Muchun)\n\n- Make page_zonenum() use head page - correctness fix since shared\n  tail pages cannot have valid zone information. (Muchun)\n\n- Added 'const' qualifier to head parameter in set_compound_head() and\n  prep_compound_tail(). (Usama)\n\n- Updated commit messages.\n\nKiryl Shutsemau (16):\n  mm: Move MAX_FOLIO_ORDER definition to mmzone.h\n  mm: Change the interface of prep_compound_tail()\n  mm: Rename the 'compound_head' field in the 'struct page' to\n    'compound_info'\n  mm: Move set/clear_compound_head() next to compound_head()\n  riscv/mm: Align vmemmap to maximal folio size\n  LoongArch/mm: Align vmemmap to maximal folio size\n  mm: Rework compound_head() for power-of-2 sizeof(struct page)\n  mm/sparse: Check memmap alignment for compound_info_has_mask()\n  mm/hugetlb: Refactor code around vmemmap_walk\n  mm/hugetlb: Remove fake head pages\n  mm: Drop fake head checks\n  hugetlb: Remove VMEMMAP_SYNCHRONIZE_RCU\n  mm/hugetlb: Remove hugetlb_optimize_vmemmap_key static key\n  mm: Remove the branch from compound_head()\n  hugetlb: Update vmemmap_dedup.rst\n  mm/slab: Use compound_head() in page_slab()\n\nKiryl Shutsemau (Meta) (2):\n  mm/hugetlb: Defer vmemmap population for bootmem hugepages\n  x86/vdso: Undefine CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP for vdso32\n\n .../admin-guide/kdump/vmcoreinfo.rst          |   2 +-\n Documentation/mm/vmemmap_dedup.rst            |  62 ++-\n arch/loongarch/include/asm/pgtable.h          |   3 +-\n arch/riscv/mm/init.c                          |   3 +-\n arch/x86/entry/vdso/vdso32/fake_32bit_build.h |   1 +\n include/linux/mm.h                            |  36 +-\n include/linux/mm_types.h                      |  20 +-\n include/linux/mmzone.h                        |  57 +++\n include/linux/page-flags.h                    | 166 ++++----\n include/linux/page_ref.h                      |   8 +-\n include/linux/types.h                         |   2 +-\n kernel/vmcore_info.c                          |   2 +-\n mm/hugetlb.c                                  |   8 +-\n mm/hugetlb_vmemmap.c                          | 362 +++++++++---------\n mm/internal.h                                 |  18 +-\n mm/mm_init.c                                  |   2 +-\n mm/page_alloc.c                               |   4 +-\n mm/slab.h                                     |   8 +-\n mm/sparse-vmemmap.c                           | 110 +++---\n mm/sparse.c                                   |   5 +\n mm/util.c                                     |  16 +-\n 21 files changed, 448 insertions(+), 447 deletions(-)\n\n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nInstead of passing down the head page and tail page index, pass the tail\nand head pages directly, as well as the order of the compound page.\n\nThis is a preparation for changing how the head position is encoded in\nthe tail page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/page-flags.h |  5 +++--\n mm/hugetlb.c               |  8 +++++---\n mm/internal.h              | 11 +++++------\n mm/mm_init.c               |  2 +-\n mm/page_alloc.c            |  2 +-\n 5 files changed, 15 insertions(+), 13 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex f7a0e4af0c73..5e7687ccccf8 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -865,9 +865,10 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page, struct page *head)\n+static __always_inline void set_compound_head(struct page *tail,\n+\t\tconst struct page *head, unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n+\tWRITE_ONCE(tail->compound_head, (unsigned long)head + 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 0beb6e22bc26..fc55f22c9e41 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -3168,6 +3168,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)\n \n /* Initialize [start_page:end_page_number] tail struct pages of a hugepage */\n static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n+\t\t\t\t\tstruct hstate *h,\n \t\t\t\t\tunsigned long start_page_number,\n \t\t\t\t\tunsigned long end_page_number)\n {\n@@ -3176,6 +3177,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \tstruct page *page = folio_page(folio, start_page_number);\n \tunsigned long head_pfn = folio_pfn(folio);\n \tunsigned long pfn, end_pfn = head_pfn + end_page_number;\n+\tunsigned int order = huge_page_order(h);\n \n \t/*\n \t * As we marked all tail pages with memblock_reserved_mark_noinit(),\n@@ -3183,7 +3185,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \t */\n \tfor (pfn = head_pfn + start_page_number; pfn < end_pfn; page++, pfn++) {\n \t\t__init_single_page(page, pfn, zone, nid);\n-\t\tprep_compound_tail((struct page *)folio, pfn - head_pfn);\n+\t\tprep_compound_tail(page, &folio->page, order);\n \t\tset_page_count(page, 0);\n \t}\n }\n@@ -3203,7 +3205,7 @@ static void __init hugetlb_folio_init_vmemmap(struct folio *folio,\n \t__folio_set_head(folio);\n \tret = folio_ref_freeze(folio, 1);\n \tVM_BUG_ON(!ret);\n-\thugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages);\n+\thugetlb_folio_init_tail_vmemmap(folio, h, 1, nr_pages);\n \tprep_compound_head(&folio->page, huge_page_order(h));\n }\n \n@@ -3260,7 +3262,7 @@ static void __init prep_and_add_bootmem_folios(struct hstate *h,\n \t\t\t * time as this is early in boot and there should\n \t\t\t * be no contention.\n \t\t\t */\n-\t\t\thugetlb_folio_init_tail_vmemmap(folio,\n+\t\t\thugetlb_folio_init_tail_vmemmap(folio, h,\n \t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_PAGES,\n \t\t\t\t\tpages_per_huge_page(h));\n \t\t}\ndiff --git a/mm/internal.h b/mm/internal.h\nindex cb0af847d7d9..c76122f22294 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -878,13 +878,12 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n }\n \n-static inline void prep_compound_tail(struct page *head, int tail_idx)\n+static inline void prep_compound_tail(struct page *tail,\n+\t\tconst struct page *head, unsigned int order)\n {\n-\tstruct page *p = head + tail_idx;\n-\n-\tp->mapping = TAIL_MAPPING;\n-\tset_compound_head(p, head);\n-\tset_page_private(p, 0);\n+\ttail->mapping = TAIL_MAPPING;\n+\tset_compound_head(tail, head, order);\n+\tset_page_private(tail, 0);\n }\n \n void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 61d983d23f55..0a12a9be0bcc 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -1099,7 +1099,7 @@ static void __ref memmap_init_compound(struct page *head,\n \t\tstruct page *page = pfn_to_page(pfn);\n \n \t\t__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);\n-\t\tprep_compound_tail(head, pfn - head_pfn);\n+\t\tprep_compound_tail(page, head, order);\n \t\tset_page_count(page, 0);\n \t}\n \tprep_compound_head(head, order);\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex fcc32737f451..aa657e4a99e8 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -744,7 +744,7 @@ void prep_compound_page(struct page *page, unsigned int order)\n \n \t__SetPageHead(page);\n \tfor (i = 1; i < nr_pages; i++)\n-\t\tprep_compound_tail(page, i);\n+\t\tprep_compound_tail(page + i, page, order);\n \n \tprep_compound_head(page, order);\n }\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-27",
              "message_id": "20260202155634.650837-1-kas@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author addressed a concern about the naming of the 'compound_head' field in struct page, explaining that it encodes whether the page is a tail and where to locate the head page. The author renamed the field to 'compound_info', which will be used for the new mask-based encoding.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Kiryl Shutsemau <kas@kernel.org>\n\nThe 'compound_head' field in the 'struct page' encodes whether the page\nis a tail and where to locate the head page. Bit 0 is set if the page is\na tail, and the remaining bits in the field point to the head page.\n\nAs preparation for changing how the field encodes information about the\nhead page, rename the field to 'compound_info'.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n .../admin-guide/kdump/vmcoreinfo.rst          |  2 +-\n Documentation/mm/vmemmap_dedup.rst            |  6 +++---\n include/linux/mm_types.h                      | 20 +++++++++----------\n include/linux/page-flags.h                    | 18 ++++++++---------\n include/linux/types.h                         |  2 +-\n kernel/vmcore_info.c                          |  2 +-\n mm/page_alloc.c                               |  2 +-\n mm/slab.h                                     |  2 +-\n mm/util.c                                     |  2 +-\n 9 files changed, 28 insertions(+), 28 deletions(-)\n\ndiff --git a/Documentation/admin-guide/kdump/vmcoreinfo.rst b/Documentation/admin-guide/kdump/vmcoreinfo.rst\nindex 404a15f6782c..7663c610fe90 100644\n--- a/Documentation/admin-guide/kdump/vmcoreinfo.rst\n+++ b/Documentation/admin-guide/kdump/vmcoreinfo.rst\n@@ -141,7 +141,7 @@ nodemask_t\n The size of a nodemask_t type. Used to compute the number of online\n nodes.\n \n-(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_head)\n+(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_info)\n ----------------------------------------------------------------------------------\n \n User-space tools compute their values based on the offset of these\ndiff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst\nindex b4a55b6569fa..1863d88d2dcb 100644\n--- a/Documentation/mm/vmemmap_dedup.rst\n+++ b/Documentation/mm/vmemmap_dedup.rst\n@@ -24,7 +24,7 @@ For each base page, there is a corresponding ``struct page``.\n Within the HugeTLB subsystem, only the first 4 ``struct page`` are used to\n contain unique information about a HugeTLB page. ``__NR_USED_SUBPAGE`` provides\n this upper limit. The only 'useful' information in the remaining ``struct page``\n-is the compound_head field, and this field is the same for all tail pages.\n+is the compound_info field, and this field is the same for all tail pages.\n \n By removing redundant ``struct page`` for HugeTLB pages, memory can be returned\n to the buddy allocator for other uses.\n@@ -124,10 +124,10 @@ Here is how things look before optimization::\n  |           |\n  +-----------+\n \n-The value of page->compound_head is the same for all tail pages. The first\n+The value of page->compound_info is the same for all tail pages. The first\n page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4\n ``struct page`` necessary to describe the HugeTLB. The only use of the remaining\n-pages of ``struct page`` (page 1 to page 7) is to point to page->compound_head.\n+pages of ``struct page`` (page 1 to page 7) is to point to page->compound_info.\n Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``\n will be used for each HugeTLB page. This will allow us to free the remaining\n 7 pages to the buddy allocator.\ndiff --git a/include/linux/mm_types.h b/include/linux/mm_types.h\nindex 3cc8ae722886..7bc82a2b889f 100644\n--- a/include/linux/mm_types.h\n+++ b/include/linux/mm_types.h\n@@ -126,14 +126,14 @@ struct page {\n \t\t\tatomic_long_t pp_ref_count;\n \t\t};\n \t\tstruct {\t/* Tail pages of compound page */\n-\t\t\tunsigned long compound_head;\t/* Bit zero is set */\n+\t\t\tunsigned long compound_info;\t/* Bit zero is set */\n \t\t};\n \t\tstruct {\t/* ZONE_DEVICE pages */\n \t\t\t/*\n-\t\t\t * The first word is used for compound_head or folio\n+\t\t\t * The first word is used for compound_info or folio\n \t\t\t * pgmap\n \t\t\t */\n-\t\t\tvoid *_unused_pgmap_compound_head;\n+\t\t\tvoid *_unused_pgmap_compound_info;\n \t\t\tvoid *zone_device_data;\n \t\t\t/*\n \t\t\t * ZONE_DEVICE private pages are counted as being\n@@ -409,7 +409,7 @@ struct folio {\n \t/* private: avoid cluttering the output */\n \t\t\t\t/* For the Unevictable \"LRU list\" slot */\n \t\t\t\tstruct {\n-\t\t\t\t\t/* Avoid compound_head */\n+\t\t\t\t\t/* Avoid compound_info */\n \t\t\t\t\tvoid *__filler;\n \t/* public: */\n \t\t\t\t\tunsigned int mlock_count;\n@@ -510,7 +510,7 @@ struct folio {\n FOLIO_MATCH(flags, flags);\n FOLIO_MATCH(lru, lru);\n FOLIO_MATCH(mapping, mapping);\n-FOLIO_MATCH(compound_head, lru);\n+FOLIO_MATCH(compound_info, lru);\n FOLIO_MATCH(__folio_index, index);\n FOLIO_MATCH(private, private);\n FOLIO_MATCH(_mapcount, _mapcount);\n@@ -529,7 +529,7 @@ FOLIO_MATCH(_last_cpupid, _last_cpupid);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + sizeof(struct page))\n FOLIO_MATCH(flags, _flags_1);\n-FOLIO_MATCH(compound_head, _head_1);\n+FOLIO_MATCH(compound_info, _head_1);\n FOLIO_MATCH(_mapcount, _mapcount_1);\n FOLIO_MATCH(_refcount, _refcount_1);\n #undef FOLIO_MATCH\n@@ -537,13 +537,13 @@ FOLIO_MATCH(_refcount, _refcount_1);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 2 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_2);\n-FOLIO_MATCH(compound_head, _head_2);\n+FOLIO_MATCH(compound_info, _head_2);\n #undef FOLIO_MATCH\n #define FOLIO_MATCH(pg, fl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 3 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_3);\n-FOLIO_MATCH(compound_head, _head_3);\n+FOLIO_MATCH(compound_info, _head_3);\n #undef FOLIO_MATCH\n \n /**\n@@ -609,8 +609,8 @@ struct ptdesc {\n #define TABLE_MATCH(pg, pt)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))\n TABLE_MATCH(flags, pt_flags);\n-TABLE_MATCH(compound_head, pt_list);\n-TABLE_MATCH(compound_head, _pt_pad_1);\n+TABLE_MATCH(compound_info, pt_list);\n+TABLE_MATCH(compound_info, _pt_pad_1);\n TABLE_MATCH(mapping, __page_mapping);\n TABLE_MATCH(__folio_index, pt_index);\n TABLE_MATCH(rcu_head, pt_rcu_head);\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 5e7687ccccf8..70c4e43f2d9a 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -213,7 +213,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n-\t * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)\n+\t * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)\n \t * cold cacheline in some cases.\n \t */\n \tif (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &&\n@@ -223,7 +223,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_head);\n+\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n \n \t\tif (likely(head & 1))\n \t\t\treturn (const struct page *)(head - 1);\n@@ -281,7 +281,7 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_head);\n+\tunsigned long head = READ_ONCE(page->compound_info);\n \n \tif (unlikely(head & 1))\n \t\treturn head - 1;\n@@ -320,13 +320,13 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n static __always_inline int PageTail(const struct page *page)\n {\n-\treturn READ_ONCE(page->compound_head) & 1 || page_is_fake_head(page);\n+\treturn READ_ONCE(page->compound_info) & 1 || page_is_fake_head(page);\n }\n \n static __always_inline int PageCompound(const struct page *page)\n {\n \treturn test_bit(PG_head, &page->flags.f) ||\n-\t       READ_ONCE(page->compound_head) & 1;\n+\t       READ_ONCE(page->compound_info) & 1;\n }\n \n #define\tPAGE_POISON_PATTERN\t-1l\n@@ -348,7 +348,7 @@ static const unsigned long *const_folio_flags(const struct folio *folio,\n {\n \tconst struct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -357,7 +357,7 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)\n {\n \tstruct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -868,12 +868,12 @@ static inline bool folio_test_large(const struct folio *folio)\n static __always_inline void set_compound_head(struct page *tail,\n \t\tconst struct page *head, unsigned int order)\n {\n-\tWRITE_ONCE(tail->compound_head, (unsigned long)head + 1);\n+\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\n {\n-\tWRITE_ONCE(page->compound_head, 0);\n+\tWRITE_ONCE(page->compound_info, 0);\n }\n \n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\ndiff --git a/include/linux/types.h b/include/linux/types.h\nindex 7e71d260763c..608050dbca6a 100644\n--- a/include/linux/types.h\n+++ b/include/linux/types.h\n@@ -239,7 +239,7 @@ struct ustat {\n  *\n  * This guarantee is important for few reasons:\n  *  - future call_rcu_lazy() will make use of lower bits in the pointer;\n- *  - the structure shares storage space in struct page with @compound_head,\n+ *  - the structure shares storage space in struct page with @compound_info,\n  *    which encode PageTail() in bit 0. The guarantee is needed to avoid\n  *    false-positive PageTail().\n  */\ndiff --git a/kernel/vmcore_info.c b/kernel/vmcore_info.c\nindex 8d82913223a1..94e4ef75b1b2 100644\n--- a/kernel/vmcore_info.c\n+++ b/kernel/vmcore_info.c\n@@ -198,7 +198,7 @@ static int __init crash_save_vmcoreinfo_init(void)\n \tVMCOREINFO_OFFSET(page, lru);\n \tVMCOREINFO_OFFSET(page, _mapcount);\n \tVMCOREINFO_OFFSET(page, private);\n-\tVMCOREINFO_OFFSET(page, compound_head);\n+\tVMCOREINFO_OFFSET(page, compound_info);\n \tVMCOREINFO_OFFSET(pglist_data, node_zones);\n \tVMCOREINFO_OFFSET(pglist_data, nr_zones);\n #ifdef CONFIG_FLATMEM\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex aa657e4a99e8..e83f67fbbf07 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -731,7 +731,7 @@ static inline bool pcp_allowed_order(unsigned int order)\n  * The first PAGE_SIZE page is called the \"head page\" and have PG_head set.\n  *\n  * The remaining PAGE_SIZE pages are called \"tail pages\". PageTail() is encoded\n- * in bit 0 of page->compound_head. The rest of bits is pointer to head page.\n+ * in bit 0 of page->compound_info. The rest of bits is pointer to head page.\n  *\n  * The first tail page's ->compound_order holds the order of allocation.\n  * This usage means that zero-order pages may not be compound.\ndiff --git a/mm/slab.h b/mm/slab.h\nindex 71c7261bf822..62dfa50c1f01 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -94,7 +94,7 @@ struct slab {\n #define SLAB_MATCH(pg, sl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))\n SLAB_MATCH(flags, flags);\n-SLAB_MATCH(compound_head, slab_cache);\t/* Ensure bit 0 is clear */\n+SLAB_MATCH(compound_info, slab_cache);\t/* Ensure bit 0 is clear */\n SLAB_MATCH(_refcount, __page_refcount);\n #ifdef CONFIG_MEMCG\n SLAB_MATCH(memcg_data, obj_exts);\ndiff --git a/mm/util.c b/mm/util.c\nindex b05ab6f97e11..3ebcb9e6035c 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1247,7 +1247,7 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_head;\n+\thead = ps->page_snapshot.compound_info;\n \tif ((head & 1) == 0) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nMove set_compound_head() and clear_compound_head() to be adjacent to the\ncompound_head() function in page-flags.h.\n\nThese functions encode and decode the same compound_info field, so\nkeeping them together makes it easier to verify their logic is\nconsistent, especially when the encoding changes.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/page-flags.h | 22 +++++++++++-----------\n 1 file changed, 11 insertions(+), 11 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 70c4e43f2d9a..42bf8ed02a29 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -290,6 +290,17 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n \n+static __always_inline void set_compound_head(struct page *tail,\n+\t\tconst struct page *head, unsigned int order)\n+{\n+\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n+}\n+\n+static __always_inline void clear_compound_head(struct page *page)\n+{\n+\tWRITE_ONCE(page->compound_info, 0);\n+}\n+\n /**\n  * page_folio - Converts from page to folio.\n  * @p: The page.\n@@ -865,17 +876,6 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *tail,\n-\t\tconst struct page *head, unsigned int order)\n-{\n-\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n-}\n-\n-static __always_inline void clear_compound_head(struct page *page)\n-{\n-\tWRITE_ONCE(page->compound_info, 0);\n-}\n-\n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n static inline void ClearPageCompound(struct page *page)\n {\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to the newly introduced MAX_FOLIO_VMEMMAP_ALIGN.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/riscv/mm/init.c   |  3 ++-\n include/linux/mmzone.h | 11 +++++++++++\n 2 files changed, 13 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c\nindex 811e03786c56..e8fb2239a0b5 100644\n--- a/arch/riscv/mm/init.c\n+++ b/arch/riscv/mm/init.c\n@@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;\n EXPORT_SYMBOL(phys_ram_base);\n \n #ifdef CONFIG_SPARSEMEM_VMEMMAP\n-#define VMEMMAP_ADDR_ALIGN\t(1ULL << SECTION_SIZE_BITS)\n+#define VMEMMAP_ADDR_ALIGN\tmax(1ULL << SECTION_SIZE_BITS, \\\n+\t\t\t\t    MAX_FOLIO_VMEMMAP_ALIGN)\n \n unsigned long vmemmap_start_pfn __ro_after_init;\n EXPORT_SYMBOL(vmemmap_start_pfn);\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex be8ce40b5638..492a5be1090f 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -92,6 +92,17 @@\n \n #define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n \n+/*\n+ * HugeTLB Vmemmap Optimization (HVO) requires struct pages of the head page to\n+ * be naturally aligned with regard to the folio size.\n+ *\n+ * HVO which is only active if the size of struct page is a power of 2.\n+ */\n+#define MAX_FOLIO_VMEMMAP_ALIGN \\\n+\t(IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP) && \\\n+\t is_power_of_2(sizeof(struct page)) ? \\\n+\t MAX_FOLIO_NR_PAGES * sizeof(struct page) : 0)\n+\n enum migratetype {\n \tMIGRATE_UNMOVABLE,\n \tMIGRATE_MOVABLE,\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_VMEMMAP_ALIGN.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/loongarch/include/asm/pgtable.h | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\nindex c33b3bcb733e..ea6c09eed2e7 100644\n--- a/arch/loongarch/include/asm/pgtable.h\n+++ b/arch/loongarch/include/asm/pgtable.h\n@@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n #endif\n \n-#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n+#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_VMEMMAP_ALIGN)\n+#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n #define VMEMMAP_END\t((unsigned long)vmemmap + VMEMMAP_SIZE - 1)\n \n #define KFENCE_AREA_START\t(VMEMMAP_END + 1)\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nMove MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n\nThis is preparation for adding the vmemmap_tails array to struct\nzone, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: David Hildenbrand (Red Hat) <david@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\nAcked-by: Muchun Song <muchun.song@linux.dev>\nAcked-by: Usama Arif <usamaarif642@gmail.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/mm.h     | 31 -------------------------------\n include/linux/mmzone.h | 31 +++++++++++++++++++++++++++++++\n 2 files changed, 31 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex 5be3d8a8f806..7f4dbbb9d783 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -27,7 +27,6 @@\n #include <linux/page-flags.h>\n #include <linux/page_ref.h>\n #include <linux/overflow.h>\n-#include <linux/sizes.h>\n #include <linux/sched.h>\n #include <linux/pgtable.h>\n #include <linux/kasan.h>\n@@ -2479,36 +2478,6 @@ static inline unsigned long folio_nr_pages(const struct folio *folio)\n \treturn folio_large_nr_pages(folio);\n }\n \n-#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n-/*\n- * We don't expect any folios that exceed buddy sizes (and consequently\n- * memory sections).\n- */\n-#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n-#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n-/*\n- * Only pages within a single memory section are guaranteed to be\n- * contiguous. By limiting folios to a single memory section, all folio\n- * pages are guaranteed to be contiguous.\n- */\n-#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n-#elif defined(CONFIG_HUGETLB_PAGE)\n-/*\n- * There is no real limit on the folio size. We limit them to the maximum we\n- * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n- * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n- */\n-#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n-#else\n-/*\n- * Without hugetlb, gigantic folios that are bigger than a single PUD are\n- * currently impossible.\n- */\n-#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n-#endif\n-\n-#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n-\n /*\n  * compound_nr() returns the number of pages in this potentially compound\n  * page.  compound_nr() can be called on a tail page, and is defined to\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 3e51190a55e4..be8ce40b5638 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -23,6 +23,7 @@\n #include <linux/page-flags.h>\n #include <linux/local_lock.h>\n #include <linux/zswap.h>\n+#include <linux/sizes.h>\n #include <asm/page.h>\n \n /* Free memory management - zoned buddy allocator.  */\n@@ -61,6 +62,36 @@\n  */\n #define PAGE_ALLOC_COSTLY_ORDER 3\n \n+#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n+/*\n+ * We don't expect any folios that exceed buddy sizes (and consequently\n+ * memory sections).\n+ */\n+#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n+#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n+/*\n+ * Only pages within a single memory section are guaranteed to be\n+ * contiguous. By limiting folios to a single memory section, all folio\n+ * pages are guaranteed to be contiguous.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n+#elif defined(CONFIG_HUGETLB_PAGE)\n+/*\n+ * There is no real limit on the folio size. We limit them to the maximum we\n+ * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n+ * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n+ */\n+#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n+#else\n+/*\n+ * Without hugetlb, gigantic folios that are bigger than a single PUD are\n+ * currently impossible.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n+#endif\n+\n+#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n+\n enum migratetype {\n \tMIGRATE_UNMOVABLE,\n \tMIGRATE_MOVABLE,\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-27",
              "message_id": "20260227193030.272078-3-kas@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author addressed a concern about the mask-based encoding of compound_info for tail pages, explaining that it only works when sizeof(struct page) is power-of-2 and struct pages are naturally aligned for all orders up to MAX_FOLIO_ORDER. The author acknowledged that this approach would work in more conditions but requires additional validation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a limitation",
                "explained the condition under which the mask-based encoding works"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Kiryl Shutsemau <kas@kernel.org>\n\nFor tail pages, the kernel uses the 'compound_info' field to get to the\nhead page. The bit 0 of the field indicates whether the page is a\ntail page, and if set, the remaining bits represent a pointer to the\nhead page.\n\nFor cases when size of struct page is power-of-2, change the encoding of\ncompound_info to store a mask that can be applied to the virtual address\nof the tail page in order to access the head page. It is possible\nbecause struct page of the head page is naturally aligned with regards\nto order of the page.\n\nThe significant impact of this modification is that all tail pages of\nthe same order will now have identical 'compound_info', regardless of\nthe compound page they are associated with. This paves the way for\neliminating fake heads.\n\nThe HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\napplied when the sizeof(struct page) is power-of-2. Having identical\ntail pages allows the same page to be mapped into the vmemmap of all\npages, maintaining memory savings without fake heads.\n\nIf sizeof(struct page) is not power-of-2, there is no functional\nchanges.\n\nLimit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\na difference. The approach with mask would work in the wider set of\nconditions, but it requires validating that struct pages are naturally\naligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (Arm) <david@kernel.org>\nAcked-by: Usama Arif <usamaarif642@gmail.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n mm/slab.h                  | 16 ++++++--\n mm/util.c                  | 16 ++++++--\n 3 files changed, 97 insertions(+), 16 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 42bf8ed02a29..01970bd38bff 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -198,6 +198,29 @@ enum pageflags {\n \n #ifndef __GENERATING_BOUNDS_H\n \n+/*\n+ * For tail pages, if the size of struct page is power-of-2 ->compound_info\n+ * encodes the mask that converts the address of the tail page address to\n+ * the head page address.\n+ *\n+ * Otherwise, ->compound_info has direct pointer to head pages.\n+ */\n+static __always_inline bool compound_info_has_mask(void)\n+{\n+\t/*\n+\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n+\t * makes a difference.\n+\t *\n+\t * The approach with mask would work in the wider set of conditions,\n+\t * but it requires validating that struct pages are naturally aligned\n+\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n+\t */\n+\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n+\t\treturn false;\n+\n+\treturn is_power_of_2(sizeof(struct page));\n+}\n+\n #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n \n@@ -207,6 +230,10 @@ DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n  */\n static __always_inline const struct page *page_fixed_fake_head(const struct page *page)\n {\n+\t/* Fake heads only exists if compound_info_has_mask() is true */\n+\tif (!compound_info_has_mask())\n+\t\treturn page;\n+\n \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n \t\treturn page;\n \n@@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n+\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n \n-\t\tif (likely(head & 1))\n-\t\t\treturn (const struct page *)(head - 1);\n+\t\t/* See set_compound_head() */\n+\t\tif (likely(info & 1)) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\treturn (const struct page *)(p & info);\n+\t\t}\n \t}\n \treturn page;\n }\n@@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_info);\n+\tunsigned long info = READ_ONCE(page->compound_info);\n \n-\tif (unlikely(head & 1))\n-\t\treturn head - 1;\n-\treturn (unsigned long)page_fixed_fake_head(page);\n+\t/* Bit 0 encodes PageTail() */\n+\tif (!(info & 1))\n+\t\treturn (unsigned long)page_fixed_fake_head(page);\n+\n+\t/*\n+\t * If compound_info_has_mask() is false, the rest of compound_info is\n+\t * the pointer to the head page.\n+\t */\n+\tif (!compound_info_has_mask())\n+\t\treturn info - 1;\n+\n+\t/*\n+\t * If compound_info_has_mask() is true the rest of the info encodes\n+\t * the mask that converts the address of the tail page to the head page.\n+\t *\n+\t * No need to clear bit 0 in the mask as 'page' always has it clear.\n+\t */\n+\treturn (unsigned long)page & info;\n }\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n@@ -293,7 +339,26 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n static __always_inline void set_compound_head(struct page *tail,\n \t\tconst struct page *head, unsigned int order)\n {\n-\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n+\tunsigned int shift;\n+\tunsigned long mask;\n+\n+\tif (!compound_info_has_mask()) {\n+\t\tWRITE_ONCE(tail->compound_info, (unsigned long)head | 1);\n+\t\treturn;\n+\t}\n+\n+\t/*\n+\t * If the size of struct page is power-of-2, bits [shift:0] of the\n+\t * virtual address of compound head are zero.\n+\t *\n+\t * Calculate mask that can be applied to the virtual address of\n+\t * the tail page to get address of the head page.\n+\t */\n+\tshift = order + order_base_2(sizeof(struct page));\n+\tmask = GENMASK(BITS_PER_LONG - 1, shift);\n+\n+\t/* Bit 0 encodes PageTail() */\n+\tWRITE_ONCE(tail->compound_info, mask | 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\ndiff --git a/mm/slab.h b/mm/slab.h\nindex 62dfa50c1f01..1a1b3758df05 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -131,11 +131,19 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist\n  */\n static inline struct slab *page_slab(const struct page *page)\n {\n-\tunsigned long head;\n+\tunsigned long info;\n+\n+\tinfo = READ_ONCE(page->compound_info);\n+\tif (info & 1) {\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\t\t\tpage = (struct page *)(p & info);\n+\t\t} else {\n+\t\t\tpage = (struct page *)(info - 1);\n+\t\t}\n+\t}\n \n-\thead = READ_ONCE(page->compound_head);\n-\tif (head & 1)\n-\t\tpage = (struct page *)(head - 1);\n \tif (data_race(page->page_type >> 24) != PGTY_slab)\n \t\tpage = NULL;\n \ndiff --git a/mm/util.c b/mm/util.c\nindex 3ebcb9e6035c..20dccf2881d7 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1237,7 +1237,7 @@ static void set_ps_flags(struct page_snapshot *ps, const struct folio *folio,\n  */\n void snapshot_page(struct page_snapshot *ps, const struct page *page)\n {\n-\tunsigned long head, nr_pages = 1;\n+\tunsigned long info, nr_pages = 1;\n \tstruct folio *foliop;\n \tint loops = 5;\n \n@@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_info;\n-\tif ((head & 1) == 0) {\n+\tinfo = ps->page_snapshot.compound_info;\n+\tif (!(info & 1)) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n \t\tif (!folio_test_large(foliop)) {\n@@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n \t\t}\n \t\tfoliop = (struct folio *)page;\n \t} else {\n-\t\tfoliop = (struct folio *)(head - 1);\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\tfoliop = (struct folio *)(p & info);\n+\t\t} else {\n+\t\t\tfoliop = (struct folio *)(info - 1);\n+\t\t}\n+\n \t\tps->idx = folio_page_idx(foliop, page);\n \t}\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nIf page->compound_info encodes a mask, it is expected that vmemmap to be\nnaturally aligned to the maximum folio size.\n\nAdd a VM_WARN_ON_ONCE() to check the alignment.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n mm/sparse.c | 5 +++++\n 1 file changed, 5 insertions(+)\n\ndiff --git a/mm/sparse.c b/mm/sparse.c\nindex b5b2b6f7041b..dfabe554adf8 100644\n--- a/mm/sparse.c\n+++ b/mm/sparse.c\n@@ -600,6 +600,11 @@ void __init sparse_init(void)\n \tBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n \tmemblocks_present();\n \n+\tif (compound_info_has_mask()) {\n+\t\tVM_WARN_ON_ONCE(!IS_ALIGNED((unsigned long) pfn_to_page(0),\n+\t\t\t\t    MAX_FOLIO_VMEMMAP_ALIGN));\n+\t}\n+\n \tpnum_begin = first_present_section_nr();\n \tnid_begin = sparse_early_nid(__nr_to_section(pnum_begin));\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nCurrently, the vmemmap for bootmem-allocated gigantic pages is populated\nearly in hugetlb_vmemmap_init_early(). However, the zone information is\nonly available after zones are initialized. If it is later discovered\nthat a page spans multiple zones, the HVO mapping must be undone and\nreplaced with a normal mapping using vmemmap_undo_hvo().\n\nDefer the actual vmemmap population to hugetlb_vmemmap_init_late(). At\nthis stage, zones are already initialized, so it can be checked if the\npage is valid for HVO before deciding how to populate the vmemmap.\n\nThis allows us to remove vmemmap_undo_hvo() and the complex logic\nrequired to rollback HVO mappings.\n\nIn hugetlb_vmemmap_init_late(), if HVO population fails or if the zones\nare invalid, fall back to a normal vmemmap population.\n\nPostponing population until hugetlb_vmemmap_init_late() also makes zone\ninformation available from within vmemmap_populate_hvo().\n\nSigned-off-by: Kiryl Shutsemau (Meta) <kas@kernel.org>\n---\n include/linux/mm.h   |  2 --\n mm/hugetlb_vmemmap.c | 37 +++++++++++++++----------------\n mm/sparse-vmemmap.c  | 53 --------------------------------------------\n 3 files changed, 18 insertions(+), 74 deletions(-)\n\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex 7f4dbbb9d783..0e2d45008ff4 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -4484,8 +4484,6 @@ int vmemmap_populate(unsigned long start, unsigned long end, int node,\n \t\tstruct vmem_altmap *altmap);\n int vmemmap_populate_hvo(unsigned long start, unsigned long end, int node,\n \t\t\t unsigned long headsize);\n-int vmemmap_undo_hvo(unsigned long start, unsigned long end, int node,\n-\t\t     unsigned long headsize);\n void vmemmap_wrprotect_hvo(unsigned long start, unsigned long end, int node,\n \t\t\t  unsigned long headsize);\n void vmemmap_populate_print_last(void);\ndiff --git a/mm/hugetlb_vmemmap.c b/mm/hugetlb_vmemmap.c\nindex a9280259e12a..935ec5829be9 100644\n--- a/mm/hugetlb_vmemmap.c\n+++ b/mm/hugetlb_vmemmap.c\n@@ -790,7 +790,6 @@ void __init hugetlb_vmemmap_init_early(int nid)\n {\n \tunsigned long psize, paddr, section_size;\n \tunsigned long ns, i, pnum, pfn, nr_pages;\n-\tunsigned long start, end;\n \tstruct huge_bootmem_page *m = NULL;\n \tvoid *map;\n \n@@ -808,14 +807,6 @@ void __init hugetlb_vmemmap_init_early(int nid)\n \t\tpaddr = virt_to_phys(m);\n \t\tpfn = PHYS_PFN(paddr);\n \t\tmap = pfn_to_page(pfn);\n-\t\tstart = (unsigned long)map;\n-\t\tend = start + nr_pages * sizeof(struct page);\n-\n-\t\tif (vmemmap_populate_hvo(start, end, nid,\n-\t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_SIZE) < 0)\n-\t\t\tcontinue;\n-\n-\t\tmemmap_boot_pages_add(HUGETLB_VMEMMAP_RESERVE_SIZE / PAGE_SIZE);\n \n \t\tpnum = pfn_to_section_nr(pfn);\n \t\tns = psize / section_size;\n@@ -850,28 +841,36 @@ void __init hugetlb_vmemmap_init_late(int nid)\n \t\th = m->hstate;\n \t\tpfn = PHYS_PFN(phys);\n \t\tnr_pages = pages_per_huge_page(h);\n+\t\tmap = pfn_to_page(pfn);\n+\t\tstart = (unsigned long)map;\n+\t\tend = start + nr_pages * sizeof(struct page);\n \n \t\tif (!hugetlb_bootmem_page_zones_valid(nid, m)) {\n \t\t\t/*\n \t\t\t * Oops, the hugetlb page spans multiple zones.\n-\t\t\t * Remove it from the list, and undo HVO.\n+\t\t\t * Remove it from the list, and populate it normally.\n \t\t\t */\n \t\t\tlist_del(&m->list);\n \n-\t\t\tmap = pfn_to_page(pfn);\n-\n-\t\t\tstart = (unsigned long)map;\n-\t\t\tend = start + nr_pages * sizeof(struct page);\n-\n-\t\t\tvmemmap_undo_hvo(start, end, nid,\n-\t\t\t\t\t HUGETLB_VMEMMAP_RESERVE_SIZE);\n-\t\t\tnr_mmap = end - start - HUGETLB_VMEMMAP_RESERVE_SIZE;\n+\t\t\tvmemmap_populate(start, end, nid, NULL);\n+\t\t\tnr_mmap = end - start;\n \t\t\tmemmap_boot_pages_add(DIV_ROUND_UP(nr_mmap, PAGE_SIZE));\n \n \t\t\tmemblock_phys_free(phys, huge_page_size(h));\n \t\t\tcontinue;\n-\t\t} else\n+\t\t}\n+\n+\t\tif (vmemmap_populate_hvo(start, end, nid,\n+\t\t\t\t\t HUGETLB_VMEMMAP_RESERVE_SIZE) < 0) {\n+\t\t\t/* Fallback if HVO population fails */\n+\t\t\tvmemmap_populate(start, end, nid, NULL);\n+\t\t\tnr_mmap = end - start;\n+\t\t} else {\n \t\t\tm->flags |= HUGE_BOOTMEM_ZONES_VALID;\n+\t\t\tnr_mmap = HUGETLB_VMEMMAP_RESERVE_SIZE;\n+\t\t}\n+\n+\t\tmemmap_boot_pages_add(DIV_ROUND_UP(nr_mmap, PAGE_SIZE));\n \t}\n }\n #endif\ndiff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c\nindex 37522d6cb398..032a81450838 100644\n--- a/mm/sparse-vmemmap.c\n+++ b/mm/sparse-vmemmap.c\n@@ -302,59 +302,6 @@ int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,\n \treturn vmemmap_populate_range(start, end, node, altmap, -1, 0);\n }\n \n-/*\n- * Undo populate_hvo, and replace it with a normal base page mapping.\n- * Used in memory init in case a HVO mapping needs to be undone.\n- *\n- * This can happen when it is discovered that a memblock allocated\n- * hugetlb page spans multiple zones, which can only be verified\n- * after zones have been initialized.\n- *\n- * We know that:\n- * 1) The first @headsize / PAGE_SIZE vmemmap pages were individually\n- *    allocated through memblock, and mapped.\n- *\n- * 2) The rest of the vmemmap pages are mirrors of the last head page.\n- */\n-int __meminit vmemmap_undo_hvo(unsigned long addr, unsigned long end,\n-\t\t\t\t      int node, unsigned long headsize)\n-{\n-\tunsigned long maddr, pfn;\n-\tpte_t *pte;\n-\tint headpages;\n-\n-\t/*\n-\t * Should only be called early in boot, so nothing will\n-\t * be accessing these page structures.\n-\t */\n-\tWARN_ON(!early_boot_irqs_disabled);\n-\n-\theadpages = headsize >> PAGE_SHIFT;\n-\n-\t/*\n-\t * Clear mirrored mappings for tail page structs.\n-\t */\n-\tfor (maddr = addr + headsize; maddr < end; maddr += PAGE_SIZE) {\n-\t\tpte = virt_to_kpte(maddr);\n-\t\tpte_clear(&init_mm, maddr, pte);\n-\t}\n-\n-\t/*\n-\t * Clear and free mappings for head page and first tail page\n-\t * structs.\n-\t */\n-\tfor (maddr = addr; headpages-- > 0; maddr += PAGE_SIZE) {\n-\t\tpte = virt_to_kpte(maddr);\n-\t\tpfn = pte_pfn(ptep_get(pte));\n-\t\tpte_clear(&init_mm, maddr, pte);\n-\t\tmemblock_phys_free(PFN_PHYS(pfn), PAGE_SIZE);\n-\t}\n-\n-\tflush_tlb_kernel_range(addr, end);\n-\n-\treturn vmemmap_populate(addr, end, node, NULL);\n-}\n-\n /*\n  * Write protect the mirrored tail page structs for HVO. This will be\n  * called from the hugetlb code when gathering and initializing the\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nThis series removes \"fake head pages\" from the HugeTLB vmemmap\noptimization (HVO) by changing how tail pages encode their relationship\nto the head page.\n\nIt simplifies compound_head() and page_ref_add_unless(). Both are in the\nhot path.\n\nBackground\n==========\n\nHVO reduces memory overhead by freeing vmemmap pages for HugeTLB pages\nand remapping the freed virtual addresses to a single physical page.\nPreviously, all tail page vmemmap entries were remapped to the first\nvmemmap page (containing the head struct page), creating \"fake heads\" -\ntail pages that appear to have PG_head set when accessed through the\ndeduplicated vmemmap.\n\nThis required special handling in compound_head() to detect and work\naround fake heads, adding complexity and overhead to a very hot path.\n\nNew Approach\n============\n\nFor architectures/configs where sizeof(struct page) is a power of 2 (the\ncommon case), this series changes how position of the head page is encoded\nin the tail pages.\n\nInstead of storing a pointer to the head page, the ->compound_info\n(renamed from ->compound_head) now stores a mask.\n\nThe mask can be applied to any tail page's virtual address to compute\nthe head page address. Critically, all tail pages of the same order now\nhave identical compound_info values, regardless of which compound page\nthey belong to.\n\nThe key insight is that all tail pages of the same order now have\nidentical compound_info values, regardless of which compound page they\nbelong to.\n\nIn v7, these shared tail pages are allocated per-zone. This ensures \nthat zone information (stored in page->flags) is correct even for \nshared tail pages, removing the need for the special-casing in \npage_zonenum() proposed in earlier versions.\n\nTo support per-zone shared pages for boot-allocated gigantic pages, \nthe vmemmap population is deferred until zones are initialized. This \nsimplifies the logic significantly and allows the removal of \nvmemmap_undo_hvo().\n\nBenefits\n========\n\n1. Simplified compound_head(): No fake head detection needed, can be\n   implemented in a branchless manner.\n\n2. Simplified page_ref_add_unless(): RCU protection removed since there's\n   no race with fake head remapping.\n\n3. Cleaner architecture: The shared tail pages are truly read-only and\n   contain valid tail page metadata.\n\nIf sizeof(struct page) is not power-of-2, there are no functional changes.\nHVO is not supported in this configuration.\n\nI had hoped to see performance improvement, but my testing thus far has\nshown either no change or only a slight improvement within the noise.\n\nSeries Organization\n===================\n\nPatch 1: Move MAX_FOLIO_ORDER definition to mmzone.h.\nPatches 2-4: Refactoring of field names and interfaces.\nPatches 5-6: Architecture alignment for LoongArch and RISC-V.\nPatch 7: Mask-based compound_head() implementation.\nPatch 8: Add memmap alignment checks.\nPatch 9: Branchless compound_head() optimization.\nPatch 10: Defer vmemmap population for bootmem hugepages.\nPatch 11: Refactor vmemmap_walk.\nPatch 12: x86 vDSO build fix.\nPatch 13: Eliminate fake heads with per-zone shared tail pages.\nPatches 14-16: Cleanup of fake head infrastructure.\nPatch 17: Documentation update.\nPatch 18: Use compound_head() in page_slab().\n\nChanges in v7:\n==============\n\n  - Move vmemmap_tails from per-node to per-zone. This ensures tail\n    pages have correct zone information.\n\n  - Defer vmemmap population for boot-allocated huge pages to \n    hugetlb_vmemmap_init_late(). This makes zone information available \n    during population and allows removing vmemmap_undo_hvo().\n\n  - Undefine CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP for x86 vdso32 to \n    fix build issues.\n\n  - Remove the patch that modified page_zonenum(), as per-zone \n    shared pages make it unnecessary.\n\nChanges in v6:\n==============\n  - Simplify memmap alignment check in mm/sparse.c: use VM_BUG_ON()\n    (Muchun)\n\n  - Store struct page pointers in vmemmap_tails[] instead of PFNs.\n    (Muchun)\n\n  - Fix build error on powerpc due to negative NR_VMEMMAP_TAILS.\n\nChanges in v5:\n==============\n  - Rebased to mm-everything-2026-01-27-04-35\n\n  - Add arch-specific patches to align vmemmap to maximal folio size\n    for riscv and LoongArch architectures.\n\n  - Strengthen the memmap alignment check in mm/sparse.c: use BUG()\n    for CONFIG_DEBUG_VM, WARN() otherwise. (Muchun)\n\n  - Use cmpxchg() instead of hugetlb_lock to update vmemmap_tails\n    array. (Muchun)\n\n  - Update page_slab().\n\nChanges in v4:\n==============\n  - Fix build issues due to linux/mmzone.h <-> linux/pgtable.h\n    dependency loop by avoiding including linux/pgtable.h into\n    linux/mmzone.h\n\n  - Rework vmemmap_remap_alloc() interface. (Muchun)\n\n  - Use &folio->page instead of folio address for optimization\n    target. (Muchun)\n\nChanges in v3:\n==============\n  - Fixed error recovery path in vmemmap_remap_free() to pass correct start\n    address for TLB flush. (Muchun)\n\n  - Wrapped the mask-based compound_info encoding within CONFIG_SPARSEMEM_VMEMMAP\n    check via compound_info_has_mask(). For other memory models, alignment\n    guarantees are harder to verify. (Muchun)\n\n  - Updated vmemmap_dedup.rst documentation wording: changed \"vmemmap_tail\n    shared for the struct hstate\" to \"A single, per-node page frame shared\n    among all hugepages of the same size\". (Muchun)\n\n  - Fixed build error with MAX_FOLIO_ORDER expanding to undefined PUD_ORDER\n    in certain configurations. (kernel test robot)\n\nChanges in v2:\n==============\n\n- Handle boot-allocated huge pages correctly. (Frank)\n\n- Changed from per-hstate vmemmap_tail to per-node vmemmap_tails[] array\n  in pglist_data. (Muchun)\n\n- Added spin_lock(&hugetlb_lock) protection in vmemmap_get_tail() to fix\n  a race condition where two threads could both allocate tail pages.\n  The losing thread now properly frees its allocated page. (Usama)\n\n- Add warning if memmap is not aligned to MAX_FOLIO_SIZE, which is\n  required for the mask approach. (Muchun)\n\n- Make page_zonenum() use head page - correctness fix since shared\n  tail pages cannot have valid zone information. (Muchun)\n\n- Added 'const' qualifier to head parameter in set_compound_head() and\n  prep_compound_tail(). (Usama)\n\n- Updated commit messages.\n\nKiryl Shutsemau (16):\n  mm: Move MAX_FOLIO_ORDER definition to mmzone.h\n  mm: Change the interface of prep_compound_tail()\n  mm: Rename the 'compound_head' field in the 'struct page' to\n    'compound_info'\n  mm: Move set/clear_compound_head() next to compound_head()\n  riscv/mm: Align vmemmap to maximal folio size\n  LoongArch/mm: Align vmemmap to maximal folio size\n  mm: Rework compound_head() for power-of-2 sizeof(struct page)\n  mm/sparse: Check memmap alignment for compound_info_has_mask()\n  mm/hugetlb: Refactor code around vmemmap_walk\n  mm/hugetlb: Remove fake head pages\n  mm: Drop fake head checks\n  hugetlb: Remove VMEMMAP_SYNCHRONIZE_RCU\n  mm/hugetlb: Remove hugetlb_optimize_vmemmap_key static key\n  mm: Remove the branch from compound_head()\n  hugetlb: Update vmemmap_dedup.rst\n  mm/slab: Use compound_head() in page_slab()\n\nKiryl Shutsemau (Meta) (2):\n  mm/hugetlb: Defer vmemmap population for bootmem hugepages\n  x86/vdso: Undefine CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP for vdso32\n\n .../admin-guide/kdump/vmcoreinfo.rst          |   2 +-\n Documentation/mm/vmemmap_dedup.rst            |  62 ++-\n arch/loongarch/include/asm/pgtable.h          |   3 +-\n arch/riscv/mm/init.c                          |   3 +-\n arch/x86/entry/vdso/vdso32/fake_32bit_build.h |   1 +\n include/linux/mm.h                            |  36 +-\n include/linux/mm_types.h                      |  20 +-\n include/linux/mmzone.h                        |  57 +++\n include/linux/page-flags.h                    | 166 ++++----\n include/linux/page_ref.h                      |   8 +-\n include/linux/types.h                         |   2 +-\n kernel/vmcore_info.c                          |   2 +-\n mm/hugetlb.c                                  |   8 +-\n mm/hugetlb_vmemmap.c                          | 362 +++++++++---------\n mm/internal.h                                 |  18 +-\n mm/mm_init.c                                  |   2 +-\n mm/page_alloc.c                               |   4 +-\n mm/slab.h                                     |   8 +-\n mm/sparse-vmemmap.c                           | 110 +++---\n mm/sparse.c                                   |   5 +\n mm/util.c                                     |  16 +-\n 21 files changed, 448 insertions(+), 447 deletions(-)\n\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-27",
              "message_id": "20260227193030.272078-7-kas@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Kiryl Shutsemau (author)",
              "summary": "The author is apologizing for a mistake in the submission thread, stating that they will resend the patches properly and acknowledging their own error.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "apology",
                "acknowledgment of error"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I've screwed up threading in the submission, please ignore.\nI will resend properly.\n\nSorry for this mess. I should have figured out mail by now :/\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nI've screwed up threading in the submission, please ignore.\nI will resend properly.\n\nSorry for this mess. I should have figured out mail by now :/\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-27",
              "message_id": "aaHzKykadJwN7tF1@thinkstation",
              "analysis_source": "llm"
            },
            {
              "author": "Muchun Song",
              "summary": "Reviewer Muchun Song pointed out that the shared tail pages will have incorrect zone information because huge pages on the same node may belong to different zones, and suggested always fetching zone information from the head page.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "> On Feb 2, 2026, at 23:56, Kiryl Shutsemau <kas@kernel.org> wrote:\n> \n> If page->compound_info encodes a mask, it is expected that vmemmap to be\n> naturally aligned to the maximum folio size.\n> \n> Add a VM_BUG_ON() to check the alignment.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Muchun Song <muchun.song@linux.dev>\n\n---\n\n> On Feb 2, 2026, at 23:56, Kiryl Shutsemau <kas@kernel.org> wrote:\n> \n> If page->compound_info encodes a mask, it is expected that vmemmap to be\n> naturally aligned to the maximum folio size.\n> \n> Add a VM_BUG_ON() to check the alignment.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Muchun Song <muchun.song@linux.dev>\n\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\n> On Feb 2, 2026, at 23:56, Kiryl Shutsemau <kas@kernel.org> wrote:\n> \n> With the upcoming changes to HVO, a single page of tail struct pages\n> will be shared across all huge pages of the same order on a node. Since\n> huge pages on the same node may belong to different zones, the zone\n> information stored in shared tail page flags would be incorrect.\n> \n> Always fetch zone information from the head page, which has unique and\n> correct zone flags for each compound page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n\nAcked-by: Muchun Song <muchun.song@linux.dev>\n\n---\n\n> On Feb 2, 2026, at 23:56, Kiryl Shutsemau <kas@kernel.org> wrote:\n> \n> With the upcoming changes to HVO, a single page of tail struct pages\n> will be shared across all huge pages of the same order on a node. Since\n> huge pages on the same node may belong to different zones, the zone\n> information stored in shared tail page flags would be incorrect.\n> \n> Always fetch zone information from the head page, which has unique and\n> correct zone flags for each compound page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n\nAcked-by: Muchun Song <muchun.song@linux.dev>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-03",
              "message_id": "C310A603-DB7F-4140-B045-7F1E3CC98C05@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "David (arm)",
              "summary": "Reviewer David (arm) suggested two minor code style improvements: adding two tab indents to the parameter list in set_compound_head() and renaming a function parameter from 'page' to 'tail' for consistency.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "LGTM",
                "Only nits"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Instead of passing down the head page and tail page index, pass the tail\n> and head pages directly, as well as the order of the compound page.\n> \n> This is a preparation for changing how the head position is encoded in\n> the tail page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   include/linux/page-flags.h |  4 +++-\n>   mm/hugetlb.c               |  8 +++++---\n>   mm/internal.h              | 12 ++++++------\n>   mm/mm_init.c               |  2 +-\n>   mm/page_alloc.c            |  2 +-\n>   5 files changed, 16 insertions(+), 12 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index f7a0e4af0c73..8a3694369e15 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n>   \treturn folio_test_head(folio);\n>   }\n>   \n> -static __always_inline void set_compound_head(struct page *page, struct page *head)\n> +static __always_inline void set_compound_head(struct page *page,\n> +\t\t\t\t\t      const struct page *head,\n> +\t\t\t\t\t      unsigned int order)\n\nTwo tab indents please on second+ parameter list whenever you touch code.\n\n>   {\n>   \tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n>   }\n> diff --git a/mm/hugetlb.c b/mm/hugetlb.c\n> index 6e855a32de3d..54ba7cd05a86 100644\n\n\n[...]\n\n> diff --git a/mm/internal.h b/mm/internal.h\n> index d67e8bb75734..037ddcda25ff 100644\n> --- a/mm/internal.h\n> +++ b/mm/internal.h\n> @@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n>   \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n>   }\n>   \n> -static inline void prep_compound_tail(struct page *head, int tail_idx)\n> +static inline void prep_compound_tail(struct page *tail,\n\nJust wondering whether we should call this \"struct page *page\" for \nconsistency with set_compound_head().\n\nOr alternatively, call it also \"tail\" in set_compound_head().\n\n> +\t\t\t\t      const struct page *head,\n> +\t\t\t\t      unsigned int order)\n\nTwo tab indent, then this fits into two lines in total.\n\n>   {\n> -\tstruct page *p = head + tail_idx;\n> -\n> -\tp->mapping = TAIL_MAPPING;\n> -\tset_compound_head(p, head);\n> -\tset_page_private(p, 0);\n> +\ttail->mapping = TAIL_MAPPING;\n> +\tset_compound_head(tail, head, order);\n> +\tset_page_private(tail, 0);\n>   }\nOnly nits, in general LGTM\n\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The 'compound_head' field in the 'struct page' encodes whether the page\n> is a tail and where to locate the head page. Bit 0 is set if the page is\n> a tail, and the remaining bits in the field point to the head page.\n> \n> As preparation for changing how the field encodes information about the\n> head page, rename the field to 'compound_info'.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Instead of passing down the head page and tail page index, pass the tail\n> and head pages directly, as well as the order of the compound page.\n> \n> This is a preparation for changing how the head position is encoded in\n> the tail page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   include/linux/page-flags.h |  4 +++-\n>   mm/hugetlb.c               |  8 +++++---\n>   mm/internal.h              | 12 ++++++------\n>   mm/mm_init.c               |  2 +-\n>   mm/page_alloc.c            |  2 +-\n>   5 files changed, 16 insertions(+), 12 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index f7a0e4af0c73..8a3694369e15 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n>   \treturn folio_test_head(folio);\n>   }\n>   \n> -static __always_inline void set_compound_head(struct page *page, struct page *head)\n> +static __always_inline void set_compound_head(struct page *page,\n> +\t\t\t\t\t      const struct page *head,\n> +\t\t\t\t\t      unsigned int order)\n\nTwo tab indents please on second+ parameter list whenever you touch code.\n\n>   {\n>   \tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n>   }\n> diff --git a/mm/hugetlb.c b/mm/hugetlb.c\n> index 6e855a32de3d..54ba7cd05a86 100644\n\n\n[...]\n\n> diff --git a/mm/internal.h b/mm/internal.h\n> index d67e8bb75734..037ddcda25ff 100644\n> --- a/mm/internal.h\n> +++ b/mm/internal.h\n> @@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n>   \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n>   }\n>   \n> -static inline void prep_compound_tail(struct page *head, int tail_idx)\n> +static inline void prep_compound_tail(struct page *tail,\n\nJust wondering whether we should call this \"struct page *page\" for \nconsistency with set_compound_head().\n\nOr alternatively, call it also \"tail\" in set_compound_head().\n\n> +\t\t\t\t      const struct page *head,\n> +\t\t\t\t      unsigned int order)\n\nTwo tab indent, then this fits into two lines in total.\n\n>   {\n> -\tstruct page *p = head + tail_idx;\n> -\n> -\tp->mapping = TAIL_MAPPING;\n> -\tset_compound_head(p, head);\n> -\tset_page_private(p, 0);\n> +\ttail->mapping = TAIL_MAPPING;\n> +\tset_compound_head(tail, head, order);\n> +\tset_page_private(tail, 0);\n>   }\nOnly nits, in general LGTM\n\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The 'compound_head' field in the 'struct page' encodes whether the page\n> is a tail and where to locate the head page. Bit 0 is set if the page is\n> a tail, and the remaining bits in the field point to the head page.\n> \n> As preparation for changing how the field encodes information about the\n> head page, rename the field to 'compound_info'.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Move set_compound_head() and clear_compound_head() to be adjacent to the\n> compound_head() function in page-flags.h.\n> \n> These functions encode and decode the same compound_info field, so\n> keeping them together makes it easier to verify their logic is\n> consistent, especially when the encoding changes.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   include/linux/page-flags.h | 24 ++++++++++++------------\n>   1 file changed, 12 insertions(+), 12 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index aa46d49e82f7..d14a17ffb55b 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -290,6 +290,18 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n>   \n>   #define compound_head(page)\t((typeof(page))_compound_head(page))\n>   \n> +static __always_inline void set_compound_head(struct page *page,\n> +\t\t\t\t\t      const struct page *head,\n> +\t\t\t\t\t      unsigned int order)\n\n^ :)\n\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Move set_compound_head() and clear_compound_head() to be adjacent to the\n> compound_head() function in page-flags.h.\n> \n> These functions encode and decode the same compound_info field, so\n> keeping them together makes it easier to verify their logic is\n> consistent, especially when the encoding changes.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   include/linux/page-flags.h | 24 ++++++++++++------------\n>   1 file changed, 12 insertions(+), 12 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index aa46d49e82f7..d14a17ffb55b 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -290,6 +290,18 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n>   \n>   #define compound_head(page)\t((typeof(page))_compound_head(page))\n>   \n> +static __always_inline void set_compound_head(struct page *page,\n> +\t\t\t\t\t      const struct page *head,\n> +\t\t\t\t\t      unsigned int order)\n\n^ :)\n\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> struct pages of the head page to be naturally aligned with regard to the\n> folio size.\n> \n> Align vmemmap to MAX_FOLIO_NR_PAGES.\n\nI think neither that statement nor the one in the patch description is \ncorrect?\n\n\"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is neither the maximum folio \nsize nor MAX_FOLIO_NR_PAGES.\n\nIt's the size of the memmap that a large folio could span at maximum.\n\n\nAssuming we have a 16 GiB folio, the calculation would give us\n\n\t4194304 * sizeof(struct page)\n\nWhich could be something like (assuming 80 bytes)\n\n\t335544320\n\n-> not even a power of 2, weird? (for HVO you wouldn't care as HVO would \nbe disabled, but that aliment is super weird?)\n\n\nAssuming 64 bytes, it would be a power of two (as 64 is a power of two).\n\n\t268435456 (1<< 28)\n\n\nWhich makes me wonder whether there is a way to avoid sizeof(struct \npage) here completely.\n\nOr limit the alignment to the case where HVO is actually active and \nsizeof(struct page) makes any sense?\n\n\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> ---\n>   arch/riscv/mm/init.c | 3 ++-\n>   1 file changed, 2 insertions(+), 1 deletion(-)\n> \n> diff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c\n> index 21d534824624..c555b9a4fdce 100644\n> --- a/arch/riscv/mm/init.c\n> +++ b/arch/riscv/mm/init.c\n> @@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;\n>   EXPORT_SYMBOL(phys_ram_base);\n>   \n>   #ifdef CONFIG_SPARSEMEM_VMEMMAP\n> -#define VMEMMAP_ADDR_ALIGN\t(1ULL << SECTION_SIZE_BITS)\n> +#define VMEMMAP_ADDR_ALIGN\tmax(1ULL << SECTION_SIZE_BITS, \\\n> +\t\t\t\t    MAX_FOLIO_NR_PAGES * sizeof(struct page))\n>   \n>   unsigned long vmemmap_start_pfn __ro_after_init;\n>   EXPORT_SYMBOL(vmemmap_start_pfn);\n\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> struct pages of the head page to be naturally aligned with regard to the\n> folio size.\n> \n> Align vmemmap to MAX_FOLIO_NR_PAGES.\n\nI think neither that statement nor the one in the patch description is \ncorrect?\n\n\"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is neither the maximum folio \nsize nor MAX_FOLIO_NR_PAGES.\n\nIt's the size of the memmap that a large folio could span at maximum.\n\n\nAssuming we have a 16 GiB folio, the calculation would give us\n\n\t4194304 * sizeof(struct page)\n\nWhich could be something like (assuming 80 bytes)\n\n\t335544320\n\n-> not even a power of 2, weird? (for HVO you wouldn't care as HVO would \nbe disabled, but that aliment is super weird?)\n\n\nAssuming 64 bytes, it would be a power of two (as 64 is a power of two).\n\n\t268435456 (1<< 28)\n\n\nWhich makes me wonder whether there is a way to avoid sizeof(struct \npage) here completely.\n\nOr limit the alignment to the case where HVO is actually active and \nsizeof(struct page) makes any sense?\n\n\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> ---\n>   arch/riscv/mm/init.c | 3 ++-\n>   1 file changed, 2 insertions(+), 1 deletion(-)\n> \n> diff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c\n> index 21d534824624..c555b9a4fdce 100644\n> --- a/arch/riscv/mm/init.c\n> +++ b/arch/riscv/mm/init.c\n> @@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;\n>   EXPORT_SYMBOL(phys_ram_base);\n>   \n>   #ifdef CONFIG_SPARSEMEM_VMEMMAP\n> -#define VMEMMAP_ADDR_ALIGN\t(1ULL << SECTION_SIZE_BITS)\n> +#define VMEMMAP_ADDR_ALIGN\tmax(1ULL << SECTION_SIZE_BITS, \\\n> +\t\t\t\t    MAX_FOLIO_NR_PAGES * sizeof(struct page))\n>   \n>   unsigned long vmemmap_start_pfn __ro_after_init;\n>   EXPORT_SYMBOL(vmemmap_start_pfn);\n\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> struct pages of the head page to be naturally aligned with regard to the\n> folio size.\n> \n> Align vmemmap to MAX_FOLIO_NR_PAGES.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> ---\n>   arch/loongarch/include/asm/pgtable.h | 3 ++-\n>   1 file changed, 2 insertions(+), 1 deletion(-)\n> \n> diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\n> index c33b3bcb733e..f9416acb9156 100644\n> --- a/arch/loongarch/include/asm/pgtable.h\n> +++ b/arch/loongarch/include/asm/pgtable.h\n> @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n>   \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n>   #endif\n>   \n> -#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n> +#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n> +#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n\n\nSame comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just black magic here\nand the description of the situation is wrong.\n\nMaybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" into the core and call it\n\n#define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n\nBut then special case it base on (a) HVO being configured in an (b) HVO being possible\n\n#ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n/* A very helpful comment explaining the situation. */\n#define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n#else\n#define MAX_FOLIO_VMEMMAP_ALIGN\t0\n#endif\n\nSomething like that.\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> struct pages of the head page to be naturally aligned with regard to the\n> folio size.\n> \n> Align vmemmap to MAX_FOLIO_NR_PAGES.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> ---\n>   arch/loongarch/include/asm/pgtable.h | 3 ++-\n>   1 file changed, 2 insertions(+), 1 deletion(-)\n> \n> diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\n> index c33b3bcb733e..f9416acb9156 100644\n> --- a/arch/loongarch/include/asm/pgtable.h\n> +++ b/arch/loongarch/include/asm/pgtable.h\n> @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n>   \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n>   #endif\n>   \n> -#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n> +#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n> +#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n\n\nSame comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just black magic here\nand the description of the situation is wrong.\n\nMaybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" into the core and call it\n\n#define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n\nBut then special case it base on (a) HVO being configured in an (b) HVO being possible\n\n#ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n/* A very helpful comment explaining the situation. */\n#define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n#else\n#define MAX_FOLIO_VMEMMAP_ALIGN\t0\n#endif\n\nSomething like that.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/5/26 12:35, Kiryl Shutsemau wrote:\n> On Wed, Feb 04, 2026 at 05:14:12PM +0100, David Hildenbrand (arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>>> Instead of passing down the head page and tail page index, pass the tail\n>>> and head pages directly, as well as the order of the compound page.\n>>>\n>>> This is a preparation for changing how the head position is encoded in\n>>> the tail page.\n>>>\n>>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>>> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n>>> Reviewed-by: Zi Yan <ziy@nvidia.com>\n>>> ---\n>>>    include/linux/page-flags.h |  4 +++-\n>>>    mm/hugetlb.c               |  8 +++++---\n>>>    mm/internal.h              | 12 ++++++------\n>>>    mm/mm_init.c               |  2 +-\n>>>    mm/page_alloc.c            |  2 +-\n>>>    5 files changed, 16 insertions(+), 12 deletions(-)\n>>>\n>>> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n>>> index f7a0e4af0c73..8a3694369e15 100644\n>>> --- a/include/linux/page-flags.h\n>>> +++ b/include/linux/page-flags.h\n>>> @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n>>>    \treturn folio_test_head(folio);\n>>>    }\n>>> -static __always_inline void set_compound_head(struct page *page, struct page *head)\n>>> +static __always_inline void set_compound_head(struct page *page,\n>>> +\t\t\t\t\t      const struct page *head,\n>>> +\t\t\t\t\t      unsigned int order)\n>>\n>> Two tab indents please on second+ parameter list whenever you touch code.\n> \n> Do we have this coding style preference written down somewhere?\n\nGood question. I assume not. But it's what we do in MM :)\n\n> \n> -tip tree wants the opposite. Documentation/process/maintainer-tip.rst:\n> \n> \tWhen splitting function declarations or function calls, then please align\n> \tthe first argument in the second line with the first argument in the first\n> \tline::\n> \n> I want the editor to do The Right Thing\\u2122 without my brain involvement.\n> Having different coding styles in different corners of the kernel makes\n> it hard.\n\nYeah, but unavoidable. :)\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/5/26 12:35, Kiryl Shutsemau wrote:\n> On Wed, Feb 04, 2026 at 05:14:12PM +0100, David Hildenbrand (arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>>> Instead of passing down the head page and tail page index, pass the tail\n>>> and head pages directly, as well as the order of the compound page.\n>>>\n>>> This is a preparation for changing how the head position is encoded in\n>>> the tail page.\n>>>\n>>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>>> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n>>> Reviewed-by: Zi Yan <ziy@nvidia.com>\n>>> ---\n>>>    include/linux/page-flags.h |  4 +++-\n>>>    mm/hugetlb.c               |  8 +++++---\n>>>    mm/internal.h              | 12 ++++++------\n>>>    mm/mm_init.c               |  2 +-\n>>>    mm/page_alloc.c            |  2 +-\n>>>    5 files changed, 16 insertions(+), 12 deletions(-)\n>>>\n>>> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n>>> index f7a0e4af0c73..8a3694369e15 100644\n>>> --- a/include/linux/page-flags.h\n>>> +++ b/include/linux/page-flags.h\n>>> @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n>>>    \treturn folio_test_head(folio);\n>>>    }\n>>> -static __always_inline void set_compound_head(struct page *page, struct page *head)\n>>> +static __always_inline void set_compound_head(struct page *page,\n>>> +\t\t\t\t\t      const struct page *head,\n>>> +\t\t\t\t\t      unsigned int order)\n>>\n>> Two tab indents please on second+ parameter list whenever you touch code.\n> \n> Do we have this coding style preference written down somewhere?\n\nGood question. I assume not. But it's what we do in MM :)\n\n> \n> -tip tree wants the opposite. Documentation/process/maintainer-tip.rst:\n> \n> \tWhen splitting function declarations or function calls, then please align\n> \tthe first argument in the second line with the first argument in the first\n> \tline::\n> \n> I want the editor to do The Right Thing without my brain involvement.\n> Having different coding styles in different corners of the kernel makes\n> it hard.\n\nYeah, but unavoidable. :)\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/4/26 17:56, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n>> struct pages of the head page to be naturally aligned with regard to the\n>> folio size.\n>>\n>> Align vmemmap to MAX_FOLIO_NR_PAGES.\n>>\n>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>> ---\n>>  arch/loongarch/include/asm/pgtable.h | 3 ++-\n>>  1 file changed, 2 insertions(+), 1 deletion(-)\n>>\n>> diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/ \n>> include/asm/pgtable.h\n>> index c33b3bcb733e..f9416acb9156 100644\n>> --- a/arch/loongarch/include/asm/pgtable.h\n>> +++ b/arch/loongarch/include/asm/pgtable.h\n>> @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / \n>> sizeof(unsigned long)];\n>>  min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * \n>> PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - \n>> KFENCE_AREA_SIZE)\n>>  #endif\n>> -#define vmemmap ((struct page *)((VMALLOC_END + PMD_SIZE) & \n>> PMD_MASK))\n>> +#define VMEMMAP_ALIGN max(PMD_SIZE, MAX_FOLIO_NR_PAGES * \n>> sizeof(struct page))\n>> +#define vmemmap ((struct page *)(ALIGN(VMALLOC_END, \n>> VMEMMAP_ALIGN)))\n> \n> \n> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just \n> black magic here\n> and the description of the situation is wrong.\n> \n> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct \n> page)\" into the core and call it\n> \n> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct \n> page))\n> \n> But then special case it base on (a) HVO being configured in an (b) HVO \n> being possible\n> \n> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n> /* A very helpful comment explaining the situation. */\n> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct \n> page))\n> #else\n> #define MAX_FOLIO_VMEMMAP_ALIGN 0\n> #endif\n> \n> Something like that.\n> \n\nThinking about this ...\n\nthe vmemmap start is always struct-page-aligned. Otherwise we'd be in \ntrouble already.\n\nIsn't it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?\n\nLet's assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for \nsimplicity.\n\nvmemmap start would be multiples of 512 (0x0010000000).\n\n512, 1024, 1536, 2048 ...\n\nAssume we have an 256-pages folio at 1536+256 = 0x111000000\n\nAssume we have the last page of that folio (0x011111111111), we would \njust get to the start of that folio by AND-ing with ~(256-1).\n\nWhich case am I ignoring?\n\n-- \nCheers,\n\nDavid",
              "reply_to": "",
              "message_date": "2026-02-04",
              "message_id": "a61bc0a8-cf5a-418a-aeb4-96553b87f043@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "David (arm)",
              "summary": "Reviewer David (arm) noted that the patch introduces a new alignment requirement for vmemmap, which is not strictly necessary because vmemmap start is always struct-page-aligned. He suggested pulling the magic number 'MAX_FOLIO_NR_PAGES * sizeof(struct page)' into the core and defining it conditionally based on HVO configuration and the power-of-2 property of sizeof(struct page).",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvement"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On 2/4/26 17:56, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n>> struct pages of the head page to be naturally aligned with regard to the\n>> folio size.\n>>\n>> Align vmemmap to MAX_FOLIO_NR_PAGES.\n>>\n>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>> ---\n>>  arch/loongarch/include/asm/pgtable.h | 3 ++-\n>>  1 file changed, 2 insertions(+), 1 deletion(-)\n>>\n>> diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/ \n>> include/asm/pgtable.h\n>> index c33b3bcb733e..f9416acb9156 100644\n>> --- a/arch/loongarch/include/asm/pgtable.h\n>> +++ b/arch/loongarch/include/asm/pgtable.h\n>> @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / \n>> sizeof(unsigned long)];\n>>  min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * \n>> PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - \n>> KFENCE_AREA_SIZE)\n>>  #endif\n>> -#define vmemmap ((struct page *)((VMALLOC_END + PMD_SIZE) & \n>> PMD_MASK))\n>> +#define VMEMMAP_ALIGN max(PMD_SIZE, MAX_FOLIO_NR_PAGES * \n>> sizeof(struct page))\n>> +#define vmemmap ((struct page *)(ALIGN(VMALLOC_END, \n>> VMEMMAP_ALIGN)))\n> \n> \n> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just \n> black magic here\n> and the description of the situation is wrong.\n> \n> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct \n> page)\" into the core and call it\n> \n> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct \n> page))\n> \n> But then special case it base on (a) HVO being configured in an (b) HVO \n> being possible\n> \n> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n> /* A very helpful comment explaining the situation. */\n> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct \n> page))\n> #else\n> #define MAX_FOLIO_VMEMMAP_ALIGN 0\n> #endif\n> \n> Something like that.\n> \n\nThinking about this ...\n\nthe vmemmap start is always struct-page-aligned. Otherwise we'd be in \ntrouble already.\n\nIsn't it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?\n\nLet's assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for \nsimplicity.\n\nvmemmap start would be multiples of 512 (0x0010000000).\n\n512, 1024, 1536, 2048 ...\n\nAssume we have an 256-pages folio at 1536+256 = 0x111000000\n\nAssume we have the last page of that folio (0x011111111111), we would \njust get to the start of that folio by AND-ing with ~(256-1).\n\nWhich case am I ignoring?\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> With the upcoming changes to HVO, a single page of tail struct pages\n> will be shared across all huge pages of the same order on a node. Since\n> huge pages on the same node may belong to different zones, the zone\n> information stored in shared tail page flags would be incorrect.\n> \n> Always fetch zone information from the head page, which has unique and\n> correct zone flags for each compound page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   include/linux/mmzone.h | 1 +\n>   1 file changed, 1 insertion(+)\n> \n> diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> index be8ce40b5638..192143b5cdc0 100644\n> --- a/include/linux/mmzone.h\n> +++ b/include/linux/mmzone.h\n> @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n>   \n>   static inline enum zone_type page_zonenum(const struct page *page)\n>   {\n> +\tpage = compound_head(page);\n>   \treturn memdesc_zonenum(page->flags);\n\nWe end up calling page_zonenum() without holding a reference.\n\nGiven that _compound_head() does a READ_ONCE(), this should work even if \nwe see concurrent page freeing etc.\n\nHowever, this change implies that we now perform a compound page lookup \nfor every PageHighMem() [meh], page_zone() [quite some users in the \nbuddy, including for pageblock access and page freeing].\n\nThat's a nasty compromise for making HVO better? :)\n\nWe should likely limit that special casing to kernels that really rquire \nit (HVO).\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> With the upcoming changes to HVO, a single page of tail struct pages\n> will be shared across all huge pages of the same order on a node. Since\n> huge pages on the same node may belong to different zones, the zone\n> information stored in shared tail page flags would be incorrect.\n> \n> Always fetch zone information from the head page, which has unique and\n> correct zone flags for each compound page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   include/linux/mmzone.h | 1 +\n>   1 file changed, 1 insertion(+)\n> \n> diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> index be8ce40b5638..192143b5cdc0 100644\n> --- a/include/linux/mmzone.h\n> +++ b/include/linux/mmzone.h\n> @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n>   \n>   static inline enum zone_type page_zonenum(const struct page *page)\n>   {\n> +\tpage = compound_head(page);\n>   \treturn memdesc_zonenum(page->flags);\n\nWe end up calling page_zonenum() without holding a reference.\n\nGiven that _compound_head() does a READ_ONCE(), this should work even if \nwe see concurrent page freeing etc.\n\nHowever, this change implies that we now perform a compound page lookup \nfor every PageHighMem() [meh], page_zone() [quite some users in the \nbuddy, including for pageblock access and page freeing].\n\nThat's a nasty compromise for making HVO better? :)\n\nWe should likely limit that special casing to kernels that really rquire \nit (HVO).\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> If page->compound_info encodes a mask, it is expected that vmemmap to be\n> naturally aligned to the maximum folio size.\n> \n> Add a VM_BUG_ON() to check the alignment.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   mm/sparse.c | 7 +++++++\n>   1 file changed, 7 insertions(+)\n> \n> diff --git a/mm/sparse.c b/mm/sparse.c\n> index b5b2b6f7041b..6c9b62607f3f 100644\n> --- a/mm/sparse.c\n> +++ b/mm/sparse.c\n> @@ -600,6 +600,13 @@ void __init sparse_init(void)\n>   \tBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n>   \tmemblocks_present();\n>   \n> +\tif (compound_info_has_mask()) {\n> +\t\tunsigned long alignment;\n> +\n> +\t\talignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);\n> +\t\tVM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), alignment));\n\nNo VM_BUG_ON. VM_WARN_ON_ONCE() should be good enough, no?\n\nAs discussed in the other thread, is checking for MAX_FOLIO_NR_PAGES \nalignment sufficient?\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> If page->compound_info encodes a mask, it is expected that vmemmap to be\n> naturally aligned to the maximum folio size.\n> \n> Add a VM_BUG_ON() to check the alignment.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   mm/sparse.c | 7 +++++++\n>   1 file changed, 7 insertions(+)\n> \n> diff --git a/mm/sparse.c b/mm/sparse.c\n> index b5b2b6f7041b..6c9b62607f3f 100644\n> --- a/mm/sparse.c\n> +++ b/mm/sparse.c\n> @@ -600,6 +600,13 @@ void __init sparse_init(void)\n>   \tBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n>   \tmemblocks_present();\n>   \n> +\tif (compound_info_has_mask()) {\n> +\t\tunsigned long alignment;\n> +\n> +\t\talignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);\n> +\t\tVM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), alignment));\n\nNo VM_BUG_ON. VM_WARN_ON_ONCE() should be good enough, no?\n\nAs discussed in the other thread, is checking for MAX_FOLIO_NR_PAGES \nalignment sufficient?\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/5/26 14:43, Kiryl Shutsemau wrote:\n> On Thu, Feb 05, 2026 at 01:56:36PM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/4/26 17:56, David Hildenbrand (arm) wrote:\n>>>\n>>>\n>>> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just\n>>> black magic here\n>>> and the description of the situation is wrong.\n>>>\n>>> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct\n>>> page)\" into the core and call it\n>>>\n>>> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n>>> page))\n>>>\n>>> But then special case it base on (a) HVO being configured in an (b) HVO\n>>> being possible\n>>>\n>>> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n>>> /* A very helpful comment explaining the situation. */\n>>> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n>>> page))\n>>> #else\n>>> #define MAX_FOLIO_VMEMMAP_ALIGN 0\n>>> #endif\n>>>\n>>> Something like that.\n>>>\n>>\n>> Thinking about this ...\n>>\n>> the vmemmap start is always struct-page-aligned. Otherwise we'd be in\n>> trouble already.\n>>\n>> Isn't it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?\n>>\n>> Let's assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for\n>> simplicity.\n>>\n>> vmemmap start would be multiples of 512 (0x0010000000).\n>>\n>> 512, 1024, 1536, 2048 ...\n>>\n>> Assume we have an 256-pages folio at 1536+256 = 0x111000000\n> \n> s/0x/0b/, but okay.\n\n:)\n\n> \n>> Assume we have the last page of that folio (0x011111111111), we would just\n>> get to the start of that folio by AND-ing with ~(256-1).\n>>\n>> Which case am I ignoring?\n> \n> IIUC, you are ignoring the actual size of struct page. It is not 1 byte :P\n\nI thought it wouldn't matter but, yeah, that's it.\n\n\"Align the vmemmap to the maximum folio metadata size\" it is.\n\nThen you can explain the situation also alongside \nMAX_FOLIO_VMEMMAP_ALIGN, and that we expect this to be a power of 2.\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/5/26 14:43, Kiryl Shutsemau wrote:\n> On Thu, Feb 05, 2026 at 01:56:36PM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/4/26 17:56, David Hildenbrand (arm) wrote:\n>>>\n>>>\n>>> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just\n>>> black magic here\n>>> and the description of the situation is wrong.\n>>>\n>>> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct\n>>> page)\" into the core and call it\n>>>\n>>> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n>>> page))\n>>>\n>>> But then special case it base on (a) HVO being configured in an (b) HVO\n>>> being possible\n>>>\n>>> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n>>> /* A very helpful comment explaining the situation. */\n>>> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n>>> page))\n>>> #else\n>>> #define MAX_FOLIO_VMEMMAP_ALIGN 0\n>>> #endif\n>>>\n>>> Something like that.\n>>>\n>>\n>> Thinking about this ...\n>>\n>> the vmemmap start is always struct-page-aligned. Otherwise we'd be in\n>> trouble already.\n>>\n>> Isn't it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?\n>>\n>> Let's assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for\n>> simplicity.\n>>\n>> vmemmap start would be multiples of 512 (0x0010000000).\n>>\n>> 512, 1024, 1536, 2048 ...\n>>\n>> Assume we have an 256-pages folio at 1536+256 = 0x111000000\n> \n> s/0x/0b/, but okay.\n\n:)\n\n> \n>> Assume we have the last page of that folio (0x011111111111), we would just\n>> get to the start of that folio by AND-ing with ~(256-1).\n>>\n>> Which case am I ignoring?\n> \n> IIUC, you are ignoring the actual size of struct page. It is not 1 byte :P\n\nI thought it wouldn't matter but, yeah, that's it.\n\n\"Align the vmemmap to the maximum folio metadata size\" it is.\n\nThen you can explain the situation also alongside \nMAX_FOLIO_VMEMMAP_ALIGN, and that we expect this to be a power of 2.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/5/26 14:50, Kiryl Shutsemau wrote:\n> On Wed, Feb 04, 2026 at 05:50:23PM +0100, David Hildenbrand (arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>>> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n>>> struct pages of the head page to be naturally aligned with regard to the\n>>> folio size.\n>>>\n>>> Align vmemmap to MAX_FOLIO_NR_PAGES.\n>>\n>> I think neither that statement nor the one in the patch description is\n>> correct?\n>>\n>> \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is neither the maximum folio size\n>> nor MAX_FOLIO_NR_PAGES.\n>>\n>> It's the size of the memmap that a large folio could span at maximum.\n>>\n>>\n>> Assuming we have a 16 GiB folio, the calculation would give us\n>>\n>> \t4194304 * sizeof(struct page)\n>>\n>> Which could be something like (assuming 80 bytes)\n>>\n>> \t335544320\n>>\n>> -> not even a power of 2, weird? (for HVO you wouldn't care as HVO would be\n>> disabled, but that aliment is super weird?)\n>>\n>>\n>> Assuming 64 bytes, it would be a power of two (as 64 is a power of two).\n>>\n>> \t268435456 (1<< 28)\n>>\n>>\n>> Which makes me wonder whether there is a way to avoid sizeof(struct page)\n>> here completely.\n> \n> I don't think we can. See the other thread.\n\nAgreed. You could only go for something larger (like PAGE_SIZE).\n\n> \n> What about using roundup_pow_of_two(sizeof(struct page)) here.\n\nBetter I think.\n\n> \n>> Or limit the alignment to the case where HVO is actually active and\n>> sizeof(struct page) makes any sense?\n> \n> The annoying part of HVO is that it is unknown at compile-time if it\n> will be used. You can compile kernel with HVO that will no be activated\n> due to non-power-of-2 sizeof(struct page) because of a debug config option.\nAh, and now I remember that sizeof cannot be used in macros, damnit.\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/5/26 14:50, Kiryl Shutsemau wrote:\n> On Wed, Feb 04, 2026 at 05:50:23PM +0100, David Hildenbrand (arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>>> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n>>> struct pages of the head page to be naturally aligned with regard to the\n>>> folio size.\n>>>\n>>> Align vmemmap to MAX_FOLIO_NR_PAGES.\n>>\n>> I think neither that statement nor the one in the patch description is\n>> correct?\n>>\n>> \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is neither the maximum folio size\n>> nor MAX_FOLIO_NR_PAGES.\n>>\n>> It's the size of the memmap that a large folio could span at maximum.\n>>\n>>\n>> Assuming we have a 16 GiB folio, the calculation would give us\n>>\n>> \t4194304 * sizeof(struct page)\n>>\n>> Which could be something like (assuming 80 bytes)\n>>\n>> \t335544320\n>>\n>> -> not even a power of 2, weird? (for HVO you wouldn't care as HVO would be\n>> disabled, but that aliment is super weird?)\n>>\n>>\n>> Assuming 64 bytes, it would be a power of two (as 64 is a power of two).\n>>\n>> \t268435456 (1<< 28)\n>>\n>>\n>> Which makes me wonder whether there is a way to avoid sizeof(struct page)\n>> here completely.\n> \n> I don't think we can. See the other thread.\n\nAgreed. You could only go for something larger (like PAGE_SIZE).\n\n> \n> What about using roundup_pow_of_two(sizeof(struct page)) here.\n\nBetter I think.\n\n> \n>> Or limit the alignment to the case where HVO is actually active and\n>> sizeof(struct page) makes any sense?\n> \n> The annoying part of HVO is that it is unknown at compile-time if it\n> will be used. You can compile kernel with HVO that will no be activated\n> due to non-power-of-2 sizeof(struct page) because of a debug config option.\nAh, and now I remember that sizeof cannot be used in macros, damnit.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/5/26 14:52, Kiryl Shutsemau wrote:\n> On Wed, Feb 04, 2026 at 05:56:45PM +0100, David Hildenbrand (arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>>> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n>>> struct pages of the head page to be naturally aligned with regard to the\n>>> folio size.\n>>>\n>>> Align vmemmap to MAX_FOLIO_NR_PAGES.\n>>>\n>>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>>> ---\n>>>    arch/loongarch/include/asm/pgtable.h | 3 ++-\n>>>    1 file changed, 2 insertions(+), 1 deletion(-)\n>>>\n>>> diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\n>>> index c33b3bcb733e..f9416acb9156 100644\n>>> --- a/arch/loongarch/include/asm/pgtable.h\n>>> +++ b/arch/loongarch/include/asm/pgtable.h\n>>> @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n>>>    \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n>>>    #endif\n>>> -#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n>>> +#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n>>> +#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n>>\n>>\n>> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just black magic here\n>> and the description of the situation is wrong.\n>>\n>> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" into the core and call it\n>>\n>> #define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n>>\n>> But then special case it base on (a) HVO being configured in an (b) HVO being possible\n>>\n>> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n> \n> This would require some kind of asm-offsets.c/bounds.c magic to pull the\n> struct page size condition to the preprocessor level.\n> \n\nRight.\n\nI guess you could move that into the macro and let the compiler handle it.\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/5/26 14:52, Kiryl Shutsemau wrote:\n> On Wed, Feb 04, 2026 at 05:56:45PM +0100, David Hildenbrand (arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>>> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n>>> struct pages of the head page to be naturally aligned with regard to the\n>>> folio size.\n>>>\n>>> Align vmemmap to MAX_FOLIO_NR_PAGES.\n>>>\n>>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>>> ---\n>>>    arch/loongarch/include/asm/pgtable.h | 3 ++-\n>>>    1 file changed, 2 insertions(+), 1 deletion(-)\n>>>\n>>> diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\n>>> index c33b3bcb733e..f9416acb9156 100644\n>>> --- a/arch/loongarch/include/asm/pgtable.h\n>>> +++ b/arch/loongarch/include/asm/pgtable.h\n>>> @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n>>>    \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n>>>    #endif\n>>> -#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n>>> +#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n>>> +#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n>>\n>>\n>> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just black magic here\n>> and the description of the situation is wrong.\n>>\n>> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" into the core and call it\n>>\n>> #define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n>>\n>> But then special case it base on (a) HVO being configured in an (b) HVO being possible\n>>\n>> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n> \n> This would require some kind of asm-offsets.c/bounds.c magic to pull the\n> struct page size condition to the preprocessor level.\n> \n\nRight.\n\nI guess you could move that into the macro and let the compiler handle it.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/5/26 14:31, David Hildenbrand (Arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>> If page->compound_info encodes a mask, it is expected that vmemmap to be\n>> naturally aligned to the maximum folio size.\n>>\n>> Add a VM_BUG_ON() to check the alignment.\n>>\n>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>> Acked-by: Zi Yan <ziy@nvidia.com>\n>> ---\n>>  mm/sparse.c | 7 +++++++\n>>  1 file changed, 7 insertions(+)\n>>\n>> diff --git a/mm/sparse.c b/mm/sparse.c\n>> index b5b2b6f7041b..6c9b62607f3f 100644\n>> --- a/mm/sparse.c\n>> +++ b/mm/sparse.c\n>> @@ -600,6 +600,13 @@ void __init sparse_init(void)\n>>  BUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n>>  memblocks_present();\n>> + if (compound_info_has_mask()) {\n>> + unsigned long alignment;\n>> +\n>> + alignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);\n>> + VM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), \n>> alignment));\n> \n> No VM_BUG_ON. VM_WARN_ON_ONCE() should be good enough, no?\n> \n> As discussed in the other thread, is checking for MAX_FOLIO_NR_PAGES \n> alignment sufficient?\n\nAnd after further discussions, we could use MAX_FOLIO_VMEMMAP_ALIGN \nmacro once we have that.\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/5/26 14:31, David Hildenbrand (Arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>> If page->compound_info encodes a mask, it is expected that vmemmap to be\n>> naturally aligned to the maximum folio size.\n>>\n>> Add a VM_BUG_ON() to check the alignment.\n>>\n>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>> Acked-by: Zi Yan <ziy@nvidia.com>\n>> ---\n>>  mm/sparse.c | 7 +++++++\n>>  1 file changed, 7 insertions(+)\n>>\n>> diff --git a/mm/sparse.c b/mm/sparse.c\n>> index b5b2b6f7041b..6c9b62607f3f 100644\n>> --- a/mm/sparse.c\n>> +++ b/mm/sparse.c\n>> @@ -600,6 +600,13 @@ void __init sparse_init(void)\n>>  BUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n>>  memblocks_present();\n>> + if (compound_info_has_mask()) {\n>> + unsigned long alignment;\n>> +\n>> + alignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);\n>> + VM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), \n>> alignment));\n> \n> No VM_BUG_ON. VM_WARN_ON_ONCE() should be good enough, no?\n> \n> As discussed in the other thread, is checking for MAX_FOLIO_NR_PAGES \n> alignment sufficient?\n\nAnd after further discussions, we could use MAX_FOLIO_VMEMMAP_ALIGN \nmacro once we have that.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-05",
              "message_id": "062900fa-6419-4748-81d1-9128ce6c46d0@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "David (arm)",
              "summary": "Reviewer David noted that clearing bit 0 before applying the mask to get the head page address is unnecessary, as the page pointer should not have set it in the first place.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "signal1",
                "signal2"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> For tail pages, the kernel uses the 'compound_info' field to get to the\n> head page. The bit 0 of the field indicates whether the page is a\n> tail page, and if set, the remaining bits represent a pointer to the\n> head page.\n> \n> For cases when size of struct page is power-of-2, change the encoding of\n> compound_info to store a mask that can be applied to the virtual address\n> of the tail page in order to access the head page. It is possible\n> because struct page of the head page is naturally aligned with regards\n> to order of the page.\n> \n> The significant impact of this modification is that all tail pages of\n> the same order will now have identical 'compound_info', regardless of\n> the compound page they are associated with. This paves the way for\n> eliminating fake heads.\n> \n> The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\n> applied when the sizeof(struct page) is power-of-2. Having identical\n> tail pages allows the same page to be mapped into the vmemmap of all\n> pages, maintaining memory savings without fake heads.\n> \n> If sizeof(struct page) is not power-of-2, there is no functional\n> changes.\n> \n> Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\n> a difference. The approach with mask would work in the wider set of\n> conditions, but it requires validating that struct pages are naturally\n> aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\n[...]\n\n>   \tstruct folio *foliop;\n>   \tint loops = 5;\n>   \n> @@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n>   again:\n>   \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n>   \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n> -\thead = ps->page_snapshot.compound_info;\n> -\tif ((head & 1) == 0) {\n> +\tinfo = ps->page_snapshot.compound_info;\n> +\tif (!(info & 1)) {\n>   \t\tps->idx = 0;\n>   \t\tfoliop = (struct folio *)&ps->page_snapshot;\n>   \t\tif (!folio_test_large(foliop)) {\n> @@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n>   \t\t}\n>   \t\tfoliop = (struct folio *)page;\n>   \t} else {\n> -\t\tfoliop = (struct folio *)(head - 1);\n> +\t\t/* See compound_head() */\n> +\t\tif (compound_info_has_mask()) {\n> +\t\t\tunsigned long p = (unsigned long)page;\n> +\n> +\t\t\tfoliop = (struct folio *)(p & info);\n\nIIUC, we don't care about clearing bit0 before the & as the page pointer \nshouldn't have set it in the first page.\n\nPretty neat\n\nAcked-by: David Hildenbrand (Arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> For tail pages, the kernel uses the 'compound_info' field to get to the\n> head page. The bit 0 of the field indicates whether the page is a\n> tail page, and if set, the remaining bits represent a pointer to the\n> head page.\n> \n> For cases when size of struct page is power-of-2, change the encoding of\n> compound_info to store a mask that can be applied to the virtual address\n> of the tail page in order to access the head page. It is possible\n> because struct page of the head page is naturally aligned with regards\n> to order of the page.\n> \n> The significant impact of this modification is that all tail pages of\n> the same order will now have identical 'compound_info', regardless of\n> the compound page they are associated with. This paves the way for\n> eliminating fake heads.\n> \n> The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\n> applied when the sizeof(struct page) is power-of-2. Having identical\n> tail pages allows the same page to be mapped into the vmemmap of all\n> pages, maintaining memory savings without fake heads.\n> \n> If sizeof(struct page) is not power-of-2, there is no functional\n> changes.\n> \n> Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\n> a difference. The approach with mask would work in the wider set of\n> conditions, but it requires validating that struct pages are naturally\n> aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\n[...]\n\n>   \tstruct folio *foliop;\n>   \tint loops = 5;\n>   \n> @@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n>   again:\n>   \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n>   \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n> -\thead = ps->page_snapshot.compound_info;\n> -\tif ((head & 1) == 0) {\n> +\tinfo = ps->page_snapshot.compound_info;\n> +\tif (!(info & 1)) {\n>   \t\tps->idx = 0;\n>   \t\tfoliop = (struct folio *)&ps->page_snapshot;\n>   \t\tif (!folio_test_large(foliop)) {\n> @@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n>   \t\t}\n>   \t\tfoliop = (struct folio *)page;\n>   \t} else {\n> -\t\tfoliop = (struct folio *)(head - 1);\n> +\t\t/* See compound_head() */\n> +\t\tif (compound_info_has_mask()) {\n> +\t\t\tunsigned long p = (unsigned long)page;\n> +\n> +\t\t\tfoliop = (struct folio *)(p & info);\n\nIIUC, we don't care about clearing bit0 before the & as the page pointer \nshouldn't have set it in the first page.\n\nPretty neat\n\nAcked-by: David Hildenbrand (Arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/16/26 00:13, Matthew Wilcox wrote:\n> On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n>> With the upcoming changes to HVO, a single page of tail struct pages\n>> will be shared across all huge pages of the same order on a node. Since\n>> huge pages on the same node may belong to different zones, the zone\n>> information stored in shared tail page flags would be incorrect.\n>>\n>> Always fetch zone information from the head page, which has unique and\n>> correct zone flags for each compound page.\n> \n> You're right that different pages in the same folio can have different\n> zone number.  But does it matter ... or to put it another way, why is\n> returning the zone number of the head page the correct way to resolve\n> this?\n\nHow can a folio cross zones?\n\nRuntime allocated hugetlb folios from the CMA/buddy (alloc_contig_range) \ndefinitely fall into a single zone.\n\nSo is it about ones allocated early during boot, where, by chance, we \nmanage to cross ZONE_NORMAL + ZONE_MOVABLE etc?\n\nI thought that it's also not allowed there, and I wonder whether we \nshould disallow it if it's possible.\n\n> \n> Arguably, the caller is asking for the zone number of _this page_, and\n> does not care about the zone number of the head page.  It would be good\n> to have a short discussion of this in the commit message (but probably\n> not worth putting this in a comment).\n\nAgreed, in particular, if there would be a functional change. So far I \nassumed there would be no such change.\n\nThings like shrink_zone_span() really need to know the zone of that \npage, not the one of the head; unless both fall into the same zone.\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/16/26 00:13, Matthew Wilcox wrote:\n> On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n>> With the upcoming changes to HVO, a single page of tail struct pages\n>> will be shared across all huge pages of the same order on a node. Since\n>> huge pages on the same node may belong to different zones, the zone\n>> information stored in shared tail page flags would be incorrect.\n>>\n>> Always fetch zone information from the head page, which has unique and\n>> correct zone flags for each compound page.\n> \n> You're right that different pages in the same folio can have different\n> zone number.  But does it matter ... or to put it another way, why is\n> returning the zone number of the head page the correct way to resolve\n> this?\n\nHow can a folio cross zones?\n\nRuntime allocated hugetlb folios from the CMA/buddy (alloc_contig_range) \ndefinitely fall into a single zone.\n\nSo is it about ones allocated early during boot, where, by chance, we \nmanage to cross ZONE_NORMAL + ZONE_MOVABLE etc?\n\nI thought that it's also not allowed there, and I wonder whether we \nshould disallow it if it's possible.\n\n> \n> Arguably, the caller is asking for the zone number of _this page_, and\n> does not care about the zone number of the head page.  It would be good\n> to have a short discussion of this in the commit message (but probably\n> not worth putting this in a comment).\n\nAgreed, in particular, if there would be a functional change. So far I \nassumed there would be no such change.\n\nThings like shrink_zone_span() really need to know the zone of that \npage, not the one of the head; unless both fall into the same zone.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/23/26 19:18, Matthew Wilcox wrote:\n> On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/16/26 00:13, Matthew Wilcox wrote:\n>>>\n>>> You're right that different pages in the same folio can have different\n>>> zone number.  But does it matter ... or to put it another way, why is\n>>> returning the zone number of the head page the correct way to resolve\n>>> this?\n>>\n>> How can a folio cross zones?\n> \n> I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be\n> true and isn't any more, or maybe it was never true and I was just\n> confused.\n\nI recall that 1G folios could end up in ZONE_MOVABLE (comment in\npage_is_unmovable()), but my memory is fuzzy when it comes to crossing\nzones (ZONE_NORMAL -> ZONE_MOVABLE).\n\nFreeing+reinitializing the vmemmap for HVO with such folios would\nalready be problematic I suppose: we would silently switch the zone for\nsome of these pages.\n\nWhen freeing such (boottime) hugetlb folios to the buddy, we use\nfree_frozen_pages(). In there we lookup the zone once.\n\nLikely also problematic :)\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/23/26 19:18, Matthew Wilcox wrote:\n> On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/16/26 00:13, Matthew Wilcox wrote:\n>>>\n>>> You're right that different pages in the same folio can have different\n>>> zone number.  But does it matter ... or to put it another way, why is\n>>> returning the zone number of the head page the correct way to resolve\n>>> this?\n>>\n>> How can a folio cross zones?\n> \n> I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be\n> true and isn't any more, or maybe it was never true and I was just\n> confused.\n\nI recall that 1G folios could end up in ZONE_MOVABLE (comment in\npage_is_unmovable()), but my memory is fuzzy when it comes to crossing\nzones (ZONE_NORMAL -> ZONE_MOVABLE).\n\nFreeing+reinitializing the vmemmap for HVO with such folios would\nalready be problematic I suppose: we would silently switch the zone for\nsome of these pages.\n\nWhen freeing such (boottime) hugetlb folios to the buddy, we use\nfree_frozen_pages(). In there we lookup the zone once.\n\nLikely also problematic :)\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-05",
              "message_id": "6e182cb7-6f32-41c0-ba86-520728b161c7@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Usama Arif",
              "summary": "Reviewer Usama Arif pointed out that the new mask-based compound_head() encoding has a potential issue: it requires validating that struct pages are naturally aligned for all orders up to MAX_FOLIO_ORDER, which can be tricky. He suggested limiting the usage of this approach to HugeTLB vmemmap optimization (HVO) where it makes a difference.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "limiting mask usage",
                "potential issue with alignment"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "On 02/02/2026 15:56, Kiryl Shutsemau wrote:\n> For tail pages, the kernel uses the 'compound_info' field to get to the\n> head page. The bit 0 of the field indicates whether the page is a\n> tail page, and if set, the remaining bits represent a pointer to the\n> head page.\n> \n> For cases when size of struct page is power-of-2, change the encoding of\n> compound_info to store a mask that can be applied to the virtual address\n> of the tail page in order to access the head page. It is possible\n> because struct page of the head page is naturally aligned with regards\n> to order of the page.\n> \n> The significant impact of this modification is that all tail pages of\n> the same order will now have identical 'compound_info', regardless of\n> the compound page they are associated with. This paves the way for\n> eliminating fake heads.\n> \n> The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\n> applied when the sizeof(struct page) is power-of-2. Having identical\n> tail pages allows the same page to be mapped into the vmemmap of all\n> pages, maintaining memory savings without fake heads.\n> \n> If sizeof(struct page) is not power-of-2, there is no functional\n> changes.\n> \n> Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\n> a difference. The approach with mask would work in the wider set of\n> conditions, but it requires validating that struct pages are naturally\n> aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n\nAcked-by: Usama Arif <usamaarif642@gmail.com>\n\nSmall nit below:\n\n>  include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n>  mm/slab.h                  | 16 ++++++--\n>  mm/util.c                  | 16 ++++++--\n>  3 files changed, 97 insertions(+), 16 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index d14a17ffb55b..8f2c7fbc739b 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -198,6 +198,29 @@ enum pageflags {\n>  \n>  #ifndef __GENERATING_BOUNDS_H\n>  \n> +/*\n> + * For tail pages, if the size of struct page is power-of-2 ->compound_info\n> + * encodes the mask that converts the address of the tail page address to\n> + * the head page address.\n> + *\n> + * Otherwise, ->compound_info has direct pointer to head pages.\n> + */\n> +static __always_inline bool compound_info_has_mask(void)\n> +{\n> +\t/*\n> +\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n> +\t * makes a difference.\n> +\t *\n> +\t * The approach with mask would work in the wider set of conditions,\n> +\t * but it requires validating that struct pages are naturally aligned\n> +\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> +\t */\n> +\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n> +\t\treturn false;\n> +\n> +\treturn is_power_of_2(sizeof(struct page));\n> +}\n> +\n>  #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n>  DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n>  \n> @@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n>  \t\treturn page;\n>  \n> +\t/* Fake heads only exists if compound_info_has_mask() is true */\n> +\tif (!compound_info_has_mask())\n> +\t\treturn page;\n> +\n>  \t/*\n>  \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n>  \t * struct page. The alignment check aims to avoid access the fields (\n> @@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \t\t * because the @page is a compound page composed with at least\n>  \t\t * two contiguous pages.\n>  \t\t */\n> -\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n> +\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n>  \n> -\t\tif (likely(head & 1))\n> -\t\t\treturn (const struct page *)(head - 1);\n> +\t\t/* See set_compound_head() */\n> +\t\tif (likely(info & 1)) {\n> +\t\t\tunsigned long p = (unsigned long)page;\n> +\n> +\t\t\treturn (const struct page *)(p & info);\n> +\t\t}\n>  \t}\n>  \treturn page;\n>  }\n> @@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)\n>  \n>  static __always_inline unsigned long _compound_head(const struct page *page)\n>  {\n> -\tunsigned long head = READ_ONCE(page->compound_info);\n> +\tunsigned long info = READ_ONCE(page->compound_info);\n>  \n> -\tif (unlikely(head & 1))\n> -\t\treturn head - 1;\n> -\treturn (unsigned long)page_fixed_fake_head(page);\n> +\t/* Bit 0 encodes PageTail() */\n> +\tif (!(info & 1))\n> +\t\treturn (unsigned long)page_fixed_fake_head(page);\n> +\n> +\t/*\n> +\t * If compound_info_has_mask() is false, the rest of compound_info is\n> +\t * the pointer to the head page.\n> +\t */\n> +\tif (!compound_info_has_mask())\n> +\t\treturn info - 1;\n> +\n> +\t/*\n> +\t * If compoun_info_has_mask() is true the rest of the info encodes\n\ns/compoun_info_has_mask/compound_info_has_mask/\n\n---\n\nOn 02/02/2026 15:56, Kiryl Shutsemau wrote:\n> For tail pages, the kernel uses the 'compound_info' field to get to the\n> head page. The bit 0 of the field indicates whether the page is a\n> tail page, and if set, the remaining bits represent a pointer to the\n> head page.\n> \n> For cases when size of struct page is power-of-2, change the encoding of\n> compound_info to store a mask that can be applied to the virtual address\n> of the tail page in order to access the head page. It is possible\n> because struct page of the head page is naturally aligned with regards\n> to order of the page.\n> \n> The significant impact of this modification is that all tail pages of\n> the same order will now have identical 'compound_info', regardless of\n> the compound page they are associated with. This paves the way for\n> eliminating fake heads.\n> \n> The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\n> applied when the sizeof(struct page) is power-of-2. Having identical\n> tail pages allows the same page to be mapped into the vmemmap of all\n> pages, maintaining memory savings without fake heads.\n> \n> If sizeof(struct page) is not power-of-2, there is no functional\n> changes.\n> \n> Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\n> a difference. The approach with mask would work in the wider set of\n> conditions, but it requires validating that struct pages are naturally\n> aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n\nAcked-by: Usama Arif <usamaarif642@gmail.com>\n\nSmall nit below:\n\n>  include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n>  mm/slab.h                  | 16 ++++++--\n>  mm/util.c                  | 16 ++++++--\n>  3 files changed, 97 insertions(+), 16 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index d14a17ffb55b..8f2c7fbc739b 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -198,6 +198,29 @@ enum pageflags {\n>  \n>  #ifndef __GENERATING_BOUNDS_H\n>  \n> +/*\n> + * For tail pages, if the size of struct page is power-of-2 ->compound_info\n> + * encodes the mask that converts the address of the tail page address to\n> + * the head page address.\n> + *\n> + * Otherwise, ->compound_info has direct pointer to head pages.\n> + */\n> +static __always_inline bool compound_info_has_mask(void)\n> +{\n> +\t/*\n> +\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n> +\t * makes a difference.\n> +\t *\n> +\t * The approach with mask would work in the wider set of conditions,\n> +\t * but it requires validating that struct pages are naturally aligned\n> +\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> +\t */\n> +\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n> +\t\treturn false;\n> +\n> +\treturn is_power_of_2(sizeof(struct page));\n> +}\n> +\n>  #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n>  DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n>  \n> @@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n>  \t\treturn page;\n>  \n> +\t/* Fake heads only exists if compound_info_has_mask() is true */\n> +\tif (!compound_info_has_mask())\n> +\t\treturn page;\n> +\n>  \t/*\n>  \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n>  \t * struct page. The alignment check aims to avoid access the fields (\n> @@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \t\t * because the @page is a compound page composed with at least\n>  \t\t * two contiguous pages.\n>  \t\t */\n> -\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n> +\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n>  \n> -\t\tif (likely(head & 1))\n> -\t\t\treturn (const struct page *)(head - 1);\n> +\t\t/* See set_compound_head() */\n> +\t\tif (likely(info & 1)) {\n> +\t\t\tunsigned long p = (unsigned long)page;\n> +\n> +\t\t\treturn (const struct page *)(p & info);\n> +\t\t}\n>  \t}\n>  \treturn page;\n>  }\n> @@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)\n>  \n>  static __always_inline unsigned long _compound_head(const struct page *page)\n>  {\n> -\tunsigned long head = READ_ONCE(page->compound_info);\n> +\tunsigned long info = READ_ONCE(page->compound_info);\n>  \n> -\tif (unlikely(head & 1))\n> -\t\treturn head - 1;\n> -\treturn (unsigned long)page_fixed_fake_head(page);\n> +\t/* Bit 0 encodes PageTail() */\n> +\tif (!(info & 1))\n> +\t\treturn (unsigned long)page_fixed_fake_head(page);\n> +\n> +\t/*\n> +\t * If compound_info_has_mask() is false, the rest of compound_info is\n> +\t * the pointer to the head page.\n> +\t */\n> +\tif (!compound_info_has_mask())\n> +\t\treturn info - 1;\n> +\n> +\t/*\n> +\t * If compoun_info_has_mask() is true the rest of the info encodes\n\ns/compoun_info_has_mask/compound_info_has_mask/\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 02/02/2026 15:56, Kiryl Shutsemau wrote:\n> Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n> \n> This is preparation for adding the vmemmap_tails array to struct\n> pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: David Hildenbrand (Red Hat) <david@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> Acked-by: Muchun Song <muchun.song@linux.dev>\n\nAcked-by: Usama Arif <usamaarif642@gmail.com>\n\n---\n\nOn 02/02/2026 15:56, Kiryl Shutsemau wrote:\n> Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n> \n> This is preparation for adding the vmemmap_tails array to struct\n> pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: David Hildenbrand (Red Hat) <david@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> Acked-by: Muchun Song <muchun.song@linux.dev>\n\nAcked-by: Usama Arif <usamaarif642@gmail.com>\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-07",
              "message_id": "fd80736b-7b2a-4675-82a7-1902705c6361@gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Vlastimil Babka",
              "summary": "Vlastimil Babka noted that for tail pages, the kernel uses the 'compound_info' field to get to the head page, and requested that the bit 0 of the field be checked before accessing it to prevent potential issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential issue",
                "requested change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n> \n> This is preparation for adding the vmemmap_tails array to struct\n> pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: David Hildenbrand (Red Hat) <david@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> Acked-by: Muchun Song <muchun.song@linux.dev>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n> \n> This is preparation for adding the vmemmap_tails array to struct\n> pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: David Hildenbrand (Red Hat) <david@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> Acked-by: Muchun Song <muchun.song@linux.dev>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Instead of passing down the head page and tail page index, pass the tail\n> and head pages directly, as well as the order of the compound page.\n> \n> This is a preparation for changing how the head position is encoded in\n> the tail page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Instead of passing down the head page and tail page index, pass the tail\n> and head pages directly, as well as the order of the compound page.\n> \n> This is a preparation for changing how the head position is encoded in\n> the tail page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The 'compound_head' field in the 'struct page' encodes whether the page\n> is a tail and where to locate the head page. Bit 0 is set if the page is\n> a tail, and the remaining bits in the field point to the head page.\n> \n> As preparation for changing how the field encodes information about the\n> head page, rename the field to 'compound_info'.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The 'compound_head' field in the 'struct page' encodes whether the page\n> is a tail and where to locate the head page. Bit 0 is set if the page is\n> a tail, and the remaining bits in the field point to the head page.\n> \n> As preparation for changing how the field encodes information about the\n> head page, rename the field to 'compound_info'.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Move set_compound_head() and clear_compound_head() to be adjacent to the\n> compound_head() function in page-flags.h.\n> \n> These functions encode and decode the same compound_info field, so\n> keeping them together makes it easier to verify their logic is\n> consistent, especially when the encoding changes.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Move set_compound_head() and clear_compound_head() to be adjacent to the\n> compound_head() function in page-flags.h.\n> \n> These functions encode and decode the same compound_info field, so\n> keeping them together makes it easier to verify their logic is\n> consistent, especially when the encoding changes.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> For tail pages, the kernel uses the 'compound_info' field to get to the\n> head page. The bit 0 of the field indicates whether the page is a\n> tail page, and if set, the remaining bits represent a pointer to the\n> head page.\n> \n> For cases when size of struct page is power-of-2, change the encoding of\n> compound_info to store a mask that can be applied to the virtual address\n> of the tail page in order to access the head page. It is possible\n> because struct page of the head page is naturally aligned with regards\n> to order of the page.\n> \n> The significant impact of this modification is that all tail pages of\n> the same order will now have identical 'compound_info', regardless of\n> the compound page they are associated with. This paves the way for\n> eliminating fake heads.\n> \n> The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\n> applied when the sizeof(struct page) is power-of-2. Having identical\n> tail pages allows the same page to be mapped into the vmemmap of all\n> pages, maintaining memory savings without fake heads.\n> \n> If sizeof(struct page) is not power-of-2, there is no functional\n> changes.\n> \n> Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\n> a difference. The approach with mask would work in the wider set of\n> conditions, but it requires validating that struct pages are naturally\n> aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\nnit:\n\n> ---\n>  include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n>  mm/slab.h                  | 16 ++++++--\n>  mm/util.c                  | 16 ++++++--\n>  3 files changed, 97 insertions(+), 16 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index d14a17ffb55b..8f2c7fbc739b 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -198,6 +198,29 @@ enum pageflags {\n>  \n>  #ifndef __GENERATING_BOUNDS_H\n>  \n> +/*\n> + * For tail pages, if the size of struct page is power-of-2 ->compound_info\n> + * encodes the mask that converts the address of the tail page address to\n> + * the head page address.\n> + *\n> + * Otherwise, ->compound_info has direct pointer to head pages.\n> + */\n> +static __always_inline bool compound_info_has_mask(void)\n> +{\n> +\t/*\n> +\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n> +\t * makes a difference.\n> +\t *\n> +\t * The approach with mask would work in the wider set of conditions,\n> +\t * but it requires validating that struct pages are naturally aligned\n> +\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> +\t */\n> +\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n> +\t\treturn false;\n> +\n> +\treturn is_power_of_2(sizeof(struct page));\n> +}\n> +\n>  #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n>  DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n>  \n> @@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n>  \t\treturn page;\n>  \n> +\t/* Fake heads only exists if compound_info_has_mask() is true */\n> +\tif (!compound_info_has_mask())\n> +\t\treturn page;\n> +\n\nCould we move this compile-time-constant test above the static branch test?\n\n>  \t/*\n>  \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n>  \t * struct page. The alignment check aims to avoid access the fields (\n> @@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \t\t * because the @page is a compound page composed with at least\n>  \t\t * two contiguous pages.\n>  \t\t */\n> -\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n> +\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n>  \n> -\t\tif (likely(head & 1))\n> -\t\t\treturn (const struct page *)(head - 1);\n> +\t\t/* See set_compound_head() */\n> +\t\tif (likely(info & 1)) {\n> +\t\t\tunsigned long p = (unsigned long)page;\n> +\n> +\t\t\treturn (const struct page *)(p & info);\n> +\t\t}\n>  \t}\n>  \treturn page;\n>  }\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> For tail pages, the kernel uses the 'compound_info' field to get to the\n> head page. The bit 0 of the field indicates whether the page is a\n> tail page, and if set, the remaining bits represent a pointer to the\n> head page.\n> \n> For cases when size of struct page is power-of-2, change the encoding of\n> compound_info to store a mask that can be applied to the virtual address\n> of the tail page in order to access the head page. It is possible\n> because struct page of the head page is naturally aligned with regards\n> to order of the page.\n> \n> The significant impact of this modification is that all tail pages of\n> the same order will now have identical 'compound_info', regardless of\n> the compound page they are associated with. This paves the way for\n> eliminating fake heads.\n> \n> The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\n> applied when the sizeof(struct page) is power-of-2. Having identical\n> tail pages allows the same page to be mapped into the vmemmap of all\n> pages, maintaining memory savings without fake heads.\n> \n> If sizeof(struct page) is not power-of-2, there is no functional\n> changes.\n> \n> Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\n> a difference. The approach with mask would work in the wider set of\n> conditions, but it requires validating that struct pages are naturally\n> aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\nnit:\n\n> ---\n>  include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n>  mm/slab.h                  | 16 ++++++--\n>  mm/util.c                  | 16 ++++++--\n>  3 files changed, 97 insertions(+), 16 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index d14a17ffb55b..8f2c7fbc739b 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -198,6 +198,29 @@ enum pageflags {\n>  \n>  #ifndef __GENERATING_BOUNDS_H\n>  \n> +/*\n> + * For tail pages, if the size of struct page is power-of-2 ->compound_info\n> + * encodes the mask that converts the address of the tail page address to\n> + * the head page address.\n> + *\n> + * Otherwise, ->compound_info has direct pointer to head pages.\n> + */\n> +static __always_inline bool compound_info_has_mask(void)\n> +{\n> +\t/*\n> +\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n> +\t * makes a difference.\n> +\t *\n> +\t * The approach with mask would work in the wider set of conditions,\n> +\t * but it requires validating that struct pages are naturally aligned\n> +\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> +\t */\n> +\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n> +\t\treturn false;\n> +\n> +\treturn is_power_of_2(sizeof(struct page));\n> +}\n> +\n>  #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n>  DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n>  \n> @@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n>  \t\treturn page;\n>  \n> +\t/* Fake heads only exists if compound_info_has_mask() is true */\n> +\tif (!compound_info_has_mask())\n> +\t\treturn page;\n> +\n\nCould we move this compile-time-constant test above the static branch test?\n\n>  \t/*\n>  \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n>  \t * struct page. The alignment check aims to avoid access the fields (\n> @@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \t\t * because the @page is a compound page composed with at least\n>  \t\t * two contiguous pages.\n>  \t\t */\n> -\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n> +\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n>  \n> -\t\tif (likely(head & 1))\n> -\t\t\treturn (const struct page *)(head - 1);\n> +\t\t/* See set_compound_head() */\n> +\t\tif (likely(info & 1)) {\n> +\t\t\tunsigned long p = (unsigned long)page;\n> +\n> +\t\t\treturn (const struct page *)(p & info);\n> +\t\t}\n>  \t}\n>  \treturn page;\n>  }\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/9/26 12:52, Kiryl Shutsemau wrote:\n> On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>> > With the upcoming changes to HVO, a single page of tail struct pages\n>> > will be shared across all huge pages of the same order on a node. Since\n>> > huge pages on the same node may belong to different zones, the zone\n>> > information stored in shared tail page flags would be incorrect.\n>> > \n>> > Always fetch zone information from the head page, which has unique and\n>> > correct zone flags for each compound page.\n>> > \n>> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>> > Acked-by: Zi Yan <ziy@nvidia.com>\n>> > ---\n>> >   include/linux/mmzone.h | 1 +\n>> >   1 file changed, 1 insertion(+)\n>> > \n>> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n>> > index be8ce40b5638..192143b5cdc0 100644\n>> > --- a/include/linux/mmzone.h\n>> > +++ b/include/linux/mmzone.h\n>> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n>> >   static inline enum zone_type page_zonenum(const struct page *page)\n>> >   {\n>> > +\tpage = compound_head(page);\n>> >   \treturn memdesc_zonenum(page->flags);\n>> \n>> We end up calling page_zonenum() without holding a reference.\n>> \n>> Given that _compound_head() does a READ_ONCE(), this should work even if we\n>> see concurrent page freeing etc.\n>> \n>> However, this change implies that we now perform a compound page lookup for\n>> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n>> including for pageblock access and page freeing].\n>> \n>> That's a nasty compromise for making HVO better? :)\n>> \n>> We should likely limit that special casing to kernels that really rquire it\n>> (HVO).\n> \n> I will add compound_info_has_mask() check.\n\nNot thrilled by this indeed. Would it be a problem to have the shared tail\npages per node+zone instead of just per node?\n\n---\n\nOn 2/9/26 12:52, Kiryl Shutsemau wrote:\n> On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>> > With the upcoming changes to HVO, a single page of tail struct pages\n>> > will be shared across all huge pages of the same order on a node. Since\n>> > huge pages on the same node may belong to different zones, the zone\n>> > information stored in shared tail page flags would be incorrect.\n>> > \n>> > Always fetch zone information from the head page, which has unique and\n>> > correct zone flags for each compound page.\n>> > \n>> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>> > Acked-by: Zi Yan <ziy@nvidia.com>\n>> > ---\n>> >   include/linux/mmzone.h | 1 +\n>> >   1 file changed, 1 insertion(+)\n>> > \n>> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n>> > index be8ce40b5638..192143b5cdc0 100644\n>> > --- a/include/linux/mmzone.h\n>> > +++ b/include/linux/mmzone.h\n>> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n>> >   static inline enum zone_type page_zonenum(const struct page *page)\n>> >   {\n>> > +\tpage = compound_head(page);\n>> >   \treturn memdesc_zonenum(page->flags);\n>> \n>> We end up calling page_zonenum() without holding a reference.\n>> \n>> Given that _compound_head() does a READ_ONCE(), this should work even if we\n>> see concurrent page freeing etc.\n>> \n>> However, this change implies that we now perform a compound page lookup for\n>> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n>> including for pageblock access and page freeing].\n>> \n>> That's a nasty compromise for making HVO better? :)\n>> \n>> We should likely limit that special casing to kernels that really rquire it\n>> (HVO).\n> \n> I will add compound_info_has_mask() check.\n\nNot thrilled by this indeed. Would it be a problem to have the shared tail\npages per node+zone instead of just per node?\n\n\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/16/26 10:06, David Hildenbrand (Arm) wrote:\n> On 2/16/26 00:13, Matthew Wilcox wrote:\n>> On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n>>> With the upcoming changes to HVO, a single page of tail struct pages\n>>> will be shared across all huge pages of the same order on a node. Since\n>>> huge pages on the same node may belong to different zones, the zone\n>>> information stored in shared tail page flags would be incorrect.\n>>>\n>>> Always fetch zone information from the head page, which has unique and\n>>> correct zone flags for each compound page.\n>> \n>> You're right that different pages in the same folio can have different\n>> zone number.  But does it matter ... or to put it another way, why is\n>> returning the zone number of the head page the correct way to resolve\n>> this?\n> \n> How can a folio cross zones?\n> \n> Runtime allocated hugetlb folios from the CMA/buddy (alloc_contig_range) \n> definitely fall into a single zone.\n> \n> So is it about ones allocated early during boot, where, by chance, we \n> manage to cross ZONE_NORMAL + ZONE_MOVABLE etc?\n> \n> I thought that it's also not allowed there, and I wonder whether we \n> should disallow it if it's possible.\n\nI would be surprised if things didn't break horribly if we allowed crossing\nzones in a single folio. I'd rather not allow it.\n\n(And I still don't like how this patch solves the issue)\n\n>> \n>> Arguably, the caller is asking for the zone number of _this page_, and\n>> does not care about the zone number of the head page.  It would be good\n>> to have a short discussion of this in the commit message (but probably\n>> not worth putting this in a comment).\n> \n> Agreed, in particular, if there would be a functional change. So far I \n> assumed there would be no such change.\n> \n> Things like shrink_zone_span() really need to know the zone of that \n> page, not the one of the head; unless both fall into the same zone.\n>\n\n---\n\nOn 2/16/26 10:06, David Hildenbrand (Arm) wrote:\n> On 2/16/26 00:13, Matthew Wilcox wrote:\n>> On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n>>> With the upcoming changes to HVO, a single page of tail struct pages\n>>> will be shared across all huge pages of the same order on a node. Since\n>>> huge pages on the same node may belong to different zones, the zone\n>>> information stored in shared tail page flags would be incorrect.\n>>>\n>>> Always fetch zone information from the head page, which has unique and\n>>> correct zone flags for each compound page.\n>> \n>> You're right that different pages in the same folio can have different\n>> zone number.  But does it matter ... or to put it another way, why is\n>> returning the zone number of the head page the correct way to resolve\n>> this?\n> \n> How can a folio cross zones?\n> \n> Runtime allocated hugetlb folios from the CMA/buddy (alloc_contig_range) \n> definitely fall into a single zone.\n> \n> So is it about ones allocated early during boot, where, by chance, we \n> manage to cross ZONE_NORMAL + ZONE_MOVABLE etc?\n> \n> I thought that it's also not allowed there, and I wonder whether we \n> should disallow it if it's possible.\n\nI would be surprised if things didn't break horribly if we allowed crossing\nzones in a single folio. I'd rather not allow it.\n\n(And I still don't like how this patch solves the issue)\n\n>> \n>> Arguably, the caller is asking for the zone number of _this page_, and\n>> does not care about the zone number of the head page.  It would be good\n>> to have a short discussion of this in the commit message (but probably\n>> not worth putting this in a comment).\n> \n> Agreed, in particular, if there would be a functional change. So far I \n> assumed there would be no such change.\n> \n> Things like shrink_zone_span() really need to know the zone of that \n> page, not the one of the head; unless both fall into the same zone.\n> \n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-10",
              "message_id": "ae2be3d3-57a2-44ed-9a3d-c7de2ea79970@suse.cz",
              "analysis_source": "llm"
            },
            {
              "author": "Matthew Wilcox",
              "summary": "Matthew Wilcox questioned the correctness of always returning the zone number of the head page, suggesting that the caller may be asking for the zone number of 'this page', and proposed a discussion in the commit message to clarify this.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "questioning correctness",
                "requested clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n> With the upcoming changes to HVO, a single page of tail struct pages\n> will be shared across all huge pages of the same order on a node. Since\n> huge pages on the same node may belong to different zones, the zone\n> information stored in shared tail page flags would be incorrect.\n> \n> Always fetch zone information from the head page, which has unique and\n> correct zone flags for each compound page.\n\nYou're right that different pages in the same folio can have different\nzone number.  But does it matter ... or to put it another way, why is\nreturning the zone number of the head page the correct way to resolve\nthis?\n\nArguably, the caller is asking for the zone number of _this page_, and\ndoes not care about the zone number of the head page.  It would be good\nto have a short discussion of this in the commit message (but probably\nnot worth putting this in a comment).\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n> With the upcoming changes to HVO, a single page of tail struct pages\n> will be shared across all huge pages of the same order on a node. Since\n> huge pages on the same node may belong to different zones, the zone\n> information stored in shared tail page flags would be incorrect.\n> \n> Always fetch zone information from the head page, which has unique and\n> correct zone flags for each compound page.\n\nYou're right that different pages in the same folio can have different\nzone number.  But does it matter ... or to put it another way, why is\nreturning the zone number of the head page the correct way to resolve\nthis?\n\nArguably, the caller is asking for the zone number of _this page_, and\ndoes not care about the zone number of the head page.  It would be good\nto have a short discussion of this in the commit message (but probably\nnot worth putting this in a comment).\n\n---\n\nOn Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:\n> On 2/16/26 00:13, Matthew Wilcox wrote:\n> > On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n> > > With the upcoming changes to HVO, a single page of tail struct pages\n> > > will be shared across all huge pages of the same order on a node. Since\n> > > huge pages on the same node may belong to different zones, the zone\n> > > information stored in shared tail page flags would be incorrect.\n> > > \n> > > Always fetch zone information from the head page, which has unique and\n> > > correct zone flags for each compound page.\n> > \n> > You're right that different pages in the same folio can have different\n> > zone number.  But does it matter ... or to put it another way, why is\n> > returning the zone number of the head page the correct way to resolve\n> > this?\n> \n> How can a folio cross zones?\n\nI thought 1GB pages in hugetlb could cross zones?  Maybe that used to be\ntrue and isn't any more, or maybe it was never true and I was just\nconfused.\n\n---\n\nOn Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:\n> On 2/16/26 00:13, Matthew Wilcox wrote:\n> > On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n> > > With the upcoming changes to HVO, a single page of tail struct pages\n> > > will be shared across all huge pages of the same order on a node. Since\n> > > huge pages on the same node may belong to different zones, the zone\n> > > information stored in shared tail page flags would be incorrect.\n> > > \n> > > Always fetch zone information from the head page, which has unique and\n> > > correct zone flags for each compound page.\n> > \n> > You're right that different pages in the same folio can have different\n> > zone number.  But does it matter ... or to put it another way, why is\n> > returning the zone number of the head page the correct way to resolve\n> > this?\n> \n> How can a folio cross zones?\n\nI thought 1GB pages in hugetlb could cross zones?  Maybe that used to be\ntrue and isn't any more, or maybe it was never true and I was just\nconfused.\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-15",
              "message_id": "aZJTLwV2SaaKu1k_@casper.infradead.org",
              "analysis_source": "llm"
            },
            {
              "author": "Frank Linden",
              "summary": "Frank Linden noted that HugeTLB folios could cross zones due to bootmem (memblock) allocated pages, which would cause issues when freeing and reinitializing the vmemmap for HVO. He recalled a patch (14ed3a595fa4) that fixed this issue by checking for zone intersections in hugetlb allocation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "acknowledgment"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Mon, Feb 23, 2026 at 11:32AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:\n>\n> On 2/23/26 19:18, Matthew Wilcox wrote:\n> > On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:\n> >> On 2/16/26 00:13, Matthew Wilcox wrote:\n> >>>\n> >>> You're right that different pages in the same folio can have different\n> >>> zone number.  But does it matter ... or to put it another way, why is\n> >>> returning the zone number of the head page the correct way to resolve\n> >>> this?\n> >>\n> >> How can a folio cross zones?\n> >\n> > I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be\n> > true and isn't any more, or maybe it was never true and I was just\n> > confused.\n>\n> I recall that 1G folios could end up in ZONE_MOVABLE (comment in\n> page_is_unmovable()), but my memory is fuzzy when it comes to crossing\n> zones (ZONE_NORMAL -> ZONE_MOVABLE).\n>\n> Freeing+reinitializing the vmemmap for HVO with such folios would\n> already be problematic I suppose: we would silently switch the zone for\n> some of these pages.\n>\n> When freeing such (boottime) hugetlb folios to the buddy, we use\n> free_frozen_pages(). In there we lookup the zone once.\n>\n> Likely also problematic :)\n\nHugeTLB folios weren't supposed to cross zones, but they could do that\nin some cases for bootmem (memblock) allocated pages, causing the\nissue you describe.\n\nI fixed that with 14ed3a595fa4 (\"mm/hugetlb: check bootmem pages for\nzone intersections\"), so they won't cross zones anymore. The other\nallocation methods used for HugeTLB folios, alloc_contig_pages() and\ncma_alloc_folio, won't return anything that crosses a zone boundary by\ntheir nature.\n\nSo I think that's all good.\n\n- Frank\n\n---\n\nOn Mon, Feb 23, 2026 at 11:32AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:\n>\n> On 2/23/26 19:18, Matthew Wilcox wrote:\n> > On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:\n> >> On 2/16/26 00:13, Matthew Wilcox wrote:\n> >>>\n> >>> You're right that different pages in the same folio can have different\n> >>> zone number.  But does it matter ... or to put it another way, why is\n> >>> returning the zone number of the head page the correct way to resolve\n> >>> this?\n> >>\n> >> How can a folio cross zones?\n> >\n> > I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be\n> > true and isn't any more, or maybe it was never true and I was just\n> > confused.\n>\n> I recall that 1G folios could end up in ZONE_MOVABLE (comment in\n> page_is_unmovable()), but my memory is fuzzy when it comes to crossing\n> zones (ZONE_NORMAL -> ZONE_MOVABLE).\n>\n> Freeing+reinitializing the vmemmap for HVO with such folios would\n> already be problematic I suppose: we would silently switch the zone for\n> some of these pages.\n>\n> When freeing such (boottime) hugetlb folios to the buddy, we use\n> free_frozen_pages(). In there we lookup the zone once.\n>\n> Likely also problematic :)\n\nHugeTLB folios weren't supposed to cross zones, but they could do that\nin some cases for bootmem (memblock) allocated pages, causing the\nissue you describe.\n\nI fixed that with 14ed3a595fa4 (\"mm/hugetlb: check bootmem pages for\nzone intersections\"), so they won't cross zones anymore. The other\nallocation methods used for HugeTLB folios, alloc_contig_pages() and\ncma_alloc_folio, won't return anything that crosses a zone boundary by\ntheir nature.\n\nSo I think that's all good.\n\n- Frank\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "CAPTztWbr7y0myXB17Vz5HEZTw8a3PJ4qaxRKgtZmt-qXx1ofeA@mail.gmail.com",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCHv7 00/17] mm: Eliminate fake head pages from vmemmap optimization",
          "message_id": "aaHzKykadJwN7tF1@thinkstation",
          "url": "https://lore.kernel.org/all/aaHzKykadJwN7tF1@thinkstation/",
          "date": "2026-02-27T19:42:19Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": []
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Leo Martins",
      "primary_email": "loemra.dev@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Mark Harmstone",
      "primary_email": "mark@harmstone.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: read key again after incrementing slot in move_existing_remaps()",
          "message_id": "20260225103610.18494-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260225103610.18494-1-mark@harmstone.com/",
          "date": "2026-02-25T10:36:12Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-25",
          "patch_summary": "This patch fixes a bug in the btrfs relocation code by ensuring that when incrementing the slot in move_existing_remaps(), the objectid and offset of the old item are not reused if the encountered key is not a REMAP_BACKREF. This change prevents potential double-put bugs.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Johannes Thumshirn",
              "summary": "Identified the bug in move_existing_remaps() where the objectid and offset of the old item are reused when incrementing the slot. Approved the patch as it fixes this issue.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Looks good,\r\n\r\nReviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>\r\n\r\n",
              "reply_to": "",
              "message_date": "2026-02-25",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Nhat Pham",
      "primary_email": "nphamcs@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Rik van Riel",
      "primary_email": "riel@surriel.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    }
  ]
}