{
  "date": "2026-02-21",
  "report_file": "2026-02-21_ollama_llama3.1-8b.html",
  "llm_backends": [
    [
      "ollama",
      "llama3.1:8b"
    ]
  ],
  "generation_time_seconds": 12725.994971513748,
  "developer_reports": [
    {
      "name": "Alexandre Ghiti",
      "primary_email": "alexghiti@rivosinc.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Boris Burkov",
      "primary_email": "boris@bur.io",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Dmitry Ilvokhin",
      "primary_email": "d@ilvokhin.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Gregory Price",
      "primary_email": "gourry@gourry.net",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/2] cxl/region: fix region leak when attach_target fails in cxl_add_to_region",
          "message_id": "20260221043013.1420169-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260221043013.1420169-1-gourry@gourry.net/",
          "date": "2026-02-21T04:30:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series addresses a region leak issue in the CXL driver when attach_target fails in cxl_add_to_region. The fix involves tracking whether a new region is created and calling drop_region on attach_target failure to unregister it and release HPA resources. Additionally, the second patch skips default driver attachment for memdevs with custom attach callbacks.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "The original patch was deemed unnecessary as another patch already addressed the region leak issue. The author suggested disregarding this patch series.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a CXL memdev has a custom attach callback, cxl_add_to_region()\nshould not call device_attach() on the auto-discovered region.\n\nThe default device_attach() binds the dax driver, which may online\nmemory via dax_kmem.  The custom attach callback then has to tear down\nthe dax stack to convert the region to sysram, but dax_kmem refuses to\noffline memory during its remove path, leaving regions stuck online.\n\nSkip device_attach() when cxlmd->attach is set.  The attach callback\nis responsible for setting up the region after auto-discovery completes\n(e.g. adding it as sysram directly).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/region.c | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 276046d49f88..e5edeabd9262 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -3971,6 +3971,12 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n \t}\n \n \tif (attach) {\n+\t\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n+\n+\t\t/* Skip device_attach if memdev has is own attach callback */\n+\t\tif (cxlmd->attach)\n+\t\t\treturn 0;\n+\n \t\t/*\n \t\t * If device_attach() fails the range may still be active via\n \t\t * the platform-firmware memory map, otherwise the driver for\n-- \n2.47.3\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] cxl/core: fix test_bit misuse with CXL_DECODER_F_ bitmask flags",
          "message_id": "20260221021810.1390342-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260221021810.1390342-1-gourry@gourry.net/",
          "date": "2026-02-21T02:18:16Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch fixes a misuse of the test_bit() function in the CXL core driver, where it is passed bitmask flags instead of bit numbers. The fix involves replacing test_bit() with direct bitmask tests to maintain consistency with other uses of these flags.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Benjamin Cheatham",
              "summary": "Noted that this bug was independently discovered and pointed out an existing patch that addresses the same issue. Requested a Reported-by tag for the original author of the existing patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgment",
                "request for credit"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "Gregory Price",
              "message_date": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Acknowledged the existing patch and confirmed that this issue was independently discovered. Expressed gratitude for pointing out the existing patch.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "gratitude"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Mon, Feb 23, 2026 at 11:33:13AM -0600, Cheatham, Benjamin wrote:\n> On 2/20/2026 8:18 PM, Gregory Price wrote:\n> > CXL_DECODER_F_LOCK (BIT(4) = 16) and CXL_DECODER_F_NORMALIZED_ADDRESSING\n> > (BIT(6) = 64) are bit masks, but three call sites pass them to test_bit()\n> > which expects a bit number.\n> > \n> > Replace test_bit() with direct bitmask tests, consistent with every other\n> > use of these flags.\n> > \n> > Fixes: 2230c4bdc412 (\"cxl: Add handling of locked CXL decoder\")\n> > Signed-off-by: Gregory Price <gourry@gourry.net>\n> \n> Alison sent out a patch [1] two weeks ago for this. I suspect you found this bug\n> independently, so I figured I should point it out. Otherwise, I would add a Reported-by (or some\n> other tag) with her name.\n> \n> Thanks,\n> Ben\n> \n> [1]: https://lore.kernel.org/linux-cxl/20260206181404.1025991-1-alison.schofield@intel.com/\n\nAh, yeah, missed this, and did find independently when testing unbinds.\n\nWasn't on cxl/next so I thought it hadn't been found yet.\n\nCool, thanks!\n~Gregory\n",
              "reply_to": "",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Noted that the patch is waiting on 7.0-rc1 for cxl/fixes and requested a review tag from Gregory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request for review"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\n\nOn 2/23/26 10:45 AM, Gregory Price wrote:\n> On Mon, Feb 23, 2026 at 11:33:13AM -0600, Cheatham, Benjamin wrote:\n>> On 2/20/2026 8:18 PM, Gregory Price wrote:\n>>> CXL_DECODER_F_LOCK (BIT(4) = 16) and CXL_DECODER_F_NORMALIZED_ADDRESSING\n>>> (BIT(6) = 64) are bit masks, but three call sites pass them to test_bit()\n>>> which expects a bit number.\n>>>\n>>> Replace test_bit() with direct bitmask tests, consistent with every other\n>>> use of these flags.\n>>>\n>>> Fixes: 2230c4bdc412 (\"cxl: Add handling of locked CXL decoder\")\n>>> Signed-off-by: Gregory Price <gourry@gourry.net>\n>>\n>> Alison sent out a patch [1] two weeks ago for this. I suspect you found this bug\n>> independently, so I figured I should point it out. Otherwise, I would add a Reported-by (or some\n>> other tag) with her name.\n>>\n>> Thanks,\n>> Ben\n>>\n>> [1]: https://lore.kernel.org/linux-cxl/20260206181404.1025991-1-alison.schofield@intel.com/\n> \n> Ah, yeah, missed this, and did find independently when testing unbinds.\n> \n> Wasn't on cxl/next so I thought it hadn't been found yet.\n\nYeah waiting on 7.0-rc1 for cxl/fixes. I also asked her to split the patches into 2 fixes. But if you don't mind go add your review tag that'd be great! :) \n\n> \n> Cool, thanks!\n> ~Gregory\n\n\n",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/2] cxl/region: fix region leak when attach_target fails in cxl_add_to_region",
          "message_id": "aZk_9iYMh2QPYNDz@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZk_9iYMh2QPYNDz@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-21T05:17:46Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Fixes a region leak in cxl_add_to_region() when attach_target() fails by tracking whether the region was created and dropping it if necessary.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "Asked to disregard the patch because it uses drop_region(), which is introduced by another patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a CXL memdev has a custom attach callback, cxl_add_to_region()\nshould not call device_attach() on the auto-discovered region.\n\nThe default device_attach() binds the dax driver, which may online\nmemory via dax_kmem.  The custom attach callback then has to tear down\nthe dax stack to convert the region to sysram, but dax_kmem refuses to\noffline memory during its remove path, leaving regions stuck online.\n\nSkip device_attach() when cxlmd->attach is set.  The attach callback\nis responsible for setting up the region after auto-discovery completes\n(e.g. adding it as sysram directly).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/region.c | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 276046d49f88..e5edeabd9262 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -3971,6 +3971,12 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n \t}\n \n \tif (attach) {\n+\t\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n+\n+\t\t/* Skip device_attach if memdev has is own attach callback */\n+\t\tif (cxlmd->attach)\n+\t\t\treturn 0;\n+\n \t\t/*\n \t\t * If device_attach() fails the range may still be active via\n \t\t * the platform-firmware memory map, otherwise the driver for\n-- \n2.47.3\n\n",
              "reply_to": "",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v23 10/22] cxl: Export function for unwinding cxl by accelerators",
          "message_id": "aZk5CVRELS2qo92c@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZk5CVRELS2qo92c@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-21T04:48:13Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about differentiating between CXL memory expanders and device accelerators, which was likely raised due to the patch's initialization of cxl_dev_state for both types. The author has added a new function, cxl_dev_state_init(), to handle this differentiation and has also updated the devm_cxl_dev_state_create() function to use this new initialization method.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nDifferentiate CXL memory expanders (type 3) from CXL device accelerators\n(type 2) with a new function for initializing cxl_dev_state and a macro\nfor helping accel drivers to embed cxl_dev_state inside a private\nstruct.\n\nMove structs to include/cxl as the size of the accel driver private\nstruct embedding cxl_dev_state needs to know the size of this struct.\n\nUse same new initialization with the type3 pci driver.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n---\n drivers/cxl/core/mbox.c      |  12 +-\n drivers/cxl/core/memdev.c    |  32 +++++\n drivers/cxl/cxl.h            |  97 +--------------\n drivers/cxl/cxlmem.h         |  86 +------------\n drivers/cxl/pci.c            |  14 +--\n include/cxl/cxl.h            | 226 +++++++++++++++++++++++++++++++++++\n tools/testing/cxl/test/mem.c |   3 +-\n 7 files changed, 274 insertions(+), 196 deletions(-)\n create mode 100644 include/cxl/cxl.h\n\ndiff --git a/drivers/cxl/core/mbox.c b/drivers/cxl/core/mbox.c\nindex fa6dd0c94656..bee84d0101d1 100644\n--- a/drivers/cxl/core/mbox.c\n+++ b/drivers/cxl/core/mbox.c\n@@ -1514,23 +1514,21 @@ int cxl_mailbox_init(struct cxl_mailbox *cxl_mbox, struct device *host)\n }\n EXPORT_SYMBOL_NS_GPL(cxl_mailbox_init, \"CXL\");\n \n-struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev)\n+struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n+\t\t\t\t\t\t u16 dvsec)\n {\n \tstruct cxl_memdev_state *mds;\n \tint rc;\n \n-\tmds = devm_kzalloc(dev, sizeof(*mds), GFP_KERNEL);\n+\tmds = devm_cxl_dev_state_create(dev, CXL_DEVTYPE_CLASSMEM, serial,\n+\t\t\t\t\tdvsec, struct cxl_memdev_state, cxlds,\n+\t\t\t\t\ttrue);\n \tif (!mds) {\n \t\tdev_err(dev, \"No memory available\\n\");\n \t\treturn ERR_PTR(-ENOMEM);\n \t}\n \n \tmutex_init(&mds->event.log_lock);\n-\tmds->cxlds.dev = dev;\n-\tmds->cxlds.reg_map.host = dev;\n-\tmds->cxlds.cxl_mbox.host = dev;\n-\tmds->cxlds.reg_map.resource = CXL_RESOURCE_NONE;\n-\tmds->cxlds.type = CXL_DEVTYPE_CLASSMEM;\n \n \trc = devm_cxl_register_mce_notifier(dev, &mds->mce_notifier);\n \tif (rc == -EOPNOTSUPP)\ndiff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\nindex af3d0cc65138..22d156f25305 100644\n--- a/drivers/cxl/core/memdev.c\n+++ b/drivers/cxl/core/memdev.c\n@@ -656,6 +656,38 @@ static void detach_memdev(struct work_struct *work)\n \n static struct lock_class_key cxl_memdev_key;\n \n+static void cxl_dev_state_init(struct cxl_dev_state *cxlds, struct device *dev,\n+\t\t\t       enum cxl_devtype type, u64 serial, u16 dvsec,\n+\t\t\t       bool has_mbox)\n+{\n+\t*cxlds = (struct cxl_dev_state) {\n+\t\t.dev = dev,\n+\t\t.type = type,\n+\t\t.serial = serial,\n+\t\t.cxl_dvsec = dvsec,\n+\t\t.reg_map.host = dev,\n+\t\t.reg_map.resource = CXL_RESOURCE_NONE,\n+\t};\n+\n+\tif (has_mbox)\n+\t\tcxlds->cxl_mbox.host = dev;\n+}\n+\n+struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n+\t\t\t\t\t\t enum cxl_devtype type,\n+\t\t\t\t\t\t u64 serial, u16 dvsec,\n+\t\t\t\t\t\t size_t size, bool has_mbox)\n+{\n+\tstruct cxl_dev_state *cxlds = devm_kzalloc(dev, size, GFP_KERNEL);\n+\n+\tif (!cxlds)\n+\t\treturn NULL;\n+\n+\tcxl_dev_state_init(cxlds, dev, type, serial, dvsec, has_mbox);\n+\treturn cxlds;\n+}\n+EXPORT_SYMBOL_NS_GPL(_devm_cxl_dev_state_create, \"CXL\");\n+\n static struct cxl_memdev *cxl_memdev_alloc(struct cxl_dev_state *cxlds,\n \t\t\t\t\t   const struct file_operations *fops,\n \t\t\t\t\t   const struct cxl_memdev_attach *attach)\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex e1d47062e1d3..3eaa353e430b 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -12,6 +12,7 @@\n #include <linux/node.h>\n #include <linux/io.h>\n #include <linux/range.h>\n+#include <cxl/cxl.h>\n \n extern const struct nvdimm_security_ops *cxl_security_ops;\n \n@@ -201,97 +202,6 @@ static inline int ways_to_eiw(unsigned int ways, u8 *eiw)\n #define   CXLDEV_MBOX_BG_CMD_COMMAND_VENDOR_MASK GENMASK_ULL(63, 48)\n #define CXLDEV_MBOX_PAYLOAD_OFFSET 0x20\n \n-/*\n- * Using struct_group() allows for per register-block-type helper routines,\n- * without requiring block-type agnostic code to include the prefix.\n- */\n-struct cxl_regs {\n-\t/*\n-\t * Common set of CXL Component register block base pointers\n-\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n-\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n-\t */\n-\tstruct_group_tagged(cxl_component_regs, component,\n-\t\tvoid __iomem *hdm_decoder;\n-\t\tvoid __iomem *ras;\n-\t);\n-\t/*\n-\t * Common set of CXL Device register block base pointers\n-\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n-\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n-\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n-\t */\n-\tstruct_group_tagged(cxl_device_regs, device_regs,\n-\t\tvoid __iomem *status, *mbox, *memdev;\n-\t);\n-\n-\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n-\t\tvoid __iomem *pmu;\n-\t);\n-\n-\t/*\n-\t * RCH downstream port specific RAS register\n-\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n-\t */\n-\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n-\t\tvoid __iomem *dport_aer;\n-\t);\n-\n-\t/*\n-\t * RCD upstream port specific PCIe cap register\n-\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n-\t */\n-\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n-\t\tvoid __iomem *rcd_pcie_cap;\n-\t);\n-};\n-\n-struct cxl_reg_map {\n-\tbool valid;\n-\tint id;\n-\tunsigned long offset;\n-\tunsigned long size;\n-};\n-\n-struct cxl_component_reg_map {\n-\tstruct cxl_reg_map hdm_decoder;\n-\tstruct cxl_reg_map ras;\n-};\n-\n-struct cxl_device_reg_map {\n-\tstruct cxl_reg_map status;\n-\tstruct cxl_reg_map mbox;\n-\tstruct cxl_reg_map memdev;\n-};\n-\n-struct cxl_pmu_reg_map {\n-\tstruct cxl_reg_map pmu;\n-};\n-\n-/**\n- * struct cxl_register_map - DVSEC harvested register block mapping parameters\n- * @host: device for devm operations and logging\n- * @base: virtual base of the register-block-BAR + @block_offset\n- * @resource: physical resource base of the register block\n- * @max_size: maximum mapping size to perform register search\n- * @reg_type: see enum cxl_regloc_type\n- * @component_map: cxl_reg_map for component registers\n- * @device_map: cxl_reg_maps for device registers\n- * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n- */\n-struct cxl_register_map {\n-\tstruct device *host;\n-\tvoid __iomem *base;\n-\tresource_size_t resource;\n-\tresource_size_t max_size;\n-\tu8 reg_type;\n-\tunion {\n-\t\tstruct cxl_component_reg_map component_map;\n-\t\tstruct cxl_device_reg_map device_map;\n-\t\tstruct cxl_pmu_reg_map pmu_map;\n-\t};\n-};\n-\n void cxl_probe_component_regs(struct device *dev, void __iomem *base,\n \t\t\t      struct cxl_component_reg_map *map);\n void cxl_probe_device_regs(struct device *dev, void __iomem *base,\n@@ -497,11 +407,6 @@ struct cxl_region_params {\n \tresource_size_t cache_size;\n };\n \n-enum cxl_partition_mode {\n-\tCXL_PARTMODE_RAM,\n-\tCXL_PARTMODE_PMEM,\n-};\n-\n /*\n  * Indicate whether this region has been assembled by autodetection or\n  * userspace assembly. Prevent endpoint decoders outside of automatic\ndiff --git a/drivers/cxl/cxlmem.h b/drivers/cxl/cxlmem.h\nindex ef202b34e5ea..281546de426e 100644\n--- a/drivers/cxl/cxlmem.h\n+++ b/drivers/cxl/cxlmem.h\n@@ -113,8 +113,6 @@ int devm_cxl_dpa_reserve(struct cxl_endpoint_decoder *cxled,\n \t\t\t resource_size_t base, resource_size_t len,\n \t\t\t resource_size_t skipped);\n \n-#define CXL_NR_PARTITIONS_MAX 2\n-\n struct cxl_dpa_info {\n \tu64 size;\n \tstruct cxl_dpa_part_info {\n@@ -373,87 +371,6 @@ struct cxl_security_state {\n \tstruct kernfs_node *sanitize_node;\n };\n \n-/*\n- * enum cxl_devtype - delineate type-2 from a generic type-3 device\n- * @CXL_DEVTYPE_DEVMEM - Vendor specific CXL Type-2 device implementing HDM-D or\n- *\t\t\t HDM-DB, no requirement that this device implements a\n- *\t\t\t mailbox, or other memory-device-standard manageability\n- *\t\t\t flows.\n- * @CXL_DEVTYPE_CLASSMEM - Common class definition of a CXL Type-3 device with\n- *\t\t\t   HDM-H and class-mandatory memory device registers\n- */\n-enum cxl_devtype {\n-\tCXL_DEVTYPE_DEVMEM,\n-\tCXL_DEVTYPE_CLASSMEM,\n-};\n-\n-/**\n- * struct cxl_dpa_perf - DPA performance property entry\n- * @dpa_range: range for DPA address\n- * @coord: QoS performance data (i.e. latency, bandwidth)\n- * @cdat_coord: raw QoS performance data from CDAT\n- * @qos_class: QoS Class cookies\n- */\n-struct cxl_dpa_perf {\n-\tstruct range dpa_range;\n-\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n-\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n-\tint qos_class;\n-};\n-\n-/**\n- * struct cxl_dpa_partition - DPA partition descriptor\n- * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n- * @perf: performance attributes of the partition from CDAT\n- * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n- */\n-struct cxl_dpa_partition {\n-\tstruct resource res;\n-\tstruct cxl_dpa_perf perf;\n-\tenum cxl_partition_mode mode;\n-};\n-\n-/**\n- * struct cxl_dev_state - The driver device state\n- *\n- * cxl_dev_state represents the CXL driver/device state.  It provides an\n- * interface to mailbox commands as well as some cached data about the device.\n- * Currently only memory devices are represented.\n- *\n- * @dev: The device associated with this CXL state\n- * @cxlmd: The device representing the CXL.mem capabilities of @dev\n- * @reg_map: component and ras register mapping parameters\n- * @regs: Parsed register blocks\n- * @cxl_dvsec: Offset to the PCIe device DVSEC\n- * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n- * @media_ready: Indicate whether the device media is usable\n- * @dpa_res: Overall DPA resource tree for the device\n- * @part: DPA partition array\n- * @nr_partitions: Number of DPA partitions\n- * @serial: PCIe Device Serial Number\n- * @type: Generic Memory Class device or Vendor Specific Memory device\n- * @cxl_mbox: CXL mailbox context\n- * @cxlfs: CXL features context\n- */\n-struct cxl_dev_state {\n-\tstruct device *dev;\n-\tstruct cxl_memdev *cxlmd;\n-\tstruct cxl_register_map reg_map;\n-\tstruct cxl_regs regs;\n-\tint cxl_dvsec;\n-\tbool rcd;\n-\tbool media_ready;\n-\tstruct resource dpa_res;\n-\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n-\tunsigned int nr_partitions;\n-\tu64 serial;\n-\tenum cxl_devtype type;\n-\tstruct cxl_mailbox cxl_mbox;\n-#ifdef CONFIG_CXL_FEATURES\n-\tstruct cxl_features_state *cxlfs;\n-#endif\n-};\n-\n static inline resource_size_t cxl_pmem_size(struct cxl_dev_state *cxlds)\n {\n \t/*\n@@ -858,7 +775,8 @@ int cxl_dev_state_identify(struct cxl_memdev_state *mds);\n int cxl_await_media_ready(struct cxl_dev_state *cxlds);\n int cxl_enumerate_cmds(struct cxl_memdev_state *mds);\n int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info);\n-struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev);\n+struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n+\t\t\t\t\t\t u16 dvsec);\n void set_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n \t\t\t\tunsigned long *cmds);\n void clear_exclusive_cxl_commands(struct cxl_memdev_state *mds,\ndiff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\nindex 1cf232220873..24179cc702bf 100644\n--- a/drivers/cxl/pci.c\n+++ b/drivers/cxl/pci.c\n@@ -911,25 +911,25 @@ static int cxl_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n \tint rc, pmu_count;\n \tunsigned int i;\n \tbool irq_avail;\n+\tu16 dvsec;\n \n \trc = pcim_enable_device(pdev);\n \tif (rc)\n \t\treturn rc;\n \tpci_set_master(pdev);\n \n-\tmds = cxl_memdev_state_create(&pdev->dev);\n+\tdvsec = pci_find_dvsec_capability(pdev, PCI_VENDOR_ID_CXL,\n+\t\t\t\t\t  PCI_DVSEC_CXL_DEVICE);\n+\tif (!dvsec)\n+\t\tpci_warn(pdev, \"Device DVSEC not present, skip CXL.mem init\\n\");\n+\n+\tmds = cxl_memdev_state_create(&pdev->dev, pci_get_dsn(pdev), dvsec);\n \tif (IS_ERR(mds))\n \t\treturn PTR_ERR(mds);\n \tcxlds = &mds->cxlds;\n \tpci_set_drvdata(pdev, cxlds);\n \n \tcxlds->rcd = is_cxl_restricted(pdev);\n-\tcxlds->serial = pci_get_dsn(pdev);\n-\tcxlds->cxl_dvsec = pci_find_dvsec_capability(\n-\t\tpdev, PCI_VENDOR_ID_CXL, PCI_DVSEC_CXL_DEVICE);\n-\tif (!cxlds->cxl_dvsec)\n-\t\tdev_warn(&pdev->dev,\n-\t\t\t \"Device DVSEC not present, skip CXL.mem init\\n\");\n \n \trc = cxl_pci_setup_regs(pdev, CXL_REGLOC_RBI_MEMDEV, &map);\n \tif (rc)\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nnew file mode 100644\nindex 000000000000..13d448686189\n--- /dev/null\n+++ b/include/cxl/cxl.h\n@@ -0,0 +1,226 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+/* Copyright(c) 2020 Intel Corporation. */\n+/* Copyright(c) 2025 Advanced Micro Devices, Inc. */\n+\n+#ifndef __CXL_CXL_H__\n+#define __CXL_CXL_H__\n+\n+#include <linux/node.h>\n+#include <linux/ioport.h>\n+#include <cxl/mailbox.h>\n+\n+/**\n+ * enum cxl_devtype - delineate type-2 from a generic type-3 device\n+ * @CXL_DEVTYPE_DEVMEM: Vendor specific CXL Type-2 device implementing HDM-D or\n+ *\t\t\t HDM-DB, no requirement that this device implements a\n+ *\t\t\t mailbox, or other memory-device-standard manageability\n+ *\t\t\t flows.\n+ * @CXL_DEVTYPE_CLASSMEM: Common class definition of a CXL Type-3 device with\n+ *\t\t\t   HDM-H and class-mandatory memory device registers\n+ */\n+enum cxl_devtype {\n+\tCXL_DEVTYPE_DEVMEM,\n+\tCXL_DEVTYPE_CLASSMEM,\n+};\n+\n+struct device;\n+\n+/*\n+ * Using struct_group() allows for per register-block-type helper routines,\n+ * without requiring block-type agnostic code to include the prefix.\n+ */\n+struct cxl_regs {\n+\t/*\n+\t * Common set of CXL Component register block base pointers\n+\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n+\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n+\t */\n+\tstruct_group_tagged(cxl_component_regs, component,\n+\t\tvoid __iomem *hdm_decoder;\n+\t\tvoid __iomem *ras;\n+\t);\n+\t/*\n+\t * Common set of CXL Device register block base pointers\n+\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n+\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n+\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n+\t */\n+\tstruct_group_tagged(cxl_device_regs, device_regs,\n+\t\tvoid __iomem *status, *mbox, *memdev;\n+\t);\n+\n+\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n+\t\tvoid __iomem *pmu;\n+\t);\n+\n+\t/*\n+\t * RCH downstream port specific RAS register\n+\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n+\t */\n+\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n+\t\tvoid __iomem *dport_aer;\n+\t);\n+\n+\t/*\n+\t * RCD upstream port specific PCIe cap register\n+\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n+\t */\n+\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n+\t\tvoid __iomem *rcd_pcie_cap;\n+\t);\n+};\n+\n+struct cxl_reg_map {\n+\tbool valid;\n+\tint id;\n+\tunsigned long offset;\n+\tunsigned long size;\n+};\n+\n+struct cxl_component_reg_map {\n+\tstruct cxl_reg_map hdm_decoder;\n+\tstruct cxl_reg_map ras;\n+};\n+\n+struct cxl_device_reg_map {\n+\tstruct cxl_reg_map status;\n+\tstruct cxl_reg_map mbox;\n+\tstruct cxl_reg_map memdev;\n+};\n+\n+struct cxl_pmu_reg_map {\n+\tstruct cxl_reg_map pmu;\n+};\n+\n+/**\n+ * struct cxl_register_map - DVSEC harvested register block mapping parameters\n+ * @host: device for devm operations and logging\n+ * @base: virtual base of the register-block-BAR + @block_offset\n+ * @resource: physical resource base of the register block\n+ * @max_size: maximum mapping size to perform register search\n+ * @reg_type: see enum cxl_regloc_type\n+ * @component_map: cxl_reg_map for component registers\n+ * @device_map: cxl_reg_maps for device registers\n+ * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n+ */\n+struct cxl_register_map {\n+\tstruct device *host;\n+\tvoid __iomem *base;\n+\tresource_size_t resource;\n+\tresource_size_t max_size;\n+\tu8 reg_type;\n+\tunion {\n+\t\tstruct cxl_component_reg_map component_map;\n+\t\tstruct cxl_device_reg_map device_map;\n+\t\tstruct cxl_pmu_reg_map pmu_map;\n+\t};\n+};\n+\n+/**\n+ * struct cxl_dpa_perf - DPA performance property entry\n+ * @dpa_range: range for DPA address\n+ * @coord: QoS performance data (i.e. latency, bandwidth)\n+ * @cdat_coord: raw QoS performance data from CDAT\n+ * @qos_class: QoS Class cookies\n+ */\n+struct cxl_dpa_perf {\n+\tstruct range dpa_range;\n+\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n+\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n+\tint qos_class;\n+};\n+\n+enum cxl_partition_mode {\n+\tCXL_PARTMODE_RAM,\n+\tCXL_PARTMODE_PMEM,\n+};\n+\n+/**\n+ * struct cxl_dpa_partition - DPA partition descriptor\n+ * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n+ * @perf: performance attributes of the partition from CDAT\n+ * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n+ */\n+struct cxl_dpa_partition {\n+\tstruct resource res;\n+\tstruct cxl_dpa_perf perf;\n+\tenum cxl_partition_mode mode;\n+};\n+\n+#define CXL_NR_PARTITIONS_MAX 2\n+\n+/**\n+ * struct cxl_dev_state - The driver device state\n+ *\n+ * cxl_dev_state represents the CXL driver/device state.  It provides an\n+ * interface to mailbox commands as well as some cached data about the device.\n+ * Currently only memory devices are represented.\n+ *\n+ * @dev: The device associated with this CXL state\n+ * @cxlmd: The device representing the CXL.mem capabilities of @dev\n+ * @reg_map: component and ras register mapping parameters\n+ * @regs: Parsed register blocks\n+ * @cxl_dvsec: Offset to the PCIe device DVSEC\n+ * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n+ * @media_ready: Indicate whether the device media is usable\n+ * @dpa_res: Overall DPA resource tree for the device\n+ * @part: DPA partition array\n+ * @nr_partitions: Number of DPA partitions\n+ * @serial: PCIe Device Serial Number\n+ * @type: Generic Memory Class device or Vendor Specific Memory device\n+ * @cxl_mbox: CXL mailbox context\n+ * @cxlfs: CXL features context\n+ */\n+struct cxl_dev_state {\n+\t/* public for Type2 drivers */\n+\tstruct device *dev;\n+\tstruct cxl_memdev *cxlmd;\n+\n+\t/* private for Type2 drivers */\n+\tstruct cxl_register_map reg_map;\n+\tstruct cxl_regs regs;\n+\tint cxl_dvsec;\n+\tbool rcd;\n+\tbool media_ready;\n+\tstruct resource dpa_res;\n+\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n+\tunsigned int nr_partitions;\n+\tu64 serial;\n+\tenum cxl_devtype type;\n+\tstruct cxl_mailbox cxl_mbox;\n+#ifdef CONFIG_CXL_FEATURES\n+\tstruct cxl_features_state *cxlfs;\n+#endif\n+};\n+\n+struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n+\t\t\t\t\t\t enum cxl_devtype type,\n+\t\t\t\t\t\t u64 serial, u16 dvsec,\n+\t\t\t\t\t\t size_t size, bool has_mbox);\n+\n+/**\n+ * cxl_dev_state_create - safely create and cast a cxl dev state embedded in a\n+ * driver specific struct.\n+ *\n+ * @parent: device behind the request\n+ * @type: CXL device type\n+ * @serial: device identification\n+ * @dvsec: dvsec capability offset\n+ * @drv_struct: driver struct embedding a cxl_dev_state struct\n+ * @member: drv_struct member as cxl_dev_state\n+ * @mbox: true if mailbox supported\n+ *\n+ * Returns a pointer to the drv_struct allocated and embedding a cxl_dev_state\n+ * struct initialized.\n+ *\n+ * Introduced for Type2 driver support.\n+ */\n+#define devm_cxl_dev_state_create(parent, type, serial, dvsec, drv_struct, member, mbox)\t\\\n+\t({\t\t\t\t\t\t\t\t\t\t\\\n+\t\tstatic_assert(__same_type(struct cxl_dev_state,\t\t\t\t\\\n+\t\t\t      ((drv_struct *)NULL)->member));\t\t\t\t\\\n+\t\tstatic_assert(offsetof(drv_struct, member) == 0);\t\t\t\\\n+\t\t(drv_struct *)_devm_cxl_dev_state_create(parent, type, serial, dvsec,\t\\\n+\t\t\t\t\t\t      sizeof(drv_struct), mbox);\t\\\n+\t})\n+#endif /* __CXL_CXL_H__ */\ndiff --git a/tools/testing/cxl/test/mem.c b/tools/testing/cxl/test/mem.c\nindex cb87e8c0e63c..79f42f4474d4 100644\n--- a/tools/testing/cxl/test/mem.c\n+++ b/tools/testing/cxl/test/mem.c\n@@ -1716,7 +1716,7 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n \tif (rc)\n \t\treturn rc;\n \n-\tmds = cxl_memdev_state_create(dev);\n+\tmds = cxl_memdev_state_create(dev, pdev->id + 1, 0);\n \tif (IS_ERR(mds))\n \t\treturn PTR_ERR(mds);\n \n@@ -1732,7 +1732,6 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n \tmds->event.buf = (struct cxl_get_event_payload *) mdata->event_buf;\n \tINIT_DELAYED_WORK(&mds->security.poll_dwork, cxl_mockmem_sanitize_work);\n \n-\tcxlds->serial = pdev->id + 1;\n \tif (is_rcd(pdev))\n \t\tcxlds->rcd = true;\n \n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about the dependency on Smita's patchset, specifically [PATCH v5 3/7] cxl/region: Skip decoder reset on detach for autodiscovered regions. The author explains that this patch is needed to support the default behavior with current BIOS configuration where HDM Type2 decoders will be kept unreset when the driver unloads. They confirm that this dependency has been added in v23 and will be supported in follow-up works.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nThis patchset should be applied on the cxl next branch using the base\nspecified at the end of this cover letter.\n\nDependencies on Dan's work has gone and also on Terry's as the only\npatch required is now in next. The other dependency is on Smita patchset\nbut it does not exist such a dependency as that work will not avoid the\nproblem with Type2 and DAX/hmem if soft reserved memory. This needs to\nbe solved by the BIOS and Type2 UEFI driver for populating the CXL.mem\nrange as EFI_RESERVED_TYPE instead of default EFI_CONVENTIONAL_MEMORY\nwith the EFI_MEMORY_SP attribute. There exists though a dependency on\none Smita's patches:\n\n[PATCH v5 3/7] cxl/region: Skip decoder reset on detach for autodiscovered regions\n\nThis is needed for the default behaviour with current BIOS configuration\nwhere the HDM Type2 decoders will be kept unreset when driver unloads.\nThis is the main change introduced in v23: committed decoders will not\nbe reset. Previous v22 functionality supported first driver load finding\ncommitted decoders but resetting them at unload and supporting\nuncommitted decoders in next driver loads. This will be suported in\nfollow-up works.\n\nv23 changes:\n\n  patch 11: fixing minor issues and droping change in\n\t    should_emulate_decoders (Jonathan Cameron)\n\n  patch13: refactoring unregister_region for safety type in Type2 API\n\n  sfc changes: slight modifications to error path\n\n\nv22 changes:\n\n  patch 1-3 from Dan's branch without any changes.\n\n  patch 11: new\n  \n  patch 12: moved here from v21 patch 22\n\n  patch 13-14: new\n\n  patch 23: move check ahead of type3 only checks\n\n  All patches with sfc changes adapted to support both options.\n\nv21 changes;\n\n  patch1-2: v20 patch1 splitted up doing the code move in the second\n\t    patch in v21. (Jonathan)\n \n  patch1-4: adding my Signed-off tag along with Dan's\n\n  patch5: fix duplication of CXL_NR_PARTITION definition\n\n  patch7: dropped the cxl test fixes removing unused function. It was\n\t  sent independently ahead of this version.\n\n  patch12: optimization for max free space calculation (Jonathan)\n\n  patch19: optimization for returning on error (Jonathan)\n\n\nv20 changes:\n\n  patch 1: using release helps (Jonathan).\n\n  patch 6: minor fix in comments (Jonathan).\n\n  patch 7 & 8: change commit mentioning sfc changes\n\n  patch 11:\t Fix interleave_ways setting (Jonathan)\n\t\tChange assignament location (Dave)\n\n  patch 13:  \tchanging error return order (Jonathan)\n\t\tremoving blank line (Dave)\n\n  patch 18:\tAdd check for only supporting uncommitted decoders\n\t\t\t(Ben, Dave)\n\t\tAdd check for returned value (Dave)\n\nv19 changes:\n\n  Removal of cxl_acquire_endpoint and driver callback for unexpected cxl\n  module removal. Dan's patches made them unnecessary.\n\n  patch 4: remove code already moved by Terry's patches (Ben Cheatham)\n\n  patch 6: removed unrelated change (Ben Cheatham)\n\n  patch 7: fix error report inconsistencies (Jonathan, Dave)\n\n  patch 9: remove unnecessary comment (Ben Cheatham)\n\n  patch 11: fix __free usage (Jonathan Cameron, Ben Cheatham)\n\n  patch 13: style fixes (Jonathan Cameron, Dave Jiag)\n\n  patch 14: move code to previous patch (Jonathan Cameron)\n\n  patch 18: group code in one locking (Dave Jian)\n\t    use __free helper (Ben Cheatham)\n\n\nv18 changes:\n\n  patch 1: minor changes and fixing docs generation (Jonathan, Dan)\n \n  patch4: merged with v17 patch5\n\n  patch 5: merging v17 patches 6 and 7\n\n  patch 6: adding helpers for clarity\n\n  patch 9:\n\t- minor changes (Dave)\n\t- simplifying flags check (Dan)\n\n  patch 10: minor changes (Jonathan)\n\n  patch 11:\n\t- minor changes (Dave)\n\t- fix mess (Jonathan, Dave)\n\n  patch 18: minor changes (Jonathan, Dan)\n  \nv17 changes: (Dan Williams review)\n - use devm for cxl_dev_state allocation\n - using current cxl struct for checking capability registers found by\n   the driver.\n - simplify dpa initialization without a mailbox not supporting pmem\n - add cxl_acquire_endpoint for protection during initialization\n - add callback/action to cxl_create_region for a driver notified about cxl\n   core kernel modules removal.\n - add sfc function to disable CXL-based PIO buffers if such a callback\n   is invoked.\n - Always manage a Type2 created region as private not allowing DAX.\n\nv16 changes:\n - rebase against rc4 (Dave Jiang)\n - remove duplicate line (Ben Cheatham)\n\nv15 changes:\n - remove reference to unused header file (Jonathan Cameron)\n - add proper kernel docs to exported functions (Alison Schofield)\n - using an array to map the enums to strings (Alison Schofield)\n - clarify comment when using bitmap_subset (Jonathan Cameron)\n - specify link to type2 support in all patches (Alison Schofield)\n\n  Patches changed (minor): 4, 11\n\nv14 changes:\n - static null initialization of bitmaps (Jonathan Cameron)\n - Fixing cxl tests (Alison Schofield)\n - Fixing robot compilation problems\n\n  Patches changed (minor): 1, 4, 6, 13\n\nv13 changes:\n - using names for headers checking more consistent (Jonathan Cameron)\n - using helper for caps bit setting (Jonathan Cameron)\n - provide generic function for reporting missing capabilities (Jonathan Cameron)\n - rename cxl_pci_setup_memdev_regs to cxl_pci_accel_setup_memdev_regs (Jonathan Cameron)\n - cxl_dpa_info size to be set by the Type2 driver (Jonathan Cameron)\n - avoiding rc variable when possible (Jonathan Cameron)\n - fix spelling (Simon Horman)\n - use scoped_guard (Dave Jiang)\n - use enum instead of bool (Dave Jiang)\n - dropping patch with hardware symbols\n\nv12 changes:\n - use new macro cxl_dev_state_create in pci driver (Ben Cheatham)\n - add public/private sections in now exported cxl_dev_state struct (Ben\n   Cheatham)\n - fix cxl/pci.h regarding file name for checking if defined\n - Clarify capabilities found vs expected in error message. (Ben\n   Cheatham)\n - Clarify new CXL_DECODER_F flag (Ben Cheatham)\n - Fix changes about cxl memdev creation support moving code to the\n   proper patch. (Ben Cheatham)\n - Avoid debug and function duplications (Ben Cheatham)\n\nv11 changes:\n - Dropping the use of cxl_memdev_state and going back to using\n   cxl_dev_state.\n - Using a helper for an accel driver to allocate its own cxl-related\n   struct embedding cxl_dev_state.\n - Exporting the required structs in include/cxl/cxl.h for an accel\n   driver being able to know the cxl_dev_state size required in the\n   previously mentioned helper for allocation.\n - Avoid using any struct for dpa initialization by the accel driver\n   adding a specific function for creating dpa partitions by accel\n   drivers without a mailbox.\n\nv10 changes:\n - Using cxl_memdev_state instead of cxl_dev_state for type2 which has a\n   memory after all and facilitates the setup.\n - Adapt core for using cxl_memdev_state allowing accel drivers to work\n   with them without further awareness of internal cxl structs.\n - Using last DPA changes for creating DPA partitions with accel driver\n   hardcoding mds values when no mailbox.\n - capabilities not a new field but built up when current register maps\n   is performed and returned to the caller for checking.\n - HPA free space supporting interleaving.\n - DPA free space droping max-min for a simple alloc size.\n\nv9 changes:\n - adding forward definitions (Jonathan Cameron)\n - using set_bit instead of bitmap_set (Jonathan Cameron)\n - fix rebase problem (Jonathan Cameron)\n - Improve error path (Jonathan Cameron)\n - fix build problems with cxl region dependency (robot)\n - fix error path (Simon Horman)\n\nv8 changes:\n - Change error path labeling inside sfc cxl code (Edward Cree)\n - Properly handling checks and error in sfc cxl code (Simon Horman)\n - Fix bug when checking resource_size (Simon Horman)\n - Avoid bisect problems reordering patches (Edward Cree)\n - Fix buffer allocation size in sfc (Simon Horman)\n\nv7 changes:\n\n - fixing kernel test robot complains\n - fix type with Type3 mandatory capabilities (Zhi Wang)\n - optimize code in cxl_request_resource (Kalesh Anakkur Purayil)\n - add sanity check when dealing with resources arithmetics (Fan Ni)\n - fix typos and blank lines (Fan Ni)\n - keep previous log errors/warnings in sfc driver (Martin Habets)\n - add WARN_ON_ONCE if region given is NULL\n\nv6 changes:\n\n - update sfc mcdi_pcol.h with full hardware changes most not related to\n   this patchset. This is an automatic file created from hardware design\n   changes and not touched by software. It is updated from time to time\n   and it required update for the sfc driver CXL support.\n - remove CXL capabilities definitions not used by the patchset or\n   previous kernel code. (Dave Jiang, Jonathan Cameron)\n - Use bitmap_subset instead of reinventing the wheel ... (Ben Cheatham)\n - Use cxl_accel_memdev for new device_type created (Ben Cheatham)\n - Fix construct_region use of rwsem (Zhi Wang)\n - Obtain region range instead of region params (Allison Schofield, Dave\n   Jiang)\n\nv5 changes:\n\n - Fix SFC configuration based on kernel CXL configuration\n - Add subset check for capabilities.\n - fix region creation when HDM decoders programmed by firmware/BIOS (Ben\n   Cheatham)\n - Add option for creating dax region based on driver decission (Ben\n   Cheatham)\n - Using sfc probe_data struct for keeping sfc cxl data\n\nv4 changes:\n\n - Use bitmap for capabilities new field (Jonathan Cameron)\n - Use cxl_mem attributes for sysfs based on device type (Dave Jian)\n - Add conditional cxl sfc compilation relying on kernel CXL config (kernel test robot)\n - Add sfc changes in different patches for facilitating backport (Jonathan Cameron)\n - Remove patch for dealing with cxl modules dependencies and using sfc kconfig plus\n   MODULE_SOFTDEP instead.\n\nv3 changes:\n\n - cxl_dev_state not defined as opaque but only manipulated by accel drivers\n   through accessors.\n - accessors names not identified as only for accel drivers.\n - move pci code from pci driver (drivers/cxl/pci.c) to generic pci code\n   (drivers/cxl/core/pci.c).\n - capabilities field from u8 to u32 and initialised by CXL regs discovering\n   code.\n - add capabilities check and removing current check by CXL regs discovering\n   code.\n - Not fail if CXL Device Registers not found. Not mandatory for Type2.\n - add timeout in acquire_endpoint for solving a race with the endpoint port\n   creation.\n - handle EPROBE_DEFER by sfc driver.\n - Limiting interleave ways to 1 for accel driver HPA/DPA requests.\n - factoring out interleave ways and granularity helpers from type2 region\n   creation patch.\n - restricting region_creation for type2 to one endpoint decoder.\n\nv2 changes:\n\nI have removed the introduction about the concerns with BIOS/UEFI after the\ndiscussion leading to confirm the need of the functionality implemented, at\nleast is some scenarios.\n\nThere are two main changes from the RFC:\n\n1) Following concerns about drivers using CXL core without restrictions, the CXL\nstruct to work with is opaque to those drivers, therefore functions are\nimplemented for modifying or reading those structs indirectly.\n\n2) The driver for using the added functionality is not a test driver but a real\none: the SFC ethernet network driver. It uses the CXL region mapped for PIO\nbuffers instead of regions inside PCIe BARs.\n\nRFC:\n\nCurrent CXL kernel code is focused on supporting Type3 CXL devices, aka memory\nexpanders. Type2 CXL devices, aka device accelerators, share some functionalities\nbut require some special handling.\n\nFirst of all, Type2 are by definition specific to drivers doing something and not just\na memory expander, so it is expected to work with the CXL specifics. This implies the CXL\nsetup needs to be done by such a driver instead of by a generic CXL PCI driver\nas for memory expanders. Most of such setup needs to use current CXL core code\nand therefore needs to be accessible to those vendor drivers. This is accomplished\nexporting opaque CXL structs and adding and exporting functions for working with\nthose structs indirectly.\n\nSome of the patches are based on a patchset sent by Dan Williams [1] which was just\npartially integrated, most related to making things ready for Type2 but none\nrelated to specific Type2 support. Those patches based on Dans work have Dans\nsigning as co-developer, and a link to the original patch.\n\nA final note about CXL.cache is needed. This patchset does not cover it at all,\nalthough the emulated Type2 device advertises it. From the kernel point of view\nsupporting CXL.cache will imply to be sure the CXL path supports what the Type2\ndevice needs. A device accelerator will likely be connected to a Root Switch,\nbut other configurations can not be discarded. Therefore the kernel will need to\ncheck not just HPA, DPA, interleave and granularity, but also the available\nCXL.cache support and resources in each switch in the CXL path to the Type2\ndevice. I expect to contribute to this support in the following months, and\nit would be good to discuss about it when possible.\n\n[1] https://lore.kernel.org/linux-cxl/98b1f61a-e6c2-71d4-c368-50d958501b0c@intel.com/T/\n\nAlejandro Lucero (22):\n  cxl: Add type2 device basic support\n  sfc: add cxl support\n  cxl: Move pci generic code\n  cxl/sfc: Map cxl component regs\n  cxl/sfc: Initialize dpa without a mailbox\n  cxl: Prepare memdev creation for type2\n  sfc: create type2 cxl memdev\n  cxl/hdm: Add support for getting region from committed decoder\n  cxl: Add function for obtaining region range\n  cxl: Export function for unwinding cxl by accelerators\n  sfc: obtain decoder and region if committed by firmware\n  cxl: Define a driver interface for HPA free space enumeration\n  sfc: get root decoder\n  cxl: Define a driver interface for DPA allocation\n  sfc: get endpoint decoder\n  cxl: Make region type based on endpoint type\n  cxl/region: Factor out interleave ways setup\n  cxl/region: Factor out interleave granularity setup\n  cxl: Allow region creation by type2 drivers\n  cxl: Avoid dax creation for accelerators\n  sfc: create cxl region\n  sfc: support pio mapping based on cxl\n\n drivers/cxl/core/core.h               |   5 +-\n drivers/cxl/core/hdm.c                | 123 ++++++++\n drivers/cxl/core/mbox.c               |  63 +---\n drivers/cxl/core/memdev.c             | 113 ++++++-\n drivers/cxl/core/pci.c                |  63 ++++\n drivers/cxl/core/port.c               |   1 +\n drivers/cxl/core/region.c             | 434 +++++++++++++++++++++++---\n drivers/cxl/core/regs.c               |   2 +-\n drivers/cxl/cxl.h                     | 125 +-------\n drivers/cxl/cxlmem.h                  |  92 +-----\n drivers/cxl/cxlpci.h                  |  21 +-\n drivers/cxl/mem.c                     |  45 ++-\n drivers/cxl/pci.c                     |  85 +----\n drivers/net/ethernet/sfc/Kconfig      |  10 +\n drivers/net/ethernet/sfc/Makefile     |   1 +\n drivers/net/ethernet/sfc/ef10.c       |  50 ++-\n drivers/net/ethernet/sfc/efx.c        |  15 +-\n drivers/net/ethernet/sfc/efx_cxl.c    | 186 +++++++++++\n drivers/net/ethernet/sfc/efx_cxl.h    |  41 +++\n drivers/net/ethernet/sfc/net_driver.h |  12 +\n drivers/net/ethernet/sfc/nic.h        |   3 +\n include/cxl/cxl.h                     | 287 +++++++++++++++++\n include/cxl/pci.h                     |  21 ++\n tools/testing/cxl/test/mem.c          |   3 +-\n 24 files changed, 1376 insertions(+), 425 deletions(-)\n create mode 100644 drivers/net/ethernet/sfc/efx_cxl.c\n create mode 100644 drivers/net/ethernet/sfc/efx_cxl.h\n create mode 100644 include/cxl/cxl.h\n create mode 100644 include/cxl/pci.h\n\n\nbase-commit: 3f7938b1aec7f06d5b23adca83e4542fcf027001\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "Author acknowledged that the patch needs to export core functions for a Type2 driver to discover and map device component registers, agreed to use them in sfc driver cxl initialization.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nExport cxl core functions for a Type2 driver being able to discover and\nmap the device component registers.\n\nUse it in sfc driver cxl initialization.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n---\n drivers/cxl/core/pci.c             |  1 +\n drivers/cxl/core/port.c            |  1 +\n drivers/cxl/core/regs.c            |  1 +\n drivers/cxl/cxl.h                  |  7 ------\n drivers/cxl/cxlpci.h               | 12 ----------\n drivers/cxl/pci.c                  |  1 +\n drivers/net/ethernet/sfc/efx_cxl.c | 35 ++++++++++++++++++++++++++++++\n include/cxl/cxl.h                  | 19 ++++++++++++++++\n include/cxl/pci.h                  | 21 ++++++++++++++++++\n 9 files changed, 79 insertions(+), 19 deletions(-)\n create mode 100644 include/cxl/pci.h\n\ndiff --git a/drivers/cxl/core/pci.c b/drivers/cxl/core/pci.c\nindex 6b7e50858d56..ba2d393c540a 100644\n--- a/drivers/cxl/core/pci.c\n+++ b/drivers/cxl/core/pci.c\n@@ -6,6 +6,7 @@\n #include <linux/delay.h>\n #include <linux/pci.h>\n #include <linux/pci-doe.h>\n+#include <cxl/pci.h>\n #include <linux/aer.h>\n #include <cxlpci.h>\n #include <cxlmem.h>\ndiff --git a/drivers/cxl/core/port.c b/drivers/cxl/core/port.c\nindex 54f72452fb06..385588b8b30b 100644\n--- a/drivers/cxl/core/port.c\n+++ b/drivers/cxl/core/port.c\n@@ -11,6 +11,7 @@\n #include <linux/idr.h>\n #include <linux/node.h>\n #include <cxl/einj.h>\n+#include <cxl/pci.h>\n #include <cxlmem.h>\n #include <cxlpci.h>\n #include <cxl.h>\ndiff --git a/drivers/cxl/core/regs.c b/drivers/cxl/core/regs.c\nindex 93710cf4f0a6..20c2d9fbcfe7 100644\n--- a/drivers/cxl/core/regs.c\n+++ b/drivers/cxl/core/regs.c\n@@ -4,6 +4,7 @@\n #include <linux/device.h>\n #include <linux/slab.h>\n #include <linux/pci.h>\n+#include <cxl/pci.h>\n #include <cxlmem.h>\n #include <cxlpci.h>\n #include <pmu.h>\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 5d111980d879..944c5d1ccceb 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -39,10 +39,6 @@ extern const struct nvdimm_security_ops *cxl_security_ops;\n #define   CXL_CM_CAP_HDR_ARRAY_SIZE_MASK GENMASK(31, 24)\n #define CXL_CM_CAP_PTR_MASK GENMASK(31, 20)\n \n-#define   CXL_CM_CAP_CAP_ID_RAS 0x2\n-#define   CXL_CM_CAP_CAP_ID_HDM 0x5\n-#define   CXL_CM_CAP_CAP_HDM_VERSION 1\n-\n /* HDM decoders CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure */\n #define CXL_HDM_DECODER_CAP_OFFSET 0x0\n #define   CXL_HDM_DECODER_COUNT_MASK GENMASK(3, 0)\n@@ -206,9 +202,6 @@ void cxl_probe_component_regs(struct device *dev, void __iomem *base,\n \t\t\t      struct cxl_component_reg_map *map);\n void cxl_probe_device_regs(struct device *dev, void __iomem *base,\n \t\t\t   struct cxl_device_reg_map *map);\n-int cxl_map_component_regs(const struct cxl_register_map *map,\n-\t\t\t   struct cxl_component_regs *regs,\n-\t\t\t   unsigned long map_mask);\n int cxl_map_device_regs(const struct cxl_register_map *map,\n \t\t\tstruct cxl_device_regs *regs);\n int cxl_map_pmu_regs(struct cxl_register_map *map, struct cxl_pmu_regs *regs);\ndiff --git a/drivers/cxl/cxlpci.h b/drivers/cxl/cxlpci.h\nindex d879120b2780..93df1b1fa326 100644\n--- a/drivers/cxl/cxlpci.h\n+++ b/drivers/cxl/cxlpci.h\n@@ -13,16 +13,6 @@\n  */\n #define CXL_PCI_DEFAULT_MAX_VECTORS 16\n \n-/* Register Block Identifier (RBI) */\n-enum cxl_regloc_type {\n-\tCXL_REGLOC_RBI_EMPTY = 0,\n-\tCXL_REGLOC_RBI_COMPONENT,\n-\tCXL_REGLOC_RBI_VIRT,\n-\tCXL_REGLOC_RBI_MEMDEV,\n-\tCXL_REGLOC_RBI_PMU,\n-\tCXL_REGLOC_RBI_TYPES\n-};\n-\n /*\n  * Table Access DOE, CDAT Read Entry Response\n  *\n@@ -106,6 +96,4 @@ static inline void cxl_dport_init_ras_reporting(struct cxl_dport *dport,\n \t\t\t\t\t\tstruct device *host) { }\n #endif\n \n-int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n-\t\t       struct cxl_register_map *map);\n #endif /* __CXL_PCI_H__ */\ndiff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\nindex 668d44eb1bf5..7b4699fb8870 100644\n--- a/drivers/cxl/pci.c\n+++ b/drivers/cxl/pci.c\n@@ -11,6 +11,7 @@\n #include <linux/pci.h>\n #include <linux/aer.h>\n #include <linux/io.h>\n+#include <cxl/pci.h>\n #include <cxl/mailbox.h>\n #include \"cxlmem.h\"\n #include \"cxlpci.h\"\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 8e0481d8dced..34126bc4826c 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -7,6 +7,8 @@\n \n #include <linux/pci.h>\n \n+#include <cxl/cxl.h>\n+#include <cxl/pci.h>\n #include \"net_driver.h\"\n #include \"efx_cxl.h\"\n \n@@ -18,6 +20,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \tstruct pci_dev *pci_dev = efx->pci_dev;\n \tstruct efx_cxl *cxl;\n \tu16 dvsec;\n+\tint rc;\n \n \tprobe_data->cxl_pio_initialised = false;\n \n@@ -44,6 +47,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \tif (!cxl)\n \t\treturn -ENOMEM;\n \n+\trc = cxl_pci_setup_regs(pci_dev, CXL_REGLOC_RBI_COMPONENT,\n+\t\t\t\t&cxl->cxlds.reg_map);\n+\tif (rc) {\n+\t\tpci_err(pci_dev, \"No component registers\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\tif (!cxl->cxlds.reg_map.component_map.hdm_decoder.valid) {\n+\t\tpci_err(pci_dev, \"Expected HDM component register not found\\n\");\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tif (!cxl->cxlds.reg_map.component_map.ras.valid) {\n+\t\tpci_err(pci_dev, \"Expected RAS component register not found\\n\");\n+\t\treturn -ENODEV;\n+\t}\n+\n+\trc = cxl_map_component_regs(&cxl->cxlds.reg_map,\n+\t\t\t\t    &cxl->cxlds.regs.component,\n+\t\t\t\t    BIT(CXL_CM_CAP_CAP_ID_RAS));\n+\tif (rc) {\n+\t\tpci_err(pci_dev, \"Failed to map RAS capability.\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\t/*\n+\t * Set media ready explicitly as there are neither mailbox for checking\n+\t * this state nor the CXL register involved, both not mandatory for\n+\t * type2.\n+\t */\n+\tcxl->cxlds.media_ready = true;\n+\n \tprobe_data->cxl = cxl;\n \n \treturn 0;\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 13d448686189..7f2e23bce1f7 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -70,6 +70,10 @@ struct cxl_regs {\n \t);\n };\n \n+#define   CXL_CM_CAP_CAP_ID_RAS 0x2\n+#define   CXL_CM_CAP_CAP_ID_HDM 0x5\n+#define   CXL_CM_CAP_CAP_HDM_VERSION 1\n+\n struct cxl_reg_map {\n \tbool valid;\n \tint id;\n@@ -223,4 +227,19 @@ struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n \t\t(drv_struct *)_devm_cxl_dev_state_create(parent, type, serial, dvsec,\t\\\n \t\t\t\t\t\t      sizeof(drv_struct), mbox);\t\\\n \t})\n+\n+/**\n+ * cxl_map_component_regs - map cxl component registers\n+ *\n+ * @map: cxl register map to update with the mappings\n+ * @regs: cxl component registers to work with\n+ * @map_mask: cxl component regs to map\n+ *\n+ * Returns integer: success (0) or error (-ENOMEM)\n+ *\n+ * Made public for Type2 driver support.\n+ */\n+int cxl_map_component_regs(const struct cxl_register_map *map,\n+\t\t\t   struct cxl_component_regs *regs,\n+\t\t\t   unsigned long map_mask);\n #endif /* __CXL_CXL_H__ */\ndiff --git a/include/cxl/pci.h b/include/cxl/pci.h\nnew file mode 100644\nindex 000000000000..a172439f08c6\n--- /dev/null\n+++ b/include/cxl/pci.h\n@@ -0,0 +1,21 @@\n+/* SPDX-License-Identifier: GPL-2.0-only */\n+/* Copyright(c) 2020 Intel Corporation. All rights reserved. */\n+\n+#ifndef __CXL_CXL_PCI_H__\n+#define __CXL_CXL_PCI_H__\n+\n+/* Register Block Identifier (RBI) */\n+enum cxl_regloc_type {\n+\tCXL_REGLOC_RBI_EMPTY = 0,\n+\tCXL_REGLOC_RBI_COMPONENT,\n+\tCXL_REGLOC_RBI_VIRT,\n+\tCXL_REGLOC_RBI_MEMDEV,\n+\tCXL_REGLOC_RBI_PMU,\n+\tCXL_REGLOC_RBI_TYPES\n+};\n+\n+struct cxl_register_map;\n+\n+int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n+\t\t       struct cxl_register_map *map);\n+#endif\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed the reviewer's concern that cxl_pci_setup_regs() was not correctly handling RCRB for CXL Type2 devices by explaining that it is actually a helper function and moving the necessary code from cxl/pci_drv.c to cxl/core/pci.c. The author confirmed that this change will be exported and shared with CXL Type2 device initialization.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nInside cxl/core/pci.c there are helpers for CXL PCIe initialization\nmeanwhile cxl/pci_drv.c implements the functionality for a Type3 device\ninitialization.\n\nMove helper functions from cxl/core/pci_drv.c to cxl/core/pci.c in order\nto be exported and shared with CXL Type2 device initialization.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Fan Ni <fan.ni@samsung.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\n---\n drivers/cxl/core/core.h |  3 +-\n drivers/cxl/core/pci.c  | 62 ++++++++++++++++++++++++++++++++++++\n drivers/cxl/core/regs.c |  1 -\n drivers/cxl/cxl.h       |  2 --\n drivers/cxl/cxlpci.h    | 13 ++++++++\n drivers/cxl/pci.c       | 70 -----------------------------------------\n 6 files changed, 77 insertions(+), 74 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 422531799af2..256799d39361 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -187,5 +187,6 @@ int cxl_set_feature(struct cxl_mailbox *cxl_mbox, const uuid_t *feat_uuid,\n \t\t    size_t feat_data_size, u32 feat_flag, u16 offset,\n \t\t    u16 *return_code);\n #endif\n-\n+resource_size_t cxl_rcd_component_reg_phys(struct device *dev,\n+\t\t\t\t\t   struct cxl_dport *dport);\n #endif /* __CXL_CORE_H__ */\ndiff --git a/drivers/cxl/core/pci.c b/drivers/cxl/core/pci.c\nindex b838c59d7a3c..6b7e50858d56 100644\n--- a/drivers/cxl/core/pci.c\n+++ b/drivers/cxl/core/pci.c\n@@ -696,6 +696,68 @@ bool cxl_endpoint_decoder_reset_detected(struct cxl_port *port)\n }\n EXPORT_SYMBOL_NS_GPL(cxl_endpoint_decoder_reset_detected, \"CXL\");\n \n+static int cxl_rcrb_get_comp_regs(struct pci_dev *pdev,\n+\t\t\t\t  struct cxl_register_map *map,\n+\t\t\t\t  struct cxl_dport *dport)\n+{\n+\tresource_size_t component_reg_phys;\n+\n+\t*map = (struct cxl_register_map) {\n+\t\t.host = &pdev->dev,\n+\t\t.resource = CXL_RESOURCE_NONE,\n+\t};\n+\n+\tstruct cxl_port *port __free(put_cxl_port) =\n+\t\tcxl_pci_find_port(pdev, &dport);\n+\tif (!port)\n+\t\treturn -EPROBE_DEFER;\n+\n+\tcomponent_reg_phys = cxl_rcd_component_reg_phys(&pdev->dev, dport);\n+\tif (component_reg_phys == CXL_RESOURCE_NONE)\n+\t\treturn -ENXIO;\n+\n+\tmap->resource = component_reg_phys;\n+\tmap->reg_type = CXL_REGLOC_RBI_COMPONENT;\n+\tmap->max_size = CXL_COMPONENT_REG_BLOCK_SIZE;\n+\n+\treturn 0;\n+}\n+\n+int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n+\t\t\tstruct cxl_register_map *map)\n+{\n+\tint rc;\n+\n+\trc = cxl_find_regblock(pdev, type, map);\n+\n+\t/*\n+\t * If the Register Locator DVSEC does not exist, check if it\n+\t * is an RCH and try to extract the Component Registers from\n+\t * an RCRB.\n+\t */\n+\tif (rc && type == CXL_REGLOC_RBI_COMPONENT && is_cxl_restricted(pdev)) {\n+\t\tstruct cxl_dport *dport;\n+\t\tstruct cxl_port *port __free(put_cxl_port) =\n+\t\t\tcxl_pci_find_port(pdev, &dport);\n+\t\tif (!port)\n+\t\t\treturn -EPROBE_DEFER;\n+\n+\t\trc = cxl_rcrb_get_comp_regs(pdev, map, dport);\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\n+\t\trc = cxl_dport_map_rcd_linkcap(pdev, dport);\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\n+\t} else if (rc) {\n+\t\treturn rc;\n+\t}\n+\n+\treturn cxl_setup_regs(map);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_pci_setup_regs, \"CXL\");\n+\n int cxl_pci_get_bandwidth(struct pci_dev *pdev, struct access_coordinate *c)\n {\n \tint speed, bw;\ndiff --git a/drivers/cxl/core/regs.c b/drivers/cxl/core/regs.c\nindex a010b3214342..93710cf4f0a6 100644\n--- a/drivers/cxl/core/regs.c\n+++ b/drivers/cxl/core/regs.c\n@@ -641,4 +641,3 @@ resource_size_t cxl_rcd_component_reg_phys(struct device *dev,\n \t\treturn CXL_RESOURCE_NONE;\n \treturn __rcrb_to_component(dev, &dport->rcrb, CXL_RCRB_UPSTREAM);\n }\n-EXPORT_SYMBOL_NS_GPL(cxl_rcd_component_reg_phys, \"CXL\");\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 3eaa353e430b..5d111980d879 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -222,8 +222,6 @@ int cxl_find_regblock(struct pci_dev *pdev, enum cxl_regloc_type type,\n \t\t      struct cxl_register_map *map);\n int cxl_setup_regs(struct cxl_register_map *map);\n struct cxl_dport;\n-resource_size_t cxl_rcd_component_reg_phys(struct device *dev,\n-\t\t\t\t\t   struct cxl_dport *dport);\n int cxl_dport_map_rcd_linkcap(struct pci_dev *pdev, struct cxl_dport *dport);\n \n #define CXL_RESOURCE_NONE ((resource_size_t) -1)\ndiff --git a/drivers/cxl/cxlpci.h b/drivers/cxl/cxlpci.h\nindex 6f9c78886fd9..d879120b2780 100644\n--- a/drivers/cxl/cxlpci.h\n+++ b/drivers/cxl/cxlpci.h\n@@ -74,6 +74,17 @@ static inline bool cxl_pci_flit_256(struct pci_dev *pdev)\n \treturn lnksta2 & PCI_EXP_LNKSTA2_FLIT;\n }\n \n+/*\n+ * Assume that the caller has already validated that @pdev has CXL\n+ * capabilities, any RCiEP with CXL capabilities is treated as a\n+ * Restricted CXL Device (RCD) and finds upstream port and endpoint\n+ * registers in a Root Complex Register Block (RCRB).\n+ */\n+static inline bool is_cxl_restricted(struct pci_dev *pdev)\n+{\n+\treturn pci_pcie_type(pdev) == PCI_EXP_TYPE_RC_END;\n+}\n+\n struct cxl_dev_state;\n void read_cdat_data(struct cxl_port *port);\n \n@@ -95,4 +106,6 @@ static inline void cxl_dport_init_ras_reporting(struct cxl_dport *dport,\n \t\t\t\t\t\tstruct device *host) { }\n #endif\n \n+int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n+\t\t       struct cxl_register_map *map);\n #endif /* __CXL_PCI_H__ */\ndiff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\nindex 24179cc702bf..668d44eb1bf5 100644\n--- a/drivers/cxl/pci.c\n+++ b/drivers/cxl/pci.c\n@@ -465,76 +465,6 @@ static int cxl_pci_setup_mailbox(struct cxl_memdev_state *mds, bool irq_avail)\n \treturn 0;\n }\n \n-/*\n- * Assume that any RCIEP that emits the CXL memory expander class code\n- * is an RCD\n- */\n-static bool is_cxl_restricted(struct pci_dev *pdev)\n-{\n-\treturn pci_pcie_type(pdev) == PCI_EXP_TYPE_RC_END;\n-}\n-\n-static int cxl_rcrb_get_comp_regs(struct pci_dev *pdev,\n-\t\t\t\t  struct cxl_register_map *map,\n-\t\t\t\t  struct cxl_dport *dport)\n-{\n-\tresource_size_t component_reg_phys;\n-\n-\t*map = (struct cxl_register_map) {\n-\t\t.host = &pdev->dev,\n-\t\t.resource = CXL_RESOURCE_NONE,\n-\t};\n-\n-\tstruct cxl_port *port __free(put_cxl_port) =\n-\t\tcxl_pci_find_port(pdev, &dport);\n-\tif (!port)\n-\t\treturn -EPROBE_DEFER;\n-\n-\tcomponent_reg_phys = cxl_rcd_component_reg_phys(&pdev->dev, dport);\n-\tif (component_reg_phys == CXL_RESOURCE_NONE)\n-\t\treturn -ENXIO;\n-\n-\tmap->resource = component_reg_phys;\n-\tmap->reg_type = CXL_REGLOC_RBI_COMPONENT;\n-\tmap->max_size = CXL_COMPONENT_REG_BLOCK_SIZE;\n-\n-\treturn 0;\n-}\n-\n-static int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n-\t\t\t      struct cxl_register_map *map)\n-{\n-\tint rc;\n-\n-\trc = cxl_find_regblock(pdev, type, map);\n-\n-\t/*\n-\t * If the Register Locator DVSEC does not exist, check if it\n-\t * is an RCH and try to extract the Component Registers from\n-\t * an RCRB.\n-\t */\n-\tif (rc && type == CXL_REGLOC_RBI_COMPONENT && is_cxl_restricted(pdev)) {\n-\t\tstruct cxl_dport *dport;\n-\t\tstruct cxl_port *port __free(put_cxl_port) =\n-\t\t\tcxl_pci_find_port(pdev, &dport);\n-\t\tif (!port)\n-\t\t\treturn -EPROBE_DEFER;\n-\n-\t\trc = cxl_rcrb_get_comp_regs(pdev, map, dport);\n-\t\tif (rc)\n-\t\t\treturn rc;\n-\n-\t\trc = cxl_dport_map_rcd_linkcap(pdev, dport);\n-\t\tif (rc)\n-\t\t\treturn rc;\n-\n-\t} else if (rc) {\n-\t\treturn rc;\n-\t}\n-\n-\treturn cxl_setup_regs(map);\n-}\n-\n static int cxl_pci_ras_unmask(struct pci_dev *pdev)\n {\n \tstruct cxl_dev_state *cxlds = pci_get_drvdata(pdev);\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about the cxl_mem_get_partition_info function being static and moved to memdev, which was causing issues for Type2 drivers. The author agrees that this function should be exported and has added it to the memdev module.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "exported"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nType3 relies on mailbox CXL_MBOX_OP_IDENTIFY command for initializing\nmemdev state params which end up being used for DPA initialization.\n\nAllow a Type2 driver to initialize DPA simply by giving the size of its\nvolatile hardware partition.\n\nMove related functions to memdev.\n\nAdd sfc driver as the client.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n---\n drivers/cxl/core/core.h            |  2 +\n drivers/cxl/core/mbox.c            | 51 +----------------------\n drivers/cxl/core/memdev.c          | 66 ++++++++++++++++++++++++++++++\n drivers/net/ethernet/sfc/efx_cxl.c |  5 +++\n include/cxl/cxl.h                  |  1 +\n 5 files changed, 75 insertions(+), 50 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 256799d39361..e3c85ceda248 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -89,6 +89,8 @@ void __iomem *devm_cxl_iomap_block(struct device *dev, resource_size_t addr,\n struct dentry *cxl_debugfs_create_dir(const char *dir);\n int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n \t\t     enum cxl_partition_mode mode);\n+struct cxl_memdev_state;\n+int cxl_mem_get_partition_info(struct cxl_memdev_state *mds);\n int cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size);\n int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n resource_size_t cxl_dpa_size(struct cxl_endpoint_decoder *cxled);\ndiff --git a/drivers/cxl/core/mbox.c b/drivers/cxl/core/mbox.c\nindex bee84d0101d1..d57a0c2d39fb 100644\n--- a/drivers/cxl/core/mbox.c\n+++ b/drivers/cxl/core/mbox.c\n@@ -1144,7 +1144,7 @@ EXPORT_SYMBOL_NS_GPL(cxl_mem_get_event_records, \"CXL\");\n  *\n  * See CXL @8.2.9.5.2.1 Get Partition Info\n  */\n-static int cxl_mem_get_partition_info(struct cxl_memdev_state *mds)\n+int cxl_mem_get_partition_info(struct cxl_memdev_state *mds)\n {\n \tstruct cxl_mailbox *cxl_mbox = &mds->cxlds.cxl_mbox;\n \tstruct cxl_mbox_get_partition_info pi;\n@@ -1300,55 +1300,6 @@ int cxl_mem_sanitize(struct cxl_memdev *cxlmd, u16 cmd)\n \treturn -EBUSY;\n }\n \n-static void add_part(struct cxl_dpa_info *info, u64 start, u64 size, enum cxl_partition_mode mode)\n-{\n-\tint i = info->nr_partitions;\n-\n-\tif (size == 0)\n-\t\treturn;\n-\n-\tinfo->part[i].range = (struct range) {\n-\t\t.start = start,\n-\t\t.end = start + size - 1,\n-\t};\n-\tinfo->part[i].mode = mode;\n-\tinfo->nr_partitions++;\n-}\n-\n-int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info)\n-{\n-\tstruct cxl_dev_state *cxlds = &mds->cxlds;\n-\tstruct device *dev = cxlds->dev;\n-\tint rc;\n-\n-\tif (!cxlds->media_ready) {\n-\t\tinfo->size = 0;\n-\t\treturn 0;\n-\t}\n-\n-\tinfo->size = mds->total_bytes;\n-\n-\tif (mds->partition_align_bytes == 0) {\n-\t\tadd_part(info, 0, mds->volatile_only_bytes, CXL_PARTMODE_RAM);\n-\t\tadd_part(info, mds->volatile_only_bytes,\n-\t\t\t mds->persistent_only_bytes, CXL_PARTMODE_PMEM);\n-\t\treturn 0;\n-\t}\n-\n-\trc = cxl_mem_get_partition_info(mds);\n-\tif (rc) {\n-\t\tdev_err(dev, \"Failed to query partition information\\n\");\n-\t\treturn rc;\n-\t}\n-\n-\tadd_part(info, 0, mds->active_volatile_bytes, CXL_PARTMODE_RAM);\n-\tadd_part(info, mds->active_volatile_bytes, mds->active_persistent_bytes,\n-\t\t CXL_PARTMODE_PMEM);\n-\n-\treturn 0;\n-}\n-EXPORT_SYMBOL_NS_GPL(cxl_mem_dpa_fetch, \"CXL\");\n-\n int cxl_get_dirty_count(struct cxl_memdev_state *mds, u32 *count)\n {\n \tstruct cxl_mailbox *cxl_mbox = &mds->cxlds.cxl_mbox;\ndiff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\nindex 22d156f25305..2c5dd72f43ca 100644\n--- a/drivers/cxl/core/memdev.c\n+++ b/drivers/cxl/core/memdev.c\n@@ -582,6 +582,72 @@ bool is_cxl_memdev(const struct device *dev)\n }\n EXPORT_SYMBOL_NS_GPL(is_cxl_memdev, \"CXL\");\n \n+static void add_part(struct cxl_dpa_info *info, u64 start, u64 size, enum cxl_partition_mode mode)\n+{\n+\tint i = info->nr_partitions;\n+\n+\tif (size == 0)\n+\t\treturn;\n+\n+\tinfo->part[i].range = (struct range) {\n+\t\t.start = start,\n+\t\t.end = start + size - 1,\n+\t};\n+\tinfo->part[i].mode = mode;\n+\tinfo->nr_partitions++;\n+}\n+\n+int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info)\n+{\n+\tstruct cxl_dev_state *cxlds = &mds->cxlds;\n+\tstruct device *dev = cxlds->dev;\n+\tint rc;\n+\n+\tif (!cxlds->media_ready) {\n+\t\tinfo->size = 0;\n+\t\treturn 0;\n+\t}\n+\n+\tinfo->size = mds->total_bytes;\n+\n+\tif (mds->partition_align_bytes == 0) {\n+\t\tadd_part(info, 0, mds->volatile_only_bytes, CXL_PARTMODE_RAM);\n+\t\tadd_part(info, mds->volatile_only_bytes,\n+\t\t\t mds->persistent_only_bytes, CXL_PARTMODE_PMEM);\n+\t\treturn 0;\n+\t}\n+\n+\trc = cxl_mem_get_partition_info(mds);\n+\tif (rc) {\n+\t\tdev_err(dev, \"Failed to query partition information\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\tadd_part(info, 0, mds->active_volatile_bytes, CXL_PARTMODE_RAM);\n+\tadd_part(info, mds->active_volatile_bytes, mds->active_persistent_bytes,\n+\t\t CXL_PARTMODE_PMEM);\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_mem_dpa_fetch, \"CXL\");\n+\n+/**\n+ * cxl_set_capacity: initialize dpa by a driver without a mailbox.\n+ *\n+ * @cxlds: pointer to cxl_dev_state\n+ * @capacity: device volatile memory size\n+ */\n+int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity)\n+{\n+\tstruct cxl_dpa_info range_info = {\n+\t\t.size = capacity,\n+\t};\n+\n+\tadd_part(&range_info, 0, capacity, CXL_PARTMODE_RAM);\n+\treturn cxl_dpa_setup(cxlds, &range_info);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_set_capacity, \"CXL\");\n+\n /**\n  * set_exclusive_cxl_commands() - atomically disable user cxl commands\n  * @mds: The device state to operate on\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 34126bc4826c..0b10a2e6aceb 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -79,6 +79,11 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t */\n \tcxl->cxlds.media_ready = true;\n \n+\tif (cxl_set_capacity(&cxl->cxlds, EFX_CTPIO_BUFFER_SIZE)) {\n+\t\tpci_err(pci_dev, \"dpa capacity setup failed\\n\");\n+\t\treturn -ENODEV;\n+\t}\n+\n \tprobe_data->cxl = cxl;\n \n \treturn 0;\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 7f2e23bce1f7..fb2f8f2395d5 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -242,4 +242,5 @@ struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n int cxl_map_component_regs(const struct cxl_register_map *map,\n \t\t\t   struct cxl_component_regs *regs,\n \t\t\t   unsigned long map_mask);\n+int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the current CXL core relying on a specific device type when creating a memdev, which leads to problems with obtaining cxl_memdev_state references from another type of device. The author modified the check for obtaining cxl_memdev_state to add support for the other device type and made devm_cxl_add_memdev accessible from an accel driver.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCurrent cxl core is relying on a CXL_DEVTYPE_CLASSMEM type device when\ncreating a memdev leading to problems when obtaining cxl_memdev_state\nreferences from a CXL_DEVTYPE_DEVMEM type.\n\nModify check for obtaining cxl_memdev_state adding CXL_DEVTYPE_DEVMEM\nsupport.\n\nMake devm_cxl_add_memdev accessible from an accel driver.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\n---\n drivers/cxl/core/memdev.c | 15 +++++++++++--\n drivers/cxl/cxlmem.h      |  6 ------\n drivers/cxl/mem.c         | 45 +++++++++++++++++++++++++++++----------\n include/cxl/cxl.h         |  6 ++++++\n 4 files changed, 53 insertions(+), 19 deletions(-)\n\ndiff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\nindex 2c5dd72f43ca..1b43763b8e20 100644\n--- a/drivers/cxl/core/memdev.c\n+++ b/drivers/cxl/core/memdev.c\n@@ -7,6 +7,7 @@\n #include <linux/slab.h>\n #include <linux/idr.h>\n #include <linux/pci.h>\n+#include <cxl/cxl.h>\n #include <cxlmem.h>\n #include \"trace.h\"\n #include \"core.h\"\n@@ -576,9 +577,16 @@ static const struct device_type cxl_memdev_type = {\n \t.groups = cxl_memdev_attribute_groups,\n };\n \n+static const struct device_type cxl_accel_memdev_type = {\n+\t.name = \"cxl_accel_memdev\",\n+\t.release = cxl_memdev_release,\n+\t.devnode = cxl_memdev_devnode,\n+};\n+\n bool is_cxl_memdev(const struct device *dev)\n {\n-\treturn dev->type == &cxl_memdev_type;\n+\treturn (dev->type == &cxl_memdev_type ||\n+\t\tdev->type == &cxl_accel_memdev_type);\n }\n EXPORT_SYMBOL_NS_GPL(is_cxl_memdev, \"CXL\");\n \n@@ -781,7 +789,10 @@ static struct cxl_memdev *cxl_memdev_alloc(struct cxl_dev_state *cxlds,\n \tdev->parent = cxlds->dev;\n \tdev->bus = &cxl_bus_type;\n \tdev->devt = MKDEV(cxl_mem_major, cxlmd->id);\n-\tdev->type = &cxl_memdev_type;\n+\tif (cxlds->type == CXL_DEVTYPE_DEVMEM)\n+\t\tdev->type = &cxl_accel_memdev_type;\n+\telse\n+\t\tdev->type = &cxl_memdev_type;\n \tdevice_set_pm_not_required(dev);\n \tINIT_WORK(&cxlmd->detach_work, detach_memdev);\n \ndiff --git a/drivers/cxl/cxlmem.h b/drivers/cxl/cxlmem.h\nindex 281546de426e..c98db6f18aa2 100644\n--- a/drivers/cxl/cxlmem.h\n+++ b/drivers/cxl/cxlmem.h\n@@ -34,10 +34,6 @@\n \t(FIELD_GET(CXLMDEV_RESET_NEEDED_MASK, status) !=                       \\\n \t CXLMDEV_RESET_NEEDED_NOT)\n \n-struct cxl_memdev_attach {\n-\tint (*probe)(struct cxl_memdev *cxlmd);\n-};\n-\n /**\n  * struct cxl_memdev - CXL bus object representing a Type-3 Memory Device\n  * @dev: driver core device object\n@@ -103,8 +99,6 @@ static inline bool is_cxl_endpoint(struct cxl_port *port)\n \n struct cxl_memdev *__devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n \t\t\t\t\t const struct cxl_memdev_attach *attach);\n-struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n-\t\t\t\t       const struct cxl_memdev_attach *attach);\n int devm_cxl_sanitize_setup_notifier(struct device *host,\n \t\t\t\t     struct cxl_memdev *cxlmd);\n struct cxl_memdev_state;\ndiff --git a/drivers/cxl/mem.c b/drivers/cxl/mem.c\nindex 0958bea915ac..39687baedd1a 100644\n--- a/drivers/cxl/mem.c\n+++ b/drivers/cxl/mem.c\n@@ -65,6 +65,26 @@ static int cxl_debugfs_poison_clear(void *data, u64 dpa)\n DEFINE_DEBUGFS_ATTRIBUTE(cxl_poison_clear_fops, NULL,\n \t\t\t cxl_debugfs_poison_clear, \"%llx\\n\");\n \n+static void cxl_memdev_poison_enable(struct cxl_memdev_state *mds,\n+\t\t\t\t     struct cxl_memdev *cxlmd,\n+\t\t\t\t     struct dentry *dentry)\n+{\n+\t/*\n+\t * Avoid poison debugfs for DEVMEM aka accelerators as they rely on\n+\t * cxl_memdev_state.\n+\t */\n+\tif (!mds)\n+\t\treturn;\n+\n+\tif (test_bit(CXL_POISON_ENABLED_INJECT, mds->poison.enabled_cmds))\n+\t\tdebugfs_create_file(\"inject_poison\", 0200, dentry, cxlmd,\n+\t\t\t\t    &cxl_poison_inject_fops);\n+\n+\tif (test_bit(CXL_POISON_ENABLED_CLEAR, mds->poison.enabled_cmds))\n+\t\tdebugfs_create_file(\"clear_poison\", 0200, dentry, cxlmd,\n+\t\t\t\t    &cxl_poison_clear_fops);\n+}\n+\n static int cxl_mem_probe(struct device *dev)\n {\n \tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n@@ -92,12 +112,7 @@ static int cxl_mem_probe(struct device *dev)\n \tdentry = cxl_debugfs_create_dir(dev_name(dev));\n \tdebugfs_create_devm_seqfile(dev, \"dpamem\", dentry, cxl_mem_dpa_show);\n \n-\tif (test_bit(CXL_POISON_ENABLED_INJECT, mds->poison.enabled_cmds))\n-\t\tdebugfs_create_file(\"inject_poison\", 0200, dentry, cxlmd,\n-\t\t\t\t    &cxl_poison_inject_fops);\n-\tif (test_bit(CXL_POISON_ENABLED_CLEAR, mds->poison.enabled_cmds))\n-\t\tdebugfs_create_file(\"clear_poison\", 0200, dentry, cxlmd,\n-\t\t\t\t    &cxl_poison_clear_fops);\n+\tcxl_memdev_poison_enable(mds, cxlmd, dentry);\n \n \trc = devm_add_action_or_reset(dev, remove_debugfs, dentry);\n \tif (rc)\n@@ -208,16 +223,24 @@ static ssize_t trigger_poison_list_store(struct device *dev,\n }\n static DEVICE_ATTR_WO(trigger_poison_list);\n \n-static umode_t cxl_mem_visible(struct kobject *kobj, struct attribute *a, int n)\n+static bool cxl_poison_attr_visible(struct kobject *kobj, struct attribute *a)\n {\n \tstruct device *dev = kobj_to_dev(kobj);\n \tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n \tstruct cxl_memdev_state *mds = to_cxl_memdev_state(cxlmd->cxlds);\n \n-\tif (a == &dev_attr_trigger_poison_list.attr)\n-\t\tif (!test_bit(CXL_POISON_ENABLED_LIST,\n-\t\t\t      mds->poison.enabled_cmds))\n-\t\t\treturn 0;\n+\tif (!mds ||\n+\t    !test_bit(CXL_POISON_ENABLED_LIST, mds->poison.enabled_cmds))\n+\t\treturn false;\n+\n+\treturn true;\n+}\n+\n+static umode_t cxl_mem_visible(struct kobject *kobj, struct attribute *a, int n)\n+{\n+\tif (a == &dev_attr_trigger_poison_list.attr &&\n+\t    !cxl_poison_attr_visible(kobj, a))\n+\t\treturn 0;\n \n \treturn a->mode;\n }\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex fb2f8f2395d5..6f8d365067af 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -153,6 +153,10 @@ struct cxl_dpa_partition {\n \n #define CXL_NR_PARTITIONS_MAX 2\n \n+struct cxl_memdev_attach {\n+\tint (*probe)(struct cxl_memdev *cxlmd);\n+};\n+\n /**\n  * struct cxl_dev_state - The driver device state\n  *\n@@ -243,4 +247,6 @@ int cxl_map_component_regs(const struct cxl_register_map *map,\n \t\t\t   struct cxl_component_regs *regs,\n \t\t\t   unsigned long map_mask);\n int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n+struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n+\t\t\t\t       const struct cxl_memdev_attach *attach);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about the cxl API usage in efx_cxl.c, specifically how to create a cxl memory device using the type2 cxl_dev_state struct. The author agrees to use the cxl API for creating the cxl memory device and has added code to do so.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "added_code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nUse cxl API for creating a cxl memory device using the type2\ncxl_dev_state struct.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Martin Habets <habetsm.xilinx@gmail.com>\nReviewed-by: Fan Ni <fan.ni@samsung.com>\nAcked-by: Edward Cree <ecree.xilinx@gmail.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/net/ethernet/sfc/efx_cxl.c | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 0b10a2e6aceb..a77ef4783fcb 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -84,6 +84,12 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\treturn -ENODEV;\n \t}\n \n+\tcxl->cxlmd = devm_cxl_add_memdev(&cxl->cxlds, NULL);\n+\tif (IS_ERR(cxl->cxlmd)) {\n+\t\tpci_err(pci_dev, \"CXL accel memdev creation failed\");\n+\t\treturn PTR_ERR(cxl->cxlmd);\n+\t}\n+\n \tprobe_data->cxl = cxl;\n \n \treturn 0;\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the possibility of a Type2 device having its HDM committed before memdev creation, and responded by adding a new function cxl_get_committed_decoder() to check for this condition after memdev initialization. The function will return the committed decoder if it exists, or NULL otherwise. This change is intended to allow Type2 drivers to work with the HPA and DPA space even when the HDM is not committed.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nA Type2 device configured by the BIOS can already have its HDM\ncommitted. Add a cxl_get_committed_decoder() function for cheking\nso after memdev creation. A CXL region should have been created\nduring memdev initialization, therefore a Type2 driver can ask for\nsuch a region for working with the HPA. If the HDM is not committed,\na Type2 driver will create the region after obtaining proper HPA\nand DPA space.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/hdm.c | 39 +++++++++++++++++++++++++++++++++++++++\n include/cxl/cxl.h      |  3 +++\n 2 files changed, 42 insertions(+)\n\ndiff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\nindex 6e516c69b2d2..a172ce4e9b19 100644\n--- a/drivers/cxl/core/hdm.c\n+++ b/drivers/cxl/core/hdm.c\n@@ -686,6 +686,45 @@ int cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n \treturn devm_add_action_or_reset(&port->dev, cxl_dpa_release, cxled);\n }\n \n+static int find_committed_endpoint_decoder(struct device *dev, const void *data)\n+{\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_port *port;\n+\n+\tif (!is_endpoint_decoder(dev))\n+\t\treturn 0;\n+\n+\tcxled = to_cxl_endpoint_decoder(dev);\n+\tport = cxled_to_port(cxled);\n+\n+\treturn cxled->cxld.id == port->hdm_end;\n+}\n+\n+struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t\t       struct cxl_region **cxlr)\n+{\n+\tstruct cxl_port *endpoint = cxlmd->endpoint;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct device *cxled_dev;\n+\n+\tif (!endpoint)\n+\t\treturn NULL;\n+\n+\tguard(rwsem_read)(&cxl_rwsem.dpa);\n+\tcxled_dev = device_find_child(&endpoint->dev, NULL,\n+\t\t\t\t      find_committed_endpoint_decoder);\n+\n+\tif (!cxled_dev)\n+\t\treturn NULL;\n+\n+\tcxled = to_cxl_endpoint_decoder(cxled_dev);\n+\t*cxlr = cxled->cxld.region;\n+\n+\tput_device(cxled_dev);\n+\treturn cxled;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_get_committed_decoder, \"CXL\");\n+\n static void cxld_set_interleave(struct cxl_decoder *cxld, u32 *ctrl)\n {\n \tu16 eig;\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 6f8d365067af..928276dba952 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -249,4 +249,7 @@ int cxl_map_component_regs(const struct cxl_register_map *map,\n int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n \t\t\t\t       const struct cxl_memdev_attach *attach);\n+struct cxl_region;\n+struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t\t       struct cxl_region **cxlr);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the accelerator driver API not having a clean exit mechanism, and added a new function cxl_unregister_region() to handle this. The function is exported for use by other drivers.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nAdd cxl_unregister_region() to the accelerator driver API\nfor a clean exit.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/region.c | 17 ++++++++++++-----\n include/cxl/cxl.h         |  1 +\n 2 files changed, 13 insertions(+), 5 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex acf29ba3b205..954b8fcdbac6 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2438,9 +2438,8 @@ static struct cxl_region *to_cxl_region(struct device *dev)\n \treturn container_of(dev, struct cxl_region, dev);\n }\n \n-static void unregister_region(void *_cxlr)\n+void cxl_unregister_region(struct cxl_region *cxlr)\n {\n-\tstruct cxl_region *cxlr = _cxlr;\n \tstruct cxl_region_params *p = &cxlr->params;\n \tint i;\n \n@@ -2457,6 +2456,14 @@ static void unregister_region(void *_cxlr)\n \tcxl_region_iomem_release(cxlr);\n \tput_device(&cxlr->dev);\n }\n+EXPORT_SYMBOL_NS_GPL(cxl_unregister_region, \"CXL\");\n+\n+static void __unregister_region(void *_cxlr)\n+{\n+\tstruct cxl_region *cxlr = _cxlr;\n+\n+\treturn cxl_unregister_region(cxlr);\n+}\n \n static struct lock_class_key cxl_region_key;\n \n@@ -2608,7 +2615,7 @@ static struct cxl_region *devm_cxl_add_region(struct cxl_root_decoder *cxlrd,\n \tif (rc)\n \t\tgoto err;\n \n-\trc = devm_add_action_or_reset(port->uport_dev, unregister_region, cxlr);\n+\trc = devm_add_action_or_reset(port->uport_dev, __unregister_region, cxlr);\n \tif (rc)\n \t\treturn ERR_PTR(rc);\n \n@@ -2762,7 +2769,7 @@ static ssize_t delete_region_store(struct device *dev,\n \tif (IS_ERR(cxlr))\n \t\treturn PTR_ERR(cxlr);\n \n-\tdevm_release_action(port->uport_dev, unregister_region, cxlr);\n+\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n \tput_device(&cxlr->dev);\n \n \treturn len;\n@@ -3878,7 +3885,7 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n \n \trc = __construct_region(cxlr, cxlrd, cxled);\n \tif (rc) {\n-\t\tdevm_release_action(port->uport_dev, unregister_region, cxlr);\n+\t\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n \t\treturn ERR_PTR(rc);\n \t}\n \ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 906065e0d2a6..92880c26b2d5 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -254,4 +254,5 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n \t\t\t\t\t\t       struct cxl_region **cxlr);\n struct range;\n int cxl_get_region_range(struct cxl_region *region, struct range *range);\n+void cxl_unregister_region(struct cxl_region *cxlr);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about Type2 drivers not having access to the cxl region struct, which they acknowledge is private to the kernel CXL core. They are adding a function called cxl_get_region_range that allows Type2 drivers to get the cxl region range for mapping memory ranges.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "adding new function"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nA CXL region struct contains the physical address to work with.\n\nType2 drivers can create a CXL region but have not access to the\nrelated struct as it is defined as private by the kernel CXL core.\nAdd a function for getting the cxl region range to be used for mapping\nsuch memory range by a Type2 driver.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Zhi Wang <zhiw@nvidia.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/cxl/core/region.c | 23 +++++++++++++++++++++++\n include/cxl/cxl.h         |  2 ++\n 2 files changed, 25 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 96888d87a8df..acf29ba3b205 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2621,6 +2621,29 @@ static struct cxl_region *devm_cxl_add_region(struct cxl_root_decoder *cxlrd,\n \treturn ERR_PTR(rc);\n }\n \n+/**\n+ * cxl_get_region_range - obtain range linked to a CXL region\n+ *\n+ * @region: a pointer to struct cxl_region\n+ * @range: a pointer to a struct range to be set\n+ *\n+ * Returns 0 or error.\n+ */\n+int cxl_get_region_range(struct cxl_region *region, struct range *range)\n+{\n+\tif (WARN_ON_ONCE(!region))\n+\t\treturn -ENODEV;\n+\n+\tif (!region->params.res)\n+\t\treturn -ENOSPC;\n+\n+\trange->start = region->params.res->start;\n+\trange->end = region->params.res->end;\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_get_region_range, \"CXL\");\n+\n static ssize_t __create_region_show(struct cxl_root_decoder *cxlrd, char *buf)\n {\n \treturn sysfs_emit(buf, \"region%u\\n\", atomic_read(&cxlrd->region_id));\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 928276dba952..906065e0d2a6 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -252,4 +252,6 @@ struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n struct cxl_region;\n struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n \t\t\t\t\t\t       struct cxl_region **cxlr);\n+struct range;\n+int cxl_get_region_range(struct cxl_region *region, struct range *range);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "Author acknowledged a need to support Type2 CXL devices and explained that the new API retrieves a root decoder (platform CXL window) that fits specified constraints and available capacity, adding a complementary function for releasing the reference to such root decoder.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a need",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCXL region creation involves allocating capacity from Device Physical\nAddress (DPA) and assigning it to decode a given Host Physical Address\n(HPA). Before determining how much DPA to allocate the amount of available\nHPA must be determined. Also, not all HPA is created equal, some HPA\ntargets RAM, some targets PMEM, some is prepared for device-memory flows\nlike HDM-D and HDM-DB, and some is HDM-H (host-only).\n\nIn order to support Type2 CXL devices, wrap all of those concerns into\nan API that retrieves a root decoder (platform CXL window) that fits the\nspecified constraints and the capacity available for a new region.\n\nAdd a complementary function for releasing the reference to such root\ndecoder.\n\nBased on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n---\n drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h         |   3 +\n include/cxl/cxl.h         |   6 ++\n 3 files changed, 173 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 954b8fcdbac6..bdefd088f5f1 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n \treturn 0;\n }\n \n+struct cxlrd_max_context {\n+\tstruct device * const *host_bridges;\n+\tint interleave_ways;\n+\tunsigned long flags;\n+\tresource_size_t max_hpa;\n+\tstruct cxl_root_decoder *cxlrd;\n+};\n+\n+static int find_max_hpa(struct device *dev, void *data)\n+{\n+\tstruct cxlrd_max_context *ctx = data;\n+\tstruct cxl_switch_decoder *cxlsd;\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct resource *res, *prev;\n+\tstruct cxl_decoder *cxld;\n+\tresource_size_t free = 0;\n+\tresource_size_t max;\n+\tint found = 0;\n+\n+\tif (!is_root_decoder(dev))\n+\t\treturn 0;\n+\n+\tcxlrd = to_cxl_root_decoder(dev);\n+\tcxlsd = &cxlrd->cxlsd;\n+\tcxld = &cxlsd->cxld;\n+\n+\tif ((cxld->flags & ctx->flags) != ctx->flags) {\n+\t\tdev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n+\t\t\tcxld->flags, ctx->flags);\n+\t\treturn 0;\n+\t}\n+\n+\tfor (int i = 0; i < ctx->interleave_ways; i++) {\n+\t\tfor (int j = 0; j < ctx->interleave_ways; j++) {\n+\t\t\tif (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n+\t\t\t\tfound++;\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif (found != ctx->interleave_ways) {\n+\t\tdev_dbg(dev,\n+\t\t\t\"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n+\t\t\tfound, ctx->interleave_ways);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Walk the root decoder resource range relying on cxl_rwsem.region to\n+\t * preclude sibling arrival/departure and find the largest free space\n+\t * gap.\n+\t */\n+\tlockdep_assert_held_read(&cxl_rwsem.region);\n+\tres = cxlrd->res->child;\n+\n+\t/* With no resource child the whole parent resource is available */\n+\tif (!res)\n+\t\tmax = resource_size(cxlrd->res);\n+\telse\n+\t\tmax = 0;\n+\n+\tfor (prev = NULL; res; prev = res, res = res->sibling) {\n+\t\tif (!prev && res->start == cxlrd->res->start &&\n+\t\t    res->end == cxlrd->res->end) {\n+\t\t\tmax = resource_size(cxlrd->res);\n+\t\t\tbreak;\n+\t\t}\n+\t\t/*\n+\t\t * Sanity check for preventing arithmetic problems below as a\n+\t\t * resource with size 0 could imply using the end field below\n+\t\t * when set to unsigned zero - 1 or all f in hex.\n+\t\t */\n+\t\tif (prev && !resource_size(prev))\n+\t\t\tcontinue;\n+\n+\t\tif (!prev && res->start > cxlrd->res->start) {\n+\t\t\tfree = res->start - cxlrd->res->start;\n+\t\t\tmax = max(free, max);\n+\t\t}\n+\t\tif (prev && res->start > prev->end + 1) {\n+\t\t\tfree = res->start - prev->end + 1;\n+\t\t\tmax = max(free, max);\n+\t\t}\n+\t}\n+\n+\tif (prev && prev->end + 1 < cxlrd->res->end + 1) {\n+\t\tfree = cxlrd->res->end + 1 - prev->end + 1;\n+\t\tmax = max(free, max);\n+\t}\n+\n+\tdev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n+\tif (max > ctx->max_hpa) {\n+\t\tif (ctx->cxlrd)\n+\t\t\tput_device(cxlrd_dev(ctx->cxlrd));\n+\t\tget_device(cxlrd_dev(cxlrd));\n+\t\tctx->cxlrd = cxlrd;\n+\t\tctx->max_hpa = max;\n+\t}\n+\treturn 0;\n+}\n+\n+/**\n+ * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n+ * @cxlmd: the mem device requiring the HPA\n+ * @interleave_ways: number of entries in @host_bridges\n+ * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n+ * @max_avail_contig: output parameter of max contiguous bytes available in the\n+ *\t\t      returned decoder\n+ *\n+ * Returns a pointer to a struct cxl_root_decoder\n+ *\n+ * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n+ * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n+ * caller goes to use this decoder and its capacity is reduced then caller needs\n+ * to loop and retry.\n+ *\n+ * The returned root decoder has an elevated reference count that needs to be\n+ * put with cxl_put_root_decoder(cxlrd).\n+ */\n+struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t       int interleave_ways,\n+\t\t\t\t\t       unsigned long flags,\n+\t\t\t\t\t       resource_size_t *max_avail_contig)\n+{\n+\tstruct cxlrd_max_context ctx = {\n+\t\t.flags = flags,\n+\t\t.interleave_ways = interleave_ways,\n+\t};\n+\tstruct cxl_port *root_port;\n+\tstruct cxl_port *endpoint;\n+\n+\tendpoint = cxlmd->endpoint;\n+\tif (!endpoint) {\n+\t\tdev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n+\t\treturn ERR_PTR(-ENXIO);\n+\t}\n+\n+\tctx.host_bridges = &endpoint->host_bridge;\n+\n+\tstruct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n+\tif (!root) {\n+\t\tdev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n+\t\treturn ERR_PTR(-ENXIO);\n+\t}\n+\n+\troot_port = &root->port;\n+\tscoped_guard(rwsem_read, &cxl_rwsem.region)\n+\t\tdevice_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n+\n+\tif (!ctx.cxlrd)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\t*max_avail_contig = ctx.max_hpa;\n+\treturn ctx.cxlrd;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n+\n+void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n+{\n+\tput_device(cxlrd_dev(cxlrd));\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n+\n static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n \t\t\t  const char *buf, size_t len)\n {\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 944c5d1ccceb..c7d9b2c2908f 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n bool is_root_decoder(struct device *dev);\n+\n+#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n+\n bool is_switch_decoder(struct device *dev);\n bool is_endpoint_decoder(struct device *dev);\n struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 92880c26b2d5..834dc7e78934 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n struct range;\n int cxl_get_region_range(struct cxl_region *region, struct range *range);\n void cxl_unregister_region(struct cxl_region *cxlr);\n+struct cxl_port;\n+struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t       int interleave_ways,\n+\t\t\t\t\t       unsigned long flags,\n+\t\t\t\t\t       resource_size_t *max);\n+void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about the code's expectation of Type3 or CXL_DECODER_HOSTONLYMEM devices only, and agrees that support for Type2 requires region type to be based on endpoint type HDM-D[B]. The author has already made changes in this patch (v23) to accommodate this requirement.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCurrent code is expecting Type3 or CXL_DECODER_HOSTONLYMEM devices only.\nSupport for Type2 implies region type needs to be based on the endpoint\ntype HDM-D[B] instead.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Zhi Wang <zhiw@nvidia.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\nReviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n---\n drivers/cxl/core/region.c | 10 ++++++----\n 1 file changed, 6 insertions(+), 4 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex bdefd088f5f1..f53b2e9fd9e6 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2833,7 +2833,8 @@ static ssize_t create_ram_region_show(struct device *dev,\n }\n \n static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n-\t\t\t\t\t  enum cxl_partition_mode mode, int id)\n+\t\t\t\t\t  enum cxl_partition_mode mode, int id,\n+\t\t\t\t\t  enum cxl_decoder_type target_type)\n {\n \tint rc;\n \n@@ -2855,7 +2856,7 @@ static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n \t\treturn ERR_PTR(-EBUSY);\n \t}\n \n-\treturn devm_cxl_add_region(cxlrd, id, mode, CXL_DECODER_HOSTONLYMEM);\n+\treturn devm_cxl_add_region(cxlrd, id, mode, target_type);\n }\n \n static ssize_t create_region_store(struct device *dev, const char *buf,\n@@ -2869,7 +2870,7 @@ static ssize_t create_region_store(struct device *dev, const char *buf,\n \tif (rc != 1)\n \t\treturn -EINVAL;\n \n-\tcxlr = __create_region(cxlrd, mode, id);\n+\tcxlr = __create_region(cxlrd, mode, id, CXL_DECODER_HOSTONLYMEM);\n \tif (IS_ERR(cxlr))\n \t\treturn PTR_ERR(cxlr);\n \n@@ -4036,7 +4037,8 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n \n \tdo {\n \t\tcxlr = __create_region(cxlrd, cxlds->part[part].mode,\n-\t\t\t\t       atomic_read(&cxlrd->region_id));\n+\t\t\t\t       atomic_read(&cxlrd->region_id),\n+\t\t\t\t       cxled->cxld.target_type);\n \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n \n \tif (IS_ERR(cxlr)) {\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about using the cxl API to get the Host Physical Address (HPA) from a CXL root decoder. They modified the code to use the cxl api for getting HPA and added checks for available free space before proceeding.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "modification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nUse cxl api for getting HPA (Host Physical Address) to use from a\nCXL root decoder.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Martin Habets <habetsm.xilinx@gmail.com>\nAcked-by: Edward Cree <ecree.xilinx@gmail.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n---\n drivers/cxl/cxl.h                  | 15 ---------------\n drivers/net/ethernet/sfc/Kconfig   |  1 +\n drivers/net/ethernet/sfc/efx_cxl.c | 26 +++++++++++++++++++++++---\n drivers/net/ethernet/sfc/efx_cxl.h |  1 +\n include/cxl/cxl.h                  | 15 +++++++++++++++\n 5 files changed, 40 insertions(+), 18 deletions(-)\n\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex c7d9b2c2908f..d1b010e5e1d0 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -220,21 +220,6 @@ int cxl_dport_map_rcd_linkcap(struct pci_dev *pdev, struct cxl_dport *dport);\n #define CXL_RESOURCE_NONE ((resource_size_t) -1)\n #define CXL_TARGET_STRLEN 20\n \n-/*\n- * cxl_decoder flags that define the type of memory / devices this\n- * decoder supports as well as configuration lock status See \"CXL 2.0\n- * 8.2.5.12.7 CXL HDM Decoder 0 Control Register\" for details.\n- * Additionally indicate whether decoder settings were autodetected,\n- * user customized.\n- */\n-#define CXL_DECODER_F_RAM   BIT(0)\n-#define CXL_DECODER_F_PMEM  BIT(1)\n-#define CXL_DECODER_F_TYPE2 BIT(2)\n-#define CXL_DECODER_F_TYPE3 BIT(3)\n-#define CXL_DECODER_F_LOCK  BIT(4)\n-#define CXL_DECODER_F_ENABLE    BIT(5)\n-#define CXL_DECODER_F_MASK  GENMASK(5, 0)\n-\n enum cxl_decoder_type {\n \tCXL_DECODER_DEVMEM = 2,\n \tCXL_DECODER_HOSTONLYMEM = 3,\ndiff --git a/drivers/net/ethernet/sfc/Kconfig b/drivers/net/ethernet/sfc/Kconfig\nindex 979f2801e2a8..e959d9b4f4ce 100644\n--- a/drivers/net/ethernet/sfc/Kconfig\n+++ b/drivers/net/ethernet/sfc/Kconfig\n@@ -69,6 +69,7 @@ config SFC_MCDI_LOGGING\n config SFC_CXL\n \tbool \"Solarflare SFC9100-family CXL support\"\n \tdepends on SFC && CXL_BUS >= SFC\n+\tdepends on CXL_REGION\n \tdefault SFC\n \thelp\n \t  This enables SFC CXL support if the kernel is configuring CXL for\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 3536eccf1b2a..1a4c1097c315 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -18,6 +18,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n {\n \tstruct efx_nic *efx = &probe_data->efx;\n \tstruct pci_dev *pci_dev = efx->pci_dev;\n+\tresource_size_t max_size;\n \tstruct efx_cxl *cxl;\n \tstruct range range;\n \tu16 dvsec;\n@@ -110,9 +111,24 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\t\treturn -ENOMEM;\n \t\t}\n \n-\t\tprobe_data->cxl = cxl;\n+\t\tcxl->hdm_was_committed = true;\n+\t} else {\n+\t\tcxl->cxlrd = cxl_get_hpa_freespace(cxl->cxlmd, 1, CXL_DECODER_F_RAM |\n+\t\t\t\t\t\t   CXL_DECODER_F_TYPE2, &max_size);\n+\t\tif (IS_ERR(cxl->cxlrd)) {\n+\t\t\tdev_err(&pci_dev->dev, \"cxl_get_hpa_freespace failed\\n\");\n+\t\t\treturn PTR_ERR(cxl->cxlrd);\n+\t\t}\n+\n+\t\tif (max_size < EFX_CTPIO_BUFFER_SIZE) {\n+\t\t\tdev_err(&pci_dev->dev, \"%s: not enough free HPA space %pap < %u\\n\",\n+\t\t\t\t__func__, &max_size, EFX_CTPIO_BUFFER_SIZE);\n+\t\t\tcxl_put_root_decoder(cxl->cxlrd);\n+\t\t\treturn -ENOSPC;\n+\t\t}\n \t}\n \n+\tprobe_data->cxl = cxl;\n \treturn 0;\n }\n \n@@ -121,8 +137,12 @@ void efx_cxl_exit(struct efx_probe_data *probe_data)\n \tif (!probe_data->cxl)\n \t\treturn;\n \n-\tiounmap(probe_data->cxl->ctpio_cxl);\n-\tcxl_unregister_region(probe_data->cxl->efx_region);\n+\tif (probe_data->cxl->hdm_was_committed) {\n+\t\tiounmap(probe_data->cxl->ctpio_cxl);\n+\t\tcxl_unregister_region(probe_data->cxl->efx_region);\n+\t} else {\n+\t\tcxl_put_root_decoder(probe_data->cxl->cxlrd);\n+\t}\n }\n \n MODULE_IMPORT_NS(\"CXL\");\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.h b/drivers/net/ethernet/sfc/efx_cxl.h\nindex 961639cef692..9a92e386695b 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.h\n+++ b/drivers/net/ethernet/sfc/efx_cxl.h\n@@ -27,6 +27,7 @@ struct efx_cxl {\n \tstruct cxl_root_decoder *cxlrd;\n \tstruct cxl_port *endpoint;\n \tstruct cxl_endpoint_decoder *cxled;\n+\tbool hdm_was_committed;\n \tstruct cxl_region *efx_region;\n \tvoid __iomem *ctpio_cxl;\n };\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 834dc7e78934..783ad570a6eb 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -153,6 +153,21 @@ struct cxl_dpa_partition {\n \n #define CXL_NR_PARTITIONS_MAX 2\n \n+/*\n+ * cxl_decoder flags that define the type of memory / devices this\n+ * decoder supports as well as configuration lock status See \"CXL 2.0\n+ * 8.2.5.12.7 CXL HDM Decoder 0 Control Register\" for details.\n+ * Additionally indicate whether decoder settings were autodetected,\n+ * user customized.\n+ */\n+#define CXL_DECODER_F_RAM   BIT(0)\n+#define CXL_DECODER_F_PMEM  BIT(1)\n+#define CXL_DECODER_F_TYPE2 BIT(2)\n+#define CXL_DECODER_F_TYPE3 BIT(3)\n+#define CXL_DECODER_F_LOCK  BIT(4)\n+#define CXL_DECODER_F_ENABLE    BIT(5)\n+#define CXL_DECODER_F_MASK  GENMASK(5, 0)\n+\n struct cxl_memdev_attach {\n \tint (*probe)(struct cxl_memdev *cxlmd);\n };\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern that the patch does not properly handle the case where a CXL region is already committed during firmware/BIOS initialization. The author has added code to check if the device HDM is already committed and, if so, get the HPA from the region and map it.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCheck if device HDM is already committed during firmware/BIOS\ninitialization.\n\nA CXL region should exist if so after memdev allocation/initialization.\nGet HPA from region and map it.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/net/ethernet/sfc/efx_cxl.c | 28 +++++++++++++++++++++++++++-\n 1 file changed, 27 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex a77ef4783fcb..3536eccf1b2a 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -19,6 +19,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \tstruct efx_nic *efx = &probe_data->efx;\n \tstruct pci_dev *pci_dev = efx->pci_dev;\n \tstruct efx_cxl *cxl;\n+\tstruct range range;\n \tu16 dvsec;\n \tint rc;\n \n@@ -90,13 +91,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\treturn PTR_ERR(cxl->cxlmd);\n \t}\n \n-\tprobe_data->cxl = cxl;\n+\tcxl->cxled = cxl_get_committed_decoder(cxl->cxlmd, &cxl->efx_region);\n+\tif (cxl->cxled) {\n+\t\tif (!cxl->efx_region) {\n+\t\t\tpci_err(pci_dev, \"CXL found committed decoder without a region\");\n+\t\t\treturn -ENODEV;\n+\t\t}\n+\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n+\t\tif (rc) {\n+\t\t\tpci_err(pci_dev,\n+\t\t\t\t\"CXL getting regions params from a committed decoder failed\");\n+\t\t\treturn rc;\n+\t\t}\n+\n+\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n+\t\tif (!cxl->ctpio_cxl) {\n+\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tprobe_data->cxl = cxl;\n+\t}\n \n \treturn 0;\n }\n \n void efx_cxl_exit(struct efx_probe_data *probe_data)\n {\n+\tif (!probe_data->cxl)\n+\t\treturn;\n+\n+\tiounmap(probe_data->cxl->ctpio_cxl);\n+\tcxl_unregister_region(probe_data->cxl->efx_region);\n }\n \n MODULE_IMPORT_NS(\"CXL\");\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the cxl_create_region() call in efx_cxl_init(), explaining that it should be used to create a region using the endpoint decoder related to a DPA range, and provided updated code to implement this.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nUse cxl api for creating a region using the endpoint decoder related to\na DPA range.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/net/ethernet/sfc/efx_cxl.c | 10 +++++++++-\n 1 file changed, 9 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 2cfd0a46225f..4d5f3974e51d 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -134,6 +134,14 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\t\tcxl_put_root_decoder(cxl->cxlrd);\n \t\t\treturn PTR_ERR(cxl->cxled);\n \t\t}\n+\n+\t\tcxl->efx_region = cxl_create_region(cxl->cxlrd, &cxl->cxled, 1);\n+\t\tif (IS_ERR(cxl->efx_region)) {\n+\t\t\tpci_err(pci_dev, \"CXL accel create region failed\");\n+\t\t\tcxl_put_root_decoder(cxl->cxlrd);\n+\t\t\tcxl_dpa_free(cxl->cxled);\n+\t\t\treturn PTR_ERR(cxl->efx_region);\n+\t\t}\n \t}\n \n \tprobe_data->cxl = cxl;\n@@ -147,11 +155,11 @@ void efx_cxl_exit(struct efx_probe_data *probe_data)\n \n \tif (probe_data->cxl->hdm_was_committed) {\n \t\tiounmap(probe_data->cxl->ctpio_cxl);\n-\t\tcxl_unregister_region(probe_data->cxl->efx_region);\n \t} else {\n \t\tcxl_dpa_free(probe_data->cxl->cxled);\n \t\tcxl_put_root_decoder(probe_data->cxl->cxlrd);\n \t}\n+\tcxl_unregister_region(probe_data->cxl->efx_region);\n }\n \n MODULE_IMPORT_NS(\"CXL\");\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the interleaving ways store function, which was previously acquiring the region rwsem lock twice and then dropping it. The author refactored the code to acquire the lock once in a new helper function set_interleave_ways(), which is called from the store function. This change ensures that the lock is held for the duration of the operation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "refactoring",
                "clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup for interleave ways.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Zhi Wang <zhiw@nvidia.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\n---\n drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n 1 file changed, 27 insertions(+), 16 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex f53b2e9fd9e6..ece1d3df7cf1 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n \n static const struct attribute_group *get_cxl_region_target_group(void);\n \n-static ssize_t interleave_ways_store(struct device *dev,\n-\t\t\t\t     struct device_attribute *attr,\n-\t\t\t\t     const char *buf, size_t len)\n+static int set_interleave_ways(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n+\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tunsigned int val, save;\n-\tint rc;\n+\tint save, rc;\n \tu8 iw;\n \n-\trc = kstrtouint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = ways_to_eiw(val, &iw);\n \tif (rc)\n \t\treturn rc;\n@@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n \t\treturn -EINVAL;\n \t}\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n@@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n \tsave = p->interleave_ways;\n \tp->interleave_ways = val;\n \trc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n-\tif (rc) {\n+\tif (rc)\n \t\tp->interleave_ways = save;\n+\n+\treturn rc;\n+}\n+\n+static ssize_t interleave_ways_store(struct device *dev,\n+\t\t\t\t     struct device_attribute *attr,\n+\t\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tunsigned int val;\n+\tint rc;\n+\n+\trc = kstrtouint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_ways(cxlr, val);\n+\tif (rc)\n \t\treturn rc;\n-\t}\n \n \treturn len;\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about supporting CXL Type2 devices by defining an API, cxl_request_dpa(), to allocate DPA memory required for operation. The author explained that the memory requested should not be bigger than the max available HPA obtained previously with cxl_get_hpa_freespace().",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation involves finding available DPA (device-physical-address)\ncapacity to map into HPA (host-physical-address) space.\n\nIn order to support CXL Type2 devices, define an API, cxl_request_dpa(),\nthat tries to allocate the DPA memory the driver requires to operate.The\nmemory requested should not be bigger than the max available HPA obtained\npreviously with cxl_get_hpa_freespace().\n\nBased on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h      |  1 +\n include/cxl/cxl.h      |  5 +++\n 3 files changed, 90 insertions(+)\n\ndiff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\nindex a172ce4e9b19..d60a697f12cc 100644\n--- a/drivers/cxl/core/hdm.c\n+++ b/drivers/cxl/core/hdm.c\n@@ -3,6 +3,7 @@\n #include <linux/seq_file.h>\n #include <linux/device.h>\n #include <linux/delay.h>\n+#include <cxl/cxl.h>\n \n #include \"cxlmem.h\"\n #include \"core.h\"\n@@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n \treturn resource_contains(res, &_addr);\n }\n \n+/**\n+ * cxl_dpa_free - release DPA (Device Physical Address)\n+ * @cxled: endpoint decoder linked to the DPA\n+ *\n+ * Returns 0 or error.\n+ */\n int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n {\n \tstruct cxl_port *port = cxled_to_port(cxled);\n@@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n \tdevm_cxl_dpa_release(cxled);\n \treturn 0;\n }\n+EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n \n int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n \t\t     enum cxl_partition_mode mode)\n@@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n \treturn 0;\n }\n \n+static int find_free_decoder(struct device *dev, const void *data)\n+{\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_port *port;\n+\n+\tif (!is_endpoint_decoder(dev))\n+\t\treturn 0;\n+\n+\tcxled = to_cxl_endpoint_decoder(dev);\n+\tport = cxled_to_port(cxled);\n+\n+\treturn cxled->cxld.id == (port->hdm_end + 1);\n+}\n+\n+static struct cxl_endpoint_decoder *\n+cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_port *endpoint = cxlmd->endpoint;\n+\tstruct device *dev;\n+\n+\tguard(rwsem_read)(&cxl_rwsem.dpa);\n+\tdev = device_find_child(&endpoint->dev, NULL,\n+\t\t\t\tfind_free_decoder);\n+\tif (!dev)\n+\t\treturn NULL;\n+\n+\treturn to_cxl_endpoint_decoder(dev);\n+}\n+\n+/**\n+ * cxl_request_dpa - search and reserve DPA given input constraints\n+ * @cxlmd: memdev with an endpoint port with available decoders\n+ * @mode: CXL partition mode (ram vs pmem)\n+ * @alloc: dpa size required\n+ *\n+ * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n+ * an errno encoded pointer on failure.\n+ *\n+ * Given that a region needs to allocate from limited HPA capacity it\n+ * may be the case that a device has more mappable DPA capacity than\n+ * available HPA. The expectation is that @alloc is a driver known\n+ * value based on the device capacity but which could not be fully\n+ * available due to HPA constraints.\n+ *\n+ * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n+ * reserved, or an error pointer. The caller is also expected to own the\n+ * lifetime of the memdev registration associated with the endpoint to\n+ * pin the decoder registered as well.\n+ */\n+struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t     enum cxl_partition_mode mode,\n+\t\t\t\t\t     resource_size_t alloc)\n+{\n+\tint rc;\n+\n+\tif (!IS_ALIGNED(alloc, SZ_256M))\n+\t\treturn ERR_PTR(-EINVAL);\n+\n+\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n+\t\tcxl_find_free_decoder(cxlmd);\n+\n+\tif (!cxled)\n+\t\treturn ERR_PTR(-ENODEV);\n+\n+\trc = cxl_dpa_set_part(cxled, mode);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\trc = cxl_dpa_alloc(cxled, alloc);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\treturn no_free_ptr(cxled);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n+\n static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n {\n \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex d1b010e5e1d0..2b1f7d687a0e 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n \n DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n+DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n \ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 783ad570a6eb..4802371db00e 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -7,6 +7,7 @@\n \n #include <linux/node.h>\n #include <linux/ioport.h>\n+#include <linux/range.h>\n #include <cxl/mailbox.h>\n \n /**\n@@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n \t\t\t\t\t       unsigned long flags,\n \t\t\t\t\t       resource_size_t *max);\n void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n+struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t     enum cxl_partition_mode mode,\n+\t\t\t\t\t     resource_size_t alloc);\n+int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the interleaving granularity being set in user space, agreeing to factor out a common helper from the user-sysfs region setup for interleave granularity before kernel-driven region creation is implemented.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "planned restructuring"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup forinterleave\ngranularity.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Zhi Wang <zhiw@nvidia.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\n---\n drivers/cxl/core/region.c | 39 +++++++++++++++++++++++++--------------\n 1 file changed, 25 insertions(+), 14 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex ece1d3df7cf1..63c2aeb2ee1f 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -559,21 +559,14 @@ static ssize_t interleave_granularity_show(struct device *dev,\n \treturn sysfs_emit(buf, \"%d\\n\", p->interleave_granularity);\n }\n \n-static ssize_t interleave_granularity_store(struct device *dev,\n-\t\t\t\t\t    struct device_attribute *attr,\n-\t\t\t\t\t    const char *buf, size_t len)\n+static int set_interleave_granularity(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n+\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tint rc, val;\n+\tint rc;\n \tu16 ig;\n \n-\trc = kstrtoint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = granularity_to_eig(val, &ig);\n \tif (rc)\n \t\treturn rc;\n@@ -589,14 +582,32 @@ static ssize_t interleave_granularity_store(struct device *dev,\n \tif (cxld->interleave_ways > 1 && val != cxld->interleave_granularity)\n \t\treturn -EINVAL;\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n-\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n \n \tp->interleave_granularity = val;\n+\treturn 0;\n+}\n+\n+static ssize_t interleave_granularity_store(struct device *dev,\n+\t\t\t\t\t    struct device_attribute *attr,\n+\t\t\t\t\t    const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tint rc, val;\n+\n+\trc = kstrtoint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_granularity(cxlr, val);\n+\tif (rc)\n+\t\treturn rc;\n \n \treturn len;\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about using the CXL API to get the Device Physical Address (DPA) for use through an endpoint decoder, agreeing that this is the correct approach and making changes in v23 to implement it.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed with feedback",
                "made changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nUse cxl api for getting DPA (Device Physical Address) to use through an\nendpoint decoder.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Martin Habets <habetsm.xilinx@gmail.com>\nAcked-by: Edward Cree <ecree.xilinx@gmail.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/net/ethernet/sfc/efx_cxl.c | 9 +++++++++\n 1 file changed, 9 insertions(+)\n\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 1a4c1097c315..2cfd0a46225f 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -126,6 +126,14 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\t\tcxl_put_root_decoder(cxl->cxlrd);\n \t\t\treturn -ENOSPC;\n \t\t}\n+\n+\t\tcxl->cxled = cxl_request_dpa(cxl->cxlmd, CXL_PARTMODE_RAM,\n+\t\t\t\t\t     EFX_CTPIO_BUFFER_SIZE);\n+\t\tif (IS_ERR(cxl->cxled)) {\n+\t\t\tpci_err(pci_dev, \"CXL accel request DPA failed\");\n+\t\t\tcxl_put_root_decoder(cxl->cxlrd);\n+\t\t\treturn PTR_ERR(cxl->cxled);\n+\t\t}\n \t}\n \n \tprobe_data->cxl = cxl;\n@@ -141,6 +149,7 @@ void efx_cxl_exit(struct efx_probe_data *probe_data)\n \t\tiounmap(probe_data->cxl->ctpio_cxl);\n \t\tcxl_unregister_region(probe_data->cxl->efx_region);\n \t} else {\n+\t\tcxl_dpa_free(probe_data->cxl->cxled);\n \t\tcxl_put_root_decoder(probe_data->cxl->cxlrd);\n \t}\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about type 2 CXL devices using host-managed memory, explaining that it should not be available to other uses and adding code to skip device-dax registration for such regions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nBy definition a type2 cxl device will use the host managed memory for\nspecific functionality, therefore it should not be available to other\nuses.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Davidlohr Bueso <daves@stgolabs.net>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n---\n drivers/cxl/core/region.c | 7 +++++++\n 1 file changed, 7 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 293e63dfef22..12df717cc881 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -4441,6 +4441,13 @@ static int cxl_region_probe(struct device *dev)\n \tif (rc)\n \t\treturn rc;\n \n+\t/*\n+\t * HDM-D[B] (device-memory) regions have accelerator specific usage.\n+\t * Skip device-dax registration.\n+\t */\n+\tif (cxlr->type == CXL_DECODER_DEVMEM)\n+\t\treturn 0;\n+\n \t/*\n \t * From this point on any path that changes the region's state away from\n \t * CXL_CONFIG_COMMIT is also responsible for releasing the driver.\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "Author acknowledged that the PIO buffer should be disabled when a CXL endpoint is removed, agreed to add code to handle this scenario.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nA PIO buffer is a region of device memory to which the driver can write a\npacket for TX, with the device handling the transmit doorbell without\nrequiring a DMA for getting the packet data, which helps reducing latency\nin certain exchanges. With CXL mem protocol this latency can be lowered\nfurther.\n\nWith a device supporting CXL and successfully initialised, use the cxl\nregion to map the memory range and use this mapping for PIO buffers.\n\nAdd the disabling of those CXL-based PIO buffers if the callback for\npotential cxl endpoint removal by the CXL code happens.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/net/ethernet/sfc/ef10.c       | 50 +++++++++++++++++++++++----\n drivers/net/ethernet/sfc/efx_cxl.c    | 33 ++++++++++++++----\n drivers/net/ethernet/sfc/net_driver.h |  2 ++\n drivers/net/ethernet/sfc/nic.h        |  3 ++\n 4 files changed, 75 insertions(+), 13 deletions(-)\n\ndiff --git a/drivers/net/ethernet/sfc/ef10.c b/drivers/net/ethernet/sfc/ef10.c\nindex fcec81f862ec..2bb6d3136c7c 100644\n--- a/drivers/net/ethernet/sfc/ef10.c\n+++ b/drivers/net/ethernet/sfc/ef10.c\n@@ -24,6 +24,7 @@\n #include <linux/wait.h>\n #include <linux/workqueue.h>\n #include <net/udp_tunnel.h>\n+#include \"efx_cxl.h\"\n \n /* Hardware control for EF10 architecture including 'Huntington'. */\n \n@@ -106,7 +107,7 @@ static int efx_ef10_get_vf_index(struct efx_nic *efx)\n \n static int efx_ef10_init_datapath_caps(struct efx_nic *efx)\n {\n-\tMCDI_DECLARE_BUF(outbuf, MC_CMD_GET_CAPABILITIES_V4_OUT_LEN);\n+\tMCDI_DECLARE_BUF(outbuf, MC_CMD_GET_CAPABILITIES_V7_OUT_LEN);\n \tstruct efx_ef10_nic_data *nic_data = efx->nic_data;\n \tsize_t outlen;\n \tint rc;\n@@ -177,6 +178,12 @@ static int efx_ef10_init_datapath_caps(struct efx_nic *efx)\n \t\t\t  efx->num_mac_stats);\n \t}\n \n+\tif (outlen < MC_CMD_GET_CAPABILITIES_V7_OUT_LEN)\n+\t\tnic_data->datapath_caps3 = 0;\n+\telse\n+\t\tnic_data->datapath_caps3 = MCDI_DWORD(outbuf,\n+\t\t\t\t\t\t      GET_CAPABILITIES_V7_OUT_FLAGS3);\n+\n \treturn 0;\n }\n \n@@ -919,6 +926,9 @@ static void efx_ef10_forget_old_piobufs(struct efx_nic *efx)\n static void efx_ef10_remove(struct efx_nic *efx)\n {\n \tstruct efx_ef10_nic_data *nic_data = efx->nic_data;\n+#ifdef CONFIG_SFC_CXL\n+\tstruct efx_probe_data *probe_data;\n+#endif\n \tint rc;\n \n #ifdef CONFIG_SFC_SRIOV\n@@ -949,7 +959,12 @@ static void efx_ef10_remove(struct efx_nic *efx)\n \n \tefx_mcdi_rx_free_indir_table(efx);\n \n+#ifdef CONFIG_SFC_CXL\n+\tprobe_data = container_of(efx, struct efx_probe_data, efx);\n+\tif (nic_data->wc_membase && !probe_data->cxl_pio_in_use)\n+#else\n \tif (nic_data->wc_membase)\n+#endif\n \t\tiounmap(nic_data->wc_membase);\n \n \trc = efx_mcdi_free_vis(efx);\n@@ -1140,6 +1155,9 @@ static int efx_ef10_dimension_resources(struct efx_nic *efx)\n \tunsigned int channel_vis, pio_write_vi_base, max_vis;\n \tstruct efx_ef10_nic_data *nic_data = efx->nic_data;\n \tunsigned int uc_mem_map_size, wc_mem_map_size;\n+#ifdef CONFIG_SFC_CXL\n+\tstruct efx_probe_data *probe_data;\n+#endif\n \tvoid __iomem *membase;\n \tint rc;\n \n@@ -1263,8 +1281,25 @@ static int efx_ef10_dimension_resources(struct efx_nic *efx)\n \tiounmap(efx->membase);\n \tefx->membase = membase;\n \n-\t/* Set up the WC mapping if needed */\n-\tif (wc_mem_map_size) {\n+\tif (!wc_mem_map_size)\n+\t\tgoto skip_pio;\n+\n+\t/* Set up the WC mapping */\n+\n+#ifdef CONFIG_SFC_CXL\n+\tprobe_data = container_of(efx, struct efx_probe_data, efx);\n+\tif ((nic_data->datapath_caps3 &\n+\t    (1 << MC_CMD_GET_CAPABILITIES_V7_OUT_CXL_CONFIG_ENABLE_LBN)) &&\n+\t    probe_data->cxl_pio_initialised) {\n+\t\t/* Using PIO through CXL mapping? */\n+\t\tnic_data->pio_write_base = probe_data->cxl->ctpio_cxl +\n+\t\t\t\t\t   (pio_write_vi_base * efx->vi_stride +\n+\t\t\t\t\t    ER_DZ_TX_PIOBUF - uc_mem_map_size);\n+\t\tprobe_data->cxl_pio_in_use = true;\n+\t} else\n+#endif\n+\t{\n+\t\t/* Using legacy PIO BAR mapping */\n \t\tnic_data->wc_membase = ioremap_wc(efx->membase_phys +\n \t\t\t\t\t\t  uc_mem_map_size,\n \t\t\t\t\t\t  wc_mem_map_size);\n@@ -1279,12 +1314,13 @@ static int efx_ef10_dimension_resources(struct efx_nic *efx)\n \t\t\tnic_data->wc_membase +\n \t\t\t(pio_write_vi_base * efx->vi_stride + ER_DZ_TX_PIOBUF -\n \t\t\t uc_mem_map_size);\n-\n-\t\trc = efx_ef10_link_piobufs(efx);\n-\t\tif (rc)\n-\t\t\tefx_ef10_free_piobufs(efx);\n \t}\n \n+\trc = efx_ef10_link_piobufs(efx);\n+\tif (rc)\n+\t\tefx_ef10_free_piobufs(efx);\n+\n+skip_pio:\n \tnetif_dbg(efx, probe, efx->net_dev,\n \t\t  \"memory BAR at %pa (virtual %p+%x UC, %p+%x WC)\\n\",\n \t\t  &efx->membase_phys, efx->membase, uc_mem_map_size,\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 4d5f3974e51d..c13e1f2bf7ea 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -11,6 +11,7 @@\n #include <cxl/pci.h>\n #include \"net_driver.h\"\n #include \"efx_cxl.h\"\n+#include \"efx.h\"\n \n #define EFX_CTPIO_BUFFER_SIZE\tSZ_256M\n \n@@ -138,14 +139,34 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\tcxl->efx_region = cxl_create_region(cxl->cxlrd, &cxl->cxled, 1);\n \t\tif (IS_ERR(cxl->efx_region)) {\n \t\t\tpci_err(pci_dev, \"CXL accel create region failed\");\n-\t\t\tcxl_put_root_decoder(cxl->cxlrd);\n-\t\t\tcxl_dpa_free(cxl->cxled);\n-\t\t\treturn PTR_ERR(cxl->efx_region);\n+\t\t\trc = PTR_ERR(cxl->efx_region);\n+\t\t\tgoto err_region;\n+\t\t}\n+\n+\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n+\t\tif (rc) {\n+\t\t\tpci_err(pci_dev, \"CXL getting regions params failed\");\n+\t\t\tgoto err_map;\n+\t\t}\n+\n+\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n+\t\tif (!cxl->ctpio_cxl) {\n+\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n+\t\t\trc = -ENOMEM;\n+\t\t\tgoto err_map;\n \t\t}\n \t}\n \n \tprobe_data->cxl = cxl;\n+\tprobe_data->cxl_pio_initialised = true;\n \treturn 0;\n+\n+err_map:\n+\tcxl_unregister_region(cxl->efx_region);\n+err_region:\n+\tcxl_put_root_decoder(cxl->cxlrd);\n+\tcxl_dpa_free(cxl->cxled);\n+\treturn rc;\n }\n \n void efx_cxl_exit(struct efx_probe_data *probe_data)\n@@ -153,9 +174,9 @@ void efx_cxl_exit(struct efx_probe_data *probe_data)\n \tif (!probe_data->cxl)\n \t\treturn;\n \n-\tif (probe_data->cxl->hdm_was_committed) {\n-\t\tiounmap(probe_data->cxl->ctpio_cxl);\n-\t} else {\n+\tiounmap(probe_data->cxl->ctpio_cxl);\n+\n+\tif (!probe_data->cxl->hdm_was_committed) {\n \t\tcxl_dpa_free(probe_data->cxl->cxled);\n \t\tcxl_put_root_decoder(probe_data->cxl->cxlrd);\n \t}\ndiff --git a/drivers/net/ethernet/sfc/net_driver.h b/drivers/net/ethernet/sfc/net_driver.h\nindex 3964b2c56609..bea4eecdf842 100644\n--- a/drivers/net/ethernet/sfc/net_driver.h\n+++ b/drivers/net/ethernet/sfc/net_driver.h\n@@ -1207,6 +1207,7 @@ struct efx_cxl;\n  * @efx: Efx NIC details\n  * @cxl: details of related cxl objects\n  * @cxl_pio_initialised: cxl initialization outcome.\n+ * @cxl_pio_in_use: PIO using CXL mapping\n  */\n struct efx_probe_data {\n \tstruct pci_dev *pci_dev;\n@@ -1214,6 +1215,7 @@ struct efx_probe_data {\n #ifdef CONFIG_SFC_CXL\n \tstruct efx_cxl *cxl;\n \tbool cxl_pio_initialised;\n+\tbool cxl_pio_in_use;\n #endif\n };\n \ndiff --git a/drivers/net/ethernet/sfc/nic.h b/drivers/net/ethernet/sfc/nic.h\nindex 9fa5c4c713ab..c87cc9214690 100644\n--- a/drivers/net/ethernet/sfc/nic.h\n+++ b/drivers/net/ethernet/sfc/nic.h\n@@ -152,6 +152,8 @@ enum {\n  *\t%MC_CMD_GET_CAPABILITIES response)\n  * @datapath_caps2: Further Capabilities of datapath firmware (FLAGS2 field of\n  * %MC_CMD_GET_CAPABILITIES response)\n+ * @datapath_caps3: Further Capabilities of datapath firmware (FLAGS3 field of\n+ * %MC_CMD_GET_CAPABILITIES response)\n  * @rx_dpcpu_fw_id: Firmware ID of the RxDPCPU\n  * @tx_dpcpu_fw_id: Firmware ID of the TxDPCPU\n  * @must_probe_vswitching: Flag: vswitching has yet to be setup after MC reboot\n@@ -186,6 +188,7 @@ struct efx_ef10_nic_data {\n \tbool must_check_datapath_caps;\n \tu32 datapath_caps;\n \tu32 datapath_caps2;\n+\tu32 datapath_caps3;\n \tunsigned int rx_dpcpu_fw_id;\n \tunsigned int tx_dpcpu_fw_id;\n \tbool must_probe_vswitching;\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "Author acknowledged that creating a CXL region requires userspace intervention through the cxl sysfs files, but believes accelerator drivers should be able to create such regions from kernel code. They plan to add this functionality and integrate it with current support for memory expanders.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical requirement",
                "planned to address the issue"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCreating a CXL region requires userspace intervention through the cxl\nsysfs files. Type2 support should allow accelerator drivers to create\nsuch cxl region from kernel code.\n\nAdding that functionality and integrating it with current support for\nmemory expanders.\n\nBased on https://lore.kernel.org/linux-cxl/168592159835.1948938.1647215579839222774.stgit@dwillia2-xfh.jf.intel.com/\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/cxl/core/region.c | 131 ++++++++++++++++++++++++++++++++++++--\n include/cxl/cxl.h         |   3 +\n 2 files changed, 127 insertions(+), 7 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 63c2aeb2ee1f..293e63dfef22 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2944,6 +2944,14 @@ cxl_find_region_by_name(struct cxl_root_decoder *cxlrd, const char *name)\n \treturn to_cxl_region(region_dev);\n }\n \n+static void drop_region(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n+\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n+\n+\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n+}\n+\n static ssize_t delete_region_store(struct device *dev,\n \t\t\t\t   struct device_attribute *attr,\n \t\t\t\t   const char *buf, size_t len)\n@@ -4047,14 +4055,12 @@ static int __construct_region(struct cxl_region *cxlr,\n \treturn 0;\n }\n \n-/* Establish an empty region covering the given HPA range */\n-static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n-\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n+static struct cxl_region *construct_region_begin(struct cxl_root_decoder *cxlrd,\n+\t\t\t\t\t\t struct cxl_endpoint_decoder *cxled)\n {\n \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n-\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n \tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n-\tint rc, part = READ_ONCE(cxled->part);\n+\tint part = READ_ONCE(cxled->part);\n \tstruct cxl_region *cxlr;\n \n \tdo {\n@@ -4063,13 +4069,26 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n \t\t\t\t       cxled->cxld.target_type);\n \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n \n-\tif (IS_ERR(cxlr)) {\n+\tif (IS_ERR(cxlr))\n \t\tdev_err(cxlmd->dev.parent,\n \t\t\t\"%s:%s: %s failed assign region: %ld\\n\",\n \t\t\tdev_name(&cxlmd->dev), dev_name(&cxled->cxld.dev),\n \t\t\t__func__, PTR_ERR(cxlr));\n+\n+\treturn cxlr;\n+}\n+\n+/* Establish an empty region covering the given HPA range */\n+static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n+\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n+{\n+\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n+\tstruct cxl_region *cxlr;\n+\tint rc;\n+\n+\tcxlr = construct_region_begin(cxlrd, cxled);\n+\tif (IS_ERR(cxlr))\n \t\treturn cxlr;\n-\t}\n \n \trc = __construct_region(cxlr, cxlrd, cxled);\n \tif (rc) {\n@@ -4080,6 +4099,104 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n \treturn cxlr;\n }\n \n+DEFINE_FREE(cxl_region_drop, struct cxl_region *, if (_T) drop_region(_T))\n+\n+static struct cxl_region *\n+__construct_new_region(struct cxl_root_decoder *cxlrd,\n+\t\t       struct cxl_endpoint_decoder **cxled, int ways)\n+{\n+\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled[0]);\n+\tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n+\tstruct cxl_region_params *p;\n+\tresource_size_t size = 0;\n+\tint rc, i;\n+\n+\tstruct cxl_region *cxlr __free(cxl_region_drop) =\n+\t\tconstruct_region_begin(cxlrd, cxled[0]);\n+\tif (IS_ERR(cxlr))\n+\t\treturn cxlr;\n+\n+\tguard(rwsem_write)(&cxl_rwsem.region);\n+\n+\t/*\n+\t * Sanity check. This should not happen with an accel driver handling\n+\t * the region creation.\n+\t */\n+\tp = &cxlr->params;\n+\tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE) {\n+\t\tdev_err(cxlmd->dev.parent,\n+\t\t\t\"%s:%s: %s  unexpected region state\\n\",\n+\t\t\tdev_name(&cxlmd->dev), dev_name(&cxled[0]->cxld.dev),\n+\t\t\t__func__);\n+\t\treturn ERR_PTR(-EBUSY);\n+\t}\n+\n+\trc = set_interleave_ways(cxlr, ways);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\trc = set_interleave_granularity(cxlr, cxld->interleave_granularity);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\tscoped_guard(rwsem_read, &cxl_rwsem.dpa) {\n+\t\tfor (i = 0; i < ways; i++) {\n+\t\t\tif (!cxled[i]->dpa_res)\n+\t\t\t\treturn ERR_PTR(-EINVAL);\n+\t\t\tsize += resource_size(cxled[i]->dpa_res);\n+\t\t}\n+\n+\t\trc = alloc_hpa(cxlr, size);\n+\t\tif (rc)\n+\t\t\treturn ERR_PTR(rc);\n+\n+\t\tfor (i = 0; i < ways; i++) {\n+\t\t\trc = cxl_region_attach(cxlr, cxled[i], 0);\n+\t\t\tif (rc)\n+\t\t\t\treturn ERR_PTR(rc);\n+\t\t}\n+\t}\n+\n+\trc = cxl_region_decode_commit(cxlr);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\tp->state = CXL_CONFIG_COMMIT;\n+\n+\treturn no_free_ptr(cxlr);\n+}\n+\n+/**\n+ * cxl_create_region - Establish a region given an endpoint decoder\n+ * @cxlrd: root decoder to allocate HPA\n+ * @cxled: endpoint decoders with reserved DPA capacity\n+ * @ways: interleave ways required\n+ *\n+ * Returns a fully formed region in the commit state and attached to the\n+ * cxl_region driver.\n+ */\n+struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n+\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n+\t\t\t\t     int ways)\n+{\n+\tstruct cxl_region *cxlr;\n+\n+\tmutex_lock(&cxlrd->range_lock);\n+\tcxlr = __construct_new_region(cxlrd, cxled, ways);\n+\tmutex_unlock(&cxlrd->range_lock);\n+\tif (IS_ERR(cxlr))\n+\t\treturn cxlr;\n+\n+\tif (device_attach(&cxlr->dev) <= 0) {\n+\t\tdev_err(&cxlr->dev, \"failed to create region\\n\");\n+\t\tdrop_region(cxlr);\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\treturn cxlr;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_create_region, \"CXL\");\n+\n static struct cxl_region *\n cxl_find_region_by_range(struct cxl_root_decoder *cxlrd, struct range *hpa)\n {\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 4802371db00e..50acbd13bcf8 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -281,4 +281,7 @@ struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n \t\t\t\t\t     enum cxl_partition_mode mode,\n \t\t\t\t\t     resource_size_t alloc);\n int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n+struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n+\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n+\t\t\t\t     int ways);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested moving the check for cxl_region_can_probe() to be the first thing in the function, which would avoid acquiring a lock later on and requested to keep their Reviewed-by tag.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "minor optimization"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Minor nit: Should probably move this to be the first thing in the function. It would save\nhaving to acquire a lock in cxl_region_can_probe() above. Keep my reviewed-by either way,\nit's really just a minor optimization.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "The reviewer questioned the complexity of the code and suggested simplification by removing the outer loop in cxl_get_hpa_freespace(), as ctx->host_bridges is only set to one host bridge at that point, and adding a comment to explain the purpose of cxlsd->target[] entries.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "complexity",
                "simplification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\nwhat the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\nbridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\npoint, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\nbe a struct device * const.\n\nMaybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\nI'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that interleave_ways is hardcoded to 1, which may not be the intended behavior, and suggested either removing this portion of the function or adding a doc comment explaining its purpose.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Mentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\nsomething). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\nI'm not sure of what the fix is to get the intended behavior.\n\nIt may be worth getting rid of the interleave_ways portion of this function and\nadd it later when someone needs it. You could also explain it's hard coded to 1/unused\nin the doc comment if you know of an immediate need for it.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "analysis_source": "heuristic"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested updating documentation to specify that this attribute only applies to type 3 regions, and expressed uncertainty about whether it's worth the effort.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I haven't read the ABI docs, but would it be worthwhile to update the documentation for this attribute\nto mention it only makes type 3 regions? I'm flip-flopping on whether it's worth the trouble but thought\nI should mention it.\n\nEither way:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer pointed out that the type of the '@val' parameter in a sysfs function should remain an unsigned int, as it is passed an unsigned int and was originally coded to expect this.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested change"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "@val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\nfunction was originally coded with that in mind (same with @save below). With that cleaned up:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "analysis_source": "heuristic"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "The reviewer pointed out that calling drop_region() with an error pointer returned by construct_region_begin() would lead to a garbage pointer dereference, and suggested changing the if condition to 'if (!IS_ERR_OR_NULL(_T)) drop_region(_T)'",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This needs to be \"if (!IS_ERR_OR_NULL(_T) drop_region(_T)\". If construct_region_begin() returns an\nerror pointer, drop_region() will be called with it as of now leading to a garbage pointer deref.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the position parameter in a function is hardcoded to 0, and suggested setting it to 'i' instead, referencing an earlier patch where interleaving functionality was added but appears unused.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Position parameter is hardcoded to 0. It should be set to i, right? This kind of goes back to my\nissues in patch 12/22; the interleaving functionality is there but it looks unused.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested folding the cxl_dev_state_init() call into this function, but is fine either way if it was previously decided to keep them separate",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEUTRAL"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: Having a second function to do the init seems overkill here, especially since cxl_dev_state_init() isn't called outside this\nfunction. I'd fold it into this function instead, but I'm fine with it either way (especially if you were told otherwise before).\n\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "The reviewer questioned whether the patch's method of checking if a decoder is committed is correct, citing the documentation for cxl_port which states that @hdm_end is just the last allocated decoder. The reviewer suggested either using a register read or another approach to determine the commit state.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Is this the way you're supposed to check if a decoder is committed? The doc comment for @hdm_end in\nstruct cxl_port says it's just the last allocated decoder. If allocated decoders are always committed then\nI'm fine with this, otherwise I think you'd want to a register read or something to find the commit state.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested reducing the length of the cover letter by removing change logs for revisions older than 3, and adding a lore link for even older revisions if desired.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This cover letter is really long, I'd remove the change logs for anything more\nthan 3 revisions back (assuming a v24 is needed). After that you could leave\na lore link for older revisions if you want, but it's not needed imo.\nAlso, feel free to add my Reviewed-by for anything I didn't leave a comment on\n(felt I should cut down on the mail).\n\nThanks,\nBen",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "analysis_source": "heuristic"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer noted that the error inside sfc should not be fatal for cxl sfc initialization and suggested fallback to other cxl initialization possibility; also raised question about handling multiple hdm present scenarios",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "further thinking"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Ben,\n\n\nYes, I think you are right. This works in my tests and it is safe \nbecause I check the region does exist before using it. But the error \ninside sfc should then not be fatal for cxl sfc initialization and \nfallback to the other cxl initialization possibility.\n\n\nIf I add the check for the decoder state, I guess I can keep the \nfunction names. If I rely on the region being there, I should change \nthem. I will think about it.\n\n\nThis also brings the question of what is more than one hdm present. This \nis not needed in my use case and likely this is also true for other \ncoming Type2 devices, but it does also require further thinking.\n\n\nThank you!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price noted that the patch may contain a reference leak and suggested further investigation.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential reference leak"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This review was generated by kreview-0811365ff2. \r\n\r\nThis is not an automated email, the reviewer though this report\r\nlooked valid enough to consider discussion.\r\n\r\n----\r\n\r\nCursory browse, this does look like a legitimate reference leak.\r\n\r\n~Gregory\r\n\r\n----",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "The reviewer noted that in the cxl_find_free_decoder function, a device reference is obtained by device_find_child and incremented, but then get_device is called on the same device without releasing the original reference, resulting in a permanent elevation of the endpoint decoder device refcount. The reviewer asked if this leaks the device reference.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "leak",
                "device reference"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Does this leak the device reference obtained by device_find_child()\r\nin cxl_find_free_decoder()?\r\n\r\ndevice_find_child() increments the device refcount (reference A).\r\nThen cxl_dpa_alloc() -> __cxl_dpa_reserve() calls get_device() on\r\nthe same device (reference B).\r\n\r\nOn the success path, no_free_ptr() inhibits the put_cxled cleanup,\r\nso reference A is transferred to the caller.  The matching cleanup\r\nfunction cxl_dpa_free() calls __cxl_dpa_release(), which drops\r\nreference B via put_device(), but reference A is never released:\r\n\r\n    cxl_dpa_free()\r\n      -> devm_cxl_dpa_release()\r\n        -> __cxl_dpa_release()\r\n          -> put_device()     /* drops reference B only */\r\n\r\n    /* reference A from device_find_child() is still held */\r\n\r\nThe only caller in this series (sfc efx_cxl_init/efx_cxl_exit)\r\nnever calls put_device() on the returned cxled either, so the\r\nendpoint decoder device refcount remains permanently elevated.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer noted that the patch introduces a potential ordering issue between CXL and reclaim paths, requesting further investigation.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "ordering question seems legit"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This review was generated by kreview-0811365ff2. \r\n\r\nThis is not an automated email, the reviewer though this report\r\nlooked valid enough to consider discussion.\r\n\r\n----\r\nsfc code so won't speak to the internals, but ordering question seems\r\nlegit.\r\n\r\n~Gregory\r\n----\r\n\r\nOn Sat, Feb 01, 2026, Alejandro Lucero wrote:",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "The reviewer noted that cxl_dpa_free() is called while the endpoint decoder is still attached to the region, leading to a -EBUSY return without freeing the DPA. They suggested calling cxl_unregister_region() before cxl_dpa_free() and cxl_put_root_decoder() in the else branch, matching the reverse order of allocation in efx_cxl_init().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "ordering issue",
                "potential bug"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "In the non-committed path, cxl_dpa_free() is called while the\r\nendpoint decoder is still attached to the region.  Looking at\r\ncxl_dpa_free() in drivers/cxl/core/hdm.c:\r\n\r\n    if (cxled->cxld.region) {\r\n        dev_dbg(dev, \"decoder assigned to: %s\\n\",\r\n            dev_name(&cxled->cxld.region->dev));\r\n        return -EBUSY;\r\n    }\r\n\r\nSince cxl_unregister_region() has not run yet, cxled->cxld.region\r\nis still set, and cxl_dpa_free() returns -EBUSY without freeing\r\nthe DPA.  The return value is not checked.\r\n\r\nShould cxl_unregister_region() be called before cxl_dpa_free()\r\nand cxl_put_root_decoder() in the else branch, matching the\r\nreverse order of allocation in efx_cxl_init()?\r\n\r\nThe cover letter notes that v23 expects committed decoders as the\r\nprimary flow, and uncommitted decoder support is deferred to\r\nfollow-up work, so this else branch may not be reachable in\r\npractice today.  Still worth fixing the ordering now so it\r\ndoesn't bite when the uncommitted path is enabled later.\r\n\r\nThis issue is not fixed by the remaining commits in the series\r\n(through 10fe989f9e85).",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price expressed uncertainty about the code and its purpose, stating he is unfamiliar with it but found the question posed to be reasonable.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "lack of familiarity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This review was generated by kreview-0811365ff2. \r\n\r\nThis is not an automated email, the reviewer though this report\r\nlooked valid enough to consider discussion.\r\n\r\n----\r\nI am completely unfamiliar with this code, but the question it poses\r\nat least seems reasonable.\r\n\r\n~Gregory\r\n----\r\n\r\nOn Sat, Feb 01, 2026, Alejandro Lucero wrote:",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "The reviewer noted that the CXL path does not set nic_data->pio_write_vi_base, which would cause link PIO buffers to be incorrectly assigned to VI instances when using CXL.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The CXL path sets nic_data->pio_write_base but does not set\r\nnic_data->pio_write_vi_base, while the legacy path does:\r\n\r\n    nic_data->pio_write_vi_base = pio_write_vi_base;\r\n\r\nSince nic_data is kzalloc'd, pio_write_vi_base stays at 0 in the CXL\r\npath.  efx_ef10_link_piobufs() then uses nic_data->pio_write_vi_base\r\nto issue MC_CMD_LINK_PIOBUF commands:\r\n\r\n    MCDI_SET_DWORD(inbuf, LINK_PIOBUF_IN_TXQ_INSTANCE,\r\n                   nic_data->pio_write_vi_base + index);\r\n\r\nand also for the special-case check:\r\n\r\n    if (tx_queue->queue == nic_data->pio_write_vi_base) {\r\n\r\nWouldn't this link PIO buffers to incorrect VI instances when using\r\nCXL, since the local variable pio_write_vi_base has the correct\r\nnon-zero value but the struct field was never updated?",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer Alejandro Palau noted that the patch's removal of the reference from cxl_find_free_decoder when alloc succeeds and another get happens is correct, but not trivial to understand; he added a comment suggesting to put_device(&cxled->cxld.dev) in this case, which he will apply.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This is right, and it took a good bunch of time to debug it. Was it \ndetected by an automatic tool?\n\n\nAnyways, I had one patch for solving this which I forgot to apply to v23 \nsince the focus there was to mainly support the auto-discover region \nwhich does not go through this path:\n\n+ /* removing the reference from cxl_find_free_decoder ...\n+ * when alloc succeds another get happened\n+ */\n+\n+ put_device(&cxled->cxld.dev);\n\n\nI added that comment because it is not trivial to know if it is right to \ndo the put while you get a new reference to the device. I will apply it.\n\nThanks!",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-16",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer Alejandro Palau expressed confusion about the original implementation of cxl support and requested it be changed in version 24.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "confusion",
                "requested change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Ben,\n\n\nI do not remember why this was done this way. Maybe some initial need \nwhich disappeared later.\n\nI can not see a reason now, so I will do so in v24.\n\n\nThank you!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer noted that the cxl_accel_unwind function is not properly handling the case where the accelerator is already in a failed state, and requested additional error checking to ensure the function does not attempt to access invalid memory.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "additional error checking",
                "invalid memory access"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure.\n\nThanks!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Alejandro Palau noted that interleaving support was previously removed due to its low likelihood of use in Type2 and current scenarios, but suggested reinstating it as a trivial addition.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "trivial_to_do_so",
                "previous_discussions"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Ben,\n\n\nI do remember this one.\n\n\nDan's original patches had this support for interleaving, then I removed \nit as the case for Type2 and interleaving is quite unlikely, at least \nright now and likely in the near future. But I was told why do not \nsupport it as it was trivial to do so. FWIW, If I think only about the \nuse case coming with the patchset, I agree with you, but because those \nprevious discussions, I think I have to leave it.\n\n\nThank you",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer noted that CXL initialization fails, resulting in a struct being released prematurely.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "patch needs revision"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, I do not think that is necessary. The CXL initialization fails, and \nthe result is the modified struct will be released sooner or later.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer suggested replacing kstrtouint with kstrtoint to parse cxl_region_params fields defined as int, which are used for values that could be negative.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Good catch. I wonder if I should just change the way the value is \nobtained, using kstrtoint instead of kstrtouint, as those values are \nused for cxl_region_params fields defined as int. In other words, it \nseems doing that simpler than changing all the other places you mention \nand the structs involved. I can not see a reason for using unsigned int \nso I think I will follow that approach. Tell me if you think otherwise.\n\n\nThank you",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer Alejandro Palau acknowledged the issue and agreed to make the necessary changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That's true. I will fix it.\n\n\nThank you!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer Alejandro Palau noted that the patch is acceptable and agreed to implement the requested change.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It makes sense. I'll do it.\n\nThanks",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "The reviewer questioned the use of unsigned int to represent interleave granularity/ways, suggesting that negative values would be nonsensical, but agreed with the patch's suggestion to change it to a positive value.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK",
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "If I had to guess unsigned int was used because a negative interleave granularity/ways makes no sense. I think your suggestion is fine though since no one\nin their right mind would give anything but a (relatively) small and positive value for these.\n\nThanks,\nBen",
              "reply_to": "Alejandro Palau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested modifying the patch to address an unused parameter in cxl_get_hpa_freespace() and noted that the decoder position fix should be done separately in a different patch (19/22)",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm fine with that, but I would at least do the fix with the decoder position in 19/22 and make a note that the\ninterleave_ways parameter in cxl_get_hpa_freespace() below is currently unused (unless I'm misunderstanding\nthe endpoint->host_bridge member).\n\nThat way, the support is mostly there and just requires a small, previously noted, addition to enable. If you're\nfine with that then feel free to add my Reviewed-by after implementing in v24.\n\nThanks,\nBen",
              "reply_to": "Alejandro Palau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-19",
              "analysis_source": "heuristic"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer Dave Jiang suggested simplifying the cxl code by saving one level of indentation in the if statement, specifically by removing the unnecessary check for cxled before returning 0.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "if (!cxl->cxled)\n\treturn 0;\n\nShould save you a level of indent.\n\nDJ",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-19",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer noted that the patchset must handle both module exit paths, requiring code changes to support this.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "patchset needs to support both cases",
                "code needs to deal with both module exit paths"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Gregory,\n\n\nYes, it makes sense and pointing out to those changes introduced in v22 \nand mainly in v23.\n\nI'll fix it.\n\n\nRegarding the below comment, which if I am not wrong comes from kreview, \nI think the patchset needs to support both cases and therefore the code \nneeds to deal with both module exit paths.\n\n\nThank you",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "reviewer noted that the patch's functionality has only been tested on a single Virtual Interface (VI), which explains why the issue hasn't occurred yet, but emphasized that it needs to be fixed",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "issue",
                "needs to be fixed"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, and again, it makes sense. We have only tried with one VI, so that \nexplains why we have not suffered the issue. But it needs to be fixed.\n\n\nThanks!",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer pointed out that the current patch does not handle the case where CXL is disabled, and requested that an 'else' branch be added to handle this scenario.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "missing else branch"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, but subsequent patches add the else branch ...\n\n\nThanks",
              "reply_to": "Dave Jiang",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer Dave Jiang suggested checking if ctx->cxlrd is equal to cxlrd, and if not, releasing the device associated with ctx->cxlrd and getting a new reference to the device associated with cxlrd before updating ctx->cxlrd.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Is there any chance that ctx->cxlrd == cxlrd? Maybe you can do:\n\nif (ctx->cxlrd && ctx->cxlrd != cxlrd) {\n\tput_device(cxlrd_dev(ctx->cxlrd));\n\tget_device(cxlrd_dev(cxlrd));\n\tctx->cxlrd = cxlrd;\n}\nctx->max_hpa = max;\n\nDJ",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price noted that during testing, he observed double-releases when aggressively loading and unloading certain drivers, which led him to suggest adding a function for destroying cxl regions to prevent this issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "kreview suggested you probably want this:\n\nvoid cxl_destroy_region(struct cxl_region *cxlr)\n{\n\tstruct cxl_port *port = cxlrd_to_port(cxlr->cxlrd);\n\n\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n}\nEXPORT_SYMBOL_NS_GPL(cxl_destroy_region, \"CXL\");\n\n\nDuring testing I experienced some double-releases when doing aggressive\nloads and unloads of some drivers.  This was one of the fixes.\n\n~Gregory",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Jeff Layton",
      "primary_email": "jlayton@kernel.org",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joanne Koong",
      "primary_email": "joannelkoong@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v1 03/11] io_uring/kbuf: add support for kernel-managed buffer rings",
          "message_id": "CAJnrk1Zr=9RMGpNXpe6=fSDkG2uVijB9qa1vENHpQozB3iPQtg@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1Zr=9RMGpNXpe6=fSDkG2uVijB9qa1vENHpQozB3iPQtg@mail.gmail.com/",
          "date": "2026-02-21T02:14:40Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the complexity of the io_register_pbuf_ring() function, refactored it into three separate helper functions: io_copy_and_validate_buf_reg(), io_alloc_new_buffer_list(), and io_setup_pbuf_ring(). The new code is a preparatory change for upcoming kernel-managed buffer ring support. No fix is planned in this response.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "refactoring",
                "preparatory change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Refactor the logic in io_register_pbuf_ring() into generic helpers:\n- io_copy_and_validate_buf_reg(): Copy out user arg and validate user\n  arg and buffer registration parameters\n- io_alloc_new_buffer_list(): Allocate and initialize a new buffer\n  list for the given buffer group ID\n- io_setup_pbuf_ring(): Sets up the physical buffer ring region and\n  handles memory mapping for provided buffer rings\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport which will need to reuse some of these helpers.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c | 129 +++++++++++++++++++++++++++++++-----------------\n 1 file changed, 85 insertions(+), 44 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 67d4fe576473..850b836f32ee 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -596,55 +596,73 @@ int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags)\n \treturn IOU_COMPLETE;\n }\n \n-int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+static int io_copy_and_validate_buf_reg(const void __user *arg,\n+\t\t\t\t\tstruct io_uring_buf_reg *reg,\n+\t\t\t\t\tunsigned int permitted_flags)\n {\n-\tstruct io_uring_buf_reg reg;\n-\tstruct io_buffer_list *bl;\n-\tstruct io_uring_region_desc rd;\n-\tstruct io_uring_buf_ring *br;\n-\tunsigned long mmap_offset;\n-\tunsigned long ring_size;\n-\tint ret;\n-\n-\tlockdep_assert_held(&ctx->uring_lock);\n-\n-\tif (copy_from_user(&reg, arg, sizeof(reg)))\n+\tif (copy_from_user(reg, arg, sizeof(*reg)))\n \t\treturn -EFAULT;\n-\tif (!mem_is_zero(reg.resv, sizeof(reg.resv)))\n+\n+\tif (!mem_is_zero(reg->resv, sizeof(reg->resv)))\n \t\treturn -EINVAL;\n-\tif (reg.flags & ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC))\n+\tif (reg->flags & ~permitted_flags)\n \t\treturn -EINVAL;\n-\tif (!is_power_of_2(reg.ring_entries))\n+\tif (!is_power_of_2(reg->ring_entries))\n \t\treturn -EINVAL;\n \t/* cannot disambiguate full vs empty due to head/tail size */\n-\tif (reg.ring_entries >= 65536)\n+\tif (reg->ring_entries >= 65536)\n \t\treturn -EINVAL;\n+\treturn 0;\n+}\n \n-\tbl = io_buffer_get_list(ctx, reg.bgid);\n-\tif (bl) {\n+static struct io_buffer_list *\n+io_alloc_new_buffer_list(struct io_ring_ctx *ctx,\n+\t\t\t const struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_buffer_list *list;\n+\n+\tlist = io_buffer_get_list(ctx, reg->bgid);\n+\tif (list) {\n \t\t/* if mapped buffer ring OR classic exists, don't allow */\n-\t\tif (bl->flags & IOBL_BUF_RING || !list_empty(&bl->buf_list))\n-\t\t\treturn -EEXIST;\n-\t\tio_destroy_bl(ctx, bl);\n+\t\tif (list->flags & IOBL_BUF_RING || !list_empty(&list->buf_list))\n+\t\t\treturn ERR_PTR(-EEXIST);\n+\t\tio_destroy_bl(ctx, list);\n \t}\n \n-\tbl = kzalloc(sizeof(*bl), GFP_KERNEL_ACCOUNT);\n-\tif (!bl)\n-\t\treturn -ENOMEM;\n+\tlist = kzalloc(sizeof(*list), GFP_KERNEL_ACCOUNT);\n+\tif (!list)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tlist->nr_entries = reg->ring_entries;\n+\tlist->mask = reg->ring_entries - 1;\n+\tlist->flags = IOBL_BUF_RING;\n+\n+\treturn list;\n+}\n+\n+static int io_setup_pbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t      const struct io_uring_buf_reg *reg,\n+\t\t\t      struct io_buffer_list *bl)\n+{\n+\tstruct io_uring_region_desc rd;\n+\tunsigned long mmap_offset;\n+\tunsigned long ring_size;\n+\tint ret;\n \n-\tmmap_offset = (unsigned long)reg.bgid << IORING_OFF_PBUF_SHIFT;\n-\tring_size = flex_array_size(br, bufs, reg.ring_entries);\n+\tmmap_offset = (unsigned long)reg->bgid << IORING_OFF_PBUF_SHIFT;\n+\tring_size = flex_array_size(bl->buf_ring, bufs, reg->ring_entries);\n \n \tmemset(&rd, 0, sizeof(rd));\n \trd.size = PAGE_ALIGN(ring_size);\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP)) {\n-\t\trd.user_addr = reg.ring_addr;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP)) {\n+\t\trd.user_addr = reg->ring_addr;\n \t\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n \t}\n+\n \tret = io_create_region(ctx, &bl->region, &rd, mmap_offset);\n \tif (ret)\n-\t\tgoto fail;\n-\tbr = io_region_get_ptr(&bl->region);\n+\t\treturn ret;\n+\tbl->buf_ring = io_region_get_ptr(&bl->region);\n \n #ifdef SHM_COLOUR\n \t/*\n@@ -656,25 +674,48 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t * should use IOU_PBUF_RING_MMAP instead, and liburing will handle\n \t * this transparently.\n \t */\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP) &&\n-\t    ((reg.ring_addr | (unsigned long)br) & (SHM_COLOUR - 1))) {\n-\t\tret = -EINVAL;\n-\t\tgoto fail;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP) &&\n+\t    ((reg->ring_addr | (unsigned long)bl->buf_ring) &\n+\t     (SHM_COLOUR - 1))) {\n+\t\tio_free_region(ctx->user, &bl->region);\n+\t\treturn -EINVAL;\n \t}\n #endif\n \n-\tbl->nr_entries = reg.ring_entries;\n-\tbl->mask = reg.ring_entries - 1;\n-\tbl->flags |= IOBL_BUF_RING;\n-\tbl->buf_ring = br;\n-\tif (reg.flags & IOU_PBUF_RING_INC)\n+\tif (reg->flags & IOU_PBUF_RING_INC)\n \t\tbl->flags |= IOBL_INC;\n+\n+\treturn 0;\n+}\n+\n+int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tunsigned int permitted_flags;\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tpermitted_flags = IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC;\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, permitted_flags);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_pbuf_ring(ctx, &reg, bl);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n \tret = io_buffer_add_list(ctx, bl, reg.bgid);\n-\tif (!ret)\n-\t\treturn 0;\n-fail:\n-\tio_free_region(ctx->user, &bl->region);\n-\tkfree(bl);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n \treturn ret;\n }\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the naming of io_unregister_pbuf_ring() function, which will be used for unregistering both provided buffer rings and kernel-managed buffer rings. The author agreed to rename it to io_unregister_buf_ring(), as this is a preparatory change for upcoming kernel-managed buffer ring support.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "preparatory change",
                "rename function"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Use the more generic name io_unregister_buf_ring() as this function will\nbe used for unregistering both provided buffer rings and kernel-managed\nbuffer rings.\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c     | 2 +-\n io_uring/kbuf.h     | 2 +-\n io_uring/register.c | 2 +-\n 3 files changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 850b836f32ee..aa9b70b72db4 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -719,7 +719,7 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \treturn ret;\n }\n \n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n {\n \tstruct io_uring_buf_reg reg;\n \tstruct io_buffer_list *bl;\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex bf15e26520d3..40b44f4fdb15 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -74,7 +74,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags);\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 594b1f2ce875..0882cb34f851 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -841,7 +841,7 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-\t\tret = io_unregister_pbuf_ring(ctx, arg);\n+\t\tret = io_unregister_buf_ring(ctx, arg);\n \t\tbreak;\n \tcase IORING_REGISTER_SYNC_CANCEL:\n \t\tret = -EINVAL;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the need for kernel-managed buffer rings to have non-zero and page-aligned buf_size, explained that this is validated in io_copy_and_validate_buf_reg(), and confirmed that this check will prevent incorrect usage.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for kernel-managed buffer rings (kmbuf rings), which allow\nthe kernel to allocate and manage the backing buffers for a buffer\nring, rather than requiring the application to provide and manage them.\n\nThis introduces two new registration opcodes:\n- IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring\n- IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring\n\nThe existing io_uring_buf_reg structure is extended with a union to\nsupport both application-provided buffer rings (pbuf) and kernel-managed\nbuffer rings (kmbuf):\n- For pbuf rings: ring_addr specifies the user-provided ring address\n- For kmbuf rings: buf_size specifies the size of each buffer. buf_size\n  must be non-zero and page-aligned.\n\nThe implementation follows the same pattern as pbuf ring registration,\nreusing the validation and buffer list allocation helpers introduced in\nearlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as\nkernel-managed for appropriate handling in the I/O path.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  15 ++++-\n io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-\n io_uring/kbuf.h               |   7 ++-\n io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++\n io_uring/memmap.h             |   4 ++\n io_uring/register.c           |   7 +++\n 6 files changed, 219 insertions(+), 6 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex fc473af6feb4..a0889c1744bd 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -715,6 +715,10 @@ enum io_uring_register_op {\n \t/* register bpf filtering programs */\n \tIORING_REGISTER_BPF_FILTER\t\t= 37,\n \n+\t/* register/unregister kernel-managed ring buffer group */\n+\tIORING_REGISTER_KMBUF_RING\t\t= 38,\n+\tIORING_UNREGISTER_KMBUF_RING\t\t= 39,\n+\n \t/* this goes last */\n \tIORING_REGISTER_LAST,\n \n@@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {\n \tIOU_PBUF_RING_INC\t= 2,\n };\n \n-/* argument for IORING_(UN)REGISTER_PBUF_RING */\n+/* argument for IORING_(UN)REGISTER_PBUF_RING and\n+ * IORING_(UN)REGISTER_KMBUF_RING\n+ */\n struct io_uring_buf_reg {\n-\t__u64\tring_addr;\n+\tunion {\n+\t\t/* used for pbuf rings */\n+\t\t__u64\tring_addr;\n+\t\t/* used for kmbuf rings */\n+\t\t__u32   buf_size;\n+\t};\n \t__u32\tring_entries;\n \t__u16\tbgid;\n \t__u16\tflags;\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex aa9b70b72db4..9bc36451d083 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -427,10 +427,13 @@ static int io_remove_buffers_legacy(struct io_ring_ctx *ctx,\n \n static void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)\n {\n-\tif (bl->flags & IOBL_BUF_RING)\n+\tif (bl->flags & IOBL_BUF_RING) {\n \t\tio_free_region(ctx->user, &bl->region);\n-\telse\n+\t\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\t\tkfree(bl->buf_ring);\n+\t} else {\n \t\tio_remove_buffers_legacy(ctx, bl, -1U);\n+\t}\n \n \tkfree(bl);\n }\n@@ -779,3 +782,77 @@ struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n \t\treturn NULL;\n \treturn &bl->region;\n }\n+\n+static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_buffer_list *bl,\n+\t\t\t       struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_uring_buf_ring *ring;\n+\tunsigned long ring_size;\n+\tvoid *buf_region;\n+\tunsigned int i;\n+\tint ret;\n+\n+\t/* allocate pages for the ring structure */\n+\tring_size = flex_array_size(ring, bufs, bl->nr_entries);\n+\tring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);\n+\tif (!ring)\n+\t\treturn -ENOMEM;\n+\n+\tret = io_create_region_multi_buf(ctx, &bl->region, bl->nr_entries,\n+\t\t\t\t\t reg->buf_size);\n+\tif (ret) {\n+\t\tkfree(ring);\n+\t\treturn ret;\n+\t}\n+\n+\t/* initialize ring buf entries to point to the buffers */\n+\tbuf_region = bl->region.ptr;\n+\tfor (i = 0; i < bl->nr_entries; i++) {\n+\t\tstruct io_uring_buf *buf = &ring->bufs[i];\n+\n+\t\tbuf->addr = (u64)(uintptr_t)buf_region;\n+\t\tbuf->len = reg->buf_size;\n+\t\tbuf->bid = i;\n+\n+\t\tbuf_region += reg->buf_size;\n+\t}\n+\tring->tail = bl->nr_entries;\n+\n+\tbl->buf_ring = ring;\n+\tbl->flags |= IOBL_KERNEL_MANAGED;\n+\n+\treturn 0;\n+}\n+\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, 0);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tif (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_kmbuf_ring(ctx, bl, &reg);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n+\tret = io_buffer_add_list(ctx, bl, reg.bgid);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n+\treturn ret;\n+}\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 40b44f4fdb15..62c80a1ebf03 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -7,9 +7,11 @@\n \n enum {\n \t/* ring mapped provided buffers */\n-\tIOBL_BUF_RING\t= 1,\n+\tIOBL_BUF_RING\t\t= 1,\n \t/* buffers are consumed incrementally rather than always fully */\n-\tIOBL_INC\t= 2,\n+\tIOBL_INC\t\t= 2,\n+\t/* buffers are kernel managed */\n+\tIOBL_KERNEL_MANAGED\t= 4,\n };\n \n struct io_buffer_list {\n@@ -74,6 +76,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 89f56609e50a..8d37e93c0433 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -15,6 +15,28 @@\n #include \"rsrc.h\"\n #include \"zcrx.h\"\n \n+static void release_multi_buf_pages(struct page **pages, unsigned long nr_pages)\n+{\n+\tstruct page *page;\n+\tunsigned int nr, i = 0;\n+\n+\twhile (nr_pages) {\n+\t\tpage = pages[i];\n+\n+\t\tif (!page || WARN_ON_ONCE(page != compound_head(page)))\n+\t\t\treturn;\n+\n+\t\tnr = compound_nr(page);\n+\t\tput_page(page);\n+\n+\t\tif (WARN_ON_ONCE(nr > nr_pages))\n+\t\t\treturn;\n+\n+\t\ti += nr;\n+\t\tnr_pages -= nr;\n+\t}\n+}\n+\n static bool io_mem_alloc_compound(struct page **pages, int nr_pages,\n \t\t\t\t  size_t size, gfp_t gfp)\n {\n@@ -86,6 +108,8 @@ enum {\n \tIO_REGION_F_USER_PROVIDED\t\t= 2,\n \t/* only the first page in the array is ref'ed */\n \tIO_REGION_F_SINGLE_REF\t\t\t= 4,\n+\t/* pages in the array belong to multiple discrete allocations */\n+\tIO_REGION_F_MULTI_BUF\t\t\t= 8,\n };\n \n void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n@@ -98,6 +122,8 @@ void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n \n \t\tif (mr->flags & IO_REGION_F_USER_PROVIDED)\n \t\t\tunpin_user_pages(mr->pages, nr_refs);\n+\t\telse if (mr->flags & IO_REGION_F_MULTI_BUF)\n+\t\t\trelease_multi_buf_pages(mr->pages, nr_refs);\n \t\telse\n \t\t\trelease_pages(mr->pages, nr_refs);\n \n@@ -149,6 +175,54 @@ static int io_region_pin_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+static int io_region_allocate_pages_multi_buf(struct io_mapped_region *mr,\n+\t\t\t\t\t      unsigned int nr_bufs,\n+\t\t\t\t\t      unsigned int buf_size)\n+{\n+\tgfp_t gfp = GFP_USER | __GFP_ACCOUNT | __GFP_ZERO | __GFP_NOWARN;\n+\tstruct page **pages, **cur_pages;\n+\tunsigned int nr_allocated;\n+\tunsigned int buf_pages;\n+\tunsigned int i;\n+\n+\tif (!PAGE_ALIGNED(buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbuf_pages = buf_size >> PAGE_SHIFT;\n+\n+\tpages = kvmalloc_array(mr->nr_pages, sizeof(*pages), gfp);\n+\tif (!pages)\n+\t\treturn -ENOMEM;\n+\n+\tcur_pages = pages;\n+\n+\tfor (i = 0; i < nr_bufs; i++) {\n+\t\tif (io_mem_alloc_compound(cur_pages, buf_pages, buf_size,\n+\t\t\t\t\t  gfp)) {\n+\t\t\tcur_pages += buf_pages;\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tnr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,\n+\t\t\t\t\t\t     buf_pages, cur_pages);\n+\t\tif (nr_allocated != buf_pages) {\n+\t\t\tunsigned int total =\n+\t\t\t\t(cur_pages - pages) + nr_allocated;\n+\n+\t\t\trelease_multi_buf_pages(pages, total);\n+\t\t\tkvfree(pages);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tcur_pages += buf_pages;\n+\t}\n+\n+\tmr->flags |= IO_REGION_F_MULTI_BUF;\n+\tmr->pages = pages;\n+\n+\treturn 0;\n+}\n+\n static int io_region_allocate_pages(struct io_mapped_region *mr,\n \t\t\t\t    struct io_uring_region_desc *reg,\n \t\t\t\t    unsigned long mmap_offset)\n@@ -181,6 +255,43 @@ static int io_region_allocate_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size)\n+{\n+\tunsigned int nr_pages;\n+\tint ret;\n+\n+\tif (WARN_ON_ONCE(mr->pages || mr->ptr || mr->nr_pages))\n+\t\treturn -EFAULT;\n+\n+\tif (WARN_ON_ONCE(!nr_bufs || !buf_size || !PAGE_ALIGNED(buf_size)))\n+\t\treturn -EINVAL;\n+\n+\tif (check_mul_overflow(buf_size >> PAGE_SHIFT, nr_bufs, &nr_pages))\n+\t\treturn -EINVAL;\n+\n+\tif (ctx->user) {\n+\t\tret = __io_account_mem(ctx->user, nr_pages);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\t}\n+\tmr->nr_pages = nr_pages;\n+\n+\tret = io_region_allocate_pages_multi_buf(mr, nr_bufs, buf_size);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\tret = io_region_init_ptr(mr);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\treturn 0;\n+out_free:\n+\tio_free_region(ctx->user, mr);\n+\treturn ret;\n+}\n+\n int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset)\ndiff --git a/io_uring/memmap.h b/io_uring/memmap.h\nindex f4cfbb6b9a1f..3aa1167462ae 100644\n--- a/io_uring/memmap.h\n+++ b/io_uring/memmap.h\n@@ -22,6 +22,10 @@ int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset);\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size);\n+\n static inline void *io_region_get_ptr(struct io_mapped_region *mr)\n {\n \treturn mr->ptr;\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 0882cb34f851..2db8daaf8fde 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -837,7 +837,14 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\t\tbreak;\n \t\tret = io_register_pbuf_ring(ctx, arg);\n \t\tbreak;\n+\tcase IORING_REGISTER_KMBUF_RING:\n+\t\tret = -EINVAL;\n+\t\tif (!arg || nr_args != 1)\n+\t\t\tbreak;\n+\t\tret = io_register_kmbuf_ring(ctx, arg);\n+\t\tbreak;\n \tcase IORING_UNREGISTER_PBUF_RING:\n+\tcase IORING_UNREGISTER_KMBUF_RING:\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the implementation of kernel-managed buffer rings (kmbuf) and how they interact with mmap, specifically that the kmbuf ring should be identified by its bgid shifted by IORING_OFF_KMBUF_SHIFT. The author explained their reasoning for using this approach and introduced new constants to encode the bgid in the mmap offset.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for mmapping kernel-managed buffer rings (kmbuf) to\nuserspace, allowing applications to access the kernel-allocated buffers.\n\nSimilar to application-provided buffer rings (pbuf), kmbuf rings use the\nbuffer group ID encoded in the mmap offset to identify which buffer ring\nto map. The implementation follows the same pattern as pbuf rings.\n\nNew mmap offset constants are introduced:\n  - IORING_OFF_KMBUF_RING (0x88000000): Base offset for kmbuf mappings\n  - IORING_OFF_KMBUF_SHIFT (16): Shift value to encode buffer group ID\n\nThe mmap offset encodes the bgid shifted by IORING_OFF_KMBUF_SHIFT.\nThe io_buf_get_region() helper retrieves the appropriate region.\n\nThis allows userspace to mmap the kernel-allocated buffer region and\naccess the buffers directly.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  2 ++\n io_uring/kbuf.c               | 11 +++++++++--\n io_uring/kbuf.h               |  5 +++--\n io_uring/memmap.c             |  5 ++++-\n 4 files changed, 18 insertions(+), 5 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex a0889c1744bd..42a2812c9922 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -545,6 +545,8 @@ struct io_uring_cqe {\n #define IORING_OFF_SQES\t\t\t0x10000000ULL\n #define IORING_OFF_PBUF_RING\t\t0x80000000ULL\n #define IORING_OFF_PBUF_SHIFT\t\t16\n+#define IORING_OFF_KMBUF_RING\t\t0x88000000ULL\n+#define IORING_OFF_KMBUF_SHIFT\t\t16\n #define IORING_OFF_MMAP_MASK\t\t0xf8000000ULL\n \n /*\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9bc36451d083..ccf5b213087b 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -770,16 +770,23 @@ int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg)\n \treturn 0;\n }\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid)\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed)\n {\n \tstruct io_buffer_list *bl;\n+\tbool is_kernel_managed;\n \n \tlockdep_assert_held(&ctx->mmap_lock);\n \n \tbl = xa_load(&ctx->io_bl_xa, bgid);\n \tif (!bl || !(bl->flags & IOBL_BUF_RING))\n \t\treturn NULL;\n+\n+\tis_kernel_managed = !!(bl->flags & IOBL_KERNEL_MANAGED);\n+\tif (is_kernel_managed != kernel_managed)\n+\t\treturn NULL;\n+\n \treturn &bl->region;\n }\n \ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 62c80a1ebf03..11d165888b8e 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -88,8 +88,9 @@ unsigned int __io_put_kbufs(struct io_kiocb *req, struct io_buffer_list *bl,\n bool io_kbuf_commit(struct io_kiocb *req,\n \t\t    struct io_buffer_list *bl, int len, int nr);\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid);\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed);\n \n static inline bool io_kbuf_recycle_ring(struct io_kiocb *req,\n \t\t\t\t\tstruct io_buffer_list *bl)\ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 8d37e93c0433..916315122323 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -356,7 +356,10 @@ static struct io_mapped_region *io_mmap_get_region(struct io_ring_ctx *ctx,\n \t\treturn &ctx->sq_region;\n \tcase IORING_OFF_PBUF_RING:\n \t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_PBUF_SHIFT;\n-\t\treturn io_pbuf_get_region(ctx, id);\n+\t\treturn io_buf_get_region(ctx, id, false);\n+\tcase IORING_OFF_KMBUF_RING:\n+\t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_KMBUF_SHIFT;\n+\t\treturn io_buf_get_region(ctx, id, true);\n \tcase IORING_MAP_OFF_PARAM_REGION:\n \t\treturn &ctx->param_region;\n \tcase IORING_MAP_OFF_ZCRX_REGION:\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about distinguishing between kernel-managed buffer addresses and negative values in error checking. They modified the io_br_sel struct to separate address and value fields, allowing for kernel-managed buffers to be selected. The author also added auto-commit logic for kernel-managed buffers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "added code changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Allow kernel-managed buffers to be selected. This requires modifying the\nio_br_sel struct to separate the fields for address and val, since a\nkernel address cannot be distinguished from a negative val when error\nchecking.\n\nAuto-commit any selected kernel-managed buffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring_types.h |  8 ++++----\n io_uring/kbuf.c                | 16 ++++++++++++----\n 2 files changed, 16 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 3e4a82a6f817..36cc2e0346d9 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -93,13 +93,13 @@ struct io_mapped_region {\n  */\n struct io_br_sel {\n \tstruct io_buffer_list *buf_list;\n-\t/*\n-\t * Some selection parts return the user address, others return an error.\n-\t */\n \tunion {\n+\t\t/* for classic/ring provided buffers */\n \t\tvoid __user *addr;\n-\t\tssize_t val;\n+\t\t/* for kernel-managed buffers */\n+\t\tvoid *kaddr;\n \t};\n+\tssize_t val;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex ccf5b213087b..1e8395270227 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -155,7 +155,8 @@ static int io_provided_buffers_select(struct io_kiocb *req, size_t *len,\n \treturn 1;\n }\n \n-static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n+static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n+\t\t\t     unsigned int issue_flags)\n {\n \t/*\n \t* If we came in unlocked, we have no choice but to consume the\n@@ -170,7 +171,11 @@ static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n \tif (issue_flags & IO_URING_F_UNLOCKED)\n \t\treturn true;\n \n-\t/* uring_cmd commits kbuf upfront, no need to auto-commit */\n+\t/* kernel-managed buffers are auto-committed */\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\treturn true;\n+\n+\t/* multishot uring_cmd commits kbuf upfront, no need to auto-commit */\n \tif (!io_file_can_poll(req) && req->opcode != IORING_OP_URING_CMD)\n \t\treturn true;\n \treturn false;\n@@ -200,9 +205,12 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n-\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n+\telse\n+\t\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n \n-\tif (io_should_commit(req, issue_flags)) {\n+\tif (io_should_commit(req, bl, issue_flags)) {\n \t\tio_kbuf_commit(req, sel.buf_list, *len, 1);\n \t\tsel.buf_list = NULL;\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about buffer ring pinning and unpinning, explaining that the kernel-managed buffer rings need to be pinned by fuse in atomic contexts. The author added APIs for pinning and unpinning buffer rings, preventing userspace from unregistering a buffer ring while it is pinned.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "added new code",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add kernel APIs to pin and unpin buffer rings, preventing userspace from\nunregistering a buffer ring while it is pinned by the kernel.\n\nThis provides a mechanism for kernel subsystems to safely access buffer\nring contents while ensuring the buffer ring remains valid. A pinned\nbuffer ring cannot be unregistered until explicitly unpinned. On the\nuserspace side, trying to unregister a pinned buffer will return -EBUSY.\n\nThis is a preparatory change for upcoming fuse usage of kernel-managed\nbuffer rings. It is necessary for fuse to pin the buffer ring because\nfuse may need to select a buffer in atomic contexts, which it can only\ndo so by using the underlying buffer list pointer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 17 +++++++++++++\n io_uring/kbuf.c              | 48 ++++++++++++++++++++++++++++++++++++\n io_uring/kbuf.h              |  5 ++++\n 3 files changed, 70 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 375fd048c4cb..702b1903e6ee 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -84,6 +84,10 @@ struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n \t\t\t\t struct io_br_sel *sel, unsigned int issue_flags);\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t    unsigned issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -126,6 +130,19 @@ static inline bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n {\n \treturn true;\n }\n+static inline int io_uring_buf_ring_pin(struct io_uring_cmd *cmd,\n+\t\t\t\t\tunsigned buf_group,\n+\t\t\t\t\tunsigned issue_flags,\n+\t\t\t\t\tstruct io_buffer_list **bl)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n+static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned buf_group,\n+\t\t\t\t\t  unsigned issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 1e8395270227..dee1764ed19f 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -9,6 +9,7 @@\n #include <linux/poll.h>\n #include <linux/vmalloc.h>\n #include <linux/io_uring.h>\n+#include <linux/io_uring/cmd.h>\n \n #include <uapi/linux/io_uring.h>\n \n@@ -237,6 +238,51 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \treturn sel;\n }\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *buffer_list;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbuffer_list = io_buffer_get_list(ctx, buf_group);\n+\tif (buffer_list && (buffer_list->flags & IOBL_BUF_RING)) {\n+\t\tif (unlikely(buffer_list->flags & IOBL_PINNED)) {\n+\t\t\tret = -EALREADY;\n+\t\t} else {\n+\t\t\tbuffer_list->flags |= IOBL_PINNED;\n+\t\t\tret = 0;\n+\t\t\t*bl = buffer_list;\n+\t\t}\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_pin);\n+\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t       unsigned issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (bl && (bl->flags & IOBL_BUF_RING) && (bl->flags & IOBL_PINNED)) {\n+\t\tbl->flags &= ~IOBL_PINNED;\n+\t\tret = 0;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_unpin);\n+\n /* cap it at a reasonable 256, will be one page even for 4K */\n #define PEEK_MAX_IMPORT\t\t256\n \n@@ -747,6 +793,8 @@ int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t\treturn -ENOENT;\n \tif (!(bl->flags & IOBL_BUF_RING))\n \t\treturn -EINVAL;\n+\tif (bl->flags & IOBL_PINNED)\n+\t\treturn -EBUSY;\n \n \tscoped_guard(mutex, &ctx->mmap_lock)\n \t\txa_erase(&ctx->io_bl_xa, bl->bgid);\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 11d165888b8e..781630c2cc10 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -12,6 +12,11 @@ enum {\n \tIOBL_INC\t\t= 2,\n \t/* buffers are kernel managed */\n \tIOBL_KERNEL_MANAGED\t= 4,\n+\t/*\n+\t * buffer ring is pinned and cannot be unregistered by userspace until\n+\t * it has been unpinned\n+\t */\n+\tIOBL_PINNED\t\t= 8,\n };\n \n struct io_buffer_list {\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the lack of an interface for recycling buffers back into kernel-managed buffer rings, adding a new function io_uring_kmbuf_recycle() to handle this task.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add an interface for buffers to be recycled back into a kernel-managed\nbuffer ring.\n\nThis is a preparatory patch for fuse over io-uring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 11 +++++++++\n io_uring/kbuf.c              | 44 ++++++++++++++++++++++++++++++++++++\n 2 files changed, 55 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 702b1903e6ee..a488e945f883 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -88,6 +88,10 @@ int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t    unsigned issue_flags);\n+\n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -143,6 +147,13 @@ static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n+\t\t\t\t\t unsigned int buf_group, u64 addr,\n+\t\t\t\t\t unsigned int len, unsigned int bid,\n+\t\t\t\t\t unsigned int issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex dee1764ed19f..17b6178be4ce 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -102,6 +102,50 @@ void io_kbuf_drop_legacy(struct io_kiocb *req)\n \treq->kbuf = NULL;\n }\n \n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags)\n+{\n+\tstruct io_kiocb *req = cmd_to_io_kiocb(cmd);\n+\tstruct io_ring_ctx *ctx = req->ctx;\n+\tstruct io_uring_buf_ring *br;\n+\tstruct io_uring_buf *buf;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tif (WARN_ON_ONCE(req->flags & REQ_F_BUFFERS_COMMIT))\n+\t\treturn ret;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\n+\tif (!bl || WARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING)) ||\n+\t    WARN_ON_ONCE(!(bl->flags & IOBL_KERNEL_MANAGED)))\n+\t\tgoto done;\n+\n+\tbr = bl->buf_ring;\n+\n+\tif (WARN_ON_ONCE((br->tail - bl->head) >= bl->nr_entries))\n+\t\tgoto done;\n+\n+\tbuf = &br->bufs[(br->tail) & bl->mask];\n+\n+\tbuf->addr = addr;\n+\tbuf->len = len;\n+\tbuf->bid = bid;\n+\n+\treq->flags &= ~REQ_F_BUFFER_RING;\n+\n+\tbr->tail++;\n+\tret = 0;\n+\n+done:\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_kmbuf_recycle);\n+\n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)\n {\n \tstruct io_ring_ctx *ctx = req->ctx;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring_is_kmbuf_ring() function, which was missing an implementation to check if a buffer ring is kernel-managed. The author provided the missing implementation in this reply, checking for the IOBL_KERNEL_MANAGED flag and returning true if it's set.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "provided a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "io_uring_is_kmbuf_ring() returns true if there is a kernel-managed\nbuffer ring at the specified buffer group.\n\nThis is a preparatory patch for upcoming fuse kernel-managed buffer\nsupport, which needs to ensure the buffer ring registered by the server\nis a kernel-managed buffer ring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h |  9 +++++++++\n io_uring/kbuf.c              | 20 ++++++++++++++++++++\n 2 files changed, 29 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex a488e945f883..04a937f6f4d3 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -92,6 +92,9 @@ int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t   u64 addr, unsigned int len, unsigned int bid,\n \t\t\t   unsigned int issue_flags);\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -154,6 +157,12 @@ static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned int buf_group,\n+\t\t\t\t\t  unsigned int issue_flags)\n+{\n+\treturn false;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 17b6178be4ce..797cc2f0a5e9 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -963,3 +963,23 @@ int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \n \treturn ret;\n }\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tbool is_kmbuf_ring = false;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (likely(bl) && (bl->flags & IOBL_KERNEL_MANAGED)) {\n+\t\tWARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING));\n+\t\tis_kmbuf_ring = true;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn is_kmbuf_ring;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_is_kmbuf_ring);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about the io_uring mutex being held by in-progress commits and needing to select a buffer from a kernel-managed bufring while the mutex may already be held. The author agrees that exporting io_ring_buffer_select() will be necessary for fuse io-uring, which needs to select a buffer in atomic contexts.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges fix needed",
                "preparatory patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Export io_ring_buffer_select() so that it may be used by callers who\npass in a pinned bufring without needing to grab the io_uring mutex.\n\nThis is a preparatory patch that will be needed by fuse io-uring, which\nwill need to select a buffer from a kernel-managed bufring while the\nuring mutex may already be held by in-progress commits, and may need to\nselect a buffer in atomic contexts.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 14 ++++++++++++++\n io_uring/kbuf.c              |  7 ++++---\n 2 files changed, 18 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 04a937f6f4d3..d4b5943bdeb1 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -95,6 +95,10 @@ int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \n bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t    unsigned int issue_flags);\n+\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -163,6 +167,16 @@ static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n {\n \treturn false;\n }\n+static inline struct io_br_sel io_ring_buffer_select(struct io_kiocb *req,\n+\t\t\t\t\t\t     size_t *len,\n+\t\t\t\t\t\t     struct io_buffer_list *bl,\n+\t\t\t\t\t\t     unsigned int issue_flags)\n+{\n+\tstruct io_br_sel sel = {\n+\t\t.val = -EOPNOTSUPP,\n+\t};\n+\treturn sel;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 797cc2f0a5e9..9a93f10d3214 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -226,9 +226,9 @@ static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n \treturn false;\n }\n \n-static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n-\t\t\t\t\t      struct io_buffer_list *bl,\n-\t\t\t\t\t      unsigned int issue_flags)\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags)\n {\n \tstruct io_uring_buf_ring *br = bl->buf_ring;\n \t__u16 tail, head = bl->head;\n@@ -261,6 +261,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \t}\n \treturn sel;\n }\n+EXPORT_SYMBOL_GPL(io_ring_buffer_select);\n \n struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \t\t\t\t  unsigned buf_group, unsigned int issue_flags)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about returning the id of the selected buffer in io_buffer_select(), agreeing to modify the function to return the buffer's address, size, and id.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed_to_modify_function"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Return the id of the selected buffer in io_buffer_select(). This is\nneeded for kernel-managed buffer rings to later recycle the selected\nbuffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h   | 2 +-\n include/linux/io_uring_types.h | 2 ++\n io_uring/kbuf.c                | 7 +++++--\n 3 files changed, 8 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex d4b5943bdeb1..94df2bdebe77 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);\n \n /*\n  * Select a buffer from the provided buffer group for multishot uring_cmd.\n- * Returns the selected buffer address and size.\n+ * Returns the selected buffer address, size, and id.\n  */\n struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n \t\t\t\t\t    unsigned buf_group, size_t *len,\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 36cc2e0346d9..5a56bb341337 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -100,6 +100,8 @@ struct io_br_sel {\n \t\tvoid *kaddr;\n \t};\n \tssize_t val;\n+\t/* id of the selected buffer */\n+\tunsigned buf_id;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9a93f10d3214..24c1e34ea23e 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -250,6 +250,7 @@ struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n+\tsel.buf_id = req->buf_index;\n \tif (bl->flags & IOBL_KERNEL_MANAGED)\n \t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n \telse\n@@ -274,10 +275,12 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \n \tbl = io_buffer_get_list(ctx, buf_group);\n \tif (likely(bl)) {\n-\t\tif (bl->flags & IOBL_BUF_RING)\n+\t\tif (bl->flags & IOBL_BUF_RING) {\n \t\t\tsel = io_ring_buffer_select(req, len, bl, issue_flags);\n-\t\telse\n+\t\t} else {\n \t\t\tsel.addr = io_provided_buffer_select(req, len, bl);\n+\t\t\tsel.buf_id = req->buf_index;\n+\t\t}\n \t}\n \tio_ring_submit_unlock(req->ctx, issue_flags);\n \treturn sel;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about indicating which buffer was selected in the completion queue entry, explaining that this is necessary for fuse to relay the information to userspace and agreeing to add the IORING_CQE_F_BUFFER flag along with the buffer index.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a requirement",
                "agreed to implement a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When uring_cmd operations select a buffer, the completion queue entry\nshould indicate which buffer was selected.\n\nSet IORING_CQE_F_BUFFER on the completed entry and encode the buffer\nindex if a buffer was selected.\n\nThis will be needed for fuse, which needs to relay to userspace which\nselected buffer contains the data.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/uring_cmd.c | 6 +++++-\n 1 file changed, 5 insertions(+), 1 deletion(-)\n\ndiff --git a/io_uring/uring_cmd.c b/io_uring/uring_cmd.c\nindex ee7b49f47cb5..6d38df1a812d 100644\n--- a/io_uring/uring_cmd.c\n+++ b/io_uring/uring_cmd.c\n@@ -151,6 +151,7 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \t\t       unsigned issue_flags, bool is_cqe32)\n {\n \tstruct io_kiocb *req = cmd_to_io_kiocb(ioucmd);\n+\tu32 cflags = 0;\n \n \tif (WARN_ON_ONCE(req->flags & REQ_F_APOLL_MULTISHOT))\n \t\treturn;\n@@ -160,7 +161,10 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \tif (ret < 0)\n \t\treq_set_fail(req);\n \n-\tio_req_set_res(req, ret, 0);\n+\tif (req->flags & (REQ_F_BUFFER_SELECTED | REQ_F_BUFFER_RING))\n+\t\tcflags |= IORING_CQE_F_BUFFER |\n+\t\t\t(req->buf_index << IORING_CQE_BUFFER_SHIFT);\n+\tio_req_set_res(req, ret, cflags);\n \tif (is_cqe32) {\n \t\tif (req->ctx->flags & IORING_SETUP_CQE_MIXED)\n \t\t\treq->cqe.flags |= IORING_CQE_F_32;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested adding a WARN_ON_ONCE() check to prevent int promotion from causing issues when calculating the difference between br->tail and bl->head, recommending that this be done across multiple patches.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you want:\n\n\tif (WARN_ON_ONCE((__u16)(br->tail - bl->head) >= bl->nr_entries))\n\nhere to avoid int promotion from messing this up if tail has wrapped.\n\nIn general, across the patches for the WARN_ON_ONCE(), it's not a huge\nissue to have a litter of them for now. Hopefully we can prune some of\nthese down the line, however.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe questioned the need for a new field 'kbuf_ring' in the io_uring registration, suggesting that req->buf_index could be used instead.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm probably missing something here, but why can't the caller just use\nreq->buf_index for this?\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe requested that Joanne Koong provide a branch with all patches, including user code, for easier cross-referencing and evaluation of the helpers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request",
                "clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Generally looks pretty good - for context, do you have a branch with\nthese patches and the users on top too? Makes it a bit easier for cross\nreferencing, as some of these really do need an exposed user to make a\ngood judgement on the helpers.\n\nI know there's the older series, but I'm assuming the latter patches\nchanged somewhat too, and it'd be nicer to look at a current set rather\nthan go back to the older ones.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested refactoring io_pbuf_get_region() to handle kernel-managed buffer rings by adding a new helper function, io_kbuf_get_region(), and modifying the existing function to check for the IOBL_KERNEL_MANAGED flag.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "minor nit"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "For this, I think just add another helper - leave io_pbuf_get_region()\nand add a bl->flags & IOBL_KERNEL_MANAGED error check in there, and\nadd a io_kbuf_get_region() or similar and have a !(bl->flags &\nIOBL_KERNEL_MANAGED) error check in that one.\n\nThat's easier to read, and there's little reason to avoid duplicating\nthe xa_load() part.\n\nMinor nit, but imho it's more readable that way.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested using a pointer to struct io_buffer_list instead of passing it by value, and recommended returning an ERR_PTR if the function fails",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Probably use the usual struct io_buffer_list *bl here and either use an\nERR_PTR return, or rename the passed on **bl to **blret or something.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Jens Axboe suggested a more efficient way to check for pinned buffer rings by combining two flags into one condition, and also recommended adding an early return statement if the buffer list is empty.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested improvement",
                "code optimization"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Usually done as:\n\n\tif ((bl->flags & (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))\n\nand maybe then just have an earlier\n\n\tif (!bl)\n\t\tgoto err;",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested that the patch should not enforce a character limit on io_uring strings, as it is acceptable to exceed the 80-character limit in certain cases.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "to avoid making it way too long. For io_uring, it's fine to exceed 80\nchars where it makes sense.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the code should allow regions to work with user-passed memory, which would enable optimizations such as huge page usage.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "If you're creating a region, there should be no reason why it\ncan't work with user passed memory. You're fencing yourself off\noptimisations that are already there like huge pages.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that io_create_region() should be used instead of the new function in the patch, as it does not introduce any new functionality and violates abstractions. He suggested stripping buffer allocation from IORING_REGISTER_KMBUF_RING and requiring users to register a memory region of appropriate size, which would allow fuse to populate the buffer ring using that memory region.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Please use io_create_region(), the new function does nothing new\nand only violates abstractions.\n\nProvided buffer rings with kernel addresses could be an interesting\nabstraction, but why is it also responsible for allocating buffers?\nWhat I'd do:\n\n1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.\n2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.\n    Or maybe don't expose it to the user at all and create it from\n    fuse via internal API.\n3. Require the user to register a memory region of appropriate size,\n    see IORING_REGISTER_MEM_REGION, ctx->param_region. Make fuse\n    populating the buffer ring using the memory region.\n\nI wanted to make regions shareable anyway (need it for other purposes),\nI can toss patches for that tomorrow.\n\nA separate question is whether extending buffer rings is the right\napproach as it seems like you're only using it for fuse requests and\nnot for passing buffers to normal requests, but I don't see the\nbig picture here.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the removal of io_create_region_multi_buf() means there is no need to align every buffer, which could result in wasted memory due to 64KB page sizes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested optimization",
                "concern about performance"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "With io_create_region_multi_buf() gone, you shouldn't need\nto align every buffer, that could be a lot of wasted memory\n(thinking about 64KB pages).",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Caleb Mateos",
              "summary": "Reviewer Caleb Mateos noted that the optimization !(~bl->flags & (IOBL_BUF_RING|IOBL_PINNED)) is unnecessary because modern compilers will perform it automatically, and provided a link to a Godbolt example demonstrating this.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested_change",
                "optimization"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, modern compilers will perform this optimization automatically.\nThey'll even optimize it further to !(~bl->flags &\n(IOBL_BUF_RING|IOBL_PINNED)): https://godbolt.org/z/xGoP4TfhP\n\nBest,\nCaleb",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested that the patch's implementation should follow a common pattern for better readability, citing an example where the current implementation is easier to read than the original.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes",
                "acknowledged difficulty in reading"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure, it's not about that, it's more about the common way of doing it,\nwhich makes it easier to read for people. FWIW, your example is easier\nto read too than the original.\n\n-- \nJens Axboe",
              "reply_to": "Caleb Mateos",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author asked a clarifying question about whether there are any optimizations possible with user-allocated buffers that wouldn't be achievable with kernel-allocated buffers, specifically for huge pages.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Are there any optimizations with user-allocated buffers that wouldn't\nbe possible with kernel-allocated buffers? For huge pages, can't the\nkernel do this as well (eg I see in io_mem_alloc_compound(), it calls\ninto alloc_pages() with order > 0)?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong addressed Pavel Begunkov's feedback about using a single function call for creating regions, explaining that separate checks and allocation calls are needed for different types of regions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "There's separate checks needed between io_create_region() and\nio_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag\nchecking) and different allocation calls (eg\nio_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).\nMaybe I'm misinterpreting your comment (or the code), but I'm not\nseeing how this can just use io_create_region().",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addresses Pavel Begunkov's concern about the benefits of kernel-managed buffer rings, explaining that it simplifies interface and lifecycle management, ensures contiguous page allocation, and avoids complications with user-allocated buffers. The author asks for clarification on the advantages of allocating buffers from userspace.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request_for_clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Conceptually, I think it makes the interface and lifecycle management\nsimpler/cleaner. With registering it from userspace, imo there's\nadditional complications with no tangible benefits, eg it's not\nguaranteed that the memory regions registered for the buffers are the\nsame size, with allocating it from the kernel-side we can guarantee\nthat the pages are allocated physically contiguously, userspace setup\nwith user-allocated buffers is less straightforward, etc. In general,\nI'm just not really seeing what advantages there are in allocating the\nbuffers from userspace. Could you elaborate on that part more?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern about squashing kernel-managed buffer rings into existing pbuf rings, explaining that this would require adding pinning support to pbuf rings and citing a previous patch idea that was dropped due to feedback from Jens and Caleb. Author implies that pinning will be necessary for fuse contexts.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If kmbuf rings are squashed into pbuf rings, then pbuf rings will need\nto support pinning. In fuse, there are some contexts where you can't\ngrab the uring mutex because you're running in atomic context and this\ncan be encountered while recycling the buffer. I originally had a\npatch adding pinning to pbuf rings (to mitigate the overhead of\nregistered buffers lookups) but dropped it when Jens and Caleb didn't\nlike the idea. But for kmbuf rings, pinning will be necessary for\nfuse.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to Pavel Begunkov's question about what constitutes a 'normal request' in the context of io_uring buffer rings, explaining that for fuse's use case there are only fuse requests.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "What are 'normal requests'? For fuse's use case, there are only fuse requests.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing Jens' feedback about how to access the buffer ID from the io_uring command structure, suggesting that returning the buf_id as part of the io_br_sel struct was a cleaner approach but offering alternatives such as adding a helper function. The author is open to suggestions and willing to make changes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "open_to_suggestions",
                "willing_to_make_changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The caller can, but from the caller side they only have access to the\ncmd so they would need to do something like\n\nstruct io_kiocb *req = cmd_to_iocb_kiocb(ent->cmd);\nbuf_id = req->buf_index;\n\nwhich may be kind of ugly with looking inside io-uring internals.\nMaybe a helper here would be nicer, something like\nio_uring_cmd_buf_id() or io_uring_req_buf_id(). It seemed cleaner to\nme to just return the buf id as part of the io_br_sel struct, but I'm\nhappy to do it another way if you have a preference.\n\nThanks,\nJoanne",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that additional changes are needed for v2, including addressing feedback from other comments and incorporating Pavel's requested changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for further revision",
                "planned to address in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for reviewing the patches. The branch containing the userside\nchanges on top of these patches is in [1]. I'll make the changes you\npointed out in your other comments as part of v2. Once the discussion\nwith Pavel is resolved / figured out with the changes he wants for v2,\nI'll submit v2.\n\nThanks,\nJoanne\n\n[1] https://github.com/joannekoong/linux/commits/fuse_zero_copy/",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that allocating 1MB in kernel space will not result in a PMD mappable huge page, unlike user space where 2MB can be allocated and reused for other purposes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change to allocation size"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, there is handful of differences. To name one, 1MB allocation won't\nget you a PMD mappable huge page, while user space can allocate 2MB,\nregister the first 1MB and reuse the rest for other purposes.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested that instead of modifying io_create_region() to be less strict, the caller should filter arguments to ensure only necessary types are passed, specifically recommending against passing IORING_MEM_REGION_TYPE_USER if it's not used.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "If io_create_region() is too strict, let's discuss that in\nexamples if there are any, but it's likely not a good idea changing\nthat. If it's too lax, filter arguments in the caller. IOW, don't\npass IORING_MEM_REGION_TYPE_USER if it's not used.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that the memmap.c changes in the patch can be dropped because they are not necessary for kernel-managed buffer rings, which only require contiguous memory within a single buffer. He suggested that using io_create_region() would be sufficient and would avoid issues with io_mem_alloc_compound().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I saw that and saying that all memmap.c changes can get dropped.\nYou're using it as one big virtually contig kernel memory range then\nchunked into buffers, and that's pretty much what you're getting with\nnormal io_create_region(). I get that you only need it to be\ncontiguous within a single buffer, but that's not what you're doing,\nand it'll be only worse than default io_create_region() e.g.\neffectively disabling any usefulness of io_mem_alloc_compound(),\nand ultimately you don't need to care.\n\nRegions shouldn't know anything about your buffers, how it's\nsubdivided after, etc.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested adding a mechanism to support user-provided memory for kernel-managed buffer rings, proposing the use of io_create_region() with additional flags and fields to handle user memory",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "struct io_uring_region_desc rd = {};\ntotal_size = nr_bufs * buf_size;\nrd.size = PAGE_ALIGN(total_size);\nio_create_region(&region, &rd);\n\nAdd something like this for user provided memory:\n\nif (use_user_memory) {\n\trd.user_addr = uaddr;\n\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n}",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov suggested separating ring creation from population, and provided an example of how this could be done in the fuse kernel module, allowing other users to use empty rings for IO operations.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested separation",
                "provided alternative implementation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I don't think I follow. I'm saying that it might be interesting\nto separate rings from how and with what they're populated on the\nkernel API level, but the fuse kernel module can do the population\nand get exactly same layout as you currently have:\n\nint fuse_create_ring(size_t region_offset /* user space argument */) {\n\tstruct io_mapped_region *mr = get_mem_region(ctx);\n\t// that can take full control of the ring\n\tring = grab_empty_ring(io_uring_ctx);\n\n\tsize = nr_bufs * buf_size;\n\tif (region_offset + size > get_size(mr)) // + other validation\n\t\treturn error;\n\n\tbuf = mr_get_ptr(mr) + offset;\n\tfor (i = 0; i < nr_bufs; i++) {\n\t\tring_push_buffer(ring, buf, buf_size);\n\t\tbuf += buf_size;\n\t}\n}\n\nfuse might not care, but with empty rings other users will get a\nchannel they can use to do IO (e.g. read requests) using their\nkernel addresses in the future.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested simplifying kernel-managed buffer rings by piggybacking on existing pbuf implementation and using a flag to differentiate, rather than introducing separate uapi and internal changes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It'd change uapi but not internals, you already piggy back it\non pbuf implementation and differentiate with a flag.\n\nIt could basically be:\n\nif (flags & IOU_PBUF_RING_KM)\n\tbl->flags |= IOBL_KERNEL_MANAGED;\n\nPinning can be gated on that flag as well. Pretty likely uapi\nand internals will be a bit cleaner, but that's not a huge deal,\njust don't see why would you roll out a separate set of uapi\n([un]register, offsets, etc.) when essentially it can be treated\nas the same thing.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the patch was pinning the registered buffer table without providing buffer rings, which is a bad idea, and suggested an alternative approach where all memory is kept in one larger registered buffer and pinned only once.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IIRC, you was pinning the registered buffer table and not provided\nbuffer rings? Which would indeed be a bad idea. Thinking about it,\nfwiw, instead of creating multiple registered buffers and trying to\nlock the entire table, you could've kept all memory in one larger\nregistered buffer and pinned only it. It's already refcounted, so\nshouldn't have been much of a problem.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed concerns that creating many small regions for kernel-managed buffer rings would lead to extra mmap()s, user space management overhead, wasted space, over-accounting, and increased memory footprint. He also suggested that this approach would limit the ability to free buffers while there are requests for the context.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "expressed concerns"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "To explain why, I don't think that creating many small regions\nis a good direction going forward. In case of kernel allocation,\nit's extra mmap()s, extra user space management, and wasted space.\nFor user provided memory it's over-accounting and extra memory\nfootprint. It'll also give you better lifecycle guarantees, i.e.\nyou won't be able to free buffers while there are requests for the\ncontext. I'm not so sure about ring bound memory, let's say I have\nmy suspicions, and you'd need to be extra careful about buffer\nlifetimes even after a fuse instance dies.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that kernel-managed buffer rings would be particularly useful for operations like reading or receiving data from provided buffers, and suggested that the implementation should not require changes to opcode-specific code in kbuf.c.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggestion",
                "opinion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Any kind of read/recv/etc. that can use provided buffers. It's\nwhere kernel memory filled rings would shine, as you'd be able\nto use them together without changing any opcode specific code.\nI.e. not changes in read request implementation, only kbuf.c\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that any pages mapped to userspace can be allocated in the kernel, allowing for a buffer ring that is only mapped read-only into userspace, enabling zero-copy raids if the device requires stable pages for checksumming or raid.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "appreciation for design",
                "positive feedback"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Any pages mapped to userspace can be allocated in the kernel as well.\n\nAnd I really do like this design, because it means we can have a\nbuffer ring that is only mapped read-only into userspace.  That way\nwe can still do zero-copy raids if the device requires stable pages\nfor checksumming or raid.  I was going to implement this as soon\nas this series lands upstream.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about using io_create_region(), explaining that it fails due to allocating too much memory at once, and instead opted for io_region_allocate_pages_multi_buf() to bypass allocation errors.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical issue",
                "provided explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When I originally implemented it, I had it use\nio_region_allocate_pages() but this fails because it's allocating way\ntoo much memory at once. For fuse's use case, each buffer is usually\nat least 1 MB if not more. Allocating the memory one buffer a time in\nio_region_allocate_pages_multi_buf() bypasses the allocation errors I\nwas seeing. That's the main reason I don't think this can just use\nio_create_region().",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarifies her understanding of reviewer's feedback, confirming that she and Christoph thought the user should allocate buffers but now understands the kernel can do so through IORING_REGISTER_MEM_REGION",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "understanding"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oh okay, from your first message I (and I think christoph too) thought\nwhat you were saying is that the user should be responsible for\nallocating the buffers with complete ownership over them, and then\njust pass those allocated to the kernel to use. But what you're saying\nis that just use a different way for getting the kernel to allocate\nthe buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am\nI reading this correctly?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed Pavel Begunkov's concern that combining kernel-managed buffer rings (kmbufs) and regular pbufs into a single API would make the pbuf API unnecessarily complex. The author explained that having separate APIs for pbufs, kmbufs, and regular pbufs clarifies their different expectations and behaviors.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged feedback",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "imo, it looked cleaner as a separate api because it has different\nexpectations and behaviors and squashing kmbuf into the pbuf api makes\nthe pbuf api needlessly more complex. Though I guess from the\nuserspace pov, liburing could have a wrapper that takes care of\nsetting up the pbuf details for kernel-managed pbufs. But in my head,\nhaving pbufs vs. kmbufs makes it clearer what each one does vs regular\npbufs vs. pbufs that are kernel-managed.\n\nEspecially with now having kmbufs go through the ioring mem region\ninterface, it makes things more confusing imo if they're combined, eg\npbufs that are kernel-managed are created empty and then populated\nfrom the kernel side by whatever subsystem is using them. Right now\nthere's only one mem region supported per ring, but in the future if\nthere's the possibility that multiple mem regions can be registered\n(eg if userspace doesn't know upfront what mem region length they'll\nneed), then we should also probably add in a region id param for the\nregistration arg, which if kmbuf rings go through the pbuf ring\nregistration api, is not possible to do.\n\nBut I'm happy to combine the interfaces and go with your suggestion.\nI'll make this change for v2 unless someone else objects.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that they previously pinned the wrong data structure (registered buffer table) instead of the intended one (pbuf ring), but this is a minor mistake and does not affect the overall patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged minor mistake"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah, you're right I misremembered and the objections / patch I\ndropped was pinning the registered buffer table, not the pbuf ring",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing Pavel Begunkov's concern about sparse buffers populated by the kernel and how they would be unregistered. The author suggests that if those buffers are automatically pinned, then users would need to unregister them individually instead of using IORING_UNREGISTER_BUFFERS, which could be annoying for them.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explaining trade-offs"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hmm, I'm not sure this idea would work for sparse buffers populated by\nthe kernel, unless those are automatically pinned too but then from\nthe user POV for unregistration they'd need to unregister buffers\nindividually instead of just calling IORING_UNREGISTER_BUFFERS but it\nmight be annoying for them to now need to know which buffers are\npinned vs not. When i benchmarked the fuse code with vs without pinned\nregistered buffers, it didn't seem to make much of a difference\nperformance-wise thankfully, so I just dropped it.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about individual buffers being allocated separately by the kernel, and explains that they are not convinced this would reduce userspace management or wasted space.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "To clarify, is this in reply to why the individual buffers shouldn't\nbe allocated separately by the kernel?\nI added a comment about this above in the discussion about\nio_region_allocate_pages_multi_buf(), and if the memory allocation\nissue I was seeing is bypassable and the region can be allocated all\nat once, I'm happy to make that change. With having the allocation be\nseparate buffers though, I'm not sure I agree that there are extra\nmmaps / userspace management. All the pages across the buffers are\nvmapped together and the userspace just needs to do 1 mmap call for\nthem. On the userspace side, I don't think there's more management\nsince the mmapped address represents the range across all the buffers.\nI'm not seeing how there's wasted space either since the only\nrequirement is that the buffer size is page aligned. I think also\nthere's a higher chance of the entire buffer region being physically\ncontiguous if each buffer is allocated separately vs. all the buffers\nare allocated as 1 region. I don't feel strongly about this either way\nand I'm happy to allocate the entire region at once if that's\npossible.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author asks for clarification on reviewer's concerns about over-accounting and extra memory footprint in kernel-managed buffer rings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Just out of curiosity, could you elaborate on the over-accounting and\nextra memory footprint? I was under the impression it would be the\nsame since the accounting gets adjusted by the total bytes allocated?\nFor the extra memory footprint, is the extra footprint from the\nmetadata to describe each buffer region, or are you referring to\nsomething else?",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author is addressing concerns about the API and kernel buffer allocation in the io_uring/kbuf series. She plans to make changes for v2, including removing the KMBUF_RING API interface, having kernel buffer allocation go through IORING_REGISTER_MEM_REGION, and adding APIs for subsystems to populate a kernel-managed buffer ring with addresses from the registered memory region.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges fix is needed",
                "plans changes for v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for your input on the series. To iterate / sum up, these are\nchanges for v2 I'll be making:\n- api-wise from userspace/liburing: get rid of KMBUF_RING api\ninterface and have users go through PBUF_RING api instead with a flag\nindicating the ring is kernel-managed\n- have kernel buffer allocation go through IORING_REGISTER_MEM_REGION\ninstead, which means when the pbuf ring is created and the\nkernel-managed flag is set, the ring will be empty. The memory region\nwill need to be registered before the mmap call to the ring fd.\n- add apis for subsystems to populate a kernel-managed buffer ring\nwith addresses from the registered mem region\n\nDoes this align with your understanding of the conversation as well or\nis there anything I'm missing?\n\nAnd Christoph, do these changes for v2 work for your use case as well?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer Christoph Hellwig noted that he needs a setup where the kernel fully controls buffer allocation and guarantees user processes can only read, not write, to the memory, and expressed interest in piggybacking onto this work.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "expressed need for specific feature"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm arguing exactly against this.  For my use case I need a setup\nwhere the kernel controls the allocation fully and guarantees user\nprocesses can only read the memory but never write to it.  I'd love\nto be able to piggy back than onto your work.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the current implementation of pow2 round ups will waste memory, as 1MB allocations will never become 2MB huge pages, and also questioned the handling of 1GB huge pages, suggesting users could make better placement decisions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "waste",
                "questioned"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "pow2 round ups will waste memory. 1MB allocations will never\nbecome 2MB huge pages. And there is a separate question of\n1GB huge pages. The user can be smarter about all placement\ndecisions.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested that the io_uring uapi should include fields for user-provided memory as an optional feature, and also noted that fuse can refuse to bind to buffer rings it doesn't like.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "optional feature",
                "nothing against"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That's an interesting case. To be clear, user provided memory is\nan optional feature for pbuf rings / regions / etc., and I think\nthe io_uring uapi should leave fields for the feature. However, I\nhave nothing against fuse refusing to bind to buffer rings it\ndoesn't like.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer suggested modifying IORING_REGISTER_MEM_REGION to support read-only registrations and making bounce avoidance optional or rejecting unsupported setups during initialization",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IORING_REGISTER_MEM_REGION supports both types of allocations. It can\nhave a new registration flag for read-only, and then you either make\nthe bounce avoidance optional or reject binding fuse to unsupported\nsetups during init. Any arguments against that? I need to go over\nJoanne's reply, but I don't see any contradiction in principal with\nyour use case.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author is clarifying the difference between kernel-managed buffer rings and user-initiated kbuf ring setup. She explains that if userspace initiates the setup, it would be semantically similar to IORING_REGISTER_MEM_REGION, but uglier. No fix planned.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"control the allocation fully\" do you mean for your use case, the\nallocation/setup isn't triggered by userspace but is initiated by the\nkernel (eg user never explicitly registers any kbuf ring, the kernel\njust uses the kbuf ring data structure internally and users can read\nthe buffer contents)? If userspace initiates the setup of the kbuf\nring, going through IORING_REGISTER_MEM_REGION would be semantically\nthe same, except the buffer allocation by the kernel now happens\nbefore the ring is created and then later populated into the ring.\nuserspace would still need to make an mmap call to the region and the\nkernel could enforce that as read-only. But if userspace doesn't\ninitiate the setup, then going through IORING_REGISTER_MEM_REGION gets\nuglier.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged a concern about the complexity of kernel-managed buffer rings and proposed an alternative design, suggesting a straightforward kmbuf ring that uses the pbuf interface, to be followed by an additional interface for IORING_REGISTERED_MEM_REGIONS if needed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "overkill",
                "over-engineered"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "So i guess the flow would have to be:\na) user calls io_uring_register_region(&ring, &mem_region_reg) with\nmem_region_reg.region_uptr's size field set to the total buffer size\n(and mem_region_reg.flags read-only bit set if needed)\n     kernel allocates region\nb) user calls mmap() to get the address of the region. If read-only\nbit was set, it gets a read-only address\nc) user calls io_uring_register_buf_ring(&ring, &buf_reg, flags) with\nbuf_reg.flags |= IOU_PBUF_RING_KERNEL_MANAGED\n     kernel creates an empty kernel-managed ring. None of the buffers\nare populated\nd) user tells X subsystem to populate the ring starting from offset Z\nin the registered mem region\ne) on the kernel side, the subsystem populates the ring starting from\noffset Z, filling it up using the buf_size and ring_entries values\nthat the user registered the ring with in c)\n\nTo be completely honest, the more I look at this the more this feels\nlike overkill / over-engineered to me. I get that now the user can do\nthe PMD optimization, but does that actually lead to noticeable\nperformance benefits? It seems especially confusing with them going\nthrough the same pbuf ring interface but having totally different\nexpectations.\n\nWhat about adding a straightforward kmbuf ring that goes through the\npbuf interface (eg the design in this patchset) and then in the future\nadding an interface for pbuf rings (both kernel-managed and\nnon-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if\nusers end up needing/wanting to have their rings populated that way?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer noted that if an application is concerned about TLB pressure, it can simply round up the buffer size to a multiple of PTE levels instead of requiring kernel-managed buffer rings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested consideration for alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure.  But if the application cares that much about TLB pressure\nI'd just round up to nice multtiple of PTE levels.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer Christoph Hellwig asked for clarification on what 'pbuf' means in the context of kernel-managed buffer rings, indicating confusion about the terminology used and suggesting that it may be due to a lack of expertise in io_uring APIs.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "lack of clarity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Can you clarify what you mean with 'pbuf'?  The only fixed buffer API I\nknow is io_uring_register_buffers* which always takes user provided\nbuffers, so I have a hard time parsing what you're saying there.  But\nthat might just be sign that I'm no expert in io_uring APIs, and that\nweb searches have degraded to the point of not being very useful\nanymore.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "reviewer expressed confusion about the purpose of IORING_REGISTER_MEM_REGION, as it is described in the commit message and public documentation to be related to cqs (completion queues), but the reviewer's comment suggests otherwise",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "lack of clarity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IORING_REGISTER_MEM_REGION seems to be all about cqs from both your\ncommit message and the public documentation.  I'm confused.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "reviewer noted that their use case is unrelated to fuse, but rather focuses on traditional block and file system I/O",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "My use case is not about fuse, but good old block and file system\nI/O.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that io_uring_register_buffers() currently only pins memory, leaving it vulnerable to modification by other processes or devices, and suggested implementing a version of the function where the kernel provides read-only buffers mapped into the application's address space.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The idea is that the application tells the kernel that it wants to use\na fixed buffer pool for reads.  Right now the application does this\nusing io_uring_register_buffers().  The problem with that is that\nio_uring_register_buffers ends up just doing a pin of the memory,\nbut the application or, in case of shared memory, someone else could\nstill modify the memory.  If the underlying file system or storage\ndevice needs verify checksums, or worse rebuild data from parity\n(or uncompress), it needs to ensure that the memory it is operating\non can't be modified by someone else.\n\nSo I've been thinking of a version of io_uring_register_buffers where\nthe buffers are not provided by the application, but instead by the\nkernel and mapped into the application address space read-only for\na while, and I thought I could implement this on top of your series,\nbut I have to admit I haven't really looked into the details all\nthat much.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer noted that the PMD mapping is not a significant concern, and both AMD and ARM architectures have optimizations for contiguous PTEs.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear signal of approval or disapproval"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes.  The PMD mapping also is not that relevant.  Both AMD (implicit)\nand ARM (explicit) have optimizations for contiguous PTEs that are\nalmost as valuable.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer pointed out that the patch introduces a new io_uring uapi for kernel-managed buffer rings, but this is inflexible and requires a new uapi. They also questioned why fuse needs to use io_uring provided buffer rings when it can contain everything internally. Additionally, they expressed concerns about the lifetime of buffer memory being tied to the ring object.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "inflexibility",
                "lifetime concerns"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Registered, aka fixed, buffers are the ones you pass to\nIORING_OP_[READ,WRITE]_FIXED and some other requests. It's normally\ncreated by io_uring_register_buffers*() / IORING_REGISTER_BUFFERS*\nwith user memory, but there are special cases when it's installed\ninternally by other kernel components, e.g. ublk.\nThis series has nothing to do with them, and relevant parts of\nthe discussion here don't mention them either.\n\nProvided buffer rings, a.k.a pbuf rings, IORING_REGISTER_PBUF_RING\nis a kernel-user shared ring. The entries are user buffers\n{uaddr, size}. The user space adds entries, the kernel (io_uring\nrequests) consumes them and issues I/O using the user addresses.\nE.g. you can issue a IORING_OP_RECV request (+IOSQE_BUFFER_SELECT)\nand it'll grab a buffer from the ring instead of using sqe->addr.\n\npbuf rings, IORING_REGISTER_MEM_REGION, completion/submission\nqueues and all other kernel-user rings/etc. are internally based\non so called regions. All of them support both user allocated\nmemory and kernel allocations + mmap.\n\nThis series essentially creates provided buffer rings, where\n1. the ring now contains kernel addresses\n2. the ring itself is in-kernel only and not shared with user space\n3. it also allocates kernel buffers (as a region), populates the ring\n    with them, and allows mapping the buffers into the user space.\n\nFuse is doing both adding (kernel) buffers to the ring and consuming\nthem. At which point it's not clear:\n\n1. Why it even needs io_uring provided buffer rings, it can be all\n    contained in fuse. Maybe it's trying to reuse pbuf ring code as\n    basically an internal memory allocator, but then why expose buffer\n    rings as an io_uring uapi instead of keeping it internally.\n\n    That's also why I mentioned whether those buffers are supposed to\n    be used with other types of io_uring requests like recv, etc.\n\n2. Why making io_uring to allocate payload memory. The answer to which\n    is probably to reuse the region api with mmap and so on. And why\n    payload buffers are inseparably created together with the ring\n    and via a new io_uring uapi.\n\n    And yes, I believe in the current form it's inflexible, it requires\n    a new io_uring uapi. It requires the number of buffers to match\n    the number of ring entries, which are related but not the same\n    thing. You can't easily add more memory as it's bound to the ring\n    object. The buffer memory won't even have same lifetime as the\n    ring object -- allow using that km buffer ring with recv requests\n    and highly likely I'll most likely give you a way to crash the\n    kernel.\n\nBut hey, I'm tired. I don't have any beef here and am only trying\nto make it a bit cleaner and flexible for fuse in the first place\nwithout even questioning the I/O path. If everyone believes\neverything is right, just ask Jens to merge it.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer expressed concern that the kernel-managed buffer rings are being used for large payload buffers, which was not their original intention and may lead to memory waste.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "memory waste",
                "not intended use"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Think of it as an area of memory for kernel-user communication. Used\nfor syscall parameters passing to avoid copy_from_user, but I added\nit for a bunch of use cases. We'll hopefully get support at some\npoint for passing request arguments like struct iovec. BPF patches\nuse it for communication. I need to respin patches placing SQ/CQ onto\nit (avoid some memory waste).\n\nTbh, I never meant it nor io_uring regions to be used for huge\npayload buffers, but this series already uses regions for that.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov questioned the feasibility of kernel-managed buffer rings in io_uring without a kernel component returning buffers into the ring, and suggested that maybe an elaborate API is needed instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Then I'm confused. Take a look at the other reply, this series is\nabout buffer rings with kernel memory, it can't work without a kernel\ncomponent returning buffers into the ring, and io_uring doesn't do\nthat. But maybe you're thinking about adding some more elaborate API.\n\nIIUC, Joanne also wants to add support for fuse installing registered\nbuffers, which would allow zero-copy, but those got split out of\nthis series.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested a workaround for an issue, noting that the code should instead use the kernel-managed buffer rings for metadata like fuse headers and payloads, which would allow zero-copying.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "workaround",
                "zero copying"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Let's fix that then. For now, just work it around by wrapping\ninto a loop.\n\nBtw, I thought you're going to use it for metadata like some\nfuse headers and payloads would be zero copied by installing\nit as registered buffers.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested moving ring population from io_uring uapi to fuse, and using IORING_REGISTER_MEM_REGION to allocate memory for the ring",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The main point is disentangling memory allocation from ring\ncreation in the io_uring uapi, and moving ring population\ninto fuse instead of doing it at creation. And it'll still be\npopulated by the kernel (fuse), user space doesn't have access\nto the ring. IORING_REGISTER_MEM_REGION is just the easiest way\nto achieve that without any extra uapi.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the differences between kernel-managed buffer rings and regular buffer rings are minimal, except for special region path and embedded buffer allocations, but did not strongly object to making a separate opcode.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no strong objection",
                "open to alternative"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It appeared to me that they're different because of special\nregion path and embedded buffer allocations, and otherwise\ndifferences would be minimal. But if you think it's still\nbetter to be made as a separate opcode, I'm not opposing it,\ngo for it.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that the current implementation does not provide a clear control path for fuse to bind kernel-managed buffer rings, and suggested adding an io-uring command (FUSE_CMD_BIND_BUFFER_RING) to pass necessary parameters to bind the ring. They also questioned the need to expose the ring part as an io_uring uapi.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Not having patches using the functionality is inconvenient. How\nfuse looks up the buffer ring from io_uring? I could imagine you\nhave some control path io-uring command:\n\ncase FUSE_CMD_BIND_BUFFER_RING:\n\treturn bind_queue(params);\n\nThen you can pass all necessary parameters to it, pseudo code:\n\nstruct fuse_bind_kmbuf_ring_params {\n\tregion_id;\n\tbuf_ring_id;\n\t...\n};\n\nbind_queue(cmd, struct fuse_bind_kmbuf_ring_params *p)\n{\n\tregion = io_uring_get_region(cmd, p->region_id);\n\t// get exclusive access:\n\tbuf_ring = io_uring_get_buf_ring(cmd, p->buf_ring_id);\n\n\tif (!validate_buf_ring(buf_ring))\n\t\treturn NOTSUPPORTED;\n\n\tio_uring_pin(buf_ring);\n\tfuse_populate_buf_ring(buf_ring, region, ...);\n}\n\nDoes that match expectations? I don't think you even need\nthe ring part exposed as an io_uring uapi, tbh, as it\nstays completely in fuse and doesn't meaningfully interact\nwith the rest of io_uring.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "reviewer pointed out that the patch does not use IORING_REGISTER_MEM_REGION for kernel-managed buffer rings, instead opting for a separate region, and questioned whether buffers should be bound to the ring",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "technical disagreement"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That was about an argument for using IORING_REGISTER_MEM_REGION\ninstead a separate region. And it's separate from whether\nbuffers should be bound to the ring.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that when allocating huge pages for non-pow2 buffer sizes, the kernel may allocate a larger page than necessary, potentially wasting memory.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential memory waste"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I shouldn't affect you much since you have such large buffers,\nbut imagine the total allocation size is not being pow2, and\nthe kernel allocating it as a single folio. E.g. 3 buffers,\n0.5 MB each, total = 1.5MB, and the kernel allocates a 2MB\nhuge page.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the patch series does not address registered buffers and suggested separating kernel-managed buffer rings from io_uring, arguing that reusing buffer allocation would introduce unnecessary complexity.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested separation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There is nothing about registered buffers in this series. And even\nif you try to reuse buffer allocation out of it, it'll come with\na circular buffer you'll have no need for. And I'm pretty much\narguing about separating those for io_uring.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested reusing regions for allocations and mmap() operations, wrapping them into a registered buffer to avoid vmap'ing altogether.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, the easiest solution is to internally reuse regions for\nallocations and mmap()'ing and wrap it into a registered buffer.\nIt just need to make vmap'ing optional as it won't be needed.\n\n-- \nPavel Begunkov",
              "reply_to": "",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed concerns that the io_uring uapi should not be tied to fuse-specific requirements, such as uniform buffer sizes, matching ring size and number of buffers, and kernel-allocated buffers. He questioned why the total size is required at creation and how additional memory could be added at runtime.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "strong opinion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, it's mainly about not keeping payload buffers and rings in the same\nobject from the io_uring uapi perspective.\n\n1. If it's an io_uring uapi, it shouldn't be fuse specific or with\na bunch of use case specific expectations attached. Why does it\nrequire all buffers to be uniform in size? Why does it require\nthe ring size to match the number of buffers? Why does it require\nbuffers to be allocated by io_uring in the first place? Maybe some\nsubsystem got memory from somewhere else and wants to do use it\nwith io_uring. Why does it need to know the total size at creation,\nand what would you do if you want to add more memory at runtime\nwhile using the same ring?\n\n2. If it's meant to be fuse specific and _not_ used with other requests\nlike recv/read/etc., then what's the point of having it as an io_uring\nuapi? Which also adds additional trouble like the once you're solving\nwith pinning.\n\nIf it's supposed to be used with other requests, then buffers and\nrings will have different in-kernel lifetime expectations imposed\nby io_uring, so having them together won't even help with\nmanagement.\n\nI have a strong opinion about the memmap.c change. For the\nrest, if you believe it's fine, just send it out and let Jens\ndecide.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the patch's assumption about separating buffers from rings is unclear, specifically questioning what expectations are different between kernel-managed and user-visible buffer rings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "unclear expectation",
                "questioning assumptions"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It's predicated on separating buffers from rings, see above,\nand assuming that I'm not sure what expectations are different\napart from one being in-kernel with kernel addresses and the\nother user visible with user addresses.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern about wasted space in io_ring buffers by explaining the benefits of using a circular buffer, specifically for Christoph's use case and fuse's needs.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged benefit",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think the circular buffer will be useful for Christoph's use case in\nthe same way it'll be useful for fuse's. The read payload could be\ndifferently sized across requests, so it's a lot of wasted space to\nhave to allocate a buffer large enough to support the max-size request\nper entry in the io_ring. With using a circular buffer, buffers have a\nway to be shared across entries, which means we can significantly\nreduce how much memory needs to be allocated.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that Christoph's use case requires read-only buffers and explained that their kernel-managed buffer rings will provide similar memory benefits, implying no immediate fix is planned but the approach remains aligned.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "aligned",
                "no immediate fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "(resending because I hit reply instead of reply-all)\n\nI think we have the exact same use case, except your buffers need to\nbe read-only. I think your use case benefits from the same memory wins\nwe'll get with incremental buffer consumption, which is the primary\nreason fuse is using a bufring instead of fixed buffers.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Christoph's concern about making the mmap call return a read-only mapping by proposing to pass a read-only flag from userspace and adding a patch to this series, suggesting that the patch will need further revision.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to add patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think you can and it'll be very easy to do so. All that would be\nneeded is to pass in a read-only flag from the userspace side when it\nregisters the bufring, and then when userspace makes the mmap call to\nthe bufring, the kernel checks if that read-only flag is set on the\nbufring and if so returns a read-only mapping. I'm happy to add that\npatch to this series if that would make things easier for you. The\nio_uring_register_buffers() api registers fixed buffers (which have to\nbe user-allocated memory) so you would need to go through the\nio_uring_register_buf_ring() api once kmbufs are squashed into the\npbuf interface.\n\nWith going through IORING_MEM_REGION, this would work for your use\ncase as well. The user would have to register the mem region with\nio_uring_register_region() and pass in a read-only flag, and then the\nkernel will allocate the memory region. Then userspace would mmap the\nmemory region and on the kernel side, it would set the mapping to be\nread-only. When the kmbufring then gets registered, the buffers in it\nwill be empty. The filesystem will then have to populate the buffers\nin it from the mem region that was previously registered.\n\nThanks,\nJoanne",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Bernd Schubert",
              "summary": "Reviewer questioned the purpose of sharing buffers across io_uring entries, suggesting it would only reduce the ring size and not provide any benefits.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Dunno, what we actually want is requests of multiple sizes. Sharing\nbuffers across entries sounds like just reducing the ring size - I\npersonally don't see the point here.\n\n\nThanks,\nBernd",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarified that sharing buffers across io_uring entries means allowing concurrent access to different parts of a single buffer, not necessarily the entire buffer.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"sharing buffers across entries\" what I mean is different regions\nof the buffer can now be used concurrently by multiple entries.\n\nThanks,\nJoanne",
              "reply_to": "Bernd Schubert",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing Pavel Begunkov's concern about the need for kernel-managed buffer rings, specifically in the context of fuse's use case where buffers are shared between the kernel and a server. The author explains that the kernel needs to control when buffers get recycled back into the ring because the server writes data back to the kernel in those buffers after submitting an sqe.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The most important part and the whole reason fuse needs the buffer\nring to be kernel-managed is because the kernel needs to control when\nbuffers get recycled back into the ring. For fuse's use case, the\nbuffer is used for passing data between the kernel and the server. We\ncan't have the server recycle the buffer because the server writes\nback data to the kernel in that buffer when it submits the sqe. After\nfuse receives the sqe and reads the reply from the server, it then\nneeds to recycle that buffer back into the ring so it can be reused\nfor a future cqe (eg sending a future request).",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern about userspace applications using kernel-managed buffer rings for operations other than io_uring, explaining that they are used for reading or writing contents from/to a locally-backed file.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On the userspace/server side, it uses the buffers for other io-uring\noperations (eg reading or writing the contents from/to a\nlocally-backed file).",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addresses Pavel Begunkov's concern about adding complexity by using a registered memory region, explaining that they believe most use cases of kmbufs do not benefit from the optimizations available when using such regions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges need for fix",
                "promises no noticeable benefit"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "My main motivation for this is simplicity. I see (and thanks for\nexplaining) that using a registered mem region allows the use of some\noptimizations (the only one I know of right now is the PMD one you\nmentioned but maybe there's more I'm missing) that could be useful for\nsome workloads, but I don't think (and this could just be my lack of\nunderstanding of what more optimizations there are) most use cases of\nkmbufs benefit from those optimizations, so to me it feels like we're\nadding non-trivial complexity for no noticeable benefit.\n\nI feel like we get the best of both worlds by letting users have both:\nthe simple kernel-managed pbuf where the kernel allocates the buffers\nand the buffers are tied to the lifecycle of the ring, and the more\nadvanced kernel-managed pbuf where buffers are tied to a registered\nmemory region that the subsystem is responsible for later populating\nthe ring with.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addresses Pavel Begunkov's concern that combining kernel-managed buffer rings (kmbufs) and user-provided buffer rings (pbufs) in a single UAPI is confusing, as they have different expectations and behaviors. The author explains their reasoning for separating kmbufs into its own UAPI, but acknowledges that Pavel likely knows better what's best for io-uring users. As a result, the author plans to restructure v2 to put kmbufs through the pbuf UAPI.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledges feedback",
                "explains reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "imo it felt cleaner to have a new uapi for it because kmbufs and pbufs\nhave different expectations and behaviors (eg pbufs only work with\nuser-provided buffers and requires userspace to populate the ring\nbefore using it, whereas for kmbufs the kernel allocates the buffers\nand populates it for you; pbufs require userspace to recycle back the\nbuffer, whereas for kmbufs the kernel is the one in control of\nrecycling) and from the user pov it seemed confusing to have kmbufs as\npart of the pbuf ring uapi, instead of separating it out as a\ndifferent type of ringbuffer with a different expectation and\nbehavior. I was trying to make the point that combining the interface\nif we go with IORING_MEM_REGION gets even more confusing because now\npbufs that are kernel-managed are also empty at initialization and\nonly can point to areas inside a registered mem region and the\nresponsibility of populating it is now on whatever subsystem is using\nit.\n\nI still have this opinion but I also think in general, you likely know\nbetter than I do what kind of io-uring uapi is best for io-uring's\nusers. For v2 I'll have kmbufs go through the pbuf uapi.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author responded to feedback about the purpose of having a ring entry with no buffer associated, suggesting it can be fixed by passing in the number of buffers from the uapi for kernel-managed pbuf rings.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm not really seeing what the purpose of having a ring entry with no\nbuffer associated with it is. In the existing code for non-kernel\nmanaged pbuf rings, there's the same tie between reg->ring_entries\nbeing used as the marker for how many buffers the ring supports. But\nif the number of buffers should be different than the number of ring\nentries, this can be easily fixed by passing in the number of buffers\nfrom the uapi for kernel-managed pbuf rings.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that kernel-managed buffer rings require upfront allocation of memory for the lifetime of the ring, and highlighted difficulties in dynamically adding more memory to the registered mem region.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a limitation",
                "highlighted a challenge"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "To play devil's advocate, we also can't easily add more memory to the\nmem region once it's been registered. I think there's also a worse\npenalty where the user needs to know upfront how much memory to\nallocate for the mem region for the lifetime of the ring, which imo\nmay be hard to do (eg if a kernel-managed buf ring only needs to be\nregistered for some code paths and not others, the mem region\nregistration would still have to allocate the memory a potential kbuf\nring would use).",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern that buffer memory has a different lifetime than the ring object by explaining that the buffers are only freed when the ring is freed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I'm a bit confused by this part. The buffer memory does have the same\nlifetime as the ring object, no? The buffers only get freed when the\nring itself is freed.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author asked reviewer if they are open to having both a simple kernel-managed pbuf interface and one that goes through a registered memory region, indicating flexibility in the design.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "flexibility",
                "openness"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I appreciate you looking at this and giving your feedback and insight.\nThank you for doing so. I don't want to merge in something you're\nunhappy with.\n\nAre you open to having support for both a simple kernel-managed pbuf\ninterface and later on if/when the need arises, a kernel-managed pbuf\ninterface that goes through a registered memory region? If the answer\nis no, then I'll make the change to have kmbufs go through the\nregistered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that buffer rings are not suitable for storage read/write requests because they bind to a buffer immediately, which is in contrast to other types of requests like recv where io_uring polls the socket before taking a buffer from the ring. Additionally, the reviewer pointed out that someone needs to return buffers back into the private kernel ring, and this responsibility is currently assumed to be handled by the fuse driver for this patchset, but there is no clear solution for normal rw requests.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "problematic design"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Provided buffer rings are not useful for storage read/write requests\nbecause they bind to a buffer right away, that's in contrast to some\nrecv request, where io_uring will first poll the socket to confirm\nthe data is there, and only then take a buffer from the buffer ring\nand copy into it. With storage rw it makes more sense to specify\nthe buffer directly gain control over where exactly data lands\nIOW, instead of the usual \"read data into a given pointer\" request\nsemantics like what read(2) gives you, buffer rings are rather\n\"read data somewhere and return a pointer to where you placed it\".\n\nAnother problem is that someone needs to return buffers back into\nthe buffer ring, and it's a kernel private ring. For this patchset\nit's assumed the fuse driver is going to be doing that, but there\nis no one for normal rw requests.",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested using IORING_MEM_REGION to provide buffers without extra semantics, or creating a standalone registered buffer extension that reuses regions internally.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes. You only need buffers, and it'll be better to base on sth that\ngives you buffers/memory without extra semantics, i.e.\nIORING_MEM_REGION. Or it can be a standalone registered buffer\nextension, likely reusing regions internally. That might even yield\na finer API.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov asked whether kernel-managed buffer rings can be used with other requests, specifically IORING_OP_RECV with IOSQE_BUFFER_SELECT set and bgid specifying the kernel-managed buffer ring.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification_request"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Oops, typo. I was asking whether the buffer rings (not buffers) are\nsupposed to be used with other requests. E.g. submitting a\nIORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying\nyour kernel-managed buffer ring.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov raised two separate concerns: first, he suggests separating buffers from buffer rings in the io_uring user API to avoid making them inseparable; second, he proposes allowing optional user memory for buffer creation by reusing the region abstraction and passing an argument while creating a region.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no objection",
                "trivial implementation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There are two separate arguments. The first is about not making buffers\ninseparable from buffer rings in the io_uring user API. Whether it's\nIORING_REGISTER_MEM_REGION or something else is not that important.\nI have no objection if it's a part of fuse instead though, e.g. if\nfuse binds two objects together when you register it with fuse, or even\nif fuse create a buffer ring internally (assuming it doesn't indirectly\nleak into io_uring uapi).\n\nAnd the second was about optionally allowing user memory for buffer\ncreation as you're reusing the region abstraction. You can find pros\nand cons for both modes, and funnily enough, SQ/CQ were first kernel\nallocated and then people asked for backing it by user memory, and IIRC\nit was in the reverse order for pbuf rings.\n\nImplementing this is trivial as well, you just need to pass an argument\nwhile creating a region. All new region users use struct\nio_uring_region_desc for uapi and forward it to io_create_region()\nwithout caring if it's user or kernel allocated memory.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov questioned the necessity of introducing an io_uring API for kernel-managed buffer rings, suggesting that it could be simpler to implement in fuse or as an implementation detail within io_uring",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The stress is on why it's an _io_uring_ API. It doesn't matter to me\nwhether it's a separate opcode or not. Currently, buffer rings don't give\nyou anything that can't be pure fuse, and it might be simpler to have\nit implemented in fuse than binding to some io_uring object. Or it could\ncreate buffer rings internally to reuse code but it doesn't become an\nio_uring uapi but rather implementation detail. And that predicates on\nwhether km rings are intended to be used with other / non-fuse requests.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed concern that the kernel-managed buffer rings (km rings) API is too specific to the fuse use case and should be more reusable for other users, suggesting a middle ground where km rings can be registered together with memory as a pure region without a notion of a buffer.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I believe the source of disagreement is that you're thinking\nabout how it's going to look like for fuse specifically, and I\nbelieve you that it'll be nicer for the fuse use case. However,\non the other hand it's an io_uring uapi, and if it is an io_uring\nuapi, we need reusable blocks that are not specific to particular\nusers.\n\nIf it km rings has to stay an io_uring uapi, I guess a middle\nground would be to allow registering km rings together with memory,\nbut make it a pure region without a notion of a buffer, and let\nfuse to chunk it. Later, we can make payload memory allocation\noptional.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the patch introduces a fuse-specific API under the guise of a generic io_uring API, citing various assumptions made throughout the codebase.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Right, intentionally so, because otherwise it's a fuse uapi that\npretends to be a generic io_uring uapi but it's not because of\nall assumptions in different places.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer noted that the current implementation only specifies the buffer ring depth but not the amount of memory allocated by userspace, which could lead to inconsistencies between the two values. They suggested considering scenarios where more memory is added at runtime or when buffers need to be recycled.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Not really, it tells the buffer ring depth but says nothing about\nhow much memory user space allocated and how it's pushed. It's a\nreasonable default but they could be different. For example, if you\nexpect adding more memory at runtime, you might create the buffer\nring a bit larger. Or when server processing takes a while and you\ncan't recycle until it finishes, you might have more buffers than\nyou need ring entries. Or you might might decide to split buffers\nand as you mentioned incremental consumption, which is an entire\nseparate topic because it doesn't do de-fragmentation and you'd\nneed to have it in fuse, just like user space does with pbufs.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer suggests that instead of passing the number of buffers to io_uring, the kernel should allocate a large chunk of memory and let fuse manage the buffer allocation and splitting.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "My entire point is that we're making lots of assumptions for io_uring\nuapi, and if it's moved to fuse because it knows better what it\nneeds, it should be a win.\n\nIOW, it sounds better if instead of passing the number of buffers to\nio_uring, you just ask it to create a large chunk of memory, and then\nfuse chunks it up and puts into the ring.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov agreed that additional memory is needed but suggested that it does not necessarily require a new IORING_REGISTER_MEM_REGION call.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "agreed",
                "suggested"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I agree, and you'd need something new in either case to add more\nmemory, and it doesn't need to be IORING_REGISTER_MEM_REGION\nspecifically.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that unregistering a kernel-managed buffer ring does not guarantee that there are no inflight requests using buffers from the ring, and requested synchronization with all other io_uring requests.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "synchronization issue",
                "inflight requests"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Unregistering a buffer ring doesn't guarantee that there are no\ninflight requests that are still using buffers that came out of\nthe buffer ring. The fuse driver can wait/terminate its requests\nbefore unregisteration, but allow userspace issued IORING_OP_RECV\nto use this km buffer ring, and you'll need to somehow synchronise\nwith all other io_uring requests.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that a fix is needed for the issue of buffer ring pinning/unpinning and agreed to restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for fix",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sorry, I submitted v2 last night thinking the conversation on this\nthread had died. After reading through your reply, I'll modify v2.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the buffer rings being used only for IORING_OP_READ/WRITE_FIXED operations, explained that they are intended to be used with other io-uring requests and highlighted the performance benefits of avoiding per-i/o page pinning overhead costs.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "clarified intention",
                "highlighted performance benefit"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes the buffer rings are intended to be used with other io-uring\nrequests. The ideal scenario is that the user can then do the\nequivalent of IORING_OP_READ/WRITE_FIXED operations on the\nkernel-managed buffers and avoid the per-i/o page pinning overhead\ncosts.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong addressed Pavel Begunkov's concern about the design of kernel-managed buffer rings, agreeing that having buffers owned by the ring and tied to its lifetime is more generically useful. She proposed a new design where users create a memory region for io-uring, mmap it, and then pass an offset into the region to the subsystem, which creates a locally managed buffer ring and adds buffers from the mem region.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "proposed new design"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree 100%. The api we add should be what's best for io-uring, not fuse.\n\nFor the majority of use cases, it seemed to me that having the buffers\nseparated from the buffer rings didn't yield perceptible benefits but\nadded complexity and more restrictions like having to statically know\nup front how big the mem region needs to be across the lifetime of the\nio-uring for anything the io-uring might use the mem region for. It\nseems more generically useful as a concept to have the buffers owned\nby the ring and tied to the lifetime of the ring. I like how with this\ndesign everything is self-contained and multiple subsystems can use it\nwithout having to reimplement functionality locally in the subsystem.\nOn the other hand, I see your point about how it might be something\nusers want in the future if they want complete control over which\nparts of the mem region get used as the backing buffers to do stuff\nlike PMD optimizations.\n\nI think this is a matter of opinion/preference and I think in general\nfor anything io-uring related, yours should take precedence.\n\nWith it going through a mem region, I don't think it should even go\nthrough the \"pbuf ring\" interface then if it's not going to specify\nthe number of entries and buffer sizes upfront, if support is added\nfor io-uring normal requests (eg IORING_OP_READ/WRITE) to use the\nbacking pages from a memory region and if we're able to guarantee that\nthe registered memory region will never be able to be unregistered by\nthe user. I think if we repurpose the\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n};\n\nfields in the struct io_uring_sqe to\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n  __u64 offset; /* offset into registered mem region */\n};\n\nand add some IOSQE_ flag to indicate it should find the pages from the\nregistered mem region, then that should work for normal requests.\nWhere on the kernel side, it looks up the associated pages stored in\nthe io_mapped_region's pages array for the offset passed in.\n\nRight now there's only a uapi to register a memory region and none to\nunregister one. Is it guaranteed that io-uring will never add\nsomething in the future that will let userspace unregister the memory\nregion or at least unregister it while it's being used (eg if we add\nfuture refcounting to it to track active uses of it)?\n\nIf so, then end-to-end, with it going through the mem region, it would\nbe something like:\n* user creates a mem region for the io-uring\n* user mmaps the mem region\n* user passes in offset into region, length of each buffer, and number\nof entries in the ring to the subsystem\n* subsystem creates a locally managed bufring and adds buffers to that\nring from the mem region\n* on the cqe side, it sends the buffer id of the registered mem region\nthrough the same \"IORING_CQE_F_BUFFER |  (buf_id <<\nIORING_CQE_BUFFER_SHIFT)\" mechanism\n\nDoes this design match what you had in mind / prefer?\n\nI think the above works for Christoph's use case too (as his and my\nuse case are the same) but if not, please let me know.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov questioned whether kernel-managed buffer rings should be exposed in the io_uring uapi, specifically asking if a server or user space program can issue I/O requests that consume buffers from the km ring without fuse kernel code involved. He requested clarification on this point to inform the decision of exposing km rings as io_uring uapi.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested clarification",
                "questioning exposure in uapi"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You mention OP_READ_FIXED and below agreed not exposing km rings\nan io_uring uapi, which makes me believe we're still talking about\ndifferent things.\n\nCorrect me if I'm wrong. Currently, only fuse cmds use the buffer\nring itself, I'm not talking about buffer, i.e. fuse cmds consume\nentries from the ring (!!! that's the part I'm interested in), then\nprocess them and tell the server \"this offset in the region has user\ndata to process or should be populated with data\".\n\nNaturally, the server should be able to use the buffers to issue\nsome I/O and process it in other ways, whether it's a normal\nOP_READ to which you pass the user space address (you can since\nit's mmap()'ed by the server) or something else is important but\na separate question than the one I'm trying to understand.\n\nSo I'm asking whether you expect that a server or other user space\nprogram should be able to issue a READ_OP_RECV, READ_OP_READ or any\nother similar request, which would consume buffers/entries from the\nkm ring without any fuse kernel code involved? Do you have some\nuse case for that in mind?\n\nUnderstanding that is the key in deciding whether km rings should\nbe exposed as io_uring uapi or not, regardless of where buffers\nto populate the ring come from.\n\n...",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov suggested reusing registered buffers instead of introducing a new mechanism for kernel-managed buffer rings, as it would be more efficient and similar to zero-copy internally registered buffers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "So you already can do all that using the mmap()'ed region user\npointer, and you just want it to be more efficient, right?\nFor that let's just reuse registered buffers, we don't need a\nnew mechanism that needs to be propagated to all request types.\nAnd registered buffer are already optimised for I/O in a bunch\nof ways. And as a bonus, it'll be similar to the zero-copy\ninternally registered buffers if you still plan to add them.\n\nThe simplest way to do that is to create a registered buffer out\nof the mmap'ed region pointer. Pseudo code:\n\n// mmap'ed if it's kernel allocated.\n{region_ptr, region_size} = create_region();\n\nstruct iovec iov;\niov.iov_base = region_ptr;\niov.iov_len = region_size;\nio_uring_register_buffers(ring, &iov, 1);\n\n// later instead of this:\nptr = region_ptr + off;\nio_uring_prep_read(sqe, fd, ptr, ...);\n\n// you use registered buffers as usual:\nio_uring_prep_read_fixed(sqe, fd, off, regbuf_idx, ...);\n\n\nIIRC the registration would fail because it doesn't allow file\nbacked pages, but it should be fine if we know it's io_uring\nregion memory, so that would need to be patched.\n\nThere might be a bunch of other ways you can do that like\ncreate a kernel allocated registered buffer like what Cristoph\nwants, and then register it as a region. Or allow creating\nregistered buffers out of a region. etc.\n\nI wanted to unify registered buffers and regions internally\nat some point, but then drifted away from active io_uring core\ninfrastructure development, so I guess that could've been useful.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that kernel-managed buffer rings would require handling page references and/or pinning regions, which could be complex if registered buffers are used instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "complexity",
                "additional work"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Let's talk about it when it's needed or something changes, but if\nyou do registered buffers instead as per above, they'll be holding\npage references and or have to pin the region in some other way.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested adding a liburing helper to handle mmap'ing for the fuse server, eliminating its need to directly interact with mmap.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, we should just add a liburing helper, so that fuse server\ndoesn't need to deal with mmap'ing.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed conditional approval, requiring confirmation that the patch allows for all desired fast path optimizations.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "conditional approval",
                "optimization requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That's sounds clean to me _if_ it allows you to achieve all\n(fast path) optimisations you want to have. I hope it does?\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author acknowledged a potential optimization for fuse servers, but noted that kernel-managed buffer rings are not exclusive to fuse and could be useful in other scenarios.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a potential optimization",
                "noted multiple use cases"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for clarifying your question. Yes, this would be a useful\noptimization in the future for fuse servers with certain workload\ncharacteristics (eg network-backed servers with high concurrency and\nunpredictable latencies). I don't think the concept of kmbufrings is\nexclusively fuse-specific though (for example, Christoph's use case\nbeing a recent instance); I think other subsystems/users that'll use\nkmbuf rings would also generically find it useful to have the option\nof READ_OP_RECV/READ_OP_READ operating directly on the ring.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author expressed concerns about increased complexity and convoluted interface in the patch, questioning the need for tying together concepts that were previously isolated. They asked why native support for memory regions wasn't added instead of introducing an extra layer of indirection.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning the design",
                "expressed concerns about complexity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I feel like this design makes the interface more convoluted and now\nmuddies different concepts together by adding new complexity /\nrelationships between them whereas they were otherwise cleanly\nisolated. Maybe I'm just not seeing/understanding the overarching\nvision for why conceptually it makes sense for them to be tied\ntogether besides as a mechanism to tell io-uring requests where to\ncopy from by reusing what exists for fixed buffer ids. There's more\ncomplexity now on the kernel side (eg having to detect if the buffer\npassed in is kernel-allocated to know whether to pin the pages /\ncharge it against the user's RLIMIT_MEMLOCK limit) but I'm not\nunderstanding what we gain from it. I got the sense from your previous\ncomments that memory regions are the de facto way to go and should be\ndecoupled from other structures, so if that's the case, why doesn't it\nmake sense for io-uring to add native support for using memory regions\nfor io-uring requests? I feel like from the userspace side it makes\nthings more confusing with this extra layer of indirection that now\nhas to go through a fixed buffer.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel's concern that kernel-managed buffer rings require the caller to register memory regions as fixed buffers, explaining that this cannot be guaranteed and proposing two possible solutions: adding pinning to registered memory regions or introducing extra overhead for every I/O operation.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a limitation",
                "proposed potential fixes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I don't think we can guarantee that the caller will register the\nmemory region as a fixed buffer (eg if it doesn't need/want to use the\nbuffer for normal io-uring requests). On the kernel side, the internal\nbuffer entry uses the kaddr of the registered memory region buffer for\nany memcpys. If it's not guaranteed that registered memory regions\npersist for the lifetime of the ring, there'll have to be extra\noverhead for every I/O (eg grab the io-uring lock, checking if the mem\nregion is still registered, grab a refcount to that mem region, unlock\nthe ring, do the memcpy to the kaddr, then grab the io-uring lock\nagain, decrement the refcount, and unlock). Or I guess we could add\npinning to a registered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Johannes Weiner",
      "primary_email": "hannes@cmpxchg.org",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joshua Hahn",
      "primary_email": "joshua.hahnjy@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "JP Kobryn",
      "primary_email": "inwardvessel@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Kiryl Shutsemau",
      "primary_email": "kas@kernel.org",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Leo Martins",
      "primary_email": "loemra.dev@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Mark Harmstone",
      "primary_email": "mark@harmstone.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix chunk offset error message in check_dev_extent_item()",
          "message_id": "20260220113013.30254-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260220113013.30254-1-mark@harmstone.com/",
          "date": "2026-02-20T11:30:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-20",
          "patch_summary": "The patch fixes a copy-paste bug in the error message of check_dev_extent_item() by reporting the correct offset instead of objectid.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Qu Wenruo",
              "summary": "Approved the patch with a Reviewed-by tag.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "APPROVED"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\n 2026/2/20 22:00, Mark Harmstone :\n> Fix a copy-paste bug in an error message in check_dev_extent_item():\n> we're reporting an incorrect offset, but actually printing the objectid.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 008e2512dc56 (\"btrfs: tree-checker: add dev extent item checks\")\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\n\nThanks,\nQu\n\n> ---\n>   fs/btrfs/tree-checker.c | 2 +-\n>   1 file changed, 1 insertion(+), 1 deletion(-)\n> \n> diff --git a/fs/btrfs/tree-checker.c b/fs/btrfs/tree-checker.c\n> index ac4c4573ee39..133510f99fc5 100644\n> --- a/fs/btrfs/tree-checker.c\n> +++ b/fs/btrfs/tree-checker.c\n> @@ -1899,7 +1899,7 @@ static int check_dev_extent_item(const struct extent_buffer *leaf,\n>   \t\t\t\t sectorsize))) {\n>   \t\tgeneric_err(leaf, slot,\n>   \t\t\t    \"invalid dev extent chunk offset, has %llu not aligned to %u\",\n> -\t\t\t    btrfs_dev_extent_chunk_objectid(leaf, de),\n> +\t\t\t    btrfs_dev_extent_chunk_offset(leaf, de),\n>   \t\t\t    sectorsize);\n>   \t\treturn -EUCLEAN;\n>   \t}\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Nhat Pham",
      "primary_email": "nphamcs@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Rik van Riel",
      "primary_email": "riel@surriel.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Shakeel Butt",
      "primary_email": "shakeel.butt@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH v2 0/5] mm/swap, memcg: Introduce swap tiers for cgroup based swap control",
          "message_id": "20260221163043.GA35350@shakeel.butt@linux.dev",
          "url": "https://lore.kernel.org/all/20260221163043.GA35350@shakeel.butt@linux.dev/",
          "date": "2026-02-21T17:44:14Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about the effective tiers being calculated separately from the configured tiers, explaining that this is done to respect the cgroup hierarchy and allowing for flexibility in tier configuration changes. The author confirmed that cgroups do not pin swap tiers, similar to the `cpuset` controller.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch integrates the swap tier infrastructure with cgroup,\nenabling the selection of specific swap devices per cgroup by\nconfiguring allowed swap tiers.\n\nThe new `memory.swap.tiers` interface controls allowed swap tiers via a mask.\nBy default, the mask is set to include all tiers, allowing specific tiers to\nbe excluded or restored. Note that effective tiers are calculated separately\nusing a dedicated mask to respect the cgroup hierarchy. Consequently,\nconfigured tiers may differ from effective ones, as they must be a subset\nof the parent's.\n\nNote that cgroups do not pin swap tiers. This is similar to the\n`cpuset` controller, which does not prevent CPU hotplug. This\napproach ensures flexibility by allowing tier configuration changes\nregardless of cgroup usage.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n Documentation/admin-guide/cgroup-v2.rst | 27 +++++++++\n include/linux/memcontrol.h              |  3 +-\n mm/memcontrol.c                         | 80 +++++++++++++++++++++++++\n mm/swap_tier.c                          | 66 ++++++++++++++++++++\n mm/swap_tier.h                          | 21 +++++++\n mm/swapfile.c                           |  5 ++\n 6 files changed, 201 insertions(+), 1 deletion(-)\n\ndiff --git a/Documentation/admin-guide/cgroup-v2.rst b/Documentation/admin-guide/cgroup-v2.rst\nindex 7f5b59d95fce..776a908ce1b9 100644\n--- a/Documentation/admin-guide/cgroup-v2.rst\n+++ b/Documentation/admin-guide/cgroup-v2.rst\n@@ -1848,6 +1848,33 @@ The following nested keys are defined.\n \tSwap usage hard limit.  If a cgroup's swap usage reaches this\n \tlimit, anonymous memory of the cgroup will not be swapped out.\n \n+  memory.swap.tiers\n+        A read-write nested-keyed file which exists on non-root\n+        cgroups. The default is to enable all tiers.\n+\n+        This interface allows selecting which swap tiers a cgroup can\n+        use for swapping out memory.\n+\n+        The effective tiers are inherited from the parent. Only tiers\n+        effective in the parent can be effective in the child. However,\n+        the child can explicitly disable tiers allowed by the parent.\n+\n+        When read, the file shows two lines:\n+          - The first line shows the operation string that was\n+            written to this file.\n+          - The second line shows the effective operation after\n+            merging with parent settings.\n+\n+        When writing, the format is:\n+          (+/-)(TIER_NAME) (+/-)(TIER_NAME) ...\n+\n+        Valid tier names are those configured in\n+        /sys/kernel/mm/swap/tiers.\n+\n+        Each tier can be prefixed with:\n+          +    Enable this tier\n+          -    Disable this tier\n+\n   memory.swap.events\n \tA read-only flat-keyed file which exists on non-root cgroups.\n \tThe following entries are defined.  Unless specified\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex b6c82c8f73e1..542bee1b5f60 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -283,7 +283,8 @@ struct mem_cgroup {\n \t/* per-memcg mm_struct list */\n \tstruct lru_gen_mm_list mm_list;\n #endif\n-\n+\tint tier_mask;\n+\tint tier_effective_mask;\n #ifdef CONFIG_MEMCG_V1\n \t/* Legacy consumer-oriented counters */\n \tstruct page_counter kmem;\t\t/* v1 only */\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 007413a53b45..c0a0a957a630 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -68,6 +68,7 @@\n #include <net/ip.h>\n #include \"slab.h\"\n #include \"memcontrol-v1.h\"\n+#include \"swap_tier.h\"\n \n #include <linux/uaccess.h>\n \n@@ -3691,6 +3692,7 @@ static void mem_cgroup_free(struct mem_cgroup *memcg)\n {\n \tlru_gen_exit_memcg(memcg);\n \tmemcg_wb_domain_exit(memcg);\n+\tswap_tiers_memcg_sync_mask(memcg);\n \t__mem_cgroup_free(memcg);\n }\n \n@@ -3792,6 +3794,9 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \tWRITE_ONCE(memcg->zswap_writeback, true);\n #endif\n \tpage_counter_set_high(&memcg->swap, PAGE_COUNTER_MAX);\n+\tmemcg->tier_mask = TIER_ALL_MASK;\n+\tswap_tiers_memcg_inherit_mask(memcg, parent);\n+\n \tif (parent) {\n \t\tWRITE_ONCE(memcg->swappiness, mem_cgroup_swappiness(parent));\n \n@@ -5352,6 +5357,75 @@ static int swap_events_show(struct seq_file *m, void *v)\n \treturn 0;\n }\n \n+static int swap_tier_show(struct seq_file *m, void *v)\n+{\n+\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);\n+\n+\tswap_tiers_mask_show(m, memcg->tier_mask);\n+\tswap_tiers_mask_show(m, memcg->tier_effective_mask);\n+\n+\treturn 0;\n+}\n+\n+static ssize_t swap_tier_write(struct kernfs_open_file *of,\n+\t\t\t\tchar *buf, size_t nbytes, loff_t off)\n+{\n+\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n+\tchar *pos, *token;\n+\tint ret = 0;\n+\n+\tpos = strstrip(buf);\n+\n+\tspin_lock(&swap_tier_lock);\n+\tif (!*pos) {\n+\t\tmemcg->tier_mask = TIER_ALL_MASK;\n+\t\tgoto sync;\n+\t}\n+\n+\twhile ((token = strsep(&pos, \" \\t\\n\")) != NULL) {\n+\t\tint mask;\n+\n+\t\tif (!*token)\n+\t\t\tcontinue;\n+\n+\t\tif (token[0] != '-' && token[0] != '+') {\n+\t\t\tret = -EINVAL;\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\tmask = swap_tiers_mask_lookup(token+1);\n+\t\tif (!mask) {\n+\t\t\tret = -EINVAL;\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\t/*\n+\t\t * if child already set, cannot add that tiers for hierarch mismatching.\n+\t\t * parent compatible, child must respect parent selected swap device.\n+\t\t */\n+\t\tswitch (token[0]) {\n+\t\tcase '-':\n+\t\t\tmemcg->tier_mask &= ~mask;\n+\t\t\tbreak;\n+\t\tcase '+':\n+\t\t\tmemcg->tier_mask |= mask;\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tret = -EINVAL;\n+\t\t\tbreak;\n+\t\t}\n+\n+\t\tif (ret)\n+\t\t\tgoto err;\n+\t}\n+\n+sync:\n+\t__swap_tiers_memcg_sync_mask(memcg);\n+err:\n+\tspin_unlock(&swap_tier_lock);\n+\treturn ret ? ret : nbytes;\n+}\n+\n static struct cftype swap_files[] = {\n \t{\n \t\t.name = \"swap.current\",\n@@ -5384,6 +5458,12 @@ static struct cftype swap_files[] = {\n \t\t.file_offset = offsetof(struct mem_cgroup, swap_events_file),\n \t\t.seq_show = swap_events_show,\n \t},\n+\t{\n+\t\t.name = \"swap.tiers\",\n+\t\t.flags = CFTYPE_NOT_ON_ROOT,\n+\t\t.seq_show = swap_tier_show,\n+\t\t.write = swap_tier_write,\n+\t},\n \t{ }\t/* terminate */\n };\n \ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nindex d90f6eccb908..e860c87292e2 100644\n--- a/mm/swap_tier.c\n+++ b/mm/swap_tier.c\n@@ -384,3 +384,69 @@ bool swap_tiers_update(void)\n \n \treturn true;\n }\n+\n+void swap_tiers_mask_show(struct seq_file *m, int mask)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tspin_lock(&swap_tier_lock);\n+\tfor_each_active_tier(tier) {\n+\t\tif (mask & TIER_MASK(tier))\n+\t\t\tseq_printf(m, \"%s \", tier->name);\n+\t}\n+\tspin_unlock(&swap_tier_lock);\n+\tseq_puts(m, \"\\n\");\n+}\n+\n+int swap_tiers_mask_lookup(const char *name)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (!strcmp(name, tier->name))\n+\t\t\treturn TIER_MASK(tier);\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static void __swap_tier_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent)\n+{\n+\tint effective_mask\n+\t\t= parent ? parent->tier_effective_mask : TIER_ALL_MASK;\n+\n+\tmemcg->tier_effective_mask\n+\t\t= effective_mask & memcg->tier_mask;\n+}\n+\n+void swap_tiers_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent)\n+{\n+\tspin_lock(&swap_tier_lock);\n+\t__swap_tier_memcg_inherit_mask(memcg, parent);\n+\tspin_unlock(&swap_tier_lock);\n+}\n+\n+void __swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg)\n+{\n+\tstruct mem_cgroup *child;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tif (memcg == root_mem_cgroup)\n+\t\treturn;\n+\n+\tfor_each_mem_cgroup_tree(child, memcg)\n+\t\t__swap_tier_memcg_inherit_mask(child, parent_mem_cgroup(child));\n+}\n+\n+void swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg)\n+{\n+\tspin_lock(&swap_tier_lock);\n+\tmemcg->tier_mask = TIER_ALL_MASK;\n+\t__swap_tiers_memcg_sync_mask(memcg);\n+\tspin_unlock(&swap_tier_lock);\n+}\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nindex de81d540e3b5..8652a7f993ab 100644\n--- a/mm/swap_tier.h\n+++ b/mm/swap_tier.h\n@@ -46,4 +46,25 @@ bool swap_tiers_update(void);\n /* Tier assignment */\n void swap_tiers_assign_dev(struct swap_info_struct *swp);\n \n+/* Memcg related functions */\n+void swap_tiers_mask_show(struct seq_file *m, int mask);\n+void swap_tiers_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent);\n+void swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg);\n+void __swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg);\n+\n+/* Mask and tier lookup */\n+int swap_tiers_mask_lookup(const char *name);\n+\n+/**\n+ * swap_tiers_mask_test - Check if the tier mask is valid\n+ * @tier_mask: The tier mask to check\n+ * @mask: The mask to compare against\n+ *\n+ * Return: true if condition matches, false otherwise\n+ */\n+static inline bool swap_tiers_mask_test(int tier_mask, int mask)\n+{\n+\treturn tier_mask & mask;\n+}\n #endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 4f8ce021c5bd..dd97e850ea2c 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1348,10 +1348,15 @@ static bool swap_alloc_fast(struct folio *folio)\n static void swap_alloc_slow(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n+\tint mask = folio_memcg(folio) ?\n+\t\tfolio_memcg(folio)->tier_effective_mask : TIER_ALL_MASK;\n \n \tspin_lock(&swap_avail_lock);\n start_over:\n \tplist_for_each_entry_safe(si, next, &swap_avail_head, avail_list) {\n+\t\tif (!swap_tiers_mask_test(si->tier_mask, mask))\n+\t\t\tcontinue;\n+\n \t\t/* Rotate the device and switch to a new cluster */\n \t\tplist_requeue(&si->avail_list, &swap_avail_head);\n \t\tspin_unlock(&swap_avail_lock);\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about swap device tier assignment, explaining that a `tier_mask` is added to identify the tier membership of swap devices and that the infrastructure allows dynamic modification of tiers without changing the tier assignment of already configured swap devices.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch connects swap devices to the swap tier infrastructure,\nensuring that devices are correctly assigned to tiers based on their\npriority.\n\nA `tier_mask` is added to identify the tier membership of swap devices.\nAlthough tier-based allocation logic is not yet implemented, this\nmapping is necessary to track which tier a device belongs to. Upon\nactivation, the device is assigned to a tier by matching its priority\nagainst the configured tier ranges.\n\nThe infrastructure allows dynamic modification of tiers, such as\nsplitting or merging ranges. These operations are permitted provided\nthat the tier assignment of already configured swap devices remains\nunchanged.\n\nThis patch also adds the documentation for the swap tier feature,\ncovering the core concepts, sysfs interface usage, and configuration\ndetails.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n Documentation/mm/swap-tier.rst | 109 +++++++++++++++++++++++++++++++++\n include/linux/swap.h           |   1 +\n mm/swap_state.c                |   2 +-\n mm/swap_tier.c                 | 106 ++++++++++++++++++++++++++++----\n mm/swap_tier.h                 |  13 +++-\n mm/swapfile.c                  |   2 +\n 6 files changed, 219 insertions(+), 14 deletions(-)\n create mode 100644 Documentation/mm/swap-tier.rst\n\ndiff --git a/Documentation/mm/swap-tier.rst b/Documentation/mm/swap-tier.rst\nnew file mode 100644\nindex 000000000000..3386161b9b18\n--- /dev/null\n+++ b/Documentation/mm/swap-tier.rst\n@@ -0,0 +1,109 @@\n+.. SPDX-License-Identifier: GPL-2.0\n+\n+:Author: Chris Li <chrisl@kernel.org> Youngjun Park <youngjun.park@lge.com>\n+\n+==========\n+Swap Tier\n+==========\n+\n+Swap tier is a collection of user-named groups classified by priority ranges.\n+It acts as a facilitation layer, allowing users to manage swap devices based\n+on their speeds.\n+\n+Users are encouraged to assign swap device priorities according to device\n+speed to fully utilize this feature. While the current implementation is\n+integrated with cgroups, the concept is designed to be extensible for other\n+subsystems in the future.\n+\n+Use case\n+-------\n+\n+Users can perform selective swapping by choosing a swap tier assigned according\n+to speed within a cgroup.\n+\n+For more information on cgroup v2, please refer to\n+``Documentation/admin-guide/cgroup-v2.rst``.\n+\n+Priority Range\n+--------------\n+\n+The specified tiers must cover the entire priority range from -1\n+(DEF_SWAP_PRIO) to SHRT_MAX.\n+\n+Consistency\n+-----------\n+\n+Tier consistency is guaranteed with a focus on maximizing flexibility. When a\n+swap device is activated within a tier range, a reference is held from the\n+start of the tier to the priority of that swap device. This ensures that the\n+tier of region containing the active swap device does not disappear.\n+\n+If a request to add a new tier with a priority higher than the current swap\n+device is received, the existing tier can be split.\n+\n+However, specifying a tier in a cgroup does not hold a reference to the tier.\n+Consequently, the corresponding tier can disappear at any time.\n+\n+Configuration Interface\n+-----------------------\n+\n+The swap tiers can be configured via the following interface:\n+\n+/sys/kernel/mm/swap/tiers\n+\n+Operations can be performed using the following syntax:\n+\n+* Add:    ``+\"<tiername>\":\"<start_priority>\"``\n+* Remove: ``-\"<tiername>\"``\n+* Modify: ``\"<tiername>\":\"<start_priority>\"``\n+\n+Multiple operations can be provided in a single write, separated by spaces (\" \")\n+or commas (\",\").\n+\n+When configuring tiers, the specified value represents the **start priority**\n+of that tier. The end priority is automatically determined by the start\n+priority of the next higher tier. Consequently, adding or modifying a tier\n+automatically adjusts (splits or merges) the ranges of adjacent tiers to\n+ensure continuity.\n+\n+Examples\n+--------\n+\n+**1. Initialization**\n+\n+A tier starting at -1 is mandatory to cover the entire priority range up to\n+SHRT_MAX. In this example, 'HDD' starts at 50, and 'NET' covers the remaining\n+lower range starting from -1.\n+\n+::\n+\n+    # echo \"+HDD:50, +NET:-1\" > /sys/kernel/mm/swap/tiers\n+    # cat /sys/kernel/mm/swap/tiers\n+    Name             Idx   PrioStart   PrioEnd\n+    HDD              0     50          32767\n+    NET              1     -1          49\n+\n+**2. Modification and Splitting**\n+\n+Here, 'HDD' is moved to start at 80, and a new tier 'SSD' is added at 100.\n+Notice how the ranges are automatically recalculated:\n+* 'SSD' takes the top range. Split HDD Tier's range. (100 to SHRT_MAX).\n+* 'HDD' is adjusted to the range between 'NET' and 'SSD' (80 to 99).\n+* 'NET' automatically extends to fill the gap below 'HDD' (-1 to 79).\n+\n+::\n+\n+    # echo \"HDD:80, +SSD:100\" > /sys/kernel/mm/swap/tiers\n+    # cat /sys/kernel/mm/swap/tiers\n+    Name             Idx   PrioStart   PrioEnd\n+    SSD              2     100         32767\n+    HDD              0     80          99\n+    NET              1     -1          79\n+\n+**3. Removal**\n+\n+Tiers can be removed using the '-' prefix.\n+\n+::\n+\n+    # echo \"-SSD,-HDD,-NET\" > /sys/kernel/mm/swap/tiers\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 62fc7499b408..1e68c220a0e7 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -262,6 +262,7 @@ struct swap_info_struct {\n \tstruct percpu_ref users;\t/* indicate and keep swap device valid. */\n \tunsigned long\tflags;\t\t/* SWP_USED etc: see above */\n \tsigned short\tprio;\t\t/* swap priority of this type */\n+\tint tier_mask;\t\t\t/* swap tier mask */\n \tstruct plist_node list;\t\t/* entry in swap_active_head */\n \tsigned char\ttype;\t\t/* strange name for an index */\n \tunsigned int\tmax;\t\t/* extent of the swap_map */\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex f1a7d9cdc648..d46ca61d2e42 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -997,7 +997,7 @@ static ssize_t tiers_store(struct kobject *kobj,\n \t\t\tgoto restore;\n \t}\n \n-\tif (!swap_tiers_validate()) {\n+\tif (!swap_tiers_update()) {\n \t\tret = -EINVAL;\n \t\tgoto restore;\n \t}\ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nindex 87882272eec8..d90f6eccb908 100644\n--- a/mm/swap_tier.c\n+++ b/mm/swap_tier.c\n@@ -14,7 +14,7 @@\n  * @name: name of the swap_tier.\n  * @prio: starting value of priority.\n  * @list: linked list of tiers.\n-*/\n+ */\n static struct swap_tier {\n \tchar name[MAX_TIERNAME];\n \tshort prio;\n@@ -34,6 +34,8 @@ static LIST_HEAD(swap_tier_inactive_list);\n \t(!list_is_first(&(tier)->list, &swap_tier_active_list) ? \\\n \tlist_prev_entry((tier), list)->prio - 1 : SHRT_MAX)\n \n+#define MASK_TO_TIER(mask) (&swap_tiers[__ffs((mask))])\n+\n #define for_each_tier(tier, idx) \\\n \tfor (idx = 0, tier = &swap_tiers[0]; idx < MAX_SWAPTIER; \\\n \t\tidx++, tier = &swap_tiers[idx])\n@@ -55,6 +57,26 @@ static bool swap_tier_is_active(void)\n \treturn !list_empty(&swap_tier_active_list) ? true : false;\n }\n \n+static bool swap_tier_prio_in_range(struct swap_tier *tier, short prio)\n+{\n+\tif (tier->prio <= prio && TIER_END_PRIO(tier) >= prio)\n+\t\treturn true;\n+\n+\treturn false;\n+}\n+\n+static bool swap_tier_prio_is_used(struct swap_tier *self, short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (tier != self && tier->prio == prio)\n+\t\t\treturn true;\n+\t}\n+\n+\treturn false;\n+}\n+\n static struct swap_tier *swap_tier_lookup(const char *name)\n {\n \tstruct swap_tier *tier;\n@@ -67,12 +89,14 @@ static struct swap_tier *swap_tier_lookup(const char *name)\n \treturn NULL;\n }\n \n+\n void swap_tiers_init(void)\n {\n \tstruct swap_tier *tier;\n \tint idx;\n \n \tBUILD_BUG_ON(BITS_PER_TYPE(int) < MAX_SWAPTIER);\n+\tBUILD_BUG_ON(MAX_SWAPTIER > TIER_DEFAULT_IDX);\n \n \tfor_each_tier(tier, idx) {\n \t\tINIT_LIST_HEAD(&tier->list);\n@@ -145,17 +169,35 @@ static struct swap_tier *swap_tier_prepare(const char *name, short prio)\n \treturn tier;\n }\n \n-static int swap_tier_check_range(short prio)\n+static int swap_tier_can_split_range(struct swap_tier *orig_tier,\n+\tshort new_prio)\n {\n+\tstruct swap_info_struct *p;\n \tstruct swap_tier *tier;\n \n \tlockdep_assert_held(&swap_lock);\n \tlockdep_assert_held(&swap_tier_lock);\n \n-\tfor_each_active_tier(tier) {\n-\t\t/* No overwrite */\n-\t\tif (tier->prio == prio)\n-\t\t\treturn -EINVAL;\n+\tplist_for_each_entry(p, &swap_active_head, list) {\n+\t\tif (p->tier_mask == TIER_DEFAULT_MASK)\n+\t\t\tcontinue;\n+\n+\t\ttier = MASK_TO_TIER(p->tier_mask);\n+\t\tif (tier->prio > new_prio)\n+\t\t\tcontinue;\n+\t\t/*\n+                 * Prohibit implicit tier reassignment.\n+                 * Case 1: Prevent orig_tier devices from dropping out\n+                 *         of the new range.\n+                 */\n+\t\tif (orig_tier == tier && (p->prio < new_prio))\n+\t\t\treturn -EBUSY;\n+                /*\n+                 * Case 2: Prevent other tier devices from entering\n+                 *         the new range.\n+                 */\n+\t\telse if (orig_tier != tier && (p->prio >= new_prio))\n+\t\t\treturn -EBUSY;\n \t}\n \n \treturn 0;\n@@ -173,7 +215,10 @@ int swap_tiers_add(const char *name, int prio)\n \tif (swap_tier_lookup(name))\n \t\treturn -EPERM;\n \n-\tret = swap_tier_check_range(prio);\n+\tif (swap_tier_prio_is_used(NULL, prio))\n+\t\treturn -EBUSY;\n+\n+\tret = swap_tier_can_split_range(NULL, prio);\n \tif (ret)\n \t\treturn ret;\n \n@@ -183,7 +228,6 @@ int swap_tiers_add(const char *name, int prio)\n \t\treturn ret;\n \t}\n \n-\n \tswap_tier_insert_by_prio(tier);\n \treturn ret;\n }\n@@ -200,11 +244,18 @@ int swap_tiers_remove(const char *name)\n \tif (!tier)\n \t\treturn -EINVAL;\n \n+\t/* Simulate adding a tier to check for conflicts */\n+\tret = swap_tier_can_split_range(NULL, tier->prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n \tlist_move(&tier->list, &swap_tier_inactive_list);\n \n \t/* Removing DEF_SWAP_PRIO merges into the higher tier. */\n-\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO)\n-\t\tlist_prev_entry(tier, list)->prio = DEF_SWAP_PRIO;\n+\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO) {\n+\t\tlist_last_entry(&swap_tier_active_list, struct swap_tier, list)\n+\t\t\t->prio = DEF_SWAP_PRIO;\n+\t}\n \n \treturn ret;\n }\n@@ -225,7 +276,10 @@ int swap_tiers_modify(const char *name, int prio)\n \tif (tier->prio == prio)\n \t\treturn 0;\n \n-\tret = swap_tier_check_range(prio);\n+\tif (swap_tier_prio_is_used(tier, prio))\n+\t\treturn -EBUSY;\n+\n+\tret = swap_tier_can_split_range(tier, prio);\n \tif (ret)\n \t\treturn ret;\n \n@@ -283,10 +337,27 @@ void swap_tiers_restore(struct swap_tier_save_ctx ctx[])\n \t}\n }\n \n-bool swap_tiers_validate(void)\n+void swap_tiers_assign_dev(struct swap_info_struct *swp)\n {\n \tstruct swap_tier *tier;\n \n+\tlockdep_assert_held(&swap_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (swap_tier_prio_in_range(tier, swp->prio)) {\n+\t\t\tswp->tier_mask = TIER_MASK(tier);\n+\t\t\treturn;\n+\t\t}\n+\t}\n+\n+\tswp->tier_mask = TIER_DEFAULT_MASK;\n+}\n+\n+bool swap_tiers_update(void)\n+{\n+\tstruct swap_tier *tier;\n+\tstruct swap_info_struct *swp;\n+\n \t/*\n \t * Initial setting might not cover DEF_SWAP_PRIO.\n \t * Swap tier must cover the full range (DEF_SWAP_PRIO to SHRT_MAX).\n@@ -300,5 +371,16 @@ bool swap_tiers_validate(void)\n \t\t\treturn false;\n \t}\n \n+\t/*\n+\t * If applied initially, the swap tier_mask may change\n+\t * from the default value.\n+\t */\n+\tplist_for_each_entry(swp, &swap_active_head, list) {\n+\t\t/* Tier is already configured */\n+\t\tif (swp->tier_mask != TIER_DEFAULT_MASK)\n+\t\t\tbreak;\n+\t\tswap_tiers_assign_dev(swp);\n+\t}\n+\n \treturn true;\n }\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nindex 4b1b0602d691..de81d540e3b5 100644\n--- a/mm/swap_tier.h\n+++ b/mm/swap_tier.h\n@@ -14,6 +14,9 @@\n #define MAX_SWAPTIER\t\t8\n #endif\n \n+/* Forward declarations */\n+struct swap_info_struct;\n+\n extern spinlock_t swap_tier_lock;\n \n struct swap_tier_save_ctx {\n@@ -24,6 +27,10 @@ struct swap_tier_save_ctx {\n #define DEFINE_SWAP_TIER_SAVE_CTX(_name) \\\n \tstruct swap_tier_save_ctx _name[MAX_SWAPTIER] = {0}\n \n+#define TIER_ALL_MASK\t\t(~0)\n+#define TIER_DEFAULT_IDX\t(31)\n+#define TIER_DEFAULT_MASK\t(1 << TIER_DEFAULT_IDX)\n+\n /* Initialization and application */\n void swap_tiers_init(void);\n ssize_t swap_tiers_sysfs_show(char *buf);\n@@ -34,5 +41,9 @@ int swap_tiers_modify(const char *name, int prio);\n \n void swap_tiers_save(struct swap_tier_save_ctx ctx[]);\n void swap_tiers_restore(struct swap_tier_save_ctx ctx[]);\n-bool swap_tiers_validate(void);\n+bool swap_tiers_update(void);\n+\n+/* Tier assignment */\n+void swap_tiers_assign_dev(struct swap_info_struct *swp);\n+\n #endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex c27952b41d4f..4f8ce021c5bd 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -2672,6 +2672,8 @@ static void _enable_swap_info(struct swap_info_struct *si)\n \n \t/* Add back to available list */\n \tadd_to_avail_list(si, true);\n+\n+\tswap_tiers_assign_dev(si);\n }\n \n static void enable_swap_info(struct swap_info_struct *si, int prio,\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about the potential for silent disappearance of tiers when using the '-' operator to exclude specific tiers. They explained that the effective tier list is limited to the parent's allowed subset, and provided an example to illustrate this point.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch introduces the \"Swap tier\" concept, which serves as an\nabstraction layer for managing swap devices based on their performance\ncharacteristics (e.g., NVMe, HDD, Network swap).\n\nSwap tiers are user-named groups representing priority ranges.\nThese tiers collectively cover the entire priority\nspace from -1 (`DEF_SWAP_PRIO`) to `SHRT_MAX`.\n\nTo configure tiers, a new sysfs interface is exposed at\n`/sys/kernel/mm/swap/tiers`. The input parser evaluates commands from\nleft to right and supports batch input, allowing users to add, remove or\nmodify multiple tiers in a single write operation.\n\nTier management enforces continuous priority ranges anchored by start\npriorities. Operations trigger range splitting or merging, but overwriting\nstart priorities is forbidden. Merging expands lower tiers upwards to\npreserve configured start priorities, except when removing `DEF_SWAP_PRIO`,\nwhich merges downwards.\n\nSuggested-by: Chris Li <chrisl@kernel.org>\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n MAINTAINERS     |   2 +\n mm/Makefile     |   2 +-\n mm/swap.h       |   4 +\n mm/swap_state.c |  70 +++++++++++\n mm/swap_tier.c  | 304 ++++++++++++++++++++++++++++++++++++++++++++++++\n mm/swap_tier.h  |  38 ++++++\n mm/swapfile.c   |   7 +-\n 7 files changed, 423 insertions(+), 4 deletions(-)\n create mode 100644 mm/swap_tier.c\n create mode 100644 mm/swap_tier.h\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 18d1ebf053db..501bf46adfb4 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16743,6 +16743,8 @@ F:\tmm/swap.c\n F:\tmm/swap.h\n F:\tmm/swap_table.h\n F:\tmm/swap_state.c\n+F:\tmm/swap_tier.c\n+F:\tmm/swap_tier.h\n F:\tmm/swapfile.c\n \n MEMORY MANAGEMENT - THP (TRANSPARENT HUGE PAGE)\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 53ca5d4b1929..3b3de2de7285 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -75,7 +75,7 @@ ifdef CONFIG_MMU\n \tobj-$(CONFIG_ADVISE_SYSCALLS)\t+= madvise.o\n endif\n \n-obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o\n+obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o swap_tier.o\n obj-$(CONFIG_ZSWAP)\t+= zswap.o\n obj-$(CONFIG_HAS_DMA)\t+= dmapool.o\n obj-$(CONFIG_HUGETLBFS)\t+= hugetlb.o hugetlb_sysfs.o hugetlb_sysctl.o\ndiff --git a/mm/swap.h b/mm/swap.h\nindex bfafa637c458..55f230cbe4e7 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -16,6 +16,10 @@ extern int page_cluster;\n #define swap_entry_order(order)\t0\n #endif\n \n+#define DEF_SWAP_PRIO  -1\n+\n+extern spinlock_t swap_lock;\n+extern struct plist_head swap_active_head;\n extern struct swap_info_struct *swap_info[];\n \n /*\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 6d0eef7470be..f1a7d9cdc648 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -25,6 +25,7 @@\n #include \"internal.h\"\n #include \"swap_table.h\"\n #include \"swap.h\"\n+#include \"swap_tier.h\"\n \n /*\n  * swapper_space is a fiction, retained to simplify the path through\n@@ -947,8 +948,77 @@ static ssize_t vma_ra_enabled_store(struct kobject *kobj,\n }\n static struct kobj_attribute vma_ra_enabled_attr = __ATTR_RW(vma_ra_enabled);\n \n+static ssize_t tiers_show(struct kobject *kobj,\n+\t\t\t\t     struct kobj_attribute *attr, char *buf)\n+{\n+\treturn swap_tiers_sysfs_show(buf);\n+}\n+\n+static ssize_t tiers_store(struct kobject *kobj,\n+\t\t\tstruct kobj_attribute *attr,\n+\t\t\tconst char *buf, size_t count)\n+{\n+\tchar *p, *token, *name, *tmp;\n+\tint ret = 0;\n+\tshort prio;\n+\tDEFINE_SWAP_TIER_SAVE_CTX(ctx);\n+\n+\ttmp = kstrdup(buf, GFP_KERNEL);\n+\tif (!tmp)\n+\t\treturn -ENOMEM;\n+\n+\tspin_lock(&swap_lock);\n+\tspin_lock(&swap_tier_lock);\n+\n+\tp = tmp;\n+\tswap_tiers_save(ctx);\n+\n+\twhile (!ret && (token = strsep(&p, \", \\t\\n\")) != NULL) {\n+\t\tif (!*token)\n+\t\t\tcontinue;\n+\n+\t\tif (token[0] == '-') {\n+\t\t\tret = swap_tiers_remove(token + 1);\n+\t\t} else {\n+\n+\t\t\tname = strsep(&token, \":\");\n+\t\t\tif (!token || kstrtos16(token, 10, &prio)) {\n+\t\t\t\tret = -EINVAL;\n+\t\t\t\tgoto out;\n+\t\t\t}\n+\n+\t\t\tif (name[0] == '+')\n+\t\t\t\tret = swap_tiers_add(name + 1, prio);\n+\t\t\telse\n+\t\t\t\tret = swap_tiers_modify(name, prio);\n+\t\t}\n+\n+\t\tif (ret)\n+\t\t\tgoto restore;\n+\t}\n+\n+\tif (!swap_tiers_validate()) {\n+\t\tret = -EINVAL;\n+\t\tgoto restore;\n+\t}\n+\n+out:\n+\tspin_unlock(&swap_tier_lock);\n+\tspin_unlock(&swap_lock);\n+\n+\tkfree(tmp);\n+\treturn ret ? ret : count;\n+\n+restore:\n+\tswap_tiers_restore(ctx);\n+\tgoto out;\n+}\n+\n+static struct kobj_attribute tier_attr = __ATTR_RW(tiers);\n+\n static struct attribute *swap_attrs[] = {\n \t&vma_ra_enabled_attr.attr,\n+\t&tier_attr.attr,\n \tNULL,\n };\n \ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nnew file mode 100644\nindex 000000000000..87882272eec8\n--- /dev/null\n+++ b/mm/swap_tier.c\n@@ -0,0 +1,304 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#include <linux/swap.h>\n+#include <linux/memcontrol.h>\n+#include \"memcontrol-v1.h\"\n+#include <linux/sysfs.h>\n+#include <linux/plist.h>\n+\n+#include \"swap.h\"\n+#include \"swap_tier.h\"\n+\n+/*\n+ * struct swap_tier - structure representing a swap tier.\n+ *\n+ * @name: name of the swap_tier.\n+ * @prio: starting value of priority.\n+ * @list: linked list of tiers.\n+*/\n+static struct swap_tier {\n+\tchar name[MAX_TIERNAME];\n+\tshort prio;\n+\tstruct list_head list;\n+} swap_tiers[MAX_SWAPTIER];\n+\n+DEFINE_SPINLOCK(swap_tier_lock);\n+/* active swap priority list, sorted in descending order */\n+static LIST_HEAD(swap_tier_active_list);\n+/* unused swap_tier object */\n+static LIST_HEAD(swap_tier_inactive_list);\n+\n+#define TIER_IDX(tier)\t((tier) - swap_tiers)\n+#define TIER_MASK(tier)\t(1 << TIER_IDX(tier))\n+#define TIER_INVALID_PRIO (DEF_SWAP_PRIO - 1)\n+#define TIER_END_PRIO(tier) \\\n+\t(!list_is_first(&(tier)->list, &swap_tier_active_list) ? \\\n+\tlist_prev_entry((tier), list)->prio - 1 : SHRT_MAX)\n+\n+#define for_each_tier(tier, idx) \\\n+\tfor (idx = 0, tier = &swap_tiers[0]; idx < MAX_SWAPTIER; \\\n+\t\tidx++, tier = &swap_tiers[idx])\n+\n+#define for_each_active_tier(tier) \\\n+\tlist_for_each_entry(tier, &swap_tier_active_list, list)\n+\n+#define for_each_inactive_tier(tier) \\\n+\tlist_for_each_entry(tier, &swap_tier_inactive_list, list)\n+\n+/*\n+ * Naming Convention:\n+ *   swap_tiers_*() - Public/exported functions\n+ *   swap_tier_*()  - Private/internal functions\n+ */\n+\n+static bool swap_tier_is_active(void)\n+{\n+\treturn !list_empty(&swap_tier_active_list) ? true : false;\n+}\n+\n+static struct swap_tier *swap_tier_lookup(const char *name)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (!strcmp(tier->name, name))\n+\t\t\treturn tier;\n+\t}\n+\n+\treturn NULL;\n+}\n+\n+void swap_tiers_init(void)\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tBUILD_BUG_ON(BITS_PER_TYPE(int) < MAX_SWAPTIER);\n+\n+\tfor_each_tier(tier, idx) {\n+\t\tINIT_LIST_HEAD(&tier->list);\n+\t\tlist_add_tail(&tier->list, &swap_tier_inactive_list);\n+\t}\n+}\n+\n+ssize_t swap_tiers_sysfs_show(char *buf)\n+{\n+\tstruct swap_tier *tier;\n+\tssize_t len = 0;\n+\n+\tlen += sysfs_emit_at(buf, len, \"%-16s %-5s %-11s %-11s\\n\",\n+\t\t\t \"Name\", \"Idx\", \"PrioStart\", \"PrioEnd\");\n+\n+\tspin_lock(&swap_tier_lock);\n+\tfor_each_active_tier(tier) {\n+\t\tlen += sysfs_emit_at(buf, len, \"%-16s %-5ld %-11d %-11d\\n\",\n+\t\t\t\t     tier->name,\n+\t\t\t\t     TIER_IDX(tier),\n+\t\t\t\t     tier->prio,\n+\t\t\t\t     TIER_END_PRIO(tier));\n+\t\tif (len >= PAGE_SIZE)\n+\t\t\tbreak;\n+\t}\n+\tspin_unlock(&swap_tier_lock);\n+\n+\treturn len;\n+}\n+\n+static void swap_tier_insert_by_prio(struct swap_tier *new)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (tier->prio > new->prio)\n+\t\t\tcontinue;\n+\n+\t\tlist_add_tail(&new->list, &tier->list);\n+\t\treturn;\n+\t}\n+\t/* First addition, or becomes the first tier */\n+\tlist_add_tail(&new->list, &swap_tier_active_list);\n+}\n+\n+static void __swap_tier_prepare(struct swap_tier *tier, const char *name,\n+\tshort prio)\n+{\n+\tlist_del_init(&tier->list);\n+\tstrscpy(tier->name, name, MAX_TIERNAME);\n+\ttier->prio = prio;\n+}\n+\n+static struct swap_tier *swap_tier_prepare(const char *name, short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tif (prio < DEF_SWAP_PRIO)\n+\t\treturn NULL;\n+\n+\tif (list_empty(&swap_tier_inactive_list))\n+\t\treturn ERR_PTR(-EPERM);\n+\n+\ttier = list_first_entry(&swap_tier_inactive_list,\n+\t\tstruct swap_tier, list);\n+\n+\t__swap_tier_prepare(tier, name, prio);\n+\treturn tier;\n+}\n+\n+static int swap_tier_check_range(short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\t/* No overwrite */\n+\t\tif (tier->prio == prio)\n+\t\t\treturn -EINVAL;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+int swap_tiers_add(const char *name, int prio)\n+{\n+\tint ret;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\t/* Duplicate check */\n+\tif (swap_tier_lookup(name))\n+\t\treturn -EPERM;\n+\n+\tret = swap_tier_check_range(prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\ttier = swap_tier_prepare(name, prio);\n+\tif (IS_ERR(tier)) {\n+\t\tret = PTR_ERR(tier);\n+\t\treturn ret;\n+\t}\n+\n+\n+\tswap_tier_insert_by_prio(tier);\n+\treturn ret;\n+}\n+\n+int swap_tiers_remove(const char *name)\n+{\n+\tint ret = 0;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\ttier = swap_tier_lookup(name);\n+\tif (!tier)\n+\t\treturn -EINVAL;\n+\n+\tlist_move(&tier->list, &swap_tier_inactive_list);\n+\n+\t/* Removing DEF_SWAP_PRIO merges into the higher tier. */\n+\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO)\n+\t\tlist_prev_entry(tier, list)->prio = DEF_SWAP_PRIO;\n+\n+\treturn ret;\n+}\n+\n+int swap_tiers_modify(const char *name, int prio)\n+{\n+\tint ret;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\ttier = swap_tier_lookup(name);\n+\tif (!tier)\n+\t\treturn -EINVAL;\n+\n+\t/* No need to modify */\n+\tif (tier->prio == prio)\n+\t\treturn 0;\n+\n+\tret = swap_tier_check_range(prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tlist_del_init(&tier->list);\n+\ttier->prio = prio;\n+\tswap_tier_insert_by_prio(tier);\n+\n+\treturn ret;\n+}\n+\n+/*\n+ * XXX: Reverting individual operations becomes complex as the number of\n+ * operations grows. Instead, we save the original state beforehand and\n+ * fully restore it if any operation fails.\n+ */\n+void swap_tiers_save(struct swap_tier_save_ctx ctx[])\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tidx = TIER_IDX(tier);\n+\t\tstrcpy(ctx[idx].name, tier->name);\n+\t\tctx[idx].prio = tier->prio;\n+\t}\n+\n+\tfor_each_inactive_tier(tier) {\n+\t\tidx = TIER_IDX(tier);\n+\t\t/* Indicator of inactive */\n+\t\tctx[idx].prio = TIER_INVALID_PRIO;\n+\t}\n+}\n+\n+void swap_tiers_restore(struct swap_tier_save_ctx ctx[])\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\t/* Invalidate active list */\n+\tlist_splice_tail_init(&swap_tier_active_list,\n+\t\t\t&swap_tier_inactive_list);\n+\n+\tfor_each_tier(tier, idx) {\n+\t\tif (ctx[idx].prio != TIER_INVALID_PRIO) {\n+\t\t\t/* Preserve idx(mask) */\n+\t\t\t__swap_tier_prepare(tier, ctx[idx].name, ctx[idx].prio);\n+\t\t\tswap_tier_insert_by_prio(tier);\n+\t\t}\n+\t}\n+}\n+\n+bool swap_tiers_validate(void)\n+{\n+\tstruct swap_tier *tier;\n+\n+\t/*\n+\t * Initial setting might not cover DEF_SWAP_PRIO.\n+\t * Swap tier must cover the full range (DEF_SWAP_PRIO to SHRT_MAX).\n+\t * Also, modify operation can change only one remaining priority.\n+\t */\n+\tif (swap_tier_is_active()) {\n+\t\ttier = list_last_entry(&swap_tier_active_list,\n+\t\t\tstruct swap_tier, list);\n+\n+\t\tif (tier->prio != DEF_SWAP_PRIO)\n+\t\t\treturn false;\n+\t}\n+\n+\treturn true;\n+}\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nnew file mode 100644\nindex 000000000000..4b1b0602d691\n--- /dev/null\n+++ b/mm/swap_tier.h\n@@ -0,0 +1,38 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _SWAP_TIER_H\n+#define _SWAP_TIER_H\n+\n+#include <linux/types.h>\n+#include <linux/spinlock.h>\n+\n+#define MAX_TIERNAME\t\t16\n+\n+/* Ensure MAX_SWAPTIER does not exceed MAX_SWAPFILES */\n+#if 8 > MAX_SWAPFILES\n+#define MAX_SWAPTIER\t\tMAX_SWAPFILES\n+#else\n+#define MAX_SWAPTIER\t\t8\n+#endif\n+\n+extern spinlock_t swap_tier_lock;\n+\n+struct swap_tier_save_ctx {\n+\tchar name[MAX_TIERNAME];\n+\tshort prio;\n+};\n+\n+#define DEFINE_SWAP_TIER_SAVE_CTX(_name) \\\n+\tstruct swap_tier_save_ctx _name[MAX_SWAPTIER] = {0}\n+\n+/* Initialization and application */\n+void swap_tiers_init(void);\n+ssize_t swap_tiers_sysfs_show(char *buf);\n+\n+int swap_tiers_add(const char *name, int prio);\n+int swap_tiers_remove(const char *name);\n+int swap_tiers_modify(const char *name, int prio);\n+\n+void swap_tiers_save(struct swap_tier_save_ctx ctx[]);\n+void swap_tiers_restore(struct swap_tier_save_ctx ctx[]);\n+bool swap_tiers_validate(void);\n+#endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 7b055f15d705..c27952b41d4f 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -50,6 +50,7 @@\n #include \"internal.h\"\n #include \"swap_table.h\"\n #include \"swap.h\"\n+#include \"swap_tier.h\"\n \n static bool swap_count_continued(struct swap_info_struct *, pgoff_t,\n \t\t\t\t unsigned char);\n@@ -65,7 +66,7 @@ static void move_cluster(struct swap_info_struct *si,\n \t\t\t struct swap_cluster_info *ci, struct list_head *list,\n \t\t\t enum swap_cluster_flags new_flags);\n \n-static DEFINE_SPINLOCK(swap_lock);\n+DEFINE_SPINLOCK(swap_lock);\n static unsigned int nr_swapfiles;\n atomic_long_t nr_swap_pages;\n /*\n@@ -76,7 +77,6 @@ atomic_long_t nr_swap_pages;\n EXPORT_SYMBOL_GPL(nr_swap_pages);\n /* protected with swap_lock. reading in vm_swap_full() doesn't need lock */\n long total_swap_pages;\n-#define DEF_SWAP_PRIO  -1\n unsigned long swapfile_maximum_size;\n #ifdef CONFIG_MIGRATION\n bool swap_migration_ad_supported;\n@@ -89,7 +89,7 @@ static const char Bad_offset[] = \"Bad swap offset entry \";\n  * all active swap_info_structs\n  * protected with swap_lock, and ordered by priority.\n  */\n-static PLIST_HEAD(swap_active_head);\n+PLIST_HEAD(swap_active_head);\n \n /*\n  * all available (active, not full) swap_info_structs\n@@ -3977,6 +3977,7 @@ static int __init swapfile_init(void)\n \t\tswap_migration_ad_supported = true;\n #endif\t/* CONFIG_MIGRATION */\n \n+\tswap_tiers_init();\n \treturn 0;\n }\n subsys_initcall(swapfile_init);\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author is addressing a concern about caching oscillation and priority inversion in swap devices due to global percpu clusters. They agree that reverting the original commit will resolve these issues, which was suggested by Kairui Song. The author has reverted the commit and modified the code to use each swap device's percpu cluster.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed with suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This reverts commit 1b7e90020eb7 (\"mm, swap: use percpu cluster as\nallocation fast path\").\n\nBecause in the newly introduced swap tiers, the global percpu cluster\nwill cause two issues:\n1) it will cause caching oscillation in the same order of different si\n   if two different memcg can only be allowed to access different si and\n   both of them are swapping out.\n2) It can cause priority inversion on swap devices. Imagine a case where\n   there are two memcg, say memcg1 and memcg2. Memcg1 can access si A, B\n   and A is higher priority device. While memcg2 can only access si B.\n   Then memcg 2 could write the global percpu cluster with si B, then\n   memcg1 take si B in fast path even though si A is not exhausted.\n\nHence in order to support swap tier, revert commit to use\neach swap device's percpu cluster.\n\nSuggested-by: Kairui Song <kasong@tencent.com>\nCo-developed-by: Baoquan He <bhe@redhat.com>\nSigned-off-by: Baoquan He <bhe@redhat.com>\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n include/linux/swap.h |  17 ++++--\n mm/swapfile.c        | 142 ++++++++++++++-----------------------------\n 2 files changed, 57 insertions(+), 102 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 1e68c220a0e7..6921e22b14d3 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -247,11 +247,18 @@ enum {\n #define SWAP_NR_ORDERS\t\t1\n #endif\n \n-/*\n- * We keep using same cluster for rotational device so IO will be sequential.\n- * The purpose is to optimize SWAP throughput on these device.\n- */\n+ /*\n+  * We assign a cluster to each CPU, so each CPU can allocate swap entry from\n+  * its own cluster and swapout sequentially. The purpose is to optimize swapout\n+  * throughput.\n+  */\n+struct percpu_cluster {\n+\tlocal_lock_t lock; /* Protect the percpu_cluster above */\n+\tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n+};\n+\n struct swap_sequential_cluster {\n+\tspinlock_t lock; /* Serialize usage of global cluster */\n \tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n };\n \n@@ -277,8 +284,8 @@ struct swap_info_struct {\n \t\t\t\t\t/* list of cluster that are fragmented or contented */\n \tunsigned int pages;\t\t/* total of usable pages of swap */\n \tatomic_long_t inuse_pages;\t/* number of those currently in use */\n+\tstruct percpu_cluster\t__percpu *percpu_cluster; /* per cpu's swap location */\n \tstruct swap_sequential_cluster *global_cluster; /* Use one global cluster for rotating device */\n-\tspinlock_t global_cluster_lock;\t/* Serialize usage of global cluster */\n \tstruct rb_root swap_extent_root;/* root of the swap extent rbtree */\n \tstruct block_device *bdev;\t/* swap device or bdev of swap file */\n \tstruct file *swap_file;\t\t/* seldom referenced */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex dd97e850ea2c..5e3b87799440 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -118,18 +118,6 @@ static atomic_t proc_poll_event = ATOMIC_INIT(0);\n \n atomic_t nr_rotate_swap = ATOMIC_INIT(0);\n \n-struct percpu_swap_cluster {\n-\tstruct swap_info_struct *si[SWAP_NR_ORDERS];\n-\tunsigned long offset[SWAP_NR_ORDERS];\n-\tlocal_lock_t lock;\n-};\n-\n-static DEFINE_PER_CPU(struct percpu_swap_cluster, percpu_swap_cluster) = {\n-\t.si = { NULL },\n-\t.offset = { SWAP_ENTRY_INVALID },\n-\t.lock = INIT_LOCAL_LOCK(),\n-};\n-\n /* May return NULL on invalid type, caller must check for NULL return */\n static struct swap_info_struct *swap_type_to_info(int type)\n {\n@@ -477,7 +465,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * Swap allocator uses percpu clusters and holds the local lock.\n \t */\n \tlockdep_assert_held(&ci->lock);\n-\tlockdep_assert_held(&this_cpu_ptr(&percpu_swap_cluster)->lock);\n+\tlockdep_assert_held(this_cpu_ptr(&si->percpu_cluster->lock));\n \n \t/* The cluster must be free and was just isolated from the free list. */\n \tVM_WARN_ON_ONCE(ci->flags || !cluster_is_empty(ci));\n@@ -495,8 +483,8 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t */\n \tspin_unlock(&ci->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_unlock(&si->global_cluster_lock);\n-\tlocal_unlock(&percpu_swap_cluster.lock);\n+\t\tspin_unlock(&si->global_cluster->lock);\n+\tlocal_unlock(&si->percpu_cluster->lock);\n \n \ttable = swap_table_alloc(__GFP_HIGH | __GFP_NOMEMALLOC | GFP_KERNEL);\n \n@@ -508,9 +496,9 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * could happen with ignoring the percpu cluster is fragmentation,\n \t * which is acceptable since this fallback and race is rare.\n \t */\n-\tlocal_lock(&percpu_swap_cluster.lock);\n+\tlocal_lock(&si->percpu_cluster->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_lock(&si->global_cluster_lock);\n+\t\tspin_lock(&si->global_cluster->lock);\n \tspin_lock(&ci->lock);\n \n \t/* Nothing except this helper should touch a dangling empty cluster. */\n@@ -622,7 +610,7 @@ static bool swap_do_scheduled_discard(struct swap_info_struct *si)\n \t\tci = list_first_entry(&si->discard_clusters, struct swap_cluster_info, list);\n \t\t/*\n \t\t * Delete the cluster from list to prepare for discard, but keep\n-\t\t * the CLUSTER_FLAG_DISCARD flag, percpu_swap_cluster could be\n+\t\t * the CLUSTER_FLAG_DISCARD flag, there could be percpu_cluster\n \t\t * pointing to it, or ran into by relocate_cluster.\n \t\t */\n \t\tlist_del(&ci->list);\n@@ -953,12 +941,11 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n out:\n \trelocate_cluster(si, ci);\n \tswap_cluster_unlock(ci);\n-\tif (si->flags & SWP_SOLIDSTATE) {\n-\t\tthis_cpu_write(percpu_swap_cluster.offset[order], next);\n-\t\tthis_cpu_write(percpu_swap_cluster.si[order], si);\n-\t} else {\n+\tif (si->flags & SWP_SOLIDSTATE)\n+\t\tthis_cpu_write(si->percpu_cluster->next[order], next);\n+\telse\n \t\tsi->global_cluster->next[order] = next;\n-\t}\n+\n \treturn found;\n }\n \n@@ -1052,13 +1039,17 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \tif (order && !(si->flags & SWP_BLKDEV))\n \t\treturn 0;\n \n-\tif (!(si->flags & SWP_SOLIDSTATE)) {\n+\tif (si->flags & SWP_SOLIDSTATE) {\n+\t\t/* Fast path using per CPU cluster */\n+\t\tlocal_lock(&si->percpu_cluster->lock);\n+\t\toffset = __this_cpu_read(si->percpu_cluster->next[order]);\n+\t} else {\n \t\t/* Serialize HDD SWAP allocation for each device. */\n-\t\tspin_lock(&si->global_cluster_lock);\n+\t\tspin_lock(&si->global_cluster->lock);\n \t\toffset = si->global_cluster->next[order];\n-\t\tif (offset == SWAP_ENTRY_INVALID)\n-\t\t\tgoto new_cluster;\n+\t}\n \n+\tif (offset != SWAP_ENTRY_INVALID) {\n \t\tci = swap_cluster_lock(si, offset);\n \t\t/* Cluster could have been used by another order */\n \t\tif (cluster_is_usable(ci, order)) {\n@@ -1072,7 +1063,6 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n \n-new_cluster:\n \t/*\n \t * If the device need discard, prefer new cluster over nonfull\n \t * to spread out the writes.\n@@ -1129,8 +1119,10 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n done:\n-\tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_unlock(&si->global_cluster_lock);\n+\tif (si->flags & SWP_SOLIDSTATE)\n+\t\tlocal_unlock(&si->percpu_cluster->lock);\n+\telse\n+\t\tspin_unlock(&si->global_cluster->lock);\n \n \treturn found;\n }\n@@ -1311,41 +1303,8 @@ static bool get_swap_device_info(struct swap_info_struct *si)\n \treturn true;\n }\n \n-/*\n- * Fast path try to get swap entries with specified order from current\n- * CPU's swap entry pool (a cluster).\n- */\n-static bool swap_alloc_fast(struct folio *folio)\n-{\n-\tunsigned int order = folio_order(folio);\n-\tstruct swap_cluster_info *ci;\n-\tstruct swap_info_struct *si;\n-\tunsigned int offset;\n-\n-\t/*\n-\t * Once allocated, swap_info_struct will never be completely freed,\n-\t * so checking it's liveness by get_swap_device_info is enough.\n-\t */\n-\tsi = this_cpu_read(percpu_swap_cluster.si[order]);\n-\toffset = this_cpu_read(percpu_swap_cluster.offset[order]);\n-\tif (!si || !offset || !get_swap_device_info(si))\n-\t\treturn false;\n-\n-\tci = swap_cluster_lock(si, offset);\n-\tif (cluster_is_usable(ci, order)) {\n-\t\tif (cluster_is_empty(ci))\n-\t\t\toffset = cluster_offset(si, ci);\n-\t\talloc_swap_scan_cluster(si, ci, folio, offset);\n-\t} else {\n-\t\tswap_cluster_unlock(ci);\n-\t}\n-\n-\tput_swap_device(si);\n-\treturn folio_test_swapcache(folio);\n-}\n-\n /* Rotate the device and switch to a new cluster */\n-static void swap_alloc_slow(struct folio *folio)\n+static void swap_alloc_entry(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n \tint mask = folio_memcg(folio) ?\n@@ -1363,6 +1322,7 @@ static void swap_alloc_slow(struct folio *folio)\n \t\tif (get_swap_device_info(si)) {\n \t\t\tcluster_alloc_swap_entry(si, folio);\n \t\t\tput_swap_device(si);\n+\n \t\t\tif (folio_test_swapcache(folio))\n \t\t\t\treturn;\n \t\t\tif (folio_test_large(folio))\n@@ -1522,11 +1482,7 @@ int folio_alloc_swap(struct folio *folio)\n \t}\n \n again:\n-\tlocal_lock(&percpu_swap_cluster.lock);\n-\tif (!swap_alloc_fast(folio))\n-\t\tswap_alloc_slow(folio);\n-\tlocal_unlock(&percpu_swap_cluster.lock);\n-\n+\tswap_alloc_entry(folio);\n \tif (!order && unlikely(!folio_test_swapcache(folio))) {\n \t\tif (swap_sync_discard())\n \t\t\tgoto again;\n@@ -1945,9 +1901,7 @@ swp_entry_t swap_alloc_hibernation_slot(int type)\n \t\t\t * Grab the local lock to be compliant\n \t\t\t * with swap table allocation.\n \t\t\t */\n-\t\t\tlocal_lock(&percpu_swap_cluster.lock);\n \t\t\toffset = cluster_alloc_swap_entry(si, NULL);\n-\t\t\tlocal_unlock(&percpu_swap_cluster.lock);\n \t\t\tif (offset)\n \t\t\t\tentry = swp_entry(si->type, offset);\n \t\t}\n@@ -2751,28 +2705,6 @@ static void free_cluster_info(struct swap_cluster_info *cluster_info,\n \tkvfree(cluster_info);\n }\n \n-/*\n- * Called after swap device's reference count is dead, so\n- * neither scan nor allocation will use it.\n- */\n-static void flush_percpu_swap_cluster(struct swap_info_struct *si)\n-{\n-\tint cpu, i;\n-\tstruct swap_info_struct **pcp_si;\n-\n-\tfor_each_possible_cpu(cpu) {\n-\t\tpcp_si = per_cpu_ptr(percpu_swap_cluster.si, cpu);\n-\t\t/*\n-\t\t * Invalidate the percpu swap cluster cache, si->users\n-\t\t * is dead, so no new user will point to it, just flush\n-\t\t * any existing user.\n-\t\t */\n-\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n-\t\t\tcmpxchg(&pcp_si[i], si, NULL);\n-\t}\n-}\n-\n-\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n@@ -2856,7 +2788,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tflush_work(&p->discard_work);\n \tflush_work(&p->reclaim_work);\n-\tflush_percpu_swap_cluster(p);\n \n \tdestroy_swap_extents(p);\n \tif (p->flags & SWP_CONTINUED)\n@@ -2885,6 +2816,8 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tarch_swap_invalidate_area(p->type);\n \tzswap_swapoff(p->type);\n \tmutex_unlock(&swapon_mutex);\n+\tfree_percpu(p->percpu_cluster);\n+\tp->percpu_cluster = NULL;\n \tkfree(p->global_cluster);\n \tp->global_cluster = NULL;\n \tvfree(swap_map);\n@@ -3268,7 +3201,7 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n {\n \tunsigned long nr_clusters = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);\n \tstruct swap_cluster_info *cluster_info;\n-\tint err = -ENOMEM;\n+\tint cpu, err = -ENOMEM;\n \tunsigned long i;\n \n \tcluster_info = kvcalloc(nr_clusters, sizeof(*cluster_info), GFP_KERNEL);\n@@ -3278,14 +3211,27 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n \tfor (i = 0; i < nr_clusters; i++)\n \t\tspin_lock_init(&cluster_info[i].lock);\n \n-\tif (!(si->flags & SWP_SOLIDSTATE)) {\n+\tif (si->flags & SWP_SOLIDSTATE) {\n+\t\tsi->percpu_cluster = alloc_percpu(struct percpu_cluster);\n+\t\tif (!si->percpu_cluster)\n+\t\t\tgoto err;\n+\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tstruct percpu_cluster *cluster;\n+\n+\t\t\tcluster = per_cpu_ptr(si->percpu_cluster, cpu);\n+\t\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n+\t\t\t\tcluster->next[i] = SWAP_ENTRY_INVALID;\n+\t\t\tlocal_lock_init(&cluster->lock);\n+\t\t}\n+\t} else {\n \t\tsi->global_cluster = kmalloc(sizeof(*si->global_cluster),\n \t\t\t\t     GFP_KERNEL);\n \t\tif (!si->global_cluster)\n \t\t\tgoto err;\n \t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n \t\t\tsi->global_cluster->next[i] = SWAP_ENTRY_INVALID;\n-\t\tspin_lock_init(&si->global_cluster_lock);\n+\t\tspin_lock_init(&si->global_cluster->lock);\n \t}\n \n \t/*\n@@ -3566,6 +3512,8 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n bad_swap_unlock_inode:\n \tinode_unlock(inode);\n bad_swap:\n+\tfree_percpu(si->percpu_cluster);\n+\tsi->percpu_cluster = NULL;\n \tkfree(si->global_cluster);\n \tsi->global_cluster = NULL;\n \tinode = NULL;\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about swap device rotation causing fragmentation and performance regression when using per-device percpu clusters. They introduced a per-cpu cache for the swap device, which prioritizes the per-cpu cluster within the cached swap device, effectively restoring the traditional fastpath and slowpath flow. This approach minimizes side effects on the existing fastpath.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When using per-device percpu clusters (instead of a global one),\na naive allocation logic triggers swap device rotation on every\nallocation. This behavior leads to severe fragmentation and performance\nregression.\n\nTo address this, this patch introduces a per-cpu cache for the swap\ndevice. The allocation logic is updated to prioritize the per-cpu\ncluster within the cached swap device, effectively restoring the\ntraditional fastpath and slowpath flow. This approach minimizes side\neffects on the existing fastpath.\n\nWith this change, swap device rotation occurs only when the current\ncached device is unable to satisfy the allocation, rather than on\nevery attempt.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n include/linux/swap.h |  1 -\n mm/swapfile.c        | 78 +++++++++++++++++++++++++++++++++++++-------\n 2 files changed, 66 insertions(+), 13 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 6921e22b14d3..ac634a21683a 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -253,7 +253,6 @@ enum {\n   * throughput.\n   */\n struct percpu_cluster {\n-\tlocal_lock_t lock; /* Protect the percpu_cluster above */\n \tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n };\n \ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 5e3b87799440..0dcd451afee5 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -106,6 +106,16 @@ PLIST_HEAD(swap_active_head);\n static PLIST_HEAD(swap_avail_head);\n static DEFINE_SPINLOCK(swap_avail_lock);\n \n+struct percpu_swap_device {\n+\tstruct swap_info_struct *si[SWAP_NR_ORDERS];\n+\tlocal_lock_t lock;\n+};\n+\n+static DEFINE_PER_CPU(struct percpu_swap_device, percpu_swap_device) = {\n+\t.si = { NULL },\n+\t.lock = INIT_LOCAL_LOCK(),\n+};\n+\n struct swap_info_struct *swap_info[MAX_SWAPFILES];\n \n static struct kmem_cache *swap_table_cachep;\n@@ -465,7 +475,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * Swap allocator uses percpu clusters and holds the local lock.\n \t */\n \tlockdep_assert_held(&ci->lock);\n-\tlockdep_assert_held(this_cpu_ptr(&si->percpu_cluster->lock));\n+\tlockdep_assert_held(this_cpu_ptr(&percpu_swap_device.lock));\n \n \t/* The cluster must be free and was just isolated from the free list. */\n \tVM_WARN_ON_ONCE(ci->flags || !cluster_is_empty(ci));\n@@ -484,7 +494,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \tspin_unlock(&ci->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_unlock(&si->global_cluster->lock);\n-\tlocal_unlock(&si->percpu_cluster->lock);\n+\tlocal_unlock(&percpu_swap_device.lock);\n \n \ttable = swap_table_alloc(__GFP_HIGH | __GFP_NOMEMALLOC | GFP_KERNEL);\n \n@@ -496,7 +506,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * could happen with ignoring the percpu cluster is fragmentation,\n \t * which is acceptable since this fallback and race is rare.\n \t */\n-\tlocal_lock(&si->percpu_cluster->lock);\n+\tlocal_lock(&percpu_swap_device.lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_lock(&si->global_cluster->lock);\n \tspin_lock(&ci->lock);\n@@ -941,9 +951,10 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n out:\n \trelocate_cluster(si, ci);\n \tswap_cluster_unlock(ci);\n-\tif (si->flags & SWP_SOLIDSTATE)\n+\tif (si->flags & SWP_SOLIDSTATE) {\n \t\tthis_cpu_write(si->percpu_cluster->next[order], next);\n-\telse\n+\t\tthis_cpu_write(percpu_swap_device.si[order], si);\n+\t} else\n \t\tsi->global_cluster->next[order] = next;\n \n \treturn found;\n@@ -1041,7 +1052,6 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \n \tif (si->flags & SWP_SOLIDSTATE) {\n \t\t/* Fast path using per CPU cluster */\n-\t\tlocal_lock(&si->percpu_cluster->lock);\n \t\toffset = __this_cpu_read(si->percpu_cluster->next[order]);\n \t} else {\n \t\t/* Serialize HDD SWAP allocation for each device. */\n@@ -1119,9 +1129,7 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n done:\n-\tif (si->flags & SWP_SOLIDSTATE)\n-\t\tlocal_unlock(&si->percpu_cluster->lock);\n-\telse\n+\tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_unlock(&si->global_cluster->lock);\n \n \treturn found;\n@@ -1303,8 +1311,27 @@ static bool get_swap_device_info(struct swap_info_struct *si)\n \treturn true;\n }\n \n+static bool swap_alloc_fast(struct folio *folio)\n+{\n+\tunsigned int order = folio_order(folio);\n+\tstruct swap_info_struct *si;\n+\n+\t/*\n+\t * Once allocated, swap_info_struct will never be completely freed,\n+\t * so checking it's liveness by get_swap_device_info is enough.\n+\t */\n+\tsi = this_cpu_read(percpu_swap_device.si[order]);\n+\tif (!si || !get_swap_device_info(si))\n+\t\treturn false;\n+\n+\tcluster_alloc_swap_entry(si, folio);\n+\tput_swap_device(si);\n+\n+\treturn folio_test_swapcache(folio);\n+}\n+\n /* Rotate the device and switch to a new cluster */\n-static void swap_alloc_entry(struct folio *folio)\n+static void swap_alloc_slow(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n \tint mask = folio_memcg(folio) ?\n@@ -1482,7 +1509,11 @@ int folio_alloc_swap(struct folio *folio)\n \t}\n \n again:\n-\tswap_alloc_entry(folio);\n+\tlocal_lock(&percpu_swap_device.lock);\n+\tif (!swap_alloc_fast(folio))\n+\t\tswap_alloc_slow(folio);\n+\tlocal_unlock(&percpu_swap_device.lock);\n+\n \tif (!order && unlikely(!folio_test_swapcache(folio))) {\n \t\tif (swap_sync_discard())\n \t\t\tgoto again;\n@@ -1901,7 +1932,9 @@ swp_entry_t swap_alloc_hibernation_slot(int type)\n \t\t\t * Grab the local lock to be compliant\n \t\t\t * with swap table allocation.\n \t\t\t */\n+\t\t\tlocal_lock(&percpu_swap_device.lock);\n \t\t\toffset = cluster_alloc_swap_entry(si, NULL);\n+\t\t\tlocal_unlock(&percpu_swap_device.lock);\n \t\t\tif (offset)\n \t\t\t\tentry = swp_entry(si->type, offset);\n \t\t}\n@@ -2705,6 +2738,27 @@ static void free_cluster_info(struct swap_cluster_info *cluster_info,\n \tkvfree(cluster_info);\n }\n \n+/*\n+ * Called after swap device's reference count is dead, so\n+ * neither scan nor allocation will use it.\n+ */\n+static void flush_percpu_swap_device(struct swap_info_struct *si)\n+{\n+\tint cpu, i;\n+\tstruct swap_info_struct **pcp_si;\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcp_si = per_cpu_ptr(percpu_swap_device.si, cpu);\n+\t\t/*\n+\t\t * Invalidate the percpu swap device cache, si->users\n+\t\t * is dead, so no new user will point to it, just flush\n+\t\t * any existing user.\n+\t\t */\n+\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n+\t\t\tcmpxchg(&pcp_si[i], si, NULL);\n+\t}\n+}\n+\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n@@ -2788,6 +2842,7 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tflush_work(&p->discard_work);\n \tflush_work(&p->reclaim_work);\n+\tflush_percpu_swap_device(p);\n \n \tdestroy_swap_extents(p);\n \tif (p->flags & SWP_CONTINUED)\n@@ -3222,7 +3277,6 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n \t\t\tcluster = per_cpu_ptr(si->percpu_cluster, cpu);\n \t\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n \t\t\t\tcluster->next[i] = SWAP_ENTRY_INVALID;\n-\t\t\tlocal_lock_init(&cluster->lock);\n \t\t}\n \t} else {\n \t\tsi->global_cluster = kmalloc(sizeof(*si->global_cluster),\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the swap_tier structure simplification removed the 'end prio' and priority lists, which were replaced with a standard list_head, but he requested that the documentation be updated to reflect this change.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Youngjun,\n\nOn Sun, Jan 25, 2026 at 10:53PM Youngjun Park <youngjun.park@lge.com> wrote:",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested breaking down the patch series into smaller, more manageable steps, starting with defining the tiers bits without any deletions, and then building upon that in subsequent steps.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Thanks for the patches series.\n\nSorry for the late reply. I have been wanting to reply to it but get\nsuper busy at work.\n\nSome high level feedback for the series. Now that you demonstrated the\nwhole series, let's focus on making small mergiable baby steps. Just\nlike the swap table has different phases. Make each step minimal, each\nstep shows some value. Do the MVP, we can always add more features as\na follow up step.\n\nI suggest the first step is getting the tiers bits defined. Add only,\nno delete.  Get that reviewed and merged, then the next step is to use\nthose tiers.\n\nChris",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested replacing per-cpu allocation for each swap device with a global per-cpu cluster per tier, citing that the number of tiers is smaller than the number of swap devices and expecting this change to be beneficial.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "One idea is that, instead of using percpu per swap device.\nYou can make the global percpu cluster per tier. Because the max tier\nnumber is smaller than the max number of swap devices. That is likely\na win.\n\nChris",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li pointed out that the swap_tier structure simplification removed the 'end prio' and priority lists, which were necessary for correct tier selection and enforcement of cgroup hierarchy principles.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Yongjun,\n\nOn Sun, Jan 25, 2026 at 10:53PM Youngjun Park <youngjun.park@lge.com> wrote:",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested introducing a CONFIG option to limit the maximum number of swap tiers, recommending a default value of 4.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "We can have a CONFIG option for the MAX_SWAPTIER. I think the default\nshould be a small number like 4.",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that when modifying a tier, it may cause swap files to move to different tiers, which could be problematic.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential issue",
                "problematic"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "When we add, modify, remove a tier. The simple case is there is no\nswap file under any tiers.\nBut if the modification causes some swap files to jump to different\ntiers. That might be problematic.",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li expressed concern about the complexity of the patch, specifically the need for save and restore operations, and requested a simpler design.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I really hope we don't have to do the save and restore thing. Is there\nanother design we can simplify this?",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested that each tier have its own swap_active_head, allowing different swap entries on different tiers to release without competing for the same lock, and proposed that swapfiles should not be allowed to jump between tiers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "One idea is to make each tier have swap_active_head. So different swap\nentry releases on different tiers don't need to be competing on the\nsame swap_active_head.\n\nThat will require the swapfile don't jump to another tiers.\n\nChris",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li initially suggested a simplification of the swap_tier structure, specifically replacing 'end prio' and priority lists with standard list_head, but later takes back this suggestion stating that adding tier names alone does not add real value.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "reversal of opinion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Just take a quick look at the series. I take that suggestion back.\nThis series is actually not too long. Adding the tiers name alone does\nnot add any real value. I actually need to look at the whole series\nrather than just the tier name alone.\n\nChris",
              "reply_to": "",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham",
              "summary": "Reviewer Nhat Pham questioned the consistency of the patch description, pointing out that the '+' operator was removed but its reference remained in the explanation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "inconsistency"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This comment seems a bit clunky to me. The \"+\" is removed, as noted\nabove, but then why are we saying \"even if a child re-enables a tier\nwith \"+\"\" here? Am I missing something?",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham",
              "summary": "Reviewer Nhat Pham questioned the logic for restricting child cgroup's allowed swap tiers, suggesting it should be a subset of its ancestors and children, finding the current system complex.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "But otherwise, I assume you mean to restrict child's allowed swap\ntiers to be a subset of children and its ancestors? That seems more\nstraightforward to me than the last system :)",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt noted that the patch does not handle the case where a cgroup is removed from its parent's hierarchy, which can cause swap tiers to be left in an inconsistent state.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "inconsistent state",
                "cgroup removal"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Youngjun,\n\nOn Mon, Jan 26, 2026 at 03:52:37PM +0900, Youngjun Park wrote:",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed concerns that adding a memcg interface for swap tier control is unnecessary, suggesting instead the use of BPF to expose this functionality. He argued that workloads should not be allowed to pick and choose swap devices, but rather it's the job orchestator or node controller's decision.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "One of the LPC feedback you missed is to not add memcg interface for\nthis functionality and explore BPF way instead.\n\nWe are normally very conservative to add new interfaces to cgroup.\nHowever I am not even convinced that memcg interface is the right way to\nexpose this functionality. Swap is currently global and the idea to\nlimit or assign specific swap devices to specific cgroups makes sense\nbut that is the decision for the job orchestator or node controller.\nAllowing workloads to pick and choose swap devices do not make sense to\nme.\n\nShakeel",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Chris Li's concern about the patch series being too large by proposing a modified roadmap to break it down into smaller, mergeable steps, ensuring each step demonstrates some value and usability.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "proposed a plan"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Chris,\n\nThank you for the direction.\n\nI agree that breaking the series into smaller, mergeable steps is the\nright approach. However, since introducing the definitions alone might\nlack immediate usage, I propose a slightly\nmodified roadmap to ensure Step 1 demonstrates some value.\n\nHere is the plan I have in mind.\n\n1. Swap Tier Definition & Addition\n   - Introduce the concept, grouping logic, and the 'add' interface.\n   - Value: Enables basic exception handling within the swap device\n     itself using tiers.\n\n2. Advanced Control (Delete/Modify)\n   - Implement logic to remove or update tiers.\n   - Value: Enhances the usability and management of the tiers\n     established in Step 1.\n\n3. External Integration (memcg, bpf etc ... )\n   - Apply swap tiers for broader swap control.\n   - Value: Connects swap tiers to other subsystems like memcg.\n\nDoes this roadmap look reasonable to you? I will proceed with preparing\nthe real patch series based on this structure.\n\nBest regards,\nYoungjun",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged the need to limit swap file allocation by adding a CONFIG option and ensuring it does not exceed MAX_SWAPFILE.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for fix",
                "agreed to add configuration option"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sounds good. I will add a CONFIG option for it and ensure it doesn't exceed\nMAX_SWAPFILE.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Chris Li's feedback about mixed operations in the swap tier interface, proposing to restrict it to single operations at a time due to concerns about error-proneness and performance.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges fix needed",
                "proposes alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I have given this a lot of thought.\n\nSince the current interface allows mixing add (+), remove (-), and modify\noperations, we must either restore from a saved state or reverse the\nsuccessful individual operations upon failure.\n\nI implemented both approaches and concluded that reversing individual\noperations is error-prone. Also, it could be slow if there are many\noperations.\n\nAnother approach could be using a \"global clone tier\" strategy.\n(Because operation globally synchronized)\n\nTherefore, I would like to propose restricting the interface to handle a\nsingle operation at a time. What do you think?",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged that the tier structure allows for limiting contention between swap devices within the same tier, and expressed agreement on this point.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreement",
                "acknowledgment"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree. With the tier structure, we can limit contention to objects within\nthe same tier.\n\nI also think swap_avail_list could be optimized in a similar way in the\nfuture.\n\nYoungjun",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledges that removing the remove and modify operations from the patch is a feasible direction, but does not explicitly confirm agreement or plan to restructure in v2.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledges feasibility",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oops, I replied to your previous email before seeing this one.\n\nStripping out the remove/modify parts is also feasible. Do you agree with\nthat direction?\n\nYoungjun",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Nhat Pham's feedback about the default state of swap tiers and how '+' is used, explaining that they are changing the model to a subtraction-based one where all tiers are selected by default and users use '-' to exclude specific ones.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "To clarify, previously, the default state used all tiers. Using \"+\"              \nswitched to \"an exclusive mode\"  where only that specific tier was used.         \n                                                                                 \nI am changing this to a subtraction-based model. By default, all tiers           \nare selected, and users use \"-\" to exclude specific ones.                        \n(Then not \"removed\" but \"changed\" is more proper?)                               \n                                                                                 \nIn this context, I intended \"+\" to be used to restore a tier that was            \npreviously excluded by \"-\".",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged a concern about the swapoff path and agreed to restructure it in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, that's right :)\n\nThanks \nYoungjun Park.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Shakeel Butt's feedback about using the BPF approach for swap control, agreeing it would provide flexibility but expressing concerns about logical contradictions and hierarchy semantics. The author believes implementing a strict cgroup interface is preferable to avoid potential conflicts.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "concerns about BPF approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Apologies for overlooking the feedback regarding the BPF approach. Thank you\nfor the suggestion.\n\nI agree that using BPF would provide greater flexibility, allowing control not\njust at the memcg level, but also per-process or for complex workloads.\n(As like orchestrator and node controller)\n\nHowever, I am concerned that this level of freedom might introduce logical\ncontradictions, particularly regarding cgroup hierarchy semantics.\n\nFor example, BPF might allow a topology that violates hierarchical constraints\n(a concern that was also touched upon during LPC)\n\n  - Group A (Parent): Assigned to SSD1\n  - Group B (Child of A): Assigned to SSD2\n\nIf Group A has a `memory.swap.max` limit, and Group B swaps out to SSD2, it\ncreates a consistency issue. Group B consumes Group A's swap quota, but it is\nutilizing a device (SSD2) that is distinct from the Parent's assignment. This\ncould lead to situations where the Parent's limit is exhausted by usage on a\ndevice it effectively doesn't \"own\" or shouldn't be using.\n\nOne might suggest restricting BPF to strictly adhere to these hierarchical\nconstraints. However, doing so would effectively eliminate the primary\nadvantage of using BPF\\u2014its flexibility. If we are to enforce standard cgroup\nsemantics anyway, a native interface seems more appropriate than a constrained\nBPF hook.\n\nBeyond this specific example, I suspect that delegating this logic to BPF\nmight introduce other unforeseen edge cases regarding hierarchy enforcement.\nIn my view, the BPF approach seems more like a \"next step.\"\n\nSince you acknowledged that the idea of assigning swap devices to cgroups\n\"makes sense,\" I believe implementing this within the standard, strictly\nconstrained \"cgroup land\" is preferable. \n\nA strict cgroup interface ensures\nthat hierarchy and accounting rules are consistently enforced, avoiding the\npotential conflicts that the unrestricted freedom of BPF might create.\n\nUltimately, I hope this swap tier mechanism can serve as a foundation to be\nleveraged by other subsystems, such as BPF and DAMON. I view this proposal as\nthe necessary first step toward that future.\n\nYoungjun Park",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "The author acknowledged that existing swapfiles' tier assignment is immutable and addressed the concern by removing tier reference, instead using operation-time validation to guarantee this invariant.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a specific issue",
                "provided a clear solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I missed one comment. \n\nThe tier of existing swapfiles is immutable once assigned at swapon.\nI removed tier reference.\nInstead of reference counting, each operation validates the tier\nrange at operation time to guarantee this invariant.\n\n- add:    Does not change existing swapfiles' tier. New tier may\n          split priority range, but existing assignments stay.\n- remove: Rejected with -EBUSY if any swapfile is attached.\n- modify: Rejected if the change would cause any swapfile to\n          move to a different tier.\n\nSo swapfiles never jump between tiers at runtime.\n\nYoungjun Park",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer requested that further discussion be concluded on the previous patch version before a new one is sent.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested conclusion of previous discussion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Please don't send a new version of the series before concluding the discussion\non the previous one.\n\nOn Fri, Feb 13, 2026 at 12:58:40PM +0900, YoungJun Park wrote:",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed concerns about introducing stable interfaces for swap tiers, requesting a BPF approach first and questioning the need for hierarchical control.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "expressed concerns"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes it provides the flexibility but that is not the main reason I am pushing for\nit. The reason I want you to first try the BPF approach without introducing any\nstable interfaces. Show how swap tiers will be used and configured in production\nenvironment and then we can talk if a stable interface is needed. I am still not\nconvinced that swap tiers need to be controlled hierarchically and the non-root\nshould be able to control it.",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that while BPF offers more flexibility, its control is limited to administrators who may inadvertently cause issues.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes",
                "concerns about admin control"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes BPF provides more power but it is controlled by admin and admin can shoot\ntheir foot in multiple ways.",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed concerns about the lack of clear requirements for workload-specific swap device ordering, questioning whether a tiered approach is necessary and suggesting that the solution should be more future-proof.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "lack of clear requirements",
                "questioning necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No need to constraint anything.\n\nTaking a step back, can you describe your use-case a bit more and share\nrequirements?\n\nYou have multiple swap devices of different properties and you want to assign\nthose swap devices to different workloads. Now couple of questions:\n\n1. If more than one device is assign to a workload, do you want to have\n   some kind of ordering between them for the worklod or do you want option to\n   have round robin kind of policy?\n\n2. What's the reason to use 'tiers' in the name? Is it similar to memory tiers\n   and you want promotion/demotion among the tiers?\n\n3. If a workload has multiple swap devices assigned, can you describe the\n   scenario where such workloads need to partition/divide given devices to their\n   sub-workloads?\n\nLet's start with these questions. Please note that I want us to not just look at\nthe current use-case but brainstorm more future use-cases and then come up with\nthe solution which is more future proof.\n\nthanks,\nShakeel",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the discussion on the patch has been ongoing for over a week without a response from YoungJun, and suggested being more lenient in considering it one of the iterations due to the difficulty of contributing to the kernel and the prevalence of differing opinions on the mailing list.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "leniency",
                "understanding"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "In this case I think it is fine.  You haven't responded to YoungJun's\nlast response in over a week. He might have mistaken that the\ndiscussion concluded.\nConsider it is one of the iterations. It is hard enough to contribute\nto the kernel. Relax.\nPlus, much of the discussion on the mailing list always has differing\nopinions. So, it's hard to determine what is truly concluded.\nDifferent people might have different interitations of the same text.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested that instead of addressing the issue directly, a config option could be added to protect the problematic feature and mark it as experimental, allowing for further testing and feedback.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "config option"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Is that your biggest concern? Many different ways exist to solve that\nproblem. e.g. We can put a config option protecting it and mark it as\nexperimental. This will unblock the development allow experiment. We\ncan have more people to try it out and give feedback.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li confirmed that his company uses different swap devices at different cgroup levels, emphasizing the practical need for control at non-root levels.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no specific technical concerns raised",
                "emphasis on real-world usage"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, my company uses a different swap device at different cgroup\nlevel. I did ask my coworker to confirm that usage. Control at the non\nroot level is a real need.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the current patch does not address the need for generic swap device control, which was a concern raised during the zswap.writeback proposal. He suggested that this patch is more generic and should be considered as an alternative to zswap.writeback.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concerns about lack of generality",
                "request for improvement"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think this swap device control is a very basic need. All your\nobjections to swapping control in the group can equally apply to\nzswap.writeback. Unlike zswap.writeback, which only control from the\nzswap behavior. This is a more generic version control swap device\nother than zswap as well. BTW, I raised that concern about\nzswap.writeback was not generic enough as swap control was limited\nwhen zswap was proposed. We did hold back zswap.writeback. The\nconsensers is interface can be improved as later iterations. So here\nwe are.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li expressed frustration about the lack of a specific patch or thread to reference, but also mentioned that their team has a cgroup swapfile control interface that could be replaced by this upstreamed solution.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "frustration",
                "opportunity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "There is a very long thread on the linux-mm maillist. I'm too lazy to dig it up.\n\nI can share our usage requirement to refresh your memory. We\ninternally use a cgroup swapfile control interface that has not been\nupstreamed. With this we can remove the need of that internal\ninterface and go upstream instead.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the swap tier system's performance is dependent on the number of devices within each tier, and suggested using a round-robin approach to distribute swap operations across devices within the same tier.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "performance concern",
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It depends on the number of devices in the tiers. Different tiers\nmaintain an order. Within the same tier round robin.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li proposed the name 'tier' for the new abstraction, acknowledging it was inspired by memory tiers and suggesting alternative names such as 'swap.device_speed_classes'.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear technical issue raised",
                "request for naming suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I propose the tier name. Guilty. Yes, in was inpired by memory tiers.\nIt just different class of swap speeds. I am not fixed on the name. We\ncan also call it swap.device_speed_classes. You can suggest\nalternatives.\n\nPromotion / demotion is possible in the future. The current state,\nwithout promotion or demotion, already provides value. Our current\ndeployment uses only one class of swap device at a time. However I do\nknow other companies use  more than one class of swap device.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that their deployment uses multiple swap devices to reduce lock contention and suggested that the patch should consider this use case, specifically allowing for jobs to be configured to tolerate slower speeds.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "consideration of real-world deployment scenarios"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "In our deployment, we always use more than one swap device to reduce\nswap device lock contention.\nThe job config can describe the swap speed it can tolerate. Some jobs\ncan tolerate slower speeds, while others cannot.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li expressed a neutral sentiment, suggesting that the patch should start from the current need and not over-engineer for future-proofing, citing zswap.writeback as an example of incremental improvement.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "incremental progress is better",
                "starting from the current need is a solid starting point"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Take zswap.writeback as example. We have a solution that worked for\nthe requirement at that time. Incremental improvement is fine as well.\nUsually, incremental progress is better. At least currently there is a\nreal need to allow different cgroups to select different swap speeds.\nThere is a risk in being too future-proof: we might design things that\npeople in the future don't use as we envisioned. I see that happen too\noften as well.\n\nSo starting from the current need is a solid starting point. It's just\na different design philosophy. Each to their own.\n\nThat is the only usage case I know. YoungJun feel free to add yours\nusage as well.\n\nChris",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledges Shakeel Butt's feedback about swapoff path and agrees to restructure in v2",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged",
                "agreed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Understood. Let's continue the discussion. :D\n\nChris has already provided a thorough response, but I would like to\nadd my perspective as well.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged a concern about committing to a stable interface too early and proposed two possible solutions: guarding the interface behind a build-time config option or marking it as experimental, but also expressed concerns that BPF-driven swap device assignments could conflict with memcg hierarchy semantics.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged concern",
                "proposed solutions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I understand your concern about committing to a stable interface too\nearly. As Chris suggested, we could reduce this concern by guarding\nthe interface behind a build-time config option or marking it as\nexperimental, which I will also touch on further below.\n\nOn that note, if BPF were to become the primary control mechanism,\nI am not sure a memcg interface would still be needed at all, since\nBPF already provides a high degree of freedom. However, that level\nof freedom is also what concerns me -- BPF-driven swap device\nassignments could subtly conflict with memcg hierarchy semantics in\nways that are hard to predict or debug. A more constrained memcg-based\napproach might actually be safer in that regard.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged a need for further clarification on how swap tiers interact with cgroup hierarchy, pointing to their response to question #3 as providing more context.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification requested",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think this concern is closely tied to your question #3 below about\nconcrete use cases for partitioning devices across sub-workloads.\nI hope my answer there helps clarify this.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author suggests that introducing build-time config or runtime constraints can help define and predict usage of the 'Swap Tiers' feature.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "author provides alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "As I mentioned above, I think guarding the feature behind a build-time\nconfig or runtime constraints could keep the usage well-defined and\npredictable, while still being useful.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged Shakeel Butt's concern about the patch's complexity and explained that their use case is simpler than initially proposed, but did not address the broader scenarios mentioned by Shakeel.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Our use case is simple at now. \nWe have two swap devices with different performance\ncharacteristics and want to assign different swap devices to different\nworkloads (cgroups).\n\nFor some background, when I initially proposed this, I suggested allowing\nper-cgroup swap device priorities so that it could also accommodate the\nbroader scenarios you mentioned. However, since even our own use case\ndoes not require reversing swap priorities within a cgroup, we pivoted\nto the \"swap tier\" mechanism that Chris proposed.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Shakeel Butt's concern about how swap devices are ordered when they have the same priority within a tier, explaining that round-robin ordering is used in this case and ordering applies otherwise.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Both. If devices are in the same tier with the same priority, round robin.\nIf they are in the same tier with different priorities, or in different\ntiers, ordering applies. The current tier structure should be able to\nsatisfy either preference.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), but did not confirm whether a fix is planned.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "did not confirm"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This was originally Chris's idea. I think he explained the rationale\nwell in his reply.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged the need to reduce lock contention in swap device allocation, suggesting a possible solution of partitioning devices between parent and child cgroups.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged",
                "possible scenario"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "One possible scenario is reducing lock contention by partitioning swap\ndevices between parent and child cgroups.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Shakeel Butt's concern about the suitability of Swap Tiers for future use cases by explaining that it is hard to design a solution at this point and proposing BPF as a natural extension path.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying explanation",
                "no clear resolution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "We have clear production use cases from both us and Chris, and I also\npresented a deployment example in the cover letter.\n\nI think it is hard to design concretely for future use cases at this\npoint. When those needs become clearer, BPF with its flexibility\nwould be a better fit then. I see BPF as a natural extension path\nrather than a starting point.\n\nFor now, guarding the memcg & tier behind a CONFIG option would\nlet us move forward without committing to a stable interface, and\nwe can always pivot to BPF later if needed\n\nThanks,\nYoungJun Park",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed concerns about the practical application of the patch, stating that he doesn't see the real use-case for controlling/partitioning swap devices among sub-workloads and believes adding a stable API is premature.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, that is secondary because I am not seeing the real use-case of\ncontrolling/partitioning swap devices among sub-workloads. Until that is\nfigured out, adding a stable API is not good.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt questioned whether the patch's concept of 'Swap Tiers' aligns with Google's previous approach to per-cgroup swap control, specifically mentioning memory.swapfiles interface and its deprecation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "unclear alignment with prior work",
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I am assuming you meant Google and particularly Prodkernel team and not\nAndroid or ChromeOS. Google's prodkernel used to have per-cgroup\nswapfiles exposed through memory.swapfiles (if I remember correctly\nSuleiman implemented this along with ghost swapfiles). Later this was\ndeprecated (by Yu Zhao) and global (ghost) swapfiles were being used.\nThe memory.swapfiles interface instead of supporting real swapfiles\nstarted having select options among default, ghost/zswap and real\n(something like that). However such interface was used to just disable\nor enable zswap for a workload and never about hierarchically\ncontrolling the swap devices (Google prodkernel only have zswap). Has\nsomething changed?",
              "reply_to": "Chris Li",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed skepticism about the introduction of a new swap tier interface without a clear use case, indicating that it may be premature to add this feature.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "skepticism",
                "premature"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This just motivates me to pushback even harder on adding a new interface\nwithout a clear use-case.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt requested a clear explanation of the real-world use case for hierarchical swap device control, expressing skepticism about the patch's value without such an example.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "skepticism",
                "lack of concrete evidence"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I already asked above but let me say it again. What's the actual real\nworld use-case to control/allow/disallow swap devices hierarchically?",
              "reply_to": "Chris Li",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt noted that having multiple swap devices reduces lock contention, but this does not address hierarchical control of swap devices among sub-workloads.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Having more than one swap devices to reduce lock contention is unrelated\nto hierarchically control swap devices among sub-workloads.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author is addressing concerns about the BPF-first approach suggested by reviewer Shakeel Butt, specifically questioning its feasibility in an embedded environment and seeking clarification on precedents for BPF prototypes becoming stable kernel interfaces.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "seeking clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "....\n\nAfter reading the reply and re-think more of it.\n\nI have a few questions regarding the BPF-first approach you\nsuggested, if you don't mind. Some of them I am re-asking\nbecause I feel they have not been clearly addressed yet.\n\n- We are in an embedded environment where enabling additional\n  kernel compile options is costly. BPF is disabled by\n  default in some of our production configurations. From a\n  trade-off perspective, does it make sense to enable BPF\n  just for swap device control?\n\n- You suggest starting with BPF and discussing a stable\n  interface later. I am genuinely curious, are there actual\n  precedents where a BPF prototype graduated into a stable\n  kernel interface? \n\n- You raised that stable interfaces are hard to remove. Would\n  gating it behind a CONFIG option or marking it experimental\n  be an acceptable compromise?\n\n- You already acknowledged the use-case for assigning\n  different swap devices to different workloads. Your\n  objection is specifically about hierarchical parent-child\n  partitioning. If the interface enforced uniform policy\n  within a subtree, would that be acceptable?\n\n- We already run a modified kernel with internal swap control\n  in production and have real feedback from it. Requiring BPF\n  as a prerequisite to gather production experience seems\n  unnecessary when we are already doing that.\n\nTo be honest, I am having trouble understanding the motivation\nbehind the BPF-first validation approach. If the real point is\nthat BPF enables more flexible swap-out policies than any fixed\ninterface can, that would make much more sense to me. I would\nappreciate it if you could share more on this.\n\nThanks,\nYoungjun Park",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-22",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that the swap_tier structure's list_head is not properly initialized in the modify operation, potentially leading to a use-after-free error.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential bug",
                "use-after-free"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi YoungJun,\n\nI see you have sent a separate email on BPF specific questions to which I will\nrespond separately, here I will respond to other questions/comments.\n\nOn Sat, Feb 21, 2026 at 11:30:59PM +0900, YoungJun Park wrote:",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-22",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt requested additional information about the cgroup hierarchy structure of Youngjun Park's deployment, specifically asking if they use cgroup v1 or v2 in their production environment.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If you don't mind, can you share a bit more about the cgroup hierarchy structure\nof your deployment. Do you use cgroup v1 or v2 on your production environment?",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-22",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt questioned whether the proposed patch introduces new or modified swap priority behavior, seeking clarification on how it differs from existing priorities.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "seeking clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I assume this is the same swap priorities as of today, right? You want similar\npriority behavior within a tier.",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-22",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt requested that the patch authors gather all options and weigh their pros and cons before making an informed decision, suggesting a more thorough evaluation process.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested changes",
                "wanted to brainstorm"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think your use-case is very clear. Before committing to any options, I want us\nto brainstorm all options and gather pros/cons and then make an informed\ndecision. Anyways I will respond to your other email (in a day or two).\n\nShakeel",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-22",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH v2 0/5] mm/swap, memcg: Introduce swap tiers for cgroup based swap control",
          "message_id": "aZjxP2sTavBRGC1l@linux.dev",
          "url": "https://lore.kernel.org/all/aZjxP2sTavBRGC1l@linux.dev/",
          "date": "2026-02-21T03:47:40Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern that the effective tiers are calculated separately from the configured tiers, and explained that this is done to respect the cgroup hierarchy. The author noted that configured tiers may differ from effective ones because they must be a subset of the parent's, but did not indicate any plans for a fix.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "no clear resolution signal",
                "clarification or explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch integrates the swap tier infrastructure with cgroup,\nenabling the selection of specific swap devices per cgroup by\nconfiguring allowed swap tiers.\n\nThe new `memory.swap.tiers` interface controls allowed swap tiers via a mask.\nBy default, the mask is set to include all tiers, allowing specific tiers to\nbe excluded or restored. Note that effective tiers are calculated separately\nusing a dedicated mask to respect the cgroup hierarchy. Consequently,\nconfigured tiers may differ from effective ones, as they must be a subset\nof the parent's.\n\nNote that cgroups do not pin swap tiers. This is similar to the\n`cpuset` controller, which does not prevent CPU hotplug. This\napproach ensures flexibility by allowing tier configuration changes\nregardless of cgroup usage.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n Documentation/admin-guide/cgroup-v2.rst | 27 +++++++++\n include/linux/memcontrol.h              |  3 +-\n mm/memcontrol.c                         | 80 +++++++++++++++++++++++++\n mm/swap_tier.c                          | 66 ++++++++++++++++++++\n mm/swap_tier.h                          | 21 +++++++\n mm/swapfile.c                           |  5 ++\n 6 files changed, 201 insertions(+), 1 deletion(-)\n\ndiff --git a/Documentation/admin-guide/cgroup-v2.rst b/Documentation/admin-guide/cgroup-v2.rst\nindex 7f5b59d95fce..776a908ce1b9 100644\n--- a/Documentation/admin-guide/cgroup-v2.rst\n+++ b/Documentation/admin-guide/cgroup-v2.rst\n@@ -1848,6 +1848,33 @@ The following nested keys are defined.\n \tSwap usage hard limit.  If a cgroup's swap usage reaches this\n \tlimit, anonymous memory of the cgroup will not be swapped out.\n \n+  memory.swap.tiers\n+        A read-write nested-keyed file which exists on non-root\n+        cgroups. The default is to enable all tiers.\n+\n+        This interface allows selecting which swap tiers a cgroup can\n+        use for swapping out memory.\n+\n+        The effective tiers are inherited from the parent. Only tiers\n+        effective in the parent can be effective in the child. However,\n+        the child can explicitly disable tiers allowed by the parent.\n+\n+        When read, the file shows two lines:\n+          - The first line shows the operation string that was\n+            written to this file.\n+          - The second line shows the effective operation after\n+            merging with parent settings.\n+\n+        When writing, the format is:\n+          (+/-)(TIER_NAME) (+/-)(TIER_NAME) ...\n+\n+        Valid tier names are those configured in\n+        /sys/kernel/mm/swap/tiers.\n+\n+        Each tier can be prefixed with:\n+          +    Enable this tier\n+          -    Disable this tier\n+\n   memory.swap.events\n \tA read-only flat-keyed file which exists on non-root cgroups.\n \tThe following entries are defined.  Unless specified\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex b6c82c8f73e1..542bee1b5f60 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -283,7 +283,8 @@ struct mem_cgroup {\n \t/* per-memcg mm_struct list */\n \tstruct lru_gen_mm_list mm_list;\n #endif\n-\n+\tint tier_mask;\n+\tint tier_effective_mask;\n #ifdef CONFIG_MEMCG_V1\n \t/* Legacy consumer-oriented counters */\n \tstruct page_counter kmem;\t\t/* v1 only */\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 007413a53b45..c0a0a957a630 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -68,6 +68,7 @@\n #include <net/ip.h>\n #include \"slab.h\"\n #include \"memcontrol-v1.h\"\n+#include \"swap_tier.h\"\n \n #include <linux/uaccess.h>\n \n@@ -3691,6 +3692,7 @@ static void mem_cgroup_free(struct mem_cgroup *memcg)\n {\n \tlru_gen_exit_memcg(memcg);\n \tmemcg_wb_domain_exit(memcg);\n+\tswap_tiers_memcg_sync_mask(memcg);\n \t__mem_cgroup_free(memcg);\n }\n \n@@ -3792,6 +3794,9 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \tWRITE_ONCE(memcg->zswap_writeback, true);\n #endif\n \tpage_counter_set_high(&memcg->swap, PAGE_COUNTER_MAX);\n+\tmemcg->tier_mask = TIER_ALL_MASK;\n+\tswap_tiers_memcg_inherit_mask(memcg, parent);\n+\n \tif (parent) {\n \t\tWRITE_ONCE(memcg->swappiness, mem_cgroup_swappiness(parent));\n \n@@ -5352,6 +5357,75 @@ static int swap_events_show(struct seq_file *m, void *v)\n \treturn 0;\n }\n \n+static int swap_tier_show(struct seq_file *m, void *v)\n+{\n+\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);\n+\n+\tswap_tiers_mask_show(m, memcg->tier_mask);\n+\tswap_tiers_mask_show(m, memcg->tier_effective_mask);\n+\n+\treturn 0;\n+}\n+\n+static ssize_t swap_tier_write(struct kernfs_open_file *of,\n+\t\t\t\tchar *buf, size_t nbytes, loff_t off)\n+{\n+\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n+\tchar *pos, *token;\n+\tint ret = 0;\n+\n+\tpos = strstrip(buf);\n+\n+\tspin_lock(&swap_tier_lock);\n+\tif (!*pos) {\n+\t\tmemcg->tier_mask = TIER_ALL_MASK;\n+\t\tgoto sync;\n+\t}\n+\n+\twhile ((token = strsep(&pos, \" \\t\\n\")) != NULL) {\n+\t\tint mask;\n+\n+\t\tif (!*token)\n+\t\t\tcontinue;\n+\n+\t\tif (token[0] != '-' && token[0] != '+') {\n+\t\t\tret = -EINVAL;\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\tmask = swap_tiers_mask_lookup(token+1);\n+\t\tif (!mask) {\n+\t\t\tret = -EINVAL;\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\t/*\n+\t\t * if child already set, cannot add that tiers for hierarch mismatching.\n+\t\t * parent compatible, child must respect parent selected swap device.\n+\t\t */\n+\t\tswitch (token[0]) {\n+\t\tcase '-':\n+\t\t\tmemcg->tier_mask &= ~mask;\n+\t\t\tbreak;\n+\t\tcase '+':\n+\t\t\tmemcg->tier_mask |= mask;\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tret = -EINVAL;\n+\t\t\tbreak;\n+\t\t}\n+\n+\t\tif (ret)\n+\t\t\tgoto err;\n+\t}\n+\n+sync:\n+\t__swap_tiers_memcg_sync_mask(memcg);\n+err:\n+\tspin_unlock(&swap_tier_lock);\n+\treturn ret ? ret : nbytes;\n+}\n+\n static struct cftype swap_files[] = {\n \t{\n \t\t.name = \"swap.current\",\n@@ -5384,6 +5458,12 @@ static struct cftype swap_files[] = {\n \t\t.file_offset = offsetof(struct mem_cgroup, swap_events_file),\n \t\t.seq_show = swap_events_show,\n \t},\n+\t{\n+\t\t.name = \"swap.tiers\",\n+\t\t.flags = CFTYPE_NOT_ON_ROOT,\n+\t\t.seq_show = swap_tier_show,\n+\t\t.write = swap_tier_write,\n+\t},\n \t{ }\t/* terminate */\n };\n \ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nindex d90f6eccb908..e860c87292e2 100644\n--- a/mm/swap_tier.c\n+++ b/mm/swap_tier.c\n@@ -384,3 +384,69 @@ bool swap_tiers_update(void)\n \n \treturn true;\n }\n+\n+void swap_tiers_mask_show(struct seq_file *m, int mask)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tspin_lock(&swap_tier_lock);\n+\tfor_each_active_tier(tier) {\n+\t\tif (mask & TIER_MASK(tier))\n+\t\t\tseq_printf(m, \"%s \", tier->name);\n+\t}\n+\tspin_unlock(&swap_tier_lock);\n+\tseq_puts(m, \"\\n\");\n+}\n+\n+int swap_tiers_mask_lookup(const char *name)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (!strcmp(name, tier->name))\n+\t\t\treturn TIER_MASK(tier);\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static void __swap_tier_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent)\n+{\n+\tint effective_mask\n+\t\t= parent ? parent->tier_effective_mask : TIER_ALL_MASK;\n+\n+\tmemcg->tier_effective_mask\n+\t\t= effective_mask & memcg->tier_mask;\n+}\n+\n+void swap_tiers_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent)\n+{\n+\tspin_lock(&swap_tier_lock);\n+\t__swap_tier_memcg_inherit_mask(memcg, parent);\n+\tspin_unlock(&swap_tier_lock);\n+}\n+\n+void __swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg)\n+{\n+\tstruct mem_cgroup *child;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tif (memcg == root_mem_cgroup)\n+\t\treturn;\n+\n+\tfor_each_mem_cgroup_tree(child, memcg)\n+\t\t__swap_tier_memcg_inherit_mask(child, parent_mem_cgroup(child));\n+}\n+\n+void swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg)\n+{\n+\tspin_lock(&swap_tier_lock);\n+\tmemcg->tier_mask = TIER_ALL_MASK;\n+\t__swap_tiers_memcg_sync_mask(memcg);\n+\tspin_unlock(&swap_tier_lock);\n+}\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nindex de81d540e3b5..8652a7f993ab 100644\n--- a/mm/swap_tier.h\n+++ b/mm/swap_tier.h\n@@ -46,4 +46,25 @@ bool swap_tiers_update(void);\n /* Tier assignment */\n void swap_tiers_assign_dev(struct swap_info_struct *swp);\n \n+/* Memcg related functions */\n+void swap_tiers_mask_show(struct seq_file *m, int mask);\n+void swap_tiers_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent);\n+void swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg);\n+void __swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg);\n+\n+/* Mask and tier lookup */\n+int swap_tiers_mask_lookup(const char *name);\n+\n+/**\n+ * swap_tiers_mask_test - Check if the tier mask is valid\n+ * @tier_mask: The tier mask to check\n+ * @mask: The mask to compare against\n+ *\n+ * Return: true if condition matches, false otherwise\n+ */\n+static inline bool swap_tiers_mask_test(int tier_mask, int mask)\n+{\n+\treturn tier_mask & mask;\n+}\n #endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 4f8ce021c5bd..dd97e850ea2c 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1348,10 +1348,15 @@ static bool swap_alloc_fast(struct folio *folio)\n static void swap_alloc_slow(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n+\tint mask = folio_memcg(folio) ?\n+\t\tfolio_memcg(folio)->tier_effective_mask : TIER_ALL_MASK;\n \n \tspin_lock(&swap_avail_lock);\n start_over:\n \tplist_for_each_entry_safe(si, next, &swap_avail_head, avail_list) {\n+\t\tif (!swap_tiers_mask_test(si->tier_mask, mask))\n+\t\t\tcontinue;\n+\n \t\t/* Rotate the device and switch to a new cluster */\n \t\tplist_requeue(&si->avail_list, &swap_avail_head);\n \t\tspin_unlock(&swap_avail_lock);\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about swap device tier membership tracking, explaining that a `tier_mask` is added to identify which tier a device belongs to and that this mapping is necessary for future allocation logic. The author confirmed that the infrastructure allows dynamic modification of tiers without affecting existing devices.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch connects swap devices to the swap tier infrastructure,\nensuring that devices are correctly assigned to tiers based on their\npriority.\n\nA `tier_mask` is added to identify the tier membership of swap devices.\nAlthough tier-based allocation logic is not yet implemented, this\nmapping is necessary to track which tier a device belongs to. Upon\nactivation, the device is assigned to a tier by matching its priority\nagainst the configured tier ranges.\n\nThe infrastructure allows dynamic modification of tiers, such as\nsplitting or merging ranges. These operations are permitted provided\nthat the tier assignment of already configured swap devices remains\nunchanged.\n\nThis patch also adds the documentation for the swap tier feature,\ncovering the core concepts, sysfs interface usage, and configuration\ndetails.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n Documentation/mm/swap-tier.rst | 109 +++++++++++++++++++++++++++++++++\n include/linux/swap.h           |   1 +\n mm/swap_state.c                |   2 +-\n mm/swap_tier.c                 | 106 ++++++++++++++++++++++++++++----\n mm/swap_tier.h                 |  13 +++-\n mm/swapfile.c                  |   2 +\n 6 files changed, 219 insertions(+), 14 deletions(-)\n create mode 100644 Documentation/mm/swap-tier.rst\n\ndiff --git a/Documentation/mm/swap-tier.rst b/Documentation/mm/swap-tier.rst\nnew file mode 100644\nindex 000000000000..3386161b9b18\n--- /dev/null\n+++ b/Documentation/mm/swap-tier.rst\n@@ -0,0 +1,109 @@\n+.. SPDX-License-Identifier: GPL-2.0\n+\n+:Author: Chris Li <chrisl@kernel.org> Youngjun Park <youngjun.park@lge.com>\n+\n+==========\n+Swap Tier\n+==========\n+\n+Swap tier is a collection of user-named groups classified by priority ranges.\n+It acts as a facilitation layer, allowing users to manage swap devices based\n+on their speeds.\n+\n+Users are encouraged to assign swap device priorities according to device\n+speed to fully utilize this feature. While the current implementation is\n+integrated with cgroups, the concept is designed to be extensible for other\n+subsystems in the future.\n+\n+Use case\n+-------\n+\n+Users can perform selective swapping by choosing a swap tier assigned according\n+to speed within a cgroup.\n+\n+For more information on cgroup v2, please refer to\n+``Documentation/admin-guide/cgroup-v2.rst``.\n+\n+Priority Range\n+--------------\n+\n+The specified tiers must cover the entire priority range from -1\n+(DEF_SWAP_PRIO) to SHRT_MAX.\n+\n+Consistency\n+-----------\n+\n+Tier consistency is guaranteed with a focus on maximizing flexibility. When a\n+swap device is activated within a tier range, a reference is held from the\n+start of the tier to the priority of that swap device. This ensures that the\n+tier of region containing the active swap device does not disappear.\n+\n+If a request to add a new tier with a priority higher than the current swap\n+device is received, the existing tier can be split.\n+\n+However, specifying a tier in a cgroup does not hold a reference to the tier.\n+Consequently, the corresponding tier can disappear at any time.\n+\n+Configuration Interface\n+-----------------------\n+\n+The swap tiers can be configured via the following interface:\n+\n+/sys/kernel/mm/swap/tiers\n+\n+Operations can be performed using the following syntax:\n+\n+* Add:    ``+\"<tiername>\":\"<start_priority>\"``\n+* Remove: ``-\"<tiername>\"``\n+* Modify: ``\"<tiername>\":\"<start_priority>\"``\n+\n+Multiple operations can be provided in a single write, separated by spaces (\" \")\n+or commas (\",\").\n+\n+When configuring tiers, the specified value represents the **start priority**\n+of that tier. The end priority is automatically determined by the start\n+priority of the next higher tier. Consequently, adding or modifying a tier\n+automatically adjusts (splits or merges) the ranges of adjacent tiers to\n+ensure continuity.\n+\n+Examples\n+--------\n+\n+**1. Initialization**\n+\n+A tier starting at -1 is mandatory to cover the entire priority range up to\n+SHRT_MAX. In this example, 'HDD' starts at 50, and 'NET' covers the remaining\n+lower range starting from -1.\n+\n+::\n+\n+    # echo \"+HDD:50, +NET:-1\" > /sys/kernel/mm/swap/tiers\n+    # cat /sys/kernel/mm/swap/tiers\n+    Name             Idx   PrioStart   PrioEnd\n+    HDD              0     50          32767\n+    NET              1     -1          49\n+\n+**2. Modification and Splitting**\n+\n+Here, 'HDD' is moved to start at 80, and a new tier 'SSD' is added at 100.\n+Notice how the ranges are automatically recalculated:\n+* 'SSD' takes the top range. Split HDD Tier's range. (100 to SHRT_MAX).\n+* 'HDD' is adjusted to the range between 'NET' and 'SSD' (80 to 99).\n+* 'NET' automatically extends to fill the gap below 'HDD' (-1 to 79).\n+\n+::\n+\n+    # echo \"HDD:80, +SSD:100\" > /sys/kernel/mm/swap/tiers\n+    # cat /sys/kernel/mm/swap/tiers\n+    Name             Idx   PrioStart   PrioEnd\n+    SSD              2     100         32767\n+    HDD              0     80          99\n+    NET              1     -1          79\n+\n+**3. Removal**\n+\n+Tiers can be removed using the '-' prefix.\n+\n+::\n+\n+    # echo \"-SSD,-HDD,-NET\" > /sys/kernel/mm/swap/tiers\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 62fc7499b408..1e68c220a0e7 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -262,6 +262,7 @@ struct swap_info_struct {\n \tstruct percpu_ref users;\t/* indicate and keep swap device valid. */\n \tunsigned long\tflags;\t\t/* SWP_USED etc: see above */\n \tsigned short\tprio;\t\t/* swap priority of this type */\n+\tint tier_mask;\t\t\t/* swap tier mask */\n \tstruct plist_node list;\t\t/* entry in swap_active_head */\n \tsigned char\ttype;\t\t/* strange name for an index */\n \tunsigned int\tmax;\t\t/* extent of the swap_map */\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex f1a7d9cdc648..d46ca61d2e42 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -997,7 +997,7 @@ static ssize_t tiers_store(struct kobject *kobj,\n \t\t\tgoto restore;\n \t}\n \n-\tif (!swap_tiers_validate()) {\n+\tif (!swap_tiers_update()) {\n \t\tret = -EINVAL;\n \t\tgoto restore;\n \t}\ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nindex 87882272eec8..d90f6eccb908 100644\n--- a/mm/swap_tier.c\n+++ b/mm/swap_tier.c\n@@ -14,7 +14,7 @@\n  * @name: name of the swap_tier.\n  * @prio: starting value of priority.\n  * @list: linked list of tiers.\n-*/\n+ */\n static struct swap_tier {\n \tchar name[MAX_TIERNAME];\n \tshort prio;\n@@ -34,6 +34,8 @@ static LIST_HEAD(swap_tier_inactive_list);\n \t(!list_is_first(&(tier)->list, &swap_tier_active_list) ? \\\n \tlist_prev_entry((tier), list)->prio - 1 : SHRT_MAX)\n \n+#define MASK_TO_TIER(mask) (&swap_tiers[__ffs((mask))])\n+\n #define for_each_tier(tier, idx) \\\n \tfor (idx = 0, tier = &swap_tiers[0]; idx < MAX_SWAPTIER; \\\n \t\tidx++, tier = &swap_tiers[idx])\n@@ -55,6 +57,26 @@ static bool swap_tier_is_active(void)\n \treturn !list_empty(&swap_tier_active_list) ? true : false;\n }\n \n+static bool swap_tier_prio_in_range(struct swap_tier *tier, short prio)\n+{\n+\tif (tier->prio <= prio && TIER_END_PRIO(tier) >= prio)\n+\t\treturn true;\n+\n+\treturn false;\n+}\n+\n+static bool swap_tier_prio_is_used(struct swap_tier *self, short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (tier != self && tier->prio == prio)\n+\t\t\treturn true;\n+\t}\n+\n+\treturn false;\n+}\n+\n static struct swap_tier *swap_tier_lookup(const char *name)\n {\n \tstruct swap_tier *tier;\n@@ -67,12 +89,14 @@ static struct swap_tier *swap_tier_lookup(const char *name)\n \treturn NULL;\n }\n \n+\n void swap_tiers_init(void)\n {\n \tstruct swap_tier *tier;\n \tint idx;\n \n \tBUILD_BUG_ON(BITS_PER_TYPE(int) < MAX_SWAPTIER);\n+\tBUILD_BUG_ON(MAX_SWAPTIER > TIER_DEFAULT_IDX);\n \n \tfor_each_tier(tier, idx) {\n \t\tINIT_LIST_HEAD(&tier->list);\n@@ -145,17 +169,35 @@ static struct swap_tier *swap_tier_prepare(const char *name, short prio)\n \treturn tier;\n }\n \n-static int swap_tier_check_range(short prio)\n+static int swap_tier_can_split_range(struct swap_tier *orig_tier,\n+\tshort new_prio)\n {\n+\tstruct swap_info_struct *p;\n \tstruct swap_tier *tier;\n \n \tlockdep_assert_held(&swap_lock);\n \tlockdep_assert_held(&swap_tier_lock);\n \n-\tfor_each_active_tier(tier) {\n-\t\t/* No overwrite */\n-\t\tif (tier->prio == prio)\n-\t\t\treturn -EINVAL;\n+\tplist_for_each_entry(p, &swap_active_head, list) {\n+\t\tif (p->tier_mask == TIER_DEFAULT_MASK)\n+\t\t\tcontinue;\n+\n+\t\ttier = MASK_TO_TIER(p->tier_mask);\n+\t\tif (tier->prio > new_prio)\n+\t\t\tcontinue;\n+\t\t/*\n+                 * Prohibit implicit tier reassignment.\n+                 * Case 1: Prevent orig_tier devices from dropping out\n+                 *         of the new range.\n+                 */\n+\t\tif (orig_tier == tier && (p->prio < new_prio))\n+\t\t\treturn -EBUSY;\n+                /*\n+                 * Case 2: Prevent other tier devices from entering\n+                 *         the new range.\n+                 */\n+\t\telse if (orig_tier != tier && (p->prio >= new_prio))\n+\t\t\treturn -EBUSY;\n \t}\n \n \treturn 0;\n@@ -173,7 +215,10 @@ int swap_tiers_add(const char *name, int prio)\n \tif (swap_tier_lookup(name))\n \t\treturn -EPERM;\n \n-\tret = swap_tier_check_range(prio);\n+\tif (swap_tier_prio_is_used(NULL, prio))\n+\t\treturn -EBUSY;\n+\n+\tret = swap_tier_can_split_range(NULL, prio);\n \tif (ret)\n \t\treturn ret;\n \n@@ -183,7 +228,6 @@ int swap_tiers_add(const char *name, int prio)\n \t\treturn ret;\n \t}\n \n-\n \tswap_tier_insert_by_prio(tier);\n \treturn ret;\n }\n@@ -200,11 +244,18 @@ int swap_tiers_remove(const char *name)\n \tif (!tier)\n \t\treturn -EINVAL;\n \n+\t/* Simulate adding a tier to check for conflicts */\n+\tret = swap_tier_can_split_range(NULL, tier->prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n \tlist_move(&tier->list, &swap_tier_inactive_list);\n \n \t/* Removing DEF_SWAP_PRIO merges into the higher tier. */\n-\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO)\n-\t\tlist_prev_entry(tier, list)->prio = DEF_SWAP_PRIO;\n+\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO) {\n+\t\tlist_last_entry(&swap_tier_active_list, struct swap_tier, list)\n+\t\t\t->prio = DEF_SWAP_PRIO;\n+\t}\n \n \treturn ret;\n }\n@@ -225,7 +276,10 @@ int swap_tiers_modify(const char *name, int prio)\n \tif (tier->prio == prio)\n \t\treturn 0;\n \n-\tret = swap_tier_check_range(prio);\n+\tif (swap_tier_prio_is_used(tier, prio))\n+\t\treturn -EBUSY;\n+\n+\tret = swap_tier_can_split_range(tier, prio);\n \tif (ret)\n \t\treturn ret;\n \n@@ -283,10 +337,27 @@ void swap_tiers_restore(struct swap_tier_save_ctx ctx[])\n \t}\n }\n \n-bool swap_tiers_validate(void)\n+void swap_tiers_assign_dev(struct swap_info_struct *swp)\n {\n \tstruct swap_tier *tier;\n \n+\tlockdep_assert_held(&swap_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (swap_tier_prio_in_range(tier, swp->prio)) {\n+\t\t\tswp->tier_mask = TIER_MASK(tier);\n+\t\t\treturn;\n+\t\t}\n+\t}\n+\n+\tswp->tier_mask = TIER_DEFAULT_MASK;\n+}\n+\n+bool swap_tiers_update(void)\n+{\n+\tstruct swap_tier *tier;\n+\tstruct swap_info_struct *swp;\n+\n \t/*\n \t * Initial setting might not cover DEF_SWAP_PRIO.\n \t * Swap tier must cover the full range (DEF_SWAP_PRIO to SHRT_MAX).\n@@ -300,5 +371,16 @@ bool swap_tiers_validate(void)\n \t\t\treturn false;\n \t}\n \n+\t/*\n+\t * If applied initially, the swap tier_mask may change\n+\t * from the default value.\n+\t */\n+\tplist_for_each_entry(swp, &swap_active_head, list) {\n+\t\t/* Tier is already configured */\n+\t\tif (swp->tier_mask != TIER_DEFAULT_MASK)\n+\t\t\tbreak;\n+\t\tswap_tiers_assign_dev(swp);\n+\t}\n+\n \treturn true;\n }\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nindex 4b1b0602d691..de81d540e3b5 100644\n--- a/mm/swap_tier.h\n+++ b/mm/swap_tier.h\n@@ -14,6 +14,9 @@\n #define MAX_SWAPTIER\t\t8\n #endif\n \n+/* Forward declarations */\n+struct swap_info_struct;\n+\n extern spinlock_t swap_tier_lock;\n \n struct swap_tier_save_ctx {\n@@ -24,6 +27,10 @@ struct swap_tier_save_ctx {\n #define DEFINE_SWAP_TIER_SAVE_CTX(_name) \\\n \tstruct swap_tier_save_ctx _name[MAX_SWAPTIER] = {0}\n \n+#define TIER_ALL_MASK\t\t(~0)\n+#define TIER_DEFAULT_IDX\t(31)\n+#define TIER_DEFAULT_MASK\t(1 << TIER_DEFAULT_IDX)\n+\n /* Initialization and application */\n void swap_tiers_init(void);\n ssize_t swap_tiers_sysfs_show(char *buf);\n@@ -34,5 +41,9 @@ int swap_tiers_modify(const char *name, int prio);\n \n void swap_tiers_save(struct swap_tier_save_ctx ctx[]);\n void swap_tiers_restore(struct swap_tier_save_ctx ctx[]);\n-bool swap_tiers_validate(void);\n+bool swap_tiers_update(void);\n+\n+/* Tier assignment */\n+void swap_tiers_assign_dev(struct swap_info_struct *swp);\n+\n #endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex c27952b41d4f..4f8ce021c5bd 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -2672,6 +2672,8 @@ static void _enable_swap_info(struct swap_info_struct *si)\n \n \t/* Add back to available list */\n \tadd_to_avail_list(si, true);\n+\n+\tswap_tiers_assign_dev(si);\n }\n \n static void enable_swap_info(struct swap_info_struct *si, int prio,\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about the sysfs interface for configuring swap tiers, explaining that it supports batch input and enforces continuous priority ranges anchored by start priorities.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch introduces the \"Swap tier\" concept, which serves as an\nabstraction layer for managing swap devices based on their performance\ncharacteristics (e.g., NVMe, HDD, Network swap).\n\nSwap tiers are user-named groups representing priority ranges.\nThese tiers collectively cover the entire priority\nspace from -1 (`DEF_SWAP_PRIO`) to `SHRT_MAX`.\n\nTo configure tiers, a new sysfs interface is exposed at\n`/sys/kernel/mm/swap/tiers`. The input parser evaluates commands from\nleft to right and supports batch input, allowing users to add, remove or\nmodify multiple tiers in a single write operation.\n\nTier management enforces continuous priority ranges anchored by start\npriorities. Operations trigger range splitting or merging, but overwriting\nstart priorities is forbidden. Merging expands lower tiers upwards to\npreserve configured start priorities, except when removing `DEF_SWAP_PRIO`,\nwhich merges downwards.\n\nSuggested-by: Chris Li <chrisl@kernel.org>\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n MAINTAINERS     |   2 +\n mm/Makefile     |   2 +-\n mm/swap.h       |   4 +\n mm/swap_state.c |  70 +++++++++++\n mm/swap_tier.c  | 304 ++++++++++++++++++++++++++++++++++++++++++++++++\n mm/swap_tier.h  |  38 ++++++\n mm/swapfile.c   |   7 +-\n 7 files changed, 423 insertions(+), 4 deletions(-)\n create mode 100644 mm/swap_tier.c\n create mode 100644 mm/swap_tier.h\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 18d1ebf053db..501bf46adfb4 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16743,6 +16743,8 @@ F:\tmm/swap.c\n F:\tmm/swap.h\n F:\tmm/swap_table.h\n F:\tmm/swap_state.c\n+F:\tmm/swap_tier.c\n+F:\tmm/swap_tier.h\n F:\tmm/swapfile.c\n \n MEMORY MANAGEMENT - THP (TRANSPARENT HUGE PAGE)\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 53ca5d4b1929..3b3de2de7285 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -75,7 +75,7 @@ ifdef CONFIG_MMU\n \tobj-$(CONFIG_ADVISE_SYSCALLS)\t+= madvise.o\n endif\n \n-obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o\n+obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o swap_tier.o\n obj-$(CONFIG_ZSWAP)\t+= zswap.o\n obj-$(CONFIG_HAS_DMA)\t+= dmapool.o\n obj-$(CONFIG_HUGETLBFS)\t+= hugetlb.o hugetlb_sysfs.o hugetlb_sysctl.o\ndiff --git a/mm/swap.h b/mm/swap.h\nindex bfafa637c458..55f230cbe4e7 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -16,6 +16,10 @@ extern int page_cluster;\n #define swap_entry_order(order)\t0\n #endif\n \n+#define DEF_SWAP_PRIO  -1\n+\n+extern spinlock_t swap_lock;\n+extern struct plist_head swap_active_head;\n extern struct swap_info_struct *swap_info[];\n \n /*\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 6d0eef7470be..f1a7d9cdc648 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -25,6 +25,7 @@\n #include \"internal.h\"\n #include \"swap_table.h\"\n #include \"swap.h\"\n+#include \"swap_tier.h\"\n \n /*\n  * swapper_space is a fiction, retained to simplify the path through\n@@ -947,8 +948,77 @@ static ssize_t vma_ra_enabled_store(struct kobject *kobj,\n }\n static struct kobj_attribute vma_ra_enabled_attr = __ATTR_RW(vma_ra_enabled);\n \n+static ssize_t tiers_show(struct kobject *kobj,\n+\t\t\t\t     struct kobj_attribute *attr, char *buf)\n+{\n+\treturn swap_tiers_sysfs_show(buf);\n+}\n+\n+static ssize_t tiers_store(struct kobject *kobj,\n+\t\t\tstruct kobj_attribute *attr,\n+\t\t\tconst char *buf, size_t count)\n+{\n+\tchar *p, *token, *name, *tmp;\n+\tint ret = 0;\n+\tshort prio;\n+\tDEFINE_SWAP_TIER_SAVE_CTX(ctx);\n+\n+\ttmp = kstrdup(buf, GFP_KERNEL);\n+\tif (!tmp)\n+\t\treturn -ENOMEM;\n+\n+\tspin_lock(&swap_lock);\n+\tspin_lock(&swap_tier_lock);\n+\n+\tp = tmp;\n+\tswap_tiers_save(ctx);\n+\n+\twhile (!ret && (token = strsep(&p, \", \\t\\n\")) != NULL) {\n+\t\tif (!*token)\n+\t\t\tcontinue;\n+\n+\t\tif (token[0] == '-') {\n+\t\t\tret = swap_tiers_remove(token + 1);\n+\t\t} else {\n+\n+\t\t\tname = strsep(&token, \":\");\n+\t\t\tif (!token || kstrtos16(token, 10, &prio)) {\n+\t\t\t\tret = -EINVAL;\n+\t\t\t\tgoto out;\n+\t\t\t}\n+\n+\t\t\tif (name[0] == '+')\n+\t\t\t\tret = swap_tiers_add(name + 1, prio);\n+\t\t\telse\n+\t\t\t\tret = swap_tiers_modify(name, prio);\n+\t\t}\n+\n+\t\tif (ret)\n+\t\t\tgoto restore;\n+\t}\n+\n+\tif (!swap_tiers_validate()) {\n+\t\tret = -EINVAL;\n+\t\tgoto restore;\n+\t}\n+\n+out:\n+\tspin_unlock(&swap_tier_lock);\n+\tspin_unlock(&swap_lock);\n+\n+\tkfree(tmp);\n+\treturn ret ? ret : count;\n+\n+restore:\n+\tswap_tiers_restore(ctx);\n+\tgoto out;\n+}\n+\n+static struct kobj_attribute tier_attr = __ATTR_RW(tiers);\n+\n static struct attribute *swap_attrs[] = {\n \t&vma_ra_enabled_attr.attr,\n+\t&tier_attr.attr,\n \tNULL,\n };\n \ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nnew file mode 100644\nindex 000000000000..87882272eec8\n--- /dev/null\n+++ b/mm/swap_tier.c\n@@ -0,0 +1,304 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#include <linux/swap.h>\n+#include <linux/memcontrol.h>\n+#include \"memcontrol-v1.h\"\n+#include <linux/sysfs.h>\n+#include <linux/plist.h>\n+\n+#include \"swap.h\"\n+#include \"swap_tier.h\"\n+\n+/*\n+ * struct swap_tier - structure representing a swap tier.\n+ *\n+ * @name: name of the swap_tier.\n+ * @prio: starting value of priority.\n+ * @list: linked list of tiers.\n+*/\n+static struct swap_tier {\n+\tchar name[MAX_TIERNAME];\n+\tshort prio;\n+\tstruct list_head list;\n+} swap_tiers[MAX_SWAPTIER];\n+\n+DEFINE_SPINLOCK(swap_tier_lock);\n+/* active swap priority list, sorted in descending order */\n+static LIST_HEAD(swap_tier_active_list);\n+/* unused swap_tier object */\n+static LIST_HEAD(swap_tier_inactive_list);\n+\n+#define TIER_IDX(tier)\t((tier) - swap_tiers)\n+#define TIER_MASK(tier)\t(1 << TIER_IDX(tier))\n+#define TIER_INVALID_PRIO (DEF_SWAP_PRIO - 1)\n+#define TIER_END_PRIO(tier) \\\n+\t(!list_is_first(&(tier)->list, &swap_tier_active_list) ? \\\n+\tlist_prev_entry((tier), list)->prio - 1 : SHRT_MAX)\n+\n+#define for_each_tier(tier, idx) \\\n+\tfor (idx = 0, tier = &swap_tiers[0]; idx < MAX_SWAPTIER; \\\n+\t\tidx++, tier = &swap_tiers[idx])\n+\n+#define for_each_active_tier(tier) \\\n+\tlist_for_each_entry(tier, &swap_tier_active_list, list)\n+\n+#define for_each_inactive_tier(tier) \\\n+\tlist_for_each_entry(tier, &swap_tier_inactive_list, list)\n+\n+/*\n+ * Naming Convention:\n+ *   swap_tiers_*() - Public/exported functions\n+ *   swap_tier_*()  - Private/internal functions\n+ */\n+\n+static bool swap_tier_is_active(void)\n+{\n+\treturn !list_empty(&swap_tier_active_list) ? true : false;\n+}\n+\n+static struct swap_tier *swap_tier_lookup(const char *name)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (!strcmp(tier->name, name))\n+\t\t\treturn tier;\n+\t}\n+\n+\treturn NULL;\n+}\n+\n+void swap_tiers_init(void)\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tBUILD_BUG_ON(BITS_PER_TYPE(int) < MAX_SWAPTIER);\n+\n+\tfor_each_tier(tier, idx) {\n+\t\tINIT_LIST_HEAD(&tier->list);\n+\t\tlist_add_tail(&tier->list, &swap_tier_inactive_list);\n+\t}\n+}\n+\n+ssize_t swap_tiers_sysfs_show(char *buf)\n+{\n+\tstruct swap_tier *tier;\n+\tssize_t len = 0;\n+\n+\tlen += sysfs_emit_at(buf, len, \"%-16s %-5s %-11s %-11s\\n\",\n+\t\t\t \"Name\", \"Idx\", \"PrioStart\", \"PrioEnd\");\n+\n+\tspin_lock(&swap_tier_lock);\n+\tfor_each_active_tier(tier) {\n+\t\tlen += sysfs_emit_at(buf, len, \"%-16s %-5ld %-11d %-11d\\n\",\n+\t\t\t\t     tier->name,\n+\t\t\t\t     TIER_IDX(tier),\n+\t\t\t\t     tier->prio,\n+\t\t\t\t     TIER_END_PRIO(tier));\n+\t\tif (len >= PAGE_SIZE)\n+\t\t\tbreak;\n+\t}\n+\tspin_unlock(&swap_tier_lock);\n+\n+\treturn len;\n+}\n+\n+static void swap_tier_insert_by_prio(struct swap_tier *new)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (tier->prio > new->prio)\n+\t\t\tcontinue;\n+\n+\t\tlist_add_tail(&new->list, &tier->list);\n+\t\treturn;\n+\t}\n+\t/* First addition, or becomes the first tier */\n+\tlist_add_tail(&new->list, &swap_tier_active_list);\n+}\n+\n+static void __swap_tier_prepare(struct swap_tier *tier, const char *name,\n+\tshort prio)\n+{\n+\tlist_del_init(&tier->list);\n+\tstrscpy(tier->name, name, MAX_TIERNAME);\n+\ttier->prio = prio;\n+}\n+\n+static struct swap_tier *swap_tier_prepare(const char *name, short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tif (prio < DEF_SWAP_PRIO)\n+\t\treturn NULL;\n+\n+\tif (list_empty(&swap_tier_inactive_list))\n+\t\treturn ERR_PTR(-EPERM);\n+\n+\ttier = list_first_entry(&swap_tier_inactive_list,\n+\t\tstruct swap_tier, list);\n+\n+\t__swap_tier_prepare(tier, name, prio);\n+\treturn tier;\n+}\n+\n+static int swap_tier_check_range(short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\t/* No overwrite */\n+\t\tif (tier->prio == prio)\n+\t\t\treturn -EINVAL;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+int swap_tiers_add(const char *name, int prio)\n+{\n+\tint ret;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\t/* Duplicate check */\n+\tif (swap_tier_lookup(name))\n+\t\treturn -EPERM;\n+\n+\tret = swap_tier_check_range(prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\ttier = swap_tier_prepare(name, prio);\n+\tif (IS_ERR(tier)) {\n+\t\tret = PTR_ERR(tier);\n+\t\treturn ret;\n+\t}\n+\n+\n+\tswap_tier_insert_by_prio(tier);\n+\treturn ret;\n+}\n+\n+int swap_tiers_remove(const char *name)\n+{\n+\tint ret = 0;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\ttier = swap_tier_lookup(name);\n+\tif (!tier)\n+\t\treturn -EINVAL;\n+\n+\tlist_move(&tier->list, &swap_tier_inactive_list);\n+\n+\t/* Removing DEF_SWAP_PRIO merges into the higher tier. */\n+\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO)\n+\t\tlist_prev_entry(tier, list)->prio = DEF_SWAP_PRIO;\n+\n+\treturn ret;\n+}\n+\n+int swap_tiers_modify(const char *name, int prio)\n+{\n+\tint ret;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\ttier = swap_tier_lookup(name);\n+\tif (!tier)\n+\t\treturn -EINVAL;\n+\n+\t/* No need to modify */\n+\tif (tier->prio == prio)\n+\t\treturn 0;\n+\n+\tret = swap_tier_check_range(prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tlist_del_init(&tier->list);\n+\ttier->prio = prio;\n+\tswap_tier_insert_by_prio(tier);\n+\n+\treturn ret;\n+}\n+\n+/*\n+ * XXX: Reverting individual operations becomes complex as the number of\n+ * operations grows. Instead, we save the original state beforehand and\n+ * fully restore it if any operation fails.\n+ */\n+void swap_tiers_save(struct swap_tier_save_ctx ctx[])\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tidx = TIER_IDX(tier);\n+\t\tstrcpy(ctx[idx].name, tier->name);\n+\t\tctx[idx].prio = tier->prio;\n+\t}\n+\n+\tfor_each_inactive_tier(tier) {\n+\t\tidx = TIER_IDX(tier);\n+\t\t/* Indicator of inactive */\n+\t\tctx[idx].prio = TIER_INVALID_PRIO;\n+\t}\n+}\n+\n+void swap_tiers_restore(struct swap_tier_save_ctx ctx[])\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\t/* Invalidate active list */\n+\tlist_splice_tail_init(&swap_tier_active_list,\n+\t\t\t&swap_tier_inactive_list);\n+\n+\tfor_each_tier(tier, idx) {\n+\t\tif (ctx[idx].prio != TIER_INVALID_PRIO) {\n+\t\t\t/* Preserve idx(mask) */\n+\t\t\t__swap_tier_prepare(tier, ctx[idx].name, ctx[idx].prio);\n+\t\t\tswap_tier_insert_by_prio(tier);\n+\t\t}\n+\t}\n+}\n+\n+bool swap_tiers_validate(void)\n+{\n+\tstruct swap_tier *tier;\n+\n+\t/*\n+\t * Initial setting might not cover DEF_SWAP_PRIO.\n+\t * Swap tier must cover the full range (DEF_SWAP_PRIO to SHRT_MAX).\n+\t * Also, modify operation can change only one remaining priority.\n+\t */\n+\tif (swap_tier_is_active()) {\n+\t\ttier = list_last_entry(&swap_tier_active_list,\n+\t\t\tstruct swap_tier, list);\n+\n+\t\tif (tier->prio != DEF_SWAP_PRIO)\n+\t\t\treturn false;\n+\t}\n+\n+\treturn true;\n+}\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nnew file mode 100644\nindex 000000000000..4b1b0602d691\n--- /dev/null\n+++ b/mm/swap_tier.h\n@@ -0,0 +1,38 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _SWAP_TIER_H\n+#define _SWAP_TIER_H\n+\n+#include <linux/types.h>\n+#include <linux/spinlock.h>\n+\n+#define MAX_TIERNAME\t\t16\n+\n+/* Ensure MAX_SWAPTIER does not exceed MAX_SWAPFILES */\n+#if 8 > MAX_SWAPFILES\n+#define MAX_SWAPTIER\t\tMAX_SWAPFILES\n+#else\n+#define MAX_SWAPTIER\t\t8\n+#endif\n+\n+extern spinlock_t swap_tier_lock;\n+\n+struct swap_tier_save_ctx {\n+\tchar name[MAX_TIERNAME];\n+\tshort prio;\n+};\n+\n+#define DEFINE_SWAP_TIER_SAVE_CTX(_name) \\\n+\tstruct swap_tier_save_ctx _name[MAX_SWAPTIER] = {0}\n+\n+/* Initialization and application */\n+void swap_tiers_init(void);\n+ssize_t swap_tiers_sysfs_show(char *buf);\n+\n+int swap_tiers_add(const char *name, int prio);\n+int swap_tiers_remove(const char *name);\n+int swap_tiers_modify(const char *name, int prio);\n+\n+void swap_tiers_save(struct swap_tier_save_ctx ctx[]);\n+void swap_tiers_restore(struct swap_tier_save_ctx ctx[]);\n+bool swap_tiers_validate(void);\n+#endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 7b055f15d705..c27952b41d4f 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -50,6 +50,7 @@\n #include \"internal.h\"\n #include \"swap_table.h\"\n #include \"swap.h\"\n+#include \"swap_tier.h\"\n \n static bool swap_count_continued(struct swap_info_struct *, pgoff_t,\n \t\t\t\t unsigned char);\n@@ -65,7 +66,7 @@ static void move_cluster(struct swap_info_struct *si,\n \t\t\t struct swap_cluster_info *ci, struct list_head *list,\n \t\t\t enum swap_cluster_flags new_flags);\n \n-static DEFINE_SPINLOCK(swap_lock);\n+DEFINE_SPINLOCK(swap_lock);\n static unsigned int nr_swapfiles;\n atomic_long_t nr_swap_pages;\n /*\n@@ -76,7 +77,6 @@ atomic_long_t nr_swap_pages;\n EXPORT_SYMBOL_GPL(nr_swap_pages);\n /* protected with swap_lock. reading in vm_swap_full() doesn't need lock */\n long total_swap_pages;\n-#define DEF_SWAP_PRIO  -1\n unsigned long swapfile_maximum_size;\n #ifdef CONFIG_MIGRATION\n bool swap_migration_ad_supported;\n@@ -89,7 +89,7 @@ static const char Bad_offset[] = \"Bad swap offset entry \";\n  * all active swap_info_structs\n  * protected with swap_lock, and ordered by priority.\n  */\n-static PLIST_HEAD(swap_active_head);\n+PLIST_HEAD(swap_active_head);\n \n /*\n  * all available (active, not full) swap_info_structs\n@@ -3977,6 +3977,7 @@ static int __init swapfile_init(void)\n \t\tswap_migration_ad_supported = true;\n #endif\t/* CONFIG_MIGRATION */\n \n+\tswap_tiers_init();\n \treturn 0;\n }\n subsys_initcall(swapfile_init);\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author is addressing a concern about caching oscillation and priority inversion in swap devices due to the global percpu cluster allocation. They agree that reverting commit 1b7e90020eb7 is necessary to support swap tiers, as it will prevent these issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This reverts commit 1b7e90020eb7 (\"mm, swap: use percpu cluster as\nallocation fast path\").\n\nBecause in the newly introduced swap tiers, the global percpu cluster\nwill cause two issues:\n1) it will cause caching oscillation in the same order of different si\n   if two different memcg can only be allowed to access different si and\n   both of them are swapping out.\n2) It can cause priority inversion on swap devices. Imagine a case where\n   there are two memcg, say memcg1 and memcg2. Memcg1 can access si A, B\n   and A is higher priority device. While memcg2 can only access si B.\n   Then memcg 2 could write the global percpu cluster with si B, then\n   memcg1 take si B in fast path even though si A is not exhausted.\n\nHence in order to support swap tier, revert commit to use\neach swap device's percpu cluster.\n\nSuggested-by: Kairui Song <kasong@tencent.com>\nCo-developed-by: Baoquan He <bhe@redhat.com>\nSigned-off-by: Baoquan He <bhe@redhat.com>\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n include/linux/swap.h |  17 ++++--\n mm/swapfile.c        | 142 ++++++++++++++-----------------------------\n 2 files changed, 57 insertions(+), 102 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 1e68c220a0e7..6921e22b14d3 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -247,11 +247,18 @@ enum {\n #define SWAP_NR_ORDERS\t\t1\n #endif\n \n-/*\n- * We keep using same cluster for rotational device so IO will be sequential.\n- * The purpose is to optimize SWAP throughput on these device.\n- */\n+ /*\n+  * We assign a cluster to each CPU, so each CPU can allocate swap entry from\n+  * its own cluster and swapout sequentially. The purpose is to optimize swapout\n+  * throughput.\n+  */\n+struct percpu_cluster {\n+\tlocal_lock_t lock; /* Protect the percpu_cluster above */\n+\tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n+};\n+\n struct swap_sequential_cluster {\n+\tspinlock_t lock; /* Serialize usage of global cluster */\n \tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n };\n \n@@ -277,8 +284,8 @@ struct swap_info_struct {\n \t\t\t\t\t/* list of cluster that are fragmented or contented */\n \tunsigned int pages;\t\t/* total of usable pages of swap */\n \tatomic_long_t inuse_pages;\t/* number of those currently in use */\n+\tstruct percpu_cluster\t__percpu *percpu_cluster; /* per cpu's swap location */\n \tstruct swap_sequential_cluster *global_cluster; /* Use one global cluster for rotating device */\n-\tspinlock_t global_cluster_lock;\t/* Serialize usage of global cluster */\n \tstruct rb_root swap_extent_root;/* root of the swap extent rbtree */\n \tstruct block_device *bdev;\t/* swap device or bdev of swap file */\n \tstruct file *swap_file;\t\t/* seldom referenced */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex dd97e850ea2c..5e3b87799440 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -118,18 +118,6 @@ static atomic_t proc_poll_event = ATOMIC_INIT(0);\n \n atomic_t nr_rotate_swap = ATOMIC_INIT(0);\n \n-struct percpu_swap_cluster {\n-\tstruct swap_info_struct *si[SWAP_NR_ORDERS];\n-\tunsigned long offset[SWAP_NR_ORDERS];\n-\tlocal_lock_t lock;\n-};\n-\n-static DEFINE_PER_CPU(struct percpu_swap_cluster, percpu_swap_cluster) = {\n-\t.si = { NULL },\n-\t.offset = { SWAP_ENTRY_INVALID },\n-\t.lock = INIT_LOCAL_LOCK(),\n-};\n-\n /* May return NULL on invalid type, caller must check for NULL return */\n static struct swap_info_struct *swap_type_to_info(int type)\n {\n@@ -477,7 +465,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * Swap allocator uses percpu clusters and holds the local lock.\n \t */\n \tlockdep_assert_held(&ci->lock);\n-\tlockdep_assert_held(&this_cpu_ptr(&percpu_swap_cluster)->lock);\n+\tlockdep_assert_held(this_cpu_ptr(&si->percpu_cluster->lock));\n \n \t/* The cluster must be free and was just isolated from the free list. */\n \tVM_WARN_ON_ONCE(ci->flags || !cluster_is_empty(ci));\n@@ -495,8 +483,8 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t */\n \tspin_unlock(&ci->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_unlock(&si->global_cluster_lock);\n-\tlocal_unlock(&percpu_swap_cluster.lock);\n+\t\tspin_unlock(&si->global_cluster->lock);\n+\tlocal_unlock(&si->percpu_cluster->lock);\n \n \ttable = swap_table_alloc(__GFP_HIGH | __GFP_NOMEMALLOC | GFP_KERNEL);\n \n@@ -508,9 +496,9 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * could happen with ignoring the percpu cluster is fragmentation,\n \t * which is acceptable since this fallback and race is rare.\n \t */\n-\tlocal_lock(&percpu_swap_cluster.lock);\n+\tlocal_lock(&si->percpu_cluster->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_lock(&si->global_cluster_lock);\n+\t\tspin_lock(&si->global_cluster->lock);\n \tspin_lock(&ci->lock);\n \n \t/* Nothing except this helper should touch a dangling empty cluster. */\n@@ -622,7 +610,7 @@ static bool swap_do_scheduled_discard(struct swap_info_struct *si)\n \t\tci = list_first_entry(&si->discard_clusters, struct swap_cluster_info, list);\n \t\t/*\n \t\t * Delete the cluster from list to prepare for discard, but keep\n-\t\t * the CLUSTER_FLAG_DISCARD flag, percpu_swap_cluster could be\n+\t\t * the CLUSTER_FLAG_DISCARD flag, there could be percpu_cluster\n \t\t * pointing to it, or ran into by relocate_cluster.\n \t\t */\n \t\tlist_del(&ci->list);\n@@ -953,12 +941,11 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n out:\n \trelocate_cluster(si, ci);\n \tswap_cluster_unlock(ci);\n-\tif (si->flags & SWP_SOLIDSTATE) {\n-\t\tthis_cpu_write(percpu_swap_cluster.offset[order], next);\n-\t\tthis_cpu_write(percpu_swap_cluster.si[order], si);\n-\t} else {\n+\tif (si->flags & SWP_SOLIDSTATE)\n+\t\tthis_cpu_write(si->percpu_cluster->next[order], next);\n+\telse\n \t\tsi->global_cluster->next[order] = next;\n-\t}\n+\n \treturn found;\n }\n \n@@ -1052,13 +1039,17 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \tif (order && !(si->flags & SWP_BLKDEV))\n \t\treturn 0;\n \n-\tif (!(si->flags & SWP_SOLIDSTATE)) {\n+\tif (si->flags & SWP_SOLIDSTATE) {\n+\t\t/* Fast path using per CPU cluster */\n+\t\tlocal_lock(&si->percpu_cluster->lock);\n+\t\toffset = __this_cpu_read(si->percpu_cluster->next[order]);\n+\t} else {\n \t\t/* Serialize HDD SWAP allocation for each device. */\n-\t\tspin_lock(&si->global_cluster_lock);\n+\t\tspin_lock(&si->global_cluster->lock);\n \t\toffset = si->global_cluster->next[order];\n-\t\tif (offset == SWAP_ENTRY_INVALID)\n-\t\t\tgoto new_cluster;\n+\t}\n \n+\tif (offset != SWAP_ENTRY_INVALID) {\n \t\tci = swap_cluster_lock(si, offset);\n \t\t/* Cluster could have been used by another order */\n \t\tif (cluster_is_usable(ci, order)) {\n@@ -1072,7 +1063,6 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n \n-new_cluster:\n \t/*\n \t * If the device need discard, prefer new cluster over nonfull\n \t * to spread out the writes.\n@@ -1129,8 +1119,10 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n done:\n-\tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_unlock(&si->global_cluster_lock);\n+\tif (si->flags & SWP_SOLIDSTATE)\n+\t\tlocal_unlock(&si->percpu_cluster->lock);\n+\telse\n+\t\tspin_unlock(&si->global_cluster->lock);\n \n \treturn found;\n }\n@@ -1311,41 +1303,8 @@ static bool get_swap_device_info(struct swap_info_struct *si)\n \treturn true;\n }\n \n-/*\n- * Fast path try to get swap entries with specified order from current\n- * CPU's swap entry pool (a cluster).\n- */\n-static bool swap_alloc_fast(struct folio *folio)\n-{\n-\tunsigned int order = folio_order(folio);\n-\tstruct swap_cluster_info *ci;\n-\tstruct swap_info_struct *si;\n-\tunsigned int offset;\n-\n-\t/*\n-\t * Once allocated, swap_info_struct will never be completely freed,\n-\t * so checking it's liveness by get_swap_device_info is enough.\n-\t */\n-\tsi = this_cpu_read(percpu_swap_cluster.si[order]);\n-\toffset = this_cpu_read(percpu_swap_cluster.offset[order]);\n-\tif (!si || !offset || !get_swap_device_info(si))\n-\t\treturn false;\n-\n-\tci = swap_cluster_lock(si, offset);\n-\tif (cluster_is_usable(ci, order)) {\n-\t\tif (cluster_is_empty(ci))\n-\t\t\toffset = cluster_offset(si, ci);\n-\t\talloc_swap_scan_cluster(si, ci, folio, offset);\n-\t} else {\n-\t\tswap_cluster_unlock(ci);\n-\t}\n-\n-\tput_swap_device(si);\n-\treturn folio_test_swapcache(folio);\n-}\n-\n /* Rotate the device and switch to a new cluster */\n-static void swap_alloc_slow(struct folio *folio)\n+static void swap_alloc_entry(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n \tint mask = folio_memcg(folio) ?\n@@ -1363,6 +1322,7 @@ static void swap_alloc_slow(struct folio *folio)\n \t\tif (get_swap_device_info(si)) {\n \t\t\tcluster_alloc_swap_entry(si, folio);\n \t\t\tput_swap_device(si);\n+\n \t\t\tif (folio_test_swapcache(folio))\n \t\t\t\treturn;\n \t\t\tif (folio_test_large(folio))\n@@ -1522,11 +1482,7 @@ int folio_alloc_swap(struct folio *folio)\n \t}\n \n again:\n-\tlocal_lock(&percpu_swap_cluster.lock);\n-\tif (!swap_alloc_fast(folio))\n-\t\tswap_alloc_slow(folio);\n-\tlocal_unlock(&percpu_swap_cluster.lock);\n-\n+\tswap_alloc_entry(folio);\n \tif (!order && unlikely(!folio_test_swapcache(folio))) {\n \t\tif (swap_sync_discard())\n \t\t\tgoto again;\n@@ -1945,9 +1901,7 @@ swp_entry_t swap_alloc_hibernation_slot(int type)\n \t\t\t * Grab the local lock to be compliant\n \t\t\t * with swap table allocation.\n \t\t\t */\n-\t\t\tlocal_lock(&percpu_swap_cluster.lock);\n \t\t\toffset = cluster_alloc_swap_entry(si, NULL);\n-\t\t\tlocal_unlock(&percpu_swap_cluster.lock);\n \t\t\tif (offset)\n \t\t\t\tentry = swp_entry(si->type, offset);\n \t\t}\n@@ -2751,28 +2705,6 @@ static void free_cluster_info(struct swap_cluster_info *cluster_info,\n \tkvfree(cluster_info);\n }\n \n-/*\n- * Called after swap device's reference count is dead, so\n- * neither scan nor allocation will use it.\n- */\n-static void flush_percpu_swap_cluster(struct swap_info_struct *si)\n-{\n-\tint cpu, i;\n-\tstruct swap_info_struct **pcp_si;\n-\n-\tfor_each_possible_cpu(cpu) {\n-\t\tpcp_si = per_cpu_ptr(percpu_swap_cluster.si, cpu);\n-\t\t/*\n-\t\t * Invalidate the percpu swap cluster cache, si->users\n-\t\t * is dead, so no new user will point to it, just flush\n-\t\t * any existing user.\n-\t\t */\n-\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n-\t\t\tcmpxchg(&pcp_si[i], si, NULL);\n-\t}\n-}\n-\n-\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n@@ -2856,7 +2788,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tflush_work(&p->discard_work);\n \tflush_work(&p->reclaim_work);\n-\tflush_percpu_swap_cluster(p);\n \n \tdestroy_swap_extents(p);\n \tif (p->flags & SWP_CONTINUED)\n@@ -2885,6 +2816,8 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tarch_swap_invalidate_area(p->type);\n \tzswap_swapoff(p->type);\n \tmutex_unlock(&swapon_mutex);\n+\tfree_percpu(p->percpu_cluster);\n+\tp->percpu_cluster = NULL;\n \tkfree(p->global_cluster);\n \tp->global_cluster = NULL;\n \tvfree(swap_map);\n@@ -3268,7 +3201,7 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n {\n \tunsigned long nr_clusters = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);\n \tstruct swap_cluster_info *cluster_info;\n-\tint err = -ENOMEM;\n+\tint cpu, err = -ENOMEM;\n \tunsigned long i;\n \n \tcluster_info = kvcalloc(nr_clusters, sizeof(*cluster_info), GFP_KERNEL);\n@@ -3278,14 +3211,27 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n \tfor (i = 0; i < nr_clusters; i++)\n \t\tspin_lock_init(&cluster_info[i].lock);\n \n-\tif (!(si->flags & SWP_SOLIDSTATE)) {\n+\tif (si->flags & SWP_SOLIDSTATE) {\n+\t\tsi->percpu_cluster = alloc_percpu(struct percpu_cluster);\n+\t\tif (!si->percpu_cluster)\n+\t\t\tgoto err;\n+\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tstruct percpu_cluster *cluster;\n+\n+\t\t\tcluster = per_cpu_ptr(si->percpu_cluster, cpu);\n+\t\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n+\t\t\t\tcluster->next[i] = SWAP_ENTRY_INVALID;\n+\t\t\tlocal_lock_init(&cluster->lock);\n+\t\t}\n+\t} else {\n \t\tsi->global_cluster = kmalloc(sizeof(*si->global_cluster),\n \t\t\t\t     GFP_KERNEL);\n \t\tif (!si->global_cluster)\n \t\t\tgoto err;\n \t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n \t\t\tsi->global_cluster->next[i] = SWAP_ENTRY_INVALID;\n-\t\tspin_lock_init(&si->global_cluster_lock);\n+\t\tspin_lock_init(&si->global_cluster->lock);\n \t}\n \n \t/*\n@@ -3566,6 +3512,8 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n bad_swap_unlock_inode:\n \tinode_unlock(inode);\n bad_swap:\n+\tfree_percpu(si->percpu_cluster);\n+\tsi->percpu_cluster = NULL;\n \tkfree(si->global_cluster);\n \tsi->global_cluster = NULL;\n \tinode = NULL;\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about swap device rotation causing fragmentation and performance regression when using per-device percpu clusters. They introduced a per-cpu cache for the swap device, prioritizing the per-cpu cluster within the cached swap device to minimize side effects on the existing fastpath.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure in v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When using per-device percpu clusters (instead of a global one),\na naive allocation logic triggers swap device rotation on every\nallocation. This behavior leads to severe fragmentation and performance\nregression.\n\nTo address this, this patch introduces a per-cpu cache for the swap\ndevice. The allocation logic is updated to prioritize the per-cpu\ncluster within the cached swap device, effectively restoring the\ntraditional fastpath and slowpath flow. This approach minimizes side\neffects on the existing fastpath.\n\nWith this change, swap device rotation occurs only when the current\ncached device is unable to satisfy the allocation, rather than on\nevery attempt.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n include/linux/swap.h |  1 -\n mm/swapfile.c        | 78 +++++++++++++++++++++++++++++++++++++-------\n 2 files changed, 66 insertions(+), 13 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 6921e22b14d3..ac634a21683a 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -253,7 +253,6 @@ enum {\n   * throughput.\n   */\n struct percpu_cluster {\n-\tlocal_lock_t lock; /* Protect the percpu_cluster above */\n \tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n };\n \ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 5e3b87799440..0dcd451afee5 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -106,6 +106,16 @@ PLIST_HEAD(swap_active_head);\n static PLIST_HEAD(swap_avail_head);\n static DEFINE_SPINLOCK(swap_avail_lock);\n \n+struct percpu_swap_device {\n+\tstruct swap_info_struct *si[SWAP_NR_ORDERS];\n+\tlocal_lock_t lock;\n+};\n+\n+static DEFINE_PER_CPU(struct percpu_swap_device, percpu_swap_device) = {\n+\t.si = { NULL },\n+\t.lock = INIT_LOCAL_LOCK(),\n+};\n+\n struct swap_info_struct *swap_info[MAX_SWAPFILES];\n \n static struct kmem_cache *swap_table_cachep;\n@@ -465,7 +475,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * Swap allocator uses percpu clusters and holds the local lock.\n \t */\n \tlockdep_assert_held(&ci->lock);\n-\tlockdep_assert_held(this_cpu_ptr(&si->percpu_cluster->lock));\n+\tlockdep_assert_held(this_cpu_ptr(&percpu_swap_device.lock));\n \n \t/* The cluster must be free and was just isolated from the free list. */\n \tVM_WARN_ON_ONCE(ci->flags || !cluster_is_empty(ci));\n@@ -484,7 +494,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \tspin_unlock(&ci->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_unlock(&si->global_cluster->lock);\n-\tlocal_unlock(&si->percpu_cluster->lock);\n+\tlocal_unlock(&percpu_swap_device.lock);\n \n \ttable = swap_table_alloc(__GFP_HIGH | __GFP_NOMEMALLOC | GFP_KERNEL);\n \n@@ -496,7 +506,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * could happen with ignoring the percpu cluster is fragmentation,\n \t * which is acceptable since this fallback and race is rare.\n \t */\n-\tlocal_lock(&si->percpu_cluster->lock);\n+\tlocal_lock(&percpu_swap_device.lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_lock(&si->global_cluster->lock);\n \tspin_lock(&ci->lock);\n@@ -941,9 +951,10 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n out:\n \trelocate_cluster(si, ci);\n \tswap_cluster_unlock(ci);\n-\tif (si->flags & SWP_SOLIDSTATE)\n+\tif (si->flags & SWP_SOLIDSTATE) {\n \t\tthis_cpu_write(si->percpu_cluster->next[order], next);\n-\telse\n+\t\tthis_cpu_write(percpu_swap_device.si[order], si);\n+\t} else\n \t\tsi->global_cluster->next[order] = next;\n \n \treturn found;\n@@ -1041,7 +1052,6 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \n \tif (si->flags & SWP_SOLIDSTATE) {\n \t\t/* Fast path using per CPU cluster */\n-\t\tlocal_lock(&si->percpu_cluster->lock);\n \t\toffset = __this_cpu_read(si->percpu_cluster->next[order]);\n \t} else {\n \t\t/* Serialize HDD SWAP allocation for each device. */\n@@ -1119,9 +1129,7 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n done:\n-\tif (si->flags & SWP_SOLIDSTATE)\n-\t\tlocal_unlock(&si->percpu_cluster->lock);\n-\telse\n+\tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_unlock(&si->global_cluster->lock);\n \n \treturn found;\n@@ -1303,8 +1311,27 @@ static bool get_swap_device_info(struct swap_info_struct *si)\n \treturn true;\n }\n \n+static bool swap_alloc_fast(struct folio *folio)\n+{\n+\tunsigned int order = folio_order(folio);\n+\tstruct swap_info_struct *si;\n+\n+\t/*\n+\t * Once allocated, swap_info_struct will never be completely freed,\n+\t * so checking it's liveness by get_swap_device_info is enough.\n+\t */\n+\tsi = this_cpu_read(percpu_swap_device.si[order]);\n+\tif (!si || !get_swap_device_info(si))\n+\t\treturn false;\n+\n+\tcluster_alloc_swap_entry(si, folio);\n+\tput_swap_device(si);\n+\n+\treturn folio_test_swapcache(folio);\n+}\n+\n /* Rotate the device and switch to a new cluster */\n-static void swap_alloc_entry(struct folio *folio)\n+static void swap_alloc_slow(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n \tint mask = folio_memcg(folio) ?\n@@ -1482,7 +1509,11 @@ int folio_alloc_swap(struct folio *folio)\n \t}\n \n again:\n-\tswap_alloc_entry(folio);\n+\tlocal_lock(&percpu_swap_device.lock);\n+\tif (!swap_alloc_fast(folio))\n+\t\tswap_alloc_slow(folio);\n+\tlocal_unlock(&percpu_swap_device.lock);\n+\n \tif (!order && unlikely(!folio_test_swapcache(folio))) {\n \t\tif (swap_sync_discard())\n \t\t\tgoto again;\n@@ -1901,7 +1932,9 @@ swp_entry_t swap_alloc_hibernation_slot(int type)\n \t\t\t * Grab the local lock to be compliant\n \t\t\t * with swap table allocation.\n \t\t\t */\n+\t\t\tlocal_lock(&percpu_swap_device.lock);\n \t\t\toffset = cluster_alloc_swap_entry(si, NULL);\n+\t\t\tlocal_unlock(&percpu_swap_device.lock);\n \t\t\tif (offset)\n \t\t\t\tentry = swp_entry(si->type, offset);\n \t\t}\n@@ -2705,6 +2738,27 @@ static void free_cluster_info(struct swap_cluster_info *cluster_info,\n \tkvfree(cluster_info);\n }\n \n+/*\n+ * Called after swap device's reference count is dead, so\n+ * neither scan nor allocation will use it.\n+ */\n+static void flush_percpu_swap_device(struct swap_info_struct *si)\n+{\n+\tint cpu, i;\n+\tstruct swap_info_struct **pcp_si;\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcp_si = per_cpu_ptr(percpu_swap_device.si, cpu);\n+\t\t/*\n+\t\t * Invalidate the percpu swap device cache, si->users\n+\t\t * is dead, so no new user will point to it, just flush\n+\t\t * any existing user.\n+\t\t */\n+\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n+\t\t\tcmpxchg(&pcp_si[i], si, NULL);\n+\t}\n+}\n+\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n@@ -2788,6 +2842,7 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tflush_work(&p->discard_work);\n \tflush_work(&p->reclaim_work);\n+\tflush_percpu_swap_device(p);\n \n \tdestroy_swap_extents(p);\n \tif (p->flags & SWP_CONTINUED)\n@@ -3222,7 +3277,6 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n \t\t\tcluster = per_cpu_ptr(si->percpu_cluster, cpu);\n \t\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n \t\t\t\tcluster->next[i] = SWAP_ENTRY_INVALID;\n-\t\t\tlocal_lock_init(&cluster->lock);\n \t\t}\n \t} else {\n \t\tsi->global_cluster = kmalloc(sizeof(*si->global_cluster),\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the swap_tier structure was simplified by replacing 'end prio' and priority lists with a standard list_head, as suggested in v2.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgment of previous feedback"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Youngjun,\n\nOn Sun, Jan 25, 2026 at 10:53PM Youngjun Park <youngjun.park@lge.com> wrote:",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested breaking down the patch series into smaller, more manageable steps, starting with defining the tiers bits without deleting any existing functionality, and then building upon that in subsequent steps.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Thanks for the patches series.\n\nSorry for the late reply. I have been wanting to reply to it but get\nsuper busy at work.\n\nSome high level feedback for the series. Now that you demonstrated the\nwhole series, let's focus on making small mergiable baby steps. Just\nlike the swap table has different phases. Make each step minimal, each\nstep shows some value. Do the MVP, we can always add more features as\na follow up step.\n\nI suggest the first step is getting the tiers bits defined. Add only,\nno delete.  Get that reviewed and merged, then the next step is to use\nthose tiers.\n\nChris",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested replacing per-cpu swap device clusters with global percpu clusters per tier, citing that the maximum number of tiers is smaller than the maximum number of swap devices and this would likely be a performance improvement.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance_improvement"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "One idea is that, instead of using percpu per swap device.\nYou can make the global percpu cluster per tier. Because the max tier\nnumber is smaller than the max number of swap devices. That is likely\na win.\n\nChris",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-11",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the swap_tier structure was simplified by replacing 'end prio' and priority lists with a standard list_head, but he also requested to add a check for NULL pointer dereference in the new code.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Yongjun,\n\nOn Sun, Jan 25, 2026 at 10:53PM Youngjun Park <youngjun.park@lge.com> wrote:",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested introducing a CONFIG option to limit the maximum number of swap tiers, recommending a default value of 4.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "We can have a CONFIG option for the MAX_SWAPTIER. I think the default\nshould be a small number like 4.",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that modifying a tier could cause swap files to move to a different tier, which may be problematic.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential issue",
                "problematic"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "When we add, modify, remove a tier. The simple case is there is no\nswap file under any tiers.\nBut if the modification causes some swap files to jump to different\ntiers. That might be problematic.",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li expressed concern about the complexity of saving and restoring swap tier configurations, requesting a simpler design.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I really hope we don't have to do the save and restore thing. Is there\nanother design we can simplify this?",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested that each tier have its own swap_active_head, allowing different swap entries on different tiers to release without competing for the same resource.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "One idea is to make each tier have swap_active_head. So different swap\nentry releases on different tiers don't need to be competing on the\nsame swap_active_head.\n\nThat will require the swapfile don't jump to another tiers.\n\nChris",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li initially suggested simplifying the swap_tier structure by removing 'end prio' and priority lists, replacing them with a standard list_head; however, upon reviewing the series, he found that adding tier names does not add real value and requires a more thorough examination of the whole series.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "reversal of opinion",
                "need for further review"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Just take a quick look at the series. I take that suggestion back.\nThis series is actually not too long. Adding the tiers name alone does\nnot add any real value. I actually need to look at the whole series\nrather than just the tier name alone.\n\nChris",
              "reply_to": "",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham",
              "summary": "Reviewer Nhat Pham questioned the consistency of the patch description, pointing out that the '+' operator was removed but still referenced in the explanation of cgroup hierarchy principles.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "inconsistency",
                "confusing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This comment seems a bit clunky to me. The \"+\" is removed, as noted\nabove, but then why are we saying \"even if a child re-enables a tier\nwith \"+\"\" here? Am I missing something?",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham",
              "summary": "Reviewer Nhat Pham questioned the logic of restricting child cgroup's allowed swap tiers to be a subset of their children and ancestors, suggesting it is more straightforward to simply follow this principle.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "But otherwise, I assume you mean to restrict child's allowed swap\ntiers to be a subset of children and its ancestors? That seems more\nstraightforward to me than the last system :)",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt noted that the patch does not handle the case where a swap tier is removed from a cgroup's configuration while it still has active swap devices assigned to it, potentially leading to memory leaks.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential memory leak",
                "handling removal of swap tiers"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Youngjun,\n\nOn Mon, Jan 26, 2026 at 03:52:37PM +0900, Youngjun Park wrote:",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt noted that adding a memcg interface for the Swap Tiers functionality is not necessary, and suggested exploring BPF as an alternative approach. He expressed concerns about allowing workloads to choose swap devices, which he believes should be handled by the job orchestator or node controller.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "One of the LPC feedback you missed is to not add memcg interface for\nthis functionality and explore BPF way instead.\n\nWe are normally very conservative to add new interfaces to cgroup.\nHowever I am not even convinced that memcg interface is the right way to\nexpose this functionality. Swap is currently global and the idea to\nlimit or assign specific swap devices to specific cgroups makes sense\nbut that is the decision for the job orchestator or node controller.\nAllowing workloads to pick and choose swap devices do not make sense to\nme.\n\nShakeel",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Chris Li's concern about breaking the series into smaller, mergeable steps by proposing a modified roadmap to ensure immediate value is demonstrated in Step 1.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "proposed a modified plan"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Chris,\n\nThank you for the direction.\n\nI agree that breaking the series into smaller, mergeable steps is the\nright approach. However, since introducing the definitions alone might\nlack immediate usage, I propose a slightly\nmodified roadmap to ensure Step 1 demonstrates some value.\n\nHere is the plan I have in mind.\n\n1. Swap Tier Definition & Addition\n   - Introduce the concept, grouping logic, and the 'add' interface.\n   - Value: Enables basic exception handling within the swap device\n     itself using tiers.\n\n2. Advanced Control (Delete/Modify)\n   - Implement logic to remove or update tiers.\n   - Value: Enhances the usability and management of the tiers\n     established in Step 1.\n\n3. External Integration (memcg, bpf etc ... )\n   - Apply swap tiers for broader swap control.\n   - Value: Connects swap tiers to other subsystems like memcg.\n\nDoes this roadmap look reasonable to you? I will proceed with preparing\nthe real patch series based on this structure.\n\nBest regards,\nYoungjun",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Chris Li's feedback about adding a configuration option to control the swap tier feature, agreeing to implement this in the patch.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "will add"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sounds good. I will add a CONFIG option for it and ensure it doesn't exceed\nMAX_SWAPFILE.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Chris Li's feedback on mixed operations in /sys/kernel/mm/swap/tiers, proposing to restrict the interface to handle one operation at a time due to potential errors and performance issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "proposed alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I have given this a lot of thought.\n\nSince the current interface allows mixing add (+), remove (-), and modify\noperations, we must either restore from a saved state or reverse the\nsuccessful individual operations upon failure.\n\nI implemented both approaches and concluded that reversing individual\noperations is error-prone. Also, it could be slow if there are many\noperations.\n\nAnother approach could be using a \"global clone tier\" strategy.\n(Because operation globally synchronized)\n\nTherefore, I would like to propose restricting the interface to handle a\nsingle operation at a time. What do you think?",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged that limiting contention to objects within the same tier is a benefit of their patch, and suggested future optimization for swap_avail_list.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged",
                "suggested"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree. With the tier structure, we can limit contention to objects within\nthe same tier.\n\nI also think swap_avail_list could be optimized in a similar way in the\nfuture.\n\nYoungjun",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledges that stripping out the remove/modify parts from the patch is a feasible direction, indicating an openness to revising the patch based on reviewer feedback.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges need for revision"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oops, I replied to your previous email before seeing this one.\n\nStripping out the remove/modify parts is also feasible. Do you agree with\nthat direction?\n\nYoungjun",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "The author addressed Nhat Pham's concern about the default state of swap tiers, clarifying that they previously used an exclusive mode where only one specific tier was used when '+' was specified, and are changing to a subtraction-based model where all tiers are selected by default and users use '-' to exclude specific ones.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "To clarify, previously, the default state used all tiers. Using \"+\"              \nswitched to \"an exclusive mode\"  where only that specific tier was used.         \n                                                                                 \nI am changing this to a subtraction-based model. By default, all tiers           \nare selected, and users use \"-\" to exclude specific ones.                        \n(Then not \"removed\" but \"changed\" is more proper?)                               \n                                                                                 \nIn this context, I intended \"+\" to be used to restore a tier that was            \npreviously excluded by \"-\".",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged Nhat Pham's feedback and agreed to restructure the swapoff path in v2 to drop the per-vswap spinlock before calling try_to_unmap().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged",
                "agreed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes, that's right :)\n\nThanks \nYoungjun Park.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Shakeel Butt's feedback about using the BPF approach for swap control, agreeing it provides flexibility but expressing concern about potential logical contradictions and hierarchy enforcement issues. Author prefers a native interface within 'cgroup land' to ensure consistency and avoid conflicts.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix is needed",
                "concerns about BPF approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Apologies for overlooking the feedback regarding the BPF approach. Thank you\nfor the suggestion.\n\nI agree that using BPF would provide greater flexibility, allowing control not\njust at the memcg level, but also per-process or for complex workloads.\n(As like orchestrator and node controller)\n\nHowever, I am concerned that this level of freedom might introduce logical\ncontradictions, particularly regarding cgroup hierarchy semantics.\n\nFor example, BPF might allow a topology that violates hierarchical constraints\n(a concern that was also touched upon during LPC)\n\n  - Group A (Parent): Assigned to SSD1\n  - Group B (Child of A): Assigned to SSD2\n\nIf Group A has a `memory.swap.max` limit, and Group B swaps out to SSD2, it\ncreates a consistency issue. Group B consumes Group A's swap quota, but it is\nutilizing a device (SSD2) that is distinct from the Parent's assignment. This\ncould lead to situations where the Parent's limit is exhausted by usage on a\ndevice it effectively doesn't \"own\" or shouldn't be using.\n\nOne might suggest restricting BPF to strictly adhere to these hierarchical\nconstraints. However, doing so would effectively eliminate the primary\nadvantage of using BPF\\u2014its flexibility. If we are to enforce standard cgroup\nsemantics anyway, a native interface seems more appropriate than a constrained\nBPF hook.\n\nBeyond this specific example, I suspect that delegating this logic to BPF\nmight introduce other unforeseen edge cases regarding hierarchy enforcement.\nIn my view, the BPF approach seems more like a \"next step.\"\n\nSince you acknowledged that the idea of assigning swap devices to cgroups\n\"makes sense,\" I believe implementing this within the standard, strictly\nconstrained \"cgroup land\" is preferable. \n\nA strict cgroup interface ensures\nthat hierarchy and accounting rules are consistently enforced, avoiding the\npotential conflicts that the unrestricted freedom of BPF might create.\n\nUltimately, I hope this swap tier mechanism can serve as a foundation to be\nleveraged by other subsystems, such as BPF and DAMON. I view this proposal as\nthe necessary first step toward that future.\n\nYoungjun Park",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged that existing swapfiles' tier is immutable and removed the tier reference, instead validating the tier range at operation time to guarantee this invariant.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I missed one comment. \n\nThe tier of existing swapfiles is immutable once assigned at swapon.\nI removed tier reference.\nInstead of reference counting, each operation validates the tier\nrange at operation time to guarantee this invariant.\n\n- add:    Does not change existing swapfiles' tier. New tier may\n          split priority range, but existing assignments stay.\n- remove: Rejected with -EBUSY if any swapfile is attached.\n- modify: Rejected if the change would cause any swapfile to\n          move to a different tier.\n\nSo swapfiles never jump between tiers at runtime.\n\nYoungjun Park",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer requested that further discussion be concluded on the previous patch series before a new version is submitted.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "lack of technical feedback"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Please don't send a new version of the series before concluding the discussion\non the previous one.\n\nOn Fri, Feb 13, 2026 at 12:58:40PM +0900, YoungJun Park wrote:",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed concerns about introducing stable interfaces for swap tiers, requesting a BPF approach first and questioning the need for hierarchical control.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "still not convinced"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes it provides the flexibility but that is not the main reason I am pushing for\nit. The reason I want you to first try the BPF approach without introducing any\nstable interfaces. Show how swap tiers will be used and configured in production\nenvironment and then we can talk if a stable interface is needed. I am still not\nconvinced that swap tiers need to be controlled hierarchically and the non-root\nshould be able to control it.",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that while BPF offers more flexibility, its control is limited to administrators who can still make mistakes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEUTRAL"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yes BPF provides more power but it is controlled by admin and admin can shoot\ntheir foot in multiple ways.",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt requested additional information about the patch's use-case, specifically asking for details on workload ordering, tier naming, and partitioning of swap devices among sub-workloads.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested clarification",
                "wanted to brainstorm future use-cases"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No need to constraint anything.\n\nTaking a step back, can you describe your use-case a bit more and share\nrequirements?\n\nYou have multiple swap devices of different properties and you want to assign\nthose swap devices to different workloads. Now couple of questions:\n\n1. If more than one device is assign to a workload, do you want to have\n   some kind of ordering between them for the worklod or do you want option to\n   have round robin kind of policy?\n\n2. What's the reason to use 'tiers' in the name? Is it similar to memory tiers\n   and you want promotion/demotion among the tiers?\n\n3. If a workload has multiple swap devices assigned, can you describe the\n   scenario where such workloads need to partition/divide given devices to their\n   sub-workloads?\n\nLet's start with these questions. Please note that I want us to not just look at\nthe current use-case but brainstorm more future use-cases and then come up with\nthe solution which is more future proof.\n\nthanks,\nShakeel",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the discussion may not be fully concluded due to lack of response from YoungJun, and suggested being more lenient in interpreting conclusions on the mailing list.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "leniency",
                "interpretation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "In this case I think it is fine.  You haven't responded to YoungJun's\nlast response in over a week. He might have mistaken that the\ndiscussion concluded.\nConsider it is one of the iterations. It is hard enough to contribute\nto the kernel. Relax.\nPlus, much of the discussion on the mailing list always has differing\nopinions. So, it's hard to determine what is truly concluded.\nDifferent people might have different interitations of the same text.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested that instead of blocking the patch, a config option could be added to protect the change as experimental, allowing for further testing and feedback.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Is that your biggest concern? Many different ways exist to solve that\nproblem. e.g. We can put a config option protecting it and mark it as\nexperimental. This will unblock the development allow experiment. We\ncan have more people to try it out and give feedback.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li confirmed that his company uses a different swap device at different cgroup levels, emphasizing the practical need for control at non-root levels.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, my company uses a different swap device at different cgroup\nlevel. I did ask my coworker to confirm that usage. Control at the non\nroot level is a real need.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the swap device control patch is a basic need and questioned why similar concerns were not raised about zswap.writeback, which only controls zswap behavior, whereas this patch provides more generic swap device control.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning previous decisions",
                "comparing to existing code"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think this swap device control is a very basic need. All your\nobjections to swapping control in the group can equally apply to\nzswap.writeback. Unlike zswap.writeback, which only control from the\nzswap behavior. This is a more generic version control swap device\nother than zswap as well. BTW, I raised that concern about\nzswap.writeback was not generic enough as swap control was limited\nwhen zswap was proposed. We did hold back zswap.writeback. The\nconsensers is interface can be improved as later iterations. So here\nwe are.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Chris Li noted that the patch could be simplified by removing the need for an internal cgroup swapfile control interface, which is currently not upstreamed, and suggested replacing it with the proposed 'Swap Tiers' concept.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There is a very long thread on the linux-mm maillist. I'm too lazy to dig it up.\n\nI can share our usage requirement to refresh your memory. We\ninternally use a cgroup swapfile control interface that has not been\nupstreamed. With this we can remove the need of that internal\ninterface and go upstream instead.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the swap tier system's performance is dependent on the number of devices within each tier, and suggested using a round-robin algorithm to distribute swaps within tiers",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "performance concern",
                "algorithm suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It depends on the number of devices in the tiers. Different tiers\nmaintain an order. Within the same tier round robin.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested alternative names for the 'tier' concept, proposing 'swap.device_speed_classes' and acknowledging that the current name was inspired by memory tiers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggestion",
                "acknowledgment"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I propose the tier name. Guilty. Yes, in was inpired by memory tiers.\nIt just different class of swap speeds. I am not fixed on the name. We\ncan also call it swap.device_speed_classes. You can suggest\nalternatives.\n\nPromotion / demotion is possible in the future. The current state,\nwithout promotion or demotion, already provides value. Our current\ndeployment uses only one class of swap device at a time. However I do\nknow other companies use  more than one class of swap device.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that their deployment uses multiple swap devices to reduce lock contention and requested consideration for a job's tolerable swap speed when selecting tiers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "In our deployment, we always use more than one swap device to reduce\nswap device lock contention.\nThe job config can describe the swap speed it can tolerate. Some jobs\ncan tolerate slower speeds, while others cannot.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li expressed concerns that the patch's design might be overly future-proof and suggested starting from the current need, citing an example of zswap.writeback where a solution was found incrementally.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "overly future-proof",
                "incremental progress is better"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Take zswap.writeback as example. We have a solution that worked for\nthe requirement at that time. Incremental improvement is fine as well.\nUsually, incremental progress is better. At least currently there is a\nreal need to allow different cgroups to select different swap speeds.\nThere is a risk in being too future-proof: we might design things that\npeople in the future don't use as we envisioned. I see that happen too\noften as well.\n\nSo starting from the current need is a solid starting point. It's just\na different design philosophy. Each to their own.\n\nThat is the only usage case I know. YoungJun feel free to add yours\nusage as well.\n\nChris",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledges that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agrees to restructure in v2",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Understood. Let's continue the discussion. :D\n\nChris has already provided a thorough response, but I would like to\nadd my perspective as well.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "The author acknowledges a concern about committing to a stable interface too early and suggests reducing this risk by adding a build-time config option or marking it as experimental.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a fix is needed",
                "suggests alternative solutions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I understand your concern about committing to a stable interface too\nearly. As Chris suggested, we could reduce this concern by guarding\nthe interface behind a build-time config option or marking it as\nexperimental, which I will also touch on further below.\n\nOn that note, if BPF were to become the primary control mechanism,\nI am not sure a memcg interface would still be needed at all, since\nBPF already provides a high degree of freedom. However, that level\nof freedom is also what concerns me -- BPF-driven swap device\nassignments could subtly conflict with memcg hierarchy semantics in\nways that are hard to predict or debug. A more constrained memcg-based\napproach might actually be safer in that regard.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), but did not explicitly state a plan for addressing this issue in future revisions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "did not promise a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think this concern is closely tied to your question #3 below about\nconcrete use cases for partitioning devices across sub-workloads.\nI hope my answer there helps clarify this.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author suggests that introducing build-time config or runtime constraints can help define and predict usage of swap tiers feature.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explaining"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "As I mentioned above, I think guarding the feature behind a build-time\nconfig or runtime constraints could keep the usage well-defined and\npredictable, while still being useful.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged Shakeel Butt's concern about the patch's ability to accommodate broader scenarios and explained that their use case is simpler, thus justifying the 'swap tier' mechanism.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a limitation",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Our use case is simple at now. \nWe have two swap devices with different performance\ncharacteristics and want to assign different swap devices to different\nworkloads (cgroups).\n\nFor some background, when I initially proposed this, I suggested allowing\nper-cgroup swap device priorities so that it could also accommodate the\nbroader scenarios you mentioned. However, since even our own use case\ndoes not require reversing swap priorities within a cgroup, we pivoted\nto the \"swap tier\" mechanism that Chris proposed.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Shakeel Butt's concern about the ordering of swap devices within a tier by explaining that round-robin allocation applies when devices have the same priority, and ordering applies otherwise.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Both. If devices are in the same tier with the same priority, round robin.\nIf they are in the same tier with different priorities, or in different\ntiers, ordering applies. The current tier structure should be able to\nsatisfy either preference.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged that the swap_tier structure simplification is a good change, but disagreed with reviewer Shakeel Butt's suggestion to add a 'struct list_head' for each tier, instead referring to Chris Li's explanation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "disagreed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This was originally Chris's idea. I think he explained the rationale\nwell in his reply.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged the potential for reducing lock contention by partitioning swap devices, but did not provide a clear plan to address Shakeel Butt's concern about the swapoff path needing to drop the per-vswap spinlock before calling try_to_unmap().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged potential for reducing lock contention"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "One possible scenario is reducing lock contention by partitioning swap\ndevices between parent and child cgroups.",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "The author addressed Shakeel Butt's concern about the patch being too rigid for future use cases by explaining that it is hard to design concretely for unknown needs, but proposing a CONFIG option as a temporary solution to allow progress without committing to a stable interface. The author also sees BPF as a natural extension path rather than a starting point.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explaining reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "We have clear production use cases from both us and Chris, and I also\npresented a deployment example in the cover letter.\n\nI think it is hard to design concretely for future use cases at this\npoint. When those needs become clearer, BPF with its flexibility\nwould be a better fit then. I see BPF as a natural extension path\nrather than a starting point.\n\nFor now, guarding the memcg & tier behind a CONFIG option would\nlet us move forward without committing to a stable interface, and\nwe can always pivot to BPF later if needed\n\nThanks,\nYoungJun Park",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed concern that the patch does not address the primary use-case of controlling/partitioning swap devices among sub-workloads and requested further exploration before adding a stable API.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "concerns"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, that is secondary because I am not seeing the real use-case of\ncontrolling/partitioning swap devices among sub-workloads. Until that is\nfigured out, adding a stable API is not good.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt questioned whether the patch's concept of swap tiers is a new feature or an evolution of existing per-cgroup swap control mechanisms, specifically referencing Google's prodkernel team and their past implementation of memory.swapfiles.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "unclear",
                "historical context"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I am assuming you meant Google and particularly Prodkernel team and not\nAndroid or ChromeOS. Google's prodkernel used to have per-cgroup\nswapfiles exposed through memory.swapfiles (if I remember correctly\nSuleiman implemented this along with ghost swapfiles). Later this was\ndeprecated (by Yu Zhao) and global (ghost) swapfiles were being used.\nThe memory.swapfiles interface instead of supporting real swapfiles\nstarted having select options among default, ghost/zswap and real\n(something like that). However such interface was used to just disable\nor enable zswap for a workload and never about hierarchically\ncontrolling the swap devices (Google prodkernel only have zswap). Has\nsomething changed?",
              "reply_to": "Chris Li",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed skepticism about the introduction of a new swap tier interface without a clear use case, prompting him to push back even harder on its addition.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "pushback",
                "skepticism"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This just motivates me to pushback even harder on adding a new interface\nwithout a clear use-case.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt requested a specific example of a real-world use case for hierarchical swap device control, expressing skepticism about the practicality of the proposed feature.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "skepticism",
                "lack of concrete example"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I already asked above but let me say it again. What's the actual real\nworld use-case to control/allow/disallow swap devices hierarchically?",
              "reply_to": "Chris Li",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt noted that having multiple swap devices reduces lock contention, but this does not address hierarchical control of swap devices among sub-workloads.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Having more than one swap devices to reduce lock contention is unrelated\nto hierarchically control swap devices among sub-workloads.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-21",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author is addressing concerns about the BPF-first approach suggested by Shakeel Butt, specifically questioning its feasibility in an embedded environment and asking for clarification on precedents where a BPF prototype graduated into a stable kernel interface.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "seeking clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "....\n\nAfter reading the reply and re-think more of it.\n\nI have a few questions regarding the BPF-first approach you\nsuggested, if you don't mind. Some of them I am re-asking\nbecause I feel they have not been clearly addressed yet.\n\n- We are in an embedded environment where enabling additional\n  kernel compile options is costly. BPF is disabled by\n  default in some of our production configurations. From a\n  trade-off perspective, does it make sense to enable BPF\n  just for swap device control?\n\n- You suggest starting with BPF and discussing a stable\n  interface later. I am genuinely curious, are there actual\n  precedents where a BPF prototype graduated into a stable\n  kernel interface? \n\n- You raised that stable interfaces are hard to remove. Would\n  gating it behind a CONFIG option or marking it experimental\n  be an acceptable compromise?\n\n- You already acknowledged the use-case for assigning\n  different swap devices to different workloads. Your\n  objection is specifically about hierarchical parent-child\n  partitioning. If the interface enforced uniform policy\n  within a subtree, would that be acceptable?\n\n- We already run a modified kernel with internal swap control\n  in production and have real feedback from it. Requiring BPF\n  as a prerequisite to gather production experience seems\n  unnecessary when we are already doing that.\n\nTo be honest, I am having trouble understanding the motivation\nbehind the BPF-first validation approach. If the real point is\nthat BPF enables more flexible swap-out policies than any fixed\ninterface can, that would make much more sense to me. I would\nappreciate it if you could share more on this.\n\nThanks,\nYoungjun Park",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-22",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that the patch does not handle the case where a child cgroup re-enables a tier excluded by its parent, and requested the logic to be updated to follow standard cgroup hierarchy principles.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi YoungJun,\n\nI see you have sent a separate email on BPF specific questions to which I will\nrespond separately, here I will respond to other questions/comments.\n\nOn Sat, Feb 21, 2026 at 11:30:59PM +0900, YoungJun Park wrote:",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-22",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt requested additional information about the cgroup hierarchy structure of Youngjun Park's deployment, specifically asking if they use cgroup v1 or v2 in their production environment.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If you don't mind, can you share a bit more about the cgroup hierarchy structure\nof your deployment. Do you use cgroup v1 or v2 on your production environment?",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-22",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt questioned whether the proposed swap tiers are equivalent to current swap priority behavior and requested clarification.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "uncertainty",
                "request for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I assume this is the same swap priorities as of today, right? You want similar\npriority behavior within a tier.",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-22",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt suggested that the patch authors should gather all possible options and their pros/cons before making an informed decision, rather than committing to any specific option.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested more discussion",
                "wanted a thorough evaluation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think your use-case is very clear. Before committing to any options, I want us\nto brainstorm all options and gather pros/cons and then make an informed\ndecision. Anyways I will respond to your other email (in a day or two).\n\nShakeel",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-22",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Usama Arif",
      "primary_email": "usama.arif@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    }
  ]
}