{
  "date": "2026-02-21",
  "report_file": "2026-02-21_ollama_llama3.1-8b.html",
  "llm_backends": [
    [
      "ollama",
      "llama3.1:8b"
    ]
  ],
  "generation_time_seconds": 3865.942081928253,
  "developer_reports": [
    {
      "name": "Alexandre Ghiti",
      "primary_email": "alexghiti@rivosinc.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Boris Burkov",
      "primary_email": "boris@bur.io",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Dmitry Ilvokhin",
      "primary_email": "d@ilvokhin.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Gregory Price",
      "primary_email": "gourry@gourry.net",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 1/2] cxl/region: fix region leak when attach_target fails in cxl_add_to_region",
          "message_id": "20260221043013.1420169-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260221043013.1420169-1-gourry@gourry.net/",
          "date": "2026-02-21T04:30:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch fixes a region leak issue in the cxl driver by tracking whether a new region was created and calling drop_region() to unregister it and release its HPA resource if attach_target() fails. This prevents subsequent region creation attempts from failing due to reserved HPA ranges.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "Author addressed a concern about the default device_attach() call in cxl_add_to_region(), explained that it binds the dax driver and prevents regions from being converted to sysram, and agreed to skip this step when a custom attach callback is present.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a CXL memdev has a custom attach callback, cxl_add_to_region()\nshould not call device_attach() on the auto-discovered region.\n\nThe default device_attach() binds the dax driver, which may online\nmemory via dax_kmem.  The custom attach callback then has to tear down\nthe dax stack to convert the region to sysram, but dax_kmem refuses to\noffline memory during its remove path, leaving regions stuck online.\n\nSkip device_attach() when cxlmd->attach is set.  The attach callback\nis responsible for setting up the region after auto-discovery completes\n(e.g. adding it as sysram directly).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/region.c | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 276046d49f88..e5edeabd9262 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -3971,6 +3971,12 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n \t}\n \n \tif (attach) {\n+\t\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n+\n+\t\t/* Skip device_attach if memdev has is own attach callback */\n+\t\tif (cxlmd->attach)\n+\t\t\treturn 0;\n+\n \t\t/*\n \t\t * If device_attach() fails the range may still be active via\n \t\t * the platform-firmware memory map, otherwise the driver for\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "20260221043013.1420169-2-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author is pushing back against the review, stating that the patch should be disregarded because it uses a function introduced by another contributor.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "pushing back",
                "disregard this patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "BAH - disregard this patch, it uses drop_region which is introduced by\nAlejandro here:\n\nhttps://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/",
              "reply_to": "",
              "message_date": "2026-02-21",
              "message_id": "aZk_9iYMh2QPYNDz@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Alison Schofield",
              "summary": "Reviewer noted that the patch drops the region only when newly created, but not for pre-existing regions, and suggested aligning this behavior with a previous patch that unregisters auto-created regions on assembly failure.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I see you dropping this, perhaps just for the moment, because\nthe drop_region() you wanted to use is not available yet.\n\nThis looks a lot like \n\thttps://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/\n\tcxl/region: Unregister auto-created region when assembly fails\n\tWhen auto-created region assembly fails the region remains registered\n\tbut disabled. The region continues to reserve its memory resource,\n\tpreventing DAX from registering the memory.\n\tUnregister the region on assembly failure to release the resource.\n\nAnd the review comments on that one, or at least on that thread in\ngeneral, was to leave all the broken things in place.\nI didn't agree with that, and hope to see this version move ahead\nwhen you have the drop_region you need.\n\n-- Alison",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "aZyvGnKfWI1Mku-c@aschofie-mobl2.lan",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that the patch is more of a future-proofing measure, as no code currently utilizes the attach_target failure handling, and expressed it's not a particularly useful cleanup in the current infrastructure.\n\nThe author addressed Alison Schofield's concern that the patch does not handle auto-regions properly, explaining that in their driver, auto-regions are explicitly converted into other things and if this conversion fails, it causes all other region creation to fail. The author notes that this is a narrow failure scenario only occurring when two devices unbind/bind at the same time.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "future-proofing",
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah it's not a particularly useful cleanup in the current\ninfrastructure because nothing actually uses this pattern (yet).\n\n---\n\nThe important note here is the difference between auto-regions and\nmanually created regions.  For auto-regions, you might have another\nendpoint show up looking for the partially created region - and then\njust go off and create it anyway because it thinks it was first.\n\nBut in my driver, i'm explicitly converting these auto-regions into\nother things, and if that fails it causes *all other* region creation to\nfail - even if it wasn't actually dependent on that original region.\n\nThis is only an issue if you have two devices unbind/bind cycling at\nthe same time - i.e.\n\n   echo 0000:d0:00.00 > cxl_pci/unbind\n   echo 0000:e0:00.00 > cxl_pci/unbind\n   echo 0000:d0:00.00 > mydriver/bind\n   echo 0000:e0:00.00 > mydriver/bind\n\nIf the platform has pre-programmed and locked the decoders, and one of\nthe two devices fails to probe and leaves a hanging partially\ncreated region, the other device will fail too.\n\nIt's a pretty narrow failure scenario.\n\n~Gregory",
              "reply_to": "Alison Schofield",
              "message_date": "2026-02-23",
              "message_id": "aZy1VGindEm-NbFn@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Alison Schofield",
              "summary": "Reviewer noted that the patch's handling of attach_target() failure is consistent with existing design, but pointed out a potential future issue where this same logic will fail again.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgment",
                "future-problem"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That's by design, and that'll eventually fail too.\n\nBut - is see how your case is different. Thanks for the explanation.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "aZzuak0CpP6kTtke@aschofie-mobl2.lan",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] cxl/core: fix test_bit misuse with CXL_DECODER_F_ bitmask flags",
          "message_id": "20260221021810.1390342-1-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260221021810.1390342-1-gourry@gourry.net/",
          "date": "2026-02-21T02:18:16Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch fixes a bug in the CXL (Compute Express Link) driver where two bitmask flags, CXL_DECODER_F_LOCK and CXL_DECODER_F_NORMALIZED_ADDRESSING, are being passed to test_bit() instead of being used directly. The fix involves replacing these calls with direct bitmask tests.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Benjamin Cheatham",
              "summary": "Pointed out that the bug was independently discovered and previously reported by Alison Schofield in a separate patch. Requested that Gregory add a Reported-by tag to his patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgment of existing work"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "Gregory Price",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Acknowledged that the patch is waiting on 7.0-rc1 for cxl/fixes and asked Gregory to add his review tag.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgment of patch status"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\n\nOn 2/23/26 10:45 AM, Gregory Price wrote:\n> On Mon, Feb 23, 2026 at 11:33:13AM -0600, Cheatham, Benjamin wrote:\n>> On 2/20/2026 8:18 PM, Gregory Price wrote:\n>>> CXL_DECODER_F_LOCK (BIT(4) = 16) and CXL_DECODER_F_NORMALIZED_ADDRESSING\n>>> (BIT(6) = 64) are bit masks, but three call sites pass them to test_bit()\n>>> which expects a bit number.\n>>>\n>>> Replace test_bit() with direct bitmask tests, consistent with every other\n>>> use of these flags.\n>>>\n>>> Fixes: 2230c4bdc412 (\"cxl: Add handling of locked CXL decoder\")\n>>> Signed-off-by: Gregory Price <gourry@gourry.net>\n>>\n>> Alison sent out a patch [1] two weeks ago for this. I suspect you found this bug\n>> independently, so I figured I should point it out. Otherwise, I would add a Reported-by (or some\n>> other tag) with her name.\n>>\n>> Thanks,\n>> Ben\n>>\n>> [1]: https://lore.kernel.org/linux-cxl/20260206181404.1025991-1-alison.schofield@intel.com/\n> \n> Ah, yeah, missed this, and did find independently when testing unbinds.\n> \n> Wasn't on cxl/next so I thought it hadn't been found yet.\n\nYeah waiting on 7.0-rc1 for cxl/fixes. I also asked her to split the patches into 2 fixes. But if you don't mind go add your review tag that'd be great! :) \n\n> \n> Cool, thanks!\n> ~Gregory\n\n\n",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/2] cxl/region: fix region leak when attach_target fails in cxl_add_to_region",
          "message_id": "aZk_9iYMh2QPYNDz@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZk_9iYMh2QPYNDz@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-21T05:17:46Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged a concern about device_attach() being called on auto-discovered regions when a custom attach callback is set in the CXL memdev, agreed to skip device_attach() and let the attach callback handle region setup.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a CXL memdev has a custom attach callback, cxl_add_to_region()\nshould not call device_attach() on the auto-discovered region.\n\nThe default device_attach() binds the dax driver, which may online\nmemory via dax_kmem.  The custom attach callback then has to tear down\nthe dax stack to convert the region to sysram, but dax_kmem refuses to\noffline memory during its remove path, leaving regions stuck online.\n\nSkip device_attach() when cxlmd->attach is set.  The attach callback\nis responsible for setting up the region after auto-discovery completes\n(e.g. adding it as sysram directly).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/region.c | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 276046d49f88..e5edeabd9262 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -3971,6 +3971,12 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled)\n \t}\n \n \tif (attach) {\n+\t\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n+\n+\t\t/* Skip device_attach if memdev has is own attach callback */\n+\t\tif (cxlmd->attach)\n+\t\t\treturn 0;\n+\n \t\t/*\n \t\t * If device_attach() fails the range may still be active via\n \t\t * the platform-firmware memory map, otherwise the driver for\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-20",
              "message_id": "20260221043013.1420169-2-gourry@gourry.net",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author pushed back against the review, stating that the patch should be disregarded because it uses a function introduced by another contributor.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "pushed back",
                "disregard"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "BAH - disregard this patch, it uses drop_region which is introduced by\nAlejandro here:\n\nhttps://lore.kernel.org/linux-cxl/20260201155438.2664640-20-alejandro.lucero-palau@amd.com/",
              "reply_to": "",
              "message_date": "2026-02-21",
              "message_id": "aZk_9iYMh2QPYNDz@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Alison Schofield",
              "summary": "Reviewer noted that the patch drops the auto-created region only when a new region is created, but not in other cases, and suggested adding the drop_region() call regardless of whether the region was just created or not.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I see you dropping this, perhaps just for the moment, because\nthe drop_region() you wanted to use is not available yet.\n\nThis looks a lot like \n\thttps://lore.kernel.org/linux-cxl/2a613604c0cdda6d9f838ae9b47ea6d936c5e4ce.1769746294.git.alison.schofield@intel.com/\n\tcxl/region: Unregister auto-created region when assembly fails\n\tWhen auto-created region assembly fails the region remains registered\n\tbut disabled. The region continues to reserve its memory resource,\n\tpreventing DAX from registering the memory.\n\tUnregister the region on assembly failure to release the resource.\n\nAnd the review comments on that one, or at least on that thread in\ngeneral, was to leave all the broken things in place.\nI didn't agree with that, and hope to see this version move ahead\nwhen you have the drop_region you need.\n\n-- Alison",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "aZyvGnKfWI1Mku-c@aschofie-mobl2.lan",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that the patch is more of a future-proofing measure, as no other code currently uses this pattern.\n\nAuthor explained that the issue is specific to auto-regions in certain scenarios where two devices unbind/bind at the same time, causing other region creations to fail due to a partially created region. The author acknowledged this as a narrow failure scenario.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "future-proofing",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Yeah it's not a particularly useful cleanup in the current\ninfrastructure because nothing actually uses this pattern (yet).\n\n---\n\nThe important note here is the difference between auto-regions and\nmanually created regions.  For auto-regions, you might have another\nendpoint show up looking for the partially created region - and then\njust go off and create it anyway because it thinks it was first.\n\nBut in my driver, i'm explicitly converting these auto-regions into\nother things, and if that fails it causes *all other* region creation to\nfail - even if it wasn't actually dependent on that original region.\n\nThis is only an issue if you have two devices unbind/bind cycling at\nthe same time - i.e.\n\n   echo 0000:d0:00.00 > cxl_pci/unbind\n   echo 0000:e0:00.00 > cxl_pci/unbind\n   echo 0000:d0:00.00 > mydriver/bind\n   echo 0000:e0:00.00 > mydriver/bind\n\nIf the platform has pre-programmed and locked the decoders, and one of\nthe two devices fails to probe and leaves a hanging partially\ncreated region, the other device will fail too.\n\nIt's a pretty narrow failure scenario.\n\n~Gregory",
              "reply_to": "Alison Schofield",
              "message_date": "2026-02-23",
              "message_id": "aZy1VGindEm-NbFn@gourry-fedora-PF4VCD3F",
              "analysis_source": "llm"
            },
            {
              "author": "Alison Schofield",
              "summary": "Reviewer noted that the patch's behavior of dropping the region on attach_target failure is actually intentional, but questioned whether it would eventually fail as well.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "questioning",
                "acknowledging"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That's by design, and that'll eventually fail too.\n\nBut - is see how your case is different. Thanks for the explanation.",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "aZzuak0CpP6kTtke@aschofie-mobl2.lan",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v23 10/22] cxl: Export function for unwinding cxl by accelerators",
          "message_id": "aZk5CVRELS2qo92c@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZk5CVRELS2qo92c@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-21T04:48:13Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about differentiating between CXL memory expanders and device accelerators, explaining that they will add a new function for initializing cxl_dev_state and a macro to help accel drivers embed it in their private structs.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nDifferentiate CXL memory expanders (type 3) from CXL device accelerators\n(type 2) with a new function for initializing cxl_dev_state and a macro\nfor helping accel drivers to embed cxl_dev_state inside a private\nstruct.\n\nMove structs to include/cxl as the size of the accel driver private\nstruct embedding cxl_dev_state needs to know the size of this struct.\n\nUse same new initialization with the type3 pci driver.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n---\n drivers/cxl/core/mbox.c      |  12 +-\n drivers/cxl/core/memdev.c    |  32 +++++\n drivers/cxl/cxl.h            |  97 +--------------\n drivers/cxl/cxlmem.h         |  86 +------------\n drivers/cxl/pci.c            |  14 +--\n include/cxl/cxl.h            | 226 +++++++++++++++++++++++++++++++++++\n tools/testing/cxl/test/mem.c |   3 +-\n 7 files changed, 274 insertions(+), 196 deletions(-)\n create mode 100644 include/cxl/cxl.h\n\ndiff --git a/drivers/cxl/core/mbox.c b/drivers/cxl/core/mbox.c\nindex fa6dd0c94656..bee84d0101d1 100644\n--- a/drivers/cxl/core/mbox.c\n+++ b/drivers/cxl/core/mbox.c\n@@ -1514,23 +1514,21 @@ int cxl_mailbox_init(struct cxl_mailbox *cxl_mbox, struct device *host)\n }\n EXPORT_SYMBOL_NS_GPL(cxl_mailbox_init, \"CXL\");\n \n-struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev)\n+struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n+\t\t\t\t\t\t u16 dvsec)\n {\n \tstruct cxl_memdev_state *mds;\n \tint rc;\n \n-\tmds = devm_kzalloc(dev, sizeof(*mds), GFP_KERNEL);\n+\tmds = devm_cxl_dev_state_create(dev, CXL_DEVTYPE_CLASSMEM, serial,\n+\t\t\t\t\tdvsec, struct cxl_memdev_state, cxlds,\n+\t\t\t\t\ttrue);\n \tif (!mds) {\n \t\tdev_err(dev, \"No memory available\\n\");\n \t\treturn ERR_PTR(-ENOMEM);\n \t}\n \n \tmutex_init(&mds->event.log_lock);\n-\tmds->cxlds.dev = dev;\n-\tmds->cxlds.reg_map.host = dev;\n-\tmds->cxlds.cxl_mbox.host = dev;\n-\tmds->cxlds.reg_map.resource = CXL_RESOURCE_NONE;\n-\tmds->cxlds.type = CXL_DEVTYPE_CLASSMEM;\n \n \trc = devm_cxl_register_mce_notifier(dev, &mds->mce_notifier);\n \tif (rc == -EOPNOTSUPP)\ndiff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\nindex af3d0cc65138..22d156f25305 100644\n--- a/drivers/cxl/core/memdev.c\n+++ b/drivers/cxl/core/memdev.c\n@@ -656,6 +656,38 @@ static void detach_memdev(struct work_struct *work)\n \n static struct lock_class_key cxl_memdev_key;\n \n+static void cxl_dev_state_init(struct cxl_dev_state *cxlds, struct device *dev,\n+\t\t\t       enum cxl_devtype type, u64 serial, u16 dvsec,\n+\t\t\t       bool has_mbox)\n+{\n+\t*cxlds = (struct cxl_dev_state) {\n+\t\t.dev = dev,\n+\t\t.type = type,\n+\t\t.serial = serial,\n+\t\t.cxl_dvsec = dvsec,\n+\t\t.reg_map.host = dev,\n+\t\t.reg_map.resource = CXL_RESOURCE_NONE,\n+\t};\n+\n+\tif (has_mbox)\n+\t\tcxlds->cxl_mbox.host = dev;\n+}\n+\n+struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n+\t\t\t\t\t\t enum cxl_devtype type,\n+\t\t\t\t\t\t u64 serial, u16 dvsec,\n+\t\t\t\t\t\t size_t size, bool has_mbox)\n+{\n+\tstruct cxl_dev_state *cxlds = devm_kzalloc(dev, size, GFP_KERNEL);\n+\n+\tif (!cxlds)\n+\t\treturn NULL;\n+\n+\tcxl_dev_state_init(cxlds, dev, type, serial, dvsec, has_mbox);\n+\treturn cxlds;\n+}\n+EXPORT_SYMBOL_NS_GPL(_devm_cxl_dev_state_create, \"CXL\");\n+\n static struct cxl_memdev *cxl_memdev_alloc(struct cxl_dev_state *cxlds,\n \t\t\t\t\t   const struct file_operations *fops,\n \t\t\t\t\t   const struct cxl_memdev_attach *attach)\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex e1d47062e1d3..3eaa353e430b 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -12,6 +12,7 @@\n #include <linux/node.h>\n #include <linux/io.h>\n #include <linux/range.h>\n+#include <cxl/cxl.h>\n \n extern const struct nvdimm_security_ops *cxl_security_ops;\n \n@@ -201,97 +202,6 @@ static inline int ways_to_eiw(unsigned int ways, u8 *eiw)\n #define   CXLDEV_MBOX_BG_CMD_COMMAND_VENDOR_MASK GENMASK_ULL(63, 48)\n #define CXLDEV_MBOX_PAYLOAD_OFFSET 0x20\n \n-/*\n- * Using struct_group() allows for per register-block-type helper routines,\n- * without requiring block-type agnostic code to include the prefix.\n- */\n-struct cxl_regs {\n-\t/*\n-\t * Common set of CXL Component register block base pointers\n-\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n-\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n-\t */\n-\tstruct_group_tagged(cxl_component_regs, component,\n-\t\tvoid __iomem *hdm_decoder;\n-\t\tvoid __iomem *ras;\n-\t);\n-\t/*\n-\t * Common set of CXL Device register block base pointers\n-\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n-\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n-\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n-\t */\n-\tstruct_group_tagged(cxl_device_regs, device_regs,\n-\t\tvoid __iomem *status, *mbox, *memdev;\n-\t);\n-\n-\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n-\t\tvoid __iomem *pmu;\n-\t);\n-\n-\t/*\n-\t * RCH downstream port specific RAS register\n-\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n-\t */\n-\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n-\t\tvoid __iomem *dport_aer;\n-\t);\n-\n-\t/*\n-\t * RCD upstream port specific PCIe cap register\n-\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n-\t */\n-\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n-\t\tvoid __iomem *rcd_pcie_cap;\n-\t);\n-};\n-\n-struct cxl_reg_map {\n-\tbool valid;\n-\tint id;\n-\tunsigned long offset;\n-\tunsigned long size;\n-};\n-\n-struct cxl_component_reg_map {\n-\tstruct cxl_reg_map hdm_decoder;\n-\tstruct cxl_reg_map ras;\n-};\n-\n-struct cxl_device_reg_map {\n-\tstruct cxl_reg_map status;\n-\tstruct cxl_reg_map mbox;\n-\tstruct cxl_reg_map memdev;\n-};\n-\n-struct cxl_pmu_reg_map {\n-\tstruct cxl_reg_map pmu;\n-};\n-\n-/**\n- * struct cxl_register_map - DVSEC harvested register block mapping parameters\n- * @host: device for devm operations and logging\n- * @base: virtual base of the register-block-BAR + @block_offset\n- * @resource: physical resource base of the register block\n- * @max_size: maximum mapping size to perform register search\n- * @reg_type: see enum cxl_regloc_type\n- * @component_map: cxl_reg_map for component registers\n- * @device_map: cxl_reg_maps for device registers\n- * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n- */\n-struct cxl_register_map {\n-\tstruct device *host;\n-\tvoid __iomem *base;\n-\tresource_size_t resource;\n-\tresource_size_t max_size;\n-\tu8 reg_type;\n-\tunion {\n-\t\tstruct cxl_component_reg_map component_map;\n-\t\tstruct cxl_device_reg_map device_map;\n-\t\tstruct cxl_pmu_reg_map pmu_map;\n-\t};\n-};\n-\n void cxl_probe_component_regs(struct device *dev, void __iomem *base,\n \t\t\t      struct cxl_component_reg_map *map);\n void cxl_probe_device_regs(struct device *dev, void __iomem *base,\n@@ -497,11 +407,6 @@ struct cxl_region_params {\n \tresource_size_t cache_size;\n };\n \n-enum cxl_partition_mode {\n-\tCXL_PARTMODE_RAM,\n-\tCXL_PARTMODE_PMEM,\n-};\n-\n /*\n  * Indicate whether this region has been assembled by autodetection or\n  * userspace assembly. Prevent endpoint decoders outside of automatic\ndiff --git a/drivers/cxl/cxlmem.h b/drivers/cxl/cxlmem.h\nindex ef202b34e5ea..281546de426e 100644\n--- a/drivers/cxl/cxlmem.h\n+++ b/drivers/cxl/cxlmem.h\n@@ -113,8 +113,6 @@ int devm_cxl_dpa_reserve(struct cxl_endpoint_decoder *cxled,\n \t\t\t resource_size_t base, resource_size_t len,\n \t\t\t resource_size_t skipped);\n \n-#define CXL_NR_PARTITIONS_MAX 2\n-\n struct cxl_dpa_info {\n \tu64 size;\n \tstruct cxl_dpa_part_info {\n@@ -373,87 +371,6 @@ struct cxl_security_state {\n \tstruct kernfs_node *sanitize_node;\n };\n \n-/*\n- * enum cxl_devtype - delineate type-2 from a generic type-3 device\n- * @CXL_DEVTYPE_DEVMEM - Vendor specific CXL Type-2 device implementing HDM-D or\n- *\t\t\t HDM-DB, no requirement that this device implements a\n- *\t\t\t mailbox, or other memory-device-standard manageability\n- *\t\t\t flows.\n- * @CXL_DEVTYPE_CLASSMEM - Common class definition of a CXL Type-3 device with\n- *\t\t\t   HDM-H and class-mandatory memory device registers\n- */\n-enum cxl_devtype {\n-\tCXL_DEVTYPE_DEVMEM,\n-\tCXL_DEVTYPE_CLASSMEM,\n-};\n-\n-/**\n- * struct cxl_dpa_perf - DPA performance property entry\n- * @dpa_range: range for DPA address\n- * @coord: QoS performance data (i.e. latency, bandwidth)\n- * @cdat_coord: raw QoS performance data from CDAT\n- * @qos_class: QoS Class cookies\n- */\n-struct cxl_dpa_perf {\n-\tstruct range dpa_range;\n-\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n-\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n-\tint qos_class;\n-};\n-\n-/**\n- * struct cxl_dpa_partition - DPA partition descriptor\n- * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n- * @perf: performance attributes of the partition from CDAT\n- * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n- */\n-struct cxl_dpa_partition {\n-\tstruct resource res;\n-\tstruct cxl_dpa_perf perf;\n-\tenum cxl_partition_mode mode;\n-};\n-\n-/**\n- * struct cxl_dev_state - The driver device state\n- *\n- * cxl_dev_state represents the CXL driver/device state.  It provides an\n- * interface to mailbox commands as well as some cached data about the device.\n- * Currently only memory devices are represented.\n- *\n- * @dev: The device associated with this CXL state\n- * @cxlmd: The device representing the CXL.mem capabilities of @dev\n- * @reg_map: component and ras register mapping parameters\n- * @regs: Parsed register blocks\n- * @cxl_dvsec: Offset to the PCIe device DVSEC\n- * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n- * @media_ready: Indicate whether the device media is usable\n- * @dpa_res: Overall DPA resource tree for the device\n- * @part: DPA partition array\n- * @nr_partitions: Number of DPA partitions\n- * @serial: PCIe Device Serial Number\n- * @type: Generic Memory Class device or Vendor Specific Memory device\n- * @cxl_mbox: CXL mailbox context\n- * @cxlfs: CXL features context\n- */\n-struct cxl_dev_state {\n-\tstruct device *dev;\n-\tstruct cxl_memdev *cxlmd;\n-\tstruct cxl_register_map reg_map;\n-\tstruct cxl_regs regs;\n-\tint cxl_dvsec;\n-\tbool rcd;\n-\tbool media_ready;\n-\tstruct resource dpa_res;\n-\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n-\tunsigned int nr_partitions;\n-\tu64 serial;\n-\tenum cxl_devtype type;\n-\tstruct cxl_mailbox cxl_mbox;\n-#ifdef CONFIG_CXL_FEATURES\n-\tstruct cxl_features_state *cxlfs;\n-#endif\n-};\n-\n static inline resource_size_t cxl_pmem_size(struct cxl_dev_state *cxlds)\n {\n \t/*\n@@ -858,7 +775,8 @@ int cxl_dev_state_identify(struct cxl_memdev_state *mds);\n int cxl_await_media_ready(struct cxl_dev_state *cxlds);\n int cxl_enumerate_cmds(struct cxl_memdev_state *mds);\n int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info);\n-struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev);\n+struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n+\t\t\t\t\t\t u16 dvsec);\n void set_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n \t\t\t\tunsigned long *cmds);\n void clear_exclusive_cxl_commands(struct cxl_memdev_state *mds,\ndiff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\nindex 1cf232220873..24179cc702bf 100644\n--- a/drivers/cxl/pci.c\n+++ b/drivers/cxl/pci.c\n@@ -911,25 +911,25 @@ static int cxl_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n \tint rc, pmu_count;\n \tunsigned int i;\n \tbool irq_avail;\n+\tu16 dvsec;\n \n \trc = pcim_enable_device(pdev);\n \tif (rc)\n \t\treturn rc;\n \tpci_set_master(pdev);\n \n-\tmds = cxl_memdev_state_create(&pdev->dev);\n+\tdvsec = pci_find_dvsec_capability(pdev, PCI_VENDOR_ID_CXL,\n+\t\t\t\t\t  PCI_DVSEC_CXL_DEVICE);\n+\tif (!dvsec)\n+\t\tpci_warn(pdev, \"Device DVSEC not present, skip CXL.mem init\\n\");\n+\n+\tmds = cxl_memdev_state_create(&pdev->dev, pci_get_dsn(pdev), dvsec);\n \tif (IS_ERR(mds))\n \t\treturn PTR_ERR(mds);\n \tcxlds = &mds->cxlds;\n \tpci_set_drvdata(pdev, cxlds);\n \n \tcxlds->rcd = is_cxl_restricted(pdev);\n-\tcxlds->serial = pci_get_dsn(pdev);\n-\tcxlds->cxl_dvsec = pci_find_dvsec_capability(\n-\t\tpdev, PCI_VENDOR_ID_CXL, PCI_DVSEC_CXL_DEVICE);\n-\tif (!cxlds->cxl_dvsec)\n-\t\tdev_warn(&pdev->dev,\n-\t\t\t \"Device DVSEC not present, skip CXL.mem init\\n\");\n \n \trc = cxl_pci_setup_regs(pdev, CXL_REGLOC_RBI_MEMDEV, &map);\n \tif (rc)\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nnew file mode 100644\nindex 000000000000..13d448686189\n--- /dev/null\n+++ b/include/cxl/cxl.h\n@@ -0,0 +1,226 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+/* Copyright(c) 2020 Intel Corporation. */\n+/* Copyright(c) 2025 Advanced Micro Devices, Inc. */\n+\n+#ifndef __CXL_CXL_H__\n+#define __CXL_CXL_H__\n+\n+#include <linux/node.h>\n+#include <linux/ioport.h>\n+#include <cxl/mailbox.h>\n+\n+/**\n+ * enum cxl_devtype - delineate type-2 from a generic type-3 device\n+ * @CXL_DEVTYPE_DEVMEM: Vendor specific CXL Type-2 device implementing HDM-D or\n+ *\t\t\t HDM-DB, no requirement that this device implements a\n+ *\t\t\t mailbox, or other memory-device-standard manageability\n+ *\t\t\t flows.\n+ * @CXL_DEVTYPE_CLASSMEM: Common class definition of a CXL Type-3 device with\n+ *\t\t\t   HDM-H and class-mandatory memory device registers\n+ */\n+enum cxl_devtype {\n+\tCXL_DEVTYPE_DEVMEM,\n+\tCXL_DEVTYPE_CLASSMEM,\n+};\n+\n+struct device;\n+\n+/*\n+ * Using struct_group() allows for per register-block-type helper routines,\n+ * without requiring block-type agnostic code to include the prefix.\n+ */\n+struct cxl_regs {\n+\t/*\n+\t * Common set of CXL Component register block base pointers\n+\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n+\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n+\t */\n+\tstruct_group_tagged(cxl_component_regs, component,\n+\t\tvoid __iomem *hdm_decoder;\n+\t\tvoid __iomem *ras;\n+\t);\n+\t/*\n+\t * Common set of CXL Device register block base pointers\n+\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n+\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n+\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n+\t */\n+\tstruct_group_tagged(cxl_device_regs, device_regs,\n+\t\tvoid __iomem *status, *mbox, *memdev;\n+\t);\n+\n+\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n+\t\tvoid __iomem *pmu;\n+\t);\n+\n+\t/*\n+\t * RCH downstream port specific RAS register\n+\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n+\t */\n+\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n+\t\tvoid __iomem *dport_aer;\n+\t);\n+\n+\t/*\n+\t * RCD upstream port specific PCIe cap register\n+\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n+\t */\n+\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n+\t\tvoid __iomem *rcd_pcie_cap;\n+\t);\n+};\n+\n+struct cxl_reg_map {\n+\tbool valid;\n+\tint id;\n+\tunsigned long offset;\n+\tunsigned long size;\n+};\n+\n+struct cxl_component_reg_map {\n+\tstruct cxl_reg_map hdm_decoder;\n+\tstruct cxl_reg_map ras;\n+};\n+\n+struct cxl_device_reg_map {\n+\tstruct cxl_reg_map status;\n+\tstruct cxl_reg_map mbox;\n+\tstruct cxl_reg_map memdev;\n+};\n+\n+struct cxl_pmu_reg_map {\n+\tstruct cxl_reg_map pmu;\n+};\n+\n+/**\n+ * struct cxl_register_map - DVSEC harvested register block mapping parameters\n+ * @host: device for devm operations and logging\n+ * @base: virtual base of the register-block-BAR + @block_offset\n+ * @resource: physical resource base of the register block\n+ * @max_size: maximum mapping size to perform register search\n+ * @reg_type: see enum cxl_regloc_type\n+ * @component_map: cxl_reg_map for component registers\n+ * @device_map: cxl_reg_maps for device registers\n+ * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n+ */\n+struct cxl_register_map {\n+\tstruct device *host;\n+\tvoid __iomem *base;\n+\tresource_size_t resource;\n+\tresource_size_t max_size;\n+\tu8 reg_type;\n+\tunion {\n+\t\tstruct cxl_component_reg_map component_map;\n+\t\tstruct cxl_device_reg_map device_map;\n+\t\tstruct cxl_pmu_reg_map pmu_map;\n+\t};\n+};\n+\n+/**\n+ * struct cxl_dpa_perf - DPA performance property entry\n+ * @dpa_range: range for DPA address\n+ * @coord: QoS performance data (i.e. latency, bandwidth)\n+ * @cdat_coord: raw QoS performance data from CDAT\n+ * @qos_class: QoS Class cookies\n+ */\n+struct cxl_dpa_perf {\n+\tstruct range dpa_range;\n+\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n+\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n+\tint qos_class;\n+};\n+\n+enum cxl_partition_mode {\n+\tCXL_PARTMODE_RAM,\n+\tCXL_PARTMODE_PMEM,\n+};\n+\n+/**\n+ * struct cxl_dpa_partition - DPA partition descriptor\n+ * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n+ * @perf: performance attributes of the partition from CDAT\n+ * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n+ */\n+struct cxl_dpa_partition {\n+\tstruct resource res;\n+\tstruct cxl_dpa_perf perf;\n+\tenum cxl_partition_mode mode;\n+};\n+\n+#define CXL_NR_PARTITIONS_MAX 2\n+\n+/**\n+ * struct cxl_dev_state - The driver device state\n+ *\n+ * cxl_dev_state represents the CXL driver/device state.  It provides an\n+ * interface to mailbox commands as well as some cached data about the device.\n+ * Currently only memory devices are represented.\n+ *\n+ * @dev: The device associated with this CXL state\n+ * @cxlmd: The device representing the CXL.mem capabilities of @dev\n+ * @reg_map: component and ras register mapping parameters\n+ * @regs: Parsed register blocks\n+ * @cxl_dvsec: Offset to the PCIe device DVSEC\n+ * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n+ * @media_ready: Indicate whether the device media is usable\n+ * @dpa_res: Overall DPA resource tree for the device\n+ * @part: DPA partition array\n+ * @nr_partitions: Number of DPA partitions\n+ * @serial: PCIe Device Serial Number\n+ * @type: Generic Memory Class device or Vendor Specific Memory device\n+ * @cxl_mbox: CXL mailbox context\n+ * @cxlfs: CXL features context\n+ */\n+struct cxl_dev_state {\n+\t/* public for Type2 drivers */\n+\tstruct device *dev;\n+\tstruct cxl_memdev *cxlmd;\n+\n+\t/* private for Type2 drivers */\n+\tstruct cxl_register_map reg_map;\n+\tstruct cxl_regs regs;\n+\tint cxl_dvsec;\n+\tbool rcd;\n+\tbool media_ready;\n+\tstruct resource dpa_res;\n+\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n+\tunsigned int nr_partitions;\n+\tu64 serial;\n+\tenum cxl_devtype type;\n+\tstruct cxl_mailbox cxl_mbox;\n+#ifdef CONFIG_CXL_FEATURES\n+\tstruct cxl_features_state *cxlfs;\n+#endif\n+};\n+\n+struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n+\t\t\t\t\t\t enum cxl_devtype type,\n+\t\t\t\t\t\t u64 serial, u16 dvsec,\n+\t\t\t\t\t\t size_t size, bool has_mbox);\n+\n+/**\n+ * cxl_dev_state_create - safely create and cast a cxl dev state embedded in a\n+ * driver specific struct.\n+ *\n+ * @parent: device behind the request\n+ * @type: CXL device type\n+ * @serial: device identification\n+ * @dvsec: dvsec capability offset\n+ * @drv_struct: driver struct embedding a cxl_dev_state struct\n+ * @member: drv_struct member as cxl_dev_state\n+ * @mbox: true if mailbox supported\n+ *\n+ * Returns a pointer to the drv_struct allocated and embedding a cxl_dev_state\n+ * struct initialized.\n+ *\n+ * Introduced for Type2 driver support.\n+ */\n+#define devm_cxl_dev_state_create(parent, type, serial, dvsec, drv_struct, member, mbox)\t\\\n+\t({\t\t\t\t\t\t\t\t\t\t\\\n+\t\tstatic_assert(__same_type(struct cxl_dev_state,\t\t\t\t\\\n+\t\t\t      ((drv_struct *)NULL)->member));\t\t\t\t\\\n+\t\tstatic_assert(offsetof(drv_struct, member) == 0);\t\t\t\\\n+\t\t(drv_struct *)_devm_cxl_dev_state_create(parent, type, serial, dvsec,\t\\\n+\t\t\t\t\t\t      sizeof(drv_struct), mbox);\t\\\n+\t})\n+#endif /* __CXL_CXL_H__ */\ndiff --git a/tools/testing/cxl/test/mem.c b/tools/testing/cxl/test/mem.c\nindex cb87e8c0e63c..79f42f4474d4 100644\n--- a/tools/testing/cxl/test/mem.c\n+++ b/tools/testing/cxl/test/mem.c\n@@ -1716,7 +1716,7 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n \tif (rc)\n \t\treturn rc;\n \n-\tmds = cxl_memdev_state_create(dev);\n+\tmds = cxl_memdev_state_create(dev, pdev->id + 1, 0);\n \tif (IS_ERR(mds))\n \t\treturn PTR_ERR(mds);\n \n@@ -1732,7 +1732,6 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n \tmds->event.buf = (struct cxl_get_event_payload *) mdata->event_buf;\n \tINIT_DELAYED_WORK(&mds->security.poll_dwork, cxl_mockmem_sanitize_work);\n \n-\tcxlds->serial = pdev->id + 1;\n \tif (is_rcd(pdev))\n \t\tcxlds->rcd = true;\n \n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about the dependency on Smita's patchset, specifically [PATCH v5 3/7] cxl/region: Skip decoder reset on detach for autodiscovered regions. The author confirms that this patch is needed to support the default behavior with current BIOS configuration and explains that it will be supported in follow-up works.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nThis patchset should be applied on the cxl next branch using the base\nspecified at the end of this cover letter.\n\nDependencies on Dan's work has gone and also on Terry's as the only\npatch required is now in next. The other dependency is on Smita patchset\nbut it does not exist such a dependency as that work will not avoid the\nproblem with Type2 and DAX/hmem if soft reserved memory. This needs to\nbe solved by the BIOS and Type2 UEFI driver for populating the CXL.mem\nrange as EFI_RESERVED_TYPE instead of default EFI_CONVENTIONAL_MEMORY\nwith the EFI_MEMORY_SP attribute. There exists though a dependency on\none Smita's patches:\n\n[PATCH v5 3/7] cxl/region: Skip decoder reset on detach for autodiscovered regions\n\nThis is needed for the default behaviour with current BIOS configuration\nwhere the HDM Type2 decoders will be kept unreset when driver unloads.\nThis is the main change introduced in v23: committed decoders will not\nbe reset. Previous v22 functionality supported first driver load finding\ncommitted decoders but resetting them at unload and supporting\nuncommitted decoders in next driver loads. This will be suported in\nfollow-up works.\n\nv23 changes:\n\n  patch 11: fixing minor issues and droping change in\n\t    should_emulate_decoders (Jonathan Cameron)\n\n  patch13: refactoring unregister_region for safety type in Type2 API\n\n  sfc changes: slight modifications to error path\n\n\nv22 changes:\n\n  patch 1-3 from Dan's branch without any changes.\n\n  patch 11: new\n  \n  patch 12: moved here from v21 patch 22\n\n  patch 13-14: new\n\n  patch 23: move check ahead of type3 only checks\n\n  All patches with sfc changes adapted to support both options.\n\nv21 changes;\n\n  patch1-2: v20 patch1 splitted up doing the code move in the second\n\t    patch in v21. (Jonathan)\n \n  patch1-4: adding my Signed-off tag along with Dan's\n\n  patch5: fix duplication of CXL_NR_PARTITION definition\n\n  patch7: dropped the cxl test fixes removing unused function. It was\n\t  sent independently ahead of this version.\n\n  patch12: optimization for max free space calculation (Jonathan)\n\n  patch19: optimization for returning on error (Jonathan)\n\n\nv20 changes:\n\n  patch 1: using release helps (Jonathan).\n\n  patch 6: minor fix in comments (Jonathan).\n\n  patch 7 & 8: change commit mentioning sfc changes\n\n  patch 11:\t Fix interleave_ways setting (Jonathan)\n\t\tChange assignament location (Dave)\n\n  patch 13:  \tchanging error return order (Jonathan)\n\t\tremoving blank line (Dave)\n\n  patch 18:\tAdd check for only supporting uncommitted decoders\n\t\t\t(Ben, Dave)\n\t\tAdd check for returned value (Dave)\n\nv19 changes:\n\n  Removal of cxl_acquire_endpoint and driver callback for unexpected cxl\n  module removal. Dan's patches made them unnecessary.\n\n  patch 4: remove code already moved by Terry's patches (Ben Cheatham)\n\n  patch 6: removed unrelated change (Ben Cheatham)\n\n  patch 7: fix error report inconsistencies (Jonathan, Dave)\n\n  patch 9: remove unnecessary comment (Ben Cheatham)\n\n  patch 11: fix __free usage (Jonathan Cameron, Ben Cheatham)\n\n  patch 13: style fixes (Jonathan Cameron, Dave Jiag)\n\n  patch 14: move code to previous patch (Jonathan Cameron)\n\n  patch 18: group code in one locking (Dave Jian)\n\t    use __free helper (Ben Cheatham)\n\n\nv18 changes:\n\n  patch 1: minor changes and fixing docs generation (Jonathan, Dan)\n \n  patch4: merged with v17 patch5\n\n  patch 5: merging v17 patches 6 and 7\n\n  patch 6: adding helpers for clarity\n\n  patch 9:\n\t- minor changes (Dave)\n\t- simplifying flags check (Dan)\n\n  patch 10: minor changes (Jonathan)\n\n  patch 11:\n\t- minor changes (Dave)\n\t- fix mess (Jonathan, Dave)\n\n  patch 18: minor changes (Jonathan, Dan)\n  \nv17 changes: (Dan Williams review)\n - use devm for cxl_dev_state allocation\n - using current cxl struct for checking capability registers found by\n   the driver.\n - simplify dpa initialization without a mailbox not supporting pmem\n - add cxl_acquire_endpoint for protection during initialization\n - add callback/action to cxl_create_region for a driver notified about cxl\n   core kernel modules removal.\n - add sfc function to disable CXL-based PIO buffers if such a callback\n   is invoked.\n - Always manage a Type2 created region as private not allowing DAX.\n\nv16 changes:\n - rebase against rc4 (Dave Jiang)\n - remove duplicate line (Ben Cheatham)\n\nv15 changes:\n - remove reference to unused header file (Jonathan Cameron)\n - add proper kernel docs to exported functions (Alison Schofield)\n - using an array to map the enums to strings (Alison Schofield)\n - clarify comment when using bitmap_subset (Jonathan Cameron)\n - specify link to type2 support in all patches (Alison Schofield)\n\n  Patches changed (minor): 4, 11\n\nv14 changes:\n - static null initialization of bitmaps (Jonathan Cameron)\n - Fixing cxl tests (Alison Schofield)\n - Fixing robot compilation problems\n\n  Patches changed (minor): 1, 4, 6, 13\n\nv13 changes:\n - using names for headers checking more consistent (Jonathan Cameron)\n - using helper for caps bit setting (Jonathan Cameron)\n - provide generic function for reporting missing capabilities (Jonathan Cameron)\n - rename cxl_pci_setup_memdev_regs to cxl_pci_accel_setup_memdev_regs (Jonathan Cameron)\n - cxl_dpa_info size to be set by the Type2 driver (Jonathan Cameron)\n - avoiding rc variable when possible (Jonathan Cameron)\n - fix spelling (Simon Horman)\n - use scoped_guard (Dave Jiang)\n - use enum instead of bool (Dave Jiang)\n - dropping patch with hardware symbols\n\nv12 changes:\n - use new macro cxl_dev_state_create in pci driver (Ben Cheatham)\n - add public/private sections in now exported cxl_dev_state struct (Ben\n   Cheatham)\n - fix cxl/pci.h regarding file name for checking if defined\n - Clarify capabilities found vs expected in error message. (Ben\n   Cheatham)\n - Clarify new CXL_DECODER_F flag (Ben Cheatham)\n - Fix changes about cxl memdev creation support moving code to the\n   proper patch. (Ben Cheatham)\n - Avoid debug and function duplications (Ben Cheatham)\n\nv11 changes:\n - Dropping the use of cxl_memdev_state and going back to using\n   cxl_dev_state.\n - Using a helper for an accel driver to allocate its own cxl-related\n   struct embedding cxl_dev_state.\n - Exporting the required structs in include/cxl/cxl.h for an accel\n   driver being able to know the cxl_dev_state size required in the\n   previously mentioned helper for allocation.\n - Avoid using any struct for dpa initialization by the accel driver\n   adding a specific function for creating dpa partitions by accel\n   drivers without a mailbox.\n\nv10 changes:\n - Using cxl_memdev_state instead of cxl_dev_state for type2 which has a\n   memory after all and facilitates the setup.\n - Adapt core for using cxl_memdev_state allowing accel drivers to work\n   with them without further awareness of internal cxl structs.\n - Using last DPA changes for creating DPA partitions with accel driver\n   hardcoding mds values when no mailbox.\n - capabilities not a new field but built up when current register maps\n   is performed and returned to the caller for checking.\n - HPA free space supporting interleaving.\n - DPA free space droping max-min for a simple alloc size.\n\nv9 changes:\n - adding forward definitions (Jonathan Cameron)\n - using set_bit instead of bitmap_set (Jonathan Cameron)\n - fix rebase problem (Jonathan Cameron)\n - Improve error path (Jonathan Cameron)\n - fix build problems with cxl region dependency (robot)\n - fix error path (Simon Horman)\n\nv8 changes:\n - Change error path labeling inside sfc cxl code (Edward Cree)\n - Properly handling checks and error in sfc cxl code (Simon Horman)\n - Fix bug when checking resource_size (Simon Horman)\n - Avoid bisect problems reordering patches (Edward Cree)\n - Fix buffer allocation size in sfc (Simon Horman)\n\nv7 changes:\n\n - fixing kernel test robot complains\n - fix type with Type3 mandatory capabilities (Zhi Wang)\n - optimize code in cxl_request_resource (Kalesh Anakkur Purayil)\n - add sanity check when dealing with resources arithmetics (Fan Ni)\n - fix typos and blank lines (Fan Ni)\n - keep previous log errors/warnings in sfc driver (Martin Habets)\n - add WARN_ON_ONCE if region given is NULL\n\nv6 changes:\n\n - update sfc mcdi_pcol.h with full hardware changes most not related to\n   this patchset. This is an automatic file created from hardware design\n   changes and not touched by software. It is updated from time to time\n   and it required update for the sfc driver CXL support.\n - remove CXL capabilities definitions not used by the patchset or\n   previous kernel code. (Dave Jiang, Jonathan Cameron)\n - Use bitmap_subset instead of reinventing the wheel ... (Ben Cheatham)\n - Use cxl_accel_memdev for new device_type created (Ben Cheatham)\n - Fix construct_region use of rwsem (Zhi Wang)\n - Obtain region range instead of region params (Allison Schofield, Dave\n   Jiang)\n\nv5 changes:\n\n - Fix SFC configuration based on kernel CXL configuration\n - Add subset check for capabilities.\n - fix region creation when HDM decoders programmed by firmware/BIOS (Ben\n   Cheatham)\n - Add option for creating dax region based on driver decission (Ben\n   Cheatham)\n - Using sfc probe_data struct for keeping sfc cxl data\n\nv4 changes:\n\n - Use bitmap for capabilities new field (Jonathan Cameron)\n - Use cxl_mem attributes for sysfs based on device type (Dave Jian)\n - Add conditional cxl sfc compilation relying on kernel CXL config (kernel test robot)\n - Add sfc changes in different patches for facilitating backport (Jonathan Cameron)\n - Remove patch for dealing with cxl modules dependencies and using sfc kconfig plus\n   MODULE_SOFTDEP instead.\n\nv3 changes:\n\n - cxl_dev_state not defined as opaque but only manipulated by accel drivers\n   through accessors.\n - accessors names not identified as only for accel drivers.\n - move pci code from pci driver (drivers/cxl/pci.c) to generic pci code\n   (drivers/cxl/core/pci.c).\n - capabilities field from u8 to u32 and initialised by CXL regs discovering\n   code.\n - add capabilities check and removing current check by CXL regs discovering\n   code.\n - Not fail if CXL Device Registers not found. Not mandatory for Type2.\n - add timeout in acquire_endpoint for solving a race with the endpoint port\n   creation.\n - handle EPROBE_DEFER by sfc driver.\n - Limiting interleave ways to 1 for accel driver HPA/DPA requests.\n - factoring out interleave ways and granularity helpers from type2 region\n   creation patch.\n - restricting region_creation for type2 to one endpoint decoder.\n\nv2 changes:\n\nI have removed the introduction about the concerns with BIOS/UEFI after the\ndiscussion leading to confirm the need of the functionality implemented, at\nleast is some scenarios.\n\nThere are two main changes from the RFC:\n\n1) Following concerns about drivers using CXL core without restrictions, the CXL\nstruct to work with is opaque to those drivers, therefore functions are\nimplemented for modifying or reading those structs indirectly.\n\n2) The driver for using the added functionality is not a test driver but a real\none: the SFC ethernet network driver. It uses the CXL region mapped for PIO\nbuffers instead of regions inside PCIe BARs.\n\nRFC:\n\nCurrent CXL kernel code is focused on supporting Type3 CXL devices, aka memory\nexpanders. Type2 CXL devices, aka device accelerators, share some functionalities\nbut require some special handling.\n\nFirst of all, Type2 are by definition specific to drivers doing something and not just\na memory expander, so it is expected to work with the CXL specifics. This implies the CXL\nsetup needs to be done by such a driver instead of by a generic CXL PCI driver\nas for memory expanders. Most of such setup needs to use current CXL core code\nand therefore needs to be accessible to those vendor drivers. This is accomplished\nexporting opaque CXL structs and adding and exporting functions for working with\nthose structs indirectly.\n\nSome of the patches are based on a patchset sent by Dan Williams [1] which was just\npartially integrated, most related to making things ready for Type2 but none\nrelated to specific Type2 support. Those patches based on Dans work have Dans\nsigning as co-developer, and a link to the original patch.\n\nA final note about CXL.cache is needed. This patchset does not cover it at all,\nalthough the emulated Type2 device advertises it. From the kernel point of view\nsupporting CXL.cache will imply to be sure the CXL path supports what the Type2\ndevice needs. A device accelerator will likely be connected to a Root Switch,\nbut other configurations can not be discarded. Therefore the kernel will need to\ncheck not just HPA, DPA, interleave and granularity, but also the available\nCXL.cache support and resources in each switch in the CXL path to the Type2\ndevice. I expect to contribute to this support in the following months, and\nit would be good to discuss about it when possible.\n\n[1] https://lore.kernel.org/linux-cxl/98b1f61a-e6c2-71d4-c368-50d958501b0c@intel.com/T/\n\nAlejandro Lucero (22):\n  cxl: Add type2 device basic support\n  sfc: add cxl support\n  cxl: Move pci generic code\n  cxl/sfc: Map cxl component regs\n  cxl/sfc: Initialize dpa without a mailbox\n  cxl: Prepare memdev creation for type2\n  sfc: create type2 cxl memdev\n  cxl/hdm: Add support for getting region from committed decoder\n  cxl: Add function for obtaining region range\n  cxl: Export function for unwinding cxl by accelerators\n  sfc: obtain decoder and region if committed by firmware\n  cxl: Define a driver interface for HPA free space enumeration\n  sfc: get root decoder\n  cxl: Define a driver interface for DPA allocation\n  sfc: get endpoint decoder\n  cxl: Make region type based on endpoint type\n  cxl/region: Factor out interleave ways setup\n  cxl/region: Factor out interleave granularity setup\n  cxl: Allow region creation by type2 drivers\n  cxl: Avoid dax creation for accelerators\n  sfc: create cxl region\n  sfc: support pio mapping based on cxl\n\n drivers/cxl/core/core.h               |   5 +-\n drivers/cxl/core/hdm.c                | 123 ++++++++\n drivers/cxl/core/mbox.c               |  63 +---\n drivers/cxl/core/memdev.c             | 113 ++++++-\n drivers/cxl/core/pci.c                |  63 ++++\n drivers/cxl/core/port.c               |   1 +\n drivers/cxl/core/region.c             | 434 +++++++++++++++++++++++---\n drivers/cxl/core/regs.c               |   2 +-\n drivers/cxl/cxl.h                     | 125 +-------\n drivers/cxl/cxlmem.h                  |  92 +-----\n drivers/cxl/cxlpci.h                  |  21 +-\n drivers/cxl/mem.c                     |  45 ++-\n drivers/cxl/pci.c                     |  85 +----\n drivers/net/ethernet/sfc/Kconfig      |  10 +\n drivers/net/ethernet/sfc/Makefile     |   1 +\n drivers/net/ethernet/sfc/ef10.c       |  50 ++-\n drivers/net/ethernet/sfc/efx.c        |  15 +-\n drivers/net/ethernet/sfc/efx_cxl.c    | 186 +++++++++++\n drivers/net/ethernet/sfc/efx_cxl.h    |  41 +++\n drivers/net/ethernet/sfc/net_driver.h |  12 +\n drivers/net/ethernet/sfc/nic.h        |   3 +\n include/cxl/cxl.h                     | 287 +++++++++++++++++\n include/cxl/pci.h                     |  21 ++\n tools/testing/cxl/test/mem.c          |   3 +-\n 24 files changed, 1376 insertions(+), 425 deletions(-)\n create mode 100644 drivers/net/ethernet/sfc/efx_cxl.c\n create mode 100644 drivers/net/ethernet/sfc/efx_cxl.h\n create mode 100644 include/cxl/cxl.h\n create mode 100644 include/cxl/pci.h\n\n\nbase-commit: 3f7938b1aec7f06d5b23adca83e4542fcf027001\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the export of cxl core functions for Type2 driver discovery and mapping, explained that they are being used in sfc driver cxl initialization, and confirmed that this is the correct approach.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "confirmed_correct_approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nExport cxl core functions for a Type2 driver being able to discover and\nmap the device component registers.\n\nUse it in sfc driver cxl initialization.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n---\n drivers/cxl/core/pci.c             |  1 +\n drivers/cxl/core/port.c            |  1 +\n drivers/cxl/core/regs.c            |  1 +\n drivers/cxl/cxl.h                  |  7 ------\n drivers/cxl/cxlpci.h               | 12 ----------\n drivers/cxl/pci.c                  |  1 +\n drivers/net/ethernet/sfc/efx_cxl.c | 35 ++++++++++++++++++++++++++++++\n include/cxl/cxl.h                  | 19 ++++++++++++++++\n include/cxl/pci.h                  | 21 ++++++++++++++++++\n 9 files changed, 79 insertions(+), 19 deletions(-)\n create mode 100644 include/cxl/pci.h\n\ndiff --git a/drivers/cxl/core/pci.c b/drivers/cxl/core/pci.c\nindex 6b7e50858d56..ba2d393c540a 100644\n--- a/drivers/cxl/core/pci.c\n+++ b/drivers/cxl/core/pci.c\n@@ -6,6 +6,7 @@\n #include <linux/delay.h>\n #include <linux/pci.h>\n #include <linux/pci-doe.h>\n+#include <cxl/pci.h>\n #include <linux/aer.h>\n #include <cxlpci.h>\n #include <cxlmem.h>\ndiff --git a/drivers/cxl/core/port.c b/drivers/cxl/core/port.c\nindex 54f72452fb06..385588b8b30b 100644\n--- a/drivers/cxl/core/port.c\n+++ b/drivers/cxl/core/port.c\n@@ -11,6 +11,7 @@\n #include <linux/idr.h>\n #include <linux/node.h>\n #include <cxl/einj.h>\n+#include <cxl/pci.h>\n #include <cxlmem.h>\n #include <cxlpci.h>\n #include <cxl.h>\ndiff --git a/drivers/cxl/core/regs.c b/drivers/cxl/core/regs.c\nindex 93710cf4f0a6..20c2d9fbcfe7 100644\n--- a/drivers/cxl/core/regs.c\n+++ b/drivers/cxl/core/regs.c\n@@ -4,6 +4,7 @@\n #include <linux/device.h>\n #include <linux/slab.h>\n #include <linux/pci.h>\n+#include <cxl/pci.h>\n #include <cxlmem.h>\n #include <cxlpci.h>\n #include <pmu.h>\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 5d111980d879..944c5d1ccceb 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -39,10 +39,6 @@ extern const struct nvdimm_security_ops *cxl_security_ops;\n #define   CXL_CM_CAP_HDR_ARRAY_SIZE_MASK GENMASK(31, 24)\n #define CXL_CM_CAP_PTR_MASK GENMASK(31, 20)\n \n-#define   CXL_CM_CAP_CAP_ID_RAS 0x2\n-#define   CXL_CM_CAP_CAP_ID_HDM 0x5\n-#define   CXL_CM_CAP_CAP_HDM_VERSION 1\n-\n /* HDM decoders CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure */\n #define CXL_HDM_DECODER_CAP_OFFSET 0x0\n #define   CXL_HDM_DECODER_COUNT_MASK GENMASK(3, 0)\n@@ -206,9 +202,6 @@ void cxl_probe_component_regs(struct device *dev, void __iomem *base,\n \t\t\t      struct cxl_component_reg_map *map);\n void cxl_probe_device_regs(struct device *dev, void __iomem *base,\n \t\t\t   struct cxl_device_reg_map *map);\n-int cxl_map_component_regs(const struct cxl_register_map *map,\n-\t\t\t   struct cxl_component_regs *regs,\n-\t\t\t   unsigned long map_mask);\n int cxl_map_device_regs(const struct cxl_register_map *map,\n \t\t\tstruct cxl_device_regs *regs);\n int cxl_map_pmu_regs(struct cxl_register_map *map, struct cxl_pmu_regs *regs);\ndiff --git a/drivers/cxl/cxlpci.h b/drivers/cxl/cxlpci.h\nindex d879120b2780..93df1b1fa326 100644\n--- a/drivers/cxl/cxlpci.h\n+++ b/drivers/cxl/cxlpci.h\n@@ -13,16 +13,6 @@\n  */\n #define CXL_PCI_DEFAULT_MAX_VECTORS 16\n \n-/* Register Block Identifier (RBI) */\n-enum cxl_regloc_type {\n-\tCXL_REGLOC_RBI_EMPTY = 0,\n-\tCXL_REGLOC_RBI_COMPONENT,\n-\tCXL_REGLOC_RBI_VIRT,\n-\tCXL_REGLOC_RBI_MEMDEV,\n-\tCXL_REGLOC_RBI_PMU,\n-\tCXL_REGLOC_RBI_TYPES\n-};\n-\n /*\n  * Table Access DOE, CDAT Read Entry Response\n  *\n@@ -106,6 +96,4 @@ static inline void cxl_dport_init_ras_reporting(struct cxl_dport *dport,\n \t\t\t\t\t\tstruct device *host) { }\n #endif\n \n-int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n-\t\t       struct cxl_register_map *map);\n #endif /* __CXL_PCI_H__ */\ndiff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\nindex 668d44eb1bf5..7b4699fb8870 100644\n--- a/drivers/cxl/pci.c\n+++ b/drivers/cxl/pci.c\n@@ -11,6 +11,7 @@\n #include <linux/pci.h>\n #include <linux/aer.h>\n #include <linux/io.h>\n+#include <cxl/pci.h>\n #include <cxl/mailbox.h>\n #include \"cxlmem.h\"\n #include \"cxlpci.h\"\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 8e0481d8dced..34126bc4826c 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -7,6 +7,8 @@\n \n #include <linux/pci.h>\n \n+#include <cxl/cxl.h>\n+#include <cxl/pci.h>\n #include \"net_driver.h\"\n #include \"efx_cxl.h\"\n \n@@ -18,6 +20,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \tstruct pci_dev *pci_dev = efx->pci_dev;\n \tstruct efx_cxl *cxl;\n \tu16 dvsec;\n+\tint rc;\n \n \tprobe_data->cxl_pio_initialised = false;\n \n@@ -44,6 +47,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \tif (!cxl)\n \t\treturn -ENOMEM;\n \n+\trc = cxl_pci_setup_regs(pci_dev, CXL_REGLOC_RBI_COMPONENT,\n+\t\t\t\t&cxl->cxlds.reg_map);\n+\tif (rc) {\n+\t\tpci_err(pci_dev, \"No component registers\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\tif (!cxl->cxlds.reg_map.component_map.hdm_decoder.valid) {\n+\t\tpci_err(pci_dev, \"Expected HDM component register not found\\n\");\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tif (!cxl->cxlds.reg_map.component_map.ras.valid) {\n+\t\tpci_err(pci_dev, \"Expected RAS component register not found\\n\");\n+\t\treturn -ENODEV;\n+\t}\n+\n+\trc = cxl_map_component_regs(&cxl->cxlds.reg_map,\n+\t\t\t\t    &cxl->cxlds.regs.component,\n+\t\t\t\t    BIT(CXL_CM_CAP_CAP_ID_RAS));\n+\tif (rc) {\n+\t\tpci_err(pci_dev, \"Failed to map RAS capability.\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\t/*\n+\t * Set media ready explicitly as there are neither mailbox for checking\n+\t * this state nor the CXL register involved, both not mandatory for\n+\t * type2.\n+\t */\n+\tcxl->cxlds.media_ready = true;\n+\n \tprobe_data->cxl = cxl;\n \n \treturn 0;\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 13d448686189..7f2e23bce1f7 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -70,6 +70,10 @@ struct cxl_regs {\n \t);\n };\n \n+#define   CXL_CM_CAP_CAP_ID_RAS 0x2\n+#define   CXL_CM_CAP_CAP_ID_HDM 0x5\n+#define   CXL_CM_CAP_CAP_HDM_VERSION 1\n+\n struct cxl_reg_map {\n \tbool valid;\n \tint id;\n@@ -223,4 +227,19 @@ struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n \t\t(drv_struct *)_devm_cxl_dev_state_create(parent, type, serial, dvsec,\t\\\n \t\t\t\t\t\t      sizeof(drv_struct), mbox);\t\\\n \t})\n+\n+/**\n+ * cxl_map_component_regs - map cxl component registers\n+ *\n+ * @map: cxl register map to update with the mappings\n+ * @regs: cxl component registers to work with\n+ * @map_mask: cxl component regs to map\n+ *\n+ * Returns integer: success (0) or error (-ENOMEM)\n+ *\n+ * Made public for Type2 driver support.\n+ */\n+int cxl_map_component_regs(const struct cxl_register_map *map,\n+\t\t\t   struct cxl_component_regs *regs,\n+\t\t\t   unsigned long map_mask);\n #endif /* __CXL_CXL_H__ */\ndiff --git a/include/cxl/pci.h b/include/cxl/pci.h\nnew file mode 100644\nindex 000000000000..a172439f08c6\n--- /dev/null\n+++ b/include/cxl/pci.h\n@@ -0,0 +1,21 @@\n+/* SPDX-License-Identifier: GPL-2.0-only */\n+/* Copyright(c) 2020 Intel Corporation. All rights reserved. */\n+\n+#ifndef __CXL_CXL_PCI_H__\n+#define __CXL_CXL_PCI_H__\n+\n+/* Register Block Identifier (RBI) */\n+enum cxl_regloc_type {\n+\tCXL_REGLOC_RBI_EMPTY = 0,\n+\tCXL_REGLOC_RBI_COMPONENT,\n+\tCXL_REGLOC_RBI_VIRT,\n+\tCXL_REGLOC_RBI_MEMDEV,\n+\tCXL_REGLOC_RBI_PMU,\n+\tCXL_REGLOC_RBI_TYPES\n+};\n+\n+struct cxl_register_map;\n+\n+int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n+\t\t       struct cxl_register_map *map);\n+#endif\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the cxl_pci_setup_regs function needing to handle both RCRB and RCiEP cases, explained that they moved helper functions from cxl/pci_drv.c to cxl/core/pci.c to be exported and shared with CXL Type2 device initialization, and confirmed that this change addresses the issue.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "confirmed the issue is resolved"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nInside cxl/core/pci.c there are helpers for CXL PCIe initialization\nmeanwhile cxl/pci_drv.c implements the functionality for a Type3 device\ninitialization.\n\nMove helper functions from cxl/core/pci_drv.c to cxl/core/pci.c in order\nto be exported and shared with CXL Type2 device initialization.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Fan Ni <fan.ni@samsung.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\n---\n drivers/cxl/core/core.h |  3 +-\n drivers/cxl/core/pci.c  | 62 ++++++++++++++++++++++++++++++++++++\n drivers/cxl/core/regs.c |  1 -\n drivers/cxl/cxl.h       |  2 --\n drivers/cxl/cxlpci.h    | 13 ++++++++\n drivers/cxl/pci.c       | 70 -----------------------------------------\n 6 files changed, 77 insertions(+), 74 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 422531799af2..256799d39361 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -187,5 +187,6 @@ int cxl_set_feature(struct cxl_mailbox *cxl_mbox, const uuid_t *feat_uuid,\n \t\t    size_t feat_data_size, u32 feat_flag, u16 offset,\n \t\t    u16 *return_code);\n #endif\n-\n+resource_size_t cxl_rcd_component_reg_phys(struct device *dev,\n+\t\t\t\t\t   struct cxl_dport *dport);\n #endif /* __CXL_CORE_H__ */\ndiff --git a/drivers/cxl/core/pci.c b/drivers/cxl/core/pci.c\nindex b838c59d7a3c..6b7e50858d56 100644\n--- a/drivers/cxl/core/pci.c\n+++ b/drivers/cxl/core/pci.c\n@@ -696,6 +696,68 @@ bool cxl_endpoint_decoder_reset_detected(struct cxl_port *port)\n }\n EXPORT_SYMBOL_NS_GPL(cxl_endpoint_decoder_reset_detected, \"CXL\");\n \n+static int cxl_rcrb_get_comp_regs(struct pci_dev *pdev,\n+\t\t\t\t  struct cxl_register_map *map,\n+\t\t\t\t  struct cxl_dport *dport)\n+{\n+\tresource_size_t component_reg_phys;\n+\n+\t*map = (struct cxl_register_map) {\n+\t\t.host = &pdev->dev,\n+\t\t.resource = CXL_RESOURCE_NONE,\n+\t};\n+\n+\tstruct cxl_port *port __free(put_cxl_port) =\n+\t\tcxl_pci_find_port(pdev, &dport);\n+\tif (!port)\n+\t\treturn -EPROBE_DEFER;\n+\n+\tcomponent_reg_phys = cxl_rcd_component_reg_phys(&pdev->dev, dport);\n+\tif (component_reg_phys == CXL_RESOURCE_NONE)\n+\t\treturn -ENXIO;\n+\n+\tmap->resource = component_reg_phys;\n+\tmap->reg_type = CXL_REGLOC_RBI_COMPONENT;\n+\tmap->max_size = CXL_COMPONENT_REG_BLOCK_SIZE;\n+\n+\treturn 0;\n+}\n+\n+int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n+\t\t\tstruct cxl_register_map *map)\n+{\n+\tint rc;\n+\n+\trc = cxl_find_regblock(pdev, type, map);\n+\n+\t/*\n+\t * If the Register Locator DVSEC does not exist, check if it\n+\t * is an RCH and try to extract the Component Registers from\n+\t * an RCRB.\n+\t */\n+\tif (rc && type == CXL_REGLOC_RBI_COMPONENT && is_cxl_restricted(pdev)) {\n+\t\tstruct cxl_dport *dport;\n+\t\tstruct cxl_port *port __free(put_cxl_port) =\n+\t\t\tcxl_pci_find_port(pdev, &dport);\n+\t\tif (!port)\n+\t\t\treturn -EPROBE_DEFER;\n+\n+\t\trc = cxl_rcrb_get_comp_regs(pdev, map, dport);\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\n+\t\trc = cxl_dport_map_rcd_linkcap(pdev, dport);\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\n+\t} else if (rc) {\n+\t\treturn rc;\n+\t}\n+\n+\treturn cxl_setup_regs(map);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_pci_setup_regs, \"CXL\");\n+\n int cxl_pci_get_bandwidth(struct pci_dev *pdev, struct access_coordinate *c)\n {\n \tint speed, bw;\ndiff --git a/drivers/cxl/core/regs.c b/drivers/cxl/core/regs.c\nindex a010b3214342..93710cf4f0a6 100644\n--- a/drivers/cxl/core/regs.c\n+++ b/drivers/cxl/core/regs.c\n@@ -641,4 +641,3 @@ resource_size_t cxl_rcd_component_reg_phys(struct device *dev,\n \t\treturn CXL_RESOURCE_NONE;\n \treturn __rcrb_to_component(dev, &dport->rcrb, CXL_RCRB_UPSTREAM);\n }\n-EXPORT_SYMBOL_NS_GPL(cxl_rcd_component_reg_phys, \"CXL\");\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 3eaa353e430b..5d111980d879 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -222,8 +222,6 @@ int cxl_find_regblock(struct pci_dev *pdev, enum cxl_regloc_type type,\n \t\t      struct cxl_register_map *map);\n int cxl_setup_regs(struct cxl_register_map *map);\n struct cxl_dport;\n-resource_size_t cxl_rcd_component_reg_phys(struct device *dev,\n-\t\t\t\t\t   struct cxl_dport *dport);\n int cxl_dport_map_rcd_linkcap(struct pci_dev *pdev, struct cxl_dport *dport);\n \n #define CXL_RESOURCE_NONE ((resource_size_t) -1)\ndiff --git a/drivers/cxl/cxlpci.h b/drivers/cxl/cxlpci.h\nindex 6f9c78886fd9..d879120b2780 100644\n--- a/drivers/cxl/cxlpci.h\n+++ b/drivers/cxl/cxlpci.h\n@@ -74,6 +74,17 @@ static inline bool cxl_pci_flit_256(struct pci_dev *pdev)\n \treturn lnksta2 & PCI_EXP_LNKSTA2_FLIT;\n }\n \n+/*\n+ * Assume that the caller has already validated that @pdev has CXL\n+ * capabilities, any RCiEP with CXL capabilities is treated as a\n+ * Restricted CXL Device (RCD) and finds upstream port and endpoint\n+ * registers in a Root Complex Register Block (RCRB).\n+ */\n+static inline bool is_cxl_restricted(struct pci_dev *pdev)\n+{\n+\treturn pci_pcie_type(pdev) == PCI_EXP_TYPE_RC_END;\n+}\n+\n struct cxl_dev_state;\n void read_cdat_data(struct cxl_port *port);\n \n@@ -95,4 +106,6 @@ static inline void cxl_dport_init_ras_reporting(struct cxl_dport *dport,\n \t\t\t\t\t\tstruct device *host) { }\n #endif\n \n+int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n+\t\t       struct cxl_register_map *map);\n #endif /* __CXL_PCI_H__ */\ndiff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\nindex 24179cc702bf..668d44eb1bf5 100644\n--- a/drivers/cxl/pci.c\n+++ b/drivers/cxl/pci.c\n@@ -465,76 +465,6 @@ static int cxl_pci_setup_mailbox(struct cxl_memdev_state *mds, bool irq_avail)\n \treturn 0;\n }\n \n-/*\n- * Assume that any RCIEP that emits the CXL memory expander class code\n- * is an RCD\n- */\n-static bool is_cxl_restricted(struct pci_dev *pdev)\n-{\n-\treturn pci_pcie_type(pdev) == PCI_EXP_TYPE_RC_END;\n-}\n-\n-static int cxl_rcrb_get_comp_regs(struct pci_dev *pdev,\n-\t\t\t\t  struct cxl_register_map *map,\n-\t\t\t\t  struct cxl_dport *dport)\n-{\n-\tresource_size_t component_reg_phys;\n-\n-\t*map = (struct cxl_register_map) {\n-\t\t.host = &pdev->dev,\n-\t\t.resource = CXL_RESOURCE_NONE,\n-\t};\n-\n-\tstruct cxl_port *port __free(put_cxl_port) =\n-\t\tcxl_pci_find_port(pdev, &dport);\n-\tif (!port)\n-\t\treturn -EPROBE_DEFER;\n-\n-\tcomponent_reg_phys = cxl_rcd_component_reg_phys(&pdev->dev, dport);\n-\tif (component_reg_phys == CXL_RESOURCE_NONE)\n-\t\treturn -ENXIO;\n-\n-\tmap->resource = component_reg_phys;\n-\tmap->reg_type = CXL_REGLOC_RBI_COMPONENT;\n-\tmap->max_size = CXL_COMPONENT_REG_BLOCK_SIZE;\n-\n-\treturn 0;\n-}\n-\n-static int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n-\t\t\t      struct cxl_register_map *map)\n-{\n-\tint rc;\n-\n-\trc = cxl_find_regblock(pdev, type, map);\n-\n-\t/*\n-\t * If the Register Locator DVSEC does not exist, check if it\n-\t * is an RCH and try to extract the Component Registers from\n-\t * an RCRB.\n-\t */\n-\tif (rc && type == CXL_REGLOC_RBI_COMPONENT && is_cxl_restricted(pdev)) {\n-\t\tstruct cxl_dport *dport;\n-\t\tstruct cxl_port *port __free(put_cxl_port) =\n-\t\t\tcxl_pci_find_port(pdev, &dport);\n-\t\tif (!port)\n-\t\t\treturn -EPROBE_DEFER;\n-\n-\t\trc = cxl_rcrb_get_comp_regs(pdev, map, dport);\n-\t\tif (rc)\n-\t\t\treturn rc;\n-\n-\t\trc = cxl_dport_map_rcd_linkcap(pdev, dport);\n-\t\tif (rc)\n-\t\t\treturn rc;\n-\n-\t} else if (rc) {\n-\t\treturn rc;\n-\t}\n-\n-\treturn cxl_setup_regs(map);\n-}\n-\n static int cxl_pci_ras_unmask(struct pci_dev *pdev)\n {\n \tstruct cxl_dev_state *cxlds = pci_get_drvdata(pdev);\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about the cxl_mem_get_partition_info function being moved to memdev.c, which was previously in mbox.c. The author explains that this move allows Type2 drivers to initialize DPA by giving the size of their volatile hardware partition, and adds the sfc driver as a client. No fix is planned for the current patch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nType3 relies on mailbox CXL_MBOX_OP_IDENTIFY command for initializing\nmemdev state params which end up being used for DPA initialization.\n\nAllow a Type2 driver to initialize DPA simply by giving the size of its\nvolatile hardware partition.\n\nMove related functions to memdev.\n\nAdd sfc driver as the client.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n---\n drivers/cxl/core/core.h            |  2 +\n drivers/cxl/core/mbox.c            | 51 +----------------------\n drivers/cxl/core/memdev.c          | 66 ++++++++++++++++++++++++++++++\n drivers/net/ethernet/sfc/efx_cxl.c |  5 +++\n include/cxl/cxl.h                  |  1 +\n 5 files changed, 75 insertions(+), 50 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 256799d39361..e3c85ceda248 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -89,6 +89,8 @@ void __iomem *devm_cxl_iomap_block(struct device *dev, resource_size_t addr,\n struct dentry *cxl_debugfs_create_dir(const char *dir);\n int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n \t\t     enum cxl_partition_mode mode);\n+struct cxl_memdev_state;\n+int cxl_mem_get_partition_info(struct cxl_memdev_state *mds);\n int cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size);\n int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n resource_size_t cxl_dpa_size(struct cxl_endpoint_decoder *cxled);\ndiff --git a/drivers/cxl/core/mbox.c b/drivers/cxl/core/mbox.c\nindex bee84d0101d1..d57a0c2d39fb 100644\n--- a/drivers/cxl/core/mbox.c\n+++ b/drivers/cxl/core/mbox.c\n@@ -1144,7 +1144,7 @@ EXPORT_SYMBOL_NS_GPL(cxl_mem_get_event_records, \"CXL\");\n  *\n  * See CXL @8.2.9.5.2.1 Get Partition Info\n  */\n-static int cxl_mem_get_partition_info(struct cxl_memdev_state *mds)\n+int cxl_mem_get_partition_info(struct cxl_memdev_state *mds)\n {\n \tstruct cxl_mailbox *cxl_mbox = &mds->cxlds.cxl_mbox;\n \tstruct cxl_mbox_get_partition_info pi;\n@@ -1300,55 +1300,6 @@ int cxl_mem_sanitize(struct cxl_memdev *cxlmd, u16 cmd)\n \treturn -EBUSY;\n }\n \n-static void add_part(struct cxl_dpa_info *info, u64 start, u64 size, enum cxl_partition_mode mode)\n-{\n-\tint i = info->nr_partitions;\n-\n-\tif (size == 0)\n-\t\treturn;\n-\n-\tinfo->part[i].range = (struct range) {\n-\t\t.start = start,\n-\t\t.end = start + size - 1,\n-\t};\n-\tinfo->part[i].mode = mode;\n-\tinfo->nr_partitions++;\n-}\n-\n-int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info)\n-{\n-\tstruct cxl_dev_state *cxlds = &mds->cxlds;\n-\tstruct device *dev = cxlds->dev;\n-\tint rc;\n-\n-\tif (!cxlds->media_ready) {\n-\t\tinfo->size = 0;\n-\t\treturn 0;\n-\t}\n-\n-\tinfo->size = mds->total_bytes;\n-\n-\tif (mds->partition_align_bytes == 0) {\n-\t\tadd_part(info, 0, mds->volatile_only_bytes, CXL_PARTMODE_RAM);\n-\t\tadd_part(info, mds->volatile_only_bytes,\n-\t\t\t mds->persistent_only_bytes, CXL_PARTMODE_PMEM);\n-\t\treturn 0;\n-\t}\n-\n-\trc = cxl_mem_get_partition_info(mds);\n-\tif (rc) {\n-\t\tdev_err(dev, \"Failed to query partition information\\n\");\n-\t\treturn rc;\n-\t}\n-\n-\tadd_part(info, 0, mds->active_volatile_bytes, CXL_PARTMODE_RAM);\n-\tadd_part(info, mds->active_volatile_bytes, mds->active_persistent_bytes,\n-\t\t CXL_PARTMODE_PMEM);\n-\n-\treturn 0;\n-}\n-EXPORT_SYMBOL_NS_GPL(cxl_mem_dpa_fetch, \"CXL\");\n-\n int cxl_get_dirty_count(struct cxl_memdev_state *mds, u32 *count)\n {\n \tstruct cxl_mailbox *cxl_mbox = &mds->cxlds.cxl_mbox;\ndiff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\nindex 22d156f25305..2c5dd72f43ca 100644\n--- a/drivers/cxl/core/memdev.c\n+++ b/drivers/cxl/core/memdev.c\n@@ -582,6 +582,72 @@ bool is_cxl_memdev(const struct device *dev)\n }\n EXPORT_SYMBOL_NS_GPL(is_cxl_memdev, \"CXL\");\n \n+static void add_part(struct cxl_dpa_info *info, u64 start, u64 size, enum cxl_partition_mode mode)\n+{\n+\tint i = info->nr_partitions;\n+\n+\tif (size == 0)\n+\t\treturn;\n+\n+\tinfo->part[i].range = (struct range) {\n+\t\t.start = start,\n+\t\t.end = start + size - 1,\n+\t};\n+\tinfo->part[i].mode = mode;\n+\tinfo->nr_partitions++;\n+}\n+\n+int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info)\n+{\n+\tstruct cxl_dev_state *cxlds = &mds->cxlds;\n+\tstruct device *dev = cxlds->dev;\n+\tint rc;\n+\n+\tif (!cxlds->media_ready) {\n+\t\tinfo->size = 0;\n+\t\treturn 0;\n+\t}\n+\n+\tinfo->size = mds->total_bytes;\n+\n+\tif (mds->partition_align_bytes == 0) {\n+\t\tadd_part(info, 0, mds->volatile_only_bytes, CXL_PARTMODE_RAM);\n+\t\tadd_part(info, mds->volatile_only_bytes,\n+\t\t\t mds->persistent_only_bytes, CXL_PARTMODE_PMEM);\n+\t\treturn 0;\n+\t}\n+\n+\trc = cxl_mem_get_partition_info(mds);\n+\tif (rc) {\n+\t\tdev_err(dev, \"Failed to query partition information\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\tadd_part(info, 0, mds->active_volatile_bytes, CXL_PARTMODE_RAM);\n+\tadd_part(info, mds->active_volatile_bytes, mds->active_persistent_bytes,\n+\t\t CXL_PARTMODE_PMEM);\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_mem_dpa_fetch, \"CXL\");\n+\n+/**\n+ * cxl_set_capacity: initialize dpa by a driver without a mailbox.\n+ *\n+ * @cxlds: pointer to cxl_dev_state\n+ * @capacity: device volatile memory size\n+ */\n+int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity)\n+{\n+\tstruct cxl_dpa_info range_info = {\n+\t\t.size = capacity,\n+\t};\n+\n+\tadd_part(&range_info, 0, capacity, CXL_PARTMODE_RAM);\n+\treturn cxl_dpa_setup(cxlds, &range_info);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_set_capacity, \"CXL\");\n+\n /**\n  * set_exclusive_cxl_commands() - atomically disable user cxl commands\n  * @mds: The device state to operate on\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 34126bc4826c..0b10a2e6aceb 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -79,6 +79,11 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t */\n \tcxl->cxlds.media_ready = true;\n \n+\tif (cxl_set_capacity(&cxl->cxlds, EFX_CTPIO_BUFFER_SIZE)) {\n+\t\tpci_err(pci_dev, \"dpa capacity setup failed\\n\");\n+\t\treturn -ENODEV;\n+\t}\n+\n \tprobe_data->cxl = cxl;\n \n \treturn 0;\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 7f2e23bce1f7..fb2f8f2395d5 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -242,4 +242,5 @@ struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n int cxl_map_component_regs(const struct cxl_register_map *map,\n \t\t\t   struct cxl_component_regs *regs,\n \t\t\t   unsigned long map_mask);\n+int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern that the current CXL core relies on a specific device type when creating a memdev, leading to issues with obtaining cxl_memdev_state references from a different device type. The author modified the check for obtaining cxl_memdev_state to add support for the CXL_DEVTYPE_DEVMEM type and made devm_cxl_add_memdev accessible from an accel driver.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCurrent cxl core is relying on a CXL_DEVTYPE_CLASSMEM type device when\ncreating a memdev leading to problems when obtaining cxl_memdev_state\nreferences from a CXL_DEVTYPE_DEVMEM type.\n\nModify check for obtaining cxl_memdev_state adding CXL_DEVTYPE_DEVMEM\nsupport.\n\nMake devm_cxl_add_memdev accessible from an accel driver.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\n---\n drivers/cxl/core/memdev.c | 15 +++++++++++--\n drivers/cxl/cxlmem.h      |  6 ------\n drivers/cxl/mem.c         | 45 +++++++++++++++++++++++++++++----------\n include/cxl/cxl.h         |  6 ++++++\n 4 files changed, 53 insertions(+), 19 deletions(-)\n\ndiff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\nindex 2c5dd72f43ca..1b43763b8e20 100644\n--- a/drivers/cxl/core/memdev.c\n+++ b/drivers/cxl/core/memdev.c\n@@ -7,6 +7,7 @@\n #include <linux/slab.h>\n #include <linux/idr.h>\n #include <linux/pci.h>\n+#include <cxl/cxl.h>\n #include <cxlmem.h>\n #include \"trace.h\"\n #include \"core.h\"\n@@ -576,9 +577,16 @@ static const struct device_type cxl_memdev_type = {\n \t.groups = cxl_memdev_attribute_groups,\n };\n \n+static const struct device_type cxl_accel_memdev_type = {\n+\t.name = \"cxl_accel_memdev\",\n+\t.release = cxl_memdev_release,\n+\t.devnode = cxl_memdev_devnode,\n+};\n+\n bool is_cxl_memdev(const struct device *dev)\n {\n-\treturn dev->type == &cxl_memdev_type;\n+\treturn (dev->type == &cxl_memdev_type ||\n+\t\tdev->type == &cxl_accel_memdev_type);\n }\n EXPORT_SYMBOL_NS_GPL(is_cxl_memdev, \"CXL\");\n \n@@ -781,7 +789,10 @@ static struct cxl_memdev *cxl_memdev_alloc(struct cxl_dev_state *cxlds,\n \tdev->parent = cxlds->dev;\n \tdev->bus = &cxl_bus_type;\n \tdev->devt = MKDEV(cxl_mem_major, cxlmd->id);\n-\tdev->type = &cxl_memdev_type;\n+\tif (cxlds->type == CXL_DEVTYPE_DEVMEM)\n+\t\tdev->type = &cxl_accel_memdev_type;\n+\telse\n+\t\tdev->type = &cxl_memdev_type;\n \tdevice_set_pm_not_required(dev);\n \tINIT_WORK(&cxlmd->detach_work, detach_memdev);\n \ndiff --git a/drivers/cxl/cxlmem.h b/drivers/cxl/cxlmem.h\nindex 281546de426e..c98db6f18aa2 100644\n--- a/drivers/cxl/cxlmem.h\n+++ b/drivers/cxl/cxlmem.h\n@@ -34,10 +34,6 @@\n \t(FIELD_GET(CXLMDEV_RESET_NEEDED_MASK, status) !=                       \\\n \t CXLMDEV_RESET_NEEDED_NOT)\n \n-struct cxl_memdev_attach {\n-\tint (*probe)(struct cxl_memdev *cxlmd);\n-};\n-\n /**\n  * struct cxl_memdev - CXL bus object representing a Type-3 Memory Device\n  * @dev: driver core device object\n@@ -103,8 +99,6 @@ static inline bool is_cxl_endpoint(struct cxl_port *port)\n \n struct cxl_memdev *__devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n \t\t\t\t\t const struct cxl_memdev_attach *attach);\n-struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n-\t\t\t\t       const struct cxl_memdev_attach *attach);\n int devm_cxl_sanitize_setup_notifier(struct device *host,\n \t\t\t\t     struct cxl_memdev *cxlmd);\n struct cxl_memdev_state;\ndiff --git a/drivers/cxl/mem.c b/drivers/cxl/mem.c\nindex 0958bea915ac..39687baedd1a 100644\n--- a/drivers/cxl/mem.c\n+++ b/drivers/cxl/mem.c\n@@ -65,6 +65,26 @@ static int cxl_debugfs_poison_clear(void *data, u64 dpa)\n DEFINE_DEBUGFS_ATTRIBUTE(cxl_poison_clear_fops, NULL,\n \t\t\t cxl_debugfs_poison_clear, \"%llx\\n\");\n \n+static void cxl_memdev_poison_enable(struct cxl_memdev_state *mds,\n+\t\t\t\t     struct cxl_memdev *cxlmd,\n+\t\t\t\t     struct dentry *dentry)\n+{\n+\t/*\n+\t * Avoid poison debugfs for DEVMEM aka accelerators as they rely on\n+\t * cxl_memdev_state.\n+\t */\n+\tif (!mds)\n+\t\treturn;\n+\n+\tif (test_bit(CXL_POISON_ENABLED_INJECT, mds->poison.enabled_cmds))\n+\t\tdebugfs_create_file(\"inject_poison\", 0200, dentry, cxlmd,\n+\t\t\t\t    &cxl_poison_inject_fops);\n+\n+\tif (test_bit(CXL_POISON_ENABLED_CLEAR, mds->poison.enabled_cmds))\n+\t\tdebugfs_create_file(\"clear_poison\", 0200, dentry, cxlmd,\n+\t\t\t\t    &cxl_poison_clear_fops);\n+}\n+\n static int cxl_mem_probe(struct device *dev)\n {\n \tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n@@ -92,12 +112,7 @@ static int cxl_mem_probe(struct device *dev)\n \tdentry = cxl_debugfs_create_dir(dev_name(dev));\n \tdebugfs_create_devm_seqfile(dev, \"dpamem\", dentry, cxl_mem_dpa_show);\n \n-\tif (test_bit(CXL_POISON_ENABLED_INJECT, mds->poison.enabled_cmds))\n-\t\tdebugfs_create_file(\"inject_poison\", 0200, dentry, cxlmd,\n-\t\t\t\t    &cxl_poison_inject_fops);\n-\tif (test_bit(CXL_POISON_ENABLED_CLEAR, mds->poison.enabled_cmds))\n-\t\tdebugfs_create_file(\"clear_poison\", 0200, dentry, cxlmd,\n-\t\t\t\t    &cxl_poison_clear_fops);\n+\tcxl_memdev_poison_enable(mds, cxlmd, dentry);\n \n \trc = devm_add_action_or_reset(dev, remove_debugfs, dentry);\n \tif (rc)\n@@ -208,16 +223,24 @@ static ssize_t trigger_poison_list_store(struct device *dev,\n }\n static DEVICE_ATTR_WO(trigger_poison_list);\n \n-static umode_t cxl_mem_visible(struct kobject *kobj, struct attribute *a, int n)\n+static bool cxl_poison_attr_visible(struct kobject *kobj, struct attribute *a)\n {\n \tstruct device *dev = kobj_to_dev(kobj);\n \tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n \tstruct cxl_memdev_state *mds = to_cxl_memdev_state(cxlmd->cxlds);\n \n-\tif (a == &dev_attr_trigger_poison_list.attr)\n-\t\tif (!test_bit(CXL_POISON_ENABLED_LIST,\n-\t\t\t      mds->poison.enabled_cmds))\n-\t\t\treturn 0;\n+\tif (!mds ||\n+\t    !test_bit(CXL_POISON_ENABLED_LIST, mds->poison.enabled_cmds))\n+\t\treturn false;\n+\n+\treturn true;\n+}\n+\n+static umode_t cxl_mem_visible(struct kobject *kobj, struct attribute *a, int n)\n+{\n+\tif (a == &dev_attr_trigger_poison_list.attr &&\n+\t    !cxl_poison_attr_visible(kobj, a))\n+\t\treturn 0;\n \n \treturn a->mode;\n }\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex fb2f8f2395d5..6f8d365067af 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -153,6 +153,10 @@ struct cxl_dpa_partition {\n \n #define CXL_NR_PARTITIONS_MAX 2\n \n+struct cxl_memdev_attach {\n+\tint (*probe)(struct cxl_memdev *cxlmd);\n+};\n+\n /**\n  * struct cxl_dev_state - The driver device state\n  *\n@@ -243,4 +247,6 @@ int cxl_map_component_regs(const struct cxl_register_map *map,\n \t\t\t   struct cxl_component_regs *regs,\n \t\t\t   unsigned long map_mask);\n int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n+struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n+\t\t\t\t       const struct cxl_memdev_attach *attach);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about the cxl memory device creation, specifically that it was not using the type2 cxl_dev_state struct as required by the new CXL API. The author has modified the code to use this struct and added error handling for creating the cxl memory device.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nUse cxl API for creating a cxl memory device using the type2\ncxl_dev_state struct.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Martin Habets <habetsm.xilinx@gmail.com>\nReviewed-by: Fan Ni <fan.ni@samsung.com>\nAcked-by: Edward Cree <ecree.xilinx@gmail.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/net/ethernet/sfc/efx_cxl.c | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 0b10a2e6aceb..a77ef4783fcb 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -84,6 +84,12 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\treturn -ENODEV;\n \t}\n \n+\tcxl->cxlmd = devm_cxl_add_memdev(&cxl->cxlds, NULL);\n+\tif (IS_ERR(cxl->cxlmd)) {\n+\t\tpci_err(pci_dev, \"CXL accel memdev creation failed\");\n+\t\treturn PTR_ERR(cxl->cxlmd);\n+\t}\n+\n \tprobe_data->cxl = cxl;\n \n \treturn 0;\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author is addressing a concern about the HDM (Host Data Management) being committed by the BIOS for Type2 devices, which can cause issues when trying to create a CXL region. The author has added a new function cxl_get_committed_decoder() that checks if the HDM is already committed after memdev creation and returns the corresponding decoder if it is. This change allows Type2 drivers to work with the HPA (Host Physical Address) and DPA (Device Physical Address) space even when the HDM is not committed.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "added new function",
                "addressed concern"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nA Type2 device configured by the BIOS can already have its HDM\ncommitted. Add a cxl_get_committed_decoder() function for cheking\nso after memdev creation. A CXL region should have been created\nduring memdev initialization, therefore a Type2 driver can ask for\nsuch a region for working with the HPA. If the HDM is not committed,\na Type2 driver will create the region after obtaining proper HPA\nand DPA space.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/hdm.c | 39 +++++++++++++++++++++++++++++++++++++++\n include/cxl/cxl.h      |  3 +++\n 2 files changed, 42 insertions(+)\n\ndiff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\nindex 6e516c69b2d2..a172ce4e9b19 100644\n--- a/drivers/cxl/core/hdm.c\n+++ b/drivers/cxl/core/hdm.c\n@@ -686,6 +686,45 @@ int cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n \treturn devm_add_action_or_reset(&port->dev, cxl_dpa_release, cxled);\n }\n \n+static int find_committed_endpoint_decoder(struct device *dev, const void *data)\n+{\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_port *port;\n+\n+\tif (!is_endpoint_decoder(dev))\n+\t\treturn 0;\n+\n+\tcxled = to_cxl_endpoint_decoder(dev);\n+\tport = cxled_to_port(cxled);\n+\n+\treturn cxled->cxld.id == port->hdm_end;\n+}\n+\n+struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t\t       struct cxl_region **cxlr)\n+{\n+\tstruct cxl_port *endpoint = cxlmd->endpoint;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct device *cxled_dev;\n+\n+\tif (!endpoint)\n+\t\treturn NULL;\n+\n+\tguard(rwsem_read)(&cxl_rwsem.dpa);\n+\tcxled_dev = device_find_child(&endpoint->dev, NULL,\n+\t\t\t\t      find_committed_endpoint_decoder);\n+\n+\tif (!cxled_dev)\n+\t\treturn NULL;\n+\n+\tcxled = to_cxl_endpoint_decoder(cxled_dev);\n+\t*cxlr = cxled->cxld.region;\n+\n+\tput_device(cxled_dev);\n+\treturn cxled;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_get_committed_decoder, \"CXL\");\n+\n static void cxld_set_interleave(struct cxl_decoder *cxld, u32 *ctrl)\n {\n \tu16 eig;\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 6f8d365067af..928276dba952 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -249,4 +249,7 @@ int cxl_map_component_regs(const struct cxl_register_map *map,\n int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n \t\t\t\t       const struct cxl_memdev_attach *attach);\n+struct cxl_region;\n+struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t\t       struct cxl_region **cxlr);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the accelerator driver API not having a clean exit mechanism, and added cxl_unregister_region() to handle this. The function is exported for use by other drivers.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nAdd cxl_unregister_region() to the accelerator driver API\nfor a clean exit.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/region.c | 17 ++++++++++++-----\n include/cxl/cxl.h         |  1 +\n 2 files changed, 13 insertions(+), 5 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex acf29ba3b205..954b8fcdbac6 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2438,9 +2438,8 @@ static struct cxl_region *to_cxl_region(struct device *dev)\n \treturn container_of(dev, struct cxl_region, dev);\n }\n \n-static void unregister_region(void *_cxlr)\n+void cxl_unregister_region(struct cxl_region *cxlr)\n {\n-\tstruct cxl_region *cxlr = _cxlr;\n \tstruct cxl_region_params *p = &cxlr->params;\n \tint i;\n \n@@ -2457,6 +2456,14 @@ static void unregister_region(void *_cxlr)\n \tcxl_region_iomem_release(cxlr);\n \tput_device(&cxlr->dev);\n }\n+EXPORT_SYMBOL_NS_GPL(cxl_unregister_region, \"CXL\");\n+\n+static void __unregister_region(void *_cxlr)\n+{\n+\tstruct cxl_region *cxlr = _cxlr;\n+\n+\treturn cxl_unregister_region(cxlr);\n+}\n \n static struct lock_class_key cxl_region_key;\n \n@@ -2608,7 +2615,7 @@ static struct cxl_region *devm_cxl_add_region(struct cxl_root_decoder *cxlrd,\n \tif (rc)\n \t\tgoto err;\n \n-\trc = devm_add_action_or_reset(port->uport_dev, unregister_region, cxlr);\n+\trc = devm_add_action_or_reset(port->uport_dev, __unregister_region, cxlr);\n \tif (rc)\n \t\treturn ERR_PTR(rc);\n \n@@ -2762,7 +2769,7 @@ static ssize_t delete_region_store(struct device *dev,\n \tif (IS_ERR(cxlr))\n \t\treturn PTR_ERR(cxlr);\n \n-\tdevm_release_action(port->uport_dev, unregister_region, cxlr);\n+\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n \tput_device(&cxlr->dev);\n \n \treturn len;\n@@ -3878,7 +3885,7 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n \n \trc = __construct_region(cxlr, cxlrd, cxled);\n \tif (rc) {\n-\t\tdevm_release_action(port->uport_dev, unregister_region, cxlr);\n+\t\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n \t\treturn ERR_PTR(rc);\n \t}\n \ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 906065e0d2a6..92880c26b2d5 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -254,4 +254,5 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n \t\t\t\t\t\t       struct cxl_region **cxlr);\n struct range;\n int cxl_get_region_range(struct cxl_region *region, struct range *range);\n+void cxl_unregister_region(struct cxl_region *cxlr);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about Type2 drivers accessing CXL region structs by adding a function to get the cxl region range, which will be used for mapping memory ranges. The function is exported and added to the cxl.h header file.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nA CXL region struct contains the physical address to work with.\n\nType2 drivers can create a CXL region but have not access to the\nrelated struct as it is defined as private by the kernel CXL core.\nAdd a function for getting the cxl region range to be used for mapping\nsuch memory range by a Type2 driver.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Zhi Wang <zhiw@nvidia.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/cxl/core/region.c | 23 +++++++++++++++++++++++\n include/cxl/cxl.h         |  2 ++\n 2 files changed, 25 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 96888d87a8df..acf29ba3b205 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2621,6 +2621,29 @@ static struct cxl_region *devm_cxl_add_region(struct cxl_root_decoder *cxlrd,\n \treturn ERR_PTR(rc);\n }\n \n+/**\n+ * cxl_get_region_range - obtain range linked to a CXL region\n+ *\n+ * @region: a pointer to struct cxl_region\n+ * @range: a pointer to a struct range to be set\n+ *\n+ * Returns 0 or error.\n+ */\n+int cxl_get_region_range(struct cxl_region *region, struct range *range)\n+{\n+\tif (WARN_ON_ONCE(!region))\n+\t\treturn -ENODEV;\n+\n+\tif (!region->params.res)\n+\t\treturn -ENOSPC;\n+\n+\trange->start = region->params.res->start;\n+\trange->end = region->params.res->end;\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_get_region_range, \"CXL\");\n+\n static ssize_t __create_region_show(struct cxl_root_decoder *cxlrd, char *buf)\n {\n \treturn sysfs_emit(buf, \"region%u\\n\", atomic_read(&cxlrd->region_id));\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 928276dba952..906065e0d2a6 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -252,4 +252,6 @@ struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n struct cxl_region;\n struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n \t\t\t\t\t\t       struct cxl_region **cxlr);\n+struct range;\n+int cxl_get_region_range(struct cxl_region *region, struct range *range);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the complexity of finding a suitable root decoder for CXL region creation, explaining that it involves determining available Host Physical Address (HPA) and allocating capacity from Device Physical Address (DPA). They added a new API to retrieve a root decoder with specific constraints and capacity, along with a complementary function to release the reference to such a decoder. The author did not mention any plans for revising the patch in response to this feedback.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCXL region creation involves allocating capacity from Device Physical\nAddress (DPA) and assigning it to decode a given Host Physical Address\n(HPA). Before determining how much DPA to allocate the amount of available\nHPA must be determined. Also, not all HPA is created equal, some HPA\ntargets RAM, some targets PMEM, some is prepared for device-memory flows\nlike HDM-D and HDM-DB, and some is HDM-H (host-only).\n\nIn order to support Type2 CXL devices, wrap all of those concerns into\nan API that retrieves a root decoder (platform CXL window) that fits the\nspecified constraints and the capacity available for a new region.\n\nAdd a complementary function for releasing the reference to such root\ndecoder.\n\nBased on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n---\n drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h         |   3 +\n include/cxl/cxl.h         |   6 ++\n 3 files changed, 173 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 954b8fcdbac6..bdefd088f5f1 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n \treturn 0;\n }\n \n+struct cxlrd_max_context {\n+\tstruct device * const *host_bridges;\n+\tint interleave_ways;\n+\tunsigned long flags;\n+\tresource_size_t max_hpa;\n+\tstruct cxl_root_decoder *cxlrd;\n+};\n+\n+static int find_max_hpa(struct device *dev, void *data)\n+{\n+\tstruct cxlrd_max_context *ctx = data;\n+\tstruct cxl_switch_decoder *cxlsd;\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct resource *res, *prev;\n+\tstruct cxl_decoder *cxld;\n+\tresource_size_t free = 0;\n+\tresource_size_t max;\n+\tint found = 0;\n+\n+\tif (!is_root_decoder(dev))\n+\t\treturn 0;\n+\n+\tcxlrd = to_cxl_root_decoder(dev);\n+\tcxlsd = &cxlrd->cxlsd;\n+\tcxld = &cxlsd->cxld;\n+\n+\tif ((cxld->flags & ctx->flags) != ctx->flags) {\n+\t\tdev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n+\t\t\tcxld->flags, ctx->flags);\n+\t\treturn 0;\n+\t}\n+\n+\tfor (int i = 0; i < ctx->interleave_ways; i++) {\n+\t\tfor (int j = 0; j < ctx->interleave_ways; j++) {\n+\t\t\tif (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n+\t\t\t\tfound++;\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif (found != ctx->interleave_ways) {\n+\t\tdev_dbg(dev,\n+\t\t\t\"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n+\t\t\tfound, ctx->interleave_ways);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Walk the root decoder resource range relying on cxl_rwsem.region to\n+\t * preclude sibling arrival/departure and find the largest free space\n+\t * gap.\n+\t */\n+\tlockdep_assert_held_read(&cxl_rwsem.region);\n+\tres = cxlrd->res->child;\n+\n+\t/* With no resource child the whole parent resource is available */\n+\tif (!res)\n+\t\tmax = resource_size(cxlrd->res);\n+\telse\n+\t\tmax = 0;\n+\n+\tfor (prev = NULL; res; prev = res, res = res->sibling) {\n+\t\tif (!prev && res->start == cxlrd->res->start &&\n+\t\t    res->end == cxlrd->res->end) {\n+\t\t\tmax = resource_size(cxlrd->res);\n+\t\t\tbreak;\n+\t\t}\n+\t\t/*\n+\t\t * Sanity check for preventing arithmetic problems below as a\n+\t\t * resource with size 0 could imply using the end field below\n+\t\t * when set to unsigned zero - 1 or all f in hex.\n+\t\t */\n+\t\tif (prev && !resource_size(prev))\n+\t\t\tcontinue;\n+\n+\t\tif (!prev && res->start > cxlrd->res->start) {\n+\t\t\tfree = res->start - cxlrd->res->start;\n+\t\t\tmax = max(free, max);\n+\t\t}\n+\t\tif (prev && res->start > prev->end + 1) {\n+\t\t\tfree = res->start - prev->end + 1;\n+\t\t\tmax = max(free, max);\n+\t\t}\n+\t}\n+\n+\tif (prev && prev->end + 1 < cxlrd->res->end + 1) {\n+\t\tfree = cxlrd->res->end + 1 - prev->end + 1;\n+\t\tmax = max(free, max);\n+\t}\n+\n+\tdev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n+\tif (max > ctx->max_hpa) {\n+\t\tif (ctx->cxlrd)\n+\t\t\tput_device(cxlrd_dev(ctx->cxlrd));\n+\t\tget_device(cxlrd_dev(cxlrd));\n+\t\tctx->cxlrd = cxlrd;\n+\t\tctx->max_hpa = max;\n+\t}\n+\treturn 0;\n+}\n+\n+/**\n+ * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n+ * @cxlmd: the mem device requiring the HPA\n+ * @interleave_ways: number of entries in @host_bridges\n+ * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n+ * @max_avail_contig: output parameter of max contiguous bytes available in the\n+ *\t\t      returned decoder\n+ *\n+ * Returns a pointer to a struct cxl_root_decoder\n+ *\n+ * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n+ * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n+ * caller goes to use this decoder and its capacity is reduced then caller needs\n+ * to loop and retry.\n+ *\n+ * The returned root decoder has an elevated reference count that needs to be\n+ * put with cxl_put_root_decoder(cxlrd).\n+ */\n+struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t       int interleave_ways,\n+\t\t\t\t\t       unsigned long flags,\n+\t\t\t\t\t       resource_size_t *max_avail_contig)\n+{\n+\tstruct cxlrd_max_context ctx = {\n+\t\t.flags = flags,\n+\t\t.interleave_ways = interleave_ways,\n+\t};\n+\tstruct cxl_port *root_port;\n+\tstruct cxl_port *endpoint;\n+\n+\tendpoint = cxlmd->endpoint;\n+\tif (!endpoint) {\n+\t\tdev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n+\t\treturn ERR_PTR(-ENXIO);\n+\t}\n+\n+\tctx.host_bridges = &endpoint->host_bridge;\n+\n+\tstruct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n+\tif (!root) {\n+\t\tdev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n+\t\treturn ERR_PTR(-ENXIO);\n+\t}\n+\n+\troot_port = &root->port;\n+\tscoped_guard(rwsem_read, &cxl_rwsem.region)\n+\t\tdevice_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n+\n+\tif (!ctx.cxlrd)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\t*max_avail_contig = ctx.max_hpa;\n+\treturn ctx.cxlrd;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n+\n+void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n+{\n+\tput_device(cxlrd_dev(cxlrd));\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n+\n static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n \t\t\t  const char *buf, size_t len)\n {\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 944c5d1ccceb..c7d9b2c2908f 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n bool is_root_decoder(struct device *dev);\n+\n+#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n+\n bool is_switch_decoder(struct device *dev);\n bool is_endpoint_decoder(struct device *dev);\n struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 92880c26b2d5..834dc7e78934 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n struct range;\n int cxl_get_region_range(struct cxl_region *region, struct range *range);\n void cxl_unregister_region(struct cxl_region *cxlr);\n+struct cxl_port;\n+struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t       int interleave_ways,\n+\t\t\t\t\t       unsigned long flags,\n+\t\t\t\t\t       resource_size_t *max);\n+void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "Author acknowledged that the current code only supports Type3 or CXL_DECODER_HOSTONLYMEM devices and agreed to modify the region type based on the endpoint type HDM-D[B] for Type2 support.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a limitation",
                "agreed to modify"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCurrent code is expecting Type3 or CXL_DECODER_HOSTONLYMEM devices only.\nSupport for Type2 implies region type needs to be based on the endpoint\ntype HDM-D[B] instead.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Zhi Wang <zhiw@nvidia.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\nReviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n---\n drivers/cxl/core/region.c | 10 ++++++----\n 1 file changed, 6 insertions(+), 4 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex bdefd088f5f1..f53b2e9fd9e6 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2833,7 +2833,8 @@ static ssize_t create_ram_region_show(struct device *dev,\n }\n \n static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n-\t\t\t\t\t  enum cxl_partition_mode mode, int id)\n+\t\t\t\t\t  enum cxl_partition_mode mode, int id,\n+\t\t\t\t\t  enum cxl_decoder_type target_type)\n {\n \tint rc;\n \n@@ -2855,7 +2856,7 @@ static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n \t\treturn ERR_PTR(-EBUSY);\n \t}\n \n-\treturn devm_cxl_add_region(cxlrd, id, mode, CXL_DECODER_HOSTONLYMEM);\n+\treturn devm_cxl_add_region(cxlrd, id, mode, target_type);\n }\n \n static ssize_t create_region_store(struct device *dev, const char *buf,\n@@ -2869,7 +2870,7 @@ static ssize_t create_region_store(struct device *dev, const char *buf,\n \tif (rc != 1)\n \t\treturn -EINVAL;\n \n-\tcxlr = __create_region(cxlrd, mode, id);\n+\tcxlr = __create_region(cxlrd, mode, id, CXL_DECODER_HOSTONLYMEM);\n \tif (IS_ERR(cxlr))\n \t\treturn PTR_ERR(cxlr);\n \n@@ -4036,7 +4037,8 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n \n \tdo {\n \t\tcxlr = __create_region(cxlrd, cxlds->part[part].mode,\n-\t\t\t\t       atomic_read(&cxlrd->region_id));\n+\t\t\t\t       atomic_read(&cxlrd->region_id),\n+\t\t\t\t       cxled->cxld.target_type);\n \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n \n \tif (IS_ERR(cxlr)) {\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about using the cxl API for getting HPA (Host Physical Address) from a CXL root decoder. They modified the code to use cxl_get_hpa_freespace() instead of directly accessing the hdm control register, and added checks for sufficient free HPA space. The patch will need further revision.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "patch modification",
                "acknowledgment of concern"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nUse cxl api for getting HPA (Host Physical Address) to use from a\nCXL root decoder.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Martin Habets <habetsm.xilinx@gmail.com>\nAcked-by: Edward Cree <ecree.xilinx@gmail.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n---\n drivers/cxl/cxl.h                  | 15 ---------------\n drivers/net/ethernet/sfc/Kconfig   |  1 +\n drivers/net/ethernet/sfc/efx_cxl.c | 26 +++++++++++++++++++++++---\n drivers/net/ethernet/sfc/efx_cxl.h |  1 +\n include/cxl/cxl.h                  | 15 +++++++++++++++\n 5 files changed, 40 insertions(+), 18 deletions(-)\n\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex c7d9b2c2908f..d1b010e5e1d0 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -220,21 +220,6 @@ int cxl_dport_map_rcd_linkcap(struct pci_dev *pdev, struct cxl_dport *dport);\n #define CXL_RESOURCE_NONE ((resource_size_t) -1)\n #define CXL_TARGET_STRLEN 20\n \n-/*\n- * cxl_decoder flags that define the type of memory / devices this\n- * decoder supports as well as configuration lock status See \"CXL 2.0\n- * 8.2.5.12.7 CXL HDM Decoder 0 Control Register\" for details.\n- * Additionally indicate whether decoder settings were autodetected,\n- * user customized.\n- */\n-#define CXL_DECODER_F_RAM   BIT(0)\n-#define CXL_DECODER_F_PMEM  BIT(1)\n-#define CXL_DECODER_F_TYPE2 BIT(2)\n-#define CXL_DECODER_F_TYPE3 BIT(3)\n-#define CXL_DECODER_F_LOCK  BIT(4)\n-#define CXL_DECODER_F_ENABLE    BIT(5)\n-#define CXL_DECODER_F_MASK  GENMASK(5, 0)\n-\n enum cxl_decoder_type {\n \tCXL_DECODER_DEVMEM = 2,\n \tCXL_DECODER_HOSTONLYMEM = 3,\ndiff --git a/drivers/net/ethernet/sfc/Kconfig b/drivers/net/ethernet/sfc/Kconfig\nindex 979f2801e2a8..e959d9b4f4ce 100644\n--- a/drivers/net/ethernet/sfc/Kconfig\n+++ b/drivers/net/ethernet/sfc/Kconfig\n@@ -69,6 +69,7 @@ config SFC_MCDI_LOGGING\n config SFC_CXL\n \tbool \"Solarflare SFC9100-family CXL support\"\n \tdepends on SFC && CXL_BUS >= SFC\n+\tdepends on CXL_REGION\n \tdefault SFC\n \thelp\n \t  This enables SFC CXL support if the kernel is configuring CXL for\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 3536eccf1b2a..1a4c1097c315 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -18,6 +18,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n {\n \tstruct efx_nic *efx = &probe_data->efx;\n \tstruct pci_dev *pci_dev = efx->pci_dev;\n+\tresource_size_t max_size;\n \tstruct efx_cxl *cxl;\n \tstruct range range;\n \tu16 dvsec;\n@@ -110,9 +111,24 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\t\treturn -ENOMEM;\n \t\t}\n \n-\t\tprobe_data->cxl = cxl;\n+\t\tcxl->hdm_was_committed = true;\n+\t} else {\n+\t\tcxl->cxlrd = cxl_get_hpa_freespace(cxl->cxlmd, 1, CXL_DECODER_F_RAM |\n+\t\t\t\t\t\t   CXL_DECODER_F_TYPE2, &max_size);\n+\t\tif (IS_ERR(cxl->cxlrd)) {\n+\t\t\tdev_err(&pci_dev->dev, \"cxl_get_hpa_freespace failed\\n\");\n+\t\t\treturn PTR_ERR(cxl->cxlrd);\n+\t\t}\n+\n+\t\tif (max_size < EFX_CTPIO_BUFFER_SIZE) {\n+\t\t\tdev_err(&pci_dev->dev, \"%s: not enough free HPA space %pap < %u\\n\",\n+\t\t\t\t__func__, &max_size, EFX_CTPIO_BUFFER_SIZE);\n+\t\t\tcxl_put_root_decoder(cxl->cxlrd);\n+\t\t\treturn -ENOSPC;\n+\t\t}\n \t}\n \n+\tprobe_data->cxl = cxl;\n \treturn 0;\n }\n \n@@ -121,8 +137,12 @@ void efx_cxl_exit(struct efx_probe_data *probe_data)\n \tif (!probe_data->cxl)\n \t\treturn;\n \n-\tiounmap(probe_data->cxl->ctpio_cxl);\n-\tcxl_unregister_region(probe_data->cxl->efx_region);\n+\tif (probe_data->cxl->hdm_was_committed) {\n+\t\tiounmap(probe_data->cxl->ctpio_cxl);\n+\t\tcxl_unregister_region(probe_data->cxl->efx_region);\n+\t} else {\n+\t\tcxl_put_root_decoder(probe_data->cxl->cxlrd);\n+\t}\n }\n \n MODULE_IMPORT_NS(\"CXL\");\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.h b/drivers/net/ethernet/sfc/efx_cxl.h\nindex 961639cef692..9a92e386695b 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.h\n+++ b/drivers/net/ethernet/sfc/efx_cxl.h\n@@ -27,6 +27,7 @@ struct efx_cxl {\n \tstruct cxl_root_decoder *cxlrd;\n \tstruct cxl_port *endpoint;\n \tstruct cxl_endpoint_decoder *cxled;\n+\tbool hdm_was_committed;\n \tstruct cxl_region *efx_region;\n \tvoid __iomem *ctpio_cxl;\n };\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 834dc7e78934..783ad570a6eb 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -153,6 +153,21 @@ struct cxl_dpa_partition {\n \n #define CXL_NR_PARTITIONS_MAX 2\n \n+/*\n+ * cxl_decoder flags that define the type of memory / devices this\n+ * decoder supports as well as configuration lock status See \"CXL 2.0\n+ * 8.2.5.12.7 CXL HDM Decoder 0 Control Register\" for details.\n+ * Additionally indicate whether decoder settings were autodetected,\n+ * user customized.\n+ */\n+#define CXL_DECODER_F_RAM   BIT(0)\n+#define CXL_DECODER_F_PMEM  BIT(1)\n+#define CXL_DECODER_F_TYPE2 BIT(2)\n+#define CXL_DECODER_F_TYPE3 BIT(3)\n+#define CXL_DECODER_F_LOCK  BIT(4)\n+#define CXL_DECODER_F_ENABLE    BIT(5)\n+#define CXL_DECODER_F_MASK  GENMASK(5, 0)\n+\n struct cxl_memdev_attach {\n \tint (*probe)(struct cxl_memdev *cxlmd);\n };\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "Author acknowledged that the CXL region should exist if a device HDM is already committed during firmware/BIOS initialization, and agreed to add code to check for this condition in efx_cxl_init().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCheck if device HDM is already committed during firmware/BIOS\ninitialization.\n\nA CXL region should exist if so after memdev allocation/initialization.\nGet HPA from region and map it.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/net/ethernet/sfc/efx_cxl.c | 28 +++++++++++++++++++++++++++-\n 1 file changed, 27 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex a77ef4783fcb..3536eccf1b2a 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -19,6 +19,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \tstruct efx_nic *efx = &probe_data->efx;\n \tstruct pci_dev *pci_dev = efx->pci_dev;\n \tstruct efx_cxl *cxl;\n+\tstruct range range;\n \tu16 dvsec;\n \tint rc;\n \n@@ -90,13 +91,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\treturn PTR_ERR(cxl->cxlmd);\n \t}\n \n-\tprobe_data->cxl = cxl;\n+\tcxl->cxled = cxl_get_committed_decoder(cxl->cxlmd, &cxl->efx_region);\n+\tif (cxl->cxled) {\n+\t\tif (!cxl->efx_region) {\n+\t\t\tpci_err(pci_dev, \"CXL found committed decoder without a region\");\n+\t\t\treturn -ENODEV;\n+\t\t}\n+\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n+\t\tif (rc) {\n+\t\t\tpci_err(pci_dev,\n+\t\t\t\t\"CXL getting regions params from a committed decoder failed\");\n+\t\t\treturn rc;\n+\t\t}\n+\n+\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n+\t\tif (!cxl->ctpio_cxl) {\n+\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tprobe_data->cxl = cxl;\n+\t}\n \n \treturn 0;\n }\n \n void efx_cxl_exit(struct efx_probe_data *probe_data)\n {\n+\tif (!probe_data->cxl)\n+\t\treturn;\n+\n+\tiounmap(probe_data->cxl->ctpio_cxl);\n+\tcxl_unregister_region(probe_data->cxl->efx_region);\n }\n \n MODULE_IMPORT_NS(\"CXL\");\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "Author addressed a concern about the cxl_create_region() call in efx_cxl_init(), explained that it should be used to create a region using the endpoint decoder related to a DPA range, and confirmed that this change will be included in the patch.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged concern",
                "confirmed fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nUse cxl api for creating a region using the endpoint decoder related to\na DPA range.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/net/ethernet/sfc/efx_cxl.c | 10 +++++++++-\n 1 file changed, 9 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 2cfd0a46225f..4d5f3974e51d 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -134,6 +134,14 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\t\tcxl_put_root_decoder(cxl->cxlrd);\n \t\t\treturn PTR_ERR(cxl->cxled);\n \t\t}\n+\n+\t\tcxl->efx_region = cxl_create_region(cxl->cxlrd, &cxl->cxled, 1);\n+\t\tif (IS_ERR(cxl->efx_region)) {\n+\t\t\tpci_err(pci_dev, \"CXL accel create region failed\");\n+\t\t\tcxl_put_root_decoder(cxl->cxlrd);\n+\t\t\tcxl_dpa_free(cxl->cxled);\n+\t\t\treturn PTR_ERR(cxl->efx_region);\n+\t\t}\n \t}\n \n \tprobe_data->cxl = cxl;\n@@ -147,11 +155,11 @@ void efx_cxl_exit(struct efx_probe_data *probe_data)\n \n \tif (probe_data->cxl->hdm_was_committed) {\n \t\tiounmap(probe_data->cxl->ctpio_cxl);\n-\t\tcxl_unregister_region(probe_data->cxl->efx_region);\n \t} else {\n \t\tcxl_dpa_free(probe_data->cxl->cxled);\n \t\tcxl_put_root_decoder(probe_data->cxl->cxlrd);\n \t}\n+\tcxl_unregister_region(probe_data->cxl->efx_region);\n }\n \n MODULE_IMPORT_NS(\"CXL\");\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about the interleave ways store function, which was previously acquiring the region rwsem lock for write and then immediately releasing it. The author refactored the code to use the set_interleave_ways helper function, which acquires the lock only once and calls the helper function. This change is intended to improve performance by reducing lock contention.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "refactoring"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup for interleave ways.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Zhi Wang <zhiw@nvidia.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\n---\n drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n 1 file changed, 27 insertions(+), 16 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex f53b2e9fd9e6..ece1d3df7cf1 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n \n static const struct attribute_group *get_cxl_region_target_group(void);\n \n-static ssize_t interleave_ways_store(struct device *dev,\n-\t\t\t\t     struct device_attribute *attr,\n-\t\t\t\t     const char *buf, size_t len)\n+static int set_interleave_ways(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n+\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tunsigned int val, save;\n-\tint rc;\n+\tint save, rc;\n \tu8 iw;\n \n-\trc = kstrtouint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = ways_to_eiw(val, &iw);\n \tif (rc)\n \t\treturn rc;\n@@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n \t\treturn -EINVAL;\n \t}\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n@@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n \tsave = p->interleave_ways;\n \tp->interleave_ways = val;\n \trc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n-\tif (rc) {\n+\tif (rc)\n \t\tp->interleave_ways = save;\n+\n+\treturn rc;\n+}\n+\n+static ssize_t interleave_ways_store(struct device *dev,\n+\t\t\t\t     struct device_attribute *attr,\n+\t\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tunsigned int val;\n+\tint rc;\n+\n+\trc = kstrtouint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_ways(cxlr, val);\n+\tif (rc)\n \t\treturn rc;\n-\t}\n \n \treturn len;\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed the concern about finding available DPA (device-physical-address) capacity to map into HPA (host-physical-address) space for CXL Type2 devices, and provided a new API cxl_request_dpa() that tries to allocate the required DPA memory. The author explained how this API works and included code changes to implement it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "new API implementation",
                "code changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation involves finding available DPA (device-physical-address)\ncapacity to map into HPA (host-physical-address) space.\n\nIn order to support CXL Type2 devices, define an API, cxl_request_dpa(),\nthat tries to allocate the DPA memory the driver requires to operate.The\nmemory requested should not be bigger than the max available HPA obtained\npreviously with cxl_get_hpa_freespace().\n\nBased on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h      |  1 +\n include/cxl/cxl.h      |  5 +++\n 3 files changed, 90 insertions(+)\n\ndiff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\nindex a172ce4e9b19..d60a697f12cc 100644\n--- a/drivers/cxl/core/hdm.c\n+++ b/drivers/cxl/core/hdm.c\n@@ -3,6 +3,7 @@\n #include <linux/seq_file.h>\n #include <linux/device.h>\n #include <linux/delay.h>\n+#include <cxl/cxl.h>\n \n #include \"cxlmem.h\"\n #include \"core.h\"\n@@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n \treturn resource_contains(res, &_addr);\n }\n \n+/**\n+ * cxl_dpa_free - release DPA (Device Physical Address)\n+ * @cxled: endpoint decoder linked to the DPA\n+ *\n+ * Returns 0 or error.\n+ */\n int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n {\n \tstruct cxl_port *port = cxled_to_port(cxled);\n@@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n \tdevm_cxl_dpa_release(cxled);\n \treturn 0;\n }\n+EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n \n int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n \t\t     enum cxl_partition_mode mode)\n@@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n \treturn 0;\n }\n \n+static int find_free_decoder(struct device *dev, const void *data)\n+{\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_port *port;\n+\n+\tif (!is_endpoint_decoder(dev))\n+\t\treturn 0;\n+\n+\tcxled = to_cxl_endpoint_decoder(dev);\n+\tport = cxled_to_port(cxled);\n+\n+\treturn cxled->cxld.id == (port->hdm_end + 1);\n+}\n+\n+static struct cxl_endpoint_decoder *\n+cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_port *endpoint = cxlmd->endpoint;\n+\tstruct device *dev;\n+\n+\tguard(rwsem_read)(&cxl_rwsem.dpa);\n+\tdev = device_find_child(&endpoint->dev, NULL,\n+\t\t\t\tfind_free_decoder);\n+\tif (!dev)\n+\t\treturn NULL;\n+\n+\treturn to_cxl_endpoint_decoder(dev);\n+}\n+\n+/**\n+ * cxl_request_dpa - search and reserve DPA given input constraints\n+ * @cxlmd: memdev with an endpoint port with available decoders\n+ * @mode: CXL partition mode (ram vs pmem)\n+ * @alloc: dpa size required\n+ *\n+ * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n+ * an errno encoded pointer on failure.\n+ *\n+ * Given that a region needs to allocate from limited HPA capacity it\n+ * may be the case that a device has more mappable DPA capacity than\n+ * available HPA. The expectation is that @alloc is a driver known\n+ * value based on the device capacity but which could not be fully\n+ * available due to HPA constraints.\n+ *\n+ * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n+ * reserved, or an error pointer. The caller is also expected to own the\n+ * lifetime of the memdev registration associated with the endpoint to\n+ * pin the decoder registered as well.\n+ */\n+struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t     enum cxl_partition_mode mode,\n+\t\t\t\t\t     resource_size_t alloc)\n+{\n+\tint rc;\n+\n+\tif (!IS_ALIGNED(alloc, SZ_256M))\n+\t\treturn ERR_PTR(-EINVAL);\n+\n+\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n+\t\tcxl_find_free_decoder(cxlmd);\n+\n+\tif (!cxled)\n+\t\treturn ERR_PTR(-ENODEV);\n+\n+\trc = cxl_dpa_set_part(cxled, mode);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\trc = cxl_dpa_alloc(cxled, alloc);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\treturn no_free_ptr(cxled);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n+\n static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n {\n \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex d1b010e5e1d0..2b1f7d687a0e 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n \n DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n+DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n \ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 783ad570a6eb..4802371db00e 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -7,6 +7,7 @@\n \n #include <linux/node.h>\n #include <linux/ioport.h>\n+#include <linux/range.h>\n #include <cxl/mailbox.h>\n \n /**\n@@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n \t\t\t\t\t       unsigned long flags,\n \t\t\t\t\t       resource_size_t *max);\n void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n+struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t     enum cxl_partition_mode mode,\n+\t\t\t\t\t     resource_size_t alloc);\n+int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about interleaving granularity being set in user space, agreeing to factor out a common helper for kernel-driven region creation and acknowledging the need to restructure the code to handle this scenario.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup forinterleave\ngranularity.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Zhi Wang <zhiw@nvidia.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\n---\n drivers/cxl/core/region.c | 39 +++++++++++++++++++++++++--------------\n 1 file changed, 25 insertions(+), 14 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex ece1d3df7cf1..63c2aeb2ee1f 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -559,21 +559,14 @@ static ssize_t interleave_granularity_show(struct device *dev,\n \treturn sysfs_emit(buf, \"%d\\n\", p->interleave_granularity);\n }\n \n-static ssize_t interleave_granularity_store(struct device *dev,\n-\t\t\t\t\t    struct device_attribute *attr,\n-\t\t\t\t\t    const char *buf, size_t len)\n+static int set_interleave_granularity(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n+\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tint rc, val;\n+\tint rc;\n \tu16 ig;\n \n-\trc = kstrtoint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = granularity_to_eig(val, &ig);\n \tif (rc)\n \t\treturn rc;\n@@ -589,14 +582,32 @@ static ssize_t interleave_granularity_store(struct device *dev,\n \tif (cxld->interleave_ways > 1 && val != cxld->interleave_granularity)\n \t\treturn -EINVAL;\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n-\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n \n \tp->interleave_granularity = val;\n+\treturn 0;\n+}\n+\n+static ssize_t interleave_granularity_store(struct device *dev,\n+\t\t\t\t\t    struct device_attribute *attr,\n+\t\t\t\t\t    const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tint rc, val;\n+\n+\trc = kstrtoint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_granularity(cxlr, val);\n+\tif (rc)\n+\t\treturn rc;\n \n \treturn len;\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "Author addressed a concern about using the CXL API to get the Device Physical Address (DPA) and agreed to use it through an endpoint decoder.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "implemented"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nUse cxl api for getting DPA (Device Physical Address) to use through an\nendpoint decoder.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Martin Habets <habetsm.xilinx@gmail.com>\nAcked-by: Edward Cree <ecree.xilinx@gmail.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/net/ethernet/sfc/efx_cxl.c | 9 +++++++++\n 1 file changed, 9 insertions(+)\n\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 1a4c1097c315..2cfd0a46225f 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -126,6 +126,14 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\t\tcxl_put_root_decoder(cxl->cxlrd);\n \t\t\treturn -ENOSPC;\n \t\t}\n+\n+\t\tcxl->cxled = cxl_request_dpa(cxl->cxlmd, CXL_PARTMODE_RAM,\n+\t\t\t\t\t     EFX_CTPIO_BUFFER_SIZE);\n+\t\tif (IS_ERR(cxl->cxled)) {\n+\t\t\tpci_err(pci_dev, \"CXL accel request DPA failed\");\n+\t\t\tcxl_put_root_decoder(cxl->cxlrd);\n+\t\t\treturn PTR_ERR(cxl->cxled);\n+\t\t}\n \t}\n \n \tprobe_data->cxl = cxl;\n@@ -141,6 +149,7 @@ void efx_cxl_exit(struct efx_probe_data *probe_data)\n \t\tiounmap(probe_data->cxl->ctpio_cxl);\n \t\tcxl_unregister_region(probe_data->cxl->efx_region);\n \t} else {\n+\t\tcxl_dpa_free(probe_data->cxl->cxled);\n \t\tcxl_put_root_decoder(probe_data->cxl->cxlrd);\n \t}\n }\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "The author addressed a concern about type2 CXL devices using host-managed memory, explaining that it should not be available to other uses and adding code to skip device-dax registration for such regions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nBy definition a type2 cxl device will use the host managed memory for\nspecific functionality, therefore it should not be available to other\nuses.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Davidlohr Bueso <daves@stgolabs.net>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n---\n drivers/cxl/core/region.c | 7 +++++++\n 1 file changed, 7 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 293e63dfef22..12df717cc881 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -4441,6 +4441,13 @@ static int cxl_region_probe(struct device *dev)\n \tif (rc)\n \t\treturn rc;\n \n+\t/*\n+\t * HDM-D[B] (device-memory) regions have accelerator specific usage.\n+\t * Skip device-dax registration.\n+\t */\n+\tif (cxlr->type == CXL_DECODER_DEVMEM)\n+\t\treturn 0;\n+\n \t/*\n \t * From this point on any path that changes the region's state away from\n \t * CXL_CONFIG_COMMIT is also responsible for releasing the driver.\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "Author acknowledged that the CXL endpoint removal callback needs to be handled by disabling CXL-based PIO buffers, agreed to add this handling in a future patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nA PIO buffer is a region of device memory to which the driver can write a\npacket for TX, with the device handling the transmit doorbell without\nrequiring a DMA for getting the packet data, which helps reducing latency\nin certain exchanges. With CXL mem protocol this latency can be lowered\nfurther.\n\nWith a device supporting CXL and successfully initialised, use the cxl\nregion to map the memory range and use this mapping for PIO buffers.\n\nAdd the disabling of those CXL-based PIO buffers if the callback for\npotential cxl endpoint removal by the CXL code happens.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/net/ethernet/sfc/ef10.c       | 50 +++++++++++++++++++++++----\n drivers/net/ethernet/sfc/efx_cxl.c    | 33 ++++++++++++++----\n drivers/net/ethernet/sfc/net_driver.h |  2 ++\n drivers/net/ethernet/sfc/nic.h        |  3 ++\n 4 files changed, 75 insertions(+), 13 deletions(-)\n\ndiff --git a/drivers/net/ethernet/sfc/ef10.c b/drivers/net/ethernet/sfc/ef10.c\nindex fcec81f862ec..2bb6d3136c7c 100644\n--- a/drivers/net/ethernet/sfc/ef10.c\n+++ b/drivers/net/ethernet/sfc/ef10.c\n@@ -24,6 +24,7 @@\n #include <linux/wait.h>\n #include <linux/workqueue.h>\n #include <net/udp_tunnel.h>\n+#include \"efx_cxl.h\"\n \n /* Hardware control for EF10 architecture including 'Huntington'. */\n \n@@ -106,7 +107,7 @@ static int efx_ef10_get_vf_index(struct efx_nic *efx)\n \n static int efx_ef10_init_datapath_caps(struct efx_nic *efx)\n {\n-\tMCDI_DECLARE_BUF(outbuf, MC_CMD_GET_CAPABILITIES_V4_OUT_LEN);\n+\tMCDI_DECLARE_BUF(outbuf, MC_CMD_GET_CAPABILITIES_V7_OUT_LEN);\n \tstruct efx_ef10_nic_data *nic_data = efx->nic_data;\n \tsize_t outlen;\n \tint rc;\n@@ -177,6 +178,12 @@ static int efx_ef10_init_datapath_caps(struct efx_nic *efx)\n \t\t\t  efx->num_mac_stats);\n \t}\n \n+\tif (outlen < MC_CMD_GET_CAPABILITIES_V7_OUT_LEN)\n+\t\tnic_data->datapath_caps3 = 0;\n+\telse\n+\t\tnic_data->datapath_caps3 = MCDI_DWORD(outbuf,\n+\t\t\t\t\t\t      GET_CAPABILITIES_V7_OUT_FLAGS3);\n+\n \treturn 0;\n }\n \n@@ -919,6 +926,9 @@ static void efx_ef10_forget_old_piobufs(struct efx_nic *efx)\n static void efx_ef10_remove(struct efx_nic *efx)\n {\n \tstruct efx_ef10_nic_data *nic_data = efx->nic_data;\n+#ifdef CONFIG_SFC_CXL\n+\tstruct efx_probe_data *probe_data;\n+#endif\n \tint rc;\n \n #ifdef CONFIG_SFC_SRIOV\n@@ -949,7 +959,12 @@ static void efx_ef10_remove(struct efx_nic *efx)\n \n \tefx_mcdi_rx_free_indir_table(efx);\n \n+#ifdef CONFIG_SFC_CXL\n+\tprobe_data = container_of(efx, struct efx_probe_data, efx);\n+\tif (nic_data->wc_membase && !probe_data->cxl_pio_in_use)\n+#else\n \tif (nic_data->wc_membase)\n+#endif\n \t\tiounmap(nic_data->wc_membase);\n \n \trc = efx_mcdi_free_vis(efx);\n@@ -1140,6 +1155,9 @@ static int efx_ef10_dimension_resources(struct efx_nic *efx)\n \tunsigned int channel_vis, pio_write_vi_base, max_vis;\n \tstruct efx_ef10_nic_data *nic_data = efx->nic_data;\n \tunsigned int uc_mem_map_size, wc_mem_map_size;\n+#ifdef CONFIG_SFC_CXL\n+\tstruct efx_probe_data *probe_data;\n+#endif\n \tvoid __iomem *membase;\n \tint rc;\n \n@@ -1263,8 +1281,25 @@ static int efx_ef10_dimension_resources(struct efx_nic *efx)\n \tiounmap(efx->membase);\n \tefx->membase = membase;\n \n-\t/* Set up the WC mapping if needed */\n-\tif (wc_mem_map_size) {\n+\tif (!wc_mem_map_size)\n+\t\tgoto skip_pio;\n+\n+\t/* Set up the WC mapping */\n+\n+#ifdef CONFIG_SFC_CXL\n+\tprobe_data = container_of(efx, struct efx_probe_data, efx);\n+\tif ((nic_data->datapath_caps3 &\n+\t    (1 << MC_CMD_GET_CAPABILITIES_V7_OUT_CXL_CONFIG_ENABLE_LBN)) &&\n+\t    probe_data->cxl_pio_initialised) {\n+\t\t/* Using PIO through CXL mapping? */\n+\t\tnic_data->pio_write_base = probe_data->cxl->ctpio_cxl +\n+\t\t\t\t\t   (pio_write_vi_base * efx->vi_stride +\n+\t\t\t\t\t    ER_DZ_TX_PIOBUF - uc_mem_map_size);\n+\t\tprobe_data->cxl_pio_in_use = true;\n+\t} else\n+#endif\n+\t{\n+\t\t/* Using legacy PIO BAR mapping */\n \t\tnic_data->wc_membase = ioremap_wc(efx->membase_phys +\n \t\t\t\t\t\t  uc_mem_map_size,\n \t\t\t\t\t\t  wc_mem_map_size);\n@@ -1279,12 +1314,13 @@ static int efx_ef10_dimension_resources(struct efx_nic *efx)\n \t\t\tnic_data->wc_membase +\n \t\t\t(pio_write_vi_base * efx->vi_stride + ER_DZ_TX_PIOBUF -\n \t\t\t uc_mem_map_size);\n-\n-\t\trc = efx_ef10_link_piobufs(efx);\n-\t\tif (rc)\n-\t\t\tefx_ef10_free_piobufs(efx);\n \t}\n \n+\trc = efx_ef10_link_piobufs(efx);\n+\tif (rc)\n+\t\tefx_ef10_free_piobufs(efx);\n+\n+skip_pio:\n \tnetif_dbg(efx, probe, efx->net_dev,\n \t\t  \"memory BAR at %pa (virtual %p+%x UC, %p+%x WC)\\n\",\n \t\t  &efx->membase_phys, efx->membase, uc_mem_map_size,\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 4d5f3974e51d..c13e1f2bf7ea 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -11,6 +11,7 @@\n #include <cxl/pci.h>\n #include \"net_driver.h\"\n #include \"efx_cxl.h\"\n+#include \"efx.h\"\n \n #define EFX_CTPIO_BUFFER_SIZE\tSZ_256M\n \n@@ -138,14 +139,34 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\tcxl->efx_region = cxl_create_region(cxl->cxlrd, &cxl->cxled, 1);\n \t\tif (IS_ERR(cxl->efx_region)) {\n \t\t\tpci_err(pci_dev, \"CXL accel create region failed\");\n-\t\t\tcxl_put_root_decoder(cxl->cxlrd);\n-\t\t\tcxl_dpa_free(cxl->cxled);\n-\t\t\treturn PTR_ERR(cxl->efx_region);\n+\t\t\trc = PTR_ERR(cxl->efx_region);\n+\t\t\tgoto err_region;\n+\t\t}\n+\n+\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n+\t\tif (rc) {\n+\t\t\tpci_err(pci_dev, \"CXL getting regions params failed\");\n+\t\t\tgoto err_map;\n+\t\t}\n+\n+\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n+\t\tif (!cxl->ctpio_cxl) {\n+\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n+\t\t\trc = -ENOMEM;\n+\t\t\tgoto err_map;\n \t\t}\n \t}\n \n \tprobe_data->cxl = cxl;\n+\tprobe_data->cxl_pio_initialised = true;\n \treturn 0;\n+\n+err_map:\n+\tcxl_unregister_region(cxl->efx_region);\n+err_region:\n+\tcxl_put_root_decoder(cxl->cxlrd);\n+\tcxl_dpa_free(cxl->cxled);\n+\treturn rc;\n }\n \n void efx_cxl_exit(struct efx_probe_data *probe_data)\n@@ -153,9 +174,9 @@ void efx_cxl_exit(struct efx_probe_data *probe_data)\n \tif (!probe_data->cxl)\n \t\treturn;\n \n-\tif (probe_data->cxl->hdm_was_committed) {\n-\t\tiounmap(probe_data->cxl->ctpio_cxl);\n-\t} else {\n+\tiounmap(probe_data->cxl->ctpio_cxl);\n+\n+\tif (!probe_data->cxl->hdm_was_committed) {\n \t\tcxl_dpa_free(probe_data->cxl->cxled);\n \t\tcxl_put_root_decoder(probe_data->cxl->cxlrd);\n \t}\ndiff --git a/drivers/net/ethernet/sfc/net_driver.h b/drivers/net/ethernet/sfc/net_driver.h\nindex 3964b2c56609..bea4eecdf842 100644\n--- a/drivers/net/ethernet/sfc/net_driver.h\n+++ b/drivers/net/ethernet/sfc/net_driver.h\n@@ -1207,6 +1207,7 @@ struct efx_cxl;\n  * @efx: Efx NIC details\n  * @cxl: details of related cxl objects\n  * @cxl_pio_initialised: cxl initialization outcome.\n+ * @cxl_pio_in_use: PIO using CXL mapping\n  */\n struct efx_probe_data {\n \tstruct pci_dev *pci_dev;\n@@ -1214,6 +1215,7 @@ struct efx_probe_data {\n #ifdef CONFIG_SFC_CXL\n \tstruct efx_cxl *cxl;\n \tbool cxl_pio_initialised;\n+\tbool cxl_pio_in_use;\n #endif\n };\n \ndiff --git a/drivers/net/ethernet/sfc/nic.h b/drivers/net/ethernet/sfc/nic.h\nindex 9fa5c4c713ab..c87cc9214690 100644\n--- a/drivers/net/ethernet/sfc/nic.h\n+++ b/drivers/net/ethernet/sfc/nic.h\n@@ -152,6 +152,8 @@ enum {\n  *\t%MC_CMD_GET_CAPABILITIES response)\n  * @datapath_caps2: Further Capabilities of datapath firmware (FLAGS2 field of\n  * %MC_CMD_GET_CAPABILITIES response)\n+ * @datapath_caps3: Further Capabilities of datapath firmware (FLAGS3 field of\n+ * %MC_CMD_GET_CAPABILITIES response)\n  * @rx_dpcpu_fw_id: Firmware ID of the RxDPCPU\n  * @tx_dpcpu_fw_id: Firmware ID of the TxDPCPU\n  * @must_probe_vswitching: Flag: vswitching has yet to be setup after MC reboot\n@@ -186,6 +188,7 @@ struct efx_ef10_nic_data {\n \tbool must_check_datapath_caps;\n \tu32 datapath_caps;\n \tu32 datapath_caps2;\n+\tu32 datapath_caps3;\n \tunsigned int rx_dpcpu_fw_id;\n \tunsigned int tx_dpcpu_fw_id;\n \tbool must_probe_vswitching;\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "alejandro.lucero-palau (author)",
              "summary": "Author acknowledged that type2 support should allow accelerator drivers to create CXL regions from kernel code, explained how this would be achieved by adding functionality and integrating it with current memory expander support.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCreating a CXL region requires userspace intervention through the cxl\nsysfs files. Type2 support should allow accelerator drivers to create\nsuch cxl region from kernel code.\n\nAdding that functionality and integrating it with current support for\nmemory expanders.\n\nBased on https://lore.kernel.org/linux-cxl/168592159835.1948938.1647215579839222774.stgit@dwillia2-xfh.jf.intel.com/\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/cxl/core/region.c | 131 ++++++++++++++++++++++++++++++++++++--\n include/cxl/cxl.h         |   3 +\n 2 files changed, 127 insertions(+), 7 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 63c2aeb2ee1f..293e63dfef22 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2944,6 +2944,14 @@ cxl_find_region_by_name(struct cxl_root_decoder *cxlrd, const char *name)\n \treturn to_cxl_region(region_dev);\n }\n \n+static void drop_region(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n+\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n+\n+\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n+}\n+\n static ssize_t delete_region_store(struct device *dev,\n \t\t\t\t   struct device_attribute *attr,\n \t\t\t\t   const char *buf, size_t len)\n@@ -4047,14 +4055,12 @@ static int __construct_region(struct cxl_region *cxlr,\n \treturn 0;\n }\n \n-/* Establish an empty region covering the given HPA range */\n-static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n-\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n+static struct cxl_region *construct_region_begin(struct cxl_root_decoder *cxlrd,\n+\t\t\t\t\t\t struct cxl_endpoint_decoder *cxled)\n {\n \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n-\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n \tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n-\tint rc, part = READ_ONCE(cxled->part);\n+\tint part = READ_ONCE(cxled->part);\n \tstruct cxl_region *cxlr;\n \n \tdo {\n@@ -4063,13 +4069,26 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n \t\t\t\t       cxled->cxld.target_type);\n \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n \n-\tif (IS_ERR(cxlr)) {\n+\tif (IS_ERR(cxlr))\n \t\tdev_err(cxlmd->dev.parent,\n \t\t\t\"%s:%s: %s failed assign region: %ld\\n\",\n \t\t\tdev_name(&cxlmd->dev), dev_name(&cxled->cxld.dev),\n \t\t\t__func__, PTR_ERR(cxlr));\n+\n+\treturn cxlr;\n+}\n+\n+/* Establish an empty region covering the given HPA range */\n+static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n+\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n+{\n+\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n+\tstruct cxl_region *cxlr;\n+\tint rc;\n+\n+\tcxlr = construct_region_begin(cxlrd, cxled);\n+\tif (IS_ERR(cxlr))\n \t\treturn cxlr;\n-\t}\n \n \trc = __construct_region(cxlr, cxlrd, cxled);\n \tif (rc) {\n@@ -4080,6 +4099,104 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n \treturn cxlr;\n }\n \n+DEFINE_FREE(cxl_region_drop, struct cxl_region *, if (_T) drop_region(_T))\n+\n+static struct cxl_region *\n+__construct_new_region(struct cxl_root_decoder *cxlrd,\n+\t\t       struct cxl_endpoint_decoder **cxled, int ways)\n+{\n+\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled[0]);\n+\tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n+\tstruct cxl_region_params *p;\n+\tresource_size_t size = 0;\n+\tint rc, i;\n+\n+\tstruct cxl_region *cxlr __free(cxl_region_drop) =\n+\t\tconstruct_region_begin(cxlrd, cxled[0]);\n+\tif (IS_ERR(cxlr))\n+\t\treturn cxlr;\n+\n+\tguard(rwsem_write)(&cxl_rwsem.region);\n+\n+\t/*\n+\t * Sanity check. This should not happen with an accel driver handling\n+\t * the region creation.\n+\t */\n+\tp = &cxlr->params;\n+\tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE) {\n+\t\tdev_err(cxlmd->dev.parent,\n+\t\t\t\"%s:%s: %s  unexpected region state\\n\",\n+\t\t\tdev_name(&cxlmd->dev), dev_name(&cxled[0]->cxld.dev),\n+\t\t\t__func__);\n+\t\treturn ERR_PTR(-EBUSY);\n+\t}\n+\n+\trc = set_interleave_ways(cxlr, ways);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\trc = set_interleave_granularity(cxlr, cxld->interleave_granularity);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\tscoped_guard(rwsem_read, &cxl_rwsem.dpa) {\n+\t\tfor (i = 0; i < ways; i++) {\n+\t\t\tif (!cxled[i]->dpa_res)\n+\t\t\t\treturn ERR_PTR(-EINVAL);\n+\t\t\tsize += resource_size(cxled[i]->dpa_res);\n+\t\t}\n+\n+\t\trc = alloc_hpa(cxlr, size);\n+\t\tif (rc)\n+\t\t\treturn ERR_PTR(rc);\n+\n+\t\tfor (i = 0; i < ways; i++) {\n+\t\t\trc = cxl_region_attach(cxlr, cxled[i], 0);\n+\t\t\tif (rc)\n+\t\t\t\treturn ERR_PTR(rc);\n+\t\t}\n+\t}\n+\n+\trc = cxl_region_decode_commit(cxlr);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\tp->state = CXL_CONFIG_COMMIT;\n+\n+\treturn no_free_ptr(cxlr);\n+}\n+\n+/**\n+ * cxl_create_region - Establish a region given an endpoint decoder\n+ * @cxlrd: root decoder to allocate HPA\n+ * @cxled: endpoint decoders with reserved DPA capacity\n+ * @ways: interleave ways required\n+ *\n+ * Returns a fully formed region in the commit state and attached to the\n+ * cxl_region driver.\n+ */\n+struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n+\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n+\t\t\t\t     int ways)\n+{\n+\tstruct cxl_region *cxlr;\n+\n+\tmutex_lock(&cxlrd->range_lock);\n+\tcxlr = __construct_new_region(cxlrd, cxled, ways);\n+\tmutex_unlock(&cxlrd->range_lock);\n+\tif (IS_ERR(cxlr))\n+\t\treturn cxlr;\n+\n+\tif (device_attach(&cxlr->dev) <= 0) {\n+\t\tdev_err(&cxlr->dev, \"failed to create region\\n\");\n+\t\tdrop_region(cxlr);\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\treturn cxlr;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_create_region, \"CXL\");\n+\n static struct cxl_region *\n cxl_find_region_by_range(struct cxl_root_decoder *cxlrd, struct range *hpa)\n {\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 4802371db00e..50acbd13bcf8 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -281,4 +281,7 @@ struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n \t\t\t\t\t     enum cxl_partition_mode mode,\n \t\t\t\t\t     resource_size_t alloc);\n int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n+struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n+\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n+\t\t\t\t     int ways);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-02-01",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer Cheatham suggested moving the function call to be the first thing in the function, which would avoid acquiring a lock in cxl_region_can_probe() above.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "minor optimization"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Minor nit: Should probably move this to be the first thing in the function. It would save\nhaving to acquire a lock in cxl_region_can_probe() above. Keep my reviewed-by either way,\nit's really just a minor optimization.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer Cheatham questioned the complexity of the code and suggested simplification by removing the outer loop in cxl_get_hpa_freespace() since ctx->host_bridges is only set to one host bridge at that point, and also proposed changing ctx->host_bridges to a struct device * const.\n\nThe reviewer noted that interleave_ways is hardcoded to 1 and suggested removing this portion of the function or adding a doc comment explaining its current unused state.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "complexity",
                "simplification",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\nwhat the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\nbridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\npoint, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\nbe a struct device * const.\n\nMaybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\nI'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n\n---\n\nMentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\nsomething). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\nI'm not sure of what the fix is to get the intended behavior.\n\nIt may be worth getting rid of the interleave_ways portion of this function and\nadd it later when someone needs it. You could also explain it's hard coded to 1/unused\nin the doc comment if you know of an immediate need for it.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> By definition a type2 cxl device will use the host managed memory for\n> specific functionality, therefore it should not be available to other\n> uses.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> ---\n>  drivers/cxl/core/region.c | 7 +++++++\n>  1 file changed, 7 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 293e63dfef22..12df717cc881 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -4441,6 +4441,13 @@ static int cxl_region_probe(struct device *dev)\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> +\t/*\n> +\t * HDM-D[B] (device-memory) regions have accelerator specific usage.\n> +\t * Skip device-dax registration.\n> +\t */\n> +\tif (cxlr->type == CXL_DECODER_DEVMEM)\n> +\t\treturn 0;\n\nMinor nit: Should probably move this to be the first thing in the function. It would save\nhaving to acquire a lock in cxl_region_can_probe() above. Keep my reviewed-by either way,\nit's really just a minor optimization.\n> +\n>  \t/*\n>  \t * From this point on any path that changes the region's state away from\n>  \t * CXL_CONFIG_COMMIT is also responsible for releasing the driver.\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> CXL region creation involves allocating capacity from Device Physical\n> Address (DPA) and assigning it to decode a given Host Physical Address\n> (HPA). Before determining how much DPA to allocate the amount of available\n> HPA must be determined. Also, not all HPA is created equal, some HPA\n> targets RAM, some targets PMEM, some is prepared for device-memory flows\n> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n> \n> In order to support Type2 CXL devices, wrap all of those concerns into\n> an API that retrieves a root decoder (platform CXL window) that fits the\n> specified constraints and the capacity available for a new region.\n> \n> Add a complementary function for releasing the reference to such root\n> decoder.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> ---\n>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h         |   3 +\n>  include/cxl/cxl.h         |   6 ++\n>  3 files changed, 173 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 954b8fcdbac6..bdefd088f5f1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>  \treturn 0;\n>  }\n>  \n> +struct cxlrd_max_context {\n> +\tstruct device * const *host_bridges;\n> +\tint interleave_ways;\n> +\tunsigned long flags;\n> +\tresource_size_t max_hpa;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +};\n> +\n> +static int find_max_hpa(struct device *dev, void *data)\n> +{\n> +\tstruct cxlrd_max_context *ctx = data;\n> +\tstruct cxl_switch_decoder *cxlsd;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +\tstruct resource *res, *prev;\n> +\tstruct cxl_decoder *cxld;\n> +\tresource_size_t free = 0;\n> +\tresource_size_t max;\n> +\tint found = 0;\n> +\n> +\tif (!is_root_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxlrd = to_cxl_root_decoder(dev);\n> +\tcxlsd = &cxlrd->cxlsd;\n> +\tcxld = &cxlsd->cxld;\n> +\n> +\tif ((cxld->flags & ctx->flags) != ctx->flags) {\n> +\t\tdev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n> +\t\t\tcxld->flags, ctx->flags);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\tfor (int i = 0; i < ctx->interleave_ways; i++) {\n> +\t\tfor (int j = 0; j < ctx->interleave_ways; j++) {\n> +\t\t\tif (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n> +\t\t\t\tfound++;\n> +\t\t\t\tbreak;\n> +\t\t\t}\n> +\t\t}\n> +\t}\n\nThis may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\nwhat the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\nbridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\npoint, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\nbe a struct device * const.\n\nMaybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\nI'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n> +\n> +\tif (found != ctx->interleave_ways) {\n> +\t\tdev_dbg(dev,\n> +\t\t\t\"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n> +\t\t\tfound, ctx->interleave_ways);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\t/*\n> +\t * Walk the root decoder resource range relying on cxl_rwsem.region to\n> +\t * preclude sibling arrival/departure and find the largest free space\n> +\t * gap.\n> +\t */\n> +\tlockdep_assert_held_read(&cxl_rwsem.region);\n> +\tres = cxlrd->res->child;\n> +\n> +\t/* With no resource child the whole parent resource is available */\n> +\tif (!res)\n> +\t\tmax = resource_size(cxlrd->res);\n> +\telse\n> +\t\tmax = 0;\n> +\n> +\tfor (prev = NULL; res; prev = res, res = res->sibling) {\n> +\t\tif (!prev && res->start == cxlrd->res->start &&\n> +\t\t    res->end == cxlrd->res->end) {\n> +\t\t\tmax = resource_size(cxlrd->res);\n> +\t\t\tbreak;\n> +\t\t}\n> +\t\t/*\n> +\t\t * Sanity check for preventing arithmetic problems below as a\n> +\t\t * resource with size 0 could imply using the end field below\n> +\t\t * when set to unsigned zero - 1 or all f in hex.\n> +\t\t */\n> +\t\tif (prev && !resource_size(prev))\n> +\t\t\tcontinue;\n> +\n> +\t\tif (!prev && res->start > cxlrd->res->start) {\n> +\t\t\tfree = res->start - cxlrd->res->start;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t\tif (prev && res->start > prev->end + 1) {\n> +\t\t\tfree = res->start - prev->end + 1;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t}\n> +\n> +\tif (prev && prev->end + 1 < cxlrd->res->end + 1) {\n> +\t\tfree = cxlrd->res->end + 1 - prev->end + 1;\n> +\t\tmax = max(free, max);\n> +\t}\n> +\n> +\tdev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n> +\tif (max > ctx->max_hpa) {\n> +\t\tif (ctx->cxlrd)\n> +\t\t\tput_device(cxlrd_dev(ctx->cxlrd));\n> +\t\tget_device(cxlrd_dev(cxlrd));\n> +\t\tctx->cxlrd = cxlrd;\n> +\t\tctx->max_hpa = max;\n> +\t}\n> +\treturn 0;\n> +}\n> +\n> +/**\n> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n> + * @cxlmd: the mem device requiring the HPA\n> + * @interleave_ways: number of entries in @host_bridges\n> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n> + *\t\t      returned decoder\n> + *\n> + * Returns a pointer to a struct cxl_root_decoder\n> + *\n> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n> + * caller goes to use this decoder and its capacity is reduced then caller needs\n> + * to loop and retry.\n> + *\n> + * The returned root decoder has an elevated reference count that needs to be\n> + * put with cxl_put_root_decoder(cxlrd).\n> + */\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max_avail_contig)\n> +{\n> +\tstruct cxlrd_max_context ctx = {\n> +\t\t.flags = flags,\n> +\t\t.interleave_ways = interleave_ways,\n> +\t};\n> +\tstruct cxl_port *root_port;\n> +\tstruct cxl_port *endpoint;\n> +\n> +\tendpoint = cxlmd->endpoint;\n> +\tif (!endpoint) {\n> +\t\tdev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\tctx.host_bridges = &endpoint->host_bridge;\n\nMentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\nsomething). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\nI'm not sure of what the fix is to get the intended behavior.\n\nIt may be worth getting rid of the interleave_ways portion of this function and\nadd it later when someone needs it. You could also explain it's hard coded to 1/unused\nin the doc comment if you know of an immediate need for it.\n\n> +\n> +\tstruct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n> +\tif (!root) {\n> +\t\tdev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\troot_port = &root->port;\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.region)\n> +\t\tdevice_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n\nCan just use a guard() here.\n\n> +\n> +\tif (!ctx.cxlrd)\n> +\t\treturn ERR_PTR(-ENOMEM);\n> +\n> +\t*max_avail_contig = ctx.max_hpa;\n> +\treturn ctx.cxlrd;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n> +\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n> +{\n> +\tput_device(cxlrd_dev(cxlrd));\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n> +\n>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>  \t\t\t  const char *buf, size_t len)\n>  {\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index 944c5d1ccceb..c7d9b2c2908f 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>  bool is_root_decoder(struct device *dev);\n> +\n> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n> +\n>  bool is_switch_decoder(struct device *dev);\n>  bool is_endpoint_decoder(struct device *dev);\n>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 92880c26b2d5..834dc7e78934 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>  struct range;\n>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>  void cxl_unregister_region(struct cxl_region *cxlr);\n> +struct cxl_port;\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max);\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Check if device HDM is already committed during firmware/BIOS\n> initialization.\n> \n> A CXL region should exist if so after memdev allocation/initialization.\n> Get HPA from region and map it.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/net/ethernet/sfc/efx_cxl.c | 28 +++++++++++++++++++++++++++-\n>  1 file changed, 27 insertions(+), 1 deletion(-)\n> \n> diff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\n> index a77ef4783fcb..3536eccf1b2a 100644\n> --- a/drivers/net/ethernet/sfc/efx_cxl.c\n> +++ b/drivers/net/ethernet/sfc/efx_cxl.c\n> @@ -19,6 +19,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \tstruct efx_nic *efx = &probe_data->efx;\n>  \tstruct pci_dev *pci_dev = efx->pci_dev;\n>  \tstruct efx_cxl *cxl;\n> +\tstruct range range;\n>  \tu16 dvsec;\n>  \tint rc;\n>  \n> @@ -90,13 +91,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \t\treturn PTR_ERR(cxl->cxlmd);\n>  \t}\n>  \n> -\tprobe_data->cxl = cxl;\n> +\tcxl->cxled = cxl_get_committed_decoder(cxl->cxlmd, &cxl->efx_region);\n> +\tif (cxl->cxled) {\n> +\t\tif (!cxl->efx_region) {\n> +\t\t\tpci_err(pci_dev, \"CXL found committed decoder without a region\");\n> +\t\t\treturn -ENODEV;\n> +\t\t}\n> +\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n\nMissing an empty line above.\n\n> +\t\tif (rc) {\n> +\t\t\tpci_err(pci_dev,\n> +\t\t\t\t\"CXL getting regions params from a committed decoder failed\");\n> +\t\t\treturn rc;\n> +\t\t}\n> +\n> +\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n\nMaybe use range_len() instead for the second parameter?\n\n> +\t\tif (!cxl->ctpio_cxl) {\n> +\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n> +\t\t\treturn -ENOMEM;\n> +\t\t}\n> +\n> +\t\tprobe_data->cxl = cxl;\n> +\t}\n>  \n>  \treturn 0;\n>  }\n>  \n>  void efx_cxl_exit(struct efx_probe_data *probe_data)\n>  {\n> +\tif (!probe_data->cxl)\n> +\t\treturn;\n> +\n> +\tiounmap(probe_data->cxl->ctpio_cxl);\n> +\tcxl_unregister_region(probe_data->cxl->efx_region);\n>  }\n>  \n>  MODULE_IMPORT_NS(\"CXL\");\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation involves finding available DPA (device-physical-address)\n> capacity to map into HPA (host-physical-address) space.\n> \n> In order to support CXL Type2 devices, define an API, cxl_request_dpa(),\n> that tries to allocate the DPA memory the driver requires to operate.The\n> memory requested should not be bigger than the max available HPA obtained\n> previously with cxl_get_hpa_freespace().\n> \n> Based on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h      |  1 +\n>  include/cxl/cxl.h      |  5 +++\n>  3 files changed, 90 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index a172ce4e9b19..d60a697f12cc 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -3,6 +3,7 @@\n>  #include <linux/seq_file.h>\n>  #include <linux/device.h>\n>  #include <linux/delay.h>\n> +#include <cxl/cxl.h>\n>  \n>  #include \"cxlmem.h\"\n>  #include \"core.h\"\n> @@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n>  \treturn resource_contains(res, &_addr);\n>  }\n>  \n> +/**\n> + * cxl_dpa_free - release DPA (Device Physical Address)\n> + * @cxled: endpoint decoder linked to the DPA\n> + *\n> + * Returns 0 or error.\n> + */\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_port *port = cxled_to_port(cxled);\n> @@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  \tdevm_cxl_dpa_release(cxled);\n>  \treturn 0;\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n>  \n>  int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \t\t     enum cxl_partition_mode mode)\n> @@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \treturn 0;\n>  }\n>  \n> +static int find_free_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == (port->hdm_end + 1);\n> +}\n> +\n> +static struct cxl_endpoint_decoder *\n> +cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct device *dev;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tdev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\tfind_free_decoder);\n> +\tif (!dev)\n> +\t\treturn NULL;\n> +\n> +\treturn to_cxl_endpoint_decoder(dev);\n> +}\n> +\n> +/**\n> + * cxl_request_dpa - search and reserve DPA given input constraints\n> + * @cxlmd: memdev with an endpoint port with available decoders\n> + * @mode: CXL partition mode (ram vs pmem)\n> + * @alloc: dpa size required\n> + *\n> + * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n> + * an errno encoded pointer on failure.\n> + *\n> + * Given that a region needs to allocate from limited HPA capacity it\n> + * may be the case that a device has more mappable DPA capacity than\n> + * available HPA. The expectation is that @alloc is a driver known\n> + * value based on the device capacity but which could not be fully\n> + * available due to HPA constraints.\n> + *\n> + * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n> + * reserved, or an error pointer. The caller is also expected to own the\n> + * lifetime of the memdev registration associated with the endpoint to\n> + * pin the decoder registered as well.\n> + */\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc)\n> +{\n> +\tint rc;\n> +\n> +\tif (!IS_ALIGNED(alloc, SZ_256M))\n> +\t\treturn ERR_PTR(-EINVAL);\n> +\n> +\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n> +\t\tcxl_find_free_decoder(cxlmd);\n> +\n> +\tif (!cxled)\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\n> +\trc = cxl_dpa_set_part(cxled, mode);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = cxl_dpa_alloc(cxled, alloc);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n\nShould cxl_dpa_set_part() be unwound here, or does it not matter? If it doesn't matter:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\n> +\treturn no_free_ptr(cxled);\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n> +\n>  static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index d1b010e5e1d0..2b1f7d687a0e 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n>  \n>  DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n>  DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n> +DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n>  DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n>  DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 783ad570a6eb..4802371db00e 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -7,6 +7,7 @@\n>  \n>  #include <linux/node.h>\n>  #include <linux/ioport.h>\n> +#include <linux/range.h>\n>  #include <cxl/mailbox.h>\n>  \n>  /**\n> @@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t       unsigned long flags,\n>  \t\t\t\t\t       resource_size_t *max);\n>  void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc);\n> +int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Current code is expecting Type3 or CXL_DECODER_HOSTONLYMEM devices only.\n> Support for Type2 implies region type needs to be based on the endpoint\n> type HDM-D[B] instead.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> Reviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n> ---\n>  drivers/cxl/core/region.c | 10 ++++++----\n>  1 file changed, 6 insertions(+), 4 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index bdefd088f5f1..f53b2e9fd9e6 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2833,7 +2833,8 @@ static ssize_t create_ram_region_show(struct device *dev,\n>  }\n>  \n>  static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n> -\t\t\t\t\t  enum cxl_partition_mode mode, int id)\n> +\t\t\t\t\t  enum cxl_partition_mode mode, int id,\n> +\t\t\t\t\t  enum cxl_decoder_type target_type)\n>  {\n>  \tint rc;\n>  \n> @@ -2855,7 +2856,7 @@ static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n>  \t\treturn ERR_PTR(-EBUSY);\n>  \t}\n>  \n> -\treturn devm_cxl_add_region(cxlrd, id, mode, CXL_DECODER_HOSTONLYMEM);\n> +\treturn devm_cxl_add_region(cxlrd, id, mode, target_type);\n>  }\n>  \n>  static ssize_t create_region_store(struct device *dev, const char *buf,\n> @@ -2869,7 +2870,7 @@ static ssize_t create_region_store(struct device *dev, const char *buf,\n>  \tif (rc != 1)\n>  \t\treturn -EINVAL;\n>  \n> -\tcxlr = __create_region(cxlrd, mode, id);\n> +\tcxlr = __create_region(cxlrd, mode, id, CXL_DECODER_HOSTONLYMEM);\n\nI haven't read the ABI docs, but would it be worthwhile to update the documentation for this attribute\nto mention it only makes type 3 regions? I'm flip-flopping on whether it's worth the trouble but thought\nI should mention it.\n\nEither way:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n\n>  \tif (IS_ERR(cxlr))\n>  \t\treturn PTR_ERR(cxlr);\n>  \n> @@ -4036,7 +4037,8 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \n>  \tdo {\n>  \t\tcxlr = __create_region(cxlrd, cxlds->part[part].mode,\n> -\t\t\t\t       atomic_read(&cxlrd->region_id));\n> +\t\t\t\t       atomic_read(&cxlrd->region_id),\n> +\t\t\t\t       cxled->cxld.target_type);\n>  \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n>  \n>  \tif (IS_ERR(cxlr)) {\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation based on Type3 devices is triggered from user space\n> allowing memory combination through interleaving.\n> \n> In preparation for kernel driven region creation, that is Type2 drivers\n> triggering region creation backed with its advertised CXL memory, factor\n> out a common helper from the user-sysfs region setup for interleave ways.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n>  1 file changed, 27 insertions(+), 16 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index f53b2e9fd9e6..ece1d3df7cf1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n>  \n>  static const struct attribute_group *get_cxl_region_target_group(void);\n>  \n> -static ssize_t interleave_ways_store(struct device *dev,\n> -\t\t\t\t     struct device_attribute *attr,\n> -\t\t\t\t     const char *buf, size_t len)\n> +static int set_interleave_ways(struct cxl_region *cxlr, int val)\n\n@val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\nfunction was originally coded with that in mind (same with @save below). With that cleaned up:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n\n>  {\n> -\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>  \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> -\tstruct cxl_region *cxlr = to_cxl_region(dev);\n>  \tstruct cxl_region_params *p = &cxlr->params;\n> -\tunsigned int val, save;\n> -\tint rc;\n> +\tint save, rc;\n>  \tu8 iw;\n>  \n> -\trc = kstrtouint(buf, 0, &val);\n> -\tif (rc)\n> -\t\treturn rc;\n> -\n>  \trc = ways_to_eiw(val, &iw);\n>  \tif (rc)\n>  \t\treturn rc;\n> @@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n>  \t\treturn -EINVAL;\n>  \t}\n>  \n> -\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> -\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> -\t\treturn rc;\n> +\tlockdep_assert_held_write(&cxl_rwsem.region);\n>  \n>  \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>  \t\treturn -EBUSY;\n> @@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n>  \tsave = p->interleave_ways;\n>  \tp->interleave_ways = val;\n>  \trc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n> -\tif (rc) {\n> +\tif (rc)\n>  \t\tp->interleave_ways = save;\n> +\n> +\treturn rc;\n> +}\n> +\n> +static ssize_t interleave_ways_store(struct device *dev,\n> +\t\t\t\t     struct device_attribute *attr,\n> +\t\t\t\t     const char *buf, size_t len)\n> +{\n> +\tstruct cxl_region *cxlr = to_cxl_region(dev);\n> +\tunsigned int val;\n> +\tint rc;\n> +\n> +\trc = kstrtouint(buf, 0, &val);\n> +\tif (rc)\n> +\t\treturn rc;\n> +\n> +\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> +\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> +\t\treturn rc;\n> +\n> +\trc = set_interleave_ways(cxlr, val);\n> +\tif (rc)\n>  \t\treturn rc;\n> -\t}\n>  \n>  \treturn len;\n>  }\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation based on Type3 devices is triggered from user space\n> allowing memory combination through interleaving.\n> \n> In preparation for kernel driven region creation, that is Type2 drivers\n> triggering region creation backed with its advertised CXL memory, factor\n> out a common helper from the user-sysfs region setup forinterleave\n> granularity.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 39 +++++++++++++++++++++++++--------------\n>  1 file changed, 25 insertions(+), 14 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index ece1d3df7cf1..63c2aeb2ee1f 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -559,21 +559,14 @@ static ssize_t interleave_granularity_show(struct device *dev,\n>  \treturn sysfs_emit(buf, \"%d\\n\", p->interleave_granularity);\n>  }\n>  \n> -static ssize_t interleave_granularity_store(struct device *dev,\n> -\t\t\t\t\t    struct device_attribute *attr,\n> -\t\t\t\t\t    const char *buf, size_t len)\n> +static int set_interleave_granularity(struct cxl_region *cxlr, int val)\n\nSame thing as last patch. Assuming it's fixed:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>  {\n> -\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>  \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> -\tstruct cxl_region *cxlr = to_cxl_region(dev);\n>  \tstruct cxl_region_params *p = &cxlr->params;\n> -\tint rc, val;\n> +\tint rc;\n>  \tu16 ig;\n>  \n> -\trc = kstrtoint(buf, 0, &val);\n> -\tif (rc)\n> -\t\treturn rc;\n> -\n>  \trc = granularity_to_eig(val, &ig);\n>  \tif (rc)\n>  \t\treturn rc;\n> @@ -589,14 +582,32 @@ static ssize_t interleave_granularity_store(struct device *dev,\n>  \tif (cxld->interleave_ways > 1 && val != cxld->interleave_granularity)\n>  \t\treturn -EINVAL;\n>  \n> -\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> -\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> -\t\treturn rc;\n> -\n> +\tlockdep_assert_held_write(&cxl_rwsem.region);\n>  \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>  \t\treturn -EBUSY;\n>  \n>  \tp->interleave_granularity = val;\n> +\treturn 0;\n> +}\n> +\n> +static ssize_t interleave_granularity_store(struct device *dev,\n> +\t\t\t\t\t    struct device_attribute *attr,\n> +\t\t\t\t\t    const char *buf, size_t len)\n> +{\n> +\tstruct cxl_region *cxlr = to_cxl_region(dev);\n> +\tint rc, val;\n> +\n> +\trc = kstrtoint(buf, 0, &val);\n> +\tif (rc)\n> +\t\treturn rc;\n> +\n> +\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> +\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> +\t\treturn rc;\n> +\n> +\trc = set_interleave_granularity(cxlr, val);\n> +\tif (rc)\n> +\t\treturn rc;\n>  \n>  \treturn len;\n>  }\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Creating a CXL region requires userspace intervention through the cxl\n> sysfs files. Type2 support should allow accelerator drivers to create\n> such cxl region from kernel code.\n> \n> Adding that functionality and integrating it with current support for\n> memory expanders.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159835.1948938.1647215579839222774.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 131 ++++++++++++++++++++++++++++++++++++--\n>  include/cxl/cxl.h         |   3 +\n>  2 files changed, 127 insertions(+), 7 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 63c2aeb2ee1f..293e63dfef22 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2944,6 +2944,14 @@ cxl_find_region_by_name(struct cxl_root_decoder *cxlrd, const char *name)\n>  \treturn to_cxl_region(region_dev);\n>  }\n>  \n> +static void drop_region(struct cxl_region *cxlr)\n> +{\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n> +\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n> +\n> +\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n> +}\n> +\n>  static ssize_t delete_region_store(struct device *dev,\n>  \t\t\t\t   struct device_attribute *attr,\n>  \t\t\t\t   const char *buf, size_t len)\n> @@ -4047,14 +4055,12 @@ static int __construct_region(struct cxl_region *cxlr,\n>  \treturn 0;\n>  }\n>  \n> -/* Establish an empty region covering the given HPA range */\n> -static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n> -\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n> +static struct cxl_region *construct_region_begin(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t\t\t struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> -\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n>  \tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n> -\tint rc, part = READ_ONCE(cxled->part);\n> +\tint part = READ_ONCE(cxled->part);\n>  \tstruct cxl_region *cxlr;\n>  \n>  \tdo {\n> @@ -4063,13 +4069,26 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \t\t\t\t       cxled->cxld.target_type);\n>  \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n>  \n> -\tif (IS_ERR(cxlr)) {\n> +\tif (IS_ERR(cxlr))\n>  \t\tdev_err(cxlmd->dev.parent,\n>  \t\t\t\"%s:%s: %s failed assign region: %ld\\n\",\n>  \t\t\tdev_name(&cxlmd->dev), dev_name(&cxled->cxld.dev),\n>  \t\t\t__func__, PTR_ERR(cxlr));\n> +\n> +\treturn cxlr;\n> +}\n> +\n> +/* Establish an empty region covering the given HPA range */\n> +static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n> +{\n> +\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n> +\tstruct cxl_region *cxlr;\n> +\tint rc;\n> +\n> +\tcxlr = construct_region_begin(cxlrd, cxled);\n> +\tif (IS_ERR(cxlr))\n>  \t\treturn cxlr;\n> -\t}\n>  \n>  \trc = __construct_region(cxlr, cxlrd, cxled);\n>  \tif (rc) {\n> @@ -4080,6 +4099,104 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \treturn cxlr;\n>  }\n>  \n> +DEFINE_FREE(cxl_region_drop, struct cxl_region *, if (_T) drop_region(_T))\n\nThis needs to be \"if (!IS_ERR_OR_NULL(_T) drop_region(_T)\". If construct_region_begin() returns an\nerror pointer, drop_region() will be called with it as of now leading to a garbage pointer deref.\n\n> +\n> +static struct cxl_region *\n> +__construct_new_region(struct cxl_root_decoder *cxlrd,\n> +\t\t       struct cxl_endpoint_decoder **cxled, int ways)\n> +{\n> +\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled[0]);\n> +\tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> +\tstruct cxl_region_params *p;\n> +\tresource_size_t size = 0;\n> +\tint rc, i;\n> +\n> +\tstruct cxl_region *cxlr __free(cxl_region_drop) =\n> +\t\tconstruct_region_begin(cxlrd, cxled[0]);\n> +\tif (IS_ERR(cxlr))\n> +\t\treturn cxlr;\n> +\n> +\tguard(rwsem_write)(&cxl_rwsem.region);\n> +\n> +\t/*\n> +\t * Sanity check. This should not happen with an accel driver handling\n> +\t * the region creation.\n> +\t */\n> +\tp = &cxlr->params;\n> +\tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE) {\n> +\t\tdev_err(cxlmd->dev.parent,\n> +\t\t\t\"%s:%s: %s  unexpected region state\\n\",\n> +\t\t\tdev_name(&cxlmd->dev), dev_name(&cxled[0]->cxld.dev),\n> +\t\t\t__func__);\n> +\t\treturn ERR_PTR(-EBUSY);\n> +\t}\n> +\n> +\trc = set_interleave_ways(cxlr, ways);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = set_interleave_granularity(cxlr, cxld->interleave_granularity);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.dpa) {\n> +\t\tfor (i = 0; i < ways; i++) {\n> +\t\t\tif (!cxled[i]->dpa_res)\n> +\t\t\t\treturn ERR_PTR(-EINVAL);\n> +\t\t\tsize += resource_size(cxled[i]->dpa_res);\n> +\t\t}\n> +\n> +\t\trc = alloc_hpa(cxlr, size);\n> +\t\tif (rc)\n> +\t\t\treturn ERR_PTR(rc);\n> +\n> +\t\tfor (i = 0; i < ways; i++) {\n> +\t\t\trc = cxl_region_attach(cxlr, cxled[i], 0);\n\nPosition parameter is hardcoded to 0. It should be set to i, right? This kind of goes back to my\nissues in patch 12/22; the interleaving functionality is there but it looks unused.\n\n> +\t\t\tif (rc)\n> +\t\t\t\treturn ERR_PTR(rc);\n> +\t\t}\n> +\t}\n> +\n> +\trc = cxl_region_decode_commit(cxlr);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\tp->state = CXL_CONFIG_COMMIT;\n> +\n> +\treturn no_free_ptr(cxlr);\n> +}\n> +\n> +/**\n> + * cxl_create_region - Establish a region given an endpoint decoder\n> + * @cxlrd: root decoder to allocate HPA\n> + * @cxled: endpoint decoders with reserved DPA capacity\n> + * @ways: interleave ways required\n> + *\n> + * Returns a fully formed region in the commit state and attached to the\n> + * cxl_region driver.\n> + */\n> +struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n> +\t\t\t\t     int ways)\n> +{\n> +\tstruct cxl_region *cxlr;\n> +\n> +\tmutex_lock(&cxlrd->range_lock);\n> +\tcxlr = __construct_new_region(cxlrd, cxled, ways);\n> +\tmutex_unlock(&cxlrd->range_lock);\n> +\tif (IS_ERR(cxlr))\n> +\t\treturn cxlr;\n> +\n> +\tif (device_attach(&cxlr->dev) <= 0) {\n> +\t\tdev_err(&cxlr->dev, \"failed to create region\\n\");\n> +\t\tdrop_region(cxlr);\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\t}\n> +\n> +\treturn cxlr;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_create_region, \"CXL\");\n> +\n>  static struct cxl_region *\n>  cxl_find_region_by_range(struct cxl_root_decoder *cxlrd, struct range *hpa)\n>  {\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 4802371db00e..50acbd13bcf8 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -281,4 +281,7 @@ struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t     enum cxl_partition_mode mode,\n>  \t\t\t\t\t     resource_size_t alloc);\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n> +struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n> +\t\t\t\t     int ways);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Differentiate CXL memory expanders (type 3) from CXL device accelerators\n> (type 2) with a new function for initializing cxl_dev_state and a macro\n> for helping accel drivers to embed cxl_dev_state inside a private\n> struct.\n> \n> Move structs to include/cxl as the size of the accel driver private\n> struct embedding cxl_dev_state needs to know the size of this struct.\n> \n> Use same new initialization with the type3 pci driver.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> ---\n>  drivers/cxl/core/mbox.c      |  12 +-\n>  drivers/cxl/core/memdev.c    |  32 +++++\n>  drivers/cxl/cxl.h            |  97 +--------------\n>  drivers/cxl/cxlmem.h         |  86 +------------\n>  drivers/cxl/pci.c            |  14 +--\n>  include/cxl/cxl.h            | 226 +++++++++++++++++++++++++++++++++++\n>  tools/testing/cxl/test/mem.c |   3 +-\n>  7 files changed, 274 insertions(+), 196 deletions(-)\n>  create mode 100644 include/cxl/cxl.h\n> \n> diff --git a/drivers/cxl/core/mbox.c b/drivers/cxl/core/mbox.c\n> index fa6dd0c94656..bee84d0101d1 100644\n> --- a/drivers/cxl/core/mbox.c\n> +++ b/drivers/cxl/core/mbox.c\n> @@ -1514,23 +1514,21 @@ int cxl_mailbox_init(struct cxl_mailbox *cxl_mbox, struct device *host)\n>  }\n>  EXPORT_SYMBOL_NS_GPL(cxl_mailbox_init, \"CXL\");\n>  \n> -struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev)\n> +struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n> +\t\t\t\t\t\t u16 dvsec)\n>  {\n>  \tstruct cxl_memdev_state *mds;\n>  \tint rc;\n>  \n> -\tmds = devm_kzalloc(dev, sizeof(*mds), GFP_KERNEL);\n> +\tmds = devm_cxl_dev_state_create(dev, CXL_DEVTYPE_CLASSMEM, serial,\n> +\t\t\t\t\tdvsec, struct cxl_memdev_state, cxlds,\n> +\t\t\t\t\ttrue);\n>  \tif (!mds) {\n>  \t\tdev_err(dev, \"No memory available\\n\");\n>  \t\treturn ERR_PTR(-ENOMEM);\n>  \t}\n>  \n>  \tmutex_init(&mds->event.log_lock);\n> -\tmds->cxlds.dev = dev;\n> -\tmds->cxlds.reg_map.host = dev;\n> -\tmds->cxlds.cxl_mbox.host = dev;\n> -\tmds->cxlds.reg_map.resource = CXL_RESOURCE_NONE;\n> -\tmds->cxlds.type = CXL_DEVTYPE_CLASSMEM;\n>  \n>  \trc = devm_cxl_register_mce_notifier(dev, &mds->mce_notifier);\n>  \tif (rc == -EOPNOTSUPP)\n> diff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\n> index af3d0cc65138..22d156f25305 100644\n> --- a/drivers/cxl/core/memdev.c\n> +++ b/drivers/cxl/core/memdev.c\n> @@ -656,6 +656,38 @@ static void detach_memdev(struct work_struct *work)\n>  \n>  static struct lock_class_key cxl_memdev_key;\n>  \n> +static void cxl_dev_state_init(struct cxl_dev_state *cxlds, struct device *dev,\n> +\t\t\t       enum cxl_devtype type, u64 serial, u16 dvsec,\n> +\t\t\t       bool has_mbox)\n> +{\n> +\t*cxlds = (struct cxl_dev_state) {\n> +\t\t.dev = dev,\n> +\t\t.type = type,\n> +\t\t.serial = serial,\n> +\t\t.cxl_dvsec = dvsec,\n> +\t\t.reg_map.host = dev,\n> +\t\t.reg_map.resource = CXL_RESOURCE_NONE,\n> +\t};\n> +\n> +\tif (has_mbox)\n> +\t\tcxlds->cxl_mbox.host = dev;\n> +}\n> +\n> +struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n> +\t\t\t\t\t\t enum cxl_devtype type,\n> +\t\t\t\t\t\t u64 serial, u16 dvsec,\n> +\t\t\t\t\t\t size_t size, bool has_mbox)\n> +{\n> +\tstruct cxl_dev_state *cxlds = devm_kzalloc(dev, size, GFP_KERNEL);\n> +\n> +\tif (!cxlds)\n> +\t\treturn NULL;\n> +\n> +\tcxl_dev_state_init(cxlds, dev, type, serial, dvsec, has_mbox);\n\nNit: Having a second function to do the init seems overkill here, especially since cxl_dev_state_init() isn't called outside this\nfunction. I'd fold it into this function instead, but I'm fine with it either way (especially if you were told otherwise before).\n\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\treturn cxlds;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(_devm_cxl_dev_state_create, \"CXL\");\n> +\n>  static struct cxl_memdev *cxl_memdev_alloc(struct cxl_dev_state *cxlds,\n>  \t\t\t\t\t   const struct file_operations *fops,\n>  \t\t\t\t\t   const struct cxl_memdev_attach *attach)\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index e1d47062e1d3..3eaa353e430b 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -12,6 +12,7 @@\n>  #include <linux/node.h>\n>  #include <linux/io.h>\n>  #include <linux/range.h>\n> +#include <cxl/cxl.h>\n>  \n>  extern const struct nvdimm_security_ops *cxl_security_ops;\n>  \n> @@ -201,97 +202,6 @@ static inline int ways_to_eiw(unsigned int ways, u8 *eiw)\n>  #define   CXLDEV_MBOX_BG_CMD_COMMAND_VENDOR_MASK GENMASK_ULL(63, 48)\n>  #define CXLDEV_MBOX_PAYLOAD_OFFSET 0x20\n>  \n> -/*\n> - * Using struct_group() allows for per register-block-type helper routines,\n> - * without requiring block-type agnostic code to include the prefix.\n> - */\n> -struct cxl_regs {\n> -\t/*\n> -\t * Common set of CXL Component register block base pointers\n> -\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n> -\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n> -\t */\n> -\tstruct_group_tagged(cxl_component_regs, component,\n> -\t\tvoid __iomem *hdm_decoder;\n> -\t\tvoid __iomem *ras;\n> -\t);\n> -\t/*\n> -\t * Common set of CXL Device register block base pointers\n> -\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n> -\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n> -\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n> -\t */\n> -\tstruct_group_tagged(cxl_device_regs, device_regs,\n> -\t\tvoid __iomem *status, *mbox, *memdev;\n> -\t);\n> -\n> -\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n> -\t\tvoid __iomem *pmu;\n> -\t);\n> -\n> -\t/*\n> -\t * RCH downstream port specific RAS register\n> -\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n> -\t */\n> -\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n> -\t\tvoid __iomem *dport_aer;\n> -\t);\n> -\n> -\t/*\n> -\t * RCD upstream port specific PCIe cap register\n> -\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n> -\t */\n> -\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n> -\t\tvoid __iomem *rcd_pcie_cap;\n> -\t);\n> -};\n> -\n> -struct cxl_reg_map {\n> -\tbool valid;\n> -\tint id;\n> -\tunsigned long offset;\n> -\tunsigned long size;\n> -};\n> -\n> -struct cxl_component_reg_map {\n> -\tstruct cxl_reg_map hdm_decoder;\n> -\tstruct cxl_reg_map ras;\n> -};\n> -\n> -struct cxl_device_reg_map {\n> -\tstruct cxl_reg_map status;\n> -\tstruct cxl_reg_map mbox;\n> -\tstruct cxl_reg_map memdev;\n> -};\n> -\n> -struct cxl_pmu_reg_map {\n> -\tstruct cxl_reg_map pmu;\n> -};\n> -\n> -/**\n> - * struct cxl_register_map - DVSEC harvested register block mapping parameters\n> - * @host: device for devm operations and logging\n> - * @base: virtual base of the register-block-BAR + @block_offset\n> - * @resource: physical resource base of the register block\n> - * @max_size: maximum mapping size to perform register search\n> - * @reg_type: see enum cxl_regloc_type\n> - * @component_map: cxl_reg_map for component registers\n> - * @device_map: cxl_reg_maps for device registers\n> - * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n> - */\n> -struct cxl_register_map {\n> -\tstruct device *host;\n> -\tvoid __iomem *base;\n> -\tresource_size_t resource;\n> -\tresource_size_t max_size;\n> -\tu8 reg_type;\n> -\tunion {\n> -\t\tstruct cxl_component_reg_map component_map;\n> -\t\tstruct cxl_device_reg_map device_map;\n> -\t\tstruct cxl_pmu_reg_map pmu_map;\n> -\t};\n> -};\n> -\n>  void cxl_probe_component_regs(struct device *dev, void __iomem *base,\n>  \t\t\t      struct cxl_component_reg_map *map);\n>  void cxl_probe_device_regs(struct device *dev, void __iomem *base,\n> @@ -497,11 +407,6 @@ struct cxl_region_params {\n>  \tresource_size_t cache_size;\n>  };\n>  \n> -enum cxl_partition_mode {\n> -\tCXL_PARTMODE_RAM,\n> -\tCXL_PARTMODE_PMEM,\n> -};\n> -\n>  /*\n>   * Indicate whether this region has been assembled by autodetection or\n>   * userspace assembly. Prevent endpoint decoders outside of automatic\n> diff --git a/drivers/cxl/cxlmem.h b/drivers/cxl/cxlmem.h\n> index ef202b34e5ea..281546de426e 100644\n> --- a/drivers/cxl/cxlmem.h\n> +++ b/drivers/cxl/cxlmem.h\n> @@ -113,8 +113,6 @@ int devm_cxl_dpa_reserve(struct cxl_endpoint_decoder *cxled,\n>  \t\t\t resource_size_t base, resource_size_t len,\n>  \t\t\t resource_size_t skipped);\n>  \n> -#define CXL_NR_PARTITIONS_MAX 2\n> -\n>  struct cxl_dpa_info {\n>  \tu64 size;\n>  \tstruct cxl_dpa_part_info {\n> @@ -373,87 +371,6 @@ struct cxl_security_state {\n>  \tstruct kernfs_node *sanitize_node;\n>  };\n>  \n> -/*\n> - * enum cxl_devtype - delineate type-2 from a generic type-3 device\n> - * @CXL_DEVTYPE_DEVMEM - Vendor specific CXL Type-2 device implementing HDM-D or\n> - *\t\t\t HDM-DB, no requirement that this device implements a\n> - *\t\t\t mailbox, or other memory-device-standard manageability\n> - *\t\t\t flows.\n> - * @CXL_DEVTYPE_CLASSMEM - Common class definition of a CXL Type-3 device with\n> - *\t\t\t   HDM-H and class-mandatory memory device registers\n> - */\n> -enum cxl_devtype {\n> -\tCXL_DEVTYPE_DEVMEM,\n> -\tCXL_DEVTYPE_CLASSMEM,\n> -};\n> -\n> -/**\n> - * struct cxl_dpa_perf - DPA performance property entry\n> - * @dpa_range: range for DPA address\n> - * @coord: QoS performance data (i.e. latency, bandwidth)\n> - * @cdat_coord: raw QoS performance data from CDAT\n> - * @qos_class: QoS Class cookies\n> - */\n> -struct cxl_dpa_perf {\n> -\tstruct range dpa_range;\n> -\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n> -\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n> -\tint qos_class;\n> -};\n> -\n> -/**\n> - * struct cxl_dpa_partition - DPA partition descriptor\n> - * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n> - * @perf: performance attributes of the partition from CDAT\n> - * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n> - */\n> -struct cxl_dpa_partition {\n> -\tstruct resource res;\n> -\tstruct cxl_dpa_perf perf;\n> -\tenum cxl_partition_mode mode;\n> -};\n> -\n> -/**\n> - * struct cxl_dev_state - The driver device state\n> - *\n> - * cxl_dev_state represents the CXL driver/device state.  It provides an\n> - * interface to mailbox commands as well as some cached data about the device.\n> - * Currently only memory devices are represented.\n> - *\n> - * @dev: The device associated with this CXL state\n> - * @cxlmd: The device representing the CXL.mem capabilities of @dev\n> - * @reg_map: component and ras register mapping parameters\n> - * @regs: Parsed register blocks\n> - * @cxl_dvsec: Offset to the PCIe device DVSEC\n> - * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n> - * @media_ready: Indicate whether the device media is usable\n> - * @dpa_res: Overall DPA resource tree for the device\n> - * @part: DPA partition array\n> - * @nr_partitions: Number of DPA partitions\n> - * @serial: PCIe Device Serial Number\n> - * @type: Generic Memory Class device or Vendor Specific Memory device\n> - * @cxl_mbox: CXL mailbox context\n> - * @cxlfs: CXL features context\n> - */\n> -struct cxl_dev_state {\n> -\tstruct device *dev;\n> -\tstruct cxl_memdev *cxlmd;\n> -\tstruct cxl_register_map reg_map;\n> -\tstruct cxl_regs regs;\n> -\tint cxl_dvsec;\n> -\tbool rcd;\n> -\tbool media_ready;\n> -\tstruct resource dpa_res;\n> -\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n> -\tunsigned int nr_partitions;\n> -\tu64 serial;\n> -\tenum cxl_devtype type;\n> -\tstruct cxl_mailbox cxl_mbox;\n> -#ifdef CONFIG_CXL_FEATURES\n> -\tstruct cxl_features_state *cxlfs;\n> -#endif\n> -};\n> -\n>  static inline resource_size_t cxl_pmem_size(struct cxl_dev_state *cxlds)\n>  {\n>  \t/*\n> @@ -858,7 +775,8 @@ int cxl_dev_state_identify(struct cxl_memdev_state *mds);\n>  int cxl_await_media_ready(struct cxl_dev_state *cxlds);\n>  int cxl_enumerate_cmds(struct cxl_memdev_state *mds);\n>  int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info);\n> -struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev);\n> +struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n> +\t\t\t\t\t\t u16 dvsec);\n>  void set_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n>  \t\t\t\tunsigned long *cmds);\n>  void clear_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n> diff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\n> index 1cf232220873..24179cc702bf 100644\n> --- a/drivers/cxl/pci.c\n> +++ b/drivers/cxl/pci.c\n> @@ -911,25 +911,25 @@ static int cxl_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n>  \tint rc, pmu_count;\n>  \tunsigned int i;\n>  \tbool irq_avail;\n> +\tu16 dvsec;\n>  \n>  \trc = pcim_enable_device(pdev);\n>  \tif (rc)\n>  \t\treturn rc;\n>  \tpci_set_master(pdev);\n>  \n> -\tmds = cxl_memdev_state_create(&pdev->dev);\n> +\tdvsec = pci_find_dvsec_capability(pdev, PCI_VENDOR_ID_CXL,\n> +\t\t\t\t\t  PCI_DVSEC_CXL_DEVICE);\n> +\tif (!dvsec)\n> +\t\tpci_warn(pdev, \"Device DVSEC not present, skip CXL.mem init\\n\");\n> +\n> +\tmds = cxl_memdev_state_create(&pdev->dev, pci_get_dsn(pdev), dvsec);\n>  \tif (IS_ERR(mds))\n>  \t\treturn PTR_ERR(mds);\n>  \tcxlds = &mds->cxlds;\n>  \tpci_set_drvdata(pdev, cxlds);\n>  \n>  \tcxlds->rcd = is_cxl_restricted(pdev);\n> -\tcxlds->serial = pci_get_dsn(pdev);\n> -\tcxlds->cxl_dvsec = pci_find_dvsec_capability(\n> -\t\tpdev, PCI_VENDOR_ID_CXL, PCI_DVSEC_CXL_DEVICE);\n> -\tif (!cxlds->cxl_dvsec)\n> -\t\tdev_warn(&pdev->dev,\n> -\t\t\t \"Device DVSEC not present, skip CXL.mem init\\n\");\n>  \n>  \trc = cxl_pci_setup_regs(pdev, CXL_REGLOC_RBI_MEMDEV, &map);\n>  \tif (rc)\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> new file mode 100644\n> index 000000000000..13d448686189\n> --- /dev/null\n> +++ b/include/cxl/cxl.h\n> @@ -0,0 +1,226 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +/* Copyright(c) 2020 Intel Corporation. */\n> +/* Copyright(c) 2025 Advanced Micro Devices, Inc. */\n> +\n> +#ifndef __CXL_CXL_H__\n> +#define __CXL_CXL_H__\n> +\n> +#include <linux/node.h>\n> +#include <linux/ioport.h>\n> +#include <cxl/mailbox.h>\n> +\n> +/**\n> + * enum cxl_devtype - delineate type-2 from a generic type-3 device\n> + * @CXL_DEVTYPE_DEVMEM: Vendor specific CXL Type-2 device implementing HDM-D or\n> + *\t\t\t HDM-DB, no requirement that this device implements a\n> + *\t\t\t mailbox, or other memory-device-standard manageability\n> + *\t\t\t flows.\n> + * @CXL_DEVTYPE_CLASSMEM: Common class definition of a CXL Type-3 device with\n> + *\t\t\t   HDM-H and class-mandatory memory device registers\n> + */\n> +enum cxl_devtype {\n> +\tCXL_DEVTYPE_DEVMEM,\n> +\tCXL_DEVTYPE_CLASSMEM,\n> +};\n> +\n> +struct device;\n> +\n> +/*\n> + * Using struct_group() allows for per register-block-type helper routines,\n> + * without requiring block-type agnostic code to include the prefix.\n> + */\n> +struct cxl_regs {\n> +\t/*\n> +\t * Common set of CXL Component register block base pointers\n> +\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n> +\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n> +\t */\n> +\tstruct_group_tagged(cxl_component_regs, component,\n> +\t\tvoid __iomem *hdm_decoder;\n> +\t\tvoid __iomem *ras;\n> +\t);\n> +\t/*\n> +\t * Common set of CXL Device register block base pointers\n> +\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n> +\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n> +\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n> +\t */\n> +\tstruct_group_tagged(cxl_device_regs, device_regs,\n> +\t\tvoid __iomem *status, *mbox, *memdev;\n> +\t);\n> +\n> +\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n> +\t\tvoid __iomem *pmu;\n> +\t);\n> +\n> +\t/*\n> +\t * RCH downstream port specific RAS register\n> +\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n> +\t */\n> +\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n> +\t\tvoid __iomem *dport_aer;\n> +\t);\n> +\n> +\t/*\n> +\t * RCD upstream port specific PCIe cap register\n> +\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n> +\t */\n> +\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n> +\t\tvoid __iomem *rcd_pcie_cap;\n> +\t);\n> +};\n> +\n> +struct cxl_reg_map {\n> +\tbool valid;\n> +\tint id;\n> +\tunsigned long offset;\n> +\tunsigned long size;\n> +};\n> +\n> +struct cxl_component_reg_map {\n> +\tstruct cxl_reg_map hdm_decoder;\n> +\tstruct cxl_reg_map ras;\n> +};\n> +\n> +struct cxl_device_reg_map {\n> +\tstruct cxl_reg_map status;\n> +\tstruct cxl_reg_map mbox;\n> +\tstruct cxl_reg_map memdev;\n> +};\n> +\n> +struct cxl_pmu_reg_map {\n> +\tstruct cxl_reg_map pmu;\n> +};\n> +\n> +/**\n> + * struct cxl_register_map - DVSEC harvested register block mapping parameters\n> + * @host: device for devm operations and logging\n> + * @base: virtual base of the register-block-BAR + @block_offset\n> + * @resource: physical resource base of the register block\n> + * @max_size: maximum mapping size to perform register search\n> + * @reg_type: see enum cxl_regloc_type\n> + * @component_map: cxl_reg_map for component registers\n> + * @device_map: cxl_reg_maps for device registers\n> + * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n> + */\n> +struct cxl_register_map {\n> +\tstruct device *host;\n> +\tvoid __iomem *base;\n> +\tresource_size_t resource;\n> +\tresource_size_t max_size;\n> +\tu8 reg_type;\n> +\tunion {\n> +\t\tstruct cxl_component_reg_map component_map;\n> +\t\tstruct cxl_device_reg_map device_map;\n> +\t\tstruct cxl_pmu_reg_map pmu_map;\n> +\t};\n> +};\n> +\n> +/**\n> + * struct cxl_dpa_perf - DPA performance property entry\n> + * @dpa_range: range for DPA address\n> + * @coord: QoS performance data (i.e. latency, bandwidth)\n> + * @cdat_coord: raw QoS performance data from CDAT\n> + * @qos_class: QoS Class cookies\n> + */\n> +struct cxl_dpa_perf {\n> +\tstruct range dpa_range;\n> +\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n> +\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n> +\tint qos_class;\n> +};\n> +\n> +enum cxl_partition_mode {\n> +\tCXL_PARTMODE_RAM,\n> +\tCXL_PARTMODE_PMEM,\n> +};\n> +\n> +/**\n> + * struct cxl_dpa_partition - DPA partition descriptor\n> + * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n> + * @perf: performance attributes of the partition from CDAT\n> + * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n> + */\n> +struct cxl_dpa_partition {\n> +\tstruct resource res;\n> +\tstruct cxl_dpa_perf perf;\n> +\tenum cxl_partition_mode mode;\n> +};\n> +\n> +#define CXL_NR_PARTITIONS_MAX 2\n> +\n> +/**\n> + * struct cxl_dev_state - The driver device state\n> + *\n> + * cxl_dev_state represents the CXL driver/device state.  It provides an\n> + * interface to mailbox commands as well as some cached data about the device.\n> + * Currently only memory devices are represented.\n> + *\n> + * @dev: The device associated with this CXL state\n> + * @cxlmd: The device representing the CXL.mem capabilities of @dev\n> + * @reg_map: component and ras register mapping parameters\n> + * @regs: Parsed register blocks\n> + * @cxl_dvsec: Offset to the PCIe device DVSEC\n> + * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n> + * @media_ready: Indicate whether the device media is usable\n> + * @dpa_res: Overall DPA resource tree for the device\n> + * @part: DPA partition array\n> + * @nr_partitions: Number of DPA partitions\n> + * @serial: PCIe Device Serial Number\n> + * @type: Generic Memory Class device or Vendor Specific Memory device\n> + * @cxl_mbox: CXL mailbox context\n> + * @cxlfs: CXL features context\n> + */\n> +struct cxl_dev_state {\n> +\t/* public for Type2 drivers */\n> +\tstruct device *dev;\n> +\tstruct cxl_memdev *cxlmd;\n> +\n> +\t/* private for Type2 drivers */\n> +\tstruct cxl_register_map reg_map;\n> +\tstruct cxl_regs regs;\n> +\tint cxl_dvsec;\n> +\tbool rcd;\n> +\tbool media_ready;\n> +\tstruct resource dpa_res;\n> +\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n> +\tunsigned int nr_partitions;\n> +\tu64 serial;\n> +\tenum cxl_devtype type;\n> +\tstruct cxl_mailbox cxl_mbox;\n> +#ifdef CONFIG_CXL_FEATURES\n> +\tstruct cxl_features_state *cxlfs;\n> +#endif\n> +};\n> +\n> +struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n> +\t\t\t\t\t\t enum cxl_devtype type,\n> +\t\t\t\t\t\t u64 serial, u16 dvsec,\n> +\t\t\t\t\t\t size_t size, bool has_mbox);\n> +\n> +/**\n> + * cxl_dev_state_create - safely create and cast a cxl dev state embedded in a\n> + * driver specific struct.\n> + *\n> + * @parent: device behind the request\n> + * @type: CXL device type\n> + * @serial: device identification\n> + * @dvsec: dvsec capability offset\n> + * @drv_struct: driver struct embedding a cxl_dev_state struct\n> + * @member: drv_struct member as cxl_dev_state\n> + * @mbox: true if mailbox supported\n> + *\n> + * Returns a pointer to the drv_struct allocated and embedding a cxl_dev_state\n> + * struct initialized.\n> + *\n> + * Introduced for Type2 driver support.\n> + */\n> +#define devm_cxl_dev_state_create(parent, type, serial, dvsec, drv_struct, member, mbox)\t\\\n> +\t({\t\t\t\t\t\t\t\t\t\t\\\n> +\t\tstatic_assert(__same_type(struct cxl_dev_state,\t\t\t\t\\\n> +\t\t\t      ((drv_struct *)NULL)->member));\t\t\t\t\\\n> +\t\tstatic_assert(offsetof(drv_struct, member) == 0);\t\t\t\\\n> +\t\t(drv_struct *)_devm_cxl_dev_state_create(parent, type, serial, dvsec,\t\\\n> +\t\t\t\t\t\t      sizeof(drv_struct), mbox);\t\\\n> +\t})\n> +#endif /* __CXL_CXL_H__ */\n> diff --git a/tools/testing/cxl/test/mem.c b/tools/testing/cxl/test/mem.c\n> index cb87e8c0e63c..79f42f4474d4 100644\n> --- a/tools/testing/cxl/test/mem.c\n> +++ b/tools/testing/cxl/test/mem.c\n> @@ -1716,7 +1716,7 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> -\tmds = cxl_memdev_state_create(dev);\n> +\tmds = cxl_memdev_state_create(dev, pdev->id + 1, 0);\n>  \tif (IS_ERR(mds))\n>  \t\treturn PTR_ERR(mds);\n>  \n> @@ -1732,7 +1732,6 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n>  \tmds->event.buf = (struct cxl_get_event_payload *) mdata->event_buf;\n>  \tINIT_DELAYED_WORK(&mds->security.poll_dwork, cxl_mockmem_sanitize_work);\n>  \n> -\tcxlds->serial = pdev->id + 1;\n>  \tif (is_rcd(pdev))\n>  \t\tcxlds->rcd = true;\n>  \n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> A Type2 device configured by the BIOS can already have its HDM\n> committed. Add a cxl_get_committed_decoder() function for cheking\n> so after memdev creation. A CXL region should have been created\n> during memdev initialization, therefore a Type2 driver can ask for\n> such a region for working with the HPA. If the HDM is not committed,\n> a Type2 driver will create the region after obtaining proper HPA\n> and DPA space.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/cxl/core/hdm.c | 39 +++++++++++++++++++++++++++++++++++++++\n>  include/cxl/cxl.h      |  3 +++\n>  2 files changed, 42 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index 6e516c69b2d2..a172ce4e9b19 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -686,6 +686,45 @@ int cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  \treturn devm_add_action_or_reset(&port->dev, cxl_dpa_release, cxled);\n>  }\n>  \n> +static int find_committed_endpoint_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == port->hdm_end;\n\nIs this the way you're supposed to check if a decoder is committed? The doc comment for @hdm_end in\nstruct cxl_port says it's just the last allocated decoder. If allocated decoders are always committed then\nI'm fine with this, otherwise I think you'd want to a register read or something to find the commit state.\n> +}\n> +\n> +struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t\t       struct cxl_region **cxlr)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct device *cxled_dev;\n> +\n> +\tif (!endpoint)\n> +\t\treturn NULL;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tcxled_dev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\t      find_committed_endpoint_decoder);\n> +\n> +\tif (!cxled_dev)\n> +\t\treturn NULL;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(cxled_dev);\n> +\t*cxlr = cxled->cxld.region;\n> +\n> +\tput_device(cxled_dev);\n> +\treturn cxled;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_committed_decoder, \"CXL\");\n> +\n>  static void cxld_set_interleave(struct cxl_decoder *cxld, u32 *ctrl)\n>  {\n>  \tu16 eig;\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 6f8d365067af..928276dba952 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -249,4 +249,7 @@ int cxl_map_component_regs(const struct cxl_register_map *map,\n>  int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n>  struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n>  \t\t\t\t       const struct cxl_memdev_attach *attach);\n> +struct cxl_region;\n> +struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t\t       struct cxl_region **cxlr);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> This patchset should be applied on the cxl next branch using the base\n> specified at the end of this cover letter.\n> \n> Dependencies on Dan's work has gone and also on Terry's as the only\n> patch required is now in next. The other dependency is on Smita patchset\n> but it does not exist such a dependency as that work will not avoid the\n> problem with Type2 and DAX/hmem if soft reserved memory. This needs to\n> be solved by the BIOS and Type2 UEFI driver for populating the CXL.mem\n> range as EFI_RESERVED_TYPE instead of default EFI_CONVENTIONAL_MEMORY\n> with the EFI_MEMORY_SP attribute. There exists though a dependency on\n> one Smita's patches:\n> \n> [PATCH v5 3/7] cxl/region: Skip decoder reset on detach for autodiscovered regions\n> \n> This is needed for the default behaviour with current BIOS configuration\n> where the HDM Type2 decoders will be kept unreset when driver unloads.\n> This is the main change introduced in v23: committed decoders will not\n> be reset. Previous v22 functionality supported first driver load finding\n> committed decoders but resetting them at unload and supporting\n> uncommitted decoders in next driver loads. This will be suported in\n> follow-up works.\n> \n> v23 changes:\n> \n>   patch 11: fixing minor issues and droping change in\n> \t    should_emulate_decoders (Jonathan Cameron)\n> \n>   patch13: refactoring unregister_region for safety type in Type2 API\n> \n>   sfc changes: slight modifications to error path\n> \n\nThis cover letter is really long, I'd remove the change logs for anything more\nthan 3 revisions back (assuming a v24 is needed). After that you could leave\na lore link for older revisions if you want, but it's not needed imo.\nAlso, feel free to add my Reviewed-by for anything I didn't leave a comment on\n(felt I should cut down on the mail).\n\nThanks,\nBen\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation involves finding available DPA (device-physical-address)\n> capacity to map into HPA (host-physical-address) space.\n> \n> In order to support CXL Type2 devices, define an API, cxl_request_dpa(),\n> that tries to allocate the DPA memory the driver requires to operate.The\n> memory requested should not be bigger than the max available HPA obtained\n> previously with cxl_get_hpa_freespace().\n> \n> Based on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h      |  1 +\n>  include/cxl/cxl.h      |  5 +++\n>  3 files changed, 90 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index a172ce4e9b19..d60a697f12cc 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -3,6 +3,7 @@\n>  #include <linux/seq_file.h>\n>  #include <linux/device.h>\n>  #include <linux/delay.h>\n> +#include <cxl/cxl.h>\n>  \n>  #include \"cxlmem.h\"\n>  #include \"core.h\"\n> @@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n>  \treturn resource_contains(res, &_addr);\n>  }\n>  \n> +/**\n> + * cxl_dpa_free - release DPA (Device Physical Address)\n> + * @cxled: endpoint decoder linked to the DPA\n> + *\n> + * Returns 0 or error.\n> + */\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_port *port = cxled_to_port(cxled);\n> @@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  \tdevm_cxl_dpa_release(cxled);\n>  \treturn 0;\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n>  \n>  int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \t\t     enum cxl_partition_mode mode)\n> @@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \treturn 0;\n>  }\n>  \n> +static int find_free_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == (port->hdm_end + 1);\n> +}\n> +\n> +static struct cxl_endpoint_decoder *\n> +cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct device *dev;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tdev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\tfind_free_decoder);\n> +\tif (!dev)\n> +\t\treturn NULL;\n> +\n> +\treturn to_cxl_endpoint_decoder(dev);\n> +}\n> +\n> +/**\n> + * cxl_request_dpa - search and reserve DPA given input constraints\n> + * @cxlmd: memdev with an endpoint port with available decoders\n> + * @mode: CXL partition mode (ram vs pmem)\n> + * @alloc: dpa size required\n> + *\n> + * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n> + * an errno encoded pointer on failure.\n> + *\n> + * Given that a region needs to allocate from limited HPA capacity it\n> + * may be the case that a device has more mappable DPA capacity than\n> + * available HPA. The expectation is that @alloc is a driver known\n> + * value based on the device capacity but which could not be fully\n> + * available due to HPA constraints.\n> + *\n> + * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n> + * reserved, or an error pointer. The caller is also expected to own the\n> + * lifetime of the memdev registration associated with the endpoint to\n> + * pin the decoder registered as well.\n> + */\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc)\n> +{\n> +\tint rc;\n> +\n> +\tif (!IS_ALIGNED(alloc, SZ_256M))\n> +\t\treturn ERR_PTR(-EINVAL);\n> +\n> +\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n> +\t\tcxl_find_free_decoder(cxlmd);\n> +\n> +\tif (!cxled)\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\n> +\trc = cxl_dpa_set_part(cxled, mode);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = cxl_dpa_alloc(cxled, alloc);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n\nShould cxl_dpa_set_part() be unwound here, or does it not matter? If it doesn't matter:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\n> +\treturn no_free_ptr(cxled);\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n> +\n>  static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index d1b010e5e1d0..2b1f7d687a0e 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n>  \n>  DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n>  DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n> +DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n>  DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n>  DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 783ad570a6eb..4802371db00e 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -7,6 +7,7 @@\n>  \n>  #include <linux/node.h>\n>  #include <linux/ioport.h>\n> +#include <linux/range.h>\n>  #include <cxl/mailbox.h>\n>  \n>  /**\n> @@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t       unsigned long flags,\n>  \t\t\t\t\t       resource_size_t *max);\n>  void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc);\n> +int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\n\n\nOn 2/19/2026 4:40 AM, Alejandro Lucero Palau wrote:\n> \n> On 2/11/26 22:11, Cheatham, Benjamin wrote:\n>> On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n>>> From: Alejandro Lucero <alucerop@amd.com>\n>>>\n>>> Region creation based on Type3 devices is triggered from user space\n>>> allowing memory combination through interleaving.\n>>>\n>>> In preparation for kernel driven region creation, that is Type2 drivers\n>>> triggering region creation backed with its advertised CXL memory, factor\n>>> out a common helper from the user-sysfs region setup for interleave ways.\n>>>\n>>> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n>>> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n>>> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n>>> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n>>> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>>> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n>>> ---\n>>>  drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n>>>  1 file changed, 27 insertions(+), 16 deletions(-)\n>>>\n>>> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n>>> index f53b2e9fd9e6..ece1d3df7cf1 100644\n>>> --- a/drivers/cxl/core/region.c\n>>> +++ b/drivers/cxl/core/region.c\n>>> @@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n>>>   static const struct attribute_group *get_cxl_region_target_group(void);\n>>>  -static ssize_t interleave_ways_store(struct device *dev,\n>>> - struct device_attribute *attr,\n>>> - const char *buf, size_t len)\n>>> +static int set_interleave_ways(struct cxl_region *cxlr, int val)\n>> @val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\n>> function was originally coded with that in mind (same with @save below).\n> \n> Good catch. I wonder if I should just change the way the value is obtained, using kstrtoint instead of kstrtouint, as those values are used for cxl_region_params fields defined as int. In other words, it seems doing that simpler than changing all the other places you mention and the structs involved. I can not see a reason for using unsigned int so I think I will follow that approach. Tell me if you think otherwise.\n> \n\nIf I had to guess unsigned int was used because a negative interleave granularity/ways makes no sense. I think your suggestion is fine though since no one\nin their right mind would give anything but a (relatively) small and positive value for these.\n\nThanks,\nBen\n\n> \n> Thank you\n> \n> \n>> With that cleaned up:\n>> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>>\n>>>  {\n>>> - struct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n>>> + struct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>>>  struct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n>>> - struct cxl_region *cxlr = to_cxl_region(dev);\n>>>  struct cxl_region_params *p = &cxlr->params;\n>>> - unsigned int val, save;\n>>> - int rc;\n>>> + int save, rc;\n>>>  u8 iw;\n>>>  - rc = kstrtouint(buf, 0, &val);\n>>> - if (rc)\n>>> - return rc;\n>>> -\n>>>  rc = ways_to_eiw(val, &iw);\n>>>  if (rc)\n>>>  return rc;\n>>> @@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n>>>  return -EINVAL;\n>>>  }\n>>>  - ACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n>>> - if ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n>>> - return rc;\n>>> + lockdep_assert_held_write(&cxl_rwsem.region);\n>>>   if (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>>>  return -EBUSY;\n>>> @@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n>>>  save = p->interleave_ways;\n>>>  p->interleave_ways = val;\n>>>  rc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n>>> - if (rc) {\n>>> + if (rc)\n>>>  p->interleave_ways = save;\n>>> +\n>>> + return rc;\n>>> +}\n>>> +\n>>> +static ssize_t interleave_ways_store(struct device *dev,\n>>> + struct device_attribute *attr,\n>>> + const char *buf, size_t len)\n>>> +{\n>>> + struct cxl_region *cxlr = to_cxl_region(dev);\n>>> + unsigned int val;\n>>> + int rc;\n>>> +\n>>> + rc = kstrtouint(buf, 0, &val);\n>>> + if (rc)\n>>> + return rc;\n>>> +\n>>> + ACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n>>> + if ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n>>> + return rc;\n>>> +\n>>> + rc = set_interleave_ways(cxlr, val);\n>>> + if (rc)\n>>>  return rc;\n>>> - }\n>>>   return len;\n>>>  }\n\n\n\n---\n\nOn 2/19/2026 3:58 AM, Alejandro Lucero Palau wrote:\n> \n> On 2/11/26 22:10, Cheatham, Benjamin wrote:\n>> On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n>>> From: Alejandro Lucero <alucerop@amd.com>\n>>>\n>>> CXL region creation involves allocating capacity from Device Physical\n>>> Address (DPA) and assigning it to decode a given Host Physical Address\n>>> (HPA). Before determining how much DPA to allocate the amount of available\n>>> HPA must be determined. Also, not all HPA is created equal, some HPA\n>>> targets RAM, some targets PMEM, some is prepared for device-memory flows\n>>> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n>>>\n>>> In order to support Type2 CXL devices, wrap all of those concerns into\n>>> an API that retrieves a root decoder (platform CXL window) that fits the\n>>> specified constraints and the capacity available for a new region.\n>>>\n>>> Add a complementary function for releasing the reference to such root\n>>> decoder.\n>>>\n>>> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n>>>\n>>> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n>>> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n>>> ---\n>>>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>>>  drivers/cxl/cxl.h | 3 +\n>>>  include/cxl/cxl.h | 6 ++\n>>>  3 files changed, 173 insertions(+)\n>>>\n>>> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n>>> index 954b8fcdbac6..bdefd088f5f1 100644\n>>> --- a/drivers/cxl/core/region.c\n>>> +++ b/drivers/cxl/core/region.c\n>>> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>>>  return 0;\n>>>  }\n>>>  +struct cxlrd_max_context {\n>>> + struct device * const *host_bridges;\n>>> + int interleave_ways;\n>>> + unsigned long flags;\n>>> + resource_size_t max_hpa;\n>>> + struct cxl_root_decoder *cxlrd;\n>>> +};\n>>> +\n>>> +static int find_max_hpa(struct device *dev, void *data)\n>>> +{\n>>> + struct cxlrd_max_context *ctx = data;\n>>> + struct cxl_switch_decoder *cxlsd;\n>>> + struct cxl_root_decoder *cxlrd;\n>>> + struct resource *res, *prev;\n>>> + struct cxl_decoder *cxld;\n>>> + resource_size_t free = 0;\n>>> + resource_size_t max;\n>>> + int found = 0;\n>>> +\n>>> + if (!is_root_decoder(dev))\n>>> + return 0;\n>>> +\n>>> + cxlrd = to_cxl_root_decoder(dev);\n>>> + cxlsd = &cxlrd->cxlsd;\n>>> + cxld = &cxlsd->cxld;\n>>> +\n>>> + if ((cxld->flags & ctx->flags) != ctx->flags) {\n>>> + dev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n>>> + cxld->flags, ctx->flags);\n>>> + return 0;\n>>> + }\n>>> +\n>>> + for (int i = 0; i < ctx->interleave_ways; i++) {\n>>> + for (int j = 0; j < ctx->interleave_ways; j++) {\n>>> + if (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n>>> + found++;\n>>> + break;\n>>> + }\n>>> + }\n>>> + }\n>> This may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\n>> what the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\n>> bridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\n>> point, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\n>> be a struct device * const.\n>>\n>> Maybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\n>> I'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n> \n> \n> Hi Ben,\n> \n> \n> I do remember this one.\n> \n> \n> Dan's original patches had this support for interleaving, then I removed it as the case for Type2 and interleaving is quite unlikely, at least right now and likely in the near future. But I was told why do not support it as it was trivial to do so. FWIW, If I think only about the use case coming with the patchset, I agree with you, but because those previous discussions, I think I have to leave it.\n> \n\nI'm fine with that, but I would at least do the fix with the decoder position in 19/22 and make a note that the\ninterleave_ways parameter in cxl_get_hpa_freespace() below is currently unused (unless I'm misunderstanding\nthe endpoint->host_bridge member).\n\nThat way, the support is mostly there and just requires a small, previously noted, addition to enable. If you're\nfine with that then feel free to add my Reviewed-by after implementing in v24.\n\nThanks,\nBen\n\n> \n> Thank you\n> \n> \n>>> +\n>>> + if (found != ctx->interleave_ways) {\n>>> + dev_dbg(dev,\n>>> + \"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n>>> + found, ctx->interleave_ways);\n>>> + return 0;\n>>> + }\n>>> +\n>>> + /*\n>>> + * Walk the root decoder resource range relying on cxl_rwsem.region to\n>>> + * preclude sibling arrival/departure and find the largest free space\n>>> + * gap.\n>>> + */\n>>> + lockdep_assert_held_read(&cxl_rwsem.region);\n>>> + res = cxlrd->res->child;\n>>> +\n>>> + /* With no resource child the whole parent resource is available */\n>>> + if (!res)\n>>> + max = resource_size(cxlrd->res);\n>>> + else\n>>> + max = 0;\n>>> +\n>>> + for (prev = NULL; res; prev = res, res = res->sibling) {\n>>> + if (!prev && res->start == cxlrd->res->start &&\n>>> + res->end == cxlrd->res->end) {\n>>> + max = resource_size(cxlrd->res);\n>>> + break;\n>>> + }\n>>> + /*\n>>> + * Sanity check for preventing arithmetic problems below as a\n>>> + * resource with size 0 could imply using the end field below\n>>> + * when set to unsigned zero - 1 or all f in hex.\n>>> + */\n>>> + if (prev && !resource_size(prev))\n>>> + continue;\n>>> +\n>>> + if (!prev && res->start > cxlrd->res->start) {\n>>> + free = res->start - cxlrd->res->start;\n>>> + max = max(free, max);\n>>> + }\n>>> + if (prev && res->start > prev->end + 1) {\n>>> + free = res->start - prev->end + 1;\n>>> + max = max(free, max);\n>>> + }\n>>> + }\n>>> +\n>>> + if (prev && prev->end + 1 < cxlrd->res->end + 1) {\n>>> + free = cxlrd->res->end + 1 - prev->end + 1;\n>>> + max = max(free, max);\n>>> + }\n>>> +\n>>> + dev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n>>> + if (max > ctx->max_hpa) {\n>>> + if (ctx->cxlrd)\n>>> + put_device(cxlrd_dev(ctx->cxlrd));\n>>> + get_device(cxlrd_dev(cxlrd));\n>>> + ctx->cxlrd = cxlrd;\n>>> + ctx->max_hpa = max;\n>>> + }\n>>> + return 0;\n>>> +}\n>>> +\n>>> +/**\n>>> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n>>> + * @cxlmd: the mem device requiring the HPA\n>>> + * @interleave_ways: number of entries in @host_bridges\n>>> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n>>> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n>>> + * returned decoder\n>>> + *\n>>> + * Returns a pointer to a struct cxl_root_decoder\n>>> + *\n>>> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n>>> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n>>> + * caller goes to use this decoder and its capacity is reduced then caller needs\n>>> + * to loop and retry.\n>>> + *\n>>> + * The returned root decoder has an elevated reference count that needs to be\n>>> + * put with cxl_put_root_decoder(cxlrd).\n>>> + */\n>>> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>>> + int interleave_ways,\n>>> + unsigned long flags,\n>>> + resource_size_t *max_avail_contig)\n>>> +{\n>>> + struct cxlrd_max_context ctx = {\n>>> + .flags = flags,\n>>> + .interleave_ways = interleave_ways,\n>>> + };\n>>> + struct cxl_port *root_port;\n>>> + struct cxl_port *endpoint;\n>>> +\n>>> + endpoint = cxlmd->endpoint;\n>>> + if (!endpoint) {\n>>> + dev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n>>> + return ERR_PTR(-ENXIO);\n>>> + }\n>>> +\n>>> + ctx.host_bridges = &endpoint->host_bridge;\n>> Mentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\n>> something). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\n>> I'm not sure of what the fix is to get the intended behavior.\n>>\n>> It may be worth getting rid of the interleave_ways portion of this function and\n>> add it later when someone needs it. You could also explain it's hard coded to 1/unused\n>> in the doc comment if you know of an immediate need for it.\n>>\n>>> +\n>>> + struct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n>>> + if (!root) {\n>>> + dev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n>>> + return ERR_PTR(-ENXIO);\n>>> + }\n>>> +\n>>> + root_port = &root->port;\n>>> + scoped_guard(rwsem_read, &cxl_rwsem.region)\n>>> + device_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n>> Can just use a guard() here.\n>>\n>>> +\n>>> + if (!ctx.cxlrd)\n>>> + return ERR_PTR(-ENOMEM);\n>>> +\n>>> + *max_avail_contig = ctx.max_hpa;\n>>> + return ctx.cxlrd;\n>>> +}\n>>> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n>>> +\n>>> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n>>> +{\n>>> + put_device(cxlrd_dev(cxlrd));\n>>> +}\n>>> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n>>> +\n>>>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>>>  const char *buf, size_t len)\n>>>  {\n>>> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n>>> index 944c5d1ccceb..c7d9b2c2908f 100644\n>>> --- a/drivers/cxl/cxl.h\n>>> +++ b/drivers/cxl/cxl.h\n>>> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>>>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>>>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>>>  bool is_root_decoder(struct device *dev);\n>>> +\n>>> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n>>> +\n>>>  bool is_switch_decoder(struct device *dev);\n>>>  bool is_endpoint_decoder(struct device *dev);\n>>>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n>>> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n>>> index 92880c26b2d5..834dc7e78934 100644\n>>> --- a/include/cxl/cxl.h\n>>> +++ b/include/cxl/cxl.h\n>>> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>>>  struct range;\n>>>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>>>  void cxl_unregister_region(struct cxl_region *cxlr);\n>>> +struct cxl_port;\n>>> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>>> + int interleave_ways,\n>>> + unsigned long flags,\n>>> + resource_size_t *max);\n>>> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>>>  #endif /* __CXL_CXL_H__ */\n\n",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested updating ABI documentation to explicitly state that this attribute only applies to type 3 regions, and offered a Reviewed-by tag.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggested update",
                "considered worthwhile"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I haven't read the ABI docs, but would it be worthwhile to update the documentation for this attribute\nto mention it only makes type 3 regions? I'm flip-flopping on whether it's worth the trouble but thought\nI should mention it.\n\nEither way:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the function parameter @val should remain an unsigned int, as it is passed as such in sysfs and was originally coded to expect this type.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested_change"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "@val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\nfunction was originally coded with that in mind (same with @save below). With that cleaned up:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> By definition a type2 cxl device will use the host managed memory for\n> specific functionality, therefore it should not be available to other\n> uses.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> ---\n>  drivers/cxl/core/region.c | 7 +++++++\n>  1 file changed, 7 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 293e63dfef22..12df717cc881 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -4441,6 +4441,13 @@ static int cxl_region_probe(struct device *dev)\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> +\t/*\n> +\t * HDM-D[B] (device-memory) regions have accelerator specific usage.\n> +\t * Skip device-dax registration.\n> +\t */\n> +\tif (cxlr->type == CXL_DECODER_DEVMEM)\n> +\t\treturn 0;\n\nMinor nit: Should probably move this to be the first thing in the function. It would save\nhaving to acquire a lock in cxl_region_can_probe() above. Keep my reviewed-by either way,\nit's really just a minor optimization.\n> +\n>  \t/*\n>  \t * From this point on any path that changes the region's state away from\n>  \t * CXL_CONFIG_COMMIT is also responsible for releasing the driver.\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> CXL region creation involves allocating capacity from Device Physical\n> Address (DPA) and assigning it to decode a given Host Physical Address\n> (HPA). Before determining how much DPA to allocate the amount of available\n> HPA must be determined. Also, not all HPA is created equal, some HPA\n> targets RAM, some targets PMEM, some is prepared for device-memory flows\n> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n> \n> In order to support Type2 CXL devices, wrap all of those concerns into\n> an API that retrieves a root decoder (platform CXL window) that fits the\n> specified constraints and the capacity available for a new region.\n> \n> Add a complementary function for releasing the reference to such root\n> decoder.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> ---\n>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h         |   3 +\n>  include/cxl/cxl.h         |   6 ++\n>  3 files changed, 173 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 954b8fcdbac6..bdefd088f5f1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>  \treturn 0;\n>  }\n>  \n> +struct cxlrd_max_context {\n> +\tstruct device * const *host_bridges;\n> +\tint interleave_ways;\n> +\tunsigned long flags;\n> +\tresource_size_t max_hpa;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +};\n> +\n> +static int find_max_hpa(struct device *dev, void *data)\n> +{\n> +\tstruct cxlrd_max_context *ctx = data;\n> +\tstruct cxl_switch_decoder *cxlsd;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +\tstruct resource *res, *prev;\n> +\tstruct cxl_decoder *cxld;\n> +\tresource_size_t free = 0;\n> +\tresource_size_t max;\n> +\tint found = 0;\n> +\n> +\tif (!is_root_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxlrd = to_cxl_root_decoder(dev);\n> +\tcxlsd = &cxlrd->cxlsd;\n> +\tcxld = &cxlsd->cxld;\n> +\n> +\tif ((cxld->flags & ctx->flags) != ctx->flags) {\n> +\t\tdev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n> +\t\t\tcxld->flags, ctx->flags);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\tfor (int i = 0; i < ctx->interleave_ways; i++) {\n> +\t\tfor (int j = 0; j < ctx->interleave_ways; j++) {\n> +\t\t\tif (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n> +\t\t\t\tfound++;\n> +\t\t\t\tbreak;\n> +\t\t\t}\n> +\t\t}\n> +\t}\n\nThis may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\nwhat the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\nbridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\npoint, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\nbe a struct device * const.\n\nMaybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\nI'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n> +\n> +\tif (found != ctx->interleave_ways) {\n> +\t\tdev_dbg(dev,\n> +\t\t\t\"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n> +\t\t\tfound, ctx->interleave_ways);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\t/*\n> +\t * Walk the root decoder resource range relying on cxl_rwsem.region to\n> +\t * preclude sibling arrival/departure and find the largest free space\n> +\t * gap.\n> +\t */\n> +\tlockdep_assert_held_read(&cxl_rwsem.region);\n> +\tres = cxlrd->res->child;\n> +\n> +\t/* With no resource child the whole parent resource is available */\n> +\tif (!res)\n> +\t\tmax = resource_size(cxlrd->res);\n> +\telse\n> +\t\tmax = 0;\n> +\n> +\tfor (prev = NULL; res; prev = res, res = res->sibling) {\n> +\t\tif (!prev && res->start == cxlrd->res->start &&\n> +\t\t    res->end == cxlrd->res->end) {\n> +\t\t\tmax = resource_size(cxlrd->res);\n> +\t\t\tbreak;\n> +\t\t}\n> +\t\t/*\n> +\t\t * Sanity check for preventing arithmetic problems below as a\n> +\t\t * resource with size 0 could imply using the end field below\n> +\t\t * when set to unsigned zero - 1 or all f in hex.\n> +\t\t */\n> +\t\tif (prev && !resource_size(prev))\n> +\t\t\tcontinue;\n> +\n> +\t\tif (!prev && res->start > cxlrd->res->start) {\n> +\t\t\tfree = res->start - cxlrd->res->start;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t\tif (prev && res->start > prev->end + 1) {\n> +\t\t\tfree = res->start - prev->end + 1;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t}\n> +\n> +\tif (prev && prev->end + 1 < cxlrd->res->end + 1) {\n> +\t\tfree = cxlrd->res->end + 1 - prev->end + 1;\n> +\t\tmax = max(free, max);\n> +\t}\n> +\n> +\tdev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n> +\tif (max > ctx->max_hpa) {\n> +\t\tif (ctx->cxlrd)\n> +\t\t\tput_device(cxlrd_dev(ctx->cxlrd));\n> +\t\tget_device(cxlrd_dev(cxlrd));\n> +\t\tctx->cxlrd = cxlrd;\n> +\t\tctx->max_hpa = max;\n> +\t}\n> +\treturn 0;\n> +}\n> +\n> +/**\n> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n> + * @cxlmd: the mem device requiring the HPA\n> + * @interleave_ways: number of entries in @host_bridges\n> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n> + *\t\t      returned decoder\n> + *\n> + * Returns a pointer to a struct cxl_root_decoder\n> + *\n> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n> + * caller goes to use this decoder and its capacity is reduced then caller needs\n> + * to loop and retry.\n> + *\n> + * The returned root decoder has an elevated reference count that needs to be\n> + * put with cxl_put_root_decoder(cxlrd).\n> + */\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max_avail_contig)\n> +{\n> +\tstruct cxlrd_max_context ctx = {\n> +\t\t.flags = flags,\n> +\t\t.interleave_ways = interleave_ways,\n> +\t};\n> +\tstruct cxl_port *root_port;\n> +\tstruct cxl_port *endpoint;\n> +\n> +\tendpoint = cxlmd->endpoint;\n> +\tif (!endpoint) {\n> +\t\tdev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\tctx.host_bridges = &endpoint->host_bridge;\n\nMentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\nsomething). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\nI'm not sure of what the fix is to get the intended behavior.\n\nIt may be worth getting rid of the interleave_ways portion of this function and\nadd it later when someone needs it. You could also explain it's hard coded to 1/unused\nin the doc comment if you know of an immediate need for it.\n\n> +\n> +\tstruct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n> +\tif (!root) {\n> +\t\tdev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\troot_port = &root->port;\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.region)\n> +\t\tdevice_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n\nCan just use a guard() here.\n\n> +\n> +\tif (!ctx.cxlrd)\n> +\t\treturn ERR_PTR(-ENOMEM);\n> +\n> +\t*max_avail_contig = ctx.max_hpa;\n> +\treturn ctx.cxlrd;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n> +\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n> +{\n> +\tput_device(cxlrd_dev(cxlrd));\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n> +\n>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>  \t\t\t  const char *buf, size_t len)\n>  {\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index 944c5d1ccceb..c7d9b2c2908f 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>  bool is_root_decoder(struct device *dev);\n> +\n> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n> +\n>  bool is_switch_decoder(struct device *dev);\n>  bool is_endpoint_decoder(struct device *dev);\n>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 92880c26b2d5..834dc7e78934 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>  struct range;\n>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>  void cxl_unregister_region(struct cxl_region *cxlr);\n> +struct cxl_port;\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max);\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Check if device HDM is already committed during firmware/BIOS\n> initialization.\n> \n> A CXL region should exist if so after memdev allocation/initialization.\n> Get HPA from region and map it.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/net/ethernet/sfc/efx_cxl.c | 28 +++++++++++++++++++++++++++-\n>  1 file changed, 27 insertions(+), 1 deletion(-)\n> \n> diff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\n> index a77ef4783fcb..3536eccf1b2a 100644\n> --- a/drivers/net/ethernet/sfc/efx_cxl.c\n> +++ b/drivers/net/ethernet/sfc/efx_cxl.c\n> @@ -19,6 +19,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \tstruct efx_nic *efx = &probe_data->efx;\n>  \tstruct pci_dev *pci_dev = efx->pci_dev;\n>  \tstruct efx_cxl *cxl;\n> +\tstruct range range;\n>  \tu16 dvsec;\n>  \tint rc;\n>  \n> @@ -90,13 +91,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \t\treturn PTR_ERR(cxl->cxlmd);\n>  \t}\n>  \n> -\tprobe_data->cxl = cxl;\n> +\tcxl->cxled = cxl_get_committed_decoder(cxl->cxlmd, &cxl->efx_region);\n> +\tif (cxl->cxled) {\n> +\t\tif (!cxl->efx_region) {\n> +\t\t\tpci_err(pci_dev, \"CXL found committed decoder without a region\");\n> +\t\t\treturn -ENODEV;\n> +\t\t}\n> +\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n\nMissing an empty line above.\n\n> +\t\tif (rc) {\n> +\t\t\tpci_err(pci_dev,\n> +\t\t\t\t\"CXL getting regions params from a committed decoder failed\");\n> +\t\t\treturn rc;\n> +\t\t}\n> +\n> +\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n\nMaybe use range_len() instead for the second parameter?\n\n> +\t\tif (!cxl->ctpio_cxl) {\n> +\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n> +\t\t\treturn -ENOMEM;\n> +\t\t}\n> +\n> +\t\tprobe_data->cxl = cxl;\n> +\t}\n>  \n>  \treturn 0;\n>  }\n>  \n>  void efx_cxl_exit(struct efx_probe_data *probe_data)\n>  {\n> +\tif (!probe_data->cxl)\n> +\t\treturn;\n> +\n> +\tiounmap(probe_data->cxl->ctpio_cxl);\n> +\tcxl_unregister_region(probe_data->cxl->efx_region);\n>  }\n>  \n>  MODULE_IMPORT_NS(\"CXL\");\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation involves finding available DPA (device-physical-address)\n> capacity to map into HPA (host-physical-address) space.\n> \n> In order to support CXL Type2 devices, define an API, cxl_request_dpa(),\n> that tries to allocate the DPA memory the driver requires to operate.The\n> memory requested should not be bigger than the max available HPA obtained\n> previously with cxl_get_hpa_freespace().\n> \n> Based on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h      |  1 +\n>  include/cxl/cxl.h      |  5 +++\n>  3 files changed, 90 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index a172ce4e9b19..d60a697f12cc 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -3,6 +3,7 @@\n>  #include <linux/seq_file.h>\n>  #include <linux/device.h>\n>  #include <linux/delay.h>\n> +#include <cxl/cxl.h>\n>  \n>  #include \"cxlmem.h\"\n>  #include \"core.h\"\n> @@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n>  \treturn resource_contains(res, &_addr);\n>  }\n>  \n> +/**\n> + * cxl_dpa_free - release DPA (Device Physical Address)\n> + * @cxled: endpoint decoder linked to the DPA\n> + *\n> + * Returns 0 or error.\n> + */\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_port *port = cxled_to_port(cxled);\n> @@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  \tdevm_cxl_dpa_release(cxled);\n>  \treturn 0;\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n>  \n>  int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \t\t     enum cxl_partition_mode mode)\n> @@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \treturn 0;\n>  }\n>  \n> +static int find_free_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == (port->hdm_end + 1);\n> +}\n> +\n> +static struct cxl_endpoint_decoder *\n> +cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct device *dev;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tdev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\tfind_free_decoder);\n> +\tif (!dev)\n> +\t\treturn NULL;\n> +\n> +\treturn to_cxl_endpoint_decoder(dev);\n> +}\n> +\n> +/**\n> + * cxl_request_dpa - search and reserve DPA given input constraints\n> + * @cxlmd: memdev with an endpoint port with available decoders\n> + * @mode: CXL partition mode (ram vs pmem)\n> + * @alloc: dpa size required\n> + *\n> + * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n> + * an errno encoded pointer on failure.\n> + *\n> + * Given that a region needs to allocate from limited HPA capacity it\n> + * may be the case that a device has more mappable DPA capacity than\n> + * available HPA. The expectation is that @alloc is a driver known\n> + * value based on the device capacity but which could not be fully\n> + * available due to HPA constraints.\n> + *\n> + * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n> + * reserved, or an error pointer. The caller is also expected to own the\n> + * lifetime of the memdev registration associated with the endpoint to\n> + * pin the decoder registered as well.\n> + */\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc)\n> +{\n> +\tint rc;\n> +\n> +\tif (!IS_ALIGNED(alloc, SZ_256M))\n> +\t\treturn ERR_PTR(-EINVAL);\n> +\n> +\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n> +\t\tcxl_find_free_decoder(cxlmd);\n> +\n> +\tif (!cxled)\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\n> +\trc = cxl_dpa_set_part(cxled, mode);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = cxl_dpa_alloc(cxled, alloc);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n\nShould cxl_dpa_set_part() be unwound here, or does it not matter? If it doesn't matter:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\n> +\treturn no_free_ptr(cxled);\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n> +\n>  static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index d1b010e5e1d0..2b1f7d687a0e 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n>  \n>  DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n>  DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n> +DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n>  DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n>  DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 783ad570a6eb..4802371db00e 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -7,6 +7,7 @@\n>  \n>  #include <linux/node.h>\n>  #include <linux/ioport.h>\n> +#include <linux/range.h>\n>  #include <cxl/mailbox.h>\n>  \n>  /**\n> @@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t       unsigned long flags,\n>  \t\t\t\t\t       resource_size_t *max);\n>  void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc);\n> +int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Current code is expecting Type3 or CXL_DECODER_HOSTONLYMEM devices only.\n> Support for Type2 implies region type needs to be based on the endpoint\n> type HDM-D[B] instead.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> Reviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n> ---\n>  drivers/cxl/core/region.c | 10 ++++++----\n>  1 file changed, 6 insertions(+), 4 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index bdefd088f5f1..f53b2e9fd9e6 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2833,7 +2833,8 @@ static ssize_t create_ram_region_show(struct device *dev,\n>  }\n>  \n>  static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n> -\t\t\t\t\t  enum cxl_partition_mode mode, int id)\n> +\t\t\t\t\t  enum cxl_partition_mode mode, int id,\n> +\t\t\t\t\t  enum cxl_decoder_type target_type)\n>  {\n>  \tint rc;\n>  \n> @@ -2855,7 +2856,7 @@ static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n>  \t\treturn ERR_PTR(-EBUSY);\n>  \t}\n>  \n> -\treturn devm_cxl_add_region(cxlrd, id, mode, CXL_DECODER_HOSTONLYMEM);\n> +\treturn devm_cxl_add_region(cxlrd, id, mode, target_type);\n>  }\n>  \n>  static ssize_t create_region_store(struct device *dev, const char *buf,\n> @@ -2869,7 +2870,7 @@ static ssize_t create_region_store(struct device *dev, const char *buf,\n>  \tif (rc != 1)\n>  \t\treturn -EINVAL;\n>  \n> -\tcxlr = __create_region(cxlrd, mode, id);\n> +\tcxlr = __create_region(cxlrd, mode, id, CXL_DECODER_HOSTONLYMEM);\n\nI haven't read the ABI docs, but would it be worthwhile to update the documentation for this attribute\nto mention it only makes type 3 regions? I'm flip-flopping on whether it's worth the trouble but thought\nI should mention it.\n\nEither way:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n\n>  \tif (IS_ERR(cxlr))\n>  \t\treturn PTR_ERR(cxlr);\n>  \n> @@ -4036,7 +4037,8 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \n>  \tdo {\n>  \t\tcxlr = __create_region(cxlrd, cxlds->part[part].mode,\n> -\t\t\t\t       atomic_read(&cxlrd->region_id));\n> +\t\t\t\t       atomic_read(&cxlrd->region_id),\n> +\t\t\t\t       cxled->cxld.target_type);\n>  \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n>  \n>  \tif (IS_ERR(cxlr)) {\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation based on Type3 devices is triggered from user space\n> allowing memory combination through interleaving.\n> \n> In preparation for kernel driven region creation, that is Type2 drivers\n> triggering region creation backed with its advertised CXL memory, factor\n> out a common helper from the user-sysfs region setup for interleave ways.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n>  1 file changed, 27 insertions(+), 16 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index f53b2e9fd9e6..ece1d3df7cf1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n>  \n>  static const struct attribute_group *get_cxl_region_target_group(void);\n>  \n> -static ssize_t interleave_ways_store(struct device *dev,\n> -\t\t\t\t     struct device_attribute *attr,\n> -\t\t\t\t     const char *buf, size_t len)\n> +static int set_interleave_ways(struct cxl_region *cxlr, int val)\n\n@val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\nfunction was originally coded with that in mind (same with @save below). With that cleaned up:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n\n>  {\n> -\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>  \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> -\tstruct cxl_region *cxlr = to_cxl_region(dev);\n>  \tstruct cxl_region_params *p = &cxlr->params;\n> -\tunsigned int val, save;\n> -\tint rc;\n> +\tint save, rc;\n>  \tu8 iw;\n>  \n> -\trc = kstrtouint(buf, 0, &val);\n> -\tif (rc)\n> -\t\treturn rc;\n> -\n>  \trc = ways_to_eiw(val, &iw);\n>  \tif (rc)\n>  \t\treturn rc;\n> @@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n>  \t\treturn -EINVAL;\n>  \t}\n>  \n> -\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> -\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> -\t\treturn rc;\n> +\tlockdep_assert_held_write(&cxl_rwsem.region);\n>  \n>  \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>  \t\treturn -EBUSY;\n> @@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n>  \tsave = p->interleave_ways;\n>  \tp->interleave_ways = val;\n>  \trc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n> -\tif (rc) {\n> +\tif (rc)\n>  \t\tp->interleave_ways = save;\n> +\n> +\treturn rc;\n> +}\n> +\n> +static ssize_t interleave_ways_store(struct device *dev,\n> +\t\t\t\t     struct device_attribute *attr,\n> +\t\t\t\t     const char *buf, size_t len)\n> +{\n> +\tstruct cxl_region *cxlr = to_cxl_region(dev);\n> +\tunsigned int val;\n> +\tint rc;\n> +\n> +\trc = kstrtouint(buf, 0, &val);\n> +\tif (rc)\n> +\t\treturn rc;\n> +\n> +\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> +\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> +\t\treturn rc;\n> +\n> +\trc = set_interleave_ways(cxlr, val);\n> +\tif (rc)\n>  \t\treturn rc;\n> -\t}\n>  \n>  \treturn len;\n>  }\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation based on Type3 devices is triggered from user space\n> allowing memory combination through interleaving.\n> \n> In preparation for kernel driven region creation, that is Type2 drivers\n> triggering region creation backed with its advertised CXL memory, factor\n> out a common helper from the user-sysfs region setup forinterleave\n> granularity.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 39 +++++++++++++++++++++++++--------------\n>  1 file changed, 25 insertions(+), 14 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index ece1d3df7cf1..63c2aeb2ee1f 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -559,21 +559,14 @@ static ssize_t interleave_granularity_show(struct device *dev,\n>  \treturn sysfs_emit(buf, \"%d\\n\", p->interleave_granularity);\n>  }\n>  \n> -static ssize_t interleave_granularity_store(struct device *dev,\n> -\t\t\t\t\t    struct device_attribute *attr,\n> -\t\t\t\t\t    const char *buf, size_t len)\n> +static int set_interleave_granularity(struct cxl_region *cxlr, int val)\n\nSame thing as last patch. Assuming it's fixed:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>  {\n> -\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>  \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> -\tstruct cxl_region *cxlr = to_cxl_region(dev);\n>  \tstruct cxl_region_params *p = &cxlr->params;\n> -\tint rc, val;\n> +\tint rc;\n>  \tu16 ig;\n>  \n> -\trc = kstrtoint(buf, 0, &val);\n> -\tif (rc)\n> -\t\treturn rc;\n> -\n>  \trc = granularity_to_eig(val, &ig);\n>  \tif (rc)\n>  \t\treturn rc;\n> @@ -589,14 +582,32 @@ static ssize_t interleave_granularity_store(struct device *dev,\n>  \tif (cxld->interleave_ways > 1 && val != cxld->interleave_granularity)\n>  \t\treturn -EINVAL;\n>  \n> -\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> -\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> -\t\treturn rc;\n> -\n> +\tlockdep_assert_held_write(&cxl_rwsem.region);\n>  \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>  \t\treturn -EBUSY;\n>  \n>  \tp->interleave_granularity = val;\n> +\treturn 0;\n> +}\n> +\n> +static ssize_t interleave_granularity_store(struct device *dev,\n> +\t\t\t\t\t    struct device_attribute *attr,\n> +\t\t\t\t\t    const char *buf, size_t len)\n> +{\n> +\tstruct cxl_region *cxlr = to_cxl_region(dev);\n> +\tint rc, val;\n> +\n> +\trc = kstrtoint(buf, 0, &val);\n> +\tif (rc)\n> +\t\treturn rc;\n> +\n> +\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> +\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> +\t\treturn rc;\n> +\n> +\trc = set_interleave_granularity(cxlr, val);\n> +\tif (rc)\n> +\t\treturn rc;\n>  \n>  \treturn len;\n>  }\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Creating a CXL region requires userspace intervention through the cxl\n> sysfs files. Type2 support should allow accelerator drivers to create\n> such cxl region from kernel code.\n> \n> Adding that functionality and integrating it with current support for\n> memory expanders.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159835.1948938.1647215579839222774.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 131 ++++++++++++++++++++++++++++++++++++--\n>  include/cxl/cxl.h         |   3 +\n>  2 files changed, 127 insertions(+), 7 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 63c2aeb2ee1f..293e63dfef22 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2944,6 +2944,14 @@ cxl_find_region_by_name(struct cxl_root_decoder *cxlrd, const char *name)\n>  \treturn to_cxl_region(region_dev);\n>  }\n>  \n> +static void drop_region(struct cxl_region *cxlr)\n> +{\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n> +\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n> +\n> +\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n> +}\n> +\n>  static ssize_t delete_region_store(struct device *dev,\n>  \t\t\t\t   struct device_attribute *attr,\n>  \t\t\t\t   const char *buf, size_t len)\n> @@ -4047,14 +4055,12 @@ static int __construct_region(struct cxl_region *cxlr,\n>  \treturn 0;\n>  }\n>  \n> -/* Establish an empty region covering the given HPA range */\n> -static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n> -\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n> +static struct cxl_region *construct_region_begin(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t\t\t struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> -\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n>  \tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n> -\tint rc, part = READ_ONCE(cxled->part);\n> +\tint part = READ_ONCE(cxled->part);\n>  \tstruct cxl_region *cxlr;\n>  \n>  \tdo {\n> @@ -4063,13 +4069,26 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \t\t\t\t       cxled->cxld.target_type);\n>  \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n>  \n> -\tif (IS_ERR(cxlr)) {\n> +\tif (IS_ERR(cxlr))\n>  \t\tdev_err(cxlmd->dev.parent,\n>  \t\t\t\"%s:%s: %s failed assign region: %ld\\n\",\n>  \t\t\tdev_name(&cxlmd->dev), dev_name(&cxled->cxld.dev),\n>  \t\t\t__func__, PTR_ERR(cxlr));\n> +\n> +\treturn cxlr;\n> +}\n> +\n> +/* Establish an empty region covering the given HPA range */\n> +static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n> +{\n> +\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n> +\tstruct cxl_region *cxlr;\n> +\tint rc;\n> +\n> +\tcxlr = construct_region_begin(cxlrd, cxled);\n> +\tif (IS_ERR(cxlr))\n>  \t\treturn cxlr;\n> -\t}\n>  \n>  \trc = __construct_region(cxlr, cxlrd, cxled);\n>  \tif (rc) {\n> @@ -4080,6 +4099,104 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \treturn cxlr;\n>  }\n>  \n> +DEFINE_FREE(cxl_region_drop, struct cxl_region *, if (_T) drop_region(_T))\n\nThis needs to be \"if (!IS_ERR_OR_NULL(_T) drop_region(_T)\". If construct_region_begin() returns an\nerror pointer, drop_region() will be called with it as of now leading to a garbage pointer deref.\n\n> +\n> +static struct cxl_region *\n> +__construct_new_region(struct cxl_root_decoder *cxlrd,\n> +\t\t       struct cxl_endpoint_decoder **cxled, int ways)\n> +{\n> +\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled[0]);\n> +\tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> +\tstruct cxl_region_params *p;\n> +\tresource_size_t size = 0;\n> +\tint rc, i;\n> +\n> +\tstruct cxl_region *cxlr __free(cxl_region_drop) =\n> +\t\tconstruct_region_begin(cxlrd, cxled[0]);\n> +\tif (IS_ERR(cxlr))\n> +\t\treturn cxlr;\n> +\n> +\tguard(rwsem_write)(&cxl_rwsem.region);\n> +\n> +\t/*\n> +\t * Sanity check. This should not happen with an accel driver handling\n> +\t * the region creation.\n> +\t */\n> +\tp = &cxlr->params;\n> +\tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE) {\n> +\t\tdev_err(cxlmd->dev.parent,\n> +\t\t\t\"%s:%s: %s  unexpected region state\\n\",\n> +\t\t\tdev_name(&cxlmd->dev), dev_name(&cxled[0]->cxld.dev),\n> +\t\t\t__func__);\n> +\t\treturn ERR_PTR(-EBUSY);\n> +\t}\n> +\n> +\trc = set_interleave_ways(cxlr, ways);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = set_interleave_granularity(cxlr, cxld->interleave_granularity);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.dpa) {\n> +\t\tfor (i = 0; i < ways; i++) {\n> +\t\t\tif (!cxled[i]->dpa_res)\n> +\t\t\t\treturn ERR_PTR(-EINVAL);\n> +\t\t\tsize += resource_size(cxled[i]->dpa_res);\n> +\t\t}\n> +\n> +\t\trc = alloc_hpa(cxlr, size);\n> +\t\tif (rc)\n> +\t\t\treturn ERR_PTR(rc);\n> +\n> +\t\tfor (i = 0; i < ways; i++) {\n> +\t\t\trc = cxl_region_attach(cxlr, cxled[i], 0);\n\nPosition parameter is hardcoded to 0. It should be set to i, right? This kind of goes back to my\nissues in patch 12/22; the interleaving functionality is there but it looks unused.\n\n> +\t\t\tif (rc)\n> +\t\t\t\treturn ERR_PTR(rc);\n> +\t\t}\n> +\t}\n> +\n> +\trc = cxl_region_decode_commit(cxlr);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\tp->state = CXL_CONFIG_COMMIT;\n> +\n> +\treturn no_free_ptr(cxlr);\n> +}\n> +\n> +/**\n> + * cxl_create_region - Establish a region given an endpoint decoder\n> + * @cxlrd: root decoder to allocate HPA\n> + * @cxled: endpoint decoders with reserved DPA capacity\n> + * @ways: interleave ways required\n> + *\n> + * Returns a fully formed region in the commit state and attached to the\n> + * cxl_region driver.\n> + */\n> +struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n> +\t\t\t\t     int ways)\n> +{\n> +\tstruct cxl_region *cxlr;\n> +\n> +\tmutex_lock(&cxlrd->range_lock);\n> +\tcxlr = __construct_new_region(cxlrd, cxled, ways);\n> +\tmutex_unlock(&cxlrd->range_lock);\n> +\tif (IS_ERR(cxlr))\n> +\t\treturn cxlr;\n> +\n> +\tif (device_attach(&cxlr->dev) <= 0) {\n> +\t\tdev_err(&cxlr->dev, \"failed to create region\\n\");\n> +\t\tdrop_region(cxlr);\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\t}\n> +\n> +\treturn cxlr;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_create_region, \"CXL\");\n> +\n>  static struct cxl_region *\n>  cxl_find_region_by_range(struct cxl_root_decoder *cxlrd, struct range *hpa)\n>  {\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 4802371db00e..50acbd13bcf8 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -281,4 +281,7 @@ struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t     enum cxl_partition_mode mode,\n>  \t\t\t\t\t     resource_size_t alloc);\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n> +struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n> +\t\t\t\t     int ways);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Differentiate CXL memory expanders (type 3) from CXL device accelerators\n> (type 2) with a new function for initializing cxl_dev_state and a macro\n> for helping accel drivers to embed cxl_dev_state inside a private\n> struct.\n> \n> Move structs to include/cxl as the size of the accel driver private\n> struct embedding cxl_dev_state needs to know the size of this struct.\n> \n> Use same new initialization with the type3 pci driver.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> ---\n>  drivers/cxl/core/mbox.c      |  12 +-\n>  drivers/cxl/core/memdev.c    |  32 +++++\n>  drivers/cxl/cxl.h            |  97 +--------------\n>  drivers/cxl/cxlmem.h         |  86 +------------\n>  drivers/cxl/pci.c            |  14 +--\n>  include/cxl/cxl.h            | 226 +++++++++++++++++++++++++++++++++++\n>  tools/testing/cxl/test/mem.c |   3 +-\n>  7 files changed, 274 insertions(+), 196 deletions(-)\n>  create mode 100644 include/cxl/cxl.h\n> \n> diff --git a/drivers/cxl/core/mbox.c b/drivers/cxl/core/mbox.c\n> index fa6dd0c94656..bee84d0101d1 100644\n> --- a/drivers/cxl/core/mbox.c\n> +++ b/drivers/cxl/core/mbox.c\n> @@ -1514,23 +1514,21 @@ int cxl_mailbox_init(struct cxl_mailbox *cxl_mbox, struct device *host)\n>  }\n>  EXPORT_SYMBOL_NS_GPL(cxl_mailbox_init, \"CXL\");\n>  \n> -struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev)\n> +struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n> +\t\t\t\t\t\t u16 dvsec)\n>  {\n>  \tstruct cxl_memdev_state *mds;\n>  \tint rc;\n>  \n> -\tmds = devm_kzalloc(dev, sizeof(*mds), GFP_KERNEL);\n> +\tmds = devm_cxl_dev_state_create(dev, CXL_DEVTYPE_CLASSMEM, serial,\n> +\t\t\t\t\tdvsec, struct cxl_memdev_state, cxlds,\n> +\t\t\t\t\ttrue);\n>  \tif (!mds) {\n>  \t\tdev_err(dev, \"No memory available\\n\");\n>  \t\treturn ERR_PTR(-ENOMEM);\n>  \t}\n>  \n>  \tmutex_init(&mds->event.log_lock);\n> -\tmds->cxlds.dev = dev;\n> -\tmds->cxlds.reg_map.host = dev;\n> -\tmds->cxlds.cxl_mbox.host = dev;\n> -\tmds->cxlds.reg_map.resource = CXL_RESOURCE_NONE;\n> -\tmds->cxlds.type = CXL_DEVTYPE_CLASSMEM;\n>  \n>  \trc = devm_cxl_register_mce_notifier(dev, &mds->mce_notifier);\n>  \tif (rc == -EOPNOTSUPP)\n> diff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\n> index af3d0cc65138..22d156f25305 100644\n> --- a/drivers/cxl/core/memdev.c\n> +++ b/drivers/cxl/core/memdev.c\n> @@ -656,6 +656,38 @@ static void detach_memdev(struct work_struct *work)\n>  \n>  static struct lock_class_key cxl_memdev_key;\n>  \n> +static void cxl_dev_state_init(struct cxl_dev_state *cxlds, struct device *dev,\n> +\t\t\t       enum cxl_devtype type, u64 serial, u16 dvsec,\n> +\t\t\t       bool has_mbox)\n> +{\n> +\t*cxlds = (struct cxl_dev_state) {\n> +\t\t.dev = dev,\n> +\t\t.type = type,\n> +\t\t.serial = serial,\n> +\t\t.cxl_dvsec = dvsec,\n> +\t\t.reg_map.host = dev,\n> +\t\t.reg_map.resource = CXL_RESOURCE_NONE,\n> +\t};\n> +\n> +\tif (has_mbox)\n> +\t\tcxlds->cxl_mbox.host = dev;\n> +}\n> +\n> +struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n> +\t\t\t\t\t\t enum cxl_devtype type,\n> +\t\t\t\t\t\t u64 serial, u16 dvsec,\n> +\t\t\t\t\t\t size_t size, bool has_mbox)\n> +{\n> +\tstruct cxl_dev_state *cxlds = devm_kzalloc(dev, size, GFP_KERNEL);\n> +\n> +\tif (!cxlds)\n> +\t\treturn NULL;\n> +\n> +\tcxl_dev_state_init(cxlds, dev, type, serial, dvsec, has_mbox);\n\nNit: Having a second function to do the init seems overkill here, especially since cxl_dev_state_init() isn't called outside this\nfunction. I'd fold it into this function instead, but I'm fine with it either way (especially if you were told otherwise before).\n\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\treturn cxlds;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(_devm_cxl_dev_state_create, \"CXL\");\n> +\n>  static struct cxl_memdev *cxl_memdev_alloc(struct cxl_dev_state *cxlds,\n>  \t\t\t\t\t   const struct file_operations *fops,\n>  \t\t\t\t\t   const struct cxl_memdev_attach *attach)\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index e1d47062e1d3..3eaa353e430b 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -12,6 +12,7 @@\n>  #include <linux/node.h>\n>  #include <linux/io.h>\n>  #include <linux/range.h>\n> +#include <cxl/cxl.h>\n>  \n>  extern const struct nvdimm_security_ops *cxl_security_ops;\n>  \n> @@ -201,97 +202,6 @@ static inline int ways_to_eiw(unsigned int ways, u8 *eiw)\n>  #define   CXLDEV_MBOX_BG_CMD_COMMAND_VENDOR_MASK GENMASK_ULL(63, 48)\n>  #define CXLDEV_MBOX_PAYLOAD_OFFSET 0x20\n>  \n> -/*\n> - * Using struct_group() allows for per register-block-type helper routines,\n> - * without requiring block-type agnostic code to include the prefix.\n> - */\n> -struct cxl_regs {\n> -\t/*\n> -\t * Common set of CXL Component register block base pointers\n> -\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n> -\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n> -\t */\n> -\tstruct_group_tagged(cxl_component_regs, component,\n> -\t\tvoid __iomem *hdm_decoder;\n> -\t\tvoid __iomem *ras;\n> -\t);\n> -\t/*\n> -\t * Common set of CXL Device register block base pointers\n> -\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n> -\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n> -\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n> -\t */\n> -\tstruct_group_tagged(cxl_device_regs, device_regs,\n> -\t\tvoid __iomem *status, *mbox, *memdev;\n> -\t);\n> -\n> -\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n> -\t\tvoid __iomem *pmu;\n> -\t);\n> -\n> -\t/*\n> -\t * RCH downstream port specific RAS register\n> -\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n> -\t */\n> -\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n> -\t\tvoid __iomem *dport_aer;\n> -\t);\n> -\n> -\t/*\n> -\t * RCD upstream port specific PCIe cap register\n> -\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n> -\t */\n> -\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n> -\t\tvoid __iomem *rcd_pcie_cap;\n> -\t);\n> -};\n> -\n> -struct cxl_reg_map {\n> -\tbool valid;\n> -\tint id;\n> -\tunsigned long offset;\n> -\tunsigned long size;\n> -};\n> -\n> -struct cxl_component_reg_map {\n> -\tstruct cxl_reg_map hdm_decoder;\n> -\tstruct cxl_reg_map ras;\n> -};\n> -\n> -struct cxl_device_reg_map {\n> -\tstruct cxl_reg_map status;\n> -\tstruct cxl_reg_map mbox;\n> -\tstruct cxl_reg_map memdev;\n> -};\n> -\n> -struct cxl_pmu_reg_map {\n> -\tstruct cxl_reg_map pmu;\n> -};\n> -\n> -/**\n> - * struct cxl_register_map - DVSEC harvested register block mapping parameters\n> - * @host: device for devm operations and logging\n> - * @base: virtual base of the register-block-BAR + @block_offset\n> - * @resource: physical resource base of the register block\n> - * @max_size: maximum mapping size to perform register search\n> - * @reg_type: see enum cxl_regloc_type\n> - * @component_map: cxl_reg_map for component registers\n> - * @device_map: cxl_reg_maps for device registers\n> - * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n> - */\n> -struct cxl_register_map {\n> -\tstruct device *host;\n> -\tvoid __iomem *base;\n> -\tresource_size_t resource;\n> -\tresource_size_t max_size;\n> -\tu8 reg_type;\n> -\tunion {\n> -\t\tstruct cxl_component_reg_map component_map;\n> -\t\tstruct cxl_device_reg_map device_map;\n> -\t\tstruct cxl_pmu_reg_map pmu_map;\n> -\t};\n> -};\n> -\n>  void cxl_probe_component_regs(struct device *dev, void __iomem *base,\n>  \t\t\t      struct cxl_component_reg_map *map);\n>  void cxl_probe_device_regs(struct device *dev, void __iomem *base,\n> @@ -497,11 +407,6 @@ struct cxl_region_params {\n>  \tresource_size_t cache_size;\n>  };\n>  \n> -enum cxl_partition_mode {\n> -\tCXL_PARTMODE_RAM,\n> -\tCXL_PARTMODE_PMEM,\n> -};\n> -\n>  /*\n>   * Indicate whether this region has been assembled by autodetection or\n>   * userspace assembly. Prevent endpoint decoders outside of automatic\n> diff --git a/drivers/cxl/cxlmem.h b/drivers/cxl/cxlmem.h\n> index ef202b34e5ea..281546de426e 100644\n> --- a/drivers/cxl/cxlmem.h\n> +++ b/drivers/cxl/cxlmem.h\n> @@ -113,8 +113,6 @@ int devm_cxl_dpa_reserve(struct cxl_endpoint_decoder *cxled,\n>  \t\t\t resource_size_t base, resource_size_t len,\n>  \t\t\t resource_size_t skipped);\n>  \n> -#define CXL_NR_PARTITIONS_MAX 2\n> -\n>  struct cxl_dpa_info {\n>  \tu64 size;\n>  \tstruct cxl_dpa_part_info {\n> @@ -373,87 +371,6 @@ struct cxl_security_state {\n>  \tstruct kernfs_node *sanitize_node;\n>  };\n>  \n> -/*\n> - * enum cxl_devtype - delineate type-2 from a generic type-3 device\n> - * @CXL_DEVTYPE_DEVMEM - Vendor specific CXL Type-2 device implementing HDM-D or\n> - *\t\t\t HDM-DB, no requirement that this device implements a\n> - *\t\t\t mailbox, or other memory-device-standard manageability\n> - *\t\t\t flows.\n> - * @CXL_DEVTYPE_CLASSMEM - Common class definition of a CXL Type-3 device with\n> - *\t\t\t   HDM-H and class-mandatory memory device registers\n> - */\n> -enum cxl_devtype {\n> -\tCXL_DEVTYPE_DEVMEM,\n> -\tCXL_DEVTYPE_CLASSMEM,\n> -};\n> -\n> -/**\n> - * struct cxl_dpa_perf - DPA performance property entry\n> - * @dpa_range: range for DPA address\n> - * @coord: QoS performance data (i.e. latency, bandwidth)\n> - * @cdat_coord: raw QoS performance data from CDAT\n> - * @qos_class: QoS Class cookies\n> - */\n> -struct cxl_dpa_perf {\n> -\tstruct range dpa_range;\n> -\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n> -\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n> -\tint qos_class;\n> -};\n> -\n> -/**\n> - * struct cxl_dpa_partition - DPA partition descriptor\n> - * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n> - * @perf: performance attributes of the partition from CDAT\n> - * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n> - */\n> -struct cxl_dpa_partition {\n> -\tstruct resource res;\n> -\tstruct cxl_dpa_perf perf;\n> -\tenum cxl_partition_mode mode;\n> -};\n> -\n> -/**\n> - * struct cxl_dev_state - The driver device state\n> - *\n> - * cxl_dev_state represents the CXL driver/device state.  It provides an\n> - * interface to mailbox commands as well as some cached data about the device.\n> - * Currently only memory devices are represented.\n> - *\n> - * @dev: The device associated with this CXL state\n> - * @cxlmd: The device representing the CXL.mem capabilities of @dev\n> - * @reg_map: component and ras register mapping parameters\n> - * @regs: Parsed register blocks\n> - * @cxl_dvsec: Offset to the PCIe device DVSEC\n> - * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n> - * @media_ready: Indicate whether the device media is usable\n> - * @dpa_res: Overall DPA resource tree for the device\n> - * @part: DPA partition array\n> - * @nr_partitions: Number of DPA partitions\n> - * @serial: PCIe Device Serial Number\n> - * @type: Generic Memory Class device or Vendor Specific Memory device\n> - * @cxl_mbox: CXL mailbox context\n> - * @cxlfs: CXL features context\n> - */\n> -struct cxl_dev_state {\n> -\tstruct device *dev;\n> -\tstruct cxl_memdev *cxlmd;\n> -\tstruct cxl_register_map reg_map;\n> -\tstruct cxl_regs regs;\n> -\tint cxl_dvsec;\n> -\tbool rcd;\n> -\tbool media_ready;\n> -\tstruct resource dpa_res;\n> -\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n> -\tunsigned int nr_partitions;\n> -\tu64 serial;\n> -\tenum cxl_devtype type;\n> -\tstruct cxl_mailbox cxl_mbox;\n> -#ifdef CONFIG_CXL_FEATURES\n> -\tstruct cxl_features_state *cxlfs;\n> -#endif\n> -};\n> -\n>  static inline resource_size_t cxl_pmem_size(struct cxl_dev_state *cxlds)\n>  {\n>  \t/*\n> @@ -858,7 +775,8 @@ int cxl_dev_state_identify(struct cxl_memdev_state *mds);\n>  int cxl_await_media_ready(struct cxl_dev_state *cxlds);\n>  int cxl_enumerate_cmds(struct cxl_memdev_state *mds);\n>  int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info);\n> -struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev);\n> +struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n> +\t\t\t\t\t\t u16 dvsec);\n>  void set_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n>  \t\t\t\tunsigned long *cmds);\n>  void clear_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n> diff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\n> index 1cf232220873..24179cc702bf 100644\n> --- a/drivers/cxl/pci.c\n> +++ b/drivers/cxl/pci.c\n> @@ -911,25 +911,25 @@ static int cxl_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n>  \tint rc, pmu_count;\n>  \tunsigned int i;\n>  \tbool irq_avail;\n> +\tu16 dvsec;\n>  \n>  \trc = pcim_enable_device(pdev);\n>  \tif (rc)\n>  \t\treturn rc;\n>  \tpci_set_master(pdev);\n>  \n> -\tmds = cxl_memdev_state_create(&pdev->dev);\n> +\tdvsec = pci_find_dvsec_capability(pdev, PCI_VENDOR_ID_CXL,\n> +\t\t\t\t\t  PCI_DVSEC_CXL_DEVICE);\n> +\tif (!dvsec)\n> +\t\tpci_warn(pdev, \"Device DVSEC not present, skip CXL.mem init\\n\");\n> +\n> +\tmds = cxl_memdev_state_create(&pdev->dev, pci_get_dsn(pdev), dvsec);\n>  \tif (IS_ERR(mds))\n>  \t\treturn PTR_ERR(mds);\n>  \tcxlds = &mds->cxlds;\n>  \tpci_set_drvdata(pdev, cxlds);\n>  \n>  \tcxlds->rcd = is_cxl_restricted(pdev);\n> -\tcxlds->serial = pci_get_dsn(pdev);\n> -\tcxlds->cxl_dvsec = pci_find_dvsec_capability(\n> -\t\tpdev, PCI_VENDOR_ID_CXL, PCI_DVSEC_CXL_DEVICE);\n> -\tif (!cxlds->cxl_dvsec)\n> -\t\tdev_warn(&pdev->dev,\n> -\t\t\t \"Device DVSEC not present, skip CXL.mem init\\n\");\n>  \n>  \trc = cxl_pci_setup_regs(pdev, CXL_REGLOC_RBI_MEMDEV, &map);\n>  \tif (rc)\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> new file mode 100644\n> index 000000000000..13d448686189\n> --- /dev/null\n> +++ b/include/cxl/cxl.h\n> @@ -0,0 +1,226 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +/* Copyright(c) 2020 Intel Corporation. */\n> +/* Copyright(c) 2025 Advanced Micro Devices, Inc. */\n> +\n> +#ifndef __CXL_CXL_H__\n> +#define __CXL_CXL_H__\n> +\n> +#include <linux/node.h>\n> +#include <linux/ioport.h>\n> +#include <cxl/mailbox.h>\n> +\n> +/**\n> + * enum cxl_devtype - delineate type-2 from a generic type-3 device\n> + * @CXL_DEVTYPE_DEVMEM: Vendor specific CXL Type-2 device implementing HDM-D or\n> + *\t\t\t HDM-DB, no requirement that this device implements a\n> + *\t\t\t mailbox, or other memory-device-standard manageability\n> + *\t\t\t flows.\n> + * @CXL_DEVTYPE_CLASSMEM: Common class definition of a CXL Type-3 device with\n> + *\t\t\t   HDM-H and class-mandatory memory device registers\n> + */\n> +enum cxl_devtype {\n> +\tCXL_DEVTYPE_DEVMEM,\n> +\tCXL_DEVTYPE_CLASSMEM,\n> +};\n> +\n> +struct device;\n> +\n> +/*\n> + * Using struct_group() allows for per register-block-type helper routines,\n> + * without requiring block-type agnostic code to include the prefix.\n> + */\n> +struct cxl_regs {\n> +\t/*\n> +\t * Common set of CXL Component register block base pointers\n> +\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n> +\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n> +\t */\n> +\tstruct_group_tagged(cxl_component_regs, component,\n> +\t\tvoid __iomem *hdm_decoder;\n> +\t\tvoid __iomem *ras;\n> +\t);\n> +\t/*\n> +\t * Common set of CXL Device register block base pointers\n> +\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n> +\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n> +\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n> +\t */\n> +\tstruct_group_tagged(cxl_device_regs, device_regs,\n> +\t\tvoid __iomem *status, *mbox, *memdev;\n> +\t);\n> +\n> +\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n> +\t\tvoid __iomem *pmu;\n> +\t);\n> +\n> +\t/*\n> +\t * RCH downstream port specific RAS register\n> +\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n> +\t */\n> +\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n> +\t\tvoid __iomem *dport_aer;\n> +\t);\n> +\n> +\t/*\n> +\t * RCD upstream port specific PCIe cap register\n> +\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n> +\t */\n> +\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n> +\t\tvoid __iomem *rcd_pcie_cap;\n> +\t);\n> +};\n> +\n> +struct cxl_reg_map {\n> +\tbool valid;\n> +\tint id;\n> +\tunsigned long offset;\n> +\tunsigned long size;\n> +};\n> +\n> +struct cxl_component_reg_map {\n> +\tstruct cxl_reg_map hdm_decoder;\n> +\tstruct cxl_reg_map ras;\n> +};\n> +\n> +struct cxl_device_reg_map {\n> +\tstruct cxl_reg_map status;\n> +\tstruct cxl_reg_map mbox;\n> +\tstruct cxl_reg_map memdev;\n> +};\n> +\n> +struct cxl_pmu_reg_map {\n> +\tstruct cxl_reg_map pmu;\n> +};\n> +\n> +/**\n> + * struct cxl_register_map - DVSEC harvested register block mapping parameters\n> + * @host: device for devm operations and logging\n> + * @base: virtual base of the register-block-BAR + @block_offset\n> + * @resource: physical resource base of the register block\n> + * @max_size: maximum mapping size to perform register search\n> + * @reg_type: see enum cxl_regloc_type\n> + * @component_map: cxl_reg_map for component registers\n> + * @device_map: cxl_reg_maps for device registers\n> + * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n> + */\n> +struct cxl_register_map {\n> +\tstruct device *host;\n> +\tvoid __iomem *base;\n> +\tresource_size_t resource;\n> +\tresource_size_t max_size;\n> +\tu8 reg_type;\n> +\tunion {\n> +\t\tstruct cxl_component_reg_map component_map;\n> +\t\tstruct cxl_device_reg_map device_map;\n> +\t\tstruct cxl_pmu_reg_map pmu_map;\n> +\t};\n> +};\n> +\n> +/**\n> + * struct cxl_dpa_perf - DPA performance property entry\n> + * @dpa_range: range for DPA address\n> + * @coord: QoS performance data (i.e. latency, bandwidth)\n> + * @cdat_coord: raw QoS performance data from CDAT\n> + * @qos_class: QoS Class cookies\n> + */\n> +struct cxl_dpa_perf {\n> +\tstruct range dpa_range;\n> +\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n> +\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n> +\tint qos_class;\n> +};\n> +\n> +enum cxl_partition_mode {\n> +\tCXL_PARTMODE_RAM,\n> +\tCXL_PARTMODE_PMEM,\n> +};\n> +\n> +/**\n> + * struct cxl_dpa_partition - DPA partition descriptor\n> + * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n> + * @perf: performance attributes of the partition from CDAT\n> + * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n> + */\n> +struct cxl_dpa_partition {\n> +\tstruct resource res;\n> +\tstruct cxl_dpa_perf perf;\n> +\tenum cxl_partition_mode mode;\n> +};\n> +\n> +#define CXL_NR_PARTITIONS_MAX 2\n> +\n> +/**\n> + * struct cxl_dev_state - The driver device state\n> + *\n> + * cxl_dev_state represents the CXL driver/device state.  It provides an\n> + * interface to mailbox commands as well as some cached data about the device.\n> + * Currently only memory devices are represented.\n> + *\n> + * @dev: The device associated with this CXL state\n> + * @cxlmd: The device representing the CXL.mem capabilities of @dev\n> + * @reg_map: component and ras register mapping parameters\n> + * @regs: Parsed register blocks\n> + * @cxl_dvsec: Offset to the PCIe device DVSEC\n> + * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n> + * @media_ready: Indicate whether the device media is usable\n> + * @dpa_res: Overall DPA resource tree for the device\n> + * @part: DPA partition array\n> + * @nr_partitions: Number of DPA partitions\n> + * @serial: PCIe Device Serial Number\n> + * @type: Generic Memory Class device or Vendor Specific Memory device\n> + * @cxl_mbox: CXL mailbox context\n> + * @cxlfs: CXL features context\n> + */\n> +struct cxl_dev_state {\n> +\t/* public for Type2 drivers */\n> +\tstruct device *dev;\n> +\tstruct cxl_memdev *cxlmd;\n> +\n> +\t/* private for Type2 drivers */\n> +\tstruct cxl_register_map reg_map;\n> +\tstruct cxl_regs regs;\n> +\tint cxl_dvsec;\n> +\tbool rcd;\n> +\tbool media_ready;\n> +\tstruct resource dpa_res;\n> +\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n> +\tunsigned int nr_partitions;\n> +\tu64 serial;\n> +\tenum cxl_devtype type;\n> +\tstruct cxl_mailbox cxl_mbox;\n> +#ifdef CONFIG_CXL_FEATURES\n> +\tstruct cxl_features_state *cxlfs;\n> +#endif\n> +};\n> +\n> +struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n> +\t\t\t\t\t\t enum cxl_devtype type,\n> +\t\t\t\t\t\t u64 serial, u16 dvsec,\n> +\t\t\t\t\t\t size_t size, bool has_mbox);\n> +\n> +/**\n> + * cxl_dev_state_create - safely create and cast a cxl dev state embedded in a\n> + * driver specific struct.\n> + *\n> + * @parent: device behind the request\n> + * @type: CXL device type\n> + * @serial: device identification\n> + * @dvsec: dvsec capability offset\n> + * @drv_struct: driver struct embedding a cxl_dev_state struct\n> + * @member: drv_struct member as cxl_dev_state\n> + * @mbox: true if mailbox supported\n> + *\n> + * Returns a pointer to the drv_struct allocated and embedding a cxl_dev_state\n> + * struct initialized.\n> + *\n> + * Introduced for Type2 driver support.\n> + */\n> +#define devm_cxl_dev_state_create(parent, type, serial, dvsec, drv_struct, member, mbox)\t\\\n> +\t({\t\t\t\t\t\t\t\t\t\t\\\n> +\t\tstatic_assert(__same_type(struct cxl_dev_state,\t\t\t\t\\\n> +\t\t\t      ((drv_struct *)NULL)->member));\t\t\t\t\\\n> +\t\tstatic_assert(offsetof(drv_struct, member) == 0);\t\t\t\\\n> +\t\t(drv_struct *)_devm_cxl_dev_state_create(parent, type, serial, dvsec,\t\\\n> +\t\t\t\t\t\t      sizeof(drv_struct), mbox);\t\\\n> +\t})\n> +#endif /* __CXL_CXL_H__ */\n> diff --git a/tools/testing/cxl/test/mem.c b/tools/testing/cxl/test/mem.c\n> index cb87e8c0e63c..79f42f4474d4 100644\n> --- a/tools/testing/cxl/test/mem.c\n> +++ b/tools/testing/cxl/test/mem.c\n> @@ -1716,7 +1716,7 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> -\tmds = cxl_memdev_state_create(dev);\n> +\tmds = cxl_memdev_state_create(dev, pdev->id + 1, 0);\n>  \tif (IS_ERR(mds))\n>  \t\treturn PTR_ERR(mds);\n>  \n> @@ -1732,7 +1732,6 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n>  \tmds->event.buf = (struct cxl_get_event_payload *) mdata->event_buf;\n>  \tINIT_DELAYED_WORK(&mds->security.poll_dwork, cxl_mockmem_sanitize_work);\n>  \n> -\tcxlds->serial = pdev->id + 1;\n>  \tif (is_rcd(pdev))\n>  \t\tcxlds->rcd = true;\n>  \n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> A Type2 device configured by the BIOS can already have its HDM\n> committed. Add a cxl_get_committed_decoder() function for cheking\n> so after memdev creation. A CXL region should have been created\n> during memdev initialization, therefore a Type2 driver can ask for\n> such a region for working with the HPA. If the HDM is not committed,\n> a Type2 driver will create the region after obtaining proper HPA\n> and DPA space.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/cxl/core/hdm.c | 39 +++++++++++++++++++++++++++++++++++++++\n>  include/cxl/cxl.h      |  3 +++\n>  2 files changed, 42 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index 6e516c69b2d2..a172ce4e9b19 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -686,6 +686,45 @@ int cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  \treturn devm_add_action_or_reset(&port->dev, cxl_dpa_release, cxled);\n>  }\n>  \n> +static int find_committed_endpoint_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == port->hdm_end;\n\nIs this the way you're supposed to check if a decoder is committed? The doc comment for @hdm_end in\nstruct cxl_port says it's just the last allocated decoder. If allocated decoders are always committed then\nI'm fine with this, otherwise I think you'd want to a register read or something to find the commit state.\n> +}\n> +\n> +struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t\t       struct cxl_region **cxlr)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct device *cxled_dev;\n> +\n> +\tif (!endpoint)\n> +\t\treturn NULL;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tcxled_dev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\t      find_committed_endpoint_decoder);\n> +\n> +\tif (!cxled_dev)\n> +\t\treturn NULL;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(cxled_dev);\n> +\t*cxlr = cxled->cxld.region;\n> +\n> +\tput_device(cxled_dev);\n> +\treturn cxled;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_committed_decoder, \"CXL\");\n> +\n>  static void cxld_set_interleave(struct cxl_decoder *cxld, u32 *ctrl)\n>  {\n>  \tu16 eig;\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 6f8d365067af..928276dba952 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -249,4 +249,7 @@ int cxl_map_component_regs(const struct cxl_register_map *map,\n>  int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n>  struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n>  \t\t\t\t       const struct cxl_memdev_attach *attach);\n> +struct cxl_region;\n> +struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t\t       struct cxl_region **cxlr);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> This patchset should be applied on the cxl next branch using the base\n> specified at the end of this cover letter.\n> \n> Dependencies on Dan's work has gone and also on Terry's as the only\n> patch required is now in next. The other dependency is on Smita patchset\n> but it does not exist such a dependency as that work will not avoid the\n> problem with Type2 and DAX/hmem if soft reserved memory. This needs to\n> be solved by the BIOS and Type2 UEFI driver for populating the CXL.mem\n> range as EFI_RESERVED_TYPE instead of default EFI_CONVENTIONAL_MEMORY\n> with the EFI_MEMORY_SP attribute. There exists though a dependency on\n> one Smita's patches:\n> \n> [PATCH v5 3/7] cxl/region: Skip decoder reset on detach for autodiscovered regions\n> \n> This is needed for the default behaviour with current BIOS configuration\n> where the HDM Type2 decoders will be kept unreset when driver unloads.\n> This is the main change introduced in v23: committed decoders will not\n> be reset. Previous v22 functionality supported first driver load finding\n> committed decoders but resetting them at unload and supporting\n> uncommitted decoders in next driver loads. This will be suported in\n> follow-up works.\n> \n> v23 changes:\n> \n>   patch 11: fixing minor issues and droping change in\n> \t    should_emulate_decoders (Jonathan Cameron)\n> \n>   patch13: refactoring unregister_region for safety type in Type2 API\n> \n>   sfc changes: slight modifications to error path\n> \n\nThis cover letter is really long, I'd remove the change logs for anything more\nthan 3 revisions back (assuming a v24 is needed). After that you could leave\na lore link for older revisions if you want, but it's not needed imo.\nAlso, feel free to add my Reviewed-by for anything I didn't leave a comment on\n(felt I should cut down on the mail).\n\nThanks,\nBen\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation involves finding available DPA (device-physical-address)\n> capacity to map into HPA (host-physical-address) space.\n> \n> In order to support CXL Type2 devices, define an API, cxl_request_dpa(),\n> that tries to allocate the DPA memory the driver requires to operate.The\n> memory requested should not be bigger than the max available HPA obtained\n> previously with cxl_get_hpa_freespace().\n> \n> Based on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h      |  1 +\n>  include/cxl/cxl.h      |  5 +++\n>  3 files changed, 90 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index a172ce4e9b19..d60a697f12cc 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -3,6 +3,7 @@\n>  #include <linux/seq_file.h>\n>  #include <linux/device.h>\n>  #include <linux/delay.h>\n> +#include <cxl/cxl.h>\n>  \n>  #include \"cxlmem.h\"\n>  #include \"core.h\"\n> @@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n>  \treturn resource_contains(res, &_addr);\n>  }\n>  \n> +/**\n> + * cxl_dpa_free - release DPA (Device Physical Address)\n> + * @cxled: endpoint decoder linked to the DPA\n> + *\n> + * Returns 0 or error.\n> + */\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_port *port = cxled_to_port(cxled);\n> @@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  \tdevm_cxl_dpa_release(cxled);\n>  \treturn 0;\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n>  \n>  int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \t\t     enum cxl_partition_mode mode)\n> @@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \treturn 0;\n>  }\n>  \n> +static int find_free_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == (port->hdm_end + 1);\n> +}\n> +\n> +static struct cxl_endpoint_decoder *\n> +cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct device *dev;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tdev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\tfind_free_decoder);\n> +\tif (!dev)\n> +\t\treturn NULL;\n> +\n> +\treturn to_cxl_endpoint_decoder(dev);\n> +}\n> +\n> +/**\n> + * cxl_request_dpa - search and reserve DPA given input constraints\n> + * @cxlmd: memdev with an endpoint port with available decoders\n> + * @mode: CXL partition mode (ram vs pmem)\n> + * @alloc: dpa size required\n> + *\n> + * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n> + * an errno encoded pointer on failure.\n> + *\n> + * Given that a region needs to allocate from limited HPA capacity it\n> + * may be the case that a device has more mappable DPA capacity than\n> + * available HPA. The expectation is that @alloc is a driver known\n> + * value based on the device capacity but which could not be fully\n> + * available due to HPA constraints.\n> + *\n> + * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n> + * reserved, or an error pointer. The caller is also expected to own the\n> + * lifetime of the memdev registration associated with the endpoint to\n> + * pin the decoder registered as well.\n> + */\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc)\n> +{\n> +\tint rc;\n> +\n> +\tif (!IS_ALIGNED(alloc, SZ_256M))\n> +\t\treturn ERR_PTR(-EINVAL);\n> +\n> +\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n> +\t\tcxl_find_free_decoder(cxlmd);\n> +\n> +\tif (!cxled)\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\n> +\trc = cxl_dpa_set_part(cxled, mode);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = cxl_dpa_alloc(cxled, alloc);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n\nShould cxl_dpa_set_part() be unwound here, or does it not matter? If it doesn't matter:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\n> +\treturn no_free_ptr(cxled);\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n> +\n>  static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index d1b010e5e1d0..2b1f7d687a0e 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n>  \n>  DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n>  DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n> +DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n>  DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n>  DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 783ad570a6eb..4802371db00e 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -7,6 +7,7 @@\n>  \n>  #include <linux/node.h>\n>  #include <linux/ioport.h>\n> +#include <linux/range.h>\n>  #include <cxl/mailbox.h>\n>  \n>  /**\n> @@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t       unsigned long flags,\n>  \t\t\t\t\t       resource_size_t *max);\n>  void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc);\n> +int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\n\n\nOn 2/19/2026 4:40 AM, Alejandro Lucero Palau wrote:\n> \n> On 2/11/26 22:11, Cheatham, Benjamin wrote:\n>> On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n>>> From: Alejandro Lucero <alucerop@amd.com>\n>>>\n>>> Region creation based on Type3 devices is triggered from user space\n>>> allowing memory combination through interleaving.\n>>>\n>>> In preparation for kernel driven region creation, that is Type2 drivers\n>>> triggering region creation backed with its advertised CXL memory, factor\n>>> out a common helper from the user-sysfs region setup for interleave ways.\n>>>\n>>> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n>>> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n>>> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n>>> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n>>> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>>> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n>>> ---\n>>>  drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n>>>  1 file changed, 27 insertions(+), 16 deletions(-)\n>>>\n>>> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n>>> index f53b2e9fd9e6..ece1d3df7cf1 100644\n>>> --- a/drivers/cxl/core/region.c\n>>> +++ b/drivers/cxl/core/region.c\n>>> @@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n>>>   static const struct attribute_group *get_cxl_region_target_group(void);\n>>>  -static ssize_t interleave_ways_store(struct device *dev,\n>>> - struct device_attribute *attr,\n>>> - const char *buf, size_t len)\n>>> +static int set_interleave_ways(struct cxl_region *cxlr, int val)\n>> @val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\n>> function was originally coded with that in mind (same with @save below).\n> \n> Good catch. I wonder if I should just change the way the value is obtained, using kstrtoint instead of kstrtouint, as those values are used for cxl_region_params fields defined as int. In other words, it seems doing that simpler than changing all the other places you mention and the structs involved. I can not see a reason for using unsigned int so I think I will follow that approach. Tell me if you think otherwise.\n> \n\nIf I had to guess unsigned int was used because a negative interleave granularity/ways makes no sense. I think your suggestion is fine though since no one\nin their right mind would give anything but a (relatively) small and positive value for these.\n\nThanks,\nBen\n\n> \n> Thank you\n> \n> \n>> With that cleaned up:\n>> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>>\n>>>  {\n>>> - struct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n>>> + struct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>>>  struct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n>>> - struct cxl_region *cxlr = to_cxl_region(dev);\n>>>  struct cxl_region_params *p = &cxlr->params;\n>>> - unsigned int val, save;\n>>> - int rc;\n>>> + int save, rc;\n>>>  u8 iw;\n>>>  - rc = kstrtouint(buf, 0, &val);\n>>> - if (rc)\n>>> - return rc;\n>>> -\n>>>  rc = ways_to_eiw(val, &iw);\n>>>  if (rc)\n>>>  return rc;\n>>> @@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n>>>  return -EINVAL;\n>>>  }\n>>>  - ACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n>>> - if ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n>>> - return rc;\n>>> + lockdep_assert_held_write(&cxl_rwsem.region);\n>>>   if (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>>>  return -EBUSY;\n>>> @@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n>>>  save = p->interleave_ways;\n>>>  p->interleave_ways = val;\n>>>  rc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n>>> - if (rc) {\n>>> + if (rc)\n>>>  p->interleave_ways = save;\n>>> +\n>>> + return rc;\n>>> +}\n>>> +\n>>> +static ssize_t interleave_ways_store(struct device *dev,\n>>> + struct device_attribute *attr,\n>>> + const char *buf, size_t len)\n>>> +{\n>>> + struct cxl_region *cxlr = to_cxl_region(dev);\n>>> + unsigned int val;\n>>> + int rc;\n>>> +\n>>> + rc = kstrtouint(buf, 0, &val);\n>>> + if (rc)\n>>> + return rc;\n>>> +\n>>> + ACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n>>> + if ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n>>> + return rc;\n>>> +\n>>> + rc = set_interleave_ways(cxlr, val);\n>>> + if (rc)\n>>>  return rc;\n>>> - }\n>>>   return len;\n>>>  }\n\n\n\n---\n\nOn 2/19/2026 3:58 AM, Alejandro Lucero Palau wrote:\n> \n> On 2/11/26 22:10, Cheatham, Benjamin wrote:\n>> On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n>>> From: Alejandro Lucero <alucerop@amd.com>\n>>>\n>>> CXL region creation involves allocating capacity from Device Physical\n>>> Address (DPA) and assigning it to decode a given Host Physical Address\n>>> (HPA). Before determining how much DPA to allocate the amount of available\n>>> HPA must be determined. Also, not all HPA is created equal, some HPA\n>>> targets RAM, some targets PMEM, some is prepared for device-memory flows\n>>> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n>>>\n>>> In order to support Type2 CXL devices, wrap all of those concerns into\n>>> an API that retrieves a root decoder (platform CXL window) that fits the\n>>> specified constraints and the capacity available for a new region.\n>>>\n>>> Add a complementary function for releasing the reference to such root\n>>> decoder.\n>>>\n>>> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n>>>\n>>> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n>>> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n>>> ---\n>>>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>>>  drivers/cxl/cxl.h | 3 +\n>>>  include/cxl/cxl.h | 6 ++\n>>>  3 files changed, 173 insertions(+)\n>>>\n>>> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n>>> index 954b8fcdbac6..bdefd088f5f1 100644\n>>> --- a/drivers/cxl/core/region.c\n>>> +++ b/drivers/cxl/core/region.c\n>>> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>>>  return 0;\n>>>  }\n>>>  +struct cxlrd_max_context {\n>>> + struct device * const *host_bridges;\n>>> + int interleave_ways;\n>>> + unsigned long flags;\n>>> + resource_size_t max_hpa;\n>>> + struct cxl_root_decoder *cxlrd;\n>>> +};\n>>> +\n>>> +static int find_max_hpa(struct device *dev, void *data)\n>>> +{\n>>> + struct cxlrd_max_context *ctx = data;\n>>> + struct cxl_switch_decoder *cxlsd;\n>>> + struct cxl_root_decoder *cxlrd;\n>>> + struct resource *res, *prev;\n>>> + struct cxl_decoder *cxld;\n>>> + resource_size_t free = 0;\n>>> + resource_size_t max;\n>>> + int found = 0;\n>>> +\n>>> + if (!is_root_decoder(dev))\n>>> + return 0;\n>>> +\n>>> + cxlrd = to_cxl_root_decoder(dev);\n>>> + cxlsd = &cxlrd->cxlsd;\n>>> + cxld = &cxlsd->cxld;\n>>> +\n>>> + if ((cxld->flags & ctx->flags) != ctx->flags) {\n>>> + dev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n>>> + cxld->flags, ctx->flags);\n>>> + return 0;\n>>> + }\n>>> +\n>>> + for (int i = 0; i < ctx->interleave_ways; i++) {\n>>> + for (int j = 0; j < ctx->interleave_ways; j++) {\n>>> + if (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n>>> + found++;\n>>> + break;\n>>> + }\n>>> + }\n>>> + }\n>> This may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\n>> what the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\n>> bridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\n>> point, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\n>> be a struct device * const.\n>>\n>> Maybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\n>> I'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n> \n> \n> Hi Ben,\n> \n> \n> I do remember this one.\n> \n> \n> Dan's original patches had this support for interleaving, then I removed it as the case for Type2 and interleaving is quite unlikely, at least right now and likely in the near future. But I was told why do not support it as it was trivial to do so. FWIW, If I think only about the use case coming with the patchset, I agree with you, but because those previous discussions, I think I have to leave it.\n> \n\nI'm fine with that, but I would at least do the fix with the decoder position in 19/22 and make a note that the\ninterleave_ways parameter in cxl_get_hpa_freespace() below is currently unused (unless I'm misunderstanding\nthe endpoint->host_bridge member).\n\nThat way, the support is mostly there and just requires a small, previously noted, addition to enable. If you're\nfine with that then feel free to add my Reviewed-by after implementing in v24.\n\nThanks,\nBen\n\n> \n> Thank you\n> \n> \n>>> +\n>>> + if (found != ctx->interleave_ways) {\n>>> + dev_dbg(dev,\n>>> + \"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n>>> + found, ctx->interleave_ways);\n>>> + return 0;\n>>> + }\n>>> +\n>>> + /*\n>>> + * Walk the root decoder resource range relying on cxl_rwsem.region to\n>>> + * preclude sibling arrival/departure and find the largest free space\n>>> + * gap.\n>>> + */\n>>> + lockdep_assert_held_read(&cxl_rwsem.region);\n>>> + res = cxlrd->res->child;\n>>> +\n>>> + /* With no resource child the whole parent resource is available */\n>>> + if (!res)\n>>> + max = resource_size(cxlrd->res);\n>>> + else\n>>> + max = 0;\n>>> +\n>>> + for (prev = NULL; res; prev = res, res = res->sibling) {\n>>> + if (!prev && res->start == cxlrd->res->start &&\n>>> + res->end == cxlrd->res->end) {\n>>> + max = resource_size(cxlrd->res);\n>>> + break;\n>>> + }\n>>> + /*\n>>> + * Sanity check for preventing arithmetic problems below as a\n>>> + * resource with size 0 could imply using the end field below\n>>> + * when set to unsigned zero - 1 or all f in hex.\n>>> + */\n>>> + if (prev && !resource_size(prev))\n>>> + continue;\n>>> +\n>>> + if (!prev && res->start > cxlrd->res->start) {\n>>> + free = res->start - cxlrd->res->start;\n>>> + max = max(free, max);\n>>> + }\n>>> + if (prev && res->start > prev->end + 1) {\n>>> + free = res->start - prev->end + 1;\n>>> + max = max(free, max);\n>>> + }\n>>> + }\n>>> +\n>>> + if (prev && prev->end + 1 < cxlrd->res->end + 1) {\n>>> + free = cxlrd->res->end + 1 - prev->end + 1;\n>>> + max = max(free, max);\n>>> + }\n>>> +\n>>> + dev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n>>> + if (max > ctx->max_hpa) {\n>>> + if (ctx->cxlrd)\n>>> + put_device(cxlrd_dev(ctx->cxlrd));\n>>> + get_device(cxlrd_dev(cxlrd));\n>>> + ctx->cxlrd = cxlrd;\n>>> + ctx->max_hpa = max;\n>>> + }\n>>> + return 0;\n>>> +}\n>>> +\n>>> +/**\n>>> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n>>> + * @cxlmd: the mem device requiring the HPA\n>>> + * @interleave_ways: number of entries in @host_bridges\n>>> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n>>> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n>>> + * returned decoder\n>>> + *\n>>> + * Returns a pointer to a struct cxl_root_decoder\n>>> + *\n>>> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n>>> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n>>> + * caller goes to use this decoder and its capacity is reduced then caller needs\n>>> + * to loop and retry.\n>>> + *\n>>> + * The returned root decoder has an elevated reference count that needs to be\n>>> + * put with cxl_put_root_decoder(cxlrd).\n>>> + */\n>>> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>>> + int interleave_ways,\n>>> + unsigned long flags,\n>>> + resource_size_t *max_avail_contig)\n>>> +{\n>>> + struct cxlrd_max_context ctx = {\n>>> + .flags = flags,\n>>> + .interleave_ways = interleave_ways,\n>>> + };\n>>> + struct cxl_port *root_port;\n>>> + struct cxl_port *endpoint;\n>>> +\n>>> + endpoint = cxlmd->endpoint;\n>>> + if (!endpoint) {\n>>> + dev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n>>> + return ERR_PTR(-ENXIO);\n>>> + }\n>>> +\n>>> + ctx.host_bridges = &endpoint->host_bridge;\n>> Mentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\n>> something). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\n>> I'm not sure of what the fix is to get the intended behavior.\n>>\n>> It may be worth getting rid of the interleave_ways portion of this function and\n>> add it later when someone needs it. You could also explain it's hard coded to 1/unused\n>> in the doc comment if you know of an immediate need for it.\n>>\n>>> +\n>>> + struct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n>>> + if (!root) {\n>>> + dev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n>>> + return ERR_PTR(-ENXIO);\n>>> + }\n>>> +\n>>> + root_port = &root->port;\n>>> + scoped_guard(rwsem_read, &cxl_rwsem.region)\n>>> + device_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n>> Can just use a guard() here.\n>>\n>>> +\n>>> + if (!ctx.cxlrd)\n>>> + return ERR_PTR(-ENOMEM);\n>>> +\n>>> + *max_avail_contig = ctx.max_hpa;\n>>> + return ctx.cxlrd;\n>>> +}\n>>> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n>>> +\n>>> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n>>> +{\n>>> + put_device(cxlrd_dev(cxlrd));\n>>> +}\n>>> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n>>> +\n>>>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>>>  const char *buf, size_t len)\n>>>  {\n>>> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n>>> index 944c5d1ccceb..c7d9b2c2908f 100644\n>>> --- a/drivers/cxl/cxl.h\n>>> +++ b/drivers/cxl/cxl.h\n>>> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>>>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>>>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>>>  bool is_root_decoder(struct device *dev);\n>>> +\n>>> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n>>> +\n>>>  bool is_switch_decoder(struct device *dev);\n>>>  bool is_endpoint_decoder(struct device *dev);\n>>>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n>>> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n>>> index 92880c26b2d5..834dc7e78934 100644\n>>> --- a/include/cxl/cxl.h\n>>> +++ b/include/cxl/cxl.h\n>>> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>>>  struct range;\n>>>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>>>  void cxl_unregister_region(struct cxl_region *cxlr);\n>>> +struct cxl_port;\n>>> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>>> + int interleave_ways,\n>>> + unsigned long flags,\n>>> + resource_size_t *max);\n>>> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>>>  #endif /* __CXL_CXL_H__ */\n\n",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer noted that the current implementation calls drop_region() with an error pointer returned by construct_region_begin(), which would lead to a garbage pointer dereference, and suggested changing the if condition to check for IS_ERR_OR_NULL before calling drop_region().\n\nReviewer noted that the position parameter in a function is hardcoded to 0, and suggested setting it to 'i' instead, referencing an earlier patch where interleaving functionality was added but appears unused.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This needs to be \"if (!IS_ERR_OR_NULL(_T) drop_region(_T)\". If construct_region_begin() returns an\nerror pointer, drop_region() will be called with it as of now leading to a garbage pointer deref.\n\n---\n\nPosition parameter is hardcoded to 0. It should be set to i, right? This kind of goes back to my\nissues in patch 12/22; the interleaving functionality is there but it looks unused.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested folding cxl_dev_state_init() into the existing function, citing that it's only called within this function and thus unnecessary to have a separate init function.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Nit: Having a second function to do the init seems overkill here, especially since cxl_dev_state_init() isn't called outside this\nfunction. I'd fold it into this function instead, but I'm fine with it either way (especially if you were told otherwise before).\n\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer questioned whether checking if a decoder is committed should be done by verifying the last allocated decoder (hdm_end), pointing out that this might not accurately reflect the commit state and suggesting an alternative approach of reading the register directly.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Is this the way you're supposed to check if a decoder is committed? The doc comment for @hdm_end in\nstruct cxl_port says it's just the last allocated decoder. If allocated decoders are always committed then\nI'm fine with this, otherwise I think you'd want to a register read or something to find the commit state.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested removing change logs for revisions more than 3 back, and adding a lore link for older revisions if desired.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "Requested changes",
                "Suggested improvement"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This cover letter is really long, I'd remove the change logs for anything more\nthan 3 revisions back (assuming a v24 is needed). After that you could leave\na lore link for older revisions if you want, but it's not needed imo.\nAlso, feel free to add my Reviewed-by for anything I didn't leave a comment on\n(felt I should cut down on the mail).\n\nThanks,\nBen",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> By definition a type2 cxl device will use the host managed memory for\n> specific functionality, therefore it should not be available to other\n> uses.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> ---\n>  drivers/cxl/core/region.c | 7 +++++++\n>  1 file changed, 7 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 293e63dfef22..12df717cc881 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -4441,6 +4441,13 @@ static int cxl_region_probe(struct device *dev)\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> +\t/*\n> +\t * HDM-D[B] (device-memory) regions have accelerator specific usage.\n> +\t * Skip device-dax registration.\n> +\t */\n> +\tif (cxlr->type == CXL_DECODER_DEVMEM)\n> +\t\treturn 0;\n\nMinor nit: Should probably move this to be the first thing in the function. It would save\nhaving to acquire a lock in cxl_region_can_probe() above. Keep my reviewed-by either way,\nit's really just a minor optimization.\n> +\n>  \t/*\n>  \t * From this point on any path that changes the region's state away from\n>  \t * CXL_CONFIG_COMMIT is also responsible for releasing the driver.\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> CXL region creation involves allocating capacity from Device Physical\n> Address (DPA) and assigning it to decode a given Host Physical Address\n> (HPA). Before determining how much DPA to allocate the amount of available\n> HPA must be determined. Also, not all HPA is created equal, some HPA\n> targets RAM, some targets PMEM, some is prepared for device-memory flows\n> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n> \n> In order to support Type2 CXL devices, wrap all of those concerns into\n> an API that retrieves a root decoder (platform CXL window) that fits the\n> specified constraints and the capacity available for a new region.\n> \n> Add a complementary function for releasing the reference to such root\n> decoder.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> ---\n>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h         |   3 +\n>  include/cxl/cxl.h         |   6 ++\n>  3 files changed, 173 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 954b8fcdbac6..bdefd088f5f1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>  \treturn 0;\n>  }\n>  \n> +struct cxlrd_max_context {\n> +\tstruct device * const *host_bridges;\n> +\tint interleave_ways;\n> +\tunsigned long flags;\n> +\tresource_size_t max_hpa;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +};\n> +\n> +static int find_max_hpa(struct device *dev, void *data)\n> +{\n> +\tstruct cxlrd_max_context *ctx = data;\n> +\tstruct cxl_switch_decoder *cxlsd;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +\tstruct resource *res, *prev;\n> +\tstruct cxl_decoder *cxld;\n> +\tresource_size_t free = 0;\n> +\tresource_size_t max;\n> +\tint found = 0;\n> +\n> +\tif (!is_root_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxlrd = to_cxl_root_decoder(dev);\n> +\tcxlsd = &cxlrd->cxlsd;\n> +\tcxld = &cxlsd->cxld;\n> +\n> +\tif ((cxld->flags & ctx->flags) != ctx->flags) {\n> +\t\tdev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n> +\t\t\tcxld->flags, ctx->flags);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\tfor (int i = 0; i < ctx->interleave_ways; i++) {\n> +\t\tfor (int j = 0; j < ctx->interleave_ways; j++) {\n> +\t\t\tif (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n> +\t\t\t\tfound++;\n> +\t\t\t\tbreak;\n> +\t\t\t}\n> +\t\t}\n> +\t}\n\nThis may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\nwhat the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\nbridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\npoint, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\nbe a struct device * const.\n\nMaybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\nI'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n> +\n> +\tif (found != ctx->interleave_ways) {\n> +\t\tdev_dbg(dev,\n> +\t\t\t\"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n> +\t\t\tfound, ctx->interleave_ways);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\t/*\n> +\t * Walk the root decoder resource range relying on cxl_rwsem.region to\n> +\t * preclude sibling arrival/departure and find the largest free space\n> +\t * gap.\n> +\t */\n> +\tlockdep_assert_held_read(&cxl_rwsem.region);\n> +\tres = cxlrd->res->child;\n> +\n> +\t/* With no resource child the whole parent resource is available */\n> +\tif (!res)\n> +\t\tmax = resource_size(cxlrd->res);\n> +\telse\n> +\t\tmax = 0;\n> +\n> +\tfor (prev = NULL; res; prev = res, res = res->sibling) {\n> +\t\tif (!prev && res->start == cxlrd->res->start &&\n> +\t\t    res->end == cxlrd->res->end) {\n> +\t\t\tmax = resource_size(cxlrd->res);\n> +\t\t\tbreak;\n> +\t\t}\n> +\t\t/*\n> +\t\t * Sanity check for preventing arithmetic problems below as a\n> +\t\t * resource with size 0 could imply using the end field below\n> +\t\t * when set to unsigned zero - 1 or all f in hex.\n> +\t\t */\n> +\t\tif (prev && !resource_size(prev))\n> +\t\t\tcontinue;\n> +\n> +\t\tif (!prev && res->start > cxlrd->res->start) {\n> +\t\t\tfree = res->start - cxlrd->res->start;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t\tif (prev && res->start > prev->end + 1) {\n> +\t\t\tfree = res->start - prev->end + 1;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t}\n> +\n> +\tif (prev && prev->end + 1 < cxlrd->res->end + 1) {\n> +\t\tfree = cxlrd->res->end + 1 - prev->end + 1;\n> +\t\tmax = max(free, max);\n> +\t}\n> +\n> +\tdev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n> +\tif (max > ctx->max_hpa) {\n> +\t\tif (ctx->cxlrd)\n> +\t\t\tput_device(cxlrd_dev(ctx->cxlrd));\n> +\t\tget_device(cxlrd_dev(cxlrd));\n> +\t\tctx->cxlrd = cxlrd;\n> +\t\tctx->max_hpa = max;\n> +\t}\n> +\treturn 0;\n> +}\n> +\n> +/**\n> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n> + * @cxlmd: the mem device requiring the HPA\n> + * @interleave_ways: number of entries in @host_bridges\n> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n> + *\t\t      returned decoder\n> + *\n> + * Returns a pointer to a struct cxl_root_decoder\n> + *\n> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n> + * caller goes to use this decoder and its capacity is reduced then caller needs\n> + * to loop and retry.\n> + *\n> + * The returned root decoder has an elevated reference count that needs to be\n> + * put with cxl_put_root_decoder(cxlrd).\n> + */\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max_avail_contig)\n> +{\n> +\tstruct cxlrd_max_context ctx = {\n> +\t\t.flags = flags,\n> +\t\t.interleave_ways = interleave_ways,\n> +\t};\n> +\tstruct cxl_port *root_port;\n> +\tstruct cxl_port *endpoint;\n> +\n> +\tendpoint = cxlmd->endpoint;\n> +\tif (!endpoint) {\n> +\t\tdev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\tctx.host_bridges = &endpoint->host_bridge;\n\nMentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\nsomething). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\nI'm not sure of what the fix is to get the intended behavior.\n\nIt may be worth getting rid of the interleave_ways portion of this function and\nadd it later when someone needs it. You could also explain it's hard coded to 1/unused\nin the doc comment if you know of an immediate need for it.\n\n> +\n> +\tstruct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n> +\tif (!root) {\n> +\t\tdev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\troot_port = &root->port;\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.region)\n> +\t\tdevice_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n\nCan just use a guard() here.\n\n> +\n> +\tif (!ctx.cxlrd)\n> +\t\treturn ERR_PTR(-ENOMEM);\n> +\n> +\t*max_avail_contig = ctx.max_hpa;\n> +\treturn ctx.cxlrd;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n> +\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n> +{\n> +\tput_device(cxlrd_dev(cxlrd));\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n> +\n>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>  \t\t\t  const char *buf, size_t len)\n>  {\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index 944c5d1ccceb..c7d9b2c2908f 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>  bool is_root_decoder(struct device *dev);\n> +\n> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n> +\n>  bool is_switch_decoder(struct device *dev);\n>  bool is_endpoint_decoder(struct device *dev);\n>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 92880c26b2d5..834dc7e78934 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>  struct range;\n>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>  void cxl_unregister_region(struct cxl_region *cxlr);\n> +struct cxl_port;\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max);\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Check if device HDM is already committed during firmware/BIOS\n> initialization.\n> \n> A CXL region should exist if so after memdev allocation/initialization.\n> Get HPA from region and map it.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/net/ethernet/sfc/efx_cxl.c | 28 +++++++++++++++++++++++++++-\n>  1 file changed, 27 insertions(+), 1 deletion(-)\n> \n> diff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\n> index a77ef4783fcb..3536eccf1b2a 100644\n> --- a/drivers/net/ethernet/sfc/efx_cxl.c\n> +++ b/drivers/net/ethernet/sfc/efx_cxl.c\n> @@ -19,6 +19,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \tstruct efx_nic *efx = &probe_data->efx;\n>  \tstruct pci_dev *pci_dev = efx->pci_dev;\n>  \tstruct efx_cxl *cxl;\n> +\tstruct range range;\n>  \tu16 dvsec;\n>  \tint rc;\n>  \n> @@ -90,13 +91,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \t\treturn PTR_ERR(cxl->cxlmd);\n>  \t}\n>  \n> -\tprobe_data->cxl = cxl;\n> +\tcxl->cxled = cxl_get_committed_decoder(cxl->cxlmd, &cxl->efx_region);\n> +\tif (cxl->cxled) {\n> +\t\tif (!cxl->efx_region) {\n> +\t\t\tpci_err(pci_dev, \"CXL found committed decoder without a region\");\n> +\t\t\treturn -ENODEV;\n> +\t\t}\n> +\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n\nMissing an empty line above.\n\n> +\t\tif (rc) {\n> +\t\t\tpci_err(pci_dev,\n> +\t\t\t\t\"CXL getting regions params from a committed decoder failed\");\n> +\t\t\treturn rc;\n> +\t\t}\n> +\n> +\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n\nMaybe use range_len() instead for the second parameter?\n\n> +\t\tif (!cxl->ctpio_cxl) {\n> +\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n> +\t\t\treturn -ENOMEM;\n> +\t\t}\n> +\n> +\t\tprobe_data->cxl = cxl;\n> +\t}\n>  \n>  \treturn 0;\n>  }\n>  \n>  void efx_cxl_exit(struct efx_probe_data *probe_data)\n>  {\n> +\tif (!probe_data->cxl)\n> +\t\treturn;\n> +\n> +\tiounmap(probe_data->cxl->ctpio_cxl);\n> +\tcxl_unregister_region(probe_data->cxl->efx_region);\n>  }\n>  \n>  MODULE_IMPORT_NS(\"CXL\");\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation involves finding available DPA (device-physical-address)\n> capacity to map into HPA (host-physical-address) space.\n> \n> In order to support CXL Type2 devices, define an API, cxl_request_dpa(),\n> that tries to allocate the DPA memory the driver requires to operate.The\n> memory requested should not be bigger than the max available HPA obtained\n> previously with cxl_get_hpa_freespace().\n> \n> Based on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h      |  1 +\n>  include/cxl/cxl.h      |  5 +++\n>  3 files changed, 90 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index a172ce4e9b19..d60a697f12cc 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -3,6 +3,7 @@\n>  #include <linux/seq_file.h>\n>  #include <linux/device.h>\n>  #include <linux/delay.h>\n> +#include <cxl/cxl.h>\n>  \n>  #include \"cxlmem.h\"\n>  #include \"core.h\"\n> @@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n>  \treturn resource_contains(res, &_addr);\n>  }\n>  \n> +/**\n> + * cxl_dpa_free - release DPA (Device Physical Address)\n> + * @cxled: endpoint decoder linked to the DPA\n> + *\n> + * Returns 0 or error.\n> + */\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_port *port = cxled_to_port(cxled);\n> @@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  \tdevm_cxl_dpa_release(cxled);\n>  \treturn 0;\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n>  \n>  int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \t\t     enum cxl_partition_mode mode)\n> @@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \treturn 0;\n>  }\n>  \n> +static int find_free_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == (port->hdm_end + 1);\n> +}\n> +\n> +static struct cxl_endpoint_decoder *\n> +cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct device *dev;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tdev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\tfind_free_decoder);\n> +\tif (!dev)\n> +\t\treturn NULL;\n> +\n> +\treturn to_cxl_endpoint_decoder(dev);\n> +}\n> +\n> +/**\n> + * cxl_request_dpa - search and reserve DPA given input constraints\n> + * @cxlmd: memdev with an endpoint port with available decoders\n> + * @mode: CXL partition mode (ram vs pmem)\n> + * @alloc: dpa size required\n> + *\n> + * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n> + * an errno encoded pointer on failure.\n> + *\n> + * Given that a region needs to allocate from limited HPA capacity it\n> + * may be the case that a device has more mappable DPA capacity than\n> + * available HPA. The expectation is that @alloc is a driver known\n> + * value based on the device capacity but which could not be fully\n> + * available due to HPA constraints.\n> + *\n> + * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n> + * reserved, or an error pointer. The caller is also expected to own the\n> + * lifetime of the memdev registration associated with the endpoint to\n> + * pin the decoder registered as well.\n> + */\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc)\n> +{\n> +\tint rc;\n> +\n> +\tif (!IS_ALIGNED(alloc, SZ_256M))\n> +\t\treturn ERR_PTR(-EINVAL);\n> +\n> +\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n> +\t\tcxl_find_free_decoder(cxlmd);\n> +\n> +\tif (!cxled)\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\n> +\trc = cxl_dpa_set_part(cxled, mode);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = cxl_dpa_alloc(cxled, alloc);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n\nShould cxl_dpa_set_part() be unwound here, or does it not matter? If it doesn't matter:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\n> +\treturn no_free_ptr(cxled);\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n> +\n>  static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index d1b010e5e1d0..2b1f7d687a0e 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n>  \n>  DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n>  DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n> +DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n>  DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n>  DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 783ad570a6eb..4802371db00e 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -7,6 +7,7 @@\n>  \n>  #include <linux/node.h>\n>  #include <linux/ioport.h>\n> +#include <linux/range.h>\n>  #include <cxl/mailbox.h>\n>  \n>  /**\n> @@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t       unsigned long flags,\n>  \t\t\t\t\t       resource_size_t *max);\n>  void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc);\n> +int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Current code is expecting Type3 or CXL_DECODER_HOSTONLYMEM devices only.\n> Support for Type2 implies region type needs to be based on the endpoint\n> type HDM-D[B] instead.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> Reviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n> ---\n>  drivers/cxl/core/region.c | 10 ++++++----\n>  1 file changed, 6 insertions(+), 4 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index bdefd088f5f1..f53b2e9fd9e6 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2833,7 +2833,8 @@ static ssize_t create_ram_region_show(struct device *dev,\n>  }\n>  \n>  static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n> -\t\t\t\t\t  enum cxl_partition_mode mode, int id)\n> +\t\t\t\t\t  enum cxl_partition_mode mode, int id,\n> +\t\t\t\t\t  enum cxl_decoder_type target_type)\n>  {\n>  \tint rc;\n>  \n> @@ -2855,7 +2856,7 @@ static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n>  \t\treturn ERR_PTR(-EBUSY);\n>  \t}\n>  \n> -\treturn devm_cxl_add_region(cxlrd, id, mode, CXL_DECODER_HOSTONLYMEM);\n> +\treturn devm_cxl_add_region(cxlrd, id, mode, target_type);\n>  }\n>  \n>  static ssize_t create_region_store(struct device *dev, const char *buf,\n> @@ -2869,7 +2870,7 @@ static ssize_t create_region_store(struct device *dev, const char *buf,\n>  \tif (rc != 1)\n>  \t\treturn -EINVAL;\n>  \n> -\tcxlr = __create_region(cxlrd, mode, id);\n> +\tcxlr = __create_region(cxlrd, mode, id, CXL_DECODER_HOSTONLYMEM);\n\nI haven't read the ABI docs, but would it be worthwhile to update the documentation for this attribute\nto mention it only makes type 3 regions? I'm flip-flopping on whether it's worth the trouble but thought\nI should mention it.\n\nEither way:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n\n>  \tif (IS_ERR(cxlr))\n>  \t\treturn PTR_ERR(cxlr);\n>  \n> @@ -4036,7 +4037,8 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \n>  \tdo {\n>  \t\tcxlr = __create_region(cxlrd, cxlds->part[part].mode,\n> -\t\t\t\t       atomic_read(&cxlrd->region_id));\n> +\t\t\t\t       atomic_read(&cxlrd->region_id),\n> +\t\t\t\t       cxled->cxld.target_type);\n>  \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n>  \n>  \tif (IS_ERR(cxlr)) {\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation based on Type3 devices is triggered from user space\n> allowing memory combination through interleaving.\n> \n> In preparation for kernel driven region creation, that is Type2 drivers\n> triggering region creation backed with its advertised CXL memory, factor\n> out a common helper from the user-sysfs region setup for interleave ways.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n>  1 file changed, 27 insertions(+), 16 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index f53b2e9fd9e6..ece1d3df7cf1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n>  \n>  static const struct attribute_group *get_cxl_region_target_group(void);\n>  \n> -static ssize_t interleave_ways_store(struct device *dev,\n> -\t\t\t\t     struct device_attribute *attr,\n> -\t\t\t\t     const char *buf, size_t len)\n> +static int set_interleave_ways(struct cxl_region *cxlr, int val)\n\n@val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\nfunction was originally coded with that in mind (same with @save below). With that cleaned up:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n\n>  {\n> -\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>  \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> -\tstruct cxl_region *cxlr = to_cxl_region(dev);\n>  \tstruct cxl_region_params *p = &cxlr->params;\n> -\tunsigned int val, save;\n> -\tint rc;\n> +\tint save, rc;\n>  \tu8 iw;\n>  \n> -\trc = kstrtouint(buf, 0, &val);\n> -\tif (rc)\n> -\t\treturn rc;\n> -\n>  \trc = ways_to_eiw(val, &iw);\n>  \tif (rc)\n>  \t\treturn rc;\n> @@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n>  \t\treturn -EINVAL;\n>  \t}\n>  \n> -\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> -\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> -\t\treturn rc;\n> +\tlockdep_assert_held_write(&cxl_rwsem.region);\n>  \n>  \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>  \t\treturn -EBUSY;\n> @@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n>  \tsave = p->interleave_ways;\n>  \tp->interleave_ways = val;\n>  \trc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n> -\tif (rc) {\n> +\tif (rc)\n>  \t\tp->interleave_ways = save;\n> +\n> +\treturn rc;\n> +}\n> +\n> +static ssize_t interleave_ways_store(struct device *dev,\n> +\t\t\t\t     struct device_attribute *attr,\n> +\t\t\t\t     const char *buf, size_t len)\n> +{\n> +\tstruct cxl_region *cxlr = to_cxl_region(dev);\n> +\tunsigned int val;\n> +\tint rc;\n> +\n> +\trc = kstrtouint(buf, 0, &val);\n> +\tif (rc)\n> +\t\treturn rc;\n> +\n> +\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> +\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> +\t\treturn rc;\n> +\n> +\trc = set_interleave_ways(cxlr, val);\n> +\tif (rc)\n>  \t\treturn rc;\n> -\t}\n>  \n>  \treturn len;\n>  }\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation based on Type3 devices is triggered from user space\n> allowing memory combination through interleaving.\n> \n> In preparation for kernel driven region creation, that is Type2 drivers\n> triggering region creation backed with its advertised CXL memory, factor\n> out a common helper from the user-sysfs region setup forinterleave\n> granularity.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 39 +++++++++++++++++++++++++--------------\n>  1 file changed, 25 insertions(+), 14 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index ece1d3df7cf1..63c2aeb2ee1f 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -559,21 +559,14 @@ static ssize_t interleave_granularity_show(struct device *dev,\n>  \treturn sysfs_emit(buf, \"%d\\n\", p->interleave_granularity);\n>  }\n>  \n> -static ssize_t interleave_granularity_store(struct device *dev,\n> -\t\t\t\t\t    struct device_attribute *attr,\n> -\t\t\t\t\t    const char *buf, size_t len)\n> +static int set_interleave_granularity(struct cxl_region *cxlr, int val)\n\nSame thing as last patch. Assuming it's fixed:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>  {\n> -\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>  \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> -\tstruct cxl_region *cxlr = to_cxl_region(dev);\n>  \tstruct cxl_region_params *p = &cxlr->params;\n> -\tint rc, val;\n> +\tint rc;\n>  \tu16 ig;\n>  \n> -\trc = kstrtoint(buf, 0, &val);\n> -\tif (rc)\n> -\t\treturn rc;\n> -\n>  \trc = granularity_to_eig(val, &ig);\n>  \tif (rc)\n>  \t\treturn rc;\n> @@ -589,14 +582,32 @@ static ssize_t interleave_granularity_store(struct device *dev,\n>  \tif (cxld->interleave_ways > 1 && val != cxld->interleave_granularity)\n>  \t\treturn -EINVAL;\n>  \n> -\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> -\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> -\t\treturn rc;\n> -\n> +\tlockdep_assert_held_write(&cxl_rwsem.region);\n>  \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>  \t\treturn -EBUSY;\n>  \n>  \tp->interleave_granularity = val;\n> +\treturn 0;\n> +}\n> +\n> +static ssize_t interleave_granularity_store(struct device *dev,\n> +\t\t\t\t\t    struct device_attribute *attr,\n> +\t\t\t\t\t    const char *buf, size_t len)\n> +{\n> +\tstruct cxl_region *cxlr = to_cxl_region(dev);\n> +\tint rc, val;\n> +\n> +\trc = kstrtoint(buf, 0, &val);\n> +\tif (rc)\n> +\t\treturn rc;\n> +\n> +\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> +\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> +\t\treturn rc;\n> +\n> +\trc = set_interleave_granularity(cxlr, val);\n> +\tif (rc)\n> +\t\treturn rc;\n>  \n>  \treturn len;\n>  }\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Creating a CXL region requires userspace intervention through the cxl\n> sysfs files. Type2 support should allow accelerator drivers to create\n> such cxl region from kernel code.\n> \n> Adding that functionality and integrating it with current support for\n> memory expanders.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159835.1948938.1647215579839222774.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 131 ++++++++++++++++++++++++++++++++++++--\n>  include/cxl/cxl.h         |   3 +\n>  2 files changed, 127 insertions(+), 7 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 63c2aeb2ee1f..293e63dfef22 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2944,6 +2944,14 @@ cxl_find_region_by_name(struct cxl_root_decoder *cxlrd, const char *name)\n>  \treturn to_cxl_region(region_dev);\n>  }\n>  \n> +static void drop_region(struct cxl_region *cxlr)\n> +{\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n> +\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n> +\n> +\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n> +}\n> +\n>  static ssize_t delete_region_store(struct device *dev,\n>  \t\t\t\t   struct device_attribute *attr,\n>  \t\t\t\t   const char *buf, size_t len)\n> @@ -4047,14 +4055,12 @@ static int __construct_region(struct cxl_region *cxlr,\n>  \treturn 0;\n>  }\n>  \n> -/* Establish an empty region covering the given HPA range */\n> -static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n> -\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n> +static struct cxl_region *construct_region_begin(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t\t\t struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> -\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n>  \tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n> -\tint rc, part = READ_ONCE(cxled->part);\n> +\tint part = READ_ONCE(cxled->part);\n>  \tstruct cxl_region *cxlr;\n>  \n>  \tdo {\n> @@ -4063,13 +4069,26 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \t\t\t\t       cxled->cxld.target_type);\n>  \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n>  \n> -\tif (IS_ERR(cxlr)) {\n> +\tif (IS_ERR(cxlr))\n>  \t\tdev_err(cxlmd->dev.parent,\n>  \t\t\t\"%s:%s: %s failed assign region: %ld\\n\",\n>  \t\t\tdev_name(&cxlmd->dev), dev_name(&cxled->cxld.dev),\n>  \t\t\t__func__, PTR_ERR(cxlr));\n> +\n> +\treturn cxlr;\n> +}\n> +\n> +/* Establish an empty region covering the given HPA range */\n> +static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n> +{\n> +\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n> +\tstruct cxl_region *cxlr;\n> +\tint rc;\n> +\n> +\tcxlr = construct_region_begin(cxlrd, cxled);\n> +\tif (IS_ERR(cxlr))\n>  \t\treturn cxlr;\n> -\t}\n>  \n>  \trc = __construct_region(cxlr, cxlrd, cxled);\n>  \tif (rc) {\n> @@ -4080,6 +4099,104 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \treturn cxlr;\n>  }\n>  \n> +DEFINE_FREE(cxl_region_drop, struct cxl_region *, if (_T) drop_region(_T))\n\nThis needs to be \"if (!IS_ERR_OR_NULL(_T) drop_region(_T)\". If construct_region_begin() returns an\nerror pointer, drop_region() will be called with it as of now leading to a garbage pointer deref.\n\n> +\n> +static struct cxl_region *\n> +__construct_new_region(struct cxl_root_decoder *cxlrd,\n> +\t\t       struct cxl_endpoint_decoder **cxled, int ways)\n> +{\n> +\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled[0]);\n> +\tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> +\tstruct cxl_region_params *p;\n> +\tresource_size_t size = 0;\n> +\tint rc, i;\n> +\n> +\tstruct cxl_region *cxlr __free(cxl_region_drop) =\n> +\t\tconstruct_region_begin(cxlrd, cxled[0]);\n> +\tif (IS_ERR(cxlr))\n> +\t\treturn cxlr;\n> +\n> +\tguard(rwsem_write)(&cxl_rwsem.region);\n> +\n> +\t/*\n> +\t * Sanity check. This should not happen with an accel driver handling\n> +\t * the region creation.\n> +\t */\n> +\tp = &cxlr->params;\n> +\tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE) {\n> +\t\tdev_err(cxlmd->dev.parent,\n> +\t\t\t\"%s:%s: %s  unexpected region state\\n\",\n> +\t\t\tdev_name(&cxlmd->dev), dev_name(&cxled[0]->cxld.dev),\n> +\t\t\t__func__);\n> +\t\treturn ERR_PTR(-EBUSY);\n> +\t}\n> +\n> +\trc = set_interleave_ways(cxlr, ways);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = set_interleave_granularity(cxlr, cxld->interleave_granularity);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.dpa) {\n> +\t\tfor (i = 0; i < ways; i++) {\n> +\t\t\tif (!cxled[i]->dpa_res)\n> +\t\t\t\treturn ERR_PTR(-EINVAL);\n> +\t\t\tsize += resource_size(cxled[i]->dpa_res);\n> +\t\t}\n> +\n> +\t\trc = alloc_hpa(cxlr, size);\n> +\t\tif (rc)\n> +\t\t\treturn ERR_PTR(rc);\n> +\n> +\t\tfor (i = 0; i < ways; i++) {\n> +\t\t\trc = cxl_region_attach(cxlr, cxled[i], 0);\n\nPosition parameter is hardcoded to 0. It should be set to i, right? This kind of goes back to my\nissues in patch 12/22; the interleaving functionality is there but it looks unused.\n\n> +\t\t\tif (rc)\n> +\t\t\t\treturn ERR_PTR(rc);\n> +\t\t}\n> +\t}\n> +\n> +\trc = cxl_region_decode_commit(cxlr);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\tp->state = CXL_CONFIG_COMMIT;\n> +\n> +\treturn no_free_ptr(cxlr);\n> +}\n> +\n> +/**\n> + * cxl_create_region - Establish a region given an endpoint decoder\n> + * @cxlrd: root decoder to allocate HPA\n> + * @cxled: endpoint decoders with reserved DPA capacity\n> + * @ways: interleave ways required\n> + *\n> + * Returns a fully formed region in the commit state and attached to the\n> + * cxl_region driver.\n> + */\n> +struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n> +\t\t\t\t     int ways)\n> +{\n> +\tstruct cxl_region *cxlr;\n> +\n> +\tmutex_lock(&cxlrd->range_lock);\n> +\tcxlr = __construct_new_region(cxlrd, cxled, ways);\n> +\tmutex_unlock(&cxlrd->range_lock);\n> +\tif (IS_ERR(cxlr))\n> +\t\treturn cxlr;\n> +\n> +\tif (device_attach(&cxlr->dev) <= 0) {\n> +\t\tdev_err(&cxlr->dev, \"failed to create region\\n\");\n> +\t\tdrop_region(cxlr);\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\t}\n> +\n> +\treturn cxlr;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_create_region, \"CXL\");\n> +\n>  static struct cxl_region *\n>  cxl_find_region_by_range(struct cxl_root_decoder *cxlrd, struct range *hpa)\n>  {\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 4802371db00e..50acbd13bcf8 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -281,4 +281,7 @@ struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t     enum cxl_partition_mode mode,\n>  \t\t\t\t\t     resource_size_t alloc);\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n> +struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n> +\t\t\t\t     int ways);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Differentiate CXL memory expanders (type 3) from CXL device accelerators\n> (type 2) with a new function for initializing cxl_dev_state and a macro\n> for helping accel drivers to embed cxl_dev_state inside a private\n> struct.\n> \n> Move structs to include/cxl as the size of the accel driver private\n> struct embedding cxl_dev_state needs to know the size of this struct.\n> \n> Use same new initialization with the type3 pci driver.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> ---\n>  drivers/cxl/core/mbox.c      |  12 +-\n>  drivers/cxl/core/memdev.c    |  32 +++++\n>  drivers/cxl/cxl.h            |  97 +--------------\n>  drivers/cxl/cxlmem.h         |  86 +------------\n>  drivers/cxl/pci.c            |  14 +--\n>  include/cxl/cxl.h            | 226 +++++++++++++++++++++++++++++++++++\n>  tools/testing/cxl/test/mem.c |   3 +-\n>  7 files changed, 274 insertions(+), 196 deletions(-)\n>  create mode 100644 include/cxl/cxl.h\n> \n> diff --git a/drivers/cxl/core/mbox.c b/drivers/cxl/core/mbox.c\n> index fa6dd0c94656..bee84d0101d1 100644\n> --- a/drivers/cxl/core/mbox.c\n> +++ b/drivers/cxl/core/mbox.c\n> @@ -1514,23 +1514,21 @@ int cxl_mailbox_init(struct cxl_mailbox *cxl_mbox, struct device *host)\n>  }\n>  EXPORT_SYMBOL_NS_GPL(cxl_mailbox_init, \"CXL\");\n>  \n> -struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev)\n> +struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n> +\t\t\t\t\t\t u16 dvsec)\n>  {\n>  \tstruct cxl_memdev_state *mds;\n>  \tint rc;\n>  \n> -\tmds = devm_kzalloc(dev, sizeof(*mds), GFP_KERNEL);\n> +\tmds = devm_cxl_dev_state_create(dev, CXL_DEVTYPE_CLASSMEM, serial,\n> +\t\t\t\t\tdvsec, struct cxl_memdev_state, cxlds,\n> +\t\t\t\t\ttrue);\n>  \tif (!mds) {\n>  \t\tdev_err(dev, \"No memory available\\n\");\n>  \t\treturn ERR_PTR(-ENOMEM);\n>  \t}\n>  \n>  \tmutex_init(&mds->event.log_lock);\n> -\tmds->cxlds.dev = dev;\n> -\tmds->cxlds.reg_map.host = dev;\n> -\tmds->cxlds.cxl_mbox.host = dev;\n> -\tmds->cxlds.reg_map.resource = CXL_RESOURCE_NONE;\n> -\tmds->cxlds.type = CXL_DEVTYPE_CLASSMEM;\n>  \n>  \trc = devm_cxl_register_mce_notifier(dev, &mds->mce_notifier);\n>  \tif (rc == -EOPNOTSUPP)\n> diff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\n> index af3d0cc65138..22d156f25305 100644\n> --- a/drivers/cxl/core/memdev.c\n> +++ b/drivers/cxl/core/memdev.c\n> @@ -656,6 +656,38 @@ static void detach_memdev(struct work_struct *work)\n>  \n>  static struct lock_class_key cxl_memdev_key;\n>  \n> +static void cxl_dev_state_init(struct cxl_dev_state *cxlds, struct device *dev,\n> +\t\t\t       enum cxl_devtype type, u64 serial, u16 dvsec,\n> +\t\t\t       bool has_mbox)\n> +{\n> +\t*cxlds = (struct cxl_dev_state) {\n> +\t\t.dev = dev,\n> +\t\t.type = type,\n> +\t\t.serial = serial,\n> +\t\t.cxl_dvsec = dvsec,\n> +\t\t.reg_map.host = dev,\n> +\t\t.reg_map.resource = CXL_RESOURCE_NONE,\n> +\t};\n> +\n> +\tif (has_mbox)\n> +\t\tcxlds->cxl_mbox.host = dev;\n> +}\n> +\n> +struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n> +\t\t\t\t\t\t enum cxl_devtype type,\n> +\t\t\t\t\t\t u64 serial, u16 dvsec,\n> +\t\t\t\t\t\t size_t size, bool has_mbox)\n> +{\n> +\tstruct cxl_dev_state *cxlds = devm_kzalloc(dev, size, GFP_KERNEL);\n> +\n> +\tif (!cxlds)\n> +\t\treturn NULL;\n> +\n> +\tcxl_dev_state_init(cxlds, dev, type, serial, dvsec, has_mbox);\n\nNit: Having a second function to do the init seems overkill here, especially since cxl_dev_state_init() isn't called outside this\nfunction. I'd fold it into this function instead, but I'm fine with it either way (especially if you were told otherwise before).\n\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\treturn cxlds;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(_devm_cxl_dev_state_create, \"CXL\");\n> +\n>  static struct cxl_memdev *cxl_memdev_alloc(struct cxl_dev_state *cxlds,\n>  \t\t\t\t\t   const struct file_operations *fops,\n>  \t\t\t\t\t   const struct cxl_memdev_attach *attach)\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index e1d47062e1d3..3eaa353e430b 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -12,6 +12,7 @@\n>  #include <linux/node.h>\n>  #include <linux/io.h>\n>  #include <linux/range.h>\n> +#include <cxl/cxl.h>\n>  \n>  extern const struct nvdimm_security_ops *cxl_security_ops;\n>  \n> @@ -201,97 +202,6 @@ static inline int ways_to_eiw(unsigned int ways, u8 *eiw)\n>  #define   CXLDEV_MBOX_BG_CMD_COMMAND_VENDOR_MASK GENMASK_ULL(63, 48)\n>  #define CXLDEV_MBOX_PAYLOAD_OFFSET 0x20\n>  \n> -/*\n> - * Using struct_group() allows for per register-block-type helper routines,\n> - * without requiring block-type agnostic code to include the prefix.\n> - */\n> -struct cxl_regs {\n> -\t/*\n> -\t * Common set of CXL Component register block base pointers\n> -\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n> -\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n> -\t */\n> -\tstruct_group_tagged(cxl_component_regs, component,\n> -\t\tvoid __iomem *hdm_decoder;\n> -\t\tvoid __iomem *ras;\n> -\t);\n> -\t/*\n> -\t * Common set of CXL Device register block base pointers\n> -\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n> -\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n> -\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n> -\t */\n> -\tstruct_group_tagged(cxl_device_regs, device_regs,\n> -\t\tvoid __iomem *status, *mbox, *memdev;\n> -\t);\n> -\n> -\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n> -\t\tvoid __iomem *pmu;\n> -\t);\n> -\n> -\t/*\n> -\t * RCH downstream port specific RAS register\n> -\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n> -\t */\n> -\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n> -\t\tvoid __iomem *dport_aer;\n> -\t);\n> -\n> -\t/*\n> -\t * RCD upstream port specific PCIe cap register\n> -\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n> -\t */\n> -\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n> -\t\tvoid __iomem *rcd_pcie_cap;\n> -\t);\n> -};\n> -\n> -struct cxl_reg_map {\n> -\tbool valid;\n> -\tint id;\n> -\tunsigned long offset;\n> -\tunsigned long size;\n> -};\n> -\n> -struct cxl_component_reg_map {\n> -\tstruct cxl_reg_map hdm_decoder;\n> -\tstruct cxl_reg_map ras;\n> -};\n> -\n> -struct cxl_device_reg_map {\n> -\tstruct cxl_reg_map status;\n> -\tstruct cxl_reg_map mbox;\n> -\tstruct cxl_reg_map memdev;\n> -};\n> -\n> -struct cxl_pmu_reg_map {\n> -\tstruct cxl_reg_map pmu;\n> -};\n> -\n> -/**\n> - * struct cxl_register_map - DVSEC harvested register block mapping parameters\n> - * @host: device for devm operations and logging\n> - * @base: virtual base of the register-block-BAR + @block_offset\n> - * @resource: physical resource base of the register block\n> - * @max_size: maximum mapping size to perform register search\n> - * @reg_type: see enum cxl_regloc_type\n> - * @component_map: cxl_reg_map for component registers\n> - * @device_map: cxl_reg_maps for device registers\n> - * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n> - */\n> -struct cxl_register_map {\n> -\tstruct device *host;\n> -\tvoid __iomem *base;\n> -\tresource_size_t resource;\n> -\tresource_size_t max_size;\n> -\tu8 reg_type;\n> -\tunion {\n> -\t\tstruct cxl_component_reg_map component_map;\n> -\t\tstruct cxl_device_reg_map device_map;\n> -\t\tstruct cxl_pmu_reg_map pmu_map;\n> -\t};\n> -};\n> -\n>  void cxl_probe_component_regs(struct device *dev, void __iomem *base,\n>  \t\t\t      struct cxl_component_reg_map *map);\n>  void cxl_probe_device_regs(struct device *dev, void __iomem *base,\n> @@ -497,11 +407,6 @@ struct cxl_region_params {\n>  \tresource_size_t cache_size;\n>  };\n>  \n> -enum cxl_partition_mode {\n> -\tCXL_PARTMODE_RAM,\n> -\tCXL_PARTMODE_PMEM,\n> -};\n> -\n>  /*\n>   * Indicate whether this region has been assembled by autodetection or\n>   * userspace assembly. Prevent endpoint decoders outside of automatic\n> diff --git a/drivers/cxl/cxlmem.h b/drivers/cxl/cxlmem.h\n> index ef202b34e5ea..281546de426e 100644\n> --- a/drivers/cxl/cxlmem.h\n> +++ b/drivers/cxl/cxlmem.h\n> @@ -113,8 +113,6 @@ int devm_cxl_dpa_reserve(struct cxl_endpoint_decoder *cxled,\n>  \t\t\t resource_size_t base, resource_size_t len,\n>  \t\t\t resource_size_t skipped);\n>  \n> -#define CXL_NR_PARTITIONS_MAX 2\n> -\n>  struct cxl_dpa_info {\n>  \tu64 size;\n>  \tstruct cxl_dpa_part_info {\n> @@ -373,87 +371,6 @@ struct cxl_security_state {\n>  \tstruct kernfs_node *sanitize_node;\n>  };\n>  \n> -/*\n> - * enum cxl_devtype - delineate type-2 from a generic type-3 device\n> - * @CXL_DEVTYPE_DEVMEM - Vendor specific CXL Type-2 device implementing HDM-D or\n> - *\t\t\t HDM-DB, no requirement that this device implements a\n> - *\t\t\t mailbox, or other memory-device-standard manageability\n> - *\t\t\t flows.\n> - * @CXL_DEVTYPE_CLASSMEM - Common class definition of a CXL Type-3 device with\n> - *\t\t\t   HDM-H and class-mandatory memory device registers\n> - */\n> -enum cxl_devtype {\n> -\tCXL_DEVTYPE_DEVMEM,\n> -\tCXL_DEVTYPE_CLASSMEM,\n> -};\n> -\n> -/**\n> - * struct cxl_dpa_perf - DPA performance property entry\n> - * @dpa_range: range for DPA address\n> - * @coord: QoS performance data (i.e. latency, bandwidth)\n> - * @cdat_coord: raw QoS performance data from CDAT\n> - * @qos_class: QoS Class cookies\n> - */\n> -struct cxl_dpa_perf {\n> -\tstruct range dpa_range;\n> -\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n> -\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n> -\tint qos_class;\n> -};\n> -\n> -/**\n> - * struct cxl_dpa_partition - DPA partition descriptor\n> - * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n> - * @perf: performance attributes of the partition from CDAT\n> - * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n> - */\n> -struct cxl_dpa_partition {\n> -\tstruct resource res;\n> -\tstruct cxl_dpa_perf perf;\n> -\tenum cxl_partition_mode mode;\n> -};\n> -\n> -/**\n> - * struct cxl_dev_state - The driver device state\n> - *\n> - * cxl_dev_state represents the CXL driver/device state.  It provides an\n> - * interface to mailbox commands as well as some cached data about the device.\n> - * Currently only memory devices are represented.\n> - *\n> - * @dev: The device associated with this CXL state\n> - * @cxlmd: The device representing the CXL.mem capabilities of @dev\n> - * @reg_map: component and ras register mapping parameters\n> - * @regs: Parsed register blocks\n> - * @cxl_dvsec: Offset to the PCIe device DVSEC\n> - * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n> - * @media_ready: Indicate whether the device media is usable\n> - * @dpa_res: Overall DPA resource tree for the device\n> - * @part: DPA partition array\n> - * @nr_partitions: Number of DPA partitions\n> - * @serial: PCIe Device Serial Number\n> - * @type: Generic Memory Class device or Vendor Specific Memory device\n> - * @cxl_mbox: CXL mailbox context\n> - * @cxlfs: CXL features context\n> - */\n> -struct cxl_dev_state {\n> -\tstruct device *dev;\n> -\tstruct cxl_memdev *cxlmd;\n> -\tstruct cxl_register_map reg_map;\n> -\tstruct cxl_regs regs;\n> -\tint cxl_dvsec;\n> -\tbool rcd;\n> -\tbool media_ready;\n> -\tstruct resource dpa_res;\n> -\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n> -\tunsigned int nr_partitions;\n> -\tu64 serial;\n> -\tenum cxl_devtype type;\n> -\tstruct cxl_mailbox cxl_mbox;\n> -#ifdef CONFIG_CXL_FEATURES\n> -\tstruct cxl_features_state *cxlfs;\n> -#endif\n> -};\n> -\n>  static inline resource_size_t cxl_pmem_size(struct cxl_dev_state *cxlds)\n>  {\n>  \t/*\n> @@ -858,7 +775,8 @@ int cxl_dev_state_identify(struct cxl_memdev_state *mds);\n>  int cxl_await_media_ready(struct cxl_dev_state *cxlds);\n>  int cxl_enumerate_cmds(struct cxl_memdev_state *mds);\n>  int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info);\n> -struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev);\n> +struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n> +\t\t\t\t\t\t u16 dvsec);\n>  void set_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n>  \t\t\t\tunsigned long *cmds);\n>  void clear_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n> diff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\n> index 1cf232220873..24179cc702bf 100644\n> --- a/drivers/cxl/pci.c\n> +++ b/drivers/cxl/pci.c\n> @@ -911,25 +911,25 @@ static int cxl_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n>  \tint rc, pmu_count;\n>  \tunsigned int i;\n>  \tbool irq_avail;\n> +\tu16 dvsec;\n>  \n>  \trc = pcim_enable_device(pdev);\n>  \tif (rc)\n>  \t\treturn rc;\n>  \tpci_set_master(pdev);\n>  \n> -\tmds = cxl_memdev_state_create(&pdev->dev);\n> +\tdvsec = pci_find_dvsec_capability(pdev, PCI_VENDOR_ID_CXL,\n> +\t\t\t\t\t  PCI_DVSEC_CXL_DEVICE);\n> +\tif (!dvsec)\n> +\t\tpci_warn(pdev, \"Device DVSEC not present, skip CXL.mem init\\n\");\n> +\n> +\tmds = cxl_memdev_state_create(&pdev->dev, pci_get_dsn(pdev), dvsec);\n>  \tif (IS_ERR(mds))\n>  \t\treturn PTR_ERR(mds);\n>  \tcxlds = &mds->cxlds;\n>  \tpci_set_drvdata(pdev, cxlds);\n>  \n>  \tcxlds->rcd = is_cxl_restricted(pdev);\n> -\tcxlds->serial = pci_get_dsn(pdev);\n> -\tcxlds->cxl_dvsec = pci_find_dvsec_capability(\n> -\t\tpdev, PCI_VENDOR_ID_CXL, PCI_DVSEC_CXL_DEVICE);\n> -\tif (!cxlds->cxl_dvsec)\n> -\t\tdev_warn(&pdev->dev,\n> -\t\t\t \"Device DVSEC not present, skip CXL.mem init\\n\");\n>  \n>  \trc = cxl_pci_setup_regs(pdev, CXL_REGLOC_RBI_MEMDEV, &map);\n>  \tif (rc)\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> new file mode 100644\n> index 000000000000..13d448686189\n> --- /dev/null\n> +++ b/include/cxl/cxl.h\n> @@ -0,0 +1,226 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +/* Copyright(c) 2020 Intel Corporation. */\n> +/* Copyright(c) 2025 Advanced Micro Devices, Inc. */\n> +\n> +#ifndef __CXL_CXL_H__\n> +#define __CXL_CXL_H__\n> +\n> +#include <linux/node.h>\n> +#include <linux/ioport.h>\n> +#include <cxl/mailbox.h>\n> +\n> +/**\n> + * enum cxl_devtype - delineate type-2 from a generic type-3 device\n> + * @CXL_DEVTYPE_DEVMEM: Vendor specific CXL Type-2 device implementing HDM-D or\n> + *\t\t\t HDM-DB, no requirement that this device implements a\n> + *\t\t\t mailbox, or other memory-device-standard manageability\n> + *\t\t\t flows.\n> + * @CXL_DEVTYPE_CLASSMEM: Common class definition of a CXL Type-3 device with\n> + *\t\t\t   HDM-H and class-mandatory memory device registers\n> + */\n> +enum cxl_devtype {\n> +\tCXL_DEVTYPE_DEVMEM,\n> +\tCXL_DEVTYPE_CLASSMEM,\n> +};\n> +\n> +struct device;\n> +\n> +/*\n> + * Using struct_group() allows for per register-block-type helper routines,\n> + * without requiring block-type agnostic code to include the prefix.\n> + */\n> +struct cxl_regs {\n> +\t/*\n> +\t * Common set of CXL Component register block base pointers\n> +\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n> +\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n> +\t */\n> +\tstruct_group_tagged(cxl_component_regs, component,\n> +\t\tvoid __iomem *hdm_decoder;\n> +\t\tvoid __iomem *ras;\n> +\t);\n> +\t/*\n> +\t * Common set of CXL Device register block base pointers\n> +\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n> +\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n> +\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n> +\t */\n> +\tstruct_group_tagged(cxl_device_regs, device_regs,\n> +\t\tvoid __iomem *status, *mbox, *memdev;\n> +\t);\n> +\n> +\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n> +\t\tvoid __iomem *pmu;\n> +\t);\n> +\n> +\t/*\n> +\t * RCH downstream port specific RAS register\n> +\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n> +\t */\n> +\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n> +\t\tvoid __iomem *dport_aer;\n> +\t);\n> +\n> +\t/*\n> +\t * RCD upstream port specific PCIe cap register\n> +\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n> +\t */\n> +\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n> +\t\tvoid __iomem *rcd_pcie_cap;\n> +\t);\n> +};\n> +\n> +struct cxl_reg_map {\n> +\tbool valid;\n> +\tint id;\n> +\tunsigned long offset;\n> +\tunsigned long size;\n> +};\n> +\n> +struct cxl_component_reg_map {\n> +\tstruct cxl_reg_map hdm_decoder;\n> +\tstruct cxl_reg_map ras;\n> +};\n> +\n> +struct cxl_device_reg_map {\n> +\tstruct cxl_reg_map status;\n> +\tstruct cxl_reg_map mbox;\n> +\tstruct cxl_reg_map memdev;\n> +};\n> +\n> +struct cxl_pmu_reg_map {\n> +\tstruct cxl_reg_map pmu;\n> +};\n> +\n> +/**\n> + * struct cxl_register_map - DVSEC harvested register block mapping parameters\n> + * @host: device for devm operations and logging\n> + * @base: virtual base of the register-block-BAR + @block_offset\n> + * @resource: physical resource base of the register block\n> + * @max_size: maximum mapping size to perform register search\n> + * @reg_type: see enum cxl_regloc_type\n> + * @component_map: cxl_reg_map for component registers\n> + * @device_map: cxl_reg_maps for device registers\n> + * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n> + */\n> +struct cxl_register_map {\n> +\tstruct device *host;\n> +\tvoid __iomem *base;\n> +\tresource_size_t resource;\n> +\tresource_size_t max_size;\n> +\tu8 reg_type;\n> +\tunion {\n> +\t\tstruct cxl_component_reg_map component_map;\n> +\t\tstruct cxl_device_reg_map device_map;\n> +\t\tstruct cxl_pmu_reg_map pmu_map;\n> +\t};\n> +};\n> +\n> +/**\n> + * struct cxl_dpa_perf - DPA performance property entry\n> + * @dpa_range: range for DPA address\n> + * @coord: QoS performance data (i.e. latency, bandwidth)\n> + * @cdat_coord: raw QoS performance data from CDAT\n> + * @qos_class: QoS Class cookies\n> + */\n> +struct cxl_dpa_perf {\n> +\tstruct range dpa_range;\n> +\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n> +\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n> +\tint qos_class;\n> +};\n> +\n> +enum cxl_partition_mode {\n> +\tCXL_PARTMODE_RAM,\n> +\tCXL_PARTMODE_PMEM,\n> +};\n> +\n> +/**\n> + * struct cxl_dpa_partition - DPA partition descriptor\n> + * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n> + * @perf: performance attributes of the partition from CDAT\n> + * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n> + */\n> +struct cxl_dpa_partition {\n> +\tstruct resource res;\n> +\tstruct cxl_dpa_perf perf;\n> +\tenum cxl_partition_mode mode;\n> +};\n> +\n> +#define CXL_NR_PARTITIONS_MAX 2\n> +\n> +/**\n> + * struct cxl_dev_state - The driver device state\n> + *\n> + * cxl_dev_state represents the CXL driver/device state.  It provides an\n> + * interface to mailbox commands as well as some cached data about the device.\n> + * Currently only memory devices are represented.\n> + *\n> + * @dev: The device associated with this CXL state\n> + * @cxlmd: The device representing the CXL.mem capabilities of @dev\n> + * @reg_map: component and ras register mapping parameters\n> + * @regs: Parsed register blocks\n> + * @cxl_dvsec: Offset to the PCIe device DVSEC\n> + * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n> + * @media_ready: Indicate whether the device media is usable\n> + * @dpa_res: Overall DPA resource tree for the device\n> + * @part: DPA partition array\n> + * @nr_partitions: Number of DPA partitions\n> + * @serial: PCIe Device Serial Number\n> + * @type: Generic Memory Class device or Vendor Specific Memory device\n> + * @cxl_mbox: CXL mailbox context\n> + * @cxlfs: CXL features context\n> + */\n> +struct cxl_dev_state {\n> +\t/* public for Type2 drivers */\n> +\tstruct device *dev;\n> +\tstruct cxl_memdev *cxlmd;\n> +\n> +\t/* private for Type2 drivers */\n> +\tstruct cxl_register_map reg_map;\n> +\tstruct cxl_regs regs;\n> +\tint cxl_dvsec;\n> +\tbool rcd;\n> +\tbool media_ready;\n> +\tstruct resource dpa_res;\n> +\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n> +\tunsigned int nr_partitions;\n> +\tu64 serial;\n> +\tenum cxl_devtype type;\n> +\tstruct cxl_mailbox cxl_mbox;\n> +#ifdef CONFIG_CXL_FEATURES\n> +\tstruct cxl_features_state *cxlfs;\n> +#endif\n> +};\n> +\n> +struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n> +\t\t\t\t\t\t enum cxl_devtype type,\n> +\t\t\t\t\t\t u64 serial, u16 dvsec,\n> +\t\t\t\t\t\t size_t size, bool has_mbox);\n> +\n> +/**\n> + * cxl_dev_state_create - safely create and cast a cxl dev state embedded in a\n> + * driver specific struct.\n> + *\n> + * @parent: device behind the request\n> + * @type: CXL device type\n> + * @serial: device identification\n> + * @dvsec: dvsec capability offset\n> + * @drv_struct: driver struct embedding a cxl_dev_state struct\n> + * @member: drv_struct member as cxl_dev_state\n> + * @mbox: true if mailbox supported\n> + *\n> + * Returns a pointer to the drv_struct allocated and embedding a cxl_dev_state\n> + * struct initialized.\n> + *\n> + * Introduced for Type2 driver support.\n> + */\n> +#define devm_cxl_dev_state_create(parent, type, serial, dvsec, drv_struct, member, mbox)\t\\\n> +\t({\t\t\t\t\t\t\t\t\t\t\\\n> +\t\tstatic_assert(__same_type(struct cxl_dev_state,\t\t\t\t\\\n> +\t\t\t      ((drv_struct *)NULL)->member));\t\t\t\t\\\n> +\t\tstatic_assert(offsetof(drv_struct, member) == 0);\t\t\t\\\n> +\t\t(drv_struct *)_devm_cxl_dev_state_create(parent, type, serial, dvsec,\t\\\n> +\t\t\t\t\t\t      sizeof(drv_struct), mbox);\t\\\n> +\t})\n> +#endif /* __CXL_CXL_H__ */\n> diff --git a/tools/testing/cxl/test/mem.c b/tools/testing/cxl/test/mem.c\n> index cb87e8c0e63c..79f42f4474d4 100644\n> --- a/tools/testing/cxl/test/mem.c\n> +++ b/tools/testing/cxl/test/mem.c\n> @@ -1716,7 +1716,7 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> -\tmds = cxl_memdev_state_create(dev);\n> +\tmds = cxl_memdev_state_create(dev, pdev->id + 1, 0);\n>  \tif (IS_ERR(mds))\n>  \t\treturn PTR_ERR(mds);\n>  \n> @@ -1732,7 +1732,6 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n>  \tmds->event.buf = (struct cxl_get_event_payload *) mdata->event_buf;\n>  \tINIT_DELAYED_WORK(&mds->security.poll_dwork, cxl_mockmem_sanitize_work);\n>  \n> -\tcxlds->serial = pdev->id + 1;\n>  \tif (is_rcd(pdev))\n>  \t\tcxlds->rcd = true;\n>  \n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> A Type2 device configured by the BIOS can already have its HDM\n> committed. Add a cxl_get_committed_decoder() function for cheking\n> so after memdev creation. A CXL region should have been created\n> during memdev initialization, therefore a Type2 driver can ask for\n> such a region for working with the HPA. If the HDM is not committed,\n> a Type2 driver will create the region after obtaining proper HPA\n> and DPA space.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/cxl/core/hdm.c | 39 +++++++++++++++++++++++++++++++++++++++\n>  include/cxl/cxl.h      |  3 +++\n>  2 files changed, 42 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index 6e516c69b2d2..a172ce4e9b19 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -686,6 +686,45 @@ int cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  \treturn devm_add_action_or_reset(&port->dev, cxl_dpa_release, cxled);\n>  }\n>  \n> +static int find_committed_endpoint_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == port->hdm_end;\n\nIs this the way you're supposed to check if a decoder is committed? The doc comment for @hdm_end in\nstruct cxl_port says it's just the last allocated decoder. If allocated decoders are always committed then\nI'm fine with this, otherwise I think you'd want to a register read or something to find the commit state.\n> +}\n> +\n> +struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t\t       struct cxl_region **cxlr)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct device *cxled_dev;\n> +\n> +\tif (!endpoint)\n> +\t\treturn NULL;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tcxled_dev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\t      find_committed_endpoint_decoder);\n> +\n> +\tif (!cxled_dev)\n> +\t\treturn NULL;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(cxled_dev);\n> +\t*cxlr = cxled->cxld.region;\n> +\n> +\tput_device(cxled_dev);\n> +\treturn cxled;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_committed_decoder, \"CXL\");\n> +\n>  static void cxld_set_interleave(struct cxl_decoder *cxld, u32 *ctrl)\n>  {\n>  \tu16 eig;\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 6f8d365067af..928276dba952 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -249,4 +249,7 @@ int cxl_map_component_regs(const struct cxl_register_map *map,\n>  int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n>  struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n>  \t\t\t\t       const struct cxl_memdev_attach *attach);\n> +struct cxl_region;\n> +struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t\t       struct cxl_region **cxlr);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> This patchset should be applied on the cxl next branch using the base\n> specified at the end of this cover letter.\n> \n> Dependencies on Dan's work has gone and also on Terry's as the only\n> patch required is now in next. The other dependency is on Smita patchset\n> but it does not exist such a dependency as that work will not avoid the\n> problem with Type2 and DAX/hmem if soft reserved memory. This needs to\n> be solved by the BIOS and Type2 UEFI driver for populating the CXL.mem\n> range as EFI_RESERVED_TYPE instead of default EFI_CONVENTIONAL_MEMORY\n> with the EFI_MEMORY_SP attribute. There exists though a dependency on\n> one Smita's patches:\n> \n> [PATCH v5 3/7] cxl/region: Skip decoder reset on detach for autodiscovered regions\n> \n> This is needed for the default behaviour with current BIOS configuration\n> where the HDM Type2 decoders will be kept unreset when driver unloads.\n> This is the main change introduced in v23: committed decoders will not\n> be reset. Previous v22 functionality supported first driver load finding\n> committed decoders but resetting them at unload and supporting\n> uncommitted decoders in next driver loads. This will be suported in\n> follow-up works.\n> \n> v23 changes:\n> \n>   patch 11: fixing minor issues and droping change in\n> \t    should_emulate_decoders (Jonathan Cameron)\n> \n>   patch13: refactoring unregister_region for safety type in Type2 API\n> \n>   sfc changes: slight modifications to error path\n> \n\nThis cover letter is really long, I'd remove the change logs for anything more\nthan 3 revisions back (assuming a v24 is needed). After that you could leave\na lore link for older revisions if you want, but it's not needed imo.\nAlso, feel free to add my Reviewed-by for anything I didn't leave a comment on\n(felt I should cut down on the mail).\n\nThanks,\nBen\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation involves finding available DPA (device-physical-address)\n> capacity to map into HPA (host-physical-address) space.\n> \n> In order to support CXL Type2 devices, define an API, cxl_request_dpa(),\n> that tries to allocate the DPA memory the driver requires to operate.The\n> memory requested should not be bigger than the max available HPA obtained\n> previously with cxl_get_hpa_freespace().\n> \n> Based on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h      |  1 +\n>  include/cxl/cxl.h      |  5 +++\n>  3 files changed, 90 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index a172ce4e9b19..d60a697f12cc 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -3,6 +3,7 @@\n>  #include <linux/seq_file.h>\n>  #include <linux/device.h>\n>  #include <linux/delay.h>\n> +#include <cxl/cxl.h>\n>  \n>  #include \"cxlmem.h\"\n>  #include \"core.h\"\n> @@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n>  \treturn resource_contains(res, &_addr);\n>  }\n>  \n> +/**\n> + * cxl_dpa_free - release DPA (Device Physical Address)\n> + * @cxled: endpoint decoder linked to the DPA\n> + *\n> + * Returns 0 or error.\n> + */\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_port *port = cxled_to_port(cxled);\n> @@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  \tdevm_cxl_dpa_release(cxled);\n>  \treturn 0;\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n>  \n>  int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \t\t     enum cxl_partition_mode mode)\n> @@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \treturn 0;\n>  }\n>  \n> +static int find_free_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == (port->hdm_end + 1);\n> +}\n> +\n> +static struct cxl_endpoint_decoder *\n> +cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct device *dev;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tdev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\tfind_free_decoder);\n> +\tif (!dev)\n> +\t\treturn NULL;\n> +\n> +\treturn to_cxl_endpoint_decoder(dev);\n> +}\n> +\n> +/**\n> + * cxl_request_dpa - search and reserve DPA given input constraints\n> + * @cxlmd: memdev with an endpoint port with available decoders\n> + * @mode: CXL partition mode (ram vs pmem)\n> + * @alloc: dpa size required\n> + *\n> + * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n> + * an errno encoded pointer on failure.\n> + *\n> + * Given that a region needs to allocate from limited HPA capacity it\n> + * may be the case that a device has more mappable DPA capacity than\n> + * available HPA. The expectation is that @alloc is a driver known\n> + * value based on the device capacity but which could not be fully\n> + * available due to HPA constraints.\n> + *\n> + * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n> + * reserved, or an error pointer. The caller is also expected to own the\n> + * lifetime of the memdev registration associated with the endpoint to\n> + * pin the decoder registered as well.\n> + */\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc)\n> +{\n> +\tint rc;\n> +\n> +\tif (!IS_ALIGNED(alloc, SZ_256M))\n> +\t\treturn ERR_PTR(-EINVAL);\n> +\n> +\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n> +\t\tcxl_find_free_decoder(cxlmd);\n> +\n> +\tif (!cxled)\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\n> +\trc = cxl_dpa_set_part(cxled, mode);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = cxl_dpa_alloc(cxled, alloc);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n\nShould cxl_dpa_set_part() be unwound here, or does it not matter? If it doesn't matter:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\n> +\treturn no_free_ptr(cxled);\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n> +\n>  static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index d1b010e5e1d0..2b1f7d687a0e 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n>  \n>  DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n>  DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n> +DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n>  DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n>  DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 783ad570a6eb..4802371db00e 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -7,6 +7,7 @@\n>  \n>  #include <linux/node.h>\n>  #include <linux/ioport.h>\n> +#include <linux/range.h>\n>  #include <cxl/mailbox.h>\n>  \n>  /**\n> @@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t       unsigned long flags,\n>  \t\t\t\t\t       resource_size_t *max);\n>  void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc);\n> +int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\n\n\nOn 2/19/2026 4:40 AM, Alejandro Lucero Palau wrote:\n> \n> On 2/11/26 22:11, Cheatham, Benjamin wrote:\n>> On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n>>> From: Alejandro Lucero <alucerop@amd.com>\n>>>\n>>> Region creation based on Type3 devices is triggered from user space\n>>> allowing memory combination through interleaving.\n>>>\n>>> In preparation for kernel driven region creation, that is Type2 drivers\n>>> triggering region creation backed with its advertised CXL memory, factor\n>>> out a common helper from the user-sysfs region setup for interleave ways.\n>>>\n>>> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n>>> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n>>> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n>>> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n>>> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>>> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n>>> ---\n>>>  drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n>>>  1 file changed, 27 insertions(+), 16 deletions(-)\n>>>\n>>> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n>>> index f53b2e9fd9e6..ece1d3df7cf1 100644\n>>> --- a/drivers/cxl/core/region.c\n>>> +++ b/drivers/cxl/core/region.c\n>>> @@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n>>>   static const struct attribute_group *get_cxl_region_target_group(void);\n>>>  -static ssize_t interleave_ways_store(struct device *dev,\n>>> - struct device_attribute *attr,\n>>> - const char *buf, size_t len)\n>>> +static int set_interleave_ways(struct cxl_region *cxlr, int val)\n>> @val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\n>> function was originally coded with that in mind (same with @save below).\n> \n> Good catch. I wonder if I should just change the way the value is obtained, using kstrtoint instead of kstrtouint, as those values are used for cxl_region_params fields defined as int. In other words, it seems doing that simpler than changing all the other places you mention and the structs involved. I can not see a reason for using unsigned int so I think I will follow that approach. Tell me if you think otherwise.\n> \n\nIf I had to guess unsigned int was used because a negative interleave granularity/ways makes no sense. I think your suggestion is fine though since no one\nin their right mind would give anything but a (relatively) small and positive value for these.\n\nThanks,\nBen\n\n> \n> Thank you\n> \n> \n>> With that cleaned up:\n>> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>>\n>>>  {\n>>> - struct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n>>> + struct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>>>  struct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n>>> - struct cxl_region *cxlr = to_cxl_region(dev);\n>>>  struct cxl_region_params *p = &cxlr->params;\n>>> - unsigned int val, save;\n>>> - int rc;\n>>> + int save, rc;\n>>>  u8 iw;\n>>>  - rc = kstrtouint(buf, 0, &val);\n>>> - if (rc)\n>>> - return rc;\n>>> -\n>>>  rc = ways_to_eiw(val, &iw);\n>>>  if (rc)\n>>>  return rc;\n>>> @@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n>>>  return -EINVAL;\n>>>  }\n>>>  - ACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n>>> - if ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n>>> - return rc;\n>>> + lockdep_assert_held_write(&cxl_rwsem.region);\n>>>   if (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>>>  return -EBUSY;\n>>> @@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n>>>  save = p->interleave_ways;\n>>>  p->interleave_ways = val;\n>>>  rc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n>>> - if (rc) {\n>>> + if (rc)\n>>>  p->interleave_ways = save;\n>>> +\n>>> + return rc;\n>>> +}\n>>> +\n>>> +static ssize_t interleave_ways_store(struct device *dev,\n>>> + struct device_attribute *attr,\n>>> + const char *buf, size_t len)\n>>> +{\n>>> + struct cxl_region *cxlr = to_cxl_region(dev);\n>>> + unsigned int val;\n>>> + int rc;\n>>> +\n>>> + rc = kstrtouint(buf, 0, &val);\n>>> + if (rc)\n>>> + return rc;\n>>> +\n>>> + ACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n>>> + if ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n>>> + return rc;\n>>> +\n>>> + rc = set_interleave_ways(cxlr, val);\n>>> + if (rc)\n>>>  return rc;\n>>> - }\n>>>   return len;\n>>>  }\n\n\n\n---\n\nOn 2/19/2026 3:58 AM, Alejandro Lucero Palau wrote:\n> \n> On 2/11/26 22:10, Cheatham, Benjamin wrote:\n>> On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n>>> From: Alejandro Lucero <alucerop@amd.com>\n>>>\n>>> CXL region creation involves allocating capacity from Device Physical\n>>> Address (DPA) and assigning it to decode a given Host Physical Address\n>>> (HPA). Before determining how much DPA to allocate the amount of available\n>>> HPA must be determined. Also, not all HPA is created equal, some HPA\n>>> targets RAM, some targets PMEM, some is prepared for device-memory flows\n>>> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n>>>\n>>> In order to support Type2 CXL devices, wrap all of those concerns into\n>>> an API that retrieves a root decoder (platform CXL window) that fits the\n>>> specified constraints and the capacity available for a new region.\n>>>\n>>> Add a complementary function for releasing the reference to such root\n>>> decoder.\n>>>\n>>> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n>>>\n>>> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n>>> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n>>> ---\n>>>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>>>  drivers/cxl/cxl.h | 3 +\n>>>  include/cxl/cxl.h | 6 ++\n>>>  3 files changed, 173 insertions(+)\n>>>\n>>> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n>>> index 954b8fcdbac6..bdefd088f5f1 100644\n>>> --- a/drivers/cxl/core/region.c\n>>> +++ b/drivers/cxl/core/region.c\n>>> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>>>  return 0;\n>>>  }\n>>>  +struct cxlrd_max_context {\n>>> + struct device * const *host_bridges;\n>>> + int interleave_ways;\n>>> + unsigned long flags;\n>>> + resource_size_t max_hpa;\n>>> + struct cxl_root_decoder *cxlrd;\n>>> +};\n>>> +\n>>> +static int find_max_hpa(struct device *dev, void *data)\n>>> +{\n>>> + struct cxlrd_max_context *ctx = data;\n>>> + struct cxl_switch_decoder *cxlsd;\n>>> + struct cxl_root_decoder *cxlrd;\n>>> + struct resource *res, *prev;\n>>> + struct cxl_decoder *cxld;\n>>> + resource_size_t free = 0;\n>>> + resource_size_t max;\n>>> + int found = 0;\n>>> +\n>>> + if (!is_root_decoder(dev))\n>>> + return 0;\n>>> +\n>>> + cxlrd = to_cxl_root_decoder(dev);\n>>> + cxlsd = &cxlrd->cxlsd;\n>>> + cxld = &cxlsd->cxld;\n>>> +\n>>> + if ((cxld->flags & ctx->flags) != ctx->flags) {\n>>> + dev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n>>> + cxld->flags, ctx->flags);\n>>> + return 0;\n>>> + }\n>>> +\n>>> + for (int i = 0; i < ctx->interleave_ways; i++) {\n>>> + for (int j = 0; j < ctx->interleave_ways; j++) {\n>>> + if (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n>>> + found++;\n>>> + break;\n>>> + }\n>>> + }\n>>> + }\n>> This may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\n>> what the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\n>> bridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\n>> point, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\n>> be a struct device * const.\n>>\n>> Maybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\n>> I'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n> \n> \n> Hi Ben,\n> \n> \n> I do remember this one.\n> \n> \n> Dan's original patches had this support for interleaving, then I removed it as the case for Type2 and interleaving is quite unlikely, at least right now and likely in the near future. But I was told why do not support it as it was trivial to do so. FWIW, If I think only about the use case coming with the patchset, I agree with you, but because those previous discussions, I think I have to leave it.\n> \n\nI'm fine with that, but I would at least do the fix with the decoder position in 19/22 and make a note that the\ninterleave_ways parameter in cxl_get_hpa_freespace() below is currently unused (unless I'm misunderstanding\nthe endpoint->host_bridge member).\n\nThat way, the support is mostly there and just requires a small, previously noted, addition to enable. If you're\nfine with that then feel free to add my Reviewed-by after implementing in v24.\n\nThanks,\nBen\n\n> \n> Thank you\n> \n> \n>>> +\n>>> + if (found != ctx->interleave_ways) {\n>>> + dev_dbg(dev,\n>>> + \"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n>>> + found, ctx->interleave_ways);\n>>> + return 0;\n>>> + }\n>>> +\n>>> + /*\n>>> + * Walk the root decoder resource range relying on cxl_rwsem.region to\n>>> + * preclude sibling arrival/departure and find the largest free space\n>>> + * gap.\n>>> + */\n>>> + lockdep_assert_held_read(&cxl_rwsem.region);\n>>> + res = cxlrd->res->child;\n>>> +\n>>> + /* With no resource child the whole parent resource is available */\n>>> + if (!res)\n>>> + max = resource_size(cxlrd->res);\n>>> + else\n>>> + max = 0;\n>>> +\n>>> + for (prev = NULL; res; prev = res, res = res->sibling) {\n>>> + if (!prev && res->start == cxlrd->res->start &&\n>>> + res->end == cxlrd->res->end) {\n>>> + max = resource_size(cxlrd->res);\n>>> + break;\n>>> + }\n>>> + /*\n>>> + * Sanity check for preventing arithmetic problems below as a\n>>> + * resource with size 0 could imply using the end field below\n>>> + * when set to unsigned zero - 1 or all f in hex.\n>>> + */\n>>> + if (prev && !resource_size(prev))\n>>> + continue;\n>>> +\n>>> + if (!prev && res->start > cxlrd->res->start) {\n>>> + free = res->start - cxlrd->res->start;\n>>> + max = max(free, max);\n>>> + }\n>>> + if (prev && res->start > prev->end + 1) {\n>>> + free = res->start - prev->end + 1;\n>>> + max = max(free, max);\n>>> + }\n>>> + }\n>>> +\n>>> + if (prev && prev->end + 1 < cxlrd->res->end + 1) {\n>>> + free = cxlrd->res->end + 1 - prev->end + 1;\n>>> + max = max(free, max);\n>>> + }\n>>> +\n>>> + dev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n>>> + if (max > ctx->max_hpa) {\n>>> + if (ctx->cxlrd)\n>>> + put_device(cxlrd_dev(ctx->cxlrd));\n>>> + get_device(cxlrd_dev(cxlrd));\n>>> + ctx->cxlrd = cxlrd;\n>>> + ctx->max_hpa = max;\n>>> + }\n>>> + return 0;\n>>> +}\n>>> +\n>>> +/**\n>>> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n>>> + * @cxlmd: the mem device requiring the HPA\n>>> + * @interleave_ways: number of entries in @host_bridges\n>>> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n>>> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n>>> + * returned decoder\n>>> + *\n>>> + * Returns a pointer to a struct cxl_root_decoder\n>>> + *\n>>> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n>>> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n>>> + * caller goes to use this decoder and its capacity is reduced then caller needs\n>>> + * to loop and retry.\n>>> + *\n>>> + * The returned root decoder has an elevated reference count that needs to be\n>>> + * put with cxl_put_root_decoder(cxlrd).\n>>> + */\n>>> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>>> + int interleave_ways,\n>>> + unsigned long flags,\n>>> + resource_size_t *max_avail_contig)\n>>> +{\n>>> + struct cxlrd_max_context ctx = {\n>>> + .flags = flags,\n>>> + .interleave_ways = interleave_ways,\n>>> + };\n>>> + struct cxl_port *root_port;\n>>> + struct cxl_port *endpoint;\n>>> +\n>>> + endpoint = cxlmd->endpoint;\n>>> + if (!endpoint) {\n>>> + dev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n>>> + return ERR_PTR(-ENXIO);\n>>> + }\n>>> +\n>>> + ctx.host_bridges = &endpoint->host_bridge;\n>> Mentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\n>> something). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\n>> I'm not sure of what the fix is to get the intended behavior.\n>>\n>> It may be worth getting rid of the interleave_ways portion of this function and\n>> add it later when someone needs it. You could also explain it's hard coded to 1/unused\n>> in the doc comment if you know of an immediate need for it.\n>>\n>>> +\n>>> + struct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n>>> + if (!root) {\n>>> + dev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n>>> + return ERR_PTR(-ENXIO);\n>>> + }\n>>> +\n>>> + root_port = &root->port;\n>>> + scoped_guard(rwsem_read, &cxl_rwsem.region)\n>>> + device_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n>> Can just use a guard() here.\n>>\n>>> +\n>>> + if (!ctx.cxlrd)\n>>> + return ERR_PTR(-ENOMEM);\n>>> +\n>>> + *max_avail_contig = ctx.max_hpa;\n>>> + return ctx.cxlrd;\n>>> +}\n>>> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n>>> +\n>>> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n>>> +{\n>>> + put_device(cxlrd_dev(cxlrd));\n>>> +}\n>>> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n>>> +\n>>>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>>>  const char *buf, size_t len)\n>>>  {\n>>> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n>>> index 944c5d1ccceb..c7d9b2c2908f 100644\n>>> --- a/drivers/cxl/cxl.h\n>>> +++ b/drivers/cxl/cxl.h\n>>> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>>>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>>>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>>>  bool is_root_decoder(struct device *dev);\n>>> +\n>>> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n>>> +\n>>>  bool is_switch_decoder(struct device *dev);\n>>>  bool is_endpoint_decoder(struct device *dev);\n>>>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n>>> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n>>> index 92880c26b2d5..834dc7e78934 100644\n>>> --- a/include/cxl/cxl.h\n>>> +++ b/include/cxl/cxl.h\n>>> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>>>  struct range;\n>>>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>>>  void cxl_unregister_region(struct cxl_region *cxlr);\n>>> +struct cxl_port;\n>>> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>>> + int interleave_ways,\n>>> + unsigned long flags,\n>>> + resource_size_t *max);\n>>> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>>>  #endif /* __CXL_CXL_H__ */\n\n",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer noted that the error inside sfc should not be fatal for cxl sfc initialization and suggested fallback to another cxl initialization possibility; also raised questions about handling multiple HDMs",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Ben,\n\n\nYes, I think you are right. This works in my tests and it is safe \nbecause I check the region does exist before using it. But the error \ninside sfc should then not be fatal for cxl sfc initialization and \nfallback to the other cxl initialization possibility.\n\n\nIf I add the check for the decoder state, I guess I can keep the \nfunction names. If I rely on the region being there, I should change \nthem. I will think about it.\n\n\nThis also brings the question of what is more than one hdm present. This \nis not needed in my use case and likely this is also true for other \ncoming Type2 devices, but it does also require further thinking.\n\n\nThank you!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price noted that the patch introduces a potential reference leak and suggested further investigation.\n\nThe reviewer noted that in the cxl_find_free_decoder function, a device reference obtained by device_find_child is not properly released due to a missing put_device call, resulting in a permanent elevation of the endpoint decoder device refcount.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential reference leak",
                "leak",
                "device reference"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This review was generated by kreview-0811365ff2. \r\n\r\nThis is not an automated email, the reviewer though this report\r\nlooked valid enough to consider discussion.\r\n\r\n----\r\n\r\nCursory browse, this does look like a legitimate reference leak.\r\n\r\n~Gregory\r\n\r\n----\n\n---\n\nDoes this leak the device reference obtained by device_find_child()\r\nin cxl_find_free_decoder()?\r\n\r\ndevice_find_child() increments the device refcount (reference A).\r\nThen cxl_dpa_alloc() -> __cxl_dpa_reserve() calls get_device() on\r\nthe same device (reference B).\r\n\r\nOn the success path, no_free_ptr() inhibits the put_cxled cleanup,\r\nso reference A is transferred to the caller.  The matching cleanup\r\nfunction cxl_dpa_free() calls __cxl_dpa_release(), which drops\r\nreference B via put_device(), but reference A is never released:\r\n\r\n    cxl_dpa_free()\r\n      -> devm_cxl_dpa_release()\r\n        -> __cxl_dpa_release()\r\n          -> put_device()     /* drops reference B only */\r\n\r\n    /* reference A from device_find_child() is still held */\r\n\r\nThe only caller in this series (sfc efx_cxl_init/efx_cxl_exit)\r\nnever calls put_device() on the returned cxled either, so the\r\nendpoint decoder device refcount remains permanently elevated.",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "reviewer noted that the cxl by accelerators patch may have ordering issues due to acquiring the per-vswap spinlock while holding the folio lock in vswap_free(), which could lead to a lock ordering violation with reclaim paths\n\nThe reviewer noted that cxl_dpa_free() is called while the endpoint decoder is still attached to the region, resulting in a -EBUSY return value without freeing the DPA. The reviewer suggested calling cxl_unregister_region() before cxl_dpa_free() and cxl_put_root_decoder() to fix the ordering issue.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "ordering question seems legit",
                "ordering issue",
                "potential bug"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This review was generated by kreview-0811365ff2. \r\n\r\nThis is not an automated email, the reviewer though this report\r\nlooked valid enough to consider discussion.\r\n\r\n----\r\nsfc code so won't speak to the internals, but ordering question seems\r\nlegit.\r\n\r\n~Gregory\r\n----\r\n\r\nOn Sat, Feb 01, 2026, Alejandro Lucero wrote:\n\n---\n\nIn the non-committed path, cxl_dpa_free() is called while the\r\nendpoint decoder is still attached to the region.  Looking at\r\ncxl_dpa_free() in drivers/cxl/core/hdm.c:\r\n\r\n    if (cxled->cxld.region) {\r\n        dev_dbg(dev, \"decoder assigned to: %s\\n\",\r\n            dev_name(&cxled->cxld.region->dev));\r\n        return -EBUSY;\r\n    }\r\n\r\nSince cxl_unregister_region() has not run yet, cxled->cxld.region\r\nis still set, and cxl_dpa_free() returns -EBUSY without freeing\r\nthe DPA.  The return value is not checked.\r\n\r\nShould cxl_unregister_region() be called before cxl_dpa_free()\r\nand cxl_put_root_decoder() in the else branch, matching the\r\nreverse order of allocation in efx_cxl_init()?\r\n\r\nThe cover letter notes that v23 expects committed decoders as the\r\nprimary flow, and uncommitted decoder support is deferred to\r\nfollow-up work, so this else branch may not be reachable in\r\npractice today.  Still worth fixing the ordering now so it\r\ndoesn't bite when the uncommitted path is enabled later.\r\n\r\nThis issue is not fixed by the remaining commits in the series\r\n(through 10fe989f9e85).",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price expressed uncertainty about the code and its purpose, stating he is unfamiliar with it but finds the question posed by the code reasonable.\n\nThe reviewer noted that the CXL path does not set nic_data->pio_write_vi_base, which causes efx_ef10_link_piobufs() to issue MC_CMD_LINK_PIOBUF commands and perform a special-case check using incorrect VI instances. The reviewer suggested that the struct field should be updated with the correct non-zero value of pio_write_vi_base.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "uncertainty",
                "lack of familiarity",
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This review was generated by kreview-0811365ff2. \r\n\r\nThis is not an automated email, the reviewer though this report\r\nlooked valid enough to consider discussion.\r\n\r\n----\r\nI am completely unfamiliar with this code, but the question it poses\r\nat least seems reasonable.\r\n\r\n~Gregory\r\n----\r\n\r\nOn Sat, Feb 01, 2026, Alejandro Lucero wrote:\n\n---\n\nThe CXL path sets nic_data->pio_write_base but does not set\r\nnic_data->pio_write_vi_base, while the legacy path does:\r\n\r\n    nic_data->pio_write_vi_base = pio_write_vi_base;\r\n\r\nSince nic_data is kzalloc'd, pio_write_vi_base stays at 0 in the CXL\r\npath.  efx_ef10_link_piobufs() then uses nic_data->pio_write_vi_base\r\nto issue MC_CMD_LINK_PIOBUF commands:\r\n\r\n    MCDI_SET_DWORD(inbuf, LINK_PIOBUF_IN_TXQ_INSTANCE,\r\n                   nic_data->pio_write_vi_base + index);\r\n\r\nand also for the special-case check:\r\n\r\n    if (tx_queue->queue == nic_data->pio_write_vi_base) {\r\n\r\nWouldn't this link PIO buffers to incorrect VI instances when using\r\nCXL, since the local variable pio_write_vi_base has the correct\r\nnon-zero value but the struct field was never updated?",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer Alejandro Palau noted that the patch caused a memory leak due to not dropping the reference to the cxl device when allocating another one, and suggested adding a put_device() call to fix it.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "memory leak",
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This is right, and it took a good bunch of time to debug it. Was it \ndetected by an automatic tool?\n\n\nAnyways, I had one patch for solving this which I forgot to apply to v23 \nsince the focus there was to mainly support the auto-discover region \nwhich does not go through this path:\n\n+ /* removing the reference from cxl_find_free_decoder ...\n+ * when alloc succeds another get happened\n+ */\n+\n+ put_device(&cxled->cxld.dev);\n\n\nI added that comment because it is not trivial to know if it is right to \ndo the put while you get a new reference to the device. I will apply it.\n\nThanks!",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-16",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer Alejandro Palau expressed confusion about the original implementation of cxl support and requested changes for version 24.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "confusion",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Ben,\n\n\nI do not remember why this was done this way. Maybe some initial need \nwhich disappeared later.\n\nI can not see a reason now, so I will do so in v24.\n\n\nThank you!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer pointed out that the cxl_accel_unwind() function is not properly handling the case where the accelerator is already being reset, and suggested adding a check to ensure that the function only unwinds the accelerator if it's not currently being reset.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure.\n\nThanks!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer Alejandro Palau expressed concern that the code does not support interleaving for Type2 accelerators, despite being trivial to implement, due to previous discussions and potential future use cases.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "previous discussions",
                "potential future use cases"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Ben,\n\n\nI do remember this one.\n\n\nDan's original patches had this support for interleaving, then I removed \nit as the case for Type2 and interleaving is quite unlikely, at least \nright now and likely in the near future. But I was told why do not \nsupport it as it was trivial to do so. FWIW, If I think only about the \nuse case coming with the patchset, I agree with you, but because those \nprevious discussions, I think I have to leave it.\n\n\nThank you",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer noted that CXL initialization fails, leading to an earlier release of the modified struct, and requested further investigation.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "failure",
                "earlier release"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, I do not think that is necessary. The CXL initialization fails, and \nthe result is the modified struct will be released sooner or later.",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer suggested replacing kstrtouint with kstrtoint to obtain values for cxl_region_params fields defined as int, considering the simplicity of this approach over modifying other places and structs.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Good catch. I wonder if I should just change the way the value is \nobtained, using kstrtoint instead of kstrtouint, as those values are \nused for cxl_region_params fields defined as int. In other words, it \nseems doing that simpler than changing all the other places you mention \nand the structs involved. I can not see a reason for using unsigned int \nso I think I will follow that approach. Tell me if you think otherwise.\n\n\nThank you",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer Alejandro Palau acknowledged the issue and agreed to make the necessary corrections.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "That's true. I will fix it.\n\n\nThank you!",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer Alejandro Palau agreed to implement the requested change, indicating that they understand its purpose.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "It makes sense. I'll do it.\n\nThanks",
              "reply_to": "Cheatham, Benjamin",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer questioned the use of unsigned int for interleave granularity/ways, suggesting it's because negative values wouldn't make sense, but agreed that using a smaller type is reasonable.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "If I had to guess unsigned int was used because a negative interleave granularity/ways makes no sense. I think your suggestion is fine though since no one\nin their right mind would give anything but a (relatively) small and positive value for these.\n\nThanks,\nBen",
              "reply_to": "Alejandro Palau",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Cheatham, Benjamin",
              "summary": "Reviewer suggested modifying the patch to address a previously noted issue by adding a small addition to enable CXL support, and requested that the decoder position be fixed in a separate patch (19/22). Additionally, he pointed out that the interleave_ways parameter in cxl_get_hpa_freespace() is currently unused.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm fine with that, but I would at least do the fix with the decoder position in 19/22 and make a note that the\ninterleave_ways parameter in cxl_get_hpa_freespace() below is currently unused (unless I'm misunderstanding\nthe endpoint->host_bridge member).\n\nThat way, the support is mostly there and just requires a small, previously noted, addition to enable. If you're\nfine with that then feel free to add my Reviewed-by after implementing in v24.\n\nThanks,\nBen",
              "reply_to": "Alejandro Palau",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Gave Reviewed-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\nOn 2/1/26 8:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Add cxl_unregister_region() to the accelerator driver API\n> for a clean exit.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n\n> ---\n>  drivers/cxl/core/region.c | 17 ++++++++++++-----\n>  include/cxl/cxl.h         |  1 +\n>  2 files changed, 13 insertions(+), 5 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index acf29ba3b205..954b8fcdbac6 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2438,9 +2438,8 @@ static struct cxl_region *to_cxl_region(struct device *dev)\n>  \treturn container_of(dev, struct cxl_region, dev);\n>  }\n>  \n> -static void unregister_region(void *_cxlr)\n> +void cxl_unregister_region(struct cxl_region *cxlr)\n>  {\n> -\tstruct cxl_region *cxlr = _cxlr;\n>  \tstruct cxl_region_params *p = &cxlr->params;\n>  \tint i;\n>  \n> @@ -2457,6 +2456,14 @@ static void unregister_region(void *_cxlr)\n>  \tcxl_region_iomem_release(cxlr);\n>  \tput_device(&cxlr->dev);\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_unregister_region, \"CXL\");\n> +\n> +static void __unregister_region(void *_cxlr)\n> +{\n> +\tstruct cxl_region *cxlr = _cxlr;\n> +\n> +\treturn cxl_unregister_region(cxlr);\n> +}\n>  \n>  static struct lock_class_key cxl_region_key;\n>  \n> @@ -2608,7 +2615,7 @@ static struct cxl_region *devm_cxl_add_region(struct cxl_root_decoder *cxlrd,\n>  \tif (rc)\n>  \t\tgoto err;\n>  \n> -\trc = devm_add_action_or_reset(port->uport_dev, unregister_region, cxlr);\n> +\trc = devm_add_action_or_reset(port->uport_dev, __unregister_region, cxlr);\n>  \tif (rc)\n>  \t\treturn ERR_PTR(rc);\n>  \n> @@ -2762,7 +2769,7 @@ static ssize_t delete_region_store(struct device *dev,\n>  \tif (IS_ERR(cxlr))\n>  \t\treturn PTR_ERR(cxlr);\n>  \n> -\tdevm_release_action(port->uport_dev, unregister_region, cxlr);\n> +\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n>  \tput_device(&cxlr->dev);\n>  \n>  \treturn len;\n> @@ -3878,7 +3885,7 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \n>  \trc = __construct_region(cxlr, cxlrd, cxled);\n>  \tif (rc) {\n> -\t\tdevm_release_action(port->uport_dev, unregister_region, cxlr);\n> +\t\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n>  \t\treturn ERR_PTR(rc);\n>  \t}\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 906065e0d2a6..92880c26b2d5 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -254,4 +254,5 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t\t       struct cxl_region **cxlr);\n>  struct range;\n>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n> +void cxl_unregister_region(struct cxl_region *cxlr);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\n\n\nOn 2/1/26 8:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Check if device HDM is already committed during firmware/BIOS\n> initialization.\n> \n> A CXL region should exist if so after memdev allocation/initialization.\n> Get HPA from region and map it.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/net/ethernet/sfc/efx_cxl.c | 28 +++++++++++++++++++++++++++-\n>  1 file changed, 27 insertions(+), 1 deletion(-)\n> \n> diff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\n> index a77ef4783fcb..3536eccf1b2a 100644\n> --- a/drivers/net/ethernet/sfc/efx_cxl.c\n> +++ b/drivers/net/ethernet/sfc/efx_cxl.c\n> @@ -19,6 +19,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \tstruct efx_nic *efx = &probe_data->efx;\n>  \tstruct pci_dev *pci_dev = efx->pci_dev;\n>  \tstruct efx_cxl *cxl;\n> +\tstruct range range;\n>  \tu16 dvsec;\n>  \tint rc;\n>  \n> @@ -90,13 +91,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \t\treturn PTR_ERR(cxl->cxlmd);\n>  \t}\n>  \n> -\tprobe_data->cxl = cxl;\n> +\tcxl->cxled = cxl_get_committed_decoder(cxl->cxlmd, &cxl->efx_region);\n> +\tif (cxl->cxled) {\n\nif (!cxl->cxled)\n\treturn 0;\n\nShould save you a level of indent.\n\nDJ\n\n> +\t\tif (!cxl->efx_region) {\n> +\t\t\tpci_err(pci_dev, \"CXL found committed decoder without a region\");\n> +\t\t\treturn -ENODEV;\n> +\t\t}\n> +\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n> +\t\tif (rc) {\n> +\t\t\tpci_err(pci_dev,\n> +\t\t\t\t\"CXL getting regions params from a committed decoder failed\");\n> +\t\t\treturn rc;\n> +\t\t}\n> +\n> +\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n> +\t\tif (!cxl->ctpio_cxl) {\n> +\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n> +\t\t\treturn -ENOMEM;\n> +\t\t}\n> +\n> +\t\tprobe_data->cxl = cxl;\n> +\t}\n>  \n>  \treturn 0;\n>  }\n>  \n>  void efx_cxl_exit(struct efx_probe_data *probe_data)\n>  {\n> +\tif (!probe_data->cxl)\n> +\t\treturn;\n> +\n> +\tiounmap(probe_data->cxl->ctpio_cxl);\n> +\tcxl_unregister_region(probe_data->cxl->efx_region);\n>  }\n>  \n>  MODULE_IMPORT_NS(\"CXL\");\n\n\n\n---\n\n\n\nOn 2/1/26 8:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> CXL region creation involves allocating capacity from Device Physical\n> Address (DPA) and assigning it to decode a given Host Physical Address\n> (HPA). Before determining how much DPA to allocate the amount of available\n> HPA must be determined. Also, not all HPA is created equal, some HPA\n> targets RAM, some targets PMEM, some is prepared for device-memory flows\n> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n> \n> In order to support Type2 CXL devices, wrap all of those concerns into\n> an API that retrieves a root decoder (platform CXL window) that fits the\n> specified constraints and the capacity available for a new region.\n> \n> Add a complementary function for releasing the reference to such root\n> decoder.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> ---\n>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h         |   3 +\n>  include/cxl/cxl.h         |   6 ++\n>  3 files changed, 173 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 954b8fcdbac6..bdefd088f5f1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>  \treturn 0;\n>  }\n>  \n> +struct cxlrd_max_context {\n> +\tstruct device * const *host_bridges;\n> +\tint interleave_ways;\n> +\tunsigned long flags;\n> +\tresource_size_t max_hpa;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +};\n> +\n> +static int find_max_hpa(struct device *dev, void *data)\n> +{\n> +\tstruct cxlrd_max_context *ctx = data;\n> +\tstruct cxl_switch_decoder *cxlsd;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +\tstruct resource *res, *prev;\n> +\tstruct cxl_decoder *cxld;\n> +\tresource_size_t free = 0;\n> +\tresource_size_t max;\n> +\tint found = 0;\n> +\n> +\tif (!is_root_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxlrd = to_cxl_root_decoder(dev);\n> +\tcxlsd = &cxlrd->cxlsd;\n> +\tcxld = &cxlsd->cxld;\n> +\n> +\tif ((cxld->flags & ctx->flags) != ctx->flags) {\n> +\t\tdev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n> +\t\t\tcxld->flags, ctx->flags);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\tfor (int i = 0; i < ctx->interleave_ways; i++) {\n> +\t\tfor (int j = 0; j < ctx->interleave_ways; j++) {\n> +\t\t\tif (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n> +\t\t\t\tfound++;\n> +\t\t\t\tbreak;\n> +\t\t\t}\n> +\t\t}\n> +\t}\n> +\n> +\tif (found != ctx->interleave_ways) {\n> +\t\tdev_dbg(dev,\n> +\t\t\t\"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n> +\t\t\tfound, ctx->interleave_ways);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\t/*\n> +\t * Walk the root decoder resource range relying on cxl_rwsem.region to\n> +\t * preclude sibling arrival/departure and find the largest free space\n> +\t * gap.\n> +\t */\n> +\tlockdep_assert_held_read(&cxl_rwsem.region);\n> +\tres = cxlrd->res->child;\n> +\n> +\t/* With no resource child the whole parent resource is available */\n> +\tif (!res)\n> +\t\tmax = resource_size(cxlrd->res);\n> +\telse\n> +\t\tmax = 0;\n> +\n> +\tfor (prev = NULL; res; prev = res, res = res->sibling) {\n> +\t\tif (!prev && res->start == cxlrd->res->start &&\n> +\t\t    res->end == cxlrd->res->end) {\n> +\t\t\tmax = resource_size(cxlrd->res);\n> +\t\t\tbreak;\n> +\t\t}\n\nCan this block be pulled out of the for loop so it only needs to run once?\n\n> +\t\t/*\n> +\t\t * Sanity check for preventing arithmetic problems below as a\n> +\t\t * resource with size 0 could imply using the end field below\n> +\t\t * when set to unsigned zero - 1 or all f in hex.\n> +\t\t */\n> +\t\tif (prev && !resource_size(prev))\n> +\t\t\tcontinue;\n> +\n> +\t\tif (!prev && res->start > cxlrd->res->start) {\n> +\t\t\tfree = res->start - cxlrd->res->start;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t\tif (prev && res->start > prev->end + 1) {\n> +\t\t\tfree = res->start - prev->end + 1;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t}\n> +\n> +\tif (prev && prev->end + 1 < cxlrd->res->end + 1) {\n> +\t\tfree = cxlrd->res->end + 1 - prev->end + 1;\n> +\t\tmax = max(free, max);\n> +\t}\n> +\n> +\tdev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n> +\tif (max > ctx->max_hpa) {\n> +\t\tif (ctx->cxlrd)\n> +\t\t\tput_device(cxlrd_dev(ctx->cxlrd));\n> +\t\tget_device(cxlrd_dev(cxlrd));\n> +\t\tctx->cxlrd = cxlrd;\n> +\t\tctx->max_hpa = max;\n\nIs there any chance that ctx->cxlrd == cxlrd? Maybe you can do:\n\nif (ctx->cxlrd && ctx->cxlrd != cxlrd) {\n\tput_device(cxlrd_dev(ctx->cxlrd));\n\tget_device(cxlrd_dev(cxlrd));\n\tctx->cxlrd = cxlrd;\n}\nctx->max_hpa = max;\n\nDJ\n\n> +\t}\n> +\treturn 0;\n> +}\n> +\n> +/**\n> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n> + * @cxlmd: the mem device requiring the HPA\n> + * @interleave_ways: number of entries in @host_bridges\n> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n> + *\t\t      returned decoder\n> + *\n> + * Returns a pointer to a struct cxl_root_decoder\n> + *\n> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n> + * caller goes to use this decoder and its capacity is reduced then caller needs\n> + * to loop and retry.\n> + *\n> + * The returned root decoder has an elevated reference count that needs to be\n> + * put with cxl_put_root_decoder(cxlrd).\n> + */\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max_avail_contig)\n> +{\n> +\tstruct cxlrd_max_context ctx = {\n> +\t\t.flags = flags,\n> +\t\t.interleave_ways = interleave_ways,\n> +\t};\n> +\tstruct cxl_port *root_port;\n> +\tstruct cxl_port *endpoint;\n> +\n> +\tendpoint = cxlmd->endpoint;\n> +\tif (!endpoint) {\n> +\t\tdev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\tctx.host_bridges = &endpoint->host_bridge;\n> +\n> +\tstruct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n> +\tif (!root) {\n> +\t\tdev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\troot_port = &root->port;\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.region)\n> +\t\tdevice_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n> +\n> +\tif (!ctx.cxlrd)\n> +\t\treturn ERR_PTR(-ENOMEM);\n> +\n> +\t*max_avail_contig = ctx.max_hpa;\n> +\treturn ctx.cxlrd;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n> +\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n> +{\n> +\tput_device(cxlrd_dev(cxlrd));\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n> +\n>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>  \t\t\t  const char *buf, size_t len)\n>  {\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index 944c5d1ccceb..c7d9b2c2908f 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>  bool is_root_decoder(struct device *dev);\n> +\n> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n> +\n>  bool is_switch_decoder(struct device *dev);\n>  bool is_endpoint_decoder(struct device *dev);\n>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 92880c26b2d5..834dc7e78934 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>  struct range;\n>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>  void cxl_unregister_region(struct cxl_region *cxlr);\n> +struct cxl_port;\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max);\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>  #endif /* __CXL_CXL_H__ */\n\n",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer Dave Jiang suggested simplifying the cxl code by saving an indentation level, and provided a specific line to modify",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "suggested improvement",
                "code optimization"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "if (!cxl->cxled)\n\treturn 0;\n\nShould save you a level of indent.\n\nDJ",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-19",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "Reviewer noted that the patchset should handle both module exit paths, specifically supporting cases where cxl is initialized before or after the driver is loaded.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Gregory,\n\n\nYes, it makes sense and pointing out to those changes introduced in v22 \nand mainly in v23.\n\nI'll fix it.\n\n\nRegarding the below comment, which if I am not wrong comes from kreview, \nI think the patchset needs to support both cases and therefore the code \nneeds to deal with both module exit paths.\n\n\nThank you",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "The reviewer noted that since they've only tested with one Virtual Interface (VI), they haven't encountered an issue, but emphasized that it's essential to fix the problem.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "issue",
                "needs to be fixed"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, and again, it makes sense. We have only tried with one VI, so that \nexplains why we have not suffered the issue. But it needs to be fixed.\n\n\nThanks!",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alejandro Palau",
              "summary": "reviewer noted that the current implementation lacks an 'else' branch for when CXL is not enabled, and requested that subsequent patches address this omission",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "missing else branch"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, but subsequent patches add the else branch ...\n\n\nThanks",
              "reply_to": "Dave Jiang",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Dave Jiang",
              "summary": "Reviewer Dave Jiang noted that the ctx->cxlrd pointer might not be updated correctly, suggesting a check to ensure it matches cxlrd before updating it, and proposed code to handle this case",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Is there any chance that ctx->cxlrd == cxlrd? Maybe you can do:\n\nif (ctx->cxlrd && ctx->cxlrd != cxlrd) {\n\tput_device(cxlrd_dev(ctx->cxlrd));\n\tget_device(cxlrd_dev(cxlrd));\n\tctx->cxlrd = cxlrd;\n}\nctx->max_hpa = max;\n\nDJ",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price noted that during testing, he observed double-releases when aggressively loading and unloading certain drivers, which was fixed by adding a function to properly unregister regions in cxl_destroy_region()",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "fixes a bug",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "kreview suggested you probably want this:\n\nvoid cxl_destroy_region(struct cxl_region *cxlr)\n{\n\tstruct cxl_port *port = cxlrd_to_port(cxlr->cxlrd);\n\n\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n}\nEXPORT_SYMBOL_NS_GPL(cxl_destroy_region, \"CXL\");\n\n\nDuring testing I experienced some double-releases when doing aggressive\nloads and unloads of some drivers.  This was one of the fixes.\n\n~Gregory",
              "reply_to": "alejandro.lucero-palau",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Jeff Layton",
      "primary_email": "jlayton@kernel.org",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joanne Koong",
      "primary_email": "joannelkoong@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v1 03/11] io_uring/kbuf: add support for kernel-managed buffer rings",
          "message_id": "CAJnrk1Zr=9RMGpNXpe6=fSDkG2uVijB9qa1vENHpQozB3iPQtg@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1Zr=9RMGpNXpe6=fSDkG2uVijB9qa1vENHpQozB3iPQtg@mail.gmail.com/",
          "date": "2026-02-21T02:14:40Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the refactoring of io_register_pbuf_ring() logic, explaining that it is being split into generic helpers to prepare for upcoming kernel-managed buffer ring support.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "neutral explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Refactor the logic in io_register_pbuf_ring() into generic helpers:\n- io_copy_and_validate_buf_reg(): Copy out user arg and validate user\n  arg and buffer registration parameters\n- io_alloc_new_buffer_list(): Allocate and initialize a new buffer\n  list for the given buffer group ID\n- io_setup_pbuf_ring(): Sets up the physical buffer ring region and\n  handles memory mapping for provided buffer rings\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport which will need to reuse some of these helpers.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c | 129 +++++++++++++++++++++++++++++++-----------------\n 1 file changed, 85 insertions(+), 44 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 67d4fe576473..850b836f32ee 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -596,55 +596,73 @@ int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags)\n \treturn IOU_COMPLETE;\n }\n \n-int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+static int io_copy_and_validate_buf_reg(const void __user *arg,\n+\t\t\t\t\tstruct io_uring_buf_reg *reg,\n+\t\t\t\t\tunsigned int permitted_flags)\n {\n-\tstruct io_uring_buf_reg reg;\n-\tstruct io_buffer_list *bl;\n-\tstruct io_uring_region_desc rd;\n-\tstruct io_uring_buf_ring *br;\n-\tunsigned long mmap_offset;\n-\tunsigned long ring_size;\n-\tint ret;\n-\n-\tlockdep_assert_held(&ctx->uring_lock);\n-\n-\tif (copy_from_user(&reg, arg, sizeof(reg)))\n+\tif (copy_from_user(reg, arg, sizeof(*reg)))\n \t\treturn -EFAULT;\n-\tif (!mem_is_zero(reg.resv, sizeof(reg.resv)))\n+\n+\tif (!mem_is_zero(reg->resv, sizeof(reg->resv)))\n \t\treturn -EINVAL;\n-\tif (reg.flags & ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC))\n+\tif (reg->flags & ~permitted_flags)\n \t\treturn -EINVAL;\n-\tif (!is_power_of_2(reg.ring_entries))\n+\tif (!is_power_of_2(reg->ring_entries))\n \t\treturn -EINVAL;\n \t/* cannot disambiguate full vs empty due to head/tail size */\n-\tif (reg.ring_entries >= 65536)\n+\tif (reg->ring_entries >= 65536)\n \t\treturn -EINVAL;\n+\treturn 0;\n+}\n \n-\tbl = io_buffer_get_list(ctx, reg.bgid);\n-\tif (bl) {\n+static struct io_buffer_list *\n+io_alloc_new_buffer_list(struct io_ring_ctx *ctx,\n+\t\t\t const struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_buffer_list *list;\n+\n+\tlist = io_buffer_get_list(ctx, reg->bgid);\n+\tif (list) {\n \t\t/* if mapped buffer ring OR classic exists, don't allow */\n-\t\tif (bl->flags & IOBL_BUF_RING || !list_empty(&bl->buf_list))\n-\t\t\treturn -EEXIST;\n-\t\tio_destroy_bl(ctx, bl);\n+\t\tif (list->flags & IOBL_BUF_RING || !list_empty(&list->buf_list))\n+\t\t\treturn ERR_PTR(-EEXIST);\n+\t\tio_destroy_bl(ctx, list);\n \t}\n \n-\tbl = kzalloc(sizeof(*bl), GFP_KERNEL_ACCOUNT);\n-\tif (!bl)\n-\t\treturn -ENOMEM;\n+\tlist = kzalloc(sizeof(*list), GFP_KERNEL_ACCOUNT);\n+\tif (!list)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tlist->nr_entries = reg->ring_entries;\n+\tlist->mask = reg->ring_entries - 1;\n+\tlist->flags = IOBL_BUF_RING;\n+\n+\treturn list;\n+}\n+\n+static int io_setup_pbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t      const struct io_uring_buf_reg *reg,\n+\t\t\t      struct io_buffer_list *bl)\n+{\n+\tstruct io_uring_region_desc rd;\n+\tunsigned long mmap_offset;\n+\tunsigned long ring_size;\n+\tint ret;\n \n-\tmmap_offset = (unsigned long)reg.bgid << IORING_OFF_PBUF_SHIFT;\n-\tring_size = flex_array_size(br, bufs, reg.ring_entries);\n+\tmmap_offset = (unsigned long)reg->bgid << IORING_OFF_PBUF_SHIFT;\n+\tring_size = flex_array_size(bl->buf_ring, bufs, reg->ring_entries);\n \n \tmemset(&rd, 0, sizeof(rd));\n \trd.size = PAGE_ALIGN(ring_size);\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP)) {\n-\t\trd.user_addr = reg.ring_addr;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP)) {\n+\t\trd.user_addr = reg->ring_addr;\n \t\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n \t}\n+\n \tret = io_create_region(ctx, &bl->region, &rd, mmap_offset);\n \tif (ret)\n-\t\tgoto fail;\n-\tbr = io_region_get_ptr(&bl->region);\n+\t\treturn ret;\n+\tbl->buf_ring = io_region_get_ptr(&bl->region);\n \n #ifdef SHM_COLOUR\n \t/*\n@@ -656,25 +674,48 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t * should use IOU_PBUF_RING_MMAP instead, and liburing will handle\n \t * this transparently.\n \t */\n-\tif (!(reg.flags & IOU_PBUF_RING_MMAP) &&\n-\t    ((reg.ring_addr | (unsigned long)br) & (SHM_COLOUR - 1))) {\n-\t\tret = -EINVAL;\n-\t\tgoto fail;\n+\tif (!(reg->flags & IOU_PBUF_RING_MMAP) &&\n+\t    ((reg->ring_addr | (unsigned long)bl->buf_ring) &\n+\t     (SHM_COLOUR - 1))) {\n+\t\tio_free_region(ctx->user, &bl->region);\n+\t\treturn -EINVAL;\n \t}\n #endif\n \n-\tbl->nr_entries = reg.ring_entries;\n-\tbl->mask = reg.ring_entries - 1;\n-\tbl->flags |= IOBL_BUF_RING;\n-\tbl->buf_ring = br;\n-\tif (reg.flags & IOU_PBUF_RING_INC)\n+\tif (reg->flags & IOU_PBUF_RING_INC)\n \t\tbl->flags |= IOBL_INC;\n+\n+\treturn 0;\n+}\n+\n+int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tunsigned int permitted_flags;\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tpermitted_flags = IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC;\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, permitted_flags);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_pbuf_ring(ctx, &reg, bl);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n \tret = io_buffer_add_list(ctx, bl, reg.bgid);\n-\tif (!ret)\n-\t\treturn 0;\n-fail:\n-\tio_free_region(ctx->user, &bl->region);\n-\tkfree(bl);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n \treturn ret;\n }\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about inconsistent function naming by renaming io_unregister_pbuf_ring() to io_unregister_buf_ring(), which will be used for both provided buffer rings and kernel-managed buffer rings, as a preparatory change for upcoming kernel-managed buffer ring support.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "preparatory change",
                "renaming function"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Use the more generic name io_unregister_buf_ring() as this function will\nbe used for unregistering both provided buffer rings and kernel-managed\nbuffer rings.\n\nThis is a preparatory change for upcoming kernel-managed buffer ring\nsupport.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/kbuf.c     | 2 +-\n io_uring/kbuf.h     | 2 +-\n io_uring/register.c | 2 +-\n 3 files changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 850b836f32ee..aa9b70b72db4 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -719,7 +719,7 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \treturn ret;\n }\n \n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n {\n \tstruct io_uring_buf_reg reg;\n \tstruct io_buffer_list *bl;\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex bf15e26520d3..40b44f4fdb15 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -74,7 +74,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags);\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 594b1f2ce875..0882cb34f851 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -841,7 +841,7 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-\t\tret = io_unregister_pbuf_ring(ctx, arg);\n+\t\tret = io_unregister_buf_ring(ctx, arg);\n \t\tbreak;\n \tcase IORING_REGISTER_SYNC_CANCEL:\n \t\tret = -EINVAL;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the implementation of kernel-managed buffer rings, explaining that they follow the same pattern as pbuf ring registration and reusing validation and buffer list allocation helpers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "neutral explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for kernel-managed buffer rings (kmbuf rings), which allow\nthe kernel to allocate and manage the backing buffers for a buffer\nring, rather than requiring the application to provide and manage them.\n\nThis introduces two new registration opcodes:\n- IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring\n- IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring\n\nThe existing io_uring_buf_reg structure is extended with a union to\nsupport both application-provided buffer rings (pbuf) and kernel-managed\nbuffer rings (kmbuf):\n- For pbuf rings: ring_addr specifies the user-provided ring address\n- For kmbuf rings: buf_size specifies the size of each buffer. buf_size\n  must be non-zero and page-aligned.\n\nThe implementation follows the same pattern as pbuf ring registration,\nreusing the validation and buffer list allocation helpers introduced in\nearlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as\nkernel-managed for appropriate handling in the I/O path.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  15 ++++-\n io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-\n io_uring/kbuf.h               |   7 ++-\n io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++\n io_uring/memmap.h             |   4 ++\n io_uring/register.c           |   7 +++\n 6 files changed, 219 insertions(+), 6 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex fc473af6feb4..a0889c1744bd 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -715,6 +715,10 @@ enum io_uring_register_op {\n \t/* register bpf filtering programs */\n \tIORING_REGISTER_BPF_FILTER\t\t= 37,\n \n+\t/* register/unregister kernel-managed ring buffer group */\n+\tIORING_REGISTER_KMBUF_RING\t\t= 38,\n+\tIORING_UNREGISTER_KMBUF_RING\t\t= 39,\n+\n \t/* this goes last */\n \tIORING_REGISTER_LAST,\n \n@@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {\n \tIOU_PBUF_RING_INC\t= 2,\n };\n \n-/* argument for IORING_(UN)REGISTER_PBUF_RING */\n+/* argument for IORING_(UN)REGISTER_PBUF_RING and\n+ * IORING_(UN)REGISTER_KMBUF_RING\n+ */\n struct io_uring_buf_reg {\n-\t__u64\tring_addr;\n+\tunion {\n+\t\t/* used for pbuf rings */\n+\t\t__u64\tring_addr;\n+\t\t/* used for kmbuf rings */\n+\t\t__u32   buf_size;\n+\t};\n \t__u32\tring_entries;\n \t__u16\tbgid;\n \t__u16\tflags;\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex aa9b70b72db4..9bc36451d083 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -427,10 +427,13 @@ static int io_remove_buffers_legacy(struct io_ring_ctx *ctx,\n \n static void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)\n {\n-\tif (bl->flags & IOBL_BUF_RING)\n+\tif (bl->flags & IOBL_BUF_RING) {\n \t\tio_free_region(ctx->user, &bl->region);\n-\telse\n+\t\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\t\tkfree(bl->buf_ring);\n+\t} else {\n \t\tio_remove_buffers_legacy(ctx, bl, -1U);\n+\t}\n \n \tkfree(bl);\n }\n@@ -779,3 +782,77 @@ struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n \t\treturn NULL;\n \treturn &bl->region;\n }\n+\n+static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_buffer_list *bl,\n+\t\t\t       struct io_uring_buf_reg *reg)\n+{\n+\tstruct io_uring_buf_ring *ring;\n+\tunsigned long ring_size;\n+\tvoid *buf_region;\n+\tunsigned int i;\n+\tint ret;\n+\n+\t/* allocate pages for the ring structure */\n+\tring_size = flex_array_size(ring, bufs, bl->nr_entries);\n+\tring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);\n+\tif (!ring)\n+\t\treturn -ENOMEM;\n+\n+\tret = io_create_region_multi_buf(ctx, &bl->region, bl->nr_entries,\n+\t\t\t\t\t reg->buf_size);\n+\tif (ret) {\n+\t\tkfree(ring);\n+\t\treturn ret;\n+\t}\n+\n+\t/* initialize ring buf entries to point to the buffers */\n+\tbuf_region = bl->region.ptr;\n+\tfor (i = 0; i < bl->nr_entries; i++) {\n+\t\tstruct io_uring_buf *buf = &ring->bufs[i];\n+\n+\t\tbuf->addr = (u64)(uintptr_t)buf_region;\n+\t\tbuf->len = reg->buf_size;\n+\t\tbuf->bid = i;\n+\n+\t\tbuf_region += reg->buf_size;\n+\t}\n+\tring->tail = bl->nr_entries;\n+\n+\tbl->buf_ring = ring;\n+\tbl->flags |= IOBL_KERNEL_MANAGED;\n+\n+\treturn 0;\n+}\n+\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n+{\n+\tstruct io_uring_buf_reg reg;\n+\tstruct io_buffer_list *bl;\n+\tint ret;\n+\n+\tlockdep_assert_held(&ctx->uring_lock);\n+\n+\tret = io_copy_and_validate_buf_reg(arg, &reg, 0);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tif (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbl = io_alloc_new_buffer_list(ctx, &reg);\n+\tif (IS_ERR(bl))\n+\t\treturn PTR_ERR(bl);\n+\n+\tret = io_setup_kmbuf_ring(ctx, bl, &reg);\n+\tif (ret) {\n+\t\tkfree(bl);\n+\t\treturn ret;\n+\t}\n+\n+\tret = io_buffer_add_list(ctx, bl, reg.bgid);\n+\tif (ret)\n+\t\tio_put_bl(ctx, bl);\n+\n+\treturn ret;\n+}\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 40b44f4fdb15..62c80a1ebf03 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -7,9 +7,11 @@\n \n enum {\n \t/* ring mapped provided buffers */\n-\tIOBL_BUF_RING\t= 1,\n+\tIOBL_BUF_RING\t\t= 1,\n \t/* buffers are consumed incrementally rather than always fully */\n-\tIOBL_INC\t= 2,\n+\tIOBL_INC\t\t= 2,\n+\t/* buffers are kernel managed */\n+\tIOBL_KERNEL_MANAGED\t= 4,\n };\n \n struct io_buffer_list {\n@@ -74,6 +76,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe\n int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);\n \n int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);\n int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);\n \ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 89f56609e50a..8d37e93c0433 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -15,6 +15,28 @@\n #include \"rsrc.h\"\n #include \"zcrx.h\"\n \n+static void release_multi_buf_pages(struct page **pages, unsigned long nr_pages)\n+{\n+\tstruct page *page;\n+\tunsigned int nr, i = 0;\n+\n+\twhile (nr_pages) {\n+\t\tpage = pages[i];\n+\n+\t\tif (!page || WARN_ON_ONCE(page != compound_head(page)))\n+\t\t\treturn;\n+\n+\t\tnr = compound_nr(page);\n+\t\tput_page(page);\n+\n+\t\tif (WARN_ON_ONCE(nr > nr_pages))\n+\t\t\treturn;\n+\n+\t\ti += nr;\n+\t\tnr_pages -= nr;\n+\t}\n+}\n+\n static bool io_mem_alloc_compound(struct page **pages, int nr_pages,\n \t\t\t\t  size_t size, gfp_t gfp)\n {\n@@ -86,6 +108,8 @@ enum {\n \tIO_REGION_F_USER_PROVIDED\t\t= 2,\n \t/* only the first page in the array is ref'ed */\n \tIO_REGION_F_SINGLE_REF\t\t\t= 4,\n+\t/* pages in the array belong to multiple discrete allocations */\n+\tIO_REGION_F_MULTI_BUF\t\t\t= 8,\n };\n \n void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n@@ -98,6 +122,8 @@ void io_free_region(struct user_struct *user, struct io_mapped_region *mr)\n \n \t\tif (mr->flags & IO_REGION_F_USER_PROVIDED)\n \t\t\tunpin_user_pages(mr->pages, nr_refs);\n+\t\telse if (mr->flags & IO_REGION_F_MULTI_BUF)\n+\t\t\trelease_multi_buf_pages(mr->pages, nr_refs);\n \t\telse\n \t\t\trelease_pages(mr->pages, nr_refs);\n \n@@ -149,6 +175,54 @@ static int io_region_pin_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+static int io_region_allocate_pages_multi_buf(struct io_mapped_region *mr,\n+\t\t\t\t\t      unsigned int nr_bufs,\n+\t\t\t\t\t      unsigned int buf_size)\n+{\n+\tgfp_t gfp = GFP_USER | __GFP_ACCOUNT | __GFP_ZERO | __GFP_NOWARN;\n+\tstruct page **pages, **cur_pages;\n+\tunsigned int nr_allocated;\n+\tunsigned int buf_pages;\n+\tunsigned int i;\n+\n+\tif (!PAGE_ALIGNED(buf_size))\n+\t\treturn -EINVAL;\n+\n+\tbuf_pages = buf_size >> PAGE_SHIFT;\n+\n+\tpages = kvmalloc_array(mr->nr_pages, sizeof(*pages), gfp);\n+\tif (!pages)\n+\t\treturn -ENOMEM;\n+\n+\tcur_pages = pages;\n+\n+\tfor (i = 0; i < nr_bufs; i++) {\n+\t\tif (io_mem_alloc_compound(cur_pages, buf_pages, buf_size,\n+\t\t\t\t\t  gfp)) {\n+\t\t\tcur_pages += buf_pages;\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tnr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,\n+\t\t\t\t\t\t     buf_pages, cur_pages);\n+\t\tif (nr_allocated != buf_pages) {\n+\t\t\tunsigned int total =\n+\t\t\t\t(cur_pages - pages) + nr_allocated;\n+\n+\t\t\trelease_multi_buf_pages(pages, total);\n+\t\t\tkvfree(pages);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tcur_pages += buf_pages;\n+\t}\n+\n+\tmr->flags |= IO_REGION_F_MULTI_BUF;\n+\tmr->pages = pages;\n+\n+\treturn 0;\n+}\n+\n static int io_region_allocate_pages(struct io_mapped_region *mr,\n \t\t\t\t    struct io_uring_region_desc *reg,\n \t\t\t\t    unsigned long mmap_offset)\n@@ -181,6 +255,43 @@ static int io_region_allocate_pages(struct io_mapped_region *mr,\n \treturn 0;\n }\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size)\n+{\n+\tunsigned int nr_pages;\n+\tint ret;\n+\n+\tif (WARN_ON_ONCE(mr->pages || mr->ptr || mr->nr_pages))\n+\t\treturn -EFAULT;\n+\n+\tif (WARN_ON_ONCE(!nr_bufs || !buf_size || !PAGE_ALIGNED(buf_size)))\n+\t\treturn -EINVAL;\n+\n+\tif (check_mul_overflow(buf_size >> PAGE_SHIFT, nr_bufs, &nr_pages))\n+\t\treturn -EINVAL;\n+\n+\tif (ctx->user) {\n+\t\tret = __io_account_mem(ctx->user, nr_pages);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\t}\n+\tmr->nr_pages = nr_pages;\n+\n+\tret = io_region_allocate_pages_multi_buf(mr, nr_bufs, buf_size);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\tret = io_region_init_ptr(mr);\n+\tif (ret)\n+\t\tgoto out_free;\n+\n+\treturn 0;\n+out_free:\n+\tio_free_region(ctx->user, mr);\n+\treturn ret;\n+}\n+\n int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset)\ndiff --git a/io_uring/memmap.h b/io_uring/memmap.h\nindex f4cfbb6b9a1f..3aa1167462ae 100644\n--- a/io_uring/memmap.h\n+++ b/io_uring/memmap.h\n@@ -22,6 +22,10 @@ int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,\n \t\t     struct io_uring_region_desc *reg,\n \t\t     unsigned long mmap_offset);\n \n+int io_create_region_multi_buf(struct io_ring_ctx *ctx,\n+\t\t\t       struct io_mapped_region *mr,\n+\t\t\t       unsigned int nr_bufs, unsigned int buf_size);\n+\n static inline void *io_region_get_ptr(struct io_mapped_region *mr)\n {\n \treturn mr->ptr;\ndiff --git a/io_uring/register.c b/io_uring/register.c\nindex 0882cb34f851..2db8daaf8fde 100644\n--- a/io_uring/register.c\n+++ b/io_uring/register.c\n@@ -837,7 +837,14 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n \t\t\tbreak;\n \t\tret = io_register_pbuf_ring(ctx, arg);\n \t\tbreak;\n+\tcase IORING_REGISTER_KMBUF_RING:\n+\t\tret = -EINVAL;\n+\t\tif (!arg || nr_args != 1)\n+\t\t\tbreak;\n+\t\tret = io_register_kmbuf_ring(ctx, arg);\n+\t\tbreak;\n \tcase IORING_UNREGISTER_PBUF_RING:\n+\tcase IORING_UNREGISTER_KMBUF_RING:\n \t\tret = -EINVAL;\n \t\tif (!arg || nr_args != 1)\n \t\t\tbreak;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about the implementation of kernel-managed buffer rings (kmbuf) mmap support, explaining that it follows the same pattern as application-provided buffer rings and introducing new constants for kmbuf mappings. The author confirmed that userspace can access kernel-allocated buffers directly through mmap.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "author provided a clear explanation of their implementation",
                "author confirmed that userspace can access kernel-allocated buffers"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add support for mmapping kernel-managed buffer rings (kmbuf) to\nuserspace, allowing applications to access the kernel-allocated buffers.\n\nSimilar to application-provided buffer rings (pbuf), kmbuf rings use the\nbuffer group ID encoded in the mmap offset to identify which buffer ring\nto map. The implementation follows the same pattern as pbuf rings.\n\nNew mmap offset constants are introduced:\n  - IORING_OFF_KMBUF_RING (0x88000000): Base offset for kmbuf mappings\n  - IORING_OFF_KMBUF_SHIFT (16): Shift value to encode buffer group ID\n\nThe mmap offset encodes the bgid shifted by IORING_OFF_KMBUF_SHIFT.\nThe io_buf_get_region() helper retrieves the appropriate region.\n\nThis allows userspace to mmap the kernel-allocated buffer region and\naccess the buffers directly.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/uapi/linux/io_uring.h |  2 ++\n io_uring/kbuf.c               | 11 +++++++++--\n io_uring/kbuf.h               |  5 +++--\n io_uring/memmap.c             |  5 ++++-\n 4 files changed, 18 insertions(+), 5 deletions(-)\n\ndiff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h\nindex a0889c1744bd..42a2812c9922 100644\n--- a/include/uapi/linux/io_uring.h\n+++ b/include/uapi/linux/io_uring.h\n@@ -545,6 +545,8 @@ struct io_uring_cqe {\n #define IORING_OFF_SQES\t\t\t0x10000000ULL\n #define IORING_OFF_PBUF_RING\t\t0x80000000ULL\n #define IORING_OFF_PBUF_SHIFT\t\t16\n+#define IORING_OFF_KMBUF_RING\t\t0x88000000ULL\n+#define IORING_OFF_KMBUF_SHIFT\t\t16\n #define IORING_OFF_MMAP_MASK\t\t0xf8000000ULL\n \n /*\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9bc36451d083..ccf5b213087b 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -770,16 +770,23 @@ int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg)\n \treturn 0;\n }\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid)\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed)\n {\n \tstruct io_buffer_list *bl;\n+\tbool is_kernel_managed;\n \n \tlockdep_assert_held(&ctx->mmap_lock);\n \n \tbl = xa_load(&ctx->io_bl_xa, bgid);\n \tif (!bl || !(bl->flags & IOBL_BUF_RING))\n \t\treturn NULL;\n+\n+\tis_kernel_managed = !!(bl->flags & IOBL_KERNEL_MANAGED);\n+\tif (is_kernel_managed != kernel_managed)\n+\t\treturn NULL;\n+\n \treturn &bl->region;\n }\n \ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 62c80a1ebf03..11d165888b8e 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -88,8 +88,9 @@ unsigned int __io_put_kbufs(struct io_kiocb *req, struct io_buffer_list *bl,\n bool io_kbuf_commit(struct io_kiocb *req,\n \t\t    struct io_buffer_list *bl, int len, int nr);\n \n-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,\n-\t\t\t\t\t    unsigned int bgid);\n+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,\n+\t\t\t\t\t   unsigned int bgid,\n+\t\t\t\t\t   bool kernel_managed);\n \n static inline bool io_kbuf_recycle_ring(struct io_kiocb *req,\n \t\t\t\t\tstruct io_buffer_list *bl)\ndiff --git a/io_uring/memmap.c b/io_uring/memmap.c\nindex 8d37e93c0433..916315122323 100644\n--- a/io_uring/memmap.c\n+++ b/io_uring/memmap.c\n@@ -356,7 +356,10 @@ static struct io_mapped_region *io_mmap_get_region(struct io_ring_ctx *ctx,\n \t\treturn &ctx->sq_region;\n \tcase IORING_OFF_PBUF_RING:\n \t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_PBUF_SHIFT;\n-\t\treturn io_pbuf_get_region(ctx, id);\n+\t\treturn io_buf_get_region(ctx, id, false);\n+\tcase IORING_OFF_KMBUF_RING:\n+\t\tid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_KMBUF_SHIFT;\n+\t\treturn io_buf_get_region(ctx, id, true);\n \tcase IORING_MAP_OFF_PARAM_REGION:\n \t\treturn &ctx->param_region;\n \tcase IORING_MAP_OFF_ZCRX_REGION:\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about distinguishing between kernel-managed buffer addresses and negative values when error checking, explaining that the io_br_sel struct needs to separate address and value fields for kernel-managed buffers. The author modified the io_uring_types.h file to add a kaddr field to the union in io_br_sel, allowing for kernel-managed buffer selection.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "modification"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Allow kernel-managed buffers to be selected. This requires modifying the\nio_br_sel struct to separate the fields for address and val, since a\nkernel address cannot be distinguished from a negative val when error\nchecking.\n\nAuto-commit any selected kernel-managed buffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring_types.h |  8 ++++----\n io_uring/kbuf.c                | 16 ++++++++++++----\n 2 files changed, 16 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 3e4a82a6f817..36cc2e0346d9 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -93,13 +93,13 @@ struct io_mapped_region {\n  */\n struct io_br_sel {\n \tstruct io_buffer_list *buf_list;\n-\t/*\n-\t * Some selection parts return the user address, others return an error.\n-\t */\n \tunion {\n+\t\t/* for classic/ring provided buffers */\n \t\tvoid __user *addr;\n-\t\tssize_t val;\n+\t\t/* for kernel-managed buffers */\n+\t\tvoid *kaddr;\n \t};\n+\tssize_t val;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex ccf5b213087b..1e8395270227 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -155,7 +155,8 @@ static int io_provided_buffers_select(struct io_kiocb *req, size_t *len,\n \treturn 1;\n }\n \n-static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n+static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n+\t\t\t     unsigned int issue_flags)\n {\n \t/*\n \t* If we came in unlocked, we have no choice but to consume the\n@@ -170,7 +171,11 @@ static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)\n \tif (issue_flags & IO_URING_F_UNLOCKED)\n \t\treturn true;\n \n-\t/* uring_cmd commits kbuf upfront, no need to auto-commit */\n+\t/* kernel-managed buffers are auto-committed */\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\treturn true;\n+\n+\t/* multishot uring_cmd commits kbuf upfront, no need to auto-commit */\n \tif (!io_file_can_poll(req) && req->opcode != IORING_OP_URING_CMD)\n \t\treturn true;\n \treturn false;\n@@ -200,9 +205,12 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n-\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n+\tif (bl->flags & IOBL_KERNEL_MANAGED)\n+\t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n+\telse\n+\t\tsel.addr = u64_to_user_ptr(READ_ONCE(buf->addr));\n \n-\tif (io_should_commit(req, issue_flags)) {\n+\tif (io_should_commit(req, bl, issue_flags)) {\n \t\tio_kbuf_commit(req, sel.buf_list, *len, 1);\n \t\tsel.buf_list = NULL;\n \t}\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about userspace unregistering a buffer ring while it is pinned by the kernel, explaining that adding kernel APIs to pin and unpin buffer rings will prevent this issue. The new APIs will allow kernel subsystems to safely access buffer ring contents while ensuring the buffer ring remains valid.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "preparatory change"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add kernel APIs to pin and unpin buffer rings, preventing userspace from\nunregistering a buffer ring while it is pinned by the kernel.\n\nThis provides a mechanism for kernel subsystems to safely access buffer\nring contents while ensuring the buffer ring remains valid. A pinned\nbuffer ring cannot be unregistered until explicitly unpinned. On the\nuserspace side, trying to unregister a pinned buffer will return -EBUSY.\n\nThis is a preparatory change for upcoming fuse usage of kernel-managed\nbuffer rings. It is necessary for fuse to pin the buffer ring because\nfuse may need to select a buffer in atomic contexts, which it can only\ndo so by using the underlying buffer list pointer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 17 +++++++++++++\n io_uring/kbuf.c              | 48 ++++++++++++++++++++++++++++++++++++\n io_uring/kbuf.h              |  5 ++++\n 3 files changed, 70 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 375fd048c4cb..702b1903e6ee 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -84,6 +84,10 @@ struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n \t\t\t\t struct io_br_sel *sel, unsigned int issue_flags);\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t    unsigned issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -126,6 +130,19 @@ static inline bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,\n {\n \treturn true;\n }\n+static inline int io_uring_buf_ring_pin(struct io_uring_cmd *cmd,\n+\t\t\t\t\tunsigned buf_group,\n+\t\t\t\t\tunsigned issue_flags,\n+\t\t\t\t\tstruct io_buffer_list **bl)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n+static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned buf_group,\n+\t\t\t\t\t  unsigned issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 1e8395270227..dee1764ed19f 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -9,6 +9,7 @@\n #include <linux/poll.h>\n #include <linux/vmalloc.h>\n #include <linux/io_uring.h>\n+#include <linux/io_uring/cmd.h>\n \n #include <uapi/linux/io_uring.h>\n \n@@ -237,6 +238,51 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \treturn sel;\n }\n \n+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t\t  unsigned issue_flags, struct io_buffer_list **bl)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *buffer_list;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbuffer_list = io_buffer_get_list(ctx, buf_group);\n+\tif (buffer_list && (buffer_list->flags & IOBL_BUF_RING)) {\n+\t\tif (unlikely(buffer_list->flags & IOBL_PINNED)) {\n+\t\t\tret = -EALREADY;\n+\t\t} else {\n+\t\t\tbuffer_list->flags |= IOBL_PINNED;\n+\t\t\tret = 0;\n+\t\t\t*bl = buffer_list;\n+\t\t}\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_pin);\n+\n+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n+\t\t       unsigned issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (bl && (bl->flags & IOBL_BUF_RING) && (bl->flags & IOBL_PINNED)) {\n+\t\tbl->flags &= ~IOBL_PINNED;\n+\t\tret = 0;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_buf_ring_unpin);\n+\n /* cap it at a reasonable 256, will be one page even for 4K */\n #define PEEK_MAX_IMPORT\t\t256\n \n@@ -747,6 +793,8 @@ int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \t\treturn -ENOENT;\n \tif (!(bl->flags & IOBL_BUF_RING))\n \t\treturn -EINVAL;\n+\tif (bl->flags & IOBL_PINNED)\n+\t\treturn -EBUSY;\n \n \tscoped_guard(mutex, &ctx->mmap_lock)\n \t\txa_erase(&ctx->io_bl_xa, bl->bgid);\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 11d165888b8e..781630c2cc10 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -12,6 +12,11 @@ enum {\n \tIOBL_INC\t\t= 2,\n \t/* buffers are kernel managed */\n \tIOBL_KERNEL_MANAGED\t= 4,\n+\t/*\n+\t * buffer ring is pinned and cannot be unregistered by userspace until\n+\t * it has been unpinned\n+\t */\n+\tIOBL_PINNED\t\t= 8,\n };\n \n struct io_buffer_list {\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about buffer recycling by adding an interface for buffers to be recycled back into a kernel-managed buffer ring, and provided the implementation in io_uring/kbuf.c.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "preparatory patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add an interface for buffers to be recycled back into a kernel-managed\nbuffer ring.\n\nThis is a preparatory patch for fuse over io-uring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 11 +++++++++\n io_uring/kbuf.c              | 44 ++++++++++++++++++++++++++++++++++++\n 2 files changed, 55 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 702b1903e6ee..a488e945f883 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -88,6 +88,10 @@ int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t  unsigned issue_flags, struct io_buffer_list **bl);\n int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n \t\t\t    unsigned issue_flags);\n+\n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -143,6 +147,13 @@ static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n+\t\t\t\t\t unsigned int buf_group, u64 addr,\n+\t\t\t\t\t unsigned int len, unsigned int bid,\n+\t\t\t\t\t unsigned int issue_flags)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex dee1764ed19f..17b6178be4ce 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -102,6 +102,50 @@ void io_kbuf_drop_legacy(struct io_kiocb *req)\n \treq->kbuf = NULL;\n }\n \n+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t   u64 addr, unsigned int len, unsigned int bid,\n+\t\t\t   unsigned int issue_flags)\n+{\n+\tstruct io_kiocb *req = cmd_to_io_kiocb(cmd);\n+\tstruct io_ring_ctx *ctx = req->ctx;\n+\tstruct io_uring_buf_ring *br;\n+\tstruct io_uring_buf *buf;\n+\tstruct io_buffer_list *bl;\n+\tint ret = -EINVAL;\n+\n+\tif (WARN_ON_ONCE(req->flags & REQ_F_BUFFERS_COMMIT))\n+\t\treturn ret;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\n+\tif (!bl || WARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING)) ||\n+\t    WARN_ON_ONCE(!(bl->flags & IOBL_KERNEL_MANAGED)))\n+\t\tgoto done;\n+\n+\tbr = bl->buf_ring;\n+\n+\tif (WARN_ON_ONCE((br->tail - bl->head) >= bl->nr_entries))\n+\t\tgoto done;\n+\n+\tbuf = &br->bufs[(br->tail) & bl->mask];\n+\n+\tbuf->addr = addr;\n+\tbuf->len = len;\n+\tbuf->bid = bid;\n+\n+\treq->flags &= ~REQ_F_BUFFER_RING;\n+\n+\tbr->tail++;\n+\tret = 0;\n+\n+done:\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_kmbuf_recycle);\n+\n bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)\n {\n \tstruct io_ring_ctx *ctx = req->ctx;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring_is_kmbuf_ring() function, which was missing an implementation to check if a buffer ring is kernel-managed. The author provided the updated code for this function in the patch, which checks the flags of the buffer list and returns true if it's a kernel-managed buffer ring.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "provided updated code",
                "addressed concern"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "io_uring_is_kmbuf_ring() returns true if there is a kernel-managed\nbuffer ring at the specified buffer group.\n\nThis is a preparatory patch for upcoming fuse kernel-managed buffer\nsupport, which needs to ensure the buffer ring registered by the server\nis a kernel-managed buffer ring.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h |  9 +++++++++\n io_uring/kbuf.c              | 20 ++++++++++++++++++++\n 2 files changed, 29 insertions(+)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex a488e945f883..04a937f6f4d3 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -92,6 +92,9 @@ int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,\n int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t   u64 addr, unsigned int len, unsigned int bid,\n \t\t\t   unsigned int issue_flags);\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -154,6 +157,12 @@ static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,\n {\n \treturn -EOPNOTSUPP;\n }\n+static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n+\t\t\t\t\t  unsigned int buf_group,\n+\t\t\t\t\t  unsigned int issue_flags)\n+{\n+\treturn false;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 17b6178be4ce..797cc2f0a5e9 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -963,3 +963,23 @@ int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n \n \treturn ret;\n }\n+\n+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n+\t\t\t    unsigned int issue_flags)\n+{\n+\tstruct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)->ctx;\n+\tstruct io_buffer_list *bl;\n+\tbool is_kmbuf_ring = false;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\n+\tbl = io_buffer_get_list(ctx, buf_group);\n+\tif (likely(bl) && (bl->flags & IOBL_KERNEL_MANAGED)) {\n+\t\tWARN_ON_ONCE(!(bl->flags & IOBL_BUF_RING));\n+\t\tis_kmbuf_ring = true;\n+\t}\n+\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn is_kmbuf_ring;\n+}\n+EXPORT_SYMBOL_GPL(io_uring_is_kmbuf_ring);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author addressed a concern about the io_uring mutex being held in atomic contexts and needing to select a buffer from a kernel-managed bufring, so they exported the io_ring_buffer_select() function to allow callers to use it without grabbing the mutex.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a specific concern",
                "provided a solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Export io_ring_buffer_select() so that it may be used by callers who\npass in a pinned bufring without needing to grab the io_uring mutex.\n\nThis is a preparatory patch that will be needed by fuse io-uring, which\nwill need to select a buffer from a kernel-managed bufring while the\nuring mutex may already be held by in-progress commits, and may need to\nselect a buffer in atomic contexts.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h | 14 ++++++++++++++\n io_uring/kbuf.c              |  7 ++++---\n 2 files changed, 18 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex 04a937f6f4d3..d4b5943bdeb1 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -95,6 +95,10 @@ int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,\n \n bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,\n \t\t\t    unsigned int issue_flags);\n+\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags);\n #else\n static inline int\n io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n@@ -163,6 +167,16 @@ static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,\n {\n \treturn false;\n }\n+static inline struct io_br_sel io_ring_buffer_select(struct io_kiocb *req,\n+\t\t\t\t\t\t     size_t *len,\n+\t\t\t\t\t\t     struct io_buffer_list *bl,\n+\t\t\t\t\t\t     unsigned int issue_flags)\n+{\n+\tstruct io_br_sel sel = {\n+\t\t.val = -EOPNOTSUPP,\n+\t};\n+\treturn sel;\n+}\n #endif\n \n static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)\ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 797cc2f0a5e9..9a93f10d3214 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -226,9 +226,9 @@ static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,\n \treturn false;\n }\n \n-static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n-\t\t\t\t\t      struct io_buffer_list *bl,\n-\t\t\t\t\t      unsigned int issue_flags)\n+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n+\t\t\t\t       struct io_buffer_list *bl,\n+\t\t\t\t       unsigned int issue_flags)\n {\n \tstruct io_uring_buf_ring *br = bl->buf_ring;\n \t__u16 tail, head = bl->head;\n@@ -261,6 +261,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \t}\n \treturn sel;\n }\n+EXPORT_SYMBOL_GPL(io_ring_buffer_select);\n \n struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \t\t\t\t  unsigned buf_group, unsigned int issue_flags)\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a concern about returning the id of the selected buffer in io_buffer_select(). They agree to modify the function to return the buffer id, which is necessary for kernel-managed buffer rings to recycle the selected buffer.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreement",
                "commitment"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Return the id of the selected buffer in io_buffer_select(). This is\nneeded for kernel-managed buffer rings to later recycle the selected\nbuffer.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n include/linux/io_uring/cmd.h   | 2 +-\n include/linux/io_uring_types.h | 2 ++\n io_uring/kbuf.c                | 7 +++++--\n 3 files changed, 8 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h\nindex d4b5943bdeb1..94df2bdebe77 100644\n--- a/include/linux/io_uring/cmd.h\n+++ b/include/linux/io_uring/cmd.h\n@@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);\n \n /*\n  * Select a buffer from the provided buffer group for multishot uring_cmd.\n- * Returns the selected buffer address and size.\n+ * Returns the selected buffer address, size, and id.\n  */\n struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,\n \t\t\t\t\t    unsigned buf_group, size_t *len,\ndiff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h\nindex 36cc2e0346d9..5a56bb341337 100644\n--- a/include/linux/io_uring_types.h\n+++ b/include/linux/io_uring_types.h\n@@ -100,6 +100,8 @@ struct io_br_sel {\n \t\tvoid *kaddr;\n \t};\n \tssize_t val;\n+\t/* id of the selected buffer */\n+\tunsigned buf_id;\n };\n \n \ndiff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex 9a93f10d3214..24c1e34ea23e 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -250,6 +250,7 @@ struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n \treq->flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;\n \treq->buf_index = READ_ONCE(buf->bid);\n \tsel.buf_list = bl;\n+\tsel.buf_id = req->buf_index;\n \tif (bl->flags & IOBL_KERNEL_MANAGED)\n \t\tsel.kaddr = (void *)(uintptr_t)READ_ONCE(buf->addr);\n \telse\n@@ -274,10 +275,12 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,\n \n \tbl = io_buffer_get_list(ctx, buf_group);\n \tif (likely(bl)) {\n-\t\tif (bl->flags & IOBL_BUF_RING)\n+\t\tif (bl->flags & IOBL_BUF_RING) {\n \t\t\tsel = io_ring_buffer_select(req, len, bl, issue_flags);\n-\t\telse\n+\t\t} else {\n \t\t\tsel.addr = io_provided_buffer_select(req, len, bl);\n+\t\t\tsel.buf_id = req->buf_index;\n+\t\t}\n \t}\n \tio_ring_submit_unlock(req->ctx, issue_flags);\n \treturn sel;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed a concern about indicating the selected buffer in uring_cmd operations, explaining that this is necessary for fuse to relay the information to userspace and agreeing to add the IORING_CQE_F_BUFFER flag along with the buffer index.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed to implement a fix",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When uring_cmd operations select a buffer, the completion queue entry\nshould indicate which buffer was selected.\n\nSet IORING_CQE_F_BUFFER on the completed entry and encode the buffer\nindex if a buffer was selected.\n\nThis will be needed for fuse, which needs to relay to userspace which\nselected buffer contains the data.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n io_uring/uring_cmd.c | 6 +++++-\n 1 file changed, 5 insertions(+), 1 deletion(-)\n\ndiff --git a/io_uring/uring_cmd.c b/io_uring/uring_cmd.c\nindex ee7b49f47cb5..6d38df1a812d 100644\n--- a/io_uring/uring_cmd.c\n+++ b/io_uring/uring_cmd.c\n@@ -151,6 +151,7 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \t\t       unsigned issue_flags, bool is_cqe32)\n {\n \tstruct io_kiocb *req = cmd_to_io_kiocb(ioucmd);\n+\tu32 cflags = 0;\n \n \tif (WARN_ON_ONCE(req->flags & REQ_F_APOLL_MULTISHOT))\n \t\treturn;\n@@ -160,7 +161,10 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,\n \tif (ret < 0)\n \t\treq_set_fail(req);\n \n-\tio_req_set_res(req, ret, 0);\n+\tif (req->flags & (REQ_F_BUFFER_SELECTED | REQ_F_BUFFER_RING))\n+\t\tcflags |= IORING_CQE_F_BUFFER |\n+\t\t\t(req->buf_index << IORING_CQE_BUFFER_SHIFT);\n+\tio_req_set_res(req, ret, cflags);\n \tif (is_cqe32) {\n \t\tif (req->ctx->flags & IORING_SETUP_CQE_MIXED)\n \t\t\treq->cqe.flags |= IORING_CQE_F_32;\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested adding a WARN_ON_ONCE() check to prevent int promotion from causing incorrect results when calculating the difference between br->tail and bl->head, recommending this change for all patches containing WARN_ON_ONCE().",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I think you want:\n\n\tif (WARN_ON_ONCE((__u16)(br->tail - bl->head) >= bl->nr_entries))\n\nhere to avoid int promotion from messing this up if tail has wrapped.\n\nIn general, across the patches for the WARN_ON_ONCE(), it's not a huge\nissue to have a litter of them for now. Hopefully we can prune some of\nthese down the line, however.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe questioned the necessity of introducing a new field 'kmbuf_ring' in the io_uring registration structure, suggesting that the existing 'req->buf_index' could be used instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm probably missing something here, but why can't the caller just use\nreq->buf_index for this?\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe requested a branch with all patches, including users, for easier cross-referencing and evaluation of helper functions.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested information",
                "asked for clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Generally looks pretty good - for context, do you have a branch with\nthese patches and the users on top too? Makes it a bit easier for cross\nreferencing, as some of these really do need an exposed user to make a\ngood judgement on the helpers.\n\nI know there's the older series, but I'm assuming the latter patches\nchanged somewhat too, and it'd be nicer to look at a current set rather\nthan go back to the older ones.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested refactoring io_pbuf_get_region() to handle kernel-managed buffer rings by adding a new helper function, io_kbuf_get_region(), and modifying the existing function to include an error check for IOBL_KERNEL_MANAGED flags.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "For this, I think just add another helper - leave io_pbuf_get_region()\nand add a bl->flags & IOBL_KERNEL_MANAGED error check in there, and\nadd a io_kbuf_get_region() or similar and have a !(bl->flags &\nIOBL_KERNEL_MANAGED) error check in that one.\n\nThat's easier to read, and there's little reason to avoid duplicating\nthe xa_load() part.\n\nMinor nit, but imho it's more readable that way.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested using a pointer to struct io_buffer_list instead of a double pointer, and recommended either returning an error pointer or renaming the passed-on pointer\n\nJens Axboe suggested a more efficient way to check for pinned buffer rings by combining two flags into one condition, and also recommended adding an early return statement when bl is NULL.\n\nReviewer Jens Axboe suggested allowing io_uring commands to exceed the standard 80 character limit, citing that it is acceptable for io_uring due to its specific requirements.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested change",
                "suggestion",
                "requested changes",
                "justification provided"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Probably use the usual struct io_buffer_list *bl here and either use an\nERR_PTR return, or rename the passed on **bl to **blret or something.\n\n---\n\nUsually done as:\n\n\tif ((bl->flags & (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))\n\nand maybe then just have an earlier\n\n\tif (!bl)\n\t\tgoto err;\n\n---\n\nto avoid making it way too long. For io_uring, it's fine to exceed 80\nchars where it makes sense.\n\n-- \nJens Axboe",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-09",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov noted that the code should allow regions to work with user-passed memory, which would enable optimizations such as huge pages, and requested a change to remove this limitation.\n\nThe reviewer suggested refactoring io_uring/kbuf: IORING_REGISTER_KMBUF_RING should not allocate buffers, instead it should register a memory region and use that for buffer allocation. They also proposed using a new flag or internal API to create the buffer ring.\n\nReviewer Pavel Begunkov noted that the removal of io_create_region_multi_buf() eliminates the need for aligning every buffer, which could result in wasted memory due to 64KB page sizes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested refactoring",
                "requested optimization"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "If you're creating a region, there should be no reason why it\ncan't work with user passed memory. You're fencing yourself off\noptimisations that are already there like huge pages.\n\n---\n\nPlease use io_create_region(), the new function does nothing new\nand only violates abstractions.\n\nProvided buffer rings with kernel addresses could be an interesting\nabstraction, but why is it also responsible for allocating buffers?\nWhat I'd do:\n\n1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.\n2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.\n    Or maybe don't expose it to the user at all and create it from\n    fuse via internal API.\n3. Require the user to register a memory region of appropriate size,\n    see IORING_REGISTER_MEM_REGION, ctx->param_region. Make fuse\n    populating the buffer ring using the memory region.\n\nI wanted to make regions shareable anyway (need it for other purposes),\nI can toss patches for that tomorrow.\n\nA separate question is whether extending buffer rings is the right\napproach as it seems like you're only using it for fuse requests and\nnot for passing buffers to normal requests, but I don't see the\nbig picture here.\n\n---\n\nWith io_create_region_multi_buf() gone, you shouldn't need\nto align every buffer, that could be a lot of wasted memory\n(thinking about 64KB pages).",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-10",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Caleb Mateos",
              "summary": "Reviewer Caleb Mateos noted that the patch's optimization !(~bl->flags & (IOBL_BUF_RING|IOBL_PINNED)) is unnecessary, as modern compilers will automatically perform this optimization and potentially optimize it further.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEUTRAL",
                "OPTIMIZATION"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "FWIW, modern compilers will perform this optimization automatically.\nThey'll even optimize it further to !(~bl->flags &\n(IOBL_BUF_RING|IOBL_PINNED)): https://godbolt.org/z/xGoP4TfhP\n\nBest,\nCaleb",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Jens Axboe",
              "summary": "Reviewer Jens Axboe suggested that the patch's implementation should follow a more common and readable approach, citing an example where his own version is easier to read than the original.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "suggestion",
                "example"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure, it's not about that, it's more about the common way of doing it,\nwhich makes it easier to read for people. FWIW, your example is easier\nto read too than the original.\n\n-- \nJens Axboe",
              "reply_to": "Caleb Mateos",
              "message_date": "2026-02-10",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author asks for clarification on whether kernel-managed buffers can optimize huge page allocation, suggesting that the kernel could handle this as well.\n\nAuthor Joanne Koong addressed Pavel Begunkov's feedback about using a single function call to create regions, explaining that separate checks and allocation calls are needed for different types of regions.\n\nThe author addresses Pavel Begunkov's feedback about kernel-managed buffer rings, specifically questioning the benefits of registering buffers from userspace and explaining how kernel allocation simplifies interface and lifecycle management.\n\nAuthor addressed Pavel Begunkov's concern about squashing kernel-managed buffer rings into existing pbuf rings, explaining that this would require adding pinning support to pbuf rings, which was previously dropped due to feedback from Jens and Caleb. The author plans to re-add pinning for kmbuf rings.\n\nAuthor responded to Pavel Begunkov's question about the definition of 'normal requests' in the context of io_uring buffer rings, explaining that for fuse's use case there are only fuse requests.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "asking for clarification",
                "questioning assumption",
                "clarification",
                "explanation",
                "questioning",
                "explaining",
                "acknowledges fix is needed",
                "plans to restructure",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Are there any optimizations with user-allocated buffers that wouldn't\nbe possible with kernel-allocated buffers? For huge pages, can't the\nkernel do this as well (eg I see in io_mem_alloc_compound(), it calls\ninto alloc_pages() with order > 0)?\n\n---\n\nThere's separate checks needed between io_create_region() and\nio_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag\nchecking) and different allocation calls (eg\nio_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).\nMaybe I'm misinterpreting your comment (or the code), but I'm not\nseeing how this can just use io_create_region().\n\n---\n\nConceptually, I think it makes the interface and lifecycle management\nsimpler/cleaner. With registering it from userspace, imo there's\nadditional complications with no tangible benefits, eg it's not\nguaranteed that the memory regions registered for the buffers are the\nsame size, with allocating it from the kernel-side we can guarantee\nthat the pages are allocated physically contiguously, userspace setup\nwith user-allocated buffers is less straightforward, etc. In general,\nI'm just not really seeing what advantages there are in allocating the\nbuffers from userspace. Could you elaborate on that part more?\n\n---\n\nIf kmbuf rings are squashed into pbuf rings, then pbuf rings will need\nto support pinning. In fuse, there are some contexts where you can't\ngrab the uring mutex because you're running in atomic context and this\ncan be encountered while recycling the buffer. I originally had a\npatch adding pinning to pbuf rings (to mitigate the overhead of\nregistered buffers lookups) but dropped it when Jens and Caleb didn't\nlike the idea. But for kmbuf rings, pinning will be necessary for\nfuse.\n\n---\n\nWhat are 'normal requests'? For fuse's use case, there are only fuse requests.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-10",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Jens' concern about accessing buffer IDs from within the io_uring internals by suggesting an alternative approach: adding a helper function to retrieve the buffer ID, such as io_uring_cmd_buf_id() or io_uring_req_buf_id().",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question",
                "open to suggestions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The caller can, but from the caller side they only have access to the\ncmd so they would need to do something like\n\nstruct io_kiocb *req = cmd_to_iocb_kiocb(ent->cmd);\nbuf_id = req->buf_index;\n\nwhich may be kind of ugly with looking inside io-uring internals.\nMaybe a helper here would be nicer, something like\nio_uring_cmd_buf_id() or io_uring_req_buf_id(). It seemed cleaner to\nme to just return the buf id as part of the io_br_sel struct, but I'm\nhappy to do it another way if you have a preference.\n\nThanks,\nJoanne",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged the need for further changes, specifically mentioned making changes pointed out in other comments as part of v2 and waiting on discussion with Pavel before submitting a revised version.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for further changes",
                "waiting on discussion with Pavel"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for reviewing the patches. The branch containing the userside\nchanges on top of these patches is in [1]. I'll make the changes you\npointed out in your other comments as part of v2. Once the discussion\nwith Pavel is resolved / figured out with the changes he wants for v2,\nI'll submit v2.\n\nThanks,\nJoanne\n\n[1] https://github.com/joannekoong/linux/commits/fuse_zero_copy/",
              "reply_to": "Jens Axboe",
              "message_date": "2026-02-10",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer noted that allocating 1MB in kernel-managed buffer rings will not result in a PMD mappable huge page, whereas user space can allocate 2MB and reuse the rest for other purposes.\n\nReviewer Pavel Begunkov suggested that instead of modifying io_create_region() to be less strict, the caller should filter its arguments to only pass IORING_MEM_REGION_TYPE_USER when it's actually used.\n\nReviewer Pavel Begunkov noted that the memmap.c changes can be dropped as they are not necessary for kernel-managed buffer rings and may even introduce issues such as disabling io_mem_alloc_compound(), suggesting a simpler approach where regions do not need to know about buffer subdivision.\n\nReviewer Pavel Begunkov suggested adding a check for user-provided memory to the io_create_region() call, and provided an example of how this could be done by setting rd.user_addr and rd.flags accordingly.\n\nThe reviewer suggests separating ring creation from population on the kernel API level, allowing other users like fuse to create rings and populate them independently.\n\nReviewer Pavel Begunkov suggested that instead of introducing new UAPI and internal changes for kernel-managed buffer rings, the existing pbuf implementation could be modified to support this feature by adding a flag to indicate kernel-managed buffers.\n\nreviewer pointed out that the patch was pinning the registered buffer table without providing buffer rings, which is a bad idea; instead suggested keeping all memory in one larger registered buffer and pinning only it\n\nReviewer Pavel Begunkov expressed concerns that creating many small regions for kernel-managed buffer rings would lead to unnecessary mmap()s, extra user space management, and wasted space, and also questioned the ring bound memory approach due to potential issues with buffer lifetimes.\n\nReviewer Pavel Begunkov noted that kernel-managed buffer rings would be particularly useful for operations like read and recv, where the kernel can fill provided buffers without requiring opcode-specific code changes in kbuf.c.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested change",
                "requested changes",
                "suggested alternative approach",
                "suggested simplification",
                "suggested improvement",
                "requested_changes",
                "expressed concerns",
                "no clear signal of approval or disapproval"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Yes, there is handful of differences. To name one, 1MB allocation won't\nget you a PMD mappable huge page, while user space can allocate 2MB,\nregister the first 1MB and reuse the rest for other purposes.\n\n---\n\nIf io_create_region() is too strict, let's discuss that in\nexamples if there are any, but it's likely not a good idea changing\nthat. If it's too lax, filter arguments in the caller. IOW, don't\npass IORING_MEM_REGION_TYPE_USER if it's not used.\n\n---\n\nI saw that and saying that all memmap.c changes can get dropped.\nYou're using it as one big virtually contig kernel memory range then\nchunked into buffers, and that's pretty much what you're getting with\nnormal io_create_region(). I get that you only need it to be\ncontiguous within a single buffer, but that's not what you're doing,\nand it'll be only worse than default io_create_region() e.g.\neffectively disabling any usefulness of io_mem_alloc_compound(),\nand ultimately you don't need to care.\n\nRegions shouldn't know anything about your buffers, how it's\nsubdivided after, etc.\n\n---\n\nstruct io_uring_region_desc rd = {};\ntotal_size = nr_bufs * buf_size;\nrd.size = PAGE_ALIGN(total_size);\nio_create_region(&region, &rd);\n\nAdd something like this for user provided memory:\n\nif (use_user_memory) {\n\trd.user_addr = uaddr;\n\trd.flags |= IORING_MEM_REGION_TYPE_USER;\n}\n\n---\n\nI don't think I follow. I'm saying that it might be interesting\nto separate rings from how and with what they're populated on the\nkernel API level, but the fuse kernel module can do the population\nand get exactly same layout as you currently have:\n\nint fuse_create_ring(size_t region_offset /* user space argument */) {\n\tstruct io_mapped_region *mr = get_mem_region(ctx);\n\t// that can take full control of the ring\n\tring = grab_empty_ring(io_uring_ctx);\n\n\tsize = nr_bufs * buf_size;\n\tif (region_offset + size > get_size(mr)) // + other validation\n\t\treturn error;\n\n\tbuf = mr_get_ptr(mr) + offset;\n\tfor (i = 0; i < nr_bufs; i++) {\n\t\tring_push_buffer(ring, buf, buf_size);\n\t\tbuf += buf_size;\n\t}\n}\n\nfuse might not care, but with empty rings other users will get a\nchannel they can use to do IO (e.g. read requests) using their\nkernel addresses in the future.\n\n---\n\nIt'd change uapi but not internals, you already piggy back it\non pbuf implementation and differentiate with a flag.\n\nIt could basically be:\n\nif (flags & IOU_PBUF_RING_KM)\n\tbl->flags |= IOBL_KERNEL_MANAGED;\n\nPinning can be gated on that flag as well. Pretty likely uapi\nand internals will be a bit cleaner, but that's not a huge deal,\njust don't see why would you roll out a separate set of uapi\n([un]register, offsets, etc.) when essentially it can be treated\nas the same thing.\n\n---\n\nIIRC, you was pinning the registered buffer table and not provided\nbuffer rings? Which would indeed be a bad idea. Thinking about it,\nfwiw, instead of creating multiple registered buffers and trying to\nlock the entire table, you could've kept all memory in one larger\nregistered buffer and pinned only it. It's already refcounted, so\nshouldn't have been much of a problem.\n\n---\n\nTo explain why, I don't think that creating many small regions\nis a good direction going forward. In case of kernel allocation,\nit's extra mmap()s, extra user space management, and wasted space.\nFor user provided memory it's over-accounting and extra memory\nfootprint. It'll also give you better lifecycle guarantees, i.e.\nyou won't be able to free buffers while there are requests for the\ncontext. I'm not so sure about ring bound memory, let's say I have\nmy suspicions, and you'd need to be extra careful about buffer\nlifetimes even after a fuse instance dies.\n\n---\n\nAny kind of read/recv/etc. that can use provided buffers. It's\nwhere kernel memory filled rings would shine, as you'd be able\nto use them together without changing any opcode specific code.\nI.e. not changes in read request implementation, only kbuf.c\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that pages mapped to userspace can be allocated in the kernel, allowing for a buffer ring that is only mapped read-only into userspace, enabling zero-copy raids if the device requires stable pages for checksumming or raid.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "likes the design",
                "zero-copy raids"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Any pages mapped to userspace can be allocated in the kernel as well.\n\nAnd I really do like this design, because it means we can have a\nbuffer ring that is only mapped read-only into userspace.  That way\nwe can still do zero-copy raids if the device requires stable pages\nfor checksumming or raid.  I was going to implement this as soon\nas this series lands upstream.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author Joanne Koong addressed Pavel Begunkov's concern that the patch uses io_region_allocate_pages_multi_buf() instead of io_create_region(), explaining that it was a necessary change to avoid memory allocation errors due to large buffer sizes in fuse's use case.\n\nAuthor clarifies her understanding of reviewer's feedback, confirming that she and Christoph thought the user should allocate buffers before passing them to kernel, but now realizes the patch uses IORING_REGISTER_MEM_REGION for kernel allocation instead.\n\nThe author addressed a concern about the complexity of the pbuf API, explaining that separating kernel-managed buffer rings (kmbufs) from regular pbufs makes it clearer what each does and avoids unnecessary complexity. They agreed to combine the interfaces in v2.\n\nAuthor acknowledged that she misremembered the issue being addressed as pinning the registered buffer table, not the pbuf ring.\n\nAuthor addressed Pavel Begunkov's concern about sparse buffers populated by the kernel, explaining that they would need to be automatically pinned and potentially requiring users to unregister buffers individually. The author notes that performance differences between pinned and unpinned registered buffers were negligible in their benchmarking.\n\nThe author is addressing a concern about buffer allocation, specifically whether individual buffers should be allocated separately by the kernel. The author acknowledges the memory allocation issue but disagrees that separate buffer allocation would lead to extra mmapping or userspace management.\n\nAuthor asks for clarification on reviewer's concerns about over-accounting and extra memory footprint in kernel-managed buffer rings.\n\nAuthor is addressing concerns about the API and kernel buffer allocation in the io_uring/kbuf series. She plans to make changes for v2, including removing the KMBUF_RING API interface, having kernel buffer allocation go through IORING_REGISTER_MEM_REGION, and adding APIs for subsystems to populate a kernel-managed buffer ring with addresses from registered memory regions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "clarification",
                "explanation",
                "understanding",
                "acknowledged a fix is needed",
                "agreed to restructure",
                "acknowledgment of mistake",
                "acknowledges issue",
                "disagrees with feedback",
                "clarifying question",
                "request for explanation",
                "acknowledges fix is needed",
                "plans changes for v2"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When I originally implemented it, I had it use\nio_region_allocate_pages() but this fails because it's allocating way\ntoo much memory at once. For fuse's use case, each buffer is usually\nat least 1 MB if not more. Allocating the memory one buffer a time in\nio_region_allocate_pages_multi_buf() bypasses the allocation errors I\nwas seeing. That's the main reason I don't think this can just use\nio_create_region().\n\n---\n\nOh okay, from your first message I (and I think christoph too) thought\nwhat you were saying is that the user should be responsible for\nallocating the buffers with complete ownership over them, and then\njust pass those allocated to the kernel to use. But what you're saying\nis that just use a different way for getting the kernel to allocate\nthe buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am\nI reading this correctly?\n\n---\n\nimo, it looked cleaner as a separate api because it has different\nexpectations and behaviors and squashing kmbuf into the pbuf api makes\nthe pbuf api needlessly more complex. Though I guess from the\nuserspace pov, liburing could have a wrapper that takes care of\nsetting up the pbuf details for kernel-managed pbufs. But in my head,\nhaving pbufs vs. kmbufs makes it clearer what each one does vs regular\npbufs vs. pbufs that are kernel-managed.\n\nEspecially with now having kmbufs go through the ioring mem region\ninterface, it makes things more confusing imo if they're combined, eg\npbufs that are kernel-managed are created empty and then populated\nfrom the kernel side by whatever subsystem is using them. Right now\nthere's only one mem region supported per ring, but in the future if\nthere's the possibility that multiple mem regions can be registered\n(eg if userspace doesn't know upfront what mem region length they'll\nneed), then we should also probably add in a region id param for the\nregistration arg, which if kmbuf rings go through the pbuf ring\nregistration api, is not possible to do.\n\nBut I'm happy to combine the interfaces and go with your suggestion.\nI'll make this change for v2 unless someone else objects.\n\n---\n\nYeah, you're right I misremembered and the objections / patch I\ndropped was pinning the registered buffer table, not the pbuf ring\n\n---\n\nHmm, I'm not sure this idea would work for sparse buffers populated by\nthe kernel, unless those are automatically pinned too but then from\nthe user POV for unregistration they'd need to unregister buffers\nindividually instead of just calling IORING_UNREGISTER_BUFFERS but it\nmight be annoying for them to now need to know which buffers are\npinned vs not. When i benchmarked the fuse code with vs without pinned\nregistered buffers, it didn't seem to make much of a difference\nperformance-wise thankfully, so I just dropped it.\n\n---\n\nTo clarify, is this in reply to why the individual buffers shouldn't\nbe allocated separately by the kernel?\nI added a comment about this above in the discussion about\nio_region_allocate_pages_multi_buf(), and if the memory allocation\nissue I was seeing is bypassable and the region can be allocated all\nat once, I'm happy to make that change. With having the allocation be\nseparate buffers though, I'm not sure I agree that there are extra\nmmaps / userspace management. All the pages across the buffers are\nvmapped together and the userspace just needs to do 1 mmap call for\nthem. On the userspace side, I don't think there's more management\nsince the mmapped address represents the range across all the buffers.\nI'm not seeing how there's wasted space either since the only\nrequirement is that the buffer size is page aligned. I think also\nthere's a higher chance of the entire buffer region being physically\ncontiguous if each buffer is allocated separately vs. all the buffers\nare allocated as 1 region. I don't feel strongly about this either way\nand I'm happy to allocate the entire region at once if that's\npossible.\n\n---\n\nJust out of curiosity, could you elaborate on the over-accounting and\nextra memory footprint? I was under the impression it would be the\nsame since the accounting gets adjusted by the total bytes allocated?\nFor the extra memory footprint, is the extra footprint from the\nmetadata to describe each buffer region, or are you referring to\nsomething else?\n\n---\n\nThanks for your input on the series. To iterate / sum up, these are\nchanges for v2 I'll be making:\n- api-wise from userspace/liburing: get rid of KMBUF_RING api\ninterface and have users go through PBUF_RING api instead with a flag\nindicating the ring is kernel-managed\n- have kernel buffer allocation go through IORING_REGISTER_MEM_REGION\ninstead, which means when the pbuf ring is created and the\nkernel-managed flag is set, the ring will be empty. The memory region\nwill need to be registered before the mmap call to the ring fd.\n- add apis for subsystems to populate a kernel-managed buffer ring\nwith addresses from the registered mem region\n\nDoes this align with your understanding of the conversation as well or\nis there anything I'm missing?\n\nAnd Christoph, do these changes for v2 work for your use case as well?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-11",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig argued against kernel-managed buffer rings, citing a need for the kernel to control allocation and guarantee user processes can only read memory without writing to it.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm arguing exactly against this.  For my use case I need a setup\nwhere the kernel controls the allocation fully and guarantees user\nprocesses can only read the memory but never write to it.  I'd love\nto be able to piggy back than onto your work.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer pointed out that the current implementation of pow2 round ups will waste memory, as 1MB allocations will not become 2MB huge pages and there is also a concern about 1GB huge pages. They suggested that users could make better placement decisions.\n\nReviewer Pavel Begunkov suggested that the io_uring uapi should include fields for user-provided memory as an optional feature, and noted that fuse can refuse to bind to buffer rings it doesn't like.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "waste of memory",
                "user can be smarter",
                "requested additional consideration for user-provided memory"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "pow2 round ups will waste memory. 1MB allocations will never\nbecome 2MB huge pages. And there is a separate question of\n1GB huge pages. The user can be smarter about all placement\ndecisions.\n\n---\n\nThat's an interesting case. To be clear, user provided memory is\nan optional feature for pbuf rings / regions / etc., and I think\nthe io_uring uapi should leave fields for the feature. However, I\nhave nothing against fuse refusing to bind to buffer rings it\ndoesn't like.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested modifying IORING_REGISTER_MEM_REGION to support read-only registrations, and proposed adding a new registration flag or rejecting unsupported setups during initialization.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IORING_REGISTER_MEM_REGION supports both types of allocations. It can\nhave a new registration flag for read-only, and then you either make\nthe bounce avoidance optional or reject binding fuse to unsupported\nsetups during init. Any arguments against that? I need to go over\nJoanne's reply, but I don't see any contradiction in principal with\nyour use case.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarified that kernel-managed buffer rings can be used internally by the kernel without explicit user registration, and explained that this would simplify the process compared to using IORING_REGISTER_MEM_REGION.\n\nAuthor acknowledged a concern about the complexity of kernel-managed buffer rings, suggesting an alternative design where a straightforward kmbuf ring uses the pbuf interface and a future interface for pbuf rings to use IORING_REGISTERED_MEM_REGIONS.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question",
                "explanation",
                "overkill",
                "over-engineered"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"control the allocation fully\" do you mean for your use case, the\nallocation/setup isn't triggered by userspace but is initiated by the\nkernel (eg user never explicitly registers any kbuf ring, the kernel\njust uses the kbuf ring data structure internally and users can read\nthe buffer contents)? If userspace initiates the setup of the kbuf\nring, going through IORING_REGISTER_MEM_REGION would be semantically\nthe same, except the buffer allocation by the kernel now happens\nbefore the ring is created and then later populated into the ring.\nuserspace would still need to make an mmap call to the region and the\nkernel could enforce that as read-only. But if userspace doesn't\ninitiate the setup, then going through IORING_REGISTER_MEM_REGION gets\nuglier.\n\n---\n\nSo i guess the flow would have to be:\na) user calls io_uring_register_region(&ring, &mem_region_reg) with\nmem_region_reg.region_uptr's size field set to the total buffer size\n(and mem_region_reg.flags read-only bit set if needed)\n     kernel allocates region\nb) user calls mmap() to get the address of the region. If read-only\nbit was set, it gets a read-only address\nc) user calls io_uring_register_buf_ring(&ring, &buf_reg, flags) with\nbuf_reg.flags |= IOU_PBUF_RING_KERNEL_MANAGED\n     kernel creates an empty kernel-managed ring. None of the buffers\nare populated\nd) user tells X subsystem to populate the ring starting from offset Z\nin the registered mem region\ne) on the kernel side, the subsystem populates the ring starting from\noffset Z, filling it up using the buf_size and ring_entries values\nthat the user registered the ring with in c)\n\nTo be completely honest, the more I look at this the more this feels\nlike overkill / over-engineered to me. I get that now the user can do\nthe PMD optimization, but does that actually lead to noticeable\nperformance benefits? It seems especially confusing with them going\nthrough the same pbuf ring interface but having totally different\nexpectations.\n\nWhat about adding a straightforward kmbuf ring that goes through the\npbuf interface (eg the design in this patchset) and then in the future\nadding an interface for pbuf rings (both kernel-managed and\nnon-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if\nusers end up needing/wanting to have their rings populated that way?\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Reviewer noted that if an application is concerned about TLB pressure, a simple solution would be to allocate buffers in multiples of page table entry (PTE) levels.\n\nReviewer Christoph Hellwig expressed confusion about the term 'pbuf' used in the patch, specifically asking for clarification on what it refers to and questioning whether it's a fixed buffer API.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "requested alternative approach",
                "confusion",
                "lack of clarity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sure.  But if the application cares that much about TLB pressure\nI'd just round up to nice multtiple of PTE levels.\n\n---\n\nCan you clarify what you mean with 'pbuf'?  The only fixed buffer API I\nknow is io_uring_register_buffers* which always takes user provided\nbuffers, so I have a hard time parsing what you're saying there.  But\nthat might just be sign that I'm no expert in io_uring APIs, and that\nweb searches have degraded to the point of not being very useful\nanymore.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "reviewer questioned the purpose of IORING_REGISTER_MEM_REGION, citing a mismatch between its description in the commit message and public documentation\n\nThe reviewer noted that their use case does not involve fuse, but rather block and file system I/O, implying that the patch may not be applicable to their specific needs.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "requested clarification",
                "highlighted a potential limitation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "IORING_REGISTER_MEM_REGION seems to be all about cqs from both your\ncommit message and the public documentation.  I'm confused.\n\n---\n\nMy use case is not about fuse, but good old block and file system\nI/O.",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Christoph Hellwig",
              "summary": "Christoph Hellwig noted that io_uring_register_buffers() only pins memory, allowing applications or other processes to modify it, which can cause issues for file systems and storage devices that need to verify checksums or rebuild data from parity.\n\nreviewer noted that the PMD mapping optimization is almost as valuable on both AMD and ARM, making it relevant despite initial concerns",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "revised opinion",
                "acknowledged relevance"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "The idea is that the application tells the kernel that it wants to use\na fixed buffer pool for reads.  Right now the application does this\nusing io_uring_register_buffers().  The problem with that is that\nio_uring_register_buffers ends up just doing a pin of the memory,\nbut the application or, in case of shared memory, someone else could\nstill modify the memory.  If the underlying file system or storage\ndevice needs verify checksums, or worse rebuild data from parity\n(or uncompress), it needs to ensure that the memory it is operating\non can't be modified by someone else.\n\nSo I've been thinking of a version of io_uring_register_buffers where\nthe buffers are not provided by the application, but instead by the\nkernel and mapped into the application address space read-only for\na while, and I thought I could implement this on top of your series,\nbut I have to admit I haven't really looked into the details all\nthat much.\n\n---\n\nYes.  The PMD mapping also is not that relevant.  Both AMD (implicit)\nand ARM (explicit) have optimizations for contiguous PTEs that are\nalmost as valuable.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-12",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "The reviewer pointed out that the patch introduces a new type of buffer ring that allocates kernel buffers and maps them into user space, but this is not clearly related to the original purpose of io_uring provided buffer rings. He questioned why fuse needs to reuse pbuf ring code as an internal memory allocator and suggested that it could be contained within fuse instead of exposing buffer rings as an io_uring uapi.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "inflexibility",
                "potential for kernel crashes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Registered, aka fixed, buffers are the ones you pass to\nIORING_OP_[READ,WRITE]_FIXED and some other requests. It's normally\ncreated by io_uring_register_buffers*() / IORING_REGISTER_BUFFERS*\nwith user memory, but there are special cases when it's installed\ninternally by other kernel components, e.g. ublk.\nThis series has nothing to do with them, and relevant parts of\nthe discussion here don't mention them either.\n\nProvided buffer rings, a.k.a pbuf rings, IORING_REGISTER_PBUF_RING\nis a kernel-user shared ring. The entries are user buffers\n{uaddr, size}. The user space adds entries, the kernel (io_uring\nrequests) consumes them and issues I/O using the user addresses.\nE.g. you can issue a IORING_OP_RECV request (+IOSQE_BUFFER_SELECT)\nand it'll grab a buffer from the ring instead of using sqe->addr.\n\npbuf rings, IORING_REGISTER_MEM_REGION, completion/submission\nqueues and all other kernel-user rings/etc. are internally based\non so called regions. All of them support both user allocated\nmemory and kernel allocations + mmap.\n\nThis series essentially creates provided buffer rings, where\n1. the ring now contains kernel addresses\n2. the ring itself is in-kernel only and not shared with user space\n3. it also allocates kernel buffers (as a region), populates the ring\n    with them, and allows mapping the buffers into the user space.\n\nFuse is doing both adding (kernel) buffers to the ring and consuming\nthem. At which point it's not clear:\n\n1. Why it even needs io_uring provided buffer rings, it can be all\n    contained in fuse. Maybe it's trying to reuse pbuf ring code as\n    basically an internal memory allocator, but then why expose buffer\n    rings as an io_uring uapi instead of keeping it internally.\n\n    That's also why I mentioned whether those buffers are supposed to\n    be used with other types of io_uring requests like recv, etc.\n\n2. Why making io_uring to allocate payload memory. The answer to which\n    is probably to reuse the region api with mmap and so on. And why\n    payload buffers are inseparably created together with the ring\n    and via a new io_uring uapi.\n\n    And yes, I believe in the current form it's inflexible, it requires\n    a new io_uring uapi. It requires the number of buffers to match\n    the number of ring entries, which are related but not the same\n    thing. You can't easily add more memory as it's bound to the ring\n    object. The buffer memory won't even have same lifetime as the\n    ring object -- allow using that km buffer ring with recv requests\n    and highly likely I'll most likely give you a way to crash the\n    kernel.\n\nBut hey, I'm tired. I don't have any beef here and am only trying\nto make it a bit cleaner and flexible for fuse in the first place\nwithout even questioning the I/O path. If everyone believes\neverything is right, just ask Jens to merge it.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed concern that the kernel-managed buffer rings are being used for large payload buffers, which was not their intended use case; they suggested respinning patches to place SQ/CQ onto a different area of memory\n\nReviewer Pavel Begunkov expressed confusion about how the kernel-managed buffer rings can work without a kernel component returning buffers into the ring, as io_uring does not currently support this.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concern about misuse of kernel-managed buffer rings",
                "confusion",
                "requested clarification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Think of it as an area of memory for kernel-user communication. Used\nfor syscall parameters passing to avoid copy_from_user, but I added\nit for a bunch of use cases. We'll hopefully get support at some\npoint for passing request arguments like struct iovec. BPF patches\nuse it for communication. I need to respin patches placing SQ/CQ onto\nit (avoid some memory waste).\n\nTbh, I never meant it nor io_uring regions to be used for huge\npayload buffers, but this series already uses regions for that.\n\n---\n\nThen I'm confused. Take a look at the other reply, this series is\nabout buffer rings with kernel memory, it can't work without a kernel\ncomponent returning buffers into the ring, and io_uring doesn't do\nthat. But maybe you're thinking about adding some more elaborate API.\n\nIIUC, Joanne also wants to add support for fuse installing registered\nbuffers, which would allow zero-copy, but those got split out of\nthis series.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested a workaround for an issue, noting that the patch should instead use the kernel-managed buffer ring for metadata like fuse headers and payloads, which would allow zero-copying.\n\nReviewer Pavel Begunkov suggested moving ring population from io_uring uapi to fuse, and using IORING_REGISTER_MEM_REGION to simplify the process\n\nReviewer Pavel Begunkov noted that the differences between io_uring and kmbuf are mostly due to special region path and embedded buffer allocations, but suggested that making a separate opcode might still be beneficial.\n\nThe reviewer noted that the current implementation does not provide a clear control path for fuse to bind kernel-managed buffer rings, and suggested adding an io-uring command (FUSE_CMD_BIND_BUFFER_RING) to handle this scenario. They also proposed passing necessary parameters through this command and pinning the buffer ring before populating it with data.\n\nReviewer Pavel Begunkov pointed out that the patch could use IORING_REGISTER_MEM_REGION instead of a separate region, which is a distinct issue from whether buffers should be bound to the ring.\n\nThe reviewer noted that if the total allocation size is not a power of two, the kernel may allocate a huge page larger than the actual buffer size, wasting memory.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "workaround",
                "suggested changes",
                "no clear opinion",
                "NEUTRAL",
                "open-minded",
                "suggested improvements",
                "memory waste",
                "inefficient allocation"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Let's fix that then. For now, just work it around by wrapping\ninto a loop.\n\nBtw, I thought you're going to use it for metadata like some\nfuse headers and payloads would be zero copied by installing\nit as registered buffers.\n\n...\n\n---\n\nThe main point is disentangling memory allocation from ring\ncreation in the io_uring uapi, and moving ring population\ninto fuse instead of doing it at creation. And it'll still be\npopulated by the kernel (fuse), user space doesn't have access\nto the ring. IORING_REGISTER_MEM_REGION is just the easiest way\nto achieve that without any extra uapi.\n\n...\n\n---\n\nIt appeared to me that they're different because of special\nregion path and embedded buffer allocations, and otherwise\ndifferences would be minimal. But if you think it's still\nbetter to be made as a separate opcode, I'm not opposing it,\ngo for it.\n\n---\n\nNot having patches using the functionality is inconvenient. How\nfuse looks up the buffer ring from io_uring? I could imagine you\nhave some control path io-uring command:\n\ncase FUSE_CMD_BIND_BUFFER_RING:\n\treturn bind_queue(params);\n\nThen you can pass all necessary parameters to it, pseudo code:\n\nstruct fuse_bind_kmbuf_ring_params {\n\tregion_id;\n\tbuf_ring_id;\n\t...\n};\n\nbind_queue(cmd, struct fuse_bind_kmbuf_ring_params *p)\n{\n\tregion = io_uring_get_region(cmd, p->region_id);\n\t// get exclusive access:\n\tbuf_ring = io_uring_get_buf_ring(cmd, p->buf_ring_id);\n\n\tif (!validate_buf_ring(buf_ring))\n\t\treturn NOTSUPPORTED;\n\n\tio_uring_pin(buf_ring);\n\tfuse_populate_buf_ring(buf_ring, region, ...);\n}\n\nDoes that match expectations? I don't think you even need\nthe ring part exposed as an io_uring uapi, tbh, as it\nstays completely in fuse and doesn't meaningfully interact\nwith the rest of io_uring.\n\n...\n\n---\n\nThat was about an argument for using IORING_REGISTER_MEM_REGION\ninstead a separate region. And it's separate from whether\nbuffers should be bound to the ring.\n\n---\n\nI shouldn't affect you much since you have such large buffers,\nbut imagine the total allocation size is not being pow2, and\nthe kernel allocating it as a single folio. E.g. 3 buffers,\n0.5 MB each, total = 1.5MB, and the kernel allocates a 2MB\nhuge page.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed concern that the patch series does not address registered buffers and suggested separating kernel-managed buffer rings from io_uring, arguing that reusing buffer allocation would introduce unnecessary complexity.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested separation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "There is nothing about registered buffers in this series. And even\nif you try to reuse buffer allocation out of it, it'll come with\na circular buffer you'll have no need for. And I'm pretty much\narguing about separating those for io_uring.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov suggested reusing regions for allocations and mmap() operations, wrapping them into a registered buffer, and making vmap'ing optional to avoid unnecessary overhead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "suggested improvement",
                "optional optimization"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "FWIW, the easiest solution is to internally reuse regions for\nallocations and mmap()'ing and wrap it into a registered buffer.\nIt just need to make vmap'ing optional as it won't be needed.\n\n-- \nPavel Begunkov",
              "reply_to": "",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed concerns that the io_uring uapi should not be tied to specific use cases or requirements, such as uniform buffer sizes, matching ring and buffer counts, and kernel-allocated buffers. He questioned the need for the total size at creation and suggested allowing runtime addition of memory while using the same ring.\n\nReviewer Pavel Begunkov expressed confusion about the distinction between kernel-managed buffer rings and user-visible buffer rings, questioning what expectations are different apart from one being in-kernel with kernel addresses and the other user-visible with user addresses.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "strong opinion",
                "confusion",
                "questioning"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, it's mainly about not keeping payload buffers and rings in the same\nobject from the io_uring uapi perspective.\n\n1. If it's an io_uring uapi, it shouldn't be fuse specific or with\na bunch of use case specific expectations attached. Why does it\nrequire all buffers to be uniform in size? Why does it require\nthe ring size to match the number of buffers? Why does it require\nbuffers to be allocated by io_uring in the first place? Maybe some\nsubsystem got memory from somewhere else and wants to do use it\nwith io_uring. Why does it need to know the total size at creation,\nand what would you do if you want to add more memory at runtime\nwhile using the same ring?\n\n2. If it's meant to be fuse specific and _not_ used with other requests\nlike recv/read/etc., then what's the point of having it as an io_uring\nuapi? Which also adds additional trouble like the once you're solving\nwith pinning.\n\nIf it's supposed to be used with other requests, then buffers and\nrings will have different in-kernel lifetime expectations imposed\nby io_uring, so having them together won't even help with\nmanagement.\n\nI have a strong opinion about the memmap.c change. For the\nrest, if you believe it's fine, just send it out and let Jens\ndecide.\n\n---\n\nIt's predicated on separating buffers from rings, see above,\nand assuming that I'm not sure what expectations are different\napart from one being in-kernel with kernel addresses and the\nother user visible with user addresses.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author addressed Pavel Begunkov's concern about wasted space in the io_ring buffer by explaining that a circular buffer will allow buffers to be shared across entries, reducing memory allocation.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a potential issue",
                "explained a technical solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I think the circular buffer will be useful for Christoph's use case in\nthe same way it'll be useful for fuse's. The read payload could be\ndifferently sized across requests, so it's a lot of wasted space to\nhave to allocate a buffer large enough to support the max-size request\nper entry in the io_ring. With using a circular buffer, buffers have a\nway to be shared across entries, which means we can significantly\nreduce how much memory needs to be allocated.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that Christoph Hellwig's use case for read-only buffers aligns with the benefits of kernel-managed buffer rings, which will provide memory wins through incremental buffer consumption.\n\nAuthor addressed Christoph's concern about how to handle read-only mappings for kernel-managed buffer rings, suggesting a simple solution by passing a read-only flag from userspace and checking it when mmap is called. She offered to add this patch to the series if needed.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "alignment",
                "benefits",
                "agreed",
                "offered_to_add_patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "(resending because I hit reply instead of reply-all)\n\nI think we have the exact same use case, except your buffers need to\nbe read-only. I think your use case benefits from the same memory wins\nwe'll get with incremental buffer consumption, which is the primary\nreason fuse is using a bufring instead of fixed buffers.\n\n---\n\nI think you can and it'll be very easy to do so. All that would be\nneeded is to pass in a read-only flag from the userspace side when it\nregisters the bufring, and then when userspace makes the mmap call to\nthe bufring, the kernel checks if that read-only flag is set on the\nbufring and if so returns a read-only mapping. I'm happy to add that\npatch to this series if that would make things easier for you. The\nio_uring_register_buffers() api registers fixed buffers (which have to\nbe user-allocated memory) so you would need to go through the\nio_uring_register_buf_ring() api once kmbufs are squashed into the\npbuf interface.\n\nWith going through IORING_MEM_REGION, this would work for your use\ncase as well. The user would have to register the mem region with\nio_uring_register_region() and pass in a read-only flag, and then the\nkernel will allocate the memory region. Then userspace would mmap the\nmemory region and on the kernel side, it would set the mapping to be\nread-only. When the kmbufring then gets registered, the buffers in it\nwill be empty. The filesystem will then have to populate the buffers\nin it from the mem region that was previously registered.\n\nThanks,\nJoanne",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Bernd Schubert",
              "summary": "Reviewer Bernd Schubert expressed skepticism about sharing buffers across io_uring entries, suggesting it would only reduce ring size and questioned its purpose.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "skepticism",
                "questioning"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Dunno, what we actually want is requests of multiple sizes. Sharing\nbuffers across entries sounds like just reducing the ring size - I\npersonally don't see the point here.\n\n\nThanks,\nBernd",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author clarified that kernel-managed buffer rings allow concurrent access to different regions of a shared buffer, addressing concerns about buffer sharing.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "By \"sharing buffers across entries\" what I mean is different regions\nof the buffer can now be used concurrently by multiple entries.\n\nThanks,\nJoanne",
              "reply_to": "Bernd Schubert",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing Pavel Begunkov's concern about the need for kernel-managed buffer rings, specifically in the context of fuse's use case where buffers are shared between the kernel and server. The author explains that the kernel needs to control when buffers get recycled back into the ring because the server writes data back to the kernel in those buffers after submitting an sqe.\n\nAuthor addressed Pavel Begunkov's concern about userspace applications using kernel-managed buffer rings for operations other than io_uring, explaining that they are used for reading and writing contents from/to a locally-backed file.\n\nThe author is addressing Pavel Begunkov's feedback about using a registered memory region, which allows optimizations like PMD. The author acknowledges that these optimizations may be beneficial for some workloads but believes most kmbuf use cases don't require them, and thus doesn't see the need to add complexity.\n\nThe author addressed Pavel Begunkov's concern that combining kernel-managed buffer rings (kmbufs) with user-provided buffer rings (pbufs) in a single UAPI is confusing, as kmbufs have different expectations and behaviors. The author initially thought it was cleaner to separate them into distinct UAPIs but now agrees to put kmbufs through the pbuf UAPI for v2.\n\nAuthor Joanne Koong is addressing a concern about the purpose and behavior of ring entries without associated buffers, explaining that this can be fixed by passing in the number of buffers from the uapi for kernel-managed pbuf rings.\n\nAuthor addressed Pavel Begunkov's concern about the limitations of kernel-managed buffer rings by explaining that adding more memory to the mem region is difficult and that users need to know upfront how much memory to allocate, which may be hard to do.\n\nAuthor addressed Pavel Begunkov's concern that the buffer memory has a different lifetime than the ring object by explaining that the buffers are freed when the ring is freed.\n\nAuthor is addressing Pavel's concern about whether kernel-managed buffer rings should support both simple and registered memory regions, offering to make a change if that's the case.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation",
                "acknowledges feedback",
                "expresses skepticism about added complexity",
                "acknowledged a design decision",
                "agreed to reconsider",
                "open_to_change",
                "willing_to_compromise"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The most important part and the whole reason fuse needs the buffer\nring to be kernel-managed is because the kernel needs to control when\nbuffers get recycled back into the ring. For fuse's use case, the\nbuffer is used for passing data between the kernel and the server. We\ncan't have the server recycle the buffer because the server writes\nback data to the kernel in that buffer when it submits the sqe. After\nfuse receives the sqe and reads the reply from the server, it then\nneeds to recycle that buffer back into the ring so it can be reused\nfor a future cqe (eg sending a future request).\n\n---\n\nOn the userspace/server side, it uses the buffers for other io-uring\noperations (eg reading or writing the contents from/to a\nlocally-backed file).\n\n---\n\nMy main motivation for this is simplicity. I see (and thanks for\nexplaining) that using a registered mem region allows the use of some\noptimizations (the only one I know of right now is the PMD one you\nmentioned but maybe there's more I'm missing) that could be useful for\nsome workloads, but I don't think (and this could just be my lack of\nunderstanding of what more optimizations there are) most use cases of\nkmbufs benefit from those optimizations, so to me it feels like we're\nadding non-trivial complexity for no noticeable benefit.\n\nI feel like we get the best of both worlds by letting users have both:\nthe simple kernel-managed pbuf where the kernel allocates the buffers\nand the buffers are tied to the lifecycle of the ring, and the more\nadvanced kernel-managed pbuf where buffers are tied to a registered\nmemory region that the subsystem is responsible for later populating\nthe ring with.\n\n---\n\nimo it felt cleaner to have a new uapi for it because kmbufs and pbufs\nhave different expectations and behaviors (eg pbufs only work with\nuser-provided buffers and requires userspace to populate the ring\nbefore using it, whereas for kmbufs the kernel allocates the buffers\nand populates it for you; pbufs require userspace to recycle back the\nbuffer, whereas for kmbufs the kernel is the one in control of\nrecycling) and from the user pov it seemed confusing to have kmbufs as\npart of the pbuf ring uapi, instead of separating it out as a\ndifferent type of ringbuffer with a different expectation and\nbehavior. I was trying to make the point that combining the interface\nif we go with IORING_MEM_REGION gets even more confusing because now\npbufs that are kernel-managed are also empty at initialization and\nonly can point to areas inside a registered mem region and the\nresponsibility of populating it is now on whatever subsystem is using\nit.\n\nI still have this opinion but I also think in general, you likely know\nbetter than I do what kind of io-uring uapi is best for io-uring's\nusers. For v2 I'll have kmbufs go through the pbuf uapi.\n\n---\n\nI'm not really seeing what the purpose of having a ring entry with no\nbuffer associated with it is. In the existing code for non-kernel\nmanaged pbuf rings, there's the same tie between reg->ring_entries\nbeing used as the marker for how many buffers the ring supports. But\nif the number of buffers should be different than the number of ring\nentries, this can be easily fixed by passing in the number of buffers\nfrom the uapi for kernel-managed pbuf rings.\n\n---\n\nTo play devil's advocate, we also can't easily add more memory to the\nmem region once it's been registered. I think there's also a worse\npenalty where the user needs to know upfront how much memory to\nallocate for the mem region for the lifetime of the ring, which imo\nmay be hard to do (eg if a kernel-managed buf ring only needs to be\nregistered for some code paths and not others, the mem region\nregistration would still have to allocate the memory a potential kbuf\nring would use).\n\n---\n\nI'm a bit confused by this part. The buffer memory does have the same\nlifetime as the ring object, no? The buffers only get freed when the\nring itself is freed.\n\n---\n\nI appreciate you looking at this and giving your feedback and insight.\nThank you for doing so. I don't want to merge in something you're\nunhappy with.\n\nAre you open to having support for both a simple kernel-managed pbuf\ninterface and later on if/when the need arises, a kernel-managed pbuf\ninterface that goes through a registered memory region? If the answer\nis no, then I'll make the change to have kmbufs go through the\nregistered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-13",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov noted that buffer rings are not suitable for storage read/write requests because they bind to a buffer immediately, whereas other types of requests like recv allow io_uring to poll the socket before taking a buffer from the ring. He also pointed out that someone needs to return buffers back into the kernel-private ring, which is currently assumed to be handled by the fuse driver but is unclear for normal rw requests.\n\nThe reviewer suggests using IORING_MEM_REGION or a standalone registered buffer extension to provide buffers/memory without extra semantics, potentially leading to a finer API.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "unclear implementation"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Provided buffer rings are not useful for storage read/write requests\nbecause they bind to a buffer right away, that's in contrast to some\nrecv request, where io_uring will first poll the socket to confirm\nthe data is there, and only then take a buffer from the buffer ring\nand copy into it. With storage rw it makes more sense to specify\nthe buffer directly gain control over where exactly data lands\nIOW, instead of the usual \"read data into a given pointer\" request\nsemantics like what read(2) gives you, buffer rings are rather\n\"read data somewhere and return a pointer to where you placed it\".\n\nAnother problem is that someone needs to return buffers back into\nthe buffer ring, and it's a kernel private ring. For this patchset\nit's assumed the fuse driver is going to be doing that, but there\nis no one for normal rw requests.\n\n---\n\nYes. You only need buffers, and it'll be better to base on sth that\ngives you buffers/memory without extra semantics, i.e.\nIORING_MEM_REGION. Or it can be a standalone registered buffer\nextension, likely reusing regions internally. That might even yield\na finer API.\n\n-- \nPavel Begunkov",
              "reply_to": "Christoph Hellwig",
              "message_date": "2026-02-18",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov asked whether kernel-managed buffer rings can be used with other requests, specifically IORING_OP_RECV with IOSQE_BUFFER_SELECT set and bgid specifying the kernel-managed buffer ring.\n\nReviewer Pavel Begunkov raised two separate concerns: first, the io_uring user API should not make buffers inseparable from buffer rings; second, the patch should optionally allow user memory for buffer creation by reusing the region abstraction.\n\nPavel Begunkov questioned the necessity of making kernel-managed buffer rings an io_uring API, suggesting that it could be simpler to implement in fuse or as an implementation detail within io_uring\n\nReviewer Pavel Begunkov expressed concerns that the kernel-managed buffer rings (km rings) API in io_uring is not reusable and specific to the fuse use case, suggesting a middle ground where km rings can be registered together with memory as a pure region without a notion of a buffer, allowing users like fuse to chunk it.\n\nreviewer noted that the kernel-managed buffer ring API is not fully generic and makes assumptions specific to fuse, which could lead to issues if used by other io_uring users\n\nThe reviewer noted that the current implementation only specifies the buffer ring depth but not the amount of memory allocated by userspace and how it's pushed, which could lead to issues such as over-allocation or under-allocation of buffers.\n\nReviewer Pavel Begunkov suggested that instead of passing the number of buffers to io_uring, the kernel should create a large chunk of memory and then have fuse divide it up and put into the ring, reducing assumptions about uapi\n\nReviewer Pavel Begunkov agreed with the patch but noted that adding new memory would require a new mechanism, not necessarily tied to IORING_REGISTER_MEM_REGION.\n\nThe reviewer noted that unregistering a kernel-managed buffer ring does not guarantee that there are no in-flight requests using buffers from the ring, and suggested synchronizing with all other io_uring requests to ensure proper cleanup.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "clarification",
                "question",
                "no objection",
                "trivial to implement",
                "requested changes",
                "suggested alternative approach",
                "specificity",
                "reusability",
                "agreed",
                "new mechanism",
                "synchronization issue",
                "inflight requests"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Oops, typo. I was asking whether the buffer rings (not buffers) are\nsupposed to be used with other requests. E.g. submitting a\nIORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying\nyour kernel-managed buffer ring.\n\n---\n\nThere are two separate arguments. The first is about not making buffers\ninseparable from buffer rings in the io_uring user API. Whether it's\nIORING_REGISTER_MEM_REGION or something else is not that important.\nI have no objection if it's a part of fuse instead though, e.g. if\nfuse binds two objects together when you register it with fuse, or even\nif fuse create a buffer ring internally (assuming it doesn't indirectly\nleak into io_uring uapi).\n\nAnd the second was about optionally allowing user memory for buffer\ncreation as you're reusing the region abstraction. You can find pros\nand cons for both modes, and funnily enough, SQ/CQ were first kernel\nallocated and then people asked for backing it by user memory, and IIRC\nit was in the reverse order for pbuf rings.\n\nImplementing this is trivial as well, you just need to pass an argument\nwhile creating a region. All new region users use struct\nio_uring_region_desc for uapi and forward it to io_create_region()\nwithout caring if it's user or kernel allocated memory.\n\n---\n\nThe stress is on why it's an _io_uring_ API. It doesn't matter to me\nwhether it's a separate opcode or not. Currently, buffer rings don't give\nyou anything that can't be pure fuse, and it might be simpler to have\nit implemented in fuse than binding to some io_uring object. Or it could\ncreate buffer rings internally to reuse code but it doesn't become an\nio_uring uapi but rather implementation detail. And that predicates on\nwhether km rings are intended to be used with other / non-fuse requests.\n\n---\n\nI believe the source of disagreement is that you're thinking\nabout how it's going to look like for fuse specifically, and I\nbelieve you that it'll be nicer for the fuse use case. However,\non the other hand it's an io_uring uapi, and if it is an io_uring\nuapi, we need reusable blocks that are not specific to particular\nusers.\n\nIf it km rings has to stay an io_uring uapi, I guess a middle\nground would be to allow registering km rings together with memory,\nbut make it a pure region without a notion of a buffer, and let\nfuse to chunk it. Later, we can make payload memory allocation\noptional.\n\n---\n\nRight, intentionally so, because otherwise it's a fuse uapi that\npretends to be a generic io_uring uapi but it's not because of\nall assumptions in different places.\n\n---\n\nNot really, it tells the buffer ring depth but says nothing about\nhow much memory user space allocated and how it's pushed. It's a\nreasonable default but they could be different. For example, if you\nexpect adding more memory at runtime, you might create the buffer\nring a bit larger. Or when server processing takes a while and you\ncan't recycle until it finishes, you might have more buffers than\nyou need ring entries. Or you might might decide to split buffers\nand as you mentioned incremental consumption, which is an entire\nseparate topic because it doesn't do de-fragmentation and you'd\nneed to have it in fuse, just like user space does with pbufs.\n\n---\n\nMy entire point is that we're making lots of assumptions for io_uring\nuapi, and if it's moved to fuse because it knows better what it\nneeds, it should be a win.\n\nIOW, it sounds better if instead of passing the number of buffers to\nio_uring, you just ask it to create a large chunk of memory, and then\nfuse chunks it up and puts into the ring.\n\n---\n\nI agree, and you'd need something new in either case to add more\nmemory, and it doesn't need to be IORING_REGISTER_MEM_REGION\nspecifically.\n\n---\n\nUnregistering a buffer ring doesn't guarantee that there are no\ninflight requests that are still using buffers that came out of\nthe buffer ring. The fuse driver can wait/terminate its requests\nbefore unregisteration, but allow userspace issued IORING_OP_RECV\nto use this km buffer ring, and you'll need to somehow synchronise\nwith all other io_uring requests.\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-18",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "Author acknowledged that a fix is needed and promised to modify version 2 of the patch in response to reviewer feedback.\n\nAuthor clarified that kernel-managed buffer rings are intended for use with other io-uring requests, specifically to avoid per-i/o page pinning overhead costs.\n\nAuthor Joanne Koong addressed Pavel Begunkov's concern about the design of kernel-managed buffer rings, agreeing that having buffers owned by the ring and tied to its lifetime is more generically useful. She proposed a new design where the API would allow users to specify an offset into a registered memory region for normal requests, using a new IOSQE_ flag and looking up associated pages stored in the io_mapped_region's pages array.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged need for modification",
                "promised to revise",
                "clarification",
                "explanation",
                "proposed alternative design"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sorry, I submitted v2 last night thinking the conversation on this\nthread had died. After reading through your reply, I'll modify v2.\n\n---\n\nYes the buffer rings are intended to be used with other io-uring\nrequests. The ideal scenario is that the user can then do the\nequivalent of IORING_OP_READ/WRITE_FIXED operations on the\nkernel-managed buffers and avoid the per-i/o page pinning overhead\ncosts.\n\n---\n\nI agree 100%. The api we add should be what's best for io-uring, not fuse.\n\nFor the majority of use cases, it seemed to me that having the buffers\nseparated from the buffer rings didn't yield perceptible benefits but\nadded complexity and more restrictions like having to statically know\nup front how big the mem region needs to be across the lifetime of the\nio-uring for anything the io-uring might use the mem region for. It\nseems more generically useful as a concept to have the buffers owned\nby the ring and tied to the lifetime of the ring. I like how with this\ndesign everything is self-contained and multiple subsystems can use it\nwithout having to reimplement functionality locally in the subsystem.\nOn the other hand, I see your point about how it might be something\nusers want in the future if they want complete control over which\nparts of the mem region get used as the backing buffers to do stuff\nlike PMD optimizations.\n\nI think this is a matter of opinion/preference and I think in general\nfor anything io-uring related, yours should take precedence.\n\nWith it going through a mem region, I don't think it should even go\nthrough the \"pbuf ring\" interface then if it's not going to specify\nthe number of entries and buffer sizes upfront, if support is added\nfor io-uring normal requests (eg IORING_OP_READ/WRITE) to use the\nbacking pages from a memory region and if we're able to guarantee that\nthe registered memory region will never be able to be unregistered by\nthe user. I think if we repurpose the\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n};\n\nfields in the struct io_uring_sqe to\n\nunion {\n  __u64 addr; /* pointer to buffer or iovecs */\n  __u64 splice_off_in;\n  __u64 offset; /* offset into registered mem region */\n};\n\nand add some IOSQE_ flag to indicate it should find the pages from the\nregistered mem region, then that should work for normal requests.\nWhere on the kernel side, it looks up the associated pages stored in\nthe io_mapped_region's pages array for the offset passed in.\n\nRight now there's only a uapi to register a memory region and none to\nunregister one. Is it guaranteed that io-uring will never add\nsomething in the future that will let userspace unregister the memory\nregion or at least unregister it while it's being used (eg if we add\nfuture refcounting to it to track active uses of it)?\n\nIf so, then end-to-end, with it going through the mem region, it would\nbe something like:\n* user creates a mem region for the io-uring\n* user mmaps the mem region\n* user passes in offset into region, length of each buffer, and number\nof entries in the ring to the subsystem\n* subsystem creates a locally managed bufring and adds buffers to that\nring from the mem region\n* on the cqe side, it sends the buffer id of the registered mem region\nthrough the same \"IORING_CQE_F_BUFFER |  (buf_id <<\nIORING_CQE_BUFFER_SHIFT)\" mechanism\n\nDoes this design match what you had in mind / prefer?\n\nI think the above works for Christoph's use case too (as his and my\nuse case are the same) but if not, please let me know.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-18",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Pavel Begunkov questioned whether kernel-managed buffer rings should be exposed in the io_uring uapi, specifically asking if a server or user space program can issue I/O requests that consume buffers from the km ring without fuse kernel code involved. He requested clarification on this point to inform the decision about exposing km rings in the uapi.\n\nReviewer Pavel Begunkov suggested reusing registered buffers instead of introducing a new mechanism for kernel-managed buffer rings, citing efficiency and similarity to zero-copy internally registered buffers as benefits.\n\nReviewer noted that kernel-managed buffer rings would require handling page references and/or pinning regions when using registered buffers, suggesting an alternative approach.\n\nReviewer Pavel Begunkov suggested adding a liburing helper to handle mmap'ing for the fuse server, eliminating its need to directly manage memory mappings.\n\nreviewer expressed conditional approval, requesting confirmation that the patch enables fast path optimizations",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "clarification needed",
                "suggested alternative approach",
                "suggested improvement",
                "conditional approval",
                "request for confirmation"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "You mention OP_READ_FIXED and below agreed not exposing km rings\nan io_uring uapi, which makes me believe we're still talking about\ndifferent things.\n\nCorrect me if I'm wrong. Currently, only fuse cmds use the buffer\nring itself, I'm not talking about buffer, i.e. fuse cmds consume\nentries from the ring (!!! that's the part I'm interested in), then\nprocess them and tell the server \"this offset in the region has user\ndata to process or should be populated with data\".\n\nNaturally, the server should be able to use the buffers to issue\nsome I/O and process it in other ways, whether it's a normal\nOP_READ to which you pass the user space address (you can since\nit's mmap()'ed by the server) or something else is important but\na separate question than the one I'm trying to understand.\n\nSo I'm asking whether you expect that a server or other user space\nprogram should be able to issue a READ_OP_RECV, READ_OP_READ or any\nother similar request, which would consume buffers/entries from the\nkm ring without any fuse kernel code involved? Do you have some\nuse case for that in mind?\n\nUnderstanding that is the key in deciding whether km rings should\nbe exposed as io_uring uapi or not, regardless of where buffers\nto populate the ring come from.\n\n...\n\n---\n\nSo you already can do all that using the mmap()'ed region user\npointer, and you just want it to be more efficient, right?\nFor that let's just reuse registered buffers, we don't need a\nnew mechanism that needs to be propagated to all request types.\nAnd registered buffer are already optimised for I/O in a bunch\nof ways. And as a bonus, it'll be similar to the zero-copy\ninternally registered buffers if you still plan to add them.\n\nThe simplest way to do that is to create a registered buffer out\nof the mmap'ed region pointer. Pseudo code:\n\n// mmap'ed if it's kernel allocated.\n{region_ptr, region_size} = create_region();\n\nstruct iovec iov;\niov.iov_base = region_ptr;\niov.iov_len = region_size;\nio_uring_register_buffers(ring, &iov, 1);\n\n// later instead of this:\nptr = region_ptr + off;\nio_uring_prep_read(sqe, fd, ptr, ...);\n\n// you use registered buffers as usual:\nio_uring_prep_read_fixed(sqe, fd, off, regbuf_idx, ...);\n\n\nIIRC the registration would fail because it doesn't allow file\nbacked pages, but it should be fine if we know it's io_uring\nregion memory, so that would need to be patched.\n\nThere might be a bunch of other ways you can do that like\ncreate a kernel allocated registered buffer like what Cristoph\nwants, and then register it as a region. Or allow creating\nregistered buffers out of a region. etc.\n\nI wanted to unify registered buffers and regions internally\nat some point, but then drifted away from active io_uring core\ninfrastructure development, so I guess that could've been useful.\n\n---\n\nLet's talk about it when it's needed or something changes, but if\nyou do registered buffers instead as per above, they'll be holding\npage references and or have to pin the region in some other way.\n\n---\n\nFWIW, we should just add a liburing helper, so that fuse server\ndoesn't need to deal with mmap'ing.\n\n---\n\nThat's sounds clean to me _if_ it allows you to achieve all\n(fast path) optimisations you want to have. I hope it does?\n\n-- \nPavel Begunkov",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joanne Koong (author)",
              "summary": "The author is addressing a question from Pavel Begunkov about whether kernel-managed buffer rings are fuse-specific, and they acknowledge that while it's useful for fuse servers with certain workloads, the concept isn't exclusive to fuse and other subsystems/users could also benefit from this optimization.\n\nThe author is addressing concerns about the added complexity and convoluted interface introduced by kernel-managed buffer rings, specifically questioning the need for tying together different concepts and the benefits of this approach.\n\nAuthor addressed Pavel's concern that kernel-managed buffer rings require the caller to register memory regions as fixed buffers, explaining that this cannot be guaranteed and proposing two possible solutions: adding pinning to registered memory regions or introducing extra overhead for every I/O operation.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a question",
                "explained their reasoning",
                "questioning the design",
                "expressing confusion",
                "acknowledges a fix is needed",
                "proposes alternative solutions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Thanks for clarifying your question. Yes, this would be a useful\noptimization in the future for fuse servers with certain workload\ncharacteristics (eg network-backed servers with high concurrency and\nunpredictable latencies). I don't think the concept of kmbufrings is\nexclusively fuse-specific though (for example, Christoph's use case\nbeing a recent instance); I think other subsystems/users that'll use\nkmbuf rings would also generically find it useful to have the option\nof READ_OP_RECV/READ_OP_READ operating directly on the ring.\n\n---\n\nI feel like this design makes the interface more convoluted and now\nmuddies different concepts together by adding new complexity /\nrelationships between them whereas they were otherwise cleanly\nisolated. Maybe I'm just not seeing/understanding the overarching\nvision for why conceptually it makes sense for them to be tied\ntogether besides as a mechanism to tell io-uring requests where to\ncopy from by reusing what exists for fixed buffer ids. There's more\ncomplexity now on the kernel side (eg having to detect if the buffer\npassed in is kernel-allocated to know whether to pin the pages /\ncharge it against the user's RLIMIT_MEMLOCK limit) but I'm not\nunderstanding what we gain from it. I got the sense from your previous\ncomments that memory regions are the de facto way to go and should be\ndecoupled from other structures, so if that's the case, why doesn't it\nmake sense for io-uring to add native support for using memory regions\nfor io-uring requests? I feel like from the userspace side it makes\nthings more confusing with this extra layer of indirection that now\nhas to go through a fixed buffer.\n\n---\n\nI don't think we can guarantee that the caller will register the\nmemory region as a fixed buffer (eg if it doesn't need/want to use the\nbuffer for normal io-uring requests). On the kernel side, the internal\nbuffer entry uses the kaddr of the registered memory region buffer for\nany memcpys. If it's not guaranteed that registered memory regions\npersist for the lifetime of the ring, there'll have to be extra\noverhead for every I/O (eg grab the io-uring lock, checking if the mem\nregion is still registered, grab a refcount to that mem region, unlock\nthe ring, do the memcpy to the kaddr, then grab the io-uring lock\nagain, decrement the refcount, and unlock). Or I guess we could add\npinning to a registered memory region.\n\nThanks,\nJoanne",
              "reply_to": "Pavel Begunkov",
              "message_date": "2026-02-20",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Pavel Begunkov",
              "summary": "Reviewer Pavel Begunkov expressed confusion about the relevance of kernel-managed buffer rings to a previous request from Christoph, and asked for clarification on why this feature is necessary.\n\nReviewer Pavel Begunkov questioned the exposure of internal kernel fuse API as an io_uring uapi, suggesting that it may have been discussed previously but was not clear from the patchset\n\nReviewer Pavel Begunkov noted that the patch introduces an additional way to pass buffers through io_uring, which could make the I/O path more complex and suggested reusing an existing uapi instead of creating a new one.\n\nReviewer Pavel Begunkov suggested using registered buffers instead of copying client's data into user space, citing that it would be a better abstraction; he also mentioned that he was following the main I/O path and trying to make the setup path more flexible and reusable.\n\nReviewer Pavel Begunkov expressed skepticism about the necessity of introducing a new interface for kernel-managed buffer rings, citing an existing interface as sufficient for efficient user-space implementation.\n\nReviewer Pavel Begunkov noted that the patch does not provide a clear mechanism for the fuse server to use kernel-managed buffer rings when performing I/O operations, instead requiring the user to either use OP_READ/etc. with user addresses from mmap()ing regions or registering and using OP_READ_FIXED.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "confusion",
                "lack of understanding",
                "uncertainty",
                "lack of clarity",
                "requested changes",
                "suggested reuse of existing uapi",
                "suggested alternative design",
                "skepticism",
                "necessity",
                "lack of clear mechanism for fuse server",
                "requirement for user to use specific I/O operations"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Sorry, I don't see relevance b/w km rings and what Christoph wants.\nI explained why in some sub-thread, but maybe someone can tell\nwhat I'm missing.\n\n---\n\nYep, it could be, potentially, it's just the patchset doesn't plumb\nit to other requests and uses it within fuse. It's just cases like\nthat always make me wonder, here it was why what is basically an\ninternal kernel fuse API is exposed as an io_uring uapi. Maybe there\nwas a discussion about it I missed?\n\n---\n\nThat would avoid doing a large revamp of uapi and plumbing it\nto each every request type when there is already a uapi that does\nwhat you want, does it well and have lots of things figured out.\nKeeping the I/O path sane is important, io_uring already has 3\ndifferent ways of passing buffers, let's not add a 4th one\nunless it achieves something meaningful.\n\n---\n\nSorry, maybe I wasn't clear. With what I see you're trying to do,\ni.e. copying client's data into user space (server), I think\nregistered buffers would be a better abstraction. However, I just\nwent with your design on top of regions, since it's not the first\niteration of the series and I wasn't following previous ones, and\nIIRC you was already using registered buffers in previous revisions\nbut moved from that for some reason. IOW, I was taking you main I/O\npath and was trying to make the setup path a bit more flexible and\nreusable.\n\n---\n\nThere is a high bar for adding a new interface for passing buffers\nthat needs to be propagated to a good number of request handlers,\nand there is already one that gives you all you need to write\nefficient user space.\n\n---\n\nIt's up to the user (i.e. fuse server) to either use OP_READ/etc. using\nuser addresses that you have in your design from mmap()ing regions, or\nregistering it and using OP_READ_FIXED.",
              "reply_to": "Joanne Koong",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Johannes Weiner",
      "primary_email": "hannes@cmpxchg.org",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joshua Hahn",
      "primary_email": "joshua.hahnjy@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "JP Kobryn",
      "primary_email": "inwardvessel@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Kiryl Shutsemau",
      "primary_email": "kas@kernel.org",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Leo Martins",
      "primary_email": "loemra.dev@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Mark Harmstone",
      "primary_email": "mark@harmstone.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH] btrfs: fix chunk offset error message in check_dev_extent_item()",
          "message_id": "20260220113013.30254-1-mark@harmstone.com",
          "url": "https://lore.kernel.org/all/20260220113013.30254-1-mark@harmstone.com/",
          "date": "2026-02-20T11:30:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-20",
          "patch_summary": "This patch fixes a bug in the Btrfs tree-checker where an error message incorrectly reports the offset of a dev extent chunk, instead printing its object ID.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Qu Wenruo",
              "summary": "Approved the patch with a Reviewed-by tag, noting that it fixes a bug in the error message of check_dev_extent_item().",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "APPROVED"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "\n\n 2026/2/20 22:00, Mark Harmstone :\n> Fix a copy-paste bug in an error message in check_dev_extent_item():\n> we're reporting an incorrect offset, but actually printing the objectid.\n> \n> Signed-off-by: Mark Harmstone <mark@harmstone.com>\n> Fixes: 008e2512dc56 (\"btrfs: tree-checker: add dev extent item checks\")\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\n\nThanks,\nQu\n\n> ---\n>   fs/btrfs/tree-checker.c | 2 +-\n>   1 file changed, 1 insertion(+), 1 deletion(-)\n> \n> diff --git a/fs/btrfs/tree-checker.c b/fs/btrfs/tree-checker.c\n> index ac4c4573ee39..133510f99fc5 100644\n> --- a/fs/btrfs/tree-checker.c\n> +++ b/fs/btrfs/tree-checker.c\n> @@ -1899,7 +1899,7 @@ static int check_dev_extent_item(const struct extent_buffer *leaf,\n>   \t\t\t\t sectorsize))) {\n>   \t\tgeneric_err(leaf, slot,\n>   \t\t\t    \"invalid dev extent chunk offset, has %llu not aligned to %u\",\n> -\t\t\t    btrfs_dev_extent_chunk_objectid(leaf, de),\n> +\t\t\t    btrfs_dev_extent_chunk_offset(leaf, de),\n>   \t\t\t    sectorsize);\n>   \t\treturn -EUCLEAN;\n>   \t}\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-21",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Nhat Pham",
      "primary_email": "nphamcs@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Rik van Riel",
      "primary_email": "riel@surriel.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Shakeel Butt",
      "primary_email": "shakeel.butt@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH v2 0/5] mm/swap, memcg: Introduce swap tiers for cgroup based swap control",
          "message_id": "20260221163043.GA35350@shakeel.butt@linux.dev",
          "url": "https://lore.kernel.org/all/20260221163043.GA35350@shakeel.butt@linux.dev/",
          "date": "2026-02-21T17:44:14Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about the interaction between cgroup hierarchy and swap tier configuration, explaining that effective tiers are calculated separately using a dedicated mask to respect the cgroup hierarchy.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch integrates the swap tier infrastructure with cgroup,\nenabling the selection of specific swap devices per cgroup by\nconfiguring allowed swap tiers.\n\nThe new `memory.swap.tiers` interface controls allowed swap tiers via a mask.\nBy default, the mask is set to include all tiers, allowing specific tiers to\nbe excluded or restored. Note that effective tiers are calculated separately\nusing a dedicated mask to respect the cgroup hierarchy. Consequently,\nconfigured tiers may differ from effective ones, as they must be a subset\nof the parent's.\n\nNote that cgroups do not pin swap tiers. This is similar to the\n`cpuset` controller, which does not prevent CPU hotplug. This\napproach ensures flexibility by allowing tier configuration changes\nregardless of cgroup usage.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n Documentation/admin-guide/cgroup-v2.rst | 27 +++++++++\n include/linux/memcontrol.h              |  3 +-\n mm/memcontrol.c                         | 80 +++++++++++++++++++++++++\n mm/swap_tier.c                          | 66 ++++++++++++++++++++\n mm/swap_tier.h                          | 21 +++++++\n mm/swapfile.c                           |  5 ++\n 6 files changed, 201 insertions(+), 1 deletion(-)\n\ndiff --git a/Documentation/admin-guide/cgroup-v2.rst b/Documentation/admin-guide/cgroup-v2.rst\nindex 7f5b59d95fce..776a908ce1b9 100644\n--- a/Documentation/admin-guide/cgroup-v2.rst\n+++ b/Documentation/admin-guide/cgroup-v2.rst\n@@ -1848,6 +1848,33 @@ The following nested keys are defined.\n \tSwap usage hard limit.  If a cgroup's swap usage reaches this\n \tlimit, anonymous memory of the cgroup will not be swapped out.\n \n+  memory.swap.tiers\n+        A read-write nested-keyed file which exists on non-root\n+        cgroups. The default is to enable all tiers.\n+\n+        This interface allows selecting which swap tiers a cgroup can\n+        use for swapping out memory.\n+\n+        The effective tiers are inherited from the parent. Only tiers\n+        effective in the parent can be effective in the child. However,\n+        the child can explicitly disable tiers allowed by the parent.\n+\n+        When read, the file shows two lines:\n+          - The first line shows the operation string that was\n+            written to this file.\n+          - The second line shows the effective operation after\n+            merging with parent settings.\n+\n+        When writing, the format is:\n+          (+/-)(TIER_NAME) (+/-)(TIER_NAME) ...\n+\n+        Valid tier names are those configured in\n+        /sys/kernel/mm/swap/tiers.\n+\n+        Each tier can be prefixed with:\n+          +    Enable this tier\n+          -    Disable this tier\n+\n   memory.swap.events\n \tA read-only flat-keyed file which exists on non-root cgroups.\n \tThe following entries are defined.  Unless specified\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex b6c82c8f73e1..542bee1b5f60 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -283,7 +283,8 @@ struct mem_cgroup {\n \t/* per-memcg mm_struct list */\n \tstruct lru_gen_mm_list mm_list;\n #endif\n-\n+\tint tier_mask;\n+\tint tier_effective_mask;\n #ifdef CONFIG_MEMCG_V1\n \t/* Legacy consumer-oriented counters */\n \tstruct page_counter kmem;\t\t/* v1 only */\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 007413a53b45..c0a0a957a630 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -68,6 +68,7 @@\n #include <net/ip.h>\n #include \"slab.h\"\n #include \"memcontrol-v1.h\"\n+#include \"swap_tier.h\"\n \n #include <linux/uaccess.h>\n \n@@ -3691,6 +3692,7 @@ static void mem_cgroup_free(struct mem_cgroup *memcg)\n {\n \tlru_gen_exit_memcg(memcg);\n \tmemcg_wb_domain_exit(memcg);\n+\tswap_tiers_memcg_sync_mask(memcg);\n \t__mem_cgroup_free(memcg);\n }\n \n@@ -3792,6 +3794,9 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \tWRITE_ONCE(memcg->zswap_writeback, true);\n #endif\n \tpage_counter_set_high(&memcg->swap, PAGE_COUNTER_MAX);\n+\tmemcg->tier_mask = TIER_ALL_MASK;\n+\tswap_tiers_memcg_inherit_mask(memcg, parent);\n+\n \tif (parent) {\n \t\tWRITE_ONCE(memcg->swappiness, mem_cgroup_swappiness(parent));\n \n@@ -5352,6 +5357,75 @@ static int swap_events_show(struct seq_file *m, void *v)\n \treturn 0;\n }\n \n+static int swap_tier_show(struct seq_file *m, void *v)\n+{\n+\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);\n+\n+\tswap_tiers_mask_show(m, memcg->tier_mask);\n+\tswap_tiers_mask_show(m, memcg->tier_effective_mask);\n+\n+\treturn 0;\n+}\n+\n+static ssize_t swap_tier_write(struct kernfs_open_file *of,\n+\t\t\t\tchar *buf, size_t nbytes, loff_t off)\n+{\n+\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n+\tchar *pos, *token;\n+\tint ret = 0;\n+\n+\tpos = strstrip(buf);\n+\n+\tspin_lock(&swap_tier_lock);\n+\tif (!*pos) {\n+\t\tmemcg->tier_mask = TIER_ALL_MASK;\n+\t\tgoto sync;\n+\t}\n+\n+\twhile ((token = strsep(&pos, \" \\t\\n\")) != NULL) {\n+\t\tint mask;\n+\n+\t\tif (!*token)\n+\t\t\tcontinue;\n+\n+\t\tif (token[0] != '-' && token[0] != '+') {\n+\t\t\tret = -EINVAL;\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\tmask = swap_tiers_mask_lookup(token+1);\n+\t\tif (!mask) {\n+\t\t\tret = -EINVAL;\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\t/*\n+\t\t * if child already set, cannot add that tiers for hierarch mismatching.\n+\t\t * parent compatible, child must respect parent selected swap device.\n+\t\t */\n+\t\tswitch (token[0]) {\n+\t\tcase '-':\n+\t\t\tmemcg->tier_mask &= ~mask;\n+\t\t\tbreak;\n+\t\tcase '+':\n+\t\t\tmemcg->tier_mask |= mask;\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tret = -EINVAL;\n+\t\t\tbreak;\n+\t\t}\n+\n+\t\tif (ret)\n+\t\t\tgoto err;\n+\t}\n+\n+sync:\n+\t__swap_tiers_memcg_sync_mask(memcg);\n+err:\n+\tspin_unlock(&swap_tier_lock);\n+\treturn ret ? ret : nbytes;\n+}\n+\n static struct cftype swap_files[] = {\n \t{\n \t\t.name = \"swap.current\",\n@@ -5384,6 +5458,12 @@ static struct cftype swap_files[] = {\n \t\t.file_offset = offsetof(struct mem_cgroup, swap_events_file),\n \t\t.seq_show = swap_events_show,\n \t},\n+\t{\n+\t\t.name = \"swap.tiers\",\n+\t\t.flags = CFTYPE_NOT_ON_ROOT,\n+\t\t.seq_show = swap_tier_show,\n+\t\t.write = swap_tier_write,\n+\t},\n \t{ }\t/* terminate */\n };\n \ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nindex d90f6eccb908..e860c87292e2 100644\n--- a/mm/swap_tier.c\n+++ b/mm/swap_tier.c\n@@ -384,3 +384,69 @@ bool swap_tiers_update(void)\n \n \treturn true;\n }\n+\n+void swap_tiers_mask_show(struct seq_file *m, int mask)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tspin_lock(&swap_tier_lock);\n+\tfor_each_active_tier(tier) {\n+\t\tif (mask & TIER_MASK(tier))\n+\t\t\tseq_printf(m, \"%s \", tier->name);\n+\t}\n+\tspin_unlock(&swap_tier_lock);\n+\tseq_puts(m, \"\\n\");\n+}\n+\n+int swap_tiers_mask_lookup(const char *name)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (!strcmp(name, tier->name))\n+\t\t\treturn TIER_MASK(tier);\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static void __swap_tier_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent)\n+{\n+\tint effective_mask\n+\t\t= parent ? parent->tier_effective_mask : TIER_ALL_MASK;\n+\n+\tmemcg->tier_effective_mask\n+\t\t= effective_mask & memcg->tier_mask;\n+}\n+\n+void swap_tiers_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent)\n+{\n+\tspin_lock(&swap_tier_lock);\n+\t__swap_tier_memcg_inherit_mask(memcg, parent);\n+\tspin_unlock(&swap_tier_lock);\n+}\n+\n+void __swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg)\n+{\n+\tstruct mem_cgroup *child;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tif (memcg == root_mem_cgroup)\n+\t\treturn;\n+\n+\tfor_each_mem_cgroup_tree(child, memcg)\n+\t\t__swap_tier_memcg_inherit_mask(child, parent_mem_cgroup(child));\n+}\n+\n+void swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg)\n+{\n+\tspin_lock(&swap_tier_lock);\n+\tmemcg->tier_mask = TIER_ALL_MASK;\n+\t__swap_tiers_memcg_sync_mask(memcg);\n+\tspin_unlock(&swap_tier_lock);\n+}\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nindex de81d540e3b5..8652a7f993ab 100644\n--- a/mm/swap_tier.h\n+++ b/mm/swap_tier.h\n@@ -46,4 +46,25 @@ bool swap_tiers_update(void);\n /* Tier assignment */\n void swap_tiers_assign_dev(struct swap_info_struct *swp);\n \n+/* Memcg related functions */\n+void swap_tiers_mask_show(struct seq_file *m, int mask);\n+void swap_tiers_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent);\n+void swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg);\n+void __swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg);\n+\n+/* Mask and tier lookup */\n+int swap_tiers_mask_lookup(const char *name);\n+\n+/**\n+ * swap_tiers_mask_test - Check if the tier mask is valid\n+ * @tier_mask: The tier mask to check\n+ * @mask: The mask to compare against\n+ *\n+ * Return: true if condition matches, false otherwise\n+ */\n+static inline bool swap_tiers_mask_test(int tier_mask, int mask)\n+{\n+\treturn tier_mask & mask;\n+}\n #endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 4f8ce021c5bd..dd97e850ea2c 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1348,10 +1348,15 @@ static bool swap_alloc_fast(struct folio *folio)\n static void swap_alloc_slow(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n+\tint mask = folio_memcg(folio) ?\n+\t\tfolio_memcg(folio)->tier_effective_mask : TIER_ALL_MASK;\n \n \tspin_lock(&swap_avail_lock);\n start_over:\n \tplist_for_each_entry_safe(si, next, &swap_avail_head, avail_list) {\n+\t\tif (!swap_tiers_mask_test(si->tier_mask, mask))\n+\t\t\tcontinue;\n+\n \t\t/* Rotate the device and switch to a new cluster */\n \t\tplist_requeue(&si->avail_list, &swap_avail_head);\n \t\tspin_unlock(&swap_avail_lock);\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "message_id": "20260126065242.1221862-4-youngjun.park@lge.com",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about swap device tier consistency, explaining that a reference is held from the start of the tier to the priority of the active swap device to prevent the tier from disappearing. The author also clarified how tiers are split or merged when adding new tiers with higher priorities.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch connects swap devices to the swap tier infrastructure,\nensuring that devices are correctly assigned to tiers based on their\npriority.\n\nA `tier_mask` is added to identify the tier membership of swap devices.\nAlthough tier-based allocation logic is not yet implemented, this\nmapping is necessary to track which tier a device belongs to. Upon\nactivation, the device is assigned to a tier by matching its priority\nagainst the configured tier ranges.\n\nThe infrastructure allows dynamic modification of tiers, such as\nsplitting or merging ranges. These operations are permitted provided\nthat the tier assignment of already configured swap devices remains\nunchanged.\n\nThis patch also adds the documentation for the swap tier feature,\ncovering the core concepts, sysfs interface usage, and configuration\ndetails.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n Documentation/mm/swap-tier.rst | 109 +++++++++++++++++++++++++++++++++\n include/linux/swap.h           |   1 +\n mm/swap_state.c                |   2 +-\n mm/swap_tier.c                 | 106 ++++++++++++++++++++++++++++----\n mm/swap_tier.h                 |  13 +++-\n mm/swapfile.c                  |   2 +\n 6 files changed, 219 insertions(+), 14 deletions(-)\n create mode 100644 Documentation/mm/swap-tier.rst\n\ndiff --git a/Documentation/mm/swap-tier.rst b/Documentation/mm/swap-tier.rst\nnew file mode 100644\nindex 000000000000..3386161b9b18\n--- /dev/null\n+++ b/Documentation/mm/swap-tier.rst\n@@ -0,0 +1,109 @@\n+.. SPDX-License-Identifier: GPL-2.0\n+\n+:Author: Chris Li <chrisl@kernel.org> Youngjun Park <youngjun.park@lge.com>\n+\n+==========\n+Swap Tier\n+==========\n+\n+Swap tier is a collection of user-named groups classified by priority ranges.\n+It acts as a facilitation layer, allowing users to manage swap devices based\n+on their speeds.\n+\n+Users are encouraged to assign swap device priorities according to device\n+speed to fully utilize this feature. While the current implementation is\n+integrated with cgroups, the concept is designed to be extensible for other\n+subsystems in the future.\n+\n+Use case\n+-------\n+\n+Users can perform selective swapping by choosing a swap tier assigned according\n+to speed within a cgroup.\n+\n+For more information on cgroup v2, please refer to\n+``Documentation/admin-guide/cgroup-v2.rst``.\n+\n+Priority Range\n+--------------\n+\n+The specified tiers must cover the entire priority range from -1\n+(DEF_SWAP_PRIO) to SHRT_MAX.\n+\n+Consistency\n+-----------\n+\n+Tier consistency is guaranteed with a focus on maximizing flexibility. When a\n+swap device is activated within a tier range, a reference is held from the\n+start of the tier to the priority of that swap device. This ensures that the\n+tier of region containing the active swap device does not disappear.\n+\n+If a request to add a new tier with a priority higher than the current swap\n+device is received, the existing tier can be split.\n+\n+However, specifying a tier in a cgroup does not hold a reference to the tier.\n+Consequently, the corresponding tier can disappear at any time.\n+\n+Configuration Interface\n+-----------------------\n+\n+The swap tiers can be configured via the following interface:\n+\n+/sys/kernel/mm/swap/tiers\n+\n+Operations can be performed using the following syntax:\n+\n+* Add:    ``+\"<tiername>\":\"<start_priority>\"``\n+* Remove: ``-\"<tiername>\"``\n+* Modify: ``\"<tiername>\":\"<start_priority>\"``\n+\n+Multiple operations can be provided in a single write, separated by spaces (\" \")\n+or commas (\",\").\n+\n+When configuring tiers, the specified value represents the **start priority**\n+of that tier. The end priority is automatically determined by the start\n+priority of the next higher tier. Consequently, adding or modifying a tier\n+automatically adjusts (splits or merges) the ranges of adjacent tiers to\n+ensure continuity.\n+\n+Examples\n+--------\n+\n+**1. Initialization**\n+\n+A tier starting at -1 is mandatory to cover the entire priority range up to\n+SHRT_MAX. In this example, 'HDD' starts at 50, and 'NET' covers the remaining\n+lower range starting from -1.\n+\n+::\n+\n+    # echo \"+HDD:50, +NET:-1\" > /sys/kernel/mm/swap/tiers\n+    # cat /sys/kernel/mm/swap/tiers\n+    Name             Idx   PrioStart   PrioEnd\n+    HDD              0     50          32767\n+    NET              1     -1          49\n+\n+**2. Modification and Splitting**\n+\n+Here, 'HDD' is moved to start at 80, and a new tier 'SSD' is added at 100.\n+Notice how the ranges are automatically recalculated:\n+* 'SSD' takes the top range. Split HDD Tier's range. (100 to SHRT_MAX).\n+* 'HDD' is adjusted to the range between 'NET' and 'SSD' (80 to 99).\n+* 'NET' automatically extends to fill the gap below 'HDD' (-1 to 79).\n+\n+::\n+\n+    # echo \"HDD:80, +SSD:100\" > /sys/kernel/mm/swap/tiers\n+    # cat /sys/kernel/mm/swap/tiers\n+    Name             Idx   PrioStart   PrioEnd\n+    SSD              2     100         32767\n+    HDD              0     80          99\n+    NET              1     -1          79\n+\n+**3. Removal**\n+\n+Tiers can be removed using the '-' prefix.\n+\n+::\n+\n+    # echo \"-SSD,-HDD,-NET\" > /sys/kernel/mm/swap/tiers\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 62fc7499b408..1e68c220a0e7 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -262,6 +262,7 @@ struct swap_info_struct {\n \tstruct percpu_ref users;\t/* indicate and keep swap device valid. */\n \tunsigned long\tflags;\t\t/* SWP_USED etc: see above */\n \tsigned short\tprio;\t\t/* swap priority of this type */\n+\tint tier_mask;\t\t\t/* swap tier mask */\n \tstruct plist_node list;\t\t/* entry in swap_active_head */\n \tsigned char\ttype;\t\t/* strange name for an index */\n \tunsigned int\tmax;\t\t/* extent of the swap_map */\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex f1a7d9cdc648..d46ca61d2e42 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -997,7 +997,7 @@ static ssize_t tiers_store(struct kobject *kobj,\n \t\t\tgoto restore;\n \t}\n \n-\tif (!swap_tiers_validate()) {\n+\tif (!swap_tiers_update()) {\n \t\tret = -EINVAL;\n \t\tgoto restore;\n \t}\ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nindex 87882272eec8..d90f6eccb908 100644\n--- a/mm/swap_tier.c\n+++ b/mm/swap_tier.c\n@@ -14,7 +14,7 @@\n  * @name: name of the swap_tier.\n  * @prio: starting value of priority.\n  * @list: linked list of tiers.\n-*/\n+ */\n static struct swap_tier {\n \tchar name[MAX_TIERNAME];\n \tshort prio;\n@@ -34,6 +34,8 @@ static LIST_HEAD(swap_tier_inactive_list);\n \t(!list_is_first(&(tier)->list, &swap_tier_active_list) ? \\\n \tlist_prev_entry((tier), list)->prio - 1 : SHRT_MAX)\n \n+#define MASK_TO_TIER(mask) (&swap_tiers[__ffs((mask))])\n+\n #define for_each_tier(tier, idx) \\\n \tfor (idx = 0, tier = &swap_tiers[0]; idx < MAX_SWAPTIER; \\\n \t\tidx++, tier = &swap_tiers[idx])\n@@ -55,6 +57,26 @@ static bool swap_tier_is_active(void)\n \treturn !list_empty(&swap_tier_active_list) ? true : false;\n }\n \n+static bool swap_tier_prio_in_range(struct swap_tier *tier, short prio)\n+{\n+\tif (tier->prio <= prio && TIER_END_PRIO(tier) >= prio)\n+\t\treturn true;\n+\n+\treturn false;\n+}\n+\n+static bool swap_tier_prio_is_used(struct swap_tier *self, short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (tier != self && tier->prio == prio)\n+\t\t\treturn true;\n+\t}\n+\n+\treturn false;\n+}\n+\n static struct swap_tier *swap_tier_lookup(const char *name)\n {\n \tstruct swap_tier *tier;\n@@ -67,12 +89,14 @@ static struct swap_tier *swap_tier_lookup(const char *name)\n \treturn NULL;\n }\n \n+\n void swap_tiers_init(void)\n {\n \tstruct swap_tier *tier;\n \tint idx;\n \n \tBUILD_BUG_ON(BITS_PER_TYPE(int) < MAX_SWAPTIER);\n+\tBUILD_BUG_ON(MAX_SWAPTIER > TIER_DEFAULT_IDX);\n \n \tfor_each_tier(tier, idx) {\n \t\tINIT_LIST_HEAD(&tier->list);\n@@ -145,17 +169,35 @@ static struct swap_tier *swap_tier_prepare(const char *name, short prio)\n \treturn tier;\n }\n \n-static int swap_tier_check_range(short prio)\n+static int swap_tier_can_split_range(struct swap_tier *orig_tier,\n+\tshort new_prio)\n {\n+\tstruct swap_info_struct *p;\n \tstruct swap_tier *tier;\n \n \tlockdep_assert_held(&swap_lock);\n \tlockdep_assert_held(&swap_tier_lock);\n \n-\tfor_each_active_tier(tier) {\n-\t\t/* No overwrite */\n-\t\tif (tier->prio == prio)\n-\t\t\treturn -EINVAL;\n+\tplist_for_each_entry(p, &swap_active_head, list) {\n+\t\tif (p->tier_mask == TIER_DEFAULT_MASK)\n+\t\t\tcontinue;\n+\n+\t\ttier = MASK_TO_TIER(p->tier_mask);\n+\t\tif (tier->prio > new_prio)\n+\t\t\tcontinue;\n+\t\t/*\n+                 * Prohibit implicit tier reassignment.\n+                 * Case 1: Prevent orig_tier devices from dropping out\n+                 *         of the new range.\n+                 */\n+\t\tif (orig_tier == tier && (p->prio < new_prio))\n+\t\t\treturn -EBUSY;\n+                /*\n+                 * Case 2: Prevent other tier devices from entering\n+                 *         the new range.\n+                 */\n+\t\telse if (orig_tier != tier && (p->prio >= new_prio))\n+\t\t\treturn -EBUSY;\n \t}\n \n \treturn 0;\n@@ -173,7 +215,10 @@ int swap_tiers_add(const char *name, int prio)\n \tif (swap_tier_lookup(name))\n \t\treturn -EPERM;\n \n-\tret = swap_tier_check_range(prio);\n+\tif (swap_tier_prio_is_used(NULL, prio))\n+\t\treturn -EBUSY;\n+\n+\tret = swap_tier_can_split_range(NULL, prio);\n \tif (ret)\n \t\treturn ret;\n \n@@ -183,7 +228,6 @@ int swap_tiers_add(const char *name, int prio)\n \t\treturn ret;\n \t}\n \n-\n \tswap_tier_insert_by_prio(tier);\n \treturn ret;\n }\n@@ -200,11 +244,18 @@ int swap_tiers_remove(const char *name)\n \tif (!tier)\n \t\treturn -EINVAL;\n \n+\t/* Simulate adding a tier to check for conflicts */\n+\tret = swap_tier_can_split_range(NULL, tier->prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n \tlist_move(&tier->list, &swap_tier_inactive_list);\n \n \t/* Removing DEF_SWAP_PRIO merges into the higher tier. */\n-\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO)\n-\t\tlist_prev_entry(tier, list)->prio = DEF_SWAP_PRIO;\n+\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO) {\n+\t\tlist_last_entry(&swap_tier_active_list, struct swap_tier, list)\n+\t\t\t->prio = DEF_SWAP_PRIO;\n+\t}\n \n \treturn ret;\n }\n@@ -225,7 +276,10 @@ int swap_tiers_modify(const char *name, int prio)\n \tif (tier->prio == prio)\n \t\treturn 0;\n \n-\tret = swap_tier_check_range(prio);\n+\tif (swap_tier_prio_is_used(tier, prio))\n+\t\treturn -EBUSY;\n+\n+\tret = swap_tier_can_split_range(tier, prio);\n \tif (ret)\n \t\treturn ret;\n \n@@ -283,10 +337,27 @@ void swap_tiers_restore(struct swap_tier_save_ctx ctx[])\n \t}\n }\n \n-bool swap_tiers_validate(void)\n+void swap_tiers_assign_dev(struct swap_info_struct *swp)\n {\n \tstruct swap_tier *tier;\n \n+\tlockdep_assert_held(&swap_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (swap_tier_prio_in_range(tier, swp->prio)) {\n+\t\t\tswp->tier_mask = TIER_MASK(tier);\n+\t\t\treturn;\n+\t\t}\n+\t}\n+\n+\tswp->tier_mask = TIER_DEFAULT_MASK;\n+}\n+\n+bool swap_tiers_update(void)\n+{\n+\tstruct swap_tier *tier;\n+\tstruct swap_info_struct *swp;\n+\n \t/*\n \t * Initial setting might not cover DEF_SWAP_PRIO.\n \t * Swap tier must cover the full range (DEF_SWAP_PRIO to SHRT_MAX).\n@@ -300,5 +371,16 @@ bool swap_tiers_validate(void)\n \t\t\treturn false;\n \t}\n \n+\t/*\n+\t * If applied initially, the swap tier_mask may change\n+\t * from the default value.\n+\t */\n+\tplist_for_each_entry(swp, &swap_active_head, list) {\n+\t\t/* Tier is already configured */\n+\t\tif (swp->tier_mask != TIER_DEFAULT_MASK)\n+\t\t\tbreak;\n+\t\tswap_tiers_assign_dev(swp);\n+\t}\n+\n \treturn true;\n }\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nindex 4b1b0602d691..de81d540e3b5 100644\n--- a/mm/swap_tier.h\n+++ b/mm/swap_tier.h\n@@ -14,6 +14,9 @@\n #define MAX_SWAPTIER\t\t8\n #endif\n \n+/* Forward declarations */\n+struct swap_info_struct;\n+\n extern spinlock_t swap_tier_lock;\n \n struct swap_tier_save_ctx {\n@@ -24,6 +27,10 @@ struct swap_tier_save_ctx {\n #define DEFINE_SWAP_TIER_SAVE_CTX(_name) \\\n \tstruct swap_tier_save_ctx _name[MAX_SWAPTIER] = {0}\n \n+#define TIER_ALL_MASK\t\t(~0)\n+#define TIER_DEFAULT_IDX\t(31)\n+#define TIER_DEFAULT_MASK\t(1 << TIER_DEFAULT_IDX)\n+\n /* Initialization and application */\n void swap_tiers_init(void);\n ssize_t swap_tiers_sysfs_show(char *buf);\n@@ -34,5 +41,9 @@ int swap_tiers_modify(const char *name, int prio);\n \n void swap_tiers_save(struct swap_tier_save_ctx ctx[]);\n void swap_tiers_restore(struct swap_tier_save_ctx ctx[]);\n-bool swap_tiers_validate(void);\n+bool swap_tiers_update(void);\n+\n+/* Tier assignment */\n+void swap_tiers_assign_dev(struct swap_info_struct *swp);\n+\n #endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex c27952b41d4f..4f8ce021c5bd 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -2672,6 +2672,8 @@ static void _enable_swap_info(struct swap_info_struct *si)\n \n \t/* Add back to available list */\n \tadd_to_avail_list(si, true);\n+\n+\tswap_tiers_assign_dev(si);\n }\n \n static void enable_swap_info(struct swap_info_struct *si, int prio,\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "message_id": "20260126065242.1221862-3-youngjun.park@lge.com",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about the potential for silent tier disappearance when using '-' to exclude specific tiers, explaining that this is intentional and follows cgroup hierarchy principles.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch introduces the \"Swap tier\" concept, which serves as an\nabstraction layer for managing swap devices based on their performance\ncharacteristics (e.g., NVMe, HDD, Network swap).\n\nSwap tiers are user-named groups representing priority ranges.\nThese tiers collectively cover the entire priority\nspace from -1 (`DEF_SWAP_PRIO`) to `SHRT_MAX`.\n\nTo configure tiers, a new sysfs interface is exposed at\n`/sys/kernel/mm/swap/tiers`. The input parser evaluates commands from\nleft to right and supports batch input, allowing users to add, remove or\nmodify multiple tiers in a single write operation.\n\nTier management enforces continuous priority ranges anchored by start\npriorities. Operations trigger range splitting or merging, but overwriting\nstart priorities is forbidden. Merging expands lower tiers upwards to\npreserve configured start priorities, except when removing `DEF_SWAP_PRIO`,\nwhich merges downwards.\n\nSuggested-by: Chris Li <chrisl@kernel.org>\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n MAINTAINERS     |   2 +\n mm/Makefile     |   2 +-\n mm/swap.h       |   4 +\n mm/swap_state.c |  70 +++++++++++\n mm/swap_tier.c  | 304 ++++++++++++++++++++++++++++++++++++++++++++++++\n mm/swap_tier.h  |  38 ++++++\n mm/swapfile.c   |   7 +-\n 7 files changed, 423 insertions(+), 4 deletions(-)\n create mode 100644 mm/swap_tier.c\n create mode 100644 mm/swap_tier.h\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 18d1ebf053db..501bf46adfb4 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16743,6 +16743,8 @@ F:\tmm/swap.c\n F:\tmm/swap.h\n F:\tmm/swap_table.h\n F:\tmm/swap_state.c\n+F:\tmm/swap_tier.c\n+F:\tmm/swap_tier.h\n F:\tmm/swapfile.c\n \n MEMORY MANAGEMENT - THP (TRANSPARENT HUGE PAGE)\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 53ca5d4b1929..3b3de2de7285 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -75,7 +75,7 @@ ifdef CONFIG_MMU\n \tobj-$(CONFIG_ADVISE_SYSCALLS)\t+= madvise.o\n endif\n \n-obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o\n+obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o swap_tier.o\n obj-$(CONFIG_ZSWAP)\t+= zswap.o\n obj-$(CONFIG_HAS_DMA)\t+= dmapool.o\n obj-$(CONFIG_HUGETLBFS)\t+= hugetlb.o hugetlb_sysfs.o hugetlb_sysctl.o\ndiff --git a/mm/swap.h b/mm/swap.h\nindex bfafa637c458..55f230cbe4e7 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -16,6 +16,10 @@ extern int page_cluster;\n #define swap_entry_order(order)\t0\n #endif\n \n+#define DEF_SWAP_PRIO  -1\n+\n+extern spinlock_t swap_lock;\n+extern struct plist_head swap_active_head;\n extern struct swap_info_struct *swap_info[];\n \n /*\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 6d0eef7470be..f1a7d9cdc648 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -25,6 +25,7 @@\n #include \"internal.h\"\n #include \"swap_table.h\"\n #include \"swap.h\"\n+#include \"swap_tier.h\"\n \n /*\n  * swapper_space is a fiction, retained to simplify the path through\n@@ -947,8 +948,77 @@ static ssize_t vma_ra_enabled_store(struct kobject *kobj,\n }\n static struct kobj_attribute vma_ra_enabled_attr = __ATTR_RW(vma_ra_enabled);\n \n+static ssize_t tiers_show(struct kobject *kobj,\n+\t\t\t\t     struct kobj_attribute *attr, char *buf)\n+{\n+\treturn swap_tiers_sysfs_show(buf);\n+}\n+\n+static ssize_t tiers_store(struct kobject *kobj,\n+\t\t\tstruct kobj_attribute *attr,\n+\t\t\tconst char *buf, size_t count)\n+{\n+\tchar *p, *token, *name, *tmp;\n+\tint ret = 0;\n+\tshort prio;\n+\tDEFINE_SWAP_TIER_SAVE_CTX(ctx);\n+\n+\ttmp = kstrdup(buf, GFP_KERNEL);\n+\tif (!tmp)\n+\t\treturn -ENOMEM;\n+\n+\tspin_lock(&swap_lock);\n+\tspin_lock(&swap_tier_lock);\n+\n+\tp = tmp;\n+\tswap_tiers_save(ctx);\n+\n+\twhile (!ret && (token = strsep(&p, \", \\t\\n\")) != NULL) {\n+\t\tif (!*token)\n+\t\t\tcontinue;\n+\n+\t\tif (token[0] == '-') {\n+\t\t\tret = swap_tiers_remove(token + 1);\n+\t\t} else {\n+\n+\t\t\tname = strsep(&token, \":\");\n+\t\t\tif (!token || kstrtos16(token, 10, &prio)) {\n+\t\t\t\tret = -EINVAL;\n+\t\t\t\tgoto out;\n+\t\t\t}\n+\n+\t\t\tif (name[0] == '+')\n+\t\t\t\tret = swap_tiers_add(name + 1, prio);\n+\t\t\telse\n+\t\t\t\tret = swap_tiers_modify(name, prio);\n+\t\t}\n+\n+\t\tif (ret)\n+\t\t\tgoto restore;\n+\t}\n+\n+\tif (!swap_tiers_validate()) {\n+\t\tret = -EINVAL;\n+\t\tgoto restore;\n+\t}\n+\n+out:\n+\tspin_unlock(&swap_tier_lock);\n+\tspin_unlock(&swap_lock);\n+\n+\tkfree(tmp);\n+\treturn ret ? ret : count;\n+\n+restore:\n+\tswap_tiers_restore(ctx);\n+\tgoto out;\n+}\n+\n+static struct kobj_attribute tier_attr = __ATTR_RW(tiers);\n+\n static struct attribute *swap_attrs[] = {\n \t&vma_ra_enabled_attr.attr,\n+\t&tier_attr.attr,\n \tNULL,\n };\n \ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nnew file mode 100644\nindex 000000000000..87882272eec8\n--- /dev/null\n+++ b/mm/swap_tier.c\n@@ -0,0 +1,304 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#include <linux/swap.h>\n+#include <linux/memcontrol.h>\n+#include \"memcontrol-v1.h\"\n+#include <linux/sysfs.h>\n+#include <linux/plist.h>\n+\n+#include \"swap.h\"\n+#include \"swap_tier.h\"\n+\n+/*\n+ * struct swap_tier - structure representing a swap tier.\n+ *\n+ * @name: name of the swap_tier.\n+ * @prio: starting value of priority.\n+ * @list: linked list of tiers.\n+*/\n+static struct swap_tier {\n+\tchar name[MAX_TIERNAME];\n+\tshort prio;\n+\tstruct list_head list;\n+} swap_tiers[MAX_SWAPTIER];\n+\n+DEFINE_SPINLOCK(swap_tier_lock);\n+/* active swap priority list, sorted in descending order */\n+static LIST_HEAD(swap_tier_active_list);\n+/* unused swap_tier object */\n+static LIST_HEAD(swap_tier_inactive_list);\n+\n+#define TIER_IDX(tier)\t((tier) - swap_tiers)\n+#define TIER_MASK(tier)\t(1 << TIER_IDX(tier))\n+#define TIER_INVALID_PRIO (DEF_SWAP_PRIO - 1)\n+#define TIER_END_PRIO(tier) \\\n+\t(!list_is_first(&(tier)->list, &swap_tier_active_list) ? \\\n+\tlist_prev_entry((tier), list)->prio - 1 : SHRT_MAX)\n+\n+#define for_each_tier(tier, idx) \\\n+\tfor (idx = 0, tier = &swap_tiers[0]; idx < MAX_SWAPTIER; \\\n+\t\tidx++, tier = &swap_tiers[idx])\n+\n+#define for_each_active_tier(tier) \\\n+\tlist_for_each_entry(tier, &swap_tier_active_list, list)\n+\n+#define for_each_inactive_tier(tier) \\\n+\tlist_for_each_entry(tier, &swap_tier_inactive_list, list)\n+\n+/*\n+ * Naming Convention:\n+ *   swap_tiers_*() - Public/exported functions\n+ *   swap_tier_*()  - Private/internal functions\n+ */\n+\n+static bool swap_tier_is_active(void)\n+{\n+\treturn !list_empty(&swap_tier_active_list) ? true : false;\n+}\n+\n+static struct swap_tier *swap_tier_lookup(const char *name)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (!strcmp(tier->name, name))\n+\t\t\treturn tier;\n+\t}\n+\n+\treturn NULL;\n+}\n+\n+void swap_tiers_init(void)\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tBUILD_BUG_ON(BITS_PER_TYPE(int) < MAX_SWAPTIER);\n+\n+\tfor_each_tier(tier, idx) {\n+\t\tINIT_LIST_HEAD(&tier->list);\n+\t\tlist_add_tail(&tier->list, &swap_tier_inactive_list);\n+\t}\n+}\n+\n+ssize_t swap_tiers_sysfs_show(char *buf)\n+{\n+\tstruct swap_tier *tier;\n+\tssize_t len = 0;\n+\n+\tlen += sysfs_emit_at(buf, len, \"%-16s %-5s %-11s %-11s\\n\",\n+\t\t\t \"Name\", \"Idx\", \"PrioStart\", \"PrioEnd\");\n+\n+\tspin_lock(&swap_tier_lock);\n+\tfor_each_active_tier(tier) {\n+\t\tlen += sysfs_emit_at(buf, len, \"%-16s %-5ld %-11d %-11d\\n\",\n+\t\t\t\t     tier->name,\n+\t\t\t\t     TIER_IDX(tier),\n+\t\t\t\t     tier->prio,\n+\t\t\t\t     TIER_END_PRIO(tier));\n+\t\tif (len >= PAGE_SIZE)\n+\t\t\tbreak;\n+\t}\n+\tspin_unlock(&swap_tier_lock);\n+\n+\treturn len;\n+}\n+\n+static void swap_tier_insert_by_prio(struct swap_tier *new)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (tier->prio > new->prio)\n+\t\t\tcontinue;\n+\n+\t\tlist_add_tail(&new->list, &tier->list);\n+\t\treturn;\n+\t}\n+\t/* First addition, or becomes the first tier */\n+\tlist_add_tail(&new->list, &swap_tier_active_list);\n+}\n+\n+static void __swap_tier_prepare(struct swap_tier *tier, const char *name,\n+\tshort prio)\n+{\n+\tlist_del_init(&tier->list);\n+\tstrscpy(tier->name, name, MAX_TIERNAME);\n+\ttier->prio = prio;\n+}\n+\n+static struct swap_tier *swap_tier_prepare(const char *name, short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tif (prio < DEF_SWAP_PRIO)\n+\t\treturn NULL;\n+\n+\tif (list_empty(&swap_tier_inactive_list))\n+\t\treturn ERR_PTR(-EPERM);\n+\n+\ttier = list_first_entry(&swap_tier_inactive_list,\n+\t\tstruct swap_tier, list);\n+\n+\t__swap_tier_prepare(tier, name, prio);\n+\treturn tier;\n+}\n+\n+static int swap_tier_check_range(short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\t/* No overwrite */\n+\t\tif (tier->prio == prio)\n+\t\t\treturn -EINVAL;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+int swap_tiers_add(const char *name, int prio)\n+{\n+\tint ret;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\t/* Duplicate check */\n+\tif (swap_tier_lookup(name))\n+\t\treturn -EPERM;\n+\n+\tret = swap_tier_check_range(prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\ttier = swap_tier_prepare(name, prio);\n+\tif (IS_ERR(tier)) {\n+\t\tret = PTR_ERR(tier);\n+\t\treturn ret;\n+\t}\n+\n+\n+\tswap_tier_insert_by_prio(tier);\n+\treturn ret;\n+}\n+\n+int swap_tiers_remove(const char *name)\n+{\n+\tint ret = 0;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\ttier = swap_tier_lookup(name);\n+\tif (!tier)\n+\t\treturn -EINVAL;\n+\n+\tlist_move(&tier->list, &swap_tier_inactive_list);\n+\n+\t/* Removing DEF_SWAP_PRIO merges into the higher tier. */\n+\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO)\n+\t\tlist_prev_entry(tier, list)->prio = DEF_SWAP_PRIO;\n+\n+\treturn ret;\n+}\n+\n+int swap_tiers_modify(const char *name, int prio)\n+{\n+\tint ret;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\ttier = swap_tier_lookup(name);\n+\tif (!tier)\n+\t\treturn -EINVAL;\n+\n+\t/* No need to modify */\n+\tif (tier->prio == prio)\n+\t\treturn 0;\n+\n+\tret = swap_tier_check_range(prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tlist_del_init(&tier->list);\n+\ttier->prio = prio;\n+\tswap_tier_insert_by_prio(tier);\n+\n+\treturn ret;\n+}\n+\n+/*\n+ * XXX: Reverting individual operations becomes complex as the number of\n+ * operations grows. Instead, we save the original state beforehand and\n+ * fully restore it if any operation fails.\n+ */\n+void swap_tiers_save(struct swap_tier_save_ctx ctx[])\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tidx = TIER_IDX(tier);\n+\t\tstrcpy(ctx[idx].name, tier->name);\n+\t\tctx[idx].prio = tier->prio;\n+\t}\n+\n+\tfor_each_inactive_tier(tier) {\n+\t\tidx = TIER_IDX(tier);\n+\t\t/* Indicator of inactive */\n+\t\tctx[idx].prio = TIER_INVALID_PRIO;\n+\t}\n+}\n+\n+void swap_tiers_restore(struct swap_tier_save_ctx ctx[])\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\t/* Invalidate active list */\n+\tlist_splice_tail_init(&swap_tier_active_list,\n+\t\t\t&swap_tier_inactive_list);\n+\n+\tfor_each_tier(tier, idx) {\n+\t\tif (ctx[idx].prio != TIER_INVALID_PRIO) {\n+\t\t\t/* Preserve idx(mask) */\n+\t\t\t__swap_tier_prepare(tier, ctx[idx].name, ctx[idx].prio);\n+\t\t\tswap_tier_insert_by_prio(tier);\n+\t\t}\n+\t}\n+}\n+\n+bool swap_tiers_validate(void)\n+{\n+\tstruct swap_tier *tier;\n+\n+\t/*\n+\t * Initial setting might not cover DEF_SWAP_PRIO.\n+\t * Swap tier must cover the full range (DEF_SWAP_PRIO to SHRT_MAX).\n+\t * Also, modify operation can change only one remaining priority.\n+\t */\n+\tif (swap_tier_is_active()) {\n+\t\ttier = list_last_entry(&swap_tier_active_list,\n+\t\t\tstruct swap_tier, list);\n+\n+\t\tif (tier->prio != DEF_SWAP_PRIO)\n+\t\t\treturn false;\n+\t}\n+\n+\treturn true;\n+}\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nnew file mode 100644\nindex 000000000000..4b1b0602d691\n--- /dev/null\n+++ b/mm/swap_tier.h\n@@ -0,0 +1,38 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _SWAP_TIER_H\n+#define _SWAP_TIER_H\n+\n+#include <linux/types.h>\n+#include <linux/spinlock.h>\n+\n+#define MAX_TIERNAME\t\t16\n+\n+/* Ensure MAX_SWAPTIER does not exceed MAX_SWAPFILES */\n+#if 8 > MAX_SWAPFILES\n+#define MAX_SWAPTIER\t\tMAX_SWAPFILES\n+#else\n+#define MAX_SWAPTIER\t\t8\n+#endif\n+\n+extern spinlock_t swap_tier_lock;\n+\n+struct swap_tier_save_ctx {\n+\tchar name[MAX_TIERNAME];\n+\tshort prio;\n+};\n+\n+#define DEFINE_SWAP_TIER_SAVE_CTX(_name) \\\n+\tstruct swap_tier_save_ctx _name[MAX_SWAPTIER] = {0}\n+\n+/* Initialization and application */\n+void swap_tiers_init(void);\n+ssize_t swap_tiers_sysfs_show(char *buf);\n+\n+int swap_tiers_add(const char *name, int prio);\n+int swap_tiers_remove(const char *name);\n+int swap_tiers_modify(const char *name, int prio);\n+\n+void swap_tiers_save(struct swap_tier_save_ctx ctx[]);\n+void swap_tiers_restore(struct swap_tier_save_ctx ctx[]);\n+bool swap_tiers_validate(void);\n+#endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 7b055f15d705..c27952b41d4f 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -50,6 +50,7 @@\n #include \"internal.h\"\n #include \"swap_table.h\"\n #include \"swap.h\"\n+#include \"swap_tier.h\"\n \n static bool swap_count_continued(struct swap_info_struct *, pgoff_t,\n \t\t\t\t unsigned char);\n@@ -65,7 +66,7 @@ static void move_cluster(struct swap_info_struct *si,\n \t\t\t struct swap_cluster_info *ci, struct list_head *list,\n \t\t\t enum swap_cluster_flags new_flags);\n \n-static DEFINE_SPINLOCK(swap_lock);\n+DEFINE_SPINLOCK(swap_lock);\n static unsigned int nr_swapfiles;\n atomic_long_t nr_swap_pages;\n /*\n@@ -76,7 +77,6 @@ atomic_long_t nr_swap_pages;\n EXPORT_SYMBOL_GPL(nr_swap_pages);\n /* protected with swap_lock. reading in vm_swap_full() doesn't need lock */\n long total_swap_pages;\n-#define DEF_SWAP_PRIO  -1\n unsigned long swapfile_maximum_size;\n #ifdef CONFIG_MIGRATION\n bool swap_migration_ad_supported;\n@@ -89,7 +89,7 @@ static const char Bad_offset[] = \"Bad swap offset entry \";\n  * all active swap_info_structs\n  * protected with swap_lock, and ordered by priority.\n  */\n-static PLIST_HEAD(swap_active_head);\n+PLIST_HEAD(swap_active_head);\n \n /*\n  * all available (active, not full) swap_info_structs\n@@ -3977,6 +3977,7 @@ static int __init swapfile_init(void)\n \t\tswap_migration_ad_supported = true;\n #endif\t/* CONFIG_MIGRATION */\n \n+\tswap_tiers_init();\n \treturn 0;\n }\n subsys_initcall(swapfile_init);\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "message_id": "20260126065242.1221862-2-youngjun.park@lge.com",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about caching oscillation and priority inversion in swap devices due to percpu clusters, agreed to revert commit 1b7e90020eb7 to use each swap device's percpu cluster instead of the global one. A fix is planned for v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a problem",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This reverts commit 1b7e90020eb7 (\"mm, swap: use percpu cluster as\nallocation fast path\").\n\nBecause in the newly introduced swap tiers, the global percpu cluster\nwill cause two issues:\n1) it will cause caching oscillation in the same order of different si\n   if two different memcg can only be allowed to access different si and\n   both of them are swapping out.\n2) It can cause priority inversion on swap devices. Imagine a case where\n   there are two memcg, say memcg1 and memcg2. Memcg1 can access si A, B\n   and A is higher priority device. While memcg2 can only access si B.\n   Then memcg 2 could write the global percpu cluster with si B, then\n   memcg1 take si B in fast path even though si A is not exhausted.\n\nHence in order to support swap tier, revert commit to use\neach swap device's percpu cluster.\n\nSuggested-by: Kairui Song <kasong@tencent.com>\nCo-developed-by: Baoquan He <bhe@redhat.com>\nSigned-off-by: Baoquan He <bhe@redhat.com>\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n include/linux/swap.h |  17 ++++--\n mm/swapfile.c        | 142 ++++++++++++++-----------------------------\n 2 files changed, 57 insertions(+), 102 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 1e68c220a0e7..6921e22b14d3 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -247,11 +247,18 @@ enum {\n #define SWAP_NR_ORDERS\t\t1\n #endif\n \n-/*\n- * We keep using same cluster for rotational device so IO will be sequential.\n- * The purpose is to optimize SWAP throughput on these device.\n- */\n+ /*\n+  * We assign a cluster to each CPU, so each CPU can allocate swap entry from\n+  * its own cluster and swapout sequentially. The purpose is to optimize swapout\n+  * throughput.\n+  */\n+struct percpu_cluster {\n+\tlocal_lock_t lock; /* Protect the percpu_cluster above */\n+\tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n+};\n+\n struct swap_sequential_cluster {\n+\tspinlock_t lock; /* Serialize usage of global cluster */\n \tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n };\n \n@@ -277,8 +284,8 @@ struct swap_info_struct {\n \t\t\t\t\t/* list of cluster that are fragmented or contented */\n \tunsigned int pages;\t\t/* total of usable pages of swap */\n \tatomic_long_t inuse_pages;\t/* number of those currently in use */\n+\tstruct percpu_cluster\t__percpu *percpu_cluster; /* per cpu's swap location */\n \tstruct swap_sequential_cluster *global_cluster; /* Use one global cluster for rotating device */\n-\tspinlock_t global_cluster_lock;\t/* Serialize usage of global cluster */\n \tstruct rb_root swap_extent_root;/* root of the swap extent rbtree */\n \tstruct block_device *bdev;\t/* swap device or bdev of swap file */\n \tstruct file *swap_file;\t\t/* seldom referenced */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex dd97e850ea2c..5e3b87799440 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -118,18 +118,6 @@ static atomic_t proc_poll_event = ATOMIC_INIT(0);\n \n atomic_t nr_rotate_swap = ATOMIC_INIT(0);\n \n-struct percpu_swap_cluster {\n-\tstruct swap_info_struct *si[SWAP_NR_ORDERS];\n-\tunsigned long offset[SWAP_NR_ORDERS];\n-\tlocal_lock_t lock;\n-};\n-\n-static DEFINE_PER_CPU(struct percpu_swap_cluster, percpu_swap_cluster) = {\n-\t.si = { NULL },\n-\t.offset = { SWAP_ENTRY_INVALID },\n-\t.lock = INIT_LOCAL_LOCK(),\n-};\n-\n /* May return NULL on invalid type, caller must check for NULL return */\n static struct swap_info_struct *swap_type_to_info(int type)\n {\n@@ -477,7 +465,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * Swap allocator uses percpu clusters and holds the local lock.\n \t */\n \tlockdep_assert_held(&ci->lock);\n-\tlockdep_assert_held(&this_cpu_ptr(&percpu_swap_cluster)->lock);\n+\tlockdep_assert_held(this_cpu_ptr(&si->percpu_cluster->lock));\n \n \t/* The cluster must be free and was just isolated from the free list. */\n \tVM_WARN_ON_ONCE(ci->flags || !cluster_is_empty(ci));\n@@ -495,8 +483,8 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t */\n \tspin_unlock(&ci->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_unlock(&si->global_cluster_lock);\n-\tlocal_unlock(&percpu_swap_cluster.lock);\n+\t\tspin_unlock(&si->global_cluster->lock);\n+\tlocal_unlock(&si->percpu_cluster->lock);\n \n \ttable = swap_table_alloc(__GFP_HIGH | __GFP_NOMEMALLOC | GFP_KERNEL);\n \n@@ -508,9 +496,9 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * could happen with ignoring the percpu cluster is fragmentation,\n \t * which is acceptable since this fallback and race is rare.\n \t */\n-\tlocal_lock(&percpu_swap_cluster.lock);\n+\tlocal_lock(&si->percpu_cluster->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_lock(&si->global_cluster_lock);\n+\t\tspin_lock(&si->global_cluster->lock);\n \tspin_lock(&ci->lock);\n \n \t/* Nothing except this helper should touch a dangling empty cluster. */\n@@ -622,7 +610,7 @@ static bool swap_do_scheduled_discard(struct swap_info_struct *si)\n \t\tci = list_first_entry(&si->discard_clusters, struct swap_cluster_info, list);\n \t\t/*\n \t\t * Delete the cluster from list to prepare for discard, but keep\n-\t\t * the CLUSTER_FLAG_DISCARD flag, percpu_swap_cluster could be\n+\t\t * the CLUSTER_FLAG_DISCARD flag, there could be percpu_cluster\n \t\t * pointing to it, or ran into by relocate_cluster.\n \t\t */\n \t\tlist_del(&ci->list);\n@@ -953,12 +941,11 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n out:\n \trelocate_cluster(si, ci);\n \tswap_cluster_unlock(ci);\n-\tif (si->flags & SWP_SOLIDSTATE) {\n-\t\tthis_cpu_write(percpu_swap_cluster.offset[order], next);\n-\t\tthis_cpu_write(percpu_swap_cluster.si[order], si);\n-\t} else {\n+\tif (si->flags & SWP_SOLIDSTATE)\n+\t\tthis_cpu_write(si->percpu_cluster->next[order], next);\n+\telse\n \t\tsi->global_cluster->next[order] = next;\n-\t}\n+\n \treturn found;\n }\n \n@@ -1052,13 +1039,17 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \tif (order && !(si->flags & SWP_BLKDEV))\n \t\treturn 0;\n \n-\tif (!(si->flags & SWP_SOLIDSTATE)) {\n+\tif (si->flags & SWP_SOLIDSTATE) {\n+\t\t/* Fast path using per CPU cluster */\n+\t\tlocal_lock(&si->percpu_cluster->lock);\n+\t\toffset = __this_cpu_read(si->percpu_cluster->next[order]);\n+\t} else {\n \t\t/* Serialize HDD SWAP allocation for each device. */\n-\t\tspin_lock(&si->global_cluster_lock);\n+\t\tspin_lock(&si->global_cluster->lock);\n \t\toffset = si->global_cluster->next[order];\n-\t\tif (offset == SWAP_ENTRY_INVALID)\n-\t\t\tgoto new_cluster;\n+\t}\n \n+\tif (offset != SWAP_ENTRY_INVALID) {\n \t\tci = swap_cluster_lock(si, offset);\n \t\t/* Cluster could have been used by another order */\n \t\tif (cluster_is_usable(ci, order)) {\n@@ -1072,7 +1063,6 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n \n-new_cluster:\n \t/*\n \t * If the device need discard, prefer new cluster over nonfull\n \t * to spread out the writes.\n@@ -1129,8 +1119,10 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n done:\n-\tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_unlock(&si->global_cluster_lock);\n+\tif (si->flags & SWP_SOLIDSTATE)\n+\t\tlocal_unlock(&si->percpu_cluster->lock);\n+\telse\n+\t\tspin_unlock(&si->global_cluster->lock);\n \n \treturn found;\n }\n@@ -1311,41 +1303,8 @@ static bool get_swap_device_info(struct swap_info_struct *si)\n \treturn true;\n }\n \n-/*\n- * Fast path try to get swap entries with specified order from current\n- * CPU's swap entry pool (a cluster).\n- */\n-static bool swap_alloc_fast(struct folio *folio)\n-{\n-\tunsigned int order = folio_order(folio);\n-\tstruct swap_cluster_info *ci;\n-\tstruct swap_info_struct *si;\n-\tunsigned int offset;\n-\n-\t/*\n-\t * Once allocated, swap_info_struct will never be completely freed,\n-\t * so checking it's liveness by get_swap_device_info is enough.\n-\t */\n-\tsi = this_cpu_read(percpu_swap_cluster.si[order]);\n-\toffset = this_cpu_read(percpu_swap_cluster.offset[order]);\n-\tif (!si || !offset || !get_swap_device_info(si))\n-\t\treturn false;\n-\n-\tci = swap_cluster_lock(si, offset);\n-\tif (cluster_is_usable(ci, order)) {\n-\t\tif (cluster_is_empty(ci))\n-\t\t\toffset = cluster_offset(si, ci);\n-\t\talloc_swap_scan_cluster(si, ci, folio, offset);\n-\t} else {\n-\t\tswap_cluster_unlock(ci);\n-\t}\n-\n-\tput_swap_device(si);\n-\treturn folio_test_swapcache(folio);\n-}\n-\n /* Rotate the device and switch to a new cluster */\n-static void swap_alloc_slow(struct folio *folio)\n+static void swap_alloc_entry(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n \tint mask = folio_memcg(folio) ?\n@@ -1363,6 +1322,7 @@ static void swap_alloc_slow(struct folio *folio)\n \t\tif (get_swap_device_info(si)) {\n \t\t\tcluster_alloc_swap_entry(si, folio);\n \t\t\tput_swap_device(si);\n+\n \t\t\tif (folio_test_swapcache(folio))\n \t\t\t\treturn;\n \t\t\tif (folio_test_large(folio))\n@@ -1522,11 +1482,7 @@ int folio_alloc_swap(struct folio *folio)\n \t}\n \n again:\n-\tlocal_lock(&percpu_swap_cluster.lock);\n-\tif (!swap_alloc_fast(folio))\n-\t\tswap_alloc_slow(folio);\n-\tlocal_unlock(&percpu_swap_cluster.lock);\n-\n+\tswap_alloc_entry(folio);\n \tif (!order && unlikely(!folio_test_swapcache(folio))) {\n \t\tif (swap_sync_discard())\n \t\t\tgoto again;\n@@ -1945,9 +1901,7 @@ swp_entry_t swap_alloc_hibernation_slot(int type)\n \t\t\t * Grab the local lock to be compliant\n \t\t\t * with swap table allocation.\n \t\t\t */\n-\t\t\tlocal_lock(&percpu_swap_cluster.lock);\n \t\t\toffset = cluster_alloc_swap_entry(si, NULL);\n-\t\t\tlocal_unlock(&percpu_swap_cluster.lock);\n \t\t\tif (offset)\n \t\t\t\tentry = swp_entry(si->type, offset);\n \t\t}\n@@ -2751,28 +2705,6 @@ static void free_cluster_info(struct swap_cluster_info *cluster_info,\n \tkvfree(cluster_info);\n }\n \n-/*\n- * Called after swap device's reference count is dead, so\n- * neither scan nor allocation will use it.\n- */\n-static void flush_percpu_swap_cluster(struct swap_info_struct *si)\n-{\n-\tint cpu, i;\n-\tstruct swap_info_struct **pcp_si;\n-\n-\tfor_each_possible_cpu(cpu) {\n-\t\tpcp_si = per_cpu_ptr(percpu_swap_cluster.si, cpu);\n-\t\t/*\n-\t\t * Invalidate the percpu swap cluster cache, si->users\n-\t\t * is dead, so no new user will point to it, just flush\n-\t\t * any existing user.\n-\t\t */\n-\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n-\t\t\tcmpxchg(&pcp_si[i], si, NULL);\n-\t}\n-}\n-\n-\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n@@ -2856,7 +2788,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tflush_work(&p->discard_work);\n \tflush_work(&p->reclaim_work);\n-\tflush_percpu_swap_cluster(p);\n \n \tdestroy_swap_extents(p);\n \tif (p->flags & SWP_CONTINUED)\n@@ -2885,6 +2816,8 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tarch_swap_invalidate_area(p->type);\n \tzswap_swapoff(p->type);\n \tmutex_unlock(&swapon_mutex);\n+\tfree_percpu(p->percpu_cluster);\n+\tp->percpu_cluster = NULL;\n \tkfree(p->global_cluster);\n \tp->global_cluster = NULL;\n \tvfree(swap_map);\n@@ -3268,7 +3201,7 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n {\n \tunsigned long nr_clusters = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);\n \tstruct swap_cluster_info *cluster_info;\n-\tint err = -ENOMEM;\n+\tint cpu, err = -ENOMEM;\n \tunsigned long i;\n \n \tcluster_info = kvcalloc(nr_clusters, sizeof(*cluster_info), GFP_KERNEL);\n@@ -3278,14 +3211,27 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n \tfor (i = 0; i < nr_clusters; i++)\n \t\tspin_lock_init(&cluster_info[i].lock);\n \n-\tif (!(si->flags & SWP_SOLIDSTATE)) {\n+\tif (si->flags & SWP_SOLIDSTATE) {\n+\t\tsi->percpu_cluster = alloc_percpu(struct percpu_cluster);\n+\t\tif (!si->percpu_cluster)\n+\t\t\tgoto err;\n+\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tstruct percpu_cluster *cluster;\n+\n+\t\t\tcluster = per_cpu_ptr(si->percpu_cluster, cpu);\n+\t\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n+\t\t\t\tcluster->next[i] = SWAP_ENTRY_INVALID;\n+\t\t\tlocal_lock_init(&cluster->lock);\n+\t\t}\n+\t} else {\n \t\tsi->global_cluster = kmalloc(sizeof(*si->global_cluster),\n \t\t\t\t     GFP_KERNEL);\n \t\tif (!si->global_cluster)\n \t\t\tgoto err;\n \t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n \t\t\tsi->global_cluster->next[i] = SWAP_ENTRY_INVALID;\n-\t\tspin_lock_init(&si->global_cluster_lock);\n+\t\tspin_lock_init(&si->global_cluster->lock);\n \t}\n \n \t/*\n@@ -3566,6 +3512,8 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n bad_swap_unlock_inode:\n \tinode_unlock(inode);\n bad_swap:\n+\tfree_percpu(si->percpu_cluster);\n+\tsi->percpu_cluster = NULL;\n \tkfree(si->global_cluster);\n \tsi->global_cluster = NULL;\n \tinode = NULL;\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "message_id": "20260126065242.1221862-5-youngjun.park@lge.com",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about swap device rotation on every allocation, which leads to severe fragmentation and performance regression. They introduced a per-cpu cache for the swap device, prioritizing the cached device within the cluster, effectively restoring the traditional fastpath and slowpath flow. This approach minimizes side effects on the existing fastpath.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "agreed to address it"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When using per-device percpu clusters (instead of a global one),\na naive allocation logic triggers swap device rotation on every\nallocation. This behavior leads to severe fragmentation and performance\nregression.\n\nTo address this, this patch introduces a per-cpu cache for the swap\ndevice. The allocation logic is updated to prioritize the per-cpu\ncluster within the cached swap device, effectively restoring the\ntraditional fastpath and slowpath flow. This approach minimizes side\neffects on the existing fastpath.\n\nWith this change, swap device rotation occurs only when the current\ncached device is unable to satisfy the allocation, rather than on\nevery attempt.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n include/linux/swap.h |  1 -\n mm/swapfile.c        | 78 +++++++++++++++++++++++++++++++++++++-------\n 2 files changed, 66 insertions(+), 13 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 6921e22b14d3..ac634a21683a 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -253,7 +253,6 @@ enum {\n   * throughput.\n   */\n struct percpu_cluster {\n-\tlocal_lock_t lock; /* Protect the percpu_cluster above */\n \tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n };\n \ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 5e3b87799440..0dcd451afee5 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -106,6 +106,16 @@ PLIST_HEAD(swap_active_head);\n static PLIST_HEAD(swap_avail_head);\n static DEFINE_SPINLOCK(swap_avail_lock);\n \n+struct percpu_swap_device {\n+\tstruct swap_info_struct *si[SWAP_NR_ORDERS];\n+\tlocal_lock_t lock;\n+};\n+\n+static DEFINE_PER_CPU(struct percpu_swap_device, percpu_swap_device) = {\n+\t.si = { NULL },\n+\t.lock = INIT_LOCAL_LOCK(),\n+};\n+\n struct swap_info_struct *swap_info[MAX_SWAPFILES];\n \n static struct kmem_cache *swap_table_cachep;\n@@ -465,7 +475,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * Swap allocator uses percpu clusters and holds the local lock.\n \t */\n \tlockdep_assert_held(&ci->lock);\n-\tlockdep_assert_held(this_cpu_ptr(&si->percpu_cluster->lock));\n+\tlockdep_assert_held(this_cpu_ptr(&percpu_swap_device.lock));\n \n \t/* The cluster must be free and was just isolated from the free list. */\n \tVM_WARN_ON_ONCE(ci->flags || !cluster_is_empty(ci));\n@@ -484,7 +494,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \tspin_unlock(&ci->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_unlock(&si->global_cluster->lock);\n-\tlocal_unlock(&si->percpu_cluster->lock);\n+\tlocal_unlock(&percpu_swap_device.lock);\n \n \ttable = swap_table_alloc(__GFP_HIGH | __GFP_NOMEMALLOC | GFP_KERNEL);\n \n@@ -496,7 +506,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * could happen with ignoring the percpu cluster is fragmentation,\n \t * which is acceptable since this fallback and race is rare.\n \t */\n-\tlocal_lock(&si->percpu_cluster->lock);\n+\tlocal_lock(&percpu_swap_device.lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_lock(&si->global_cluster->lock);\n \tspin_lock(&ci->lock);\n@@ -941,9 +951,10 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n out:\n \trelocate_cluster(si, ci);\n \tswap_cluster_unlock(ci);\n-\tif (si->flags & SWP_SOLIDSTATE)\n+\tif (si->flags & SWP_SOLIDSTATE) {\n \t\tthis_cpu_write(si->percpu_cluster->next[order], next);\n-\telse\n+\t\tthis_cpu_write(percpu_swap_device.si[order], si);\n+\t} else\n \t\tsi->global_cluster->next[order] = next;\n \n \treturn found;\n@@ -1041,7 +1052,6 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \n \tif (si->flags & SWP_SOLIDSTATE) {\n \t\t/* Fast path using per CPU cluster */\n-\t\tlocal_lock(&si->percpu_cluster->lock);\n \t\toffset = __this_cpu_read(si->percpu_cluster->next[order]);\n \t} else {\n \t\t/* Serialize HDD SWAP allocation for each device. */\n@@ -1119,9 +1129,7 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n done:\n-\tif (si->flags & SWP_SOLIDSTATE)\n-\t\tlocal_unlock(&si->percpu_cluster->lock);\n-\telse\n+\tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_unlock(&si->global_cluster->lock);\n \n \treturn found;\n@@ -1303,8 +1311,27 @@ static bool get_swap_device_info(struct swap_info_struct *si)\n \treturn true;\n }\n \n+static bool swap_alloc_fast(struct folio *folio)\n+{\n+\tunsigned int order = folio_order(folio);\n+\tstruct swap_info_struct *si;\n+\n+\t/*\n+\t * Once allocated, swap_info_struct will never be completely freed,\n+\t * so checking it's liveness by get_swap_device_info is enough.\n+\t */\n+\tsi = this_cpu_read(percpu_swap_device.si[order]);\n+\tif (!si || !get_swap_device_info(si))\n+\t\treturn false;\n+\n+\tcluster_alloc_swap_entry(si, folio);\n+\tput_swap_device(si);\n+\n+\treturn folio_test_swapcache(folio);\n+}\n+\n /* Rotate the device and switch to a new cluster */\n-static void swap_alloc_entry(struct folio *folio)\n+static void swap_alloc_slow(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n \tint mask = folio_memcg(folio) ?\n@@ -1482,7 +1509,11 @@ int folio_alloc_swap(struct folio *folio)\n \t}\n \n again:\n-\tswap_alloc_entry(folio);\n+\tlocal_lock(&percpu_swap_device.lock);\n+\tif (!swap_alloc_fast(folio))\n+\t\tswap_alloc_slow(folio);\n+\tlocal_unlock(&percpu_swap_device.lock);\n+\n \tif (!order && unlikely(!folio_test_swapcache(folio))) {\n \t\tif (swap_sync_discard())\n \t\t\tgoto again;\n@@ -1901,7 +1932,9 @@ swp_entry_t swap_alloc_hibernation_slot(int type)\n \t\t\t * Grab the local lock to be compliant\n \t\t\t * with swap table allocation.\n \t\t\t */\n+\t\t\tlocal_lock(&percpu_swap_device.lock);\n \t\t\toffset = cluster_alloc_swap_entry(si, NULL);\n+\t\t\tlocal_unlock(&percpu_swap_device.lock);\n \t\t\tif (offset)\n \t\t\t\tentry = swp_entry(si->type, offset);\n \t\t}\n@@ -2705,6 +2738,27 @@ static void free_cluster_info(struct swap_cluster_info *cluster_info,\n \tkvfree(cluster_info);\n }\n \n+/*\n+ * Called after swap device's reference count is dead, so\n+ * neither scan nor allocation will use it.\n+ */\n+static void flush_percpu_swap_device(struct swap_info_struct *si)\n+{\n+\tint cpu, i;\n+\tstruct swap_info_struct **pcp_si;\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcp_si = per_cpu_ptr(percpu_swap_device.si, cpu);\n+\t\t/*\n+\t\t * Invalidate the percpu swap device cache, si->users\n+\t\t * is dead, so no new user will point to it, just flush\n+\t\t * any existing user.\n+\t\t */\n+\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n+\t\t\tcmpxchg(&pcp_si[i], si, NULL);\n+\t}\n+}\n+\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n@@ -2788,6 +2842,7 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tflush_work(&p->discard_work);\n \tflush_work(&p->reclaim_work);\n+\tflush_percpu_swap_device(p);\n \n \tdestroy_swap_extents(p);\n \tif (p->flags & SWP_CONTINUED)\n@@ -3222,7 +3277,6 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n \t\t\tcluster = per_cpu_ptr(si->percpu_cluster, cpu);\n \t\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n \t\t\t\tcluster->next[i] = SWAP_ENTRY_INVALID;\n-\t\t\tlocal_lock_init(&cluster->lock);\n \t\t}\n \t} else {\n \t\tsi->global_cluster = kmalloc(sizeof(*si->global_cluster),\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "message_id": "20260126065242.1221862-6-youngjun.park@lge.com",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the swap_tier structure was simplified by replacing 'end prio' and priority lists with a standard list_head, as requested in his previous feedback.\n\nReviewer Chris Li suggested breaking down the patch series into smaller, more manageable steps, starting with defining the tiers bits without deleting any existing functionality, and then building upon that in subsequent steps.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledgment of previous feedback",
                "requested changes",
                "suggested improvements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Youngjun,\n\nOn Sun, Jan 25, 2026 at 10:53PM Youngjun Park <youngjun.park@lge.com> wrote:\n\n---\n\nThanks for the patches series.\n\nSorry for the late reply. I have been wanting to reply to it but get\nsuper busy at work.\n\nSome high level feedback for the series. Now that you demonstrated the\nwhole series, let's focus on making small mergiable baby steps. Just\nlike the swap table has different phases. Make each step minimal, each\nstep shows some value. Do the MVP, we can always add more features as\na follow up step.\n\nI suggest the first step is getting the tiers bits defined. Add only,\nno delete.  Get that reviewed and merged, then the next step is to use\nthose tiers.\n\nChris",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-11",
              "message_id": "CACePvbU3OoGg5-dHXOJk=62AkBxJCLmzwcHdHuPe2nnxfzMLBw@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested replacing per-cpu allocation for each swap device with a global per-cpu cluster per tier, as the maximum number of tiers is smaller than the maximum number of swap devices, which could be more efficient.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "One idea is that, instead of using percpu per swap device.\nYou can make the global percpu cluster per tier. Because the max tier\nnumber is smaller than the max number of swap devices. That is likely\na win.\n\nChris",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-11",
              "message_id": "CACePvbXeUx9_dyrSFoz57RnNccoMwiF5u70v6WqHJNFGEZrCPw@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the swap_tier structure simplification removed the 'end prio' and priority lists, which caused a loss of information about the tier's priority range, and requested reverting to the original implementation.\n\nReviewer Chris Li suggested introducing a CONFIG option to limit the maximum number of swap tiers, recommending a default value of 4.\n\nReviewer Chris Li noted that modifying a tier can cause swap files to move to a different tier, which may lead to issues, and requested further consideration of this scenario.\n\nReviewer Chris Li expressed concern about the complexity of the patch, specifically the need for save and restore operations, and requested a simpler design.\n\nReviewer Chris Li suggested that each tier have its own swap_active_head, so that different swap entries on different tiers do not compete for the same resource, and proposed that swapfiles should not be allowed to jump between tiers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Yongjun,\n\nOn Sun, Jan 25, 2026 at 10:53PM Youngjun Park <youngjun.park@lge.com> wrote:\n\n---\n\nWe can have a CONFIG option for the MAX_SWAPTIER. I think the default\nshould be a small number like 4.\n\n---\n\nWhen we add, modify, remove a tier. The simple case is there is no\nswap file under any tiers.\nBut if the modification causes some swap files to jump to different\ntiers. That might be problematic.\n\n---\n\nI really hope we don't have to do the save and restore thing. Is there\nanother design we can simplify this?\n\n---\n\nOne idea is to make each tier have swap_active_head. So different swap\nentry releases on different tiers don't need to be competing on the\nsame swap_active_head.\n\nThat will require the swapfile don't jump to another tiers.\n\nChris",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "message_id": "CACePvbVML6ZNJBWU9YSUCWwrbGd2eXMcsWxs6yFssfyBoEk5Uw@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li initially suggested simplifying the swap_tier structure by replacing 'end prio' and priority lists with a standard list_head, but later retracted this suggestion stating that adding tier names alone does not add real value.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "retraction of previous suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Just take a quick look at the series. I take that suggestion back.\nThis series is actually not too long. Adding the tiers name alone does\nnot add any real value. I actually need to look at the whole series\nrather than just the tier name alone.\n\nChris",
              "reply_to": "",
              "message_date": "2026-02-12",
              "message_id": "CACePvbUicBa5Oh4Vz4qX=SV3M3CegCgSJ2GjogN6Cbrkkc-uwQ@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham",
              "summary": "Reviewer Nhat Pham questioned the continued mention of '+' in the documentation, pointing out that it has been removed from the code.\n\nReviewer Nhat Pham noted that the patch description's explanation of cgroup hierarchy principle is unclear, suggesting a simpler approach where child's allowed swap tiers are a subset of its ancestors.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "confusion",
                "questioning",
                "requested clarification",
                "suggested alternative"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This comment seems a bit clunky to me. The \"+\" is removed, as noted\nabove, but then why are we saying \"even if a child re-enables a tier\nwith \"+\"\" here? Am I missing something?\n\n---\n\nBut otherwise, I assume you mean to restrict child's allowed swap\ntiers to be a subset of children and its ancestors? That seems more\nstraightforward to me than the last system :)",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "message_id": "CAKEwX=M5nH3=aqSLybCfLrtScpYKz+jRWt3JYG7im70DCoyjJg@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt noted that the patch does not handle the case where a child cgroup re-enables a tier with '+' that was excluded by its parent, and requested that the effective tier list be limited to the intersection of the parent's allowed subset and the child's selection.\n\nReviewer Shakeel Butt noted that adding a memcg interface for the Swap Tiers functionality is not necessary, suggesting instead that BPF could be used to achieve this goal. He expressed skepticism about allowing workloads to choose swap devices and believes this decision should be made by the job orchestator or node controller.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative approach"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Youngjun,\n\nOn Mon, Jan 26, 2026 at 03:52:37PM +0900, Youngjun Park wrote:\n\n---\n\nOne of the LPC feedback you missed is to not add memcg interface for\nthis functionality and explore BPF way instead.\n\nWe are normally very conservative to add new interfaces to cgroup.\nHowever I am not even convinced that memcg interface is the right way to\nexpose this functionality. Swap is currently global and the idea to\nlimit or assign specific swap devices to specific cgroups makes sense\nbut that is the decision for the job orchestator or node controller.\nAllowing workloads to pick and choose swap devices do not make sense to\nme.\n\nShakeel",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "message_id": "aY4bQFvpPRWgnOTM@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Chris Li's concern about the patch series being too large by proposing a modified roadmap to break it down into smaller, mergeable steps. The author suggests introducing the swap tier definitions first, followed by advanced control and external integration.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "proposed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Chris,\n\nThank you for the direction.\n\nI agree that breaking the series into smaller, mergeable steps is the\nright approach. However, since introducing the definitions alone might\nlack immediate usage, I propose a slightly\nmodified roadmap to ensure Step 1 demonstrates some value.\n\nHere is the plan I have in mind.\n\n1. Swap Tier Definition & Addition\n   - Introduce the concept, grouping logic, and the 'add' interface.\n   - Value: Enables basic exception handling within the swap device\n     itself using tiers.\n\n2. Advanced Control (Delete/Modify)\n   - Implement logic to remove or update tiers.\n   - Value: Enhances the usability and management of the tiers\n     established in Step 1.\n\n3. External Integration (memcg, bpf etc ... )\n   - Apply swap tiers for broader swap control.\n   - Value: Connects swap tiers to other subsystems like memcg.\n\nDoes this roadmap look reasonable to you? I will proceed with preparing\nthe real patch series based on this structure.\n\nBest regards,\nYoungjun",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "message_id": "aY6FiohercUYKyd6@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged the need to limit swap file size, agreed to add a configuration option to control this in the patch.\n\nAuthor addressed Chris Li's feedback on mixed operations in /sys/kernel/mm/swap/tiers, proposed restricting the interface to single operations due to error-prone and slow reversal of individual operations upon failure, and asked for reviewer thoughts.\n\nAuthor acknowledged that limiting contention to objects within the same tier is a benefit of the patch, and expressed agreement with the reviewer's suggestion.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged",
                "agreed",
                "asked_for_clarification",
                "proposed_alternative",
                "agreement",
                "acknowledgment"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sounds good. I will add a CONFIG option for it and ensure it doesn't exceed\nMAX_SWAPFILE.\n\n---\n\nI have given this a lot of thought.\n\nSince the current interface allows mixing add (+), remove (-), and modify\noperations, we must either restore from a saved state or reverse the\nsuccessful individual operations upon failure.\n\nI implemented both approaches and concluded that reversing individual\noperations is error-prone. Also, it could be slow if there are many\noperations.\n\nAnother approach could be using a \"global clone tier\" strategy.\n(Because operation globally synchronized)\n\nTherefore, I would like to propose restricting the interface to handle a\nsingle operation at a time. What do you think?\n\n---\n\nI agree. With the tier structure, we can limit contention to objects within\nthe same tier.\n\nI also think swap_avail_list could be optimized in a similar way in the\nfuture.\n\nYoungjun",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "message_id": "aY6J3Yky6yfcIf36@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledges that stripping out the remove/modify parts from the patch is a feasible direction, asking reviewer Chris Li for agreement on this approach.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying question",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oops, I replied to your previous email before seeing this one.\n\nStripping out the remove/modify parts is also feasible. Do you agree with\nthat direction?\n\nYoungjun",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "message_id": "aY6Ly/0OcWFJEQ1M@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Nhat Pham's concern about the default state of swap tiers and how '+' is interpreted, explaining that they are changing the model to a subtraction-based one where all tiers are selected by default and users use '-' to exclude specific ones.\n\nAuthor acknowledged a concern about the swapoff path and agreed to restructure it in v2.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation",
                "acknowledged",
                "agreed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "To clarify, previously, the default state used all tiers. Using \"+\"              \nswitched to \"an exclusive mode\"  where only that specific tier was used.         \n                                                                                 \nI am changing this to a subtraction-based model. By default, all tiers           \nare selected, and users use \"-\" to exclude specific ones.                        \n(Then not \"removed\" but \"changed\" is more proper?)                               \n                                                                                 \nIn this context, I intended \"+\" to be used to restore a tier that was            \npreviously excluded by \"-\".\n\n---\n\nYes, that's right :)\n\nThanks \nYoungjun Park.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-13",
              "message_id": "aY6P2ULxocDT7HV/@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed a concern about using the BPF approach for swap device assignment, citing potential logical contradictions and hierarchy enforcement issues. They acknowledged the flexibility of BPF but expressed concerns that it would eliminate its primary advantage if strictly constrained to adhere to cgroup semantics. Instead, they prefer implementing the swap tier mechanism within the standard 'cgroup land' to ensure consistent enforcement of hierarchy and accounting rules.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "expressed concerns about BPF approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Apologies for overlooking the feedback regarding the BPF approach. Thank you\nfor the suggestion.\n\nI agree that using BPF would provide greater flexibility, allowing control not\njust at the memcg level, but also per-process or for complex workloads.\n(As like orchestrator and node controller)\n\nHowever, I am concerned that this level of freedom might introduce logical\ncontradictions, particularly regarding cgroup hierarchy semantics.\n\nFor example, BPF might allow a topology that violates hierarchical constraints\n(a concern that was also touched upon during LPC)\n\n  - Group A (Parent): Assigned to SSD1\n  - Group B (Child of A): Assigned to SSD2\n\nIf Group A has a `memory.swap.max` limit, and Group B swaps out to SSD2, it\ncreates a consistency issue. Group B consumes Group A's swap quota, but it is\nutilizing a device (SSD2) that is distinct from the Parent's assignment. This\ncould lead to situations where the Parent's limit is exhausted by usage on a\ndevice it effectively doesn't \"own\" or shouldn't be using.\n\nOne might suggest restricting BPF to strictly adhere to these hierarchical\nconstraints. However, doing so would effectively eliminate the primary\nadvantage of using BPF\\u2014its flexibility. If we are to enforce standard cgroup\nsemantics anyway, a native interface seems more appropriate than a constrained\nBPF hook.\n\nBeyond this specific example, I suspect that delegating this logic to BPF\nmight introduce other unforeseen edge cases regarding hierarchy enforcement.\nIn my view, the BPF approach seems more like a \"next step.\"\n\nSince you acknowledged that the idea of assigning swap devices to cgroups\n\"makes sense,\" I believe implementing this within the standard, strictly\nconstrained \"cgroup land\" is preferable. \n\nA strict cgroup interface ensures\nthat hierarchy and accounting rules are consistently enforced, avoiding the\npotential conflicts that the unrestricted freedom of BPF might create.\n\nUltimately, I hope this swap tier mechanism can serve as a foundation to be\nleveraged by other subsystems, such as BPF and DAMON. I view this proposal as\nthe necessary first step toward that future.\n\nYoungjun Park",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-13",
              "message_id": "aY6hcPNxiolf5jj6@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged that existing swapfiles' tier assignment is immutable and addressed the issue by removing tier reference, instead using operation-time validation to ensure invariant.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "promised a revised approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I missed one comment. \n\nThe tier of existing swapfiles is immutable once assigned at swapon.\nI removed tier reference.\nInstead of reference counting, each operation validates the tier\nrange at operation time to guarantee this invariant.\n\n- add:    Does not change existing swapfiles' tier. New tier may\n          split priority range, but existing assignments stay.\n- remove: Rejected with -EBUSY if any swapfile is attached.\n- modify: Rejected if the change would cause any swapfile to\n          move to a different tier.\n\nSo swapfiles never jump between tiers at runtime.\n\nYoungjun Park",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "message_id": "aY82PzT1GSfmznTv@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer requested that further discussion be concluded on the previous patch series before a new version is submitted.\n\nReviewer Shakeel Butt expressed concerns about introducing stable interfaces for swap tiers, requesting a proof-of-concept using BPF before implementing hierarchical control\n\nReviewer Shakeel Butt noted that while BPF offers more flexibility, its control is limited to administrators who may inadvertently cause issues.\n\nReviewer Shakeel Butt requested clarification on the patch's use case, specifically asking about ordering policies between multiple assigned swap devices, the reason for using 'tiers' in the name, and how workloads partition/divide assigned devices to sub-workloads. He suggested brainstorming future use-cases to create a more future-proof solution.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "request for conclusion of previous discussion",
                "requested changes",
                "not convinced",
                "concerns about admin control",
                "requested clarification",
                "suggested brainstorming"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Please don't send a new version of the series before concluding the discussion\non the previous one.\n\nOn Fri, Feb 13, 2026 at 12:58:40PM +0900, YoungJun Park wrote:\n\n---\n\nYes it provides the flexibility but that is not the main reason I am pushing for\nit. The reason I want you to first try the BPF approach without introducing any\nstable interfaces. Show how swap tiers will be used and configured in production\nenvironment and then we can talk if a stable interface is needed. I am still not\nconvinced that swap tiers need to be controlled hierarchically and the non-root\nshould be able to control it.\n\n---\n\nYes BPF provides more power but it is controlled by admin and admin can shoot\ntheir foot in multiple ways.\n\n---\n\nNo need to constraint anything.\n\nTaking a step back, can you describe your use-case a bit more and share\nrequirements?\n\nYou have multiple swap devices of different properties and you want to assign\nthose swap devices to different workloads. Now couple of questions:\n\n1. If more than one device is assign to a workload, do you want to have\n   some kind of ordering between them for the worklod or do you want option to\n   have round robin kind of policy?\n\n2. What's the reason to use 'tiers' in the name? Is it similar to memory tiers\n   and you want promotion/demotion among the tiers?\n\n3. If a workload has multiple swap devices assigned, can you describe the\n   scenario where such workloads need to partition/divide given devices to their\n   sub-workloads?\n\nLet's start with these questions. Please note that I want us to not just look at\nthe current use-case but brainstorm more future use-cases and then come up with\nthe solution which is more future proof.\n\nthanks,\nShakeel",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-20",
              "message_id": "aZjxP2sTavBRGC1l@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the discussion on the patch had been ongoing for over a week without a response from YoungJun, and suggested that the reviewer should not take it as concluded, considering the difficulty of contributing to the kernel and the prevalence of differing opinions on the mailing list.\n\nReviewer Chris Li suggested that instead of blocking the patch, a config option could be added to protect the problematic feature and mark it as experimental, allowing for further testing and feedback.\n\nReviewer Chris Li confirmed their company's use case for controlling swap devices at different cgroup levels, emphasizing it as a real need.\n\nReviewer Chris Li noted that the swap device control concept in this patch is not generic enough, similar to zswap.writeback, and suggested that it could be improved further.\n\nChris Li expressed frustration about the lack of a long-standing thread on the linux-mm mailing list, which he believes would be helpful for reviewing the patch, and suggested that Shakeel Butt's team could share their internal cgroup swapfile control interface to simplify upstreaming.\n\nReviewer Chris Li noted that the swap tier system's performance is dependent on the number of devices within each tier, and suggested using a round-robin approach to distribute swap operations across devices within the same tier.\n\nReviewer Chris Li suggested renaming the 'tier' concept to something like 'swap.device_speed_classes', indicating that it's a distinct classification of swap speeds, and proposed alternative names.\n\nChris Li expressed concern that the patch does not account for multiple swap devices being used in a deployment, which can lead to lock contention and impact performance.\n\nReviewer Chris Li expressed a nuanced opinion on the patch, suggesting that starting from current needs and incremental improvements is a better approach than designing for future-proofing. He acknowledged the need to allow different cgroups to select different swap speeds but cautioned against over-engineering.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK",
                "requested changes",
                "suggested alternative solution",
                "acknowledgment of specific use case",
                "frustration",
                "suggestion",
                "performance concern",
                "requested change",
                "open-ended question",
                "lock contention",
                "performance impact",
                "incremental progress",
                "future-proofing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "In this case I think it is fine.  You haven't responded to YoungJun's\nlast response in over a week. He might have mistaken that the\ndiscussion concluded.\nConsider it is one of the iterations. It is hard enough to contribute\nto the kernel. Relax.\nPlus, much of the discussion on the mailing list always has differing\nopinions. So, it's hard to determine what is truly concluded.\nDifferent people might have different interitations of the same text.\n\n---\n\nIs that your biggest concern? Many different ways exist to solve that\nproblem. e.g. We can put a config option protecting it and mark it as\nexperimental. This will unblock the development allow experiment. We\ncan have more people to try it out and give feedback.\n\n---\n\nYes, my company uses a different swap device at different cgroup\nlevel. I did ask my coworker to confirm that usage. Control at the non\nroot level is a real need.\n\n---\n\nI think this swap device control is a very basic need. All your\nobjections to swapping control in the group can equally apply to\nzswap.writeback. Unlike zswap.writeback, which only control from the\nzswap behavior. This is a more generic version control swap device\nother than zswap as well. BTW, I raised that concern about\nzswap.writeback was not generic enough as swap control was limited\nwhen zswap was proposed. We did hold back zswap.writeback. The\nconsensers is interface can be improved as later iterations. So here\nwe are.\n\n---\n\nThere is a very long thread on the linux-mm maillist. I'm too lazy to dig it up.\n\nI can share our usage requirement to refresh your memory. We\ninternally use a cgroup swapfile control interface that has not been\nupstreamed. With this we can remove the need of that internal\ninterface and go upstream instead.\n\n---\n\nIt depends on the number of devices in the tiers. Different tiers\nmaintain an order. Within the same tier round robin.\n\n---\n\nI propose the tier name. Guilty. Yes, in was inpired by memory tiers.\nIt just different class of swap speeds. I am not fixed on the name. We\ncan also call it swap.device_speed_classes. You can suggest\nalternatives.\n\nPromotion / demotion is possible in the future. The current state,\nwithout promotion or demotion, already provides value. Our current\ndeployment uses only one class of swap device at a time. However I do\nknow other companies use  more than one class of swap device.\n\n---\n\nIn our deployment, we always use more than one swap device to reduce\nswap device lock contention.\nThe job config can describe the swap speed it can tolerate. Some jobs\ncan tolerate slower speeds, while others cannot.\n\n---\n\nTake zswap.writeback as example. We have a solution that worked for\nthe requirement at that time. Incremental improvement is fine as well.\nUsually, incremental progress is better. At least currently there is a\nreal need to allow different cgroups to select different swap speeds.\nThere is a risk in being too future-proof: we might design things that\npeople in the future don't use as we envisioned. I see that happen too\noften as well.\n\nSo starting from the current need is a solid starting point. It's just\na different design philosophy. Each to their own.\n\nThat is the only usage case I know. YoungJun feel free to add yours\nusage as well.\n\nChris",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "message_id": "CACePvbU=4f4gT5kHUBq0wD7COHN+quE5g4bPQqJYgJNx_9vuhg@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged that they need to address concerns about swap device cluster allocation rule preservation and promised to restructure in v2\n\nAuthor acknowledged a concern about committing to a stable interface too early and proposed reducing the risk by adding a build-time config option or marking it as experimental, but also expressed uncertainty about whether a memcg interface would be needed if BPF becomes primary control mechanism.\n\nAuthor acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), but did not explicitly state a plan for addressing this issue in v2.\n\nAuthor suggests that introducing build-time config or runtime constraints can help define and predict usage of the swap tiers feature.\n\nAuthor addressed Shakeel Butt's concern about the patch's ability to handle complex use cases by explaining their initial proposal for per-cgroup swap device priorities and how they pivoted to the 'swap tier' mechanism due to its simplicity.\n\nThe author addressed Shakeel Butt's feedback about the swap device selection policy by explaining that if devices are in the same tier with the same priority, round robin is used, and if they have different priorities or are in different tiers, ordering applies.\n\nAuthor acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, but did not provide a specific plan for addressing this issue.\n\nAuthor addressed Shakeel Butt's concern about reducing lock contention, suggesting a possible solution of partitioning swap devices between parent and child cgroups.\n\nAuthor addressed Shakeel Butt's concern about the patch's long-term maintainability by suggesting that a CONFIG option would allow them to move forward without committing to a stable interface, and they see BPF as a natural extension path for future use cases.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged",
                "promised",
                "uncertainty",
                "concerns",
                "did not explicitly state a plan",
                "clarification",
                "explaining",
                "explanation",
                "acknowledged a fix is needed",
                "agreed to restructure",
                "partitioning swap devices",
                "possible scenario",
                "author acknowledges a fix is needed",
                "author suggests a temporary solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Understood. Let's continue the discussion. :D\n\nChris has already provided a thorough response, but I would like to\nadd my perspective as well.\n\n---\n\nI understand your concern about committing to a stable interface too\nearly. As Chris suggested, we could reduce this concern by guarding\nthe interface behind a build-time config option or marking it as\nexperimental, which I will also touch on further below.\n\nOn that note, if BPF were to become the primary control mechanism,\nI am not sure a memcg interface would still be needed at all, since\nBPF already provides a high degree of freedom. However, that level\nof freedom is also what concerns me -- BPF-driven swap device\nassignments could subtly conflict with memcg hierarchy semantics in\nways that are hard to predict or debug. A more constrained memcg-based\napproach might actually be safer in that regard.\n\n---\n\nI think this concern is closely tied to your question #3 below about\nconcrete use cases for partitioning devices across sub-workloads.\nI hope my answer there helps clarify this.\n\n---\n\nAs I mentioned above, I think guarding the feature behind a build-time\nconfig or runtime constraints could keep the usage well-defined and\npredictable, while still being useful.\n\n---\n\nOur use case is simple at now. \nWe have two swap devices with different performance\ncharacteristics and want to assign different swap devices to different\nworkloads (cgroups).\n\nFor some background, when I initially proposed this, I suggested allowing\nper-cgroup swap device priorities so that it could also accommodate the\nbroader scenarios you mentioned. However, since even our own use case\ndoes not require reversing swap priorities within a cgroup, we pivoted\nto the \"swap tier\" mechanism that Chris proposed.\n\n---\n\nBoth. If devices are in the same tier with the same priority, round robin.\nIf they are in the same tier with different priorities, or in different\ntiers, ordering applies. The current tier structure should be able to\nsatisfy either preference.\n\n---\n\nThis was originally Chris's idea. I think he explained the rationale\nwell in his reply.\n\n---\n\nOne possible scenario is reducing lock contention by partitioning swap\ndevices between parent and child cgroups.\n\n---\n\nWe have clear production use cases from both us and Chris, and I also\npresented a deployment example in the cover letter.\n\nI think it is hard to design concretely for future use cases at this\npoint. When those needs become clearer, BPF with its flexibility\nwould be a better fit then. I see BPF as a natural extension path\nrather than a starting point.\n\nFor now, guarding the memcg & tier behind a CONFIG option would\nlet us move forward without committing to a stable interface, and\nwe can always pivot to BPF later if needed\n\nThanks,\nYoungJun Park",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "message_id": "aZnBo+P3ifskts9J@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed concerns about the practical application of the patch, stating that he doesn't see the real use-case for controlling/partitioning swap devices among sub-workloads and believes adding a stable API is premature.\n\nReviewer Shakeel Butt questioned whether the patch's concept of swap tiers is a new feature or a reimplementation of an existing interface, specifically referencing Google's prodkernel team and their use of memory.swapfiles.\n\nReviewer Shakeel Butt expressed skepticism about the introduction of a new swap tier interface without a clear use case, prompting him to push back even harder on its addition.\n\nReviewer Shakeel Butt questioned the practicality of hierarchical swap device control, specifically asking for a real-world use case to justify this feature.\n\nReviewer Shakeel Butt noted that having multiple swap devices to reduce lock contention does not address the issue of controlling swap devices among sub-workloads, which is the purpose of the 'Swap Tiers' concept.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "uncertainty",
                "lack of clarity",
                "skepticism",
                "pushback"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, that is secondary because I am not seeing the real use-case of\ncontrolling/partitioning swap devices among sub-workloads. Until that is\nfigured out, adding a stable API is not good.\n\n---\n\nI am assuming you meant Google and particularly Prodkernel team and not\nAndroid or ChromeOS. Google's prodkernel used to have per-cgroup\nswapfiles exposed through memory.swapfiles (if I remember correctly\nSuleiman implemented this along with ghost swapfiles). Later this was\ndeprecated (by Yu Zhao) and global (ghost) swapfiles were being used.\nThe memory.swapfiles interface instead of supporting real swapfiles\nstarted having select options among default, ghost/zswap and real\n(something like that). However such interface was used to just disable\nor enable zswap for a workload and never about hierarchically\ncontrolling the swap devices (Google prodkernel only have zswap). Has\nsomething changed?\n\n---\n\nThis just motivates me to pushback even harder on adding a new interface\nwithout a clear use-case.\n\n---\n\nI already asked above but let me say it again. What's the actual real\nworld use-case to control/allow/disallow swap devices hierarchically?\n\n---\n\nHaving more than one swap devices to reduce lock contention is unrelated\nto hierarchically control swap devices among sub-workloads.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-21",
              "message_id": "20260221163043.GA35350@shakeel.butt@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author is addressing Shakeel Butt's feedback about the BPF-first approach, specifically questioning its practicality and feasibility in an embedded environment where kernel compile options are costly. Author asks clarifying questions and expresses difficulty understanding the motivation behind this approach.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "author is asking for clarification",
                "author is expressing skepticism"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "....\n\nAfter reading the reply and re-think more of it.\n\nI have a few questions regarding the BPF-first approach you\nsuggested, if you don't mind. Some of them I am re-asking\nbecause I feel they have not been clearly addressed yet.\n\n- We are in an embedded environment where enabling additional\n  kernel compile options is costly. BPF is disabled by\n  default in some of our production configurations. From a\n  trade-off perspective, does it make sense to enable BPF\n  just for swap device control?\n\n- You suggest starting with BPF and discussing a stable\n  interface later. I am genuinely curious, are there actual\n  precedents where a BPF prototype graduated into a stable\n  kernel interface? \n\n- You raised that stable interfaces are hard to remove. Would\n  gating it behind a CONFIG option or marking it experimental\n  be an acceptable compromise?\n\n- You already acknowledged the use-case for assigning\n  different swap devices to different workloads. Your\n  objection is specifically about hierarchical parent-child\n  partitioning. If the interface enforced uniform policy\n  within a subtree, would that be acceptable?\n\n- We already run a modified kernel with internal swap control\n  in production and have real feedback from it. Requiring BPF\n  as a prerequisite to gather production experience seems\n  unnecessary when we are already doing that.\n\nTo be honest, I am having trouble understanding the motivation\nbehind the BPF-first validation approach. If the real point is\nthat BPF enables more flexible swap-out policies than any fixed\ninterface can, that would make much more sense to me. I would\nappreciate it if you could share more on this.\n\nThanks,\nYoungjun Park",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-22",
              "message_id": "aZpY1FIjYLtLdu5F@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that the patch does not handle the case where a child cgroup re-enables a tier with '+' that was excluded by its parent, and requested clarification on how this is handled.\n\nReviewer Shakeel Butt requested additional information about the cgroup hierarchy structure of Youngjun Park's deployment, specifically asking if they use cgroup v1 or v2 in their production environment.\n\nReviewer Shakeel Butt questioned whether the proposed swap tiers concept is identical to existing swap priority behavior and requested clarification on this point.\n\nReviewer Shakeel Butt suggested that the patch should not be committed without first brainstorming all options and gathering their pros and cons, requesting an informed decision.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested clarification",
                "handling of tier selection",
                "request for clarification",
                "lack of technical disagreement",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi YoungJun,\n\nI see you have sent a separate email on BPF specific questions to which I will\nrespond separately, here I will respond to other questions/comments.\n\nOn Sat, Feb 21, 2026 at 11:30:59PM +0900, YoungJun Park wrote:\n\n---\n\nIf you don't mind, can you share a bit more about the cgroup hierarchy structure\nof your deployment. Do you use cgroup v1 or v2 on your production environment?\n\n---\n\nI assume this is the same swap priorities as of today, right? You want similar\npriority behavior within a tier.\n\n---\n\nI think your use-case is very clear. Before committing to any options, I want us\nto brainstorm all options and gather pros/cons and then make an informed\ndecision. Anyways I will respond to your other email (in a day or two).\n\nShakeel",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-22",
              "message_id": "aZvX0HZy1PDylL8A@linux.dev",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH v2 0/5] mm/swap, memcg: Introduce swap tiers for cgroup based swap control",
          "message_id": "aZjxP2sTavBRGC1l@linux.dev",
          "url": "https://lore.kernel.org/all/aZjxP2sTavBRGC1l@linux.dev/",
          "date": "2026-02-21T03:47:40Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about the interaction between cgroup hierarchy and swap tier selection, explaining that effective tiers are calculated separately using a dedicated mask to respect the cgroup hierarchy. They noted that configured tiers may differ from effective ones as they must be a subset of the parent's.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch integrates the swap tier infrastructure with cgroup,\nenabling the selection of specific swap devices per cgroup by\nconfiguring allowed swap tiers.\n\nThe new `memory.swap.tiers` interface controls allowed swap tiers via a mask.\nBy default, the mask is set to include all tiers, allowing specific tiers to\nbe excluded or restored. Note that effective tiers are calculated separately\nusing a dedicated mask to respect the cgroup hierarchy. Consequently,\nconfigured tiers may differ from effective ones, as they must be a subset\nof the parent's.\n\nNote that cgroups do not pin swap tiers. This is similar to the\n`cpuset` controller, which does not prevent CPU hotplug. This\napproach ensures flexibility by allowing tier configuration changes\nregardless of cgroup usage.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n Documentation/admin-guide/cgroup-v2.rst | 27 +++++++++\n include/linux/memcontrol.h              |  3 +-\n mm/memcontrol.c                         | 80 +++++++++++++++++++++++++\n mm/swap_tier.c                          | 66 ++++++++++++++++++++\n mm/swap_tier.h                          | 21 +++++++\n mm/swapfile.c                           |  5 ++\n 6 files changed, 201 insertions(+), 1 deletion(-)\n\ndiff --git a/Documentation/admin-guide/cgroup-v2.rst b/Documentation/admin-guide/cgroup-v2.rst\nindex 7f5b59d95fce..776a908ce1b9 100644\n--- a/Documentation/admin-guide/cgroup-v2.rst\n+++ b/Documentation/admin-guide/cgroup-v2.rst\n@@ -1848,6 +1848,33 @@ The following nested keys are defined.\n \tSwap usage hard limit.  If a cgroup's swap usage reaches this\n \tlimit, anonymous memory of the cgroup will not be swapped out.\n \n+  memory.swap.tiers\n+        A read-write nested-keyed file which exists on non-root\n+        cgroups. The default is to enable all tiers.\n+\n+        This interface allows selecting which swap tiers a cgroup can\n+        use for swapping out memory.\n+\n+        The effective tiers are inherited from the parent. Only tiers\n+        effective in the parent can be effective in the child. However,\n+        the child can explicitly disable tiers allowed by the parent.\n+\n+        When read, the file shows two lines:\n+          - The first line shows the operation string that was\n+            written to this file.\n+          - The second line shows the effective operation after\n+            merging with parent settings.\n+\n+        When writing, the format is:\n+          (+/-)(TIER_NAME) (+/-)(TIER_NAME) ...\n+\n+        Valid tier names are those configured in\n+        /sys/kernel/mm/swap/tiers.\n+\n+        Each tier can be prefixed with:\n+          +    Enable this tier\n+          -    Disable this tier\n+\n   memory.swap.events\n \tA read-only flat-keyed file which exists on non-root cgroups.\n \tThe following entries are defined.  Unless specified\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex b6c82c8f73e1..542bee1b5f60 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -283,7 +283,8 @@ struct mem_cgroup {\n \t/* per-memcg mm_struct list */\n \tstruct lru_gen_mm_list mm_list;\n #endif\n-\n+\tint tier_mask;\n+\tint tier_effective_mask;\n #ifdef CONFIG_MEMCG_V1\n \t/* Legacy consumer-oriented counters */\n \tstruct page_counter kmem;\t\t/* v1 only */\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 007413a53b45..c0a0a957a630 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -68,6 +68,7 @@\n #include <net/ip.h>\n #include \"slab.h\"\n #include \"memcontrol-v1.h\"\n+#include \"swap_tier.h\"\n \n #include <linux/uaccess.h>\n \n@@ -3691,6 +3692,7 @@ static void mem_cgroup_free(struct mem_cgroup *memcg)\n {\n \tlru_gen_exit_memcg(memcg);\n \tmemcg_wb_domain_exit(memcg);\n+\tswap_tiers_memcg_sync_mask(memcg);\n \t__mem_cgroup_free(memcg);\n }\n \n@@ -3792,6 +3794,9 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \tWRITE_ONCE(memcg->zswap_writeback, true);\n #endif\n \tpage_counter_set_high(&memcg->swap, PAGE_COUNTER_MAX);\n+\tmemcg->tier_mask = TIER_ALL_MASK;\n+\tswap_tiers_memcg_inherit_mask(memcg, parent);\n+\n \tif (parent) {\n \t\tWRITE_ONCE(memcg->swappiness, mem_cgroup_swappiness(parent));\n \n@@ -5352,6 +5357,75 @@ static int swap_events_show(struct seq_file *m, void *v)\n \treturn 0;\n }\n \n+static int swap_tier_show(struct seq_file *m, void *v)\n+{\n+\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);\n+\n+\tswap_tiers_mask_show(m, memcg->tier_mask);\n+\tswap_tiers_mask_show(m, memcg->tier_effective_mask);\n+\n+\treturn 0;\n+}\n+\n+static ssize_t swap_tier_write(struct kernfs_open_file *of,\n+\t\t\t\tchar *buf, size_t nbytes, loff_t off)\n+{\n+\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n+\tchar *pos, *token;\n+\tint ret = 0;\n+\n+\tpos = strstrip(buf);\n+\n+\tspin_lock(&swap_tier_lock);\n+\tif (!*pos) {\n+\t\tmemcg->tier_mask = TIER_ALL_MASK;\n+\t\tgoto sync;\n+\t}\n+\n+\twhile ((token = strsep(&pos, \" \\t\\n\")) != NULL) {\n+\t\tint mask;\n+\n+\t\tif (!*token)\n+\t\t\tcontinue;\n+\n+\t\tif (token[0] != '-' && token[0] != '+') {\n+\t\t\tret = -EINVAL;\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\tmask = swap_tiers_mask_lookup(token+1);\n+\t\tif (!mask) {\n+\t\t\tret = -EINVAL;\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\t/*\n+\t\t * if child already set, cannot add that tiers for hierarch mismatching.\n+\t\t * parent compatible, child must respect parent selected swap device.\n+\t\t */\n+\t\tswitch (token[0]) {\n+\t\tcase '-':\n+\t\t\tmemcg->tier_mask &= ~mask;\n+\t\t\tbreak;\n+\t\tcase '+':\n+\t\t\tmemcg->tier_mask |= mask;\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tret = -EINVAL;\n+\t\t\tbreak;\n+\t\t}\n+\n+\t\tif (ret)\n+\t\t\tgoto err;\n+\t}\n+\n+sync:\n+\t__swap_tiers_memcg_sync_mask(memcg);\n+err:\n+\tspin_unlock(&swap_tier_lock);\n+\treturn ret ? ret : nbytes;\n+}\n+\n static struct cftype swap_files[] = {\n \t{\n \t\t.name = \"swap.current\",\n@@ -5384,6 +5458,12 @@ static struct cftype swap_files[] = {\n \t\t.file_offset = offsetof(struct mem_cgroup, swap_events_file),\n \t\t.seq_show = swap_events_show,\n \t},\n+\t{\n+\t\t.name = \"swap.tiers\",\n+\t\t.flags = CFTYPE_NOT_ON_ROOT,\n+\t\t.seq_show = swap_tier_show,\n+\t\t.write = swap_tier_write,\n+\t},\n \t{ }\t/* terminate */\n };\n \ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nindex d90f6eccb908..e860c87292e2 100644\n--- a/mm/swap_tier.c\n+++ b/mm/swap_tier.c\n@@ -384,3 +384,69 @@ bool swap_tiers_update(void)\n \n \treturn true;\n }\n+\n+void swap_tiers_mask_show(struct seq_file *m, int mask)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tspin_lock(&swap_tier_lock);\n+\tfor_each_active_tier(tier) {\n+\t\tif (mask & TIER_MASK(tier))\n+\t\t\tseq_printf(m, \"%s \", tier->name);\n+\t}\n+\tspin_unlock(&swap_tier_lock);\n+\tseq_puts(m, \"\\n\");\n+}\n+\n+int swap_tiers_mask_lookup(const char *name)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (!strcmp(name, tier->name))\n+\t\t\treturn TIER_MASK(tier);\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static void __swap_tier_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent)\n+{\n+\tint effective_mask\n+\t\t= parent ? parent->tier_effective_mask : TIER_ALL_MASK;\n+\n+\tmemcg->tier_effective_mask\n+\t\t= effective_mask & memcg->tier_mask;\n+}\n+\n+void swap_tiers_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent)\n+{\n+\tspin_lock(&swap_tier_lock);\n+\t__swap_tier_memcg_inherit_mask(memcg, parent);\n+\tspin_unlock(&swap_tier_lock);\n+}\n+\n+void __swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg)\n+{\n+\tstruct mem_cgroup *child;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tif (memcg == root_mem_cgroup)\n+\t\treturn;\n+\n+\tfor_each_mem_cgroup_tree(child, memcg)\n+\t\t__swap_tier_memcg_inherit_mask(child, parent_mem_cgroup(child));\n+}\n+\n+void swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg)\n+{\n+\tspin_lock(&swap_tier_lock);\n+\tmemcg->tier_mask = TIER_ALL_MASK;\n+\t__swap_tiers_memcg_sync_mask(memcg);\n+\tspin_unlock(&swap_tier_lock);\n+}\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nindex de81d540e3b5..8652a7f993ab 100644\n--- a/mm/swap_tier.h\n+++ b/mm/swap_tier.h\n@@ -46,4 +46,25 @@ bool swap_tiers_update(void);\n /* Tier assignment */\n void swap_tiers_assign_dev(struct swap_info_struct *swp);\n \n+/* Memcg related functions */\n+void swap_tiers_mask_show(struct seq_file *m, int mask);\n+void swap_tiers_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent);\n+void swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg);\n+void __swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg);\n+\n+/* Mask and tier lookup */\n+int swap_tiers_mask_lookup(const char *name);\n+\n+/**\n+ * swap_tiers_mask_test - Check if the tier mask is valid\n+ * @tier_mask: The tier mask to check\n+ * @mask: The mask to compare against\n+ *\n+ * Return: true if condition matches, false otherwise\n+ */\n+static inline bool swap_tiers_mask_test(int tier_mask, int mask)\n+{\n+\treturn tier_mask & mask;\n+}\n #endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 4f8ce021c5bd..dd97e850ea2c 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1348,10 +1348,15 @@ static bool swap_alloc_fast(struct folio *folio)\n static void swap_alloc_slow(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n+\tint mask = folio_memcg(folio) ?\n+\t\tfolio_memcg(folio)->tier_effective_mask : TIER_ALL_MASK;\n \n \tspin_lock(&swap_avail_lock);\n start_over:\n \tplist_for_each_entry_safe(si, next, &swap_avail_head, avail_list) {\n+\t\tif (!swap_tiers_mask_test(si->tier_mask, mask))\n+\t\t\tcontinue;\n+\n \t\t/* Rotate the device and switch to a new cluster */\n \t\tplist_requeue(&si->avail_list, &swap_avail_head);\n \t\tspin_unlock(&swap_avail_lock);\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "message_id": "20260126065242.1221862-4-youngjun.park@lge.com",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about tier consistency and how swap devices are assigned to tiers based on their priority. They explained that a `tier_mask` is added to identify the tier membership of swap devices, and upon activation, the device is assigned to a tier by matching its priority against the configured tier ranges. The author also mentioned that dynamic modification of tiers, such as splitting or merging ranges, is permitted provided that the tier assignment of already configured swap devices remains unchanged.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch connects swap devices to the swap tier infrastructure,\nensuring that devices are correctly assigned to tiers based on their\npriority.\n\nA `tier_mask` is added to identify the tier membership of swap devices.\nAlthough tier-based allocation logic is not yet implemented, this\nmapping is necessary to track which tier a device belongs to. Upon\nactivation, the device is assigned to a tier by matching its priority\nagainst the configured tier ranges.\n\nThe infrastructure allows dynamic modification of tiers, such as\nsplitting or merging ranges. These operations are permitted provided\nthat the tier assignment of already configured swap devices remains\nunchanged.\n\nThis patch also adds the documentation for the swap tier feature,\ncovering the core concepts, sysfs interface usage, and configuration\ndetails.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n Documentation/mm/swap-tier.rst | 109 +++++++++++++++++++++++++++++++++\n include/linux/swap.h           |   1 +\n mm/swap_state.c                |   2 +-\n mm/swap_tier.c                 | 106 ++++++++++++++++++++++++++++----\n mm/swap_tier.h                 |  13 +++-\n mm/swapfile.c                  |   2 +\n 6 files changed, 219 insertions(+), 14 deletions(-)\n create mode 100644 Documentation/mm/swap-tier.rst\n\ndiff --git a/Documentation/mm/swap-tier.rst b/Documentation/mm/swap-tier.rst\nnew file mode 100644\nindex 000000000000..3386161b9b18\n--- /dev/null\n+++ b/Documentation/mm/swap-tier.rst\n@@ -0,0 +1,109 @@\n+.. SPDX-License-Identifier: GPL-2.0\n+\n+:Author: Chris Li <chrisl@kernel.org> Youngjun Park <youngjun.park@lge.com>\n+\n+==========\n+Swap Tier\n+==========\n+\n+Swap tier is a collection of user-named groups classified by priority ranges.\n+It acts as a facilitation layer, allowing users to manage swap devices based\n+on their speeds.\n+\n+Users are encouraged to assign swap device priorities according to device\n+speed to fully utilize this feature. While the current implementation is\n+integrated with cgroups, the concept is designed to be extensible for other\n+subsystems in the future.\n+\n+Use case\n+-------\n+\n+Users can perform selective swapping by choosing a swap tier assigned according\n+to speed within a cgroup.\n+\n+For more information on cgroup v2, please refer to\n+``Documentation/admin-guide/cgroup-v2.rst``.\n+\n+Priority Range\n+--------------\n+\n+The specified tiers must cover the entire priority range from -1\n+(DEF_SWAP_PRIO) to SHRT_MAX.\n+\n+Consistency\n+-----------\n+\n+Tier consistency is guaranteed with a focus on maximizing flexibility. When a\n+swap device is activated within a tier range, a reference is held from the\n+start of the tier to the priority of that swap device. This ensures that the\n+tier of region containing the active swap device does not disappear.\n+\n+If a request to add a new tier with a priority higher than the current swap\n+device is received, the existing tier can be split.\n+\n+However, specifying a tier in a cgroup does not hold a reference to the tier.\n+Consequently, the corresponding tier can disappear at any time.\n+\n+Configuration Interface\n+-----------------------\n+\n+The swap tiers can be configured via the following interface:\n+\n+/sys/kernel/mm/swap/tiers\n+\n+Operations can be performed using the following syntax:\n+\n+* Add:    ``+\"<tiername>\":\"<start_priority>\"``\n+* Remove: ``-\"<tiername>\"``\n+* Modify: ``\"<tiername>\":\"<start_priority>\"``\n+\n+Multiple operations can be provided in a single write, separated by spaces (\" \")\n+or commas (\",\").\n+\n+When configuring tiers, the specified value represents the **start priority**\n+of that tier. The end priority is automatically determined by the start\n+priority of the next higher tier. Consequently, adding or modifying a tier\n+automatically adjusts (splits or merges) the ranges of adjacent tiers to\n+ensure continuity.\n+\n+Examples\n+--------\n+\n+**1. Initialization**\n+\n+A tier starting at -1 is mandatory to cover the entire priority range up to\n+SHRT_MAX. In this example, 'HDD' starts at 50, and 'NET' covers the remaining\n+lower range starting from -1.\n+\n+::\n+\n+    # echo \"+HDD:50, +NET:-1\" > /sys/kernel/mm/swap/tiers\n+    # cat /sys/kernel/mm/swap/tiers\n+    Name             Idx   PrioStart   PrioEnd\n+    HDD              0     50          32767\n+    NET              1     -1          49\n+\n+**2. Modification and Splitting**\n+\n+Here, 'HDD' is moved to start at 80, and a new tier 'SSD' is added at 100.\n+Notice how the ranges are automatically recalculated:\n+* 'SSD' takes the top range. Split HDD Tier's range. (100 to SHRT_MAX).\n+* 'HDD' is adjusted to the range between 'NET' and 'SSD' (80 to 99).\n+* 'NET' automatically extends to fill the gap below 'HDD' (-1 to 79).\n+\n+::\n+\n+    # echo \"HDD:80, +SSD:100\" > /sys/kernel/mm/swap/tiers\n+    # cat /sys/kernel/mm/swap/tiers\n+    Name             Idx   PrioStart   PrioEnd\n+    SSD              2     100         32767\n+    HDD              0     80          99\n+    NET              1     -1          79\n+\n+**3. Removal**\n+\n+Tiers can be removed using the '-' prefix.\n+\n+::\n+\n+    # echo \"-SSD,-HDD,-NET\" > /sys/kernel/mm/swap/tiers\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 62fc7499b408..1e68c220a0e7 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -262,6 +262,7 @@ struct swap_info_struct {\n \tstruct percpu_ref users;\t/* indicate and keep swap device valid. */\n \tunsigned long\tflags;\t\t/* SWP_USED etc: see above */\n \tsigned short\tprio;\t\t/* swap priority of this type */\n+\tint tier_mask;\t\t\t/* swap tier mask */\n \tstruct plist_node list;\t\t/* entry in swap_active_head */\n \tsigned char\ttype;\t\t/* strange name for an index */\n \tunsigned int\tmax;\t\t/* extent of the swap_map */\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex f1a7d9cdc648..d46ca61d2e42 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -997,7 +997,7 @@ static ssize_t tiers_store(struct kobject *kobj,\n \t\t\tgoto restore;\n \t}\n \n-\tif (!swap_tiers_validate()) {\n+\tif (!swap_tiers_update()) {\n \t\tret = -EINVAL;\n \t\tgoto restore;\n \t}\ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nindex 87882272eec8..d90f6eccb908 100644\n--- a/mm/swap_tier.c\n+++ b/mm/swap_tier.c\n@@ -14,7 +14,7 @@\n  * @name: name of the swap_tier.\n  * @prio: starting value of priority.\n  * @list: linked list of tiers.\n-*/\n+ */\n static struct swap_tier {\n \tchar name[MAX_TIERNAME];\n \tshort prio;\n@@ -34,6 +34,8 @@ static LIST_HEAD(swap_tier_inactive_list);\n \t(!list_is_first(&(tier)->list, &swap_tier_active_list) ? \\\n \tlist_prev_entry((tier), list)->prio - 1 : SHRT_MAX)\n \n+#define MASK_TO_TIER(mask) (&swap_tiers[__ffs((mask))])\n+\n #define for_each_tier(tier, idx) \\\n \tfor (idx = 0, tier = &swap_tiers[0]; idx < MAX_SWAPTIER; \\\n \t\tidx++, tier = &swap_tiers[idx])\n@@ -55,6 +57,26 @@ static bool swap_tier_is_active(void)\n \treturn !list_empty(&swap_tier_active_list) ? true : false;\n }\n \n+static bool swap_tier_prio_in_range(struct swap_tier *tier, short prio)\n+{\n+\tif (tier->prio <= prio && TIER_END_PRIO(tier) >= prio)\n+\t\treturn true;\n+\n+\treturn false;\n+}\n+\n+static bool swap_tier_prio_is_used(struct swap_tier *self, short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (tier != self && tier->prio == prio)\n+\t\t\treturn true;\n+\t}\n+\n+\treturn false;\n+}\n+\n static struct swap_tier *swap_tier_lookup(const char *name)\n {\n \tstruct swap_tier *tier;\n@@ -67,12 +89,14 @@ static struct swap_tier *swap_tier_lookup(const char *name)\n \treturn NULL;\n }\n \n+\n void swap_tiers_init(void)\n {\n \tstruct swap_tier *tier;\n \tint idx;\n \n \tBUILD_BUG_ON(BITS_PER_TYPE(int) < MAX_SWAPTIER);\n+\tBUILD_BUG_ON(MAX_SWAPTIER > TIER_DEFAULT_IDX);\n \n \tfor_each_tier(tier, idx) {\n \t\tINIT_LIST_HEAD(&tier->list);\n@@ -145,17 +169,35 @@ static struct swap_tier *swap_tier_prepare(const char *name, short prio)\n \treturn tier;\n }\n \n-static int swap_tier_check_range(short prio)\n+static int swap_tier_can_split_range(struct swap_tier *orig_tier,\n+\tshort new_prio)\n {\n+\tstruct swap_info_struct *p;\n \tstruct swap_tier *tier;\n \n \tlockdep_assert_held(&swap_lock);\n \tlockdep_assert_held(&swap_tier_lock);\n \n-\tfor_each_active_tier(tier) {\n-\t\t/* No overwrite */\n-\t\tif (tier->prio == prio)\n-\t\t\treturn -EINVAL;\n+\tplist_for_each_entry(p, &swap_active_head, list) {\n+\t\tif (p->tier_mask == TIER_DEFAULT_MASK)\n+\t\t\tcontinue;\n+\n+\t\ttier = MASK_TO_TIER(p->tier_mask);\n+\t\tif (tier->prio > new_prio)\n+\t\t\tcontinue;\n+\t\t/*\n+                 * Prohibit implicit tier reassignment.\n+                 * Case 1: Prevent orig_tier devices from dropping out\n+                 *         of the new range.\n+                 */\n+\t\tif (orig_tier == tier && (p->prio < new_prio))\n+\t\t\treturn -EBUSY;\n+                /*\n+                 * Case 2: Prevent other tier devices from entering\n+                 *         the new range.\n+                 */\n+\t\telse if (orig_tier != tier && (p->prio >= new_prio))\n+\t\t\treturn -EBUSY;\n \t}\n \n \treturn 0;\n@@ -173,7 +215,10 @@ int swap_tiers_add(const char *name, int prio)\n \tif (swap_tier_lookup(name))\n \t\treturn -EPERM;\n \n-\tret = swap_tier_check_range(prio);\n+\tif (swap_tier_prio_is_used(NULL, prio))\n+\t\treturn -EBUSY;\n+\n+\tret = swap_tier_can_split_range(NULL, prio);\n \tif (ret)\n \t\treturn ret;\n \n@@ -183,7 +228,6 @@ int swap_tiers_add(const char *name, int prio)\n \t\treturn ret;\n \t}\n \n-\n \tswap_tier_insert_by_prio(tier);\n \treturn ret;\n }\n@@ -200,11 +244,18 @@ int swap_tiers_remove(const char *name)\n \tif (!tier)\n \t\treturn -EINVAL;\n \n+\t/* Simulate adding a tier to check for conflicts */\n+\tret = swap_tier_can_split_range(NULL, tier->prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n \tlist_move(&tier->list, &swap_tier_inactive_list);\n \n \t/* Removing DEF_SWAP_PRIO merges into the higher tier. */\n-\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO)\n-\t\tlist_prev_entry(tier, list)->prio = DEF_SWAP_PRIO;\n+\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO) {\n+\t\tlist_last_entry(&swap_tier_active_list, struct swap_tier, list)\n+\t\t\t->prio = DEF_SWAP_PRIO;\n+\t}\n \n \treturn ret;\n }\n@@ -225,7 +276,10 @@ int swap_tiers_modify(const char *name, int prio)\n \tif (tier->prio == prio)\n \t\treturn 0;\n \n-\tret = swap_tier_check_range(prio);\n+\tif (swap_tier_prio_is_used(tier, prio))\n+\t\treturn -EBUSY;\n+\n+\tret = swap_tier_can_split_range(tier, prio);\n \tif (ret)\n \t\treturn ret;\n \n@@ -283,10 +337,27 @@ void swap_tiers_restore(struct swap_tier_save_ctx ctx[])\n \t}\n }\n \n-bool swap_tiers_validate(void)\n+void swap_tiers_assign_dev(struct swap_info_struct *swp)\n {\n \tstruct swap_tier *tier;\n \n+\tlockdep_assert_held(&swap_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (swap_tier_prio_in_range(tier, swp->prio)) {\n+\t\t\tswp->tier_mask = TIER_MASK(tier);\n+\t\t\treturn;\n+\t\t}\n+\t}\n+\n+\tswp->tier_mask = TIER_DEFAULT_MASK;\n+}\n+\n+bool swap_tiers_update(void)\n+{\n+\tstruct swap_tier *tier;\n+\tstruct swap_info_struct *swp;\n+\n \t/*\n \t * Initial setting might not cover DEF_SWAP_PRIO.\n \t * Swap tier must cover the full range (DEF_SWAP_PRIO to SHRT_MAX).\n@@ -300,5 +371,16 @@ bool swap_tiers_validate(void)\n \t\t\treturn false;\n \t}\n \n+\t/*\n+\t * If applied initially, the swap tier_mask may change\n+\t * from the default value.\n+\t */\n+\tplist_for_each_entry(swp, &swap_active_head, list) {\n+\t\t/* Tier is already configured */\n+\t\tif (swp->tier_mask != TIER_DEFAULT_MASK)\n+\t\t\tbreak;\n+\t\tswap_tiers_assign_dev(swp);\n+\t}\n+\n \treturn true;\n }\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nindex 4b1b0602d691..de81d540e3b5 100644\n--- a/mm/swap_tier.h\n+++ b/mm/swap_tier.h\n@@ -14,6 +14,9 @@\n #define MAX_SWAPTIER\t\t8\n #endif\n \n+/* Forward declarations */\n+struct swap_info_struct;\n+\n extern spinlock_t swap_tier_lock;\n \n struct swap_tier_save_ctx {\n@@ -24,6 +27,10 @@ struct swap_tier_save_ctx {\n #define DEFINE_SWAP_TIER_SAVE_CTX(_name) \\\n \tstruct swap_tier_save_ctx _name[MAX_SWAPTIER] = {0}\n \n+#define TIER_ALL_MASK\t\t(~0)\n+#define TIER_DEFAULT_IDX\t(31)\n+#define TIER_DEFAULT_MASK\t(1 << TIER_DEFAULT_IDX)\n+\n /* Initialization and application */\n void swap_tiers_init(void);\n ssize_t swap_tiers_sysfs_show(char *buf);\n@@ -34,5 +41,9 @@ int swap_tiers_modify(const char *name, int prio);\n \n void swap_tiers_save(struct swap_tier_save_ctx ctx[]);\n void swap_tiers_restore(struct swap_tier_save_ctx ctx[]);\n-bool swap_tiers_validate(void);\n+bool swap_tiers_update(void);\n+\n+/* Tier assignment */\n+void swap_tiers_assign_dev(struct swap_info_struct *swp);\n+\n #endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex c27952b41d4f..4f8ce021c5bd 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -2672,6 +2672,8 @@ static void _enable_swap_info(struct swap_info_struct *si)\n \n \t/* Add back to available list */\n \tadd_to_avail_list(si, true);\n+\n+\tswap_tiers_assign_dev(si);\n }\n \n static void enable_swap_info(struct swap_info_struct *si, int prio,\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "message_id": "20260126065242.1221862-3-youngjun.park@lge.com",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about the potential for silent tier disappearance when using '-' to exclude specific tiers, explaining that this is intentional and follows standard cgroup hierarchy principles. The author confirmed that the effective tier list is limited to the parent's allowed subset.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a design decision",
                "explained reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This patch introduces the \"Swap tier\" concept, which serves as an\nabstraction layer for managing swap devices based on their performance\ncharacteristics (e.g., NVMe, HDD, Network swap).\n\nSwap tiers are user-named groups representing priority ranges.\nThese tiers collectively cover the entire priority\nspace from -1 (`DEF_SWAP_PRIO`) to `SHRT_MAX`.\n\nTo configure tiers, a new sysfs interface is exposed at\n`/sys/kernel/mm/swap/tiers`. The input parser evaluates commands from\nleft to right and supports batch input, allowing users to add, remove or\nmodify multiple tiers in a single write operation.\n\nTier management enforces continuous priority ranges anchored by start\npriorities. Operations trigger range splitting or merging, but overwriting\nstart priorities is forbidden. Merging expands lower tiers upwards to\npreserve configured start priorities, except when removing `DEF_SWAP_PRIO`,\nwhich merges downwards.\n\nSuggested-by: Chris Li <chrisl@kernel.org>\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n MAINTAINERS     |   2 +\n mm/Makefile     |   2 +-\n mm/swap.h       |   4 +\n mm/swap_state.c |  70 +++++++++++\n mm/swap_tier.c  | 304 ++++++++++++++++++++++++++++++++++++++++++++++++\n mm/swap_tier.h  |  38 ++++++\n mm/swapfile.c   |   7 +-\n 7 files changed, 423 insertions(+), 4 deletions(-)\n create mode 100644 mm/swap_tier.c\n create mode 100644 mm/swap_tier.h\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 18d1ebf053db..501bf46adfb4 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16743,6 +16743,8 @@ F:\tmm/swap.c\n F:\tmm/swap.h\n F:\tmm/swap_table.h\n F:\tmm/swap_state.c\n+F:\tmm/swap_tier.c\n+F:\tmm/swap_tier.h\n F:\tmm/swapfile.c\n \n MEMORY MANAGEMENT - THP (TRANSPARENT HUGE PAGE)\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 53ca5d4b1929..3b3de2de7285 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -75,7 +75,7 @@ ifdef CONFIG_MMU\n \tobj-$(CONFIG_ADVISE_SYSCALLS)\t+= madvise.o\n endif\n \n-obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o\n+obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o swap_tier.o\n obj-$(CONFIG_ZSWAP)\t+= zswap.o\n obj-$(CONFIG_HAS_DMA)\t+= dmapool.o\n obj-$(CONFIG_HUGETLBFS)\t+= hugetlb.o hugetlb_sysfs.o hugetlb_sysctl.o\ndiff --git a/mm/swap.h b/mm/swap.h\nindex bfafa637c458..55f230cbe4e7 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -16,6 +16,10 @@ extern int page_cluster;\n #define swap_entry_order(order)\t0\n #endif\n \n+#define DEF_SWAP_PRIO  -1\n+\n+extern spinlock_t swap_lock;\n+extern struct plist_head swap_active_head;\n extern struct swap_info_struct *swap_info[];\n \n /*\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 6d0eef7470be..f1a7d9cdc648 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -25,6 +25,7 @@\n #include \"internal.h\"\n #include \"swap_table.h\"\n #include \"swap.h\"\n+#include \"swap_tier.h\"\n \n /*\n  * swapper_space is a fiction, retained to simplify the path through\n@@ -947,8 +948,77 @@ static ssize_t vma_ra_enabled_store(struct kobject *kobj,\n }\n static struct kobj_attribute vma_ra_enabled_attr = __ATTR_RW(vma_ra_enabled);\n \n+static ssize_t tiers_show(struct kobject *kobj,\n+\t\t\t\t     struct kobj_attribute *attr, char *buf)\n+{\n+\treturn swap_tiers_sysfs_show(buf);\n+}\n+\n+static ssize_t tiers_store(struct kobject *kobj,\n+\t\t\tstruct kobj_attribute *attr,\n+\t\t\tconst char *buf, size_t count)\n+{\n+\tchar *p, *token, *name, *tmp;\n+\tint ret = 0;\n+\tshort prio;\n+\tDEFINE_SWAP_TIER_SAVE_CTX(ctx);\n+\n+\ttmp = kstrdup(buf, GFP_KERNEL);\n+\tif (!tmp)\n+\t\treturn -ENOMEM;\n+\n+\tspin_lock(&swap_lock);\n+\tspin_lock(&swap_tier_lock);\n+\n+\tp = tmp;\n+\tswap_tiers_save(ctx);\n+\n+\twhile (!ret && (token = strsep(&p, \", \\t\\n\")) != NULL) {\n+\t\tif (!*token)\n+\t\t\tcontinue;\n+\n+\t\tif (token[0] == '-') {\n+\t\t\tret = swap_tiers_remove(token + 1);\n+\t\t} else {\n+\n+\t\t\tname = strsep(&token, \":\");\n+\t\t\tif (!token || kstrtos16(token, 10, &prio)) {\n+\t\t\t\tret = -EINVAL;\n+\t\t\t\tgoto out;\n+\t\t\t}\n+\n+\t\t\tif (name[0] == '+')\n+\t\t\t\tret = swap_tiers_add(name + 1, prio);\n+\t\t\telse\n+\t\t\t\tret = swap_tiers_modify(name, prio);\n+\t\t}\n+\n+\t\tif (ret)\n+\t\t\tgoto restore;\n+\t}\n+\n+\tif (!swap_tiers_validate()) {\n+\t\tret = -EINVAL;\n+\t\tgoto restore;\n+\t}\n+\n+out:\n+\tspin_unlock(&swap_tier_lock);\n+\tspin_unlock(&swap_lock);\n+\n+\tkfree(tmp);\n+\treturn ret ? ret : count;\n+\n+restore:\n+\tswap_tiers_restore(ctx);\n+\tgoto out;\n+}\n+\n+static struct kobj_attribute tier_attr = __ATTR_RW(tiers);\n+\n static struct attribute *swap_attrs[] = {\n \t&vma_ra_enabled_attr.attr,\n+\t&tier_attr.attr,\n \tNULL,\n };\n \ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nnew file mode 100644\nindex 000000000000..87882272eec8\n--- /dev/null\n+++ b/mm/swap_tier.c\n@@ -0,0 +1,304 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#include <linux/swap.h>\n+#include <linux/memcontrol.h>\n+#include \"memcontrol-v1.h\"\n+#include <linux/sysfs.h>\n+#include <linux/plist.h>\n+\n+#include \"swap.h\"\n+#include \"swap_tier.h\"\n+\n+/*\n+ * struct swap_tier - structure representing a swap tier.\n+ *\n+ * @name: name of the swap_tier.\n+ * @prio: starting value of priority.\n+ * @list: linked list of tiers.\n+*/\n+static struct swap_tier {\n+\tchar name[MAX_TIERNAME];\n+\tshort prio;\n+\tstruct list_head list;\n+} swap_tiers[MAX_SWAPTIER];\n+\n+DEFINE_SPINLOCK(swap_tier_lock);\n+/* active swap priority list, sorted in descending order */\n+static LIST_HEAD(swap_tier_active_list);\n+/* unused swap_tier object */\n+static LIST_HEAD(swap_tier_inactive_list);\n+\n+#define TIER_IDX(tier)\t((tier) - swap_tiers)\n+#define TIER_MASK(tier)\t(1 << TIER_IDX(tier))\n+#define TIER_INVALID_PRIO (DEF_SWAP_PRIO - 1)\n+#define TIER_END_PRIO(tier) \\\n+\t(!list_is_first(&(tier)->list, &swap_tier_active_list) ? \\\n+\tlist_prev_entry((tier), list)->prio - 1 : SHRT_MAX)\n+\n+#define for_each_tier(tier, idx) \\\n+\tfor (idx = 0, tier = &swap_tiers[0]; idx < MAX_SWAPTIER; \\\n+\t\tidx++, tier = &swap_tiers[idx])\n+\n+#define for_each_active_tier(tier) \\\n+\tlist_for_each_entry(tier, &swap_tier_active_list, list)\n+\n+#define for_each_inactive_tier(tier) \\\n+\tlist_for_each_entry(tier, &swap_tier_inactive_list, list)\n+\n+/*\n+ * Naming Convention:\n+ *   swap_tiers_*() - Public/exported functions\n+ *   swap_tier_*()  - Private/internal functions\n+ */\n+\n+static bool swap_tier_is_active(void)\n+{\n+\treturn !list_empty(&swap_tier_active_list) ? true : false;\n+}\n+\n+static struct swap_tier *swap_tier_lookup(const char *name)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (!strcmp(tier->name, name))\n+\t\t\treturn tier;\n+\t}\n+\n+\treturn NULL;\n+}\n+\n+void swap_tiers_init(void)\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tBUILD_BUG_ON(BITS_PER_TYPE(int) < MAX_SWAPTIER);\n+\n+\tfor_each_tier(tier, idx) {\n+\t\tINIT_LIST_HEAD(&tier->list);\n+\t\tlist_add_tail(&tier->list, &swap_tier_inactive_list);\n+\t}\n+}\n+\n+ssize_t swap_tiers_sysfs_show(char *buf)\n+{\n+\tstruct swap_tier *tier;\n+\tssize_t len = 0;\n+\n+\tlen += sysfs_emit_at(buf, len, \"%-16s %-5s %-11s %-11s\\n\",\n+\t\t\t \"Name\", \"Idx\", \"PrioStart\", \"PrioEnd\");\n+\n+\tspin_lock(&swap_tier_lock);\n+\tfor_each_active_tier(tier) {\n+\t\tlen += sysfs_emit_at(buf, len, \"%-16s %-5ld %-11d %-11d\\n\",\n+\t\t\t\t     tier->name,\n+\t\t\t\t     TIER_IDX(tier),\n+\t\t\t\t     tier->prio,\n+\t\t\t\t     TIER_END_PRIO(tier));\n+\t\tif (len >= PAGE_SIZE)\n+\t\t\tbreak;\n+\t}\n+\tspin_unlock(&swap_tier_lock);\n+\n+\treturn len;\n+}\n+\n+static void swap_tier_insert_by_prio(struct swap_tier *new)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (tier->prio > new->prio)\n+\t\t\tcontinue;\n+\n+\t\tlist_add_tail(&new->list, &tier->list);\n+\t\treturn;\n+\t}\n+\t/* First addition, or becomes the first tier */\n+\tlist_add_tail(&new->list, &swap_tier_active_list);\n+}\n+\n+static void __swap_tier_prepare(struct swap_tier *tier, const char *name,\n+\tshort prio)\n+{\n+\tlist_del_init(&tier->list);\n+\tstrscpy(tier->name, name, MAX_TIERNAME);\n+\ttier->prio = prio;\n+}\n+\n+static struct swap_tier *swap_tier_prepare(const char *name, short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tif (prio < DEF_SWAP_PRIO)\n+\t\treturn NULL;\n+\n+\tif (list_empty(&swap_tier_inactive_list))\n+\t\treturn ERR_PTR(-EPERM);\n+\n+\ttier = list_first_entry(&swap_tier_inactive_list,\n+\t\tstruct swap_tier, list);\n+\n+\t__swap_tier_prepare(tier, name, prio);\n+\treturn tier;\n+}\n+\n+static int swap_tier_check_range(short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\t/* No overwrite */\n+\t\tif (tier->prio == prio)\n+\t\t\treturn -EINVAL;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+int swap_tiers_add(const char *name, int prio)\n+{\n+\tint ret;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\t/* Duplicate check */\n+\tif (swap_tier_lookup(name))\n+\t\treturn -EPERM;\n+\n+\tret = swap_tier_check_range(prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\ttier = swap_tier_prepare(name, prio);\n+\tif (IS_ERR(tier)) {\n+\t\tret = PTR_ERR(tier);\n+\t\treturn ret;\n+\t}\n+\n+\n+\tswap_tier_insert_by_prio(tier);\n+\treturn ret;\n+}\n+\n+int swap_tiers_remove(const char *name)\n+{\n+\tint ret = 0;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\ttier = swap_tier_lookup(name);\n+\tif (!tier)\n+\t\treturn -EINVAL;\n+\n+\tlist_move(&tier->list, &swap_tier_inactive_list);\n+\n+\t/* Removing DEF_SWAP_PRIO merges into the higher tier. */\n+\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO)\n+\t\tlist_prev_entry(tier, list)->prio = DEF_SWAP_PRIO;\n+\n+\treturn ret;\n+}\n+\n+int swap_tiers_modify(const char *name, int prio)\n+{\n+\tint ret;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\ttier = swap_tier_lookup(name);\n+\tif (!tier)\n+\t\treturn -EINVAL;\n+\n+\t/* No need to modify */\n+\tif (tier->prio == prio)\n+\t\treturn 0;\n+\n+\tret = swap_tier_check_range(prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tlist_del_init(&tier->list);\n+\ttier->prio = prio;\n+\tswap_tier_insert_by_prio(tier);\n+\n+\treturn ret;\n+}\n+\n+/*\n+ * XXX: Reverting individual operations becomes complex as the number of\n+ * operations grows. Instead, we save the original state beforehand and\n+ * fully restore it if any operation fails.\n+ */\n+void swap_tiers_save(struct swap_tier_save_ctx ctx[])\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tidx = TIER_IDX(tier);\n+\t\tstrcpy(ctx[idx].name, tier->name);\n+\t\tctx[idx].prio = tier->prio;\n+\t}\n+\n+\tfor_each_inactive_tier(tier) {\n+\t\tidx = TIER_IDX(tier);\n+\t\t/* Indicator of inactive */\n+\t\tctx[idx].prio = TIER_INVALID_PRIO;\n+\t}\n+}\n+\n+void swap_tiers_restore(struct swap_tier_save_ctx ctx[])\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\t/* Invalidate active list */\n+\tlist_splice_tail_init(&swap_tier_active_list,\n+\t\t\t&swap_tier_inactive_list);\n+\n+\tfor_each_tier(tier, idx) {\n+\t\tif (ctx[idx].prio != TIER_INVALID_PRIO) {\n+\t\t\t/* Preserve idx(mask) */\n+\t\t\t__swap_tier_prepare(tier, ctx[idx].name, ctx[idx].prio);\n+\t\t\tswap_tier_insert_by_prio(tier);\n+\t\t}\n+\t}\n+}\n+\n+bool swap_tiers_validate(void)\n+{\n+\tstruct swap_tier *tier;\n+\n+\t/*\n+\t * Initial setting might not cover DEF_SWAP_PRIO.\n+\t * Swap tier must cover the full range (DEF_SWAP_PRIO to SHRT_MAX).\n+\t * Also, modify operation can change only one remaining priority.\n+\t */\n+\tif (swap_tier_is_active()) {\n+\t\ttier = list_last_entry(&swap_tier_active_list,\n+\t\t\tstruct swap_tier, list);\n+\n+\t\tif (tier->prio != DEF_SWAP_PRIO)\n+\t\t\treturn false;\n+\t}\n+\n+\treturn true;\n+}\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nnew file mode 100644\nindex 000000000000..4b1b0602d691\n--- /dev/null\n+++ b/mm/swap_tier.h\n@@ -0,0 +1,38 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _SWAP_TIER_H\n+#define _SWAP_TIER_H\n+\n+#include <linux/types.h>\n+#include <linux/spinlock.h>\n+\n+#define MAX_TIERNAME\t\t16\n+\n+/* Ensure MAX_SWAPTIER does not exceed MAX_SWAPFILES */\n+#if 8 > MAX_SWAPFILES\n+#define MAX_SWAPTIER\t\tMAX_SWAPFILES\n+#else\n+#define MAX_SWAPTIER\t\t8\n+#endif\n+\n+extern spinlock_t swap_tier_lock;\n+\n+struct swap_tier_save_ctx {\n+\tchar name[MAX_TIERNAME];\n+\tshort prio;\n+};\n+\n+#define DEFINE_SWAP_TIER_SAVE_CTX(_name) \\\n+\tstruct swap_tier_save_ctx _name[MAX_SWAPTIER] = {0}\n+\n+/* Initialization and application */\n+void swap_tiers_init(void);\n+ssize_t swap_tiers_sysfs_show(char *buf);\n+\n+int swap_tiers_add(const char *name, int prio);\n+int swap_tiers_remove(const char *name);\n+int swap_tiers_modify(const char *name, int prio);\n+\n+void swap_tiers_save(struct swap_tier_save_ctx ctx[]);\n+void swap_tiers_restore(struct swap_tier_save_ctx ctx[]);\n+bool swap_tiers_validate(void);\n+#endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 7b055f15d705..c27952b41d4f 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -50,6 +50,7 @@\n #include \"internal.h\"\n #include \"swap_table.h\"\n #include \"swap.h\"\n+#include \"swap_tier.h\"\n \n static bool swap_count_continued(struct swap_info_struct *, pgoff_t,\n \t\t\t\t unsigned char);\n@@ -65,7 +66,7 @@ static void move_cluster(struct swap_info_struct *si,\n \t\t\t struct swap_cluster_info *ci, struct list_head *list,\n \t\t\t enum swap_cluster_flags new_flags);\n \n-static DEFINE_SPINLOCK(swap_lock);\n+DEFINE_SPINLOCK(swap_lock);\n static unsigned int nr_swapfiles;\n atomic_long_t nr_swap_pages;\n /*\n@@ -76,7 +77,6 @@ atomic_long_t nr_swap_pages;\n EXPORT_SYMBOL_GPL(nr_swap_pages);\n /* protected with swap_lock. reading in vm_swap_full() doesn't need lock */\n long total_swap_pages;\n-#define DEF_SWAP_PRIO  -1\n unsigned long swapfile_maximum_size;\n #ifdef CONFIG_MIGRATION\n bool swap_migration_ad_supported;\n@@ -89,7 +89,7 @@ static const char Bad_offset[] = \"Bad swap offset entry \";\n  * all active swap_info_structs\n  * protected with swap_lock, and ordered by priority.\n  */\n-static PLIST_HEAD(swap_active_head);\n+PLIST_HEAD(swap_active_head);\n \n /*\n  * all available (active, not full) swap_info_structs\n@@ -3977,6 +3977,7 @@ static int __init swapfile_init(void)\n \t\tswap_migration_ad_supported = true;\n #endif\t/* CONFIG_MIGRATION */\n \n+\tswap_tiers_init();\n \treturn 0;\n }\n subsys_initcall(swapfile_init);\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "message_id": "20260126065242.1221862-2-youngjun.park@lge.com",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about caching oscillation and priority inversion on swap devices when using global percpu clusters. They reverted commit 1b7e90020eb7 to use each swap device's percpu cluster instead, citing two issues that would arise from the global approach: caching oscillation between different memcg accessing different si, and priority inversion on swap devices.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "reverted commit",
                "caching oscillation",
                "priority inversion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This reverts commit 1b7e90020eb7 (\"mm, swap: use percpu cluster as\nallocation fast path\").\n\nBecause in the newly introduced swap tiers, the global percpu cluster\nwill cause two issues:\n1) it will cause caching oscillation in the same order of different si\n   if two different memcg can only be allowed to access different si and\n   both of them are swapping out.\n2) It can cause priority inversion on swap devices. Imagine a case where\n   there are two memcg, say memcg1 and memcg2. Memcg1 can access si A, B\n   and A is higher priority device. While memcg2 can only access si B.\n   Then memcg 2 could write the global percpu cluster with si B, then\n   memcg1 take si B in fast path even though si A is not exhausted.\n\nHence in order to support swap tier, revert commit to use\neach swap device's percpu cluster.\n\nSuggested-by: Kairui Song <kasong@tencent.com>\nCo-developed-by: Baoquan He <bhe@redhat.com>\nSigned-off-by: Baoquan He <bhe@redhat.com>\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n include/linux/swap.h |  17 ++++--\n mm/swapfile.c        | 142 ++++++++++++++-----------------------------\n 2 files changed, 57 insertions(+), 102 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 1e68c220a0e7..6921e22b14d3 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -247,11 +247,18 @@ enum {\n #define SWAP_NR_ORDERS\t\t1\n #endif\n \n-/*\n- * We keep using same cluster for rotational device so IO will be sequential.\n- * The purpose is to optimize SWAP throughput on these device.\n- */\n+ /*\n+  * We assign a cluster to each CPU, so each CPU can allocate swap entry from\n+  * its own cluster and swapout sequentially. The purpose is to optimize swapout\n+  * throughput.\n+  */\n+struct percpu_cluster {\n+\tlocal_lock_t lock; /* Protect the percpu_cluster above */\n+\tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n+};\n+\n struct swap_sequential_cluster {\n+\tspinlock_t lock; /* Serialize usage of global cluster */\n \tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n };\n \n@@ -277,8 +284,8 @@ struct swap_info_struct {\n \t\t\t\t\t/* list of cluster that are fragmented or contented */\n \tunsigned int pages;\t\t/* total of usable pages of swap */\n \tatomic_long_t inuse_pages;\t/* number of those currently in use */\n+\tstruct percpu_cluster\t__percpu *percpu_cluster; /* per cpu's swap location */\n \tstruct swap_sequential_cluster *global_cluster; /* Use one global cluster for rotating device */\n-\tspinlock_t global_cluster_lock;\t/* Serialize usage of global cluster */\n \tstruct rb_root swap_extent_root;/* root of the swap extent rbtree */\n \tstruct block_device *bdev;\t/* swap device or bdev of swap file */\n \tstruct file *swap_file;\t\t/* seldom referenced */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex dd97e850ea2c..5e3b87799440 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -118,18 +118,6 @@ static atomic_t proc_poll_event = ATOMIC_INIT(0);\n \n atomic_t nr_rotate_swap = ATOMIC_INIT(0);\n \n-struct percpu_swap_cluster {\n-\tstruct swap_info_struct *si[SWAP_NR_ORDERS];\n-\tunsigned long offset[SWAP_NR_ORDERS];\n-\tlocal_lock_t lock;\n-};\n-\n-static DEFINE_PER_CPU(struct percpu_swap_cluster, percpu_swap_cluster) = {\n-\t.si = { NULL },\n-\t.offset = { SWAP_ENTRY_INVALID },\n-\t.lock = INIT_LOCAL_LOCK(),\n-};\n-\n /* May return NULL on invalid type, caller must check for NULL return */\n static struct swap_info_struct *swap_type_to_info(int type)\n {\n@@ -477,7 +465,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * Swap allocator uses percpu clusters and holds the local lock.\n \t */\n \tlockdep_assert_held(&ci->lock);\n-\tlockdep_assert_held(&this_cpu_ptr(&percpu_swap_cluster)->lock);\n+\tlockdep_assert_held(this_cpu_ptr(&si->percpu_cluster->lock));\n \n \t/* The cluster must be free and was just isolated from the free list. */\n \tVM_WARN_ON_ONCE(ci->flags || !cluster_is_empty(ci));\n@@ -495,8 +483,8 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t */\n \tspin_unlock(&ci->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_unlock(&si->global_cluster_lock);\n-\tlocal_unlock(&percpu_swap_cluster.lock);\n+\t\tspin_unlock(&si->global_cluster->lock);\n+\tlocal_unlock(&si->percpu_cluster->lock);\n \n \ttable = swap_table_alloc(__GFP_HIGH | __GFP_NOMEMALLOC | GFP_KERNEL);\n \n@@ -508,9 +496,9 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * could happen with ignoring the percpu cluster is fragmentation,\n \t * which is acceptable since this fallback and race is rare.\n \t */\n-\tlocal_lock(&percpu_swap_cluster.lock);\n+\tlocal_lock(&si->percpu_cluster->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_lock(&si->global_cluster_lock);\n+\t\tspin_lock(&si->global_cluster->lock);\n \tspin_lock(&ci->lock);\n \n \t/* Nothing except this helper should touch a dangling empty cluster. */\n@@ -622,7 +610,7 @@ static bool swap_do_scheduled_discard(struct swap_info_struct *si)\n \t\tci = list_first_entry(&si->discard_clusters, struct swap_cluster_info, list);\n \t\t/*\n \t\t * Delete the cluster from list to prepare for discard, but keep\n-\t\t * the CLUSTER_FLAG_DISCARD flag, percpu_swap_cluster could be\n+\t\t * the CLUSTER_FLAG_DISCARD flag, there could be percpu_cluster\n \t\t * pointing to it, or ran into by relocate_cluster.\n \t\t */\n \t\tlist_del(&ci->list);\n@@ -953,12 +941,11 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n out:\n \trelocate_cluster(si, ci);\n \tswap_cluster_unlock(ci);\n-\tif (si->flags & SWP_SOLIDSTATE) {\n-\t\tthis_cpu_write(percpu_swap_cluster.offset[order], next);\n-\t\tthis_cpu_write(percpu_swap_cluster.si[order], si);\n-\t} else {\n+\tif (si->flags & SWP_SOLIDSTATE)\n+\t\tthis_cpu_write(si->percpu_cluster->next[order], next);\n+\telse\n \t\tsi->global_cluster->next[order] = next;\n-\t}\n+\n \treturn found;\n }\n \n@@ -1052,13 +1039,17 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \tif (order && !(si->flags & SWP_BLKDEV))\n \t\treturn 0;\n \n-\tif (!(si->flags & SWP_SOLIDSTATE)) {\n+\tif (si->flags & SWP_SOLIDSTATE) {\n+\t\t/* Fast path using per CPU cluster */\n+\t\tlocal_lock(&si->percpu_cluster->lock);\n+\t\toffset = __this_cpu_read(si->percpu_cluster->next[order]);\n+\t} else {\n \t\t/* Serialize HDD SWAP allocation for each device. */\n-\t\tspin_lock(&si->global_cluster_lock);\n+\t\tspin_lock(&si->global_cluster->lock);\n \t\toffset = si->global_cluster->next[order];\n-\t\tif (offset == SWAP_ENTRY_INVALID)\n-\t\t\tgoto new_cluster;\n+\t}\n \n+\tif (offset != SWAP_ENTRY_INVALID) {\n \t\tci = swap_cluster_lock(si, offset);\n \t\t/* Cluster could have been used by another order */\n \t\tif (cluster_is_usable(ci, order)) {\n@@ -1072,7 +1063,6 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n \n-new_cluster:\n \t/*\n \t * If the device need discard, prefer new cluster over nonfull\n \t * to spread out the writes.\n@@ -1129,8 +1119,10 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n done:\n-\tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_unlock(&si->global_cluster_lock);\n+\tif (si->flags & SWP_SOLIDSTATE)\n+\t\tlocal_unlock(&si->percpu_cluster->lock);\n+\telse\n+\t\tspin_unlock(&si->global_cluster->lock);\n \n \treturn found;\n }\n@@ -1311,41 +1303,8 @@ static bool get_swap_device_info(struct swap_info_struct *si)\n \treturn true;\n }\n \n-/*\n- * Fast path try to get swap entries with specified order from current\n- * CPU's swap entry pool (a cluster).\n- */\n-static bool swap_alloc_fast(struct folio *folio)\n-{\n-\tunsigned int order = folio_order(folio);\n-\tstruct swap_cluster_info *ci;\n-\tstruct swap_info_struct *si;\n-\tunsigned int offset;\n-\n-\t/*\n-\t * Once allocated, swap_info_struct will never be completely freed,\n-\t * so checking it's liveness by get_swap_device_info is enough.\n-\t */\n-\tsi = this_cpu_read(percpu_swap_cluster.si[order]);\n-\toffset = this_cpu_read(percpu_swap_cluster.offset[order]);\n-\tif (!si || !offset || !get_swap_device_info(si))\n-\t\treturn false;\n-\n-\tci = swap_cluster_lock(si, offset);\n-\tif (cluster_is_usable(ci, order)) {\n-\t\tif (cluster_is_empty(ci))\n-\t\t\toffset = cluster_offset(si, ci);\n-\t\talloc_swap_scan_cluster(si, ci, folio, offset);\n-\t} else {\n-\t\tswap_cluster_unlock(ci);\n-\t}\n-\n-\tput_swap_device(si);\n-\treturn folio_test_swapcache(folio);\n-}\n-\n /* Rotate the device and switch to a new cluster */\n-static void swap_alloc_slow(struct folio *folio)\n+static void swap_alloc_entry(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n \tint mask = folio_memcg(folio) ?\n@@ -1363,6 +1322,7 @@ static void swap_alloc_slow(struct folio *folio)\n \t\tif (get_swap_device_info(si)) {\n \t\t\tcluster_alloc_swap_entry(si, folio);\n \t\t\tput_swap_device(si);\n+\n \t\t\tif (folio_test_swapcache(folio))\n \t\t\t\treturn;\n \t\t\tif (folio_test_large(folio))\n@@ -1522,11 +1482,7 @@ int folio_alloc_swap(struct folio *folio)\n \t}\n \n again:\n-\tlocal_lock(&percpu_swap_cluster.lock);\n-\tif (!swap_alloc_fast(folio))\n-\t\tswap_alloc_slow(folio);\n-\tlocal_unlock(&percpu_swap_cluster.lock);\n-\n+\tswap_alloc_entry(folio);\n \tif (!order && unlikely(!folio_test_swapcache(folio))) {\n \t\tif (swap_sync_discard())\n \t\t\tgoto again;\n@@ -1945,9 +1901,7 @@ swp_entry_t swap_alloc_hibernation_slot(int type)\n \t\t\t * Grab the local lock to be compliant\n \t\t\t * with swap table allocation.\n \t\t\t */\n-\t\t\tlocal_lock(&percpu_swap_cluster.lock);\n \t\t\toffset = cluster_alloc_swap_entry(si, NULL);\n-\t\t\tlocal_unlock(&percpu_swap_cluster.lock);\n \t\t\tif (offset)\n \t\t\t\tentry = swp_entry(si->type, offset);\n \t\t}\n@@ -2751,28 +2705,6 @@ static void free_cluster_info(struct swap_cluster_info *cluster_info,\n \tkvfree(cluster_info);\n }\n \n-/*\n- * Called after swap device's reference count is dead, so\n- * neither scan nor allocation will use it.\n- */\n-static void flush_percpu_swap_cluster(struct swap_info_struct *si)\n-{\n-\tint cpu, i;\n-\tstruct swap_info_struct **pcp_si;\n-\n-\tfor_each_possible_cpu(cpu) {\n-\t\tpcp_si = per_cpu_ptr(percpu_swap_cluster.si, cpu);\n-\t\t/*\n-\t\t * Invalidate the percpu swap cluster cache, si->users\n-\t\t * is dead, so no new user will point to it, just flush\n-\t\t * any existing user.\n-\t\t */\n-\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n-\t\t\tcmpxchg(&pcp_si[i], si, NULL);\n-\t}\n-}\n-\n-\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n@@ -2856,7 +2788,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tflush_work(&p->discard_work);\n \tflush_work(&p->reclaim_work);\n-\tflush_percpu_swap_cluster(p);\n \n \tdestroy_swap_extents(p);\n \tif (p->flags & SWP_CONTINUED)\n@@ -2885,6 +2816,8 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tarch_swap_invalidate_area(p->type);\n \tzswap_swapoff(p->type);\n \tmutex_unlock(&swapon_mutex);\n+\tfree_percpu(p->percpu_cluster);\n+\tp->percpu_cluster = NULL;\n \tkfree(p->global_cluster);\n \tp->global_cluster = NULL;\n \tvfree(swap_map);\n@@ -3268,7 +3201,7 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n {\n \tunsigned long nr_clusters = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);\n \tstruct swap_cluster_info *cluster_info;\n-\tint err = -ENOMEM;\n+\tint cpu, err = -ENOMEM;\n \tunsigned long i;\n \n \tcluster_info = kvcalloc(nr_clusters, sizeof(*cluster_info), GFP_KERNEL);\n@@ -3278,14 +3211,27 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n \tfor (i = 0; i < nr_clusters; i++)\n \t\tspin_lock_init(&cluster_info[i].lock);\n \n-\tif (!(si->flags & SWP_SOLIDSTATE)) {\n+\tif (si->flags & SWP_SOLIDSTATE) {\n+\t\tsi->percpu_cluster = alloc_percpu(struct percpu_cluster);\n+\t\tif (!si->percpu_cluster)\n+\t\t\tgoto err;\n+\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tstruct percpu_cluster *cluster;\n+\n+\t\t\tcluster = per_cpu_ptr(si->percpu_cluster, cpu);\n+\t\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n+\t\t\t\tcluster->next[i] = SWAP_ENTRY_INVALID;\n+\t\t\tlocal_lock_init(&cluster->lock);\n+\t\t}\n+\t} else {\n \t\tsi->global_cluster = kmalloc(sizeof(*si->global_cluster),\n \t\t\t\t     GFP_KERNEL);\n \t\tif (!si->global_cluster)\n \t\t\tgoto err;\n \t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n \t\t\tsi->global_cluster->next[i] = SWAP_ENTRY_INVALID;\n-\t\tspin_lock_init(&si->global_cluster_lock);\n+\t\tspin_lock_init(&si->global_cluster->lock);\n \t}\n \n \t/*\n@@ -3566,6 +3512,8 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n bad_swap_unlock_inode:\n \tinode_unlock(inode);\n bad_swap:\n+\tfree_percpu(si->percpu_cluster);\n+\tsi->percpu_cluster = NULL;\n \tkfree(si->global_cluster);\n \tsi->global_cluster = NULL;\n \tinode = NULL;\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "message_id": "20260126065242.1221862-5-youngjun.park@lge.com",
              "analysis_source": "llm"
            },
            {
              "author": "Youngjun Park (author)",
              "summary": "The author addressed a concern about swap device rotation causing fragmentation and performance regression when using per-device percpu clusters. They introduced a per-cpu cache for the swap device, which prioritizes the cached swap device over other devices, minimizing side effects on the existing fastpath.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "introduced a new approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When using per-device percpu clusters (instead of a global one),\na naive allocation logic triggers swap device rotation on every\nallocation. This behavior leads to severe fragmentation and performance\nregression.\n\nTo address this, this patch introduces a per-cpu cache for the swap\ndevice. The allocation logic is updated to prioritize the per-cpu\ncluster within the cached swap device, effectively restoring the\ntraditional fastpath and slowpath flow. This approach minimizes side\neffects on the existing fastpath.\n\nWith this change, swap device rotation occurs only when the current\ncached device is unable to satisfy the allocation, rather than on\nevery attempt.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n include/linux/swap.h |  1 -\n mm/swapfile.c        | 78 +++++++++++++++++++++++++++++++++++++-------\n 2 files changed, 66 insertions(+), 13 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 6921e22b14d3..ac634a21683a 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -253,7 +253,6 @@ enum {\n   * throughput.\n   */\n struct percpu_cluster {\n-\tlocal_lock_t lock; /* Protect the percpu_cluster above */\n \tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n };\n \ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 5e3b87799440..0dcd451afee5 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -106,6 +106,16 @@ PLIST_HEAD(swap_active_head);\n static PLIST_HEAD(swap_avail_head);\n static DEFINE_SPINLOCK(swap_avail_lock);\n \n+struct percpu_swap_device {\n+\tstruct swap_info_struct *si[SWAP_NR_ORDERS];\n+\tlocal_lock_t lock;\n+};\n+\n+static DEFINE_PER_CPU(struct percpu_swap_device, percpu_swap_device) = {\n+\t.si = { NULL },\n+\t.lock = INIT_LOCAL_LOCK(),\n+};\n+\n struct swap_info_struct *swap_info[MAX_SWAPFILES];\n \n static struct kmem_cache *swap_table_cachep;\n@@ -465,7 +475,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * Swap allocator uses percpu clusters and holds the local lock.\n \t */\n \tlockdep_assert_held(&ci->lock);\n-\tlockdep_assert_held(this_cpu_ptr(&si->percpu_cluster->lock));\n+\tlockdep_assert_held(this_cpu_ptr(&percpu_swap_device.lock));\n \n \t/* The cluster must be free and was just isolated from the free list. */\n \tVM_WARN_ON_ONCE(ci->flags || !cluster_is_empty(ci));\n@@ -484,7 +494,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \tspin_unlock(&ci->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_unlock(&si->global_cluster->lock);\n-\tlocal_unlock(&si->percpu_cluster->lock);\n+\tlocal_unlock(&percpu_swap_device.lock);\n \n \ttable = swap_table_alloc(__GFP_HIGH | __GFP_NOMEMALLOC | GFP_KERNEL);\n \n@@ -496,7 +506,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * could happen with ignoring the percpu cluster is fragmentation,\n \t * which is acceptable since this fallback and race is rare.\n \t */\n-\tlocal_lock(&si->percpu_cluster->lock);\n+\tlocal_lock(&percpu_swap_device.lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_lock(&si->global_cluster->lock);\n \tspin_lock(&ci->lock);\n@@ -941,9 +951,10 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n out:\n \trelocate_cluster(si, ci);\n \tswap_cluster_unlock(ci);\n-\tif (si->flags & SWP_SOLIDSTATE)\n+\tif (si->flags & SWP_SOLIDSTATE) {\n \t\tthis_cpu_write(si->percpu_cluster->next[order], next);\n-\telse\n+\t\tthis_cpu_write(percpu_swap_device.si[order], si);\n+\t} else\n \t\tsi->global_cluster->next[order] = next;\n \n \treturn found;\n@@ -1041,7 +1052,6 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \n \tif (si->flags & SWP_SOLIDSTATE) {\n \t\t/* Fast path using per CPU cluster */\n-\t\tlocal_lock(&si->percpu_cluster->lock);\n \t\toffset = __this_cpu_read(si->percpu_cluster->next[order]);\n \t} else {\n \t\t/* Serialize HDD SWAP allocation for each device. */\n@@ -1119,9 +1129,7 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n done:\n-\tif (si->flags & SWP_SOLIDSTATE)\n-\t\tlocal_unlock(&si->percpu_cluster->lock);\n-\telse\n+\tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_unlock(&si->global_cluster->lock);\n \n \treturn found;\n@@ -1303,8 +1311,27 @@ static bool get_swap_device_info(struct swap_info_struct *si)\n \treturn true;\n }\n \n+static bool swap_alloc_fast(struct folio *folio)\n+{\n+\tunsigned int order = folio_order(folio);\n+\tstruct swap_info_struct *si;\n+\n+\t/*\n+\t * Once allocated, swap_info_struct will never be completely freed,\n+\t * so checking it's liveness by get_swap_device_info is enough.\n+\t */\n+\tsi = this_cpu_read(percpu_swap_device.si[order]);\n+\tif (!si || !get_swap_device_info(si))\n+\t\treturn false;\n+\n+\tcluster_alloc_swap_entry(si, folio);\n+\tput_swap_device(si);\n+\n+\treturn folio_test_swapcache(folio);\n+}\n+\n /* Rotate the device and switch to a new cluster */\n-static void swap_alloc_entry(struct folio *folio)\n+static void swap_alloc_slow(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n \tint mask = folio_memcg(folio) ?\n@@ -1482,7 +1509,11 @@ int folio_alloc_swap(struct folio *folio)\n \t}\n \n again:\n-\tswap_alloc_entry(folio);\n+\tlocal_lock(&percpu_swap_device.lock);\n+\tif (!swap_alloc_fast(folio))\n+\t\tswap_alloc_slow(folio);\n+\tlocal_unlock(&percpu_swap_device.lock);\n+\n \tif (!order && unlikely(!folio_test_swapcache(folio))) {\n \t\tif (swap_sync_discard())\n \t\t\tgoto again;\n@@ -1901,7 +1932,9 @@ swp_entry_t swap_alloc_hibernation_slot(int type)\n \t\t\t * Grab the local lock to be compliant\n \t\t\t * with swap table allocation.\n \t\t\t */\n+\t\t\tlocal_lock(&percpu_swap_device.lock);\n \t\t\toffset = cluster_alloc_swap_entry(si, NULL);\n+\t\t\tlocal_unlock(&percpu_swap_device.lock);\n \t\t\tif (offset)\n \t\t\t\tentry = swp_entry(si->type, offset);\n \t\t}\n@@ -2705,6 +2738,27 @@ static void free_cluster_info(struct swap_cluster_info *cluster_info,\n \tkvfree(cluster_info);\n }\n \n+/*\n+ * Called after swap device's reference count is dead, so\n+ * neither scan nor allocation will use it.\n+ */\n+static void flush_percpu_swap_device(struct swap_info_struct *si)\n+{\n+\tint cpu, i;\n+\tstruct swap_info_struct **pcp_si;\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcp_si = per_cpu_ptr(percpu_swap_device.si, cpu);\n+\t\t/*\n+\t\t * Invalidate the percpu swap device cache, si->users\n+\t\t * is dead, so no new user will point to it, just flush\n+\t\t * any existing user.\n+\t\t */\n+\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n+\t\t\tcmpxchg(&pcp_si[i], si, NULL);\n+\t}\n+}\n+\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n@@ -2788,6 +2842,7 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tflush_work(&p->discard_work);\n \tflush_work(&p->reclaim_work);\n+\tflush_percpu_swap_device(p);\n \n \tdestroy_swap_extents(p);\n \tif (p->flags & SWP_CONTINUED)\n@@ -3222,7 +3277,6 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n \t\t\tcluster = per_cpu_ptr(si->percpu_cluster, cpu);\n \t\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n \t\t\t\tcluster->next[i] = SWAP_ENTRY_INVALID;\n-\t\t\tlocal_lock_init(&cluster->lock);\n \t\t}\n \t} else {\n \t\tsi->global_cluster = kmalloc(sizeof(*si->global_cluster),\n-- \n2.34.1",
              "reply_to": "",
              "message_date": "2026-01-26",
              "message_id": "20260126065242.1221862-6-youngjun.park@lge.com",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the swap_tier structure was simplified by replacing 'end prio' and priority lists with a standard list_head, as requested in v2.\n\nReviewer Chris Li suggested breaking down the patch series into smaller, incremental steps, starting with defining the tiers bits without deletion, and then building upon that in subsequent steps.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledgment of patch changes",
                "requested changes",
                "suggested incremental development"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Youngjun,\n\nOn Sun, Jan 25, 2026 at 10:53PM Youngjun Park <youngjun.park@lge.com> wrote:\n\n---\n\nThanks for the patches series.\n\nSorry for the late reply. I have been wanting to reply to it but get\nsuper busy at work.\n\nSome high level feedback for the series. Now that you demonstrated the\nwhole series, let's focus on making small mergiable baby steps. Just\nlike the swap table has different phases. Make each step minimal, each\nstep shows some value. Do the MVP, we can always add more features as\na follow up step.\n\nI suggest the first step is getting the tiers bits defined. Add only,\nno delete.  Get that reviewed and merged, then the next step is to use\nthose tiers.\n\nChris",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-11",
              "message_id": "CACePvbU3OoGg5-dHXOJk=62AkBxJCLmzwcHdHuPe2nnxfzMLBw@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li suggested replacing per-cpu per-swap-device cluster allocation with a global percpu cluster per tier, citing that the maximum number of tiers is smaller than the maximum number of swap devices and expecting this change to be beneficial.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "One idea is that, instead of using percpu per swap device.\nYou can make the global percpu cluster per tier. Because the max tier\nnumber is smaller than the max number of swap devices. That is likely\na win.\n\nChris",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-11",
              "message_id": "CACePvbXeUx9_dyrSFoz57RnNccoMwiF5u70v6WqHJNFGEZrCPw@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li noted that the patch does not handle the case where a child cgroup re-enables a tier with '+' that was excluded by its parent, and requested that the effective tier list be limited to the parent's allowed subset.\n\nReviewer Chris Li suggested introducing a CONFIG option to limit the maximum number of swap tiers, recommending a default value of 4.\n\nReviewer Chris Li noted that when modifying a tier, if swap files are moved to a different tier, it could cause issues, and requested further consideration of this scenario.\n\nReviewer Chris Li expressed concern about the complexity of the patch, specifically the need for save and restore operations, and requested a simpler design.\n\nReviewer Chris Li suggested that each tier have its own swap_active_head, so different swap entries on different tiers do not compete for the same resource, and proposed that swapfiles should not be allowed to switch between tiers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "complexity"
              ],
              "has_inline_review": true,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Yongjun,\n\nOn Sun, Jan 25, 2026 at 10:53PM Youngjun Park <youngjun.park@lge.com> wrote:\n\n---\n\nWe can have a CONFIG option for the MAX_SWAPTIER. I think the default\nshould be a small number like 4.\n\n---\n\nWhen we add, modify, remove a tier. The simple case is there is no\nswap file under any tiers.\nBut if the modification causes some swap files to jump to different\ntiers. That might be problematic.\n\n---\n\nI really hope we don't have to do the save and restore thing. Is there\nanother design we can simplify this?\n\n---\n\nOne idea is to make each tier have swap_active_head. So different swap\nentry releases on different tiers don't need to be competing on the\nsame swap_active_head.\n\nThat will require the swapfile don't jump to another tiers.\n\nChris",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "message_id": "CACePvbVML6ZNJBWU9YSUCWwrbGd2eXMcsWxs6yFssfyBoEk5Uw@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li initially suggested that adding tier names would be beneficial, but after reevaluating the patch series, he took back his suggestion and stated that looking at the whole series is necessary rather than just focusing on the tier name.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "reconsidered opinion",
                "need to look at the whole series"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Just take a quick look at the series. I take that suggestion back.\nThis series is actually not too long. Adding the tiers name alone does\nnot add any real value. I actually need to look at the whole series\nrather than just the tier name alone.\n\nChris",
              "reply_to": "",
              "message_date": "2026-02-12",
              "message_id": "CACePvbUicBa5Oh4Vz4qX=SV3M3CegCgSJ2GjogN6Cbrkkc-uwQ@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Nhat Pham",
              "summary": "Reviewer Nhat Pham questioned the consistency of the patch description, pointing out that the '+' operator was removed but its mention in the text seemed unnecessary.\n\nReviewer Nhat Pham questioned the logic of restricting child cgroup's allowed swap tiers to be a subset of their children and ancestors, suggesting an alternative approach as more straightforward.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "questioning inconsistency",
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This comment seems a bit clunky to me. The \"+\" is removed, as noted\nabove, but then why are we saying \"even if a child re-enables a tier\nwith \"+\"\" here? Am I missing something?\n\n---\n\nBut otherwise, I assume you mean to restrict child's allowed swap\ntiers to be a subset of children and its ancestors? That seems more\nstraightforward to me than the last system :)",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "message_id": "CAKEwX=M5nH3=aqSLybCfLrtScpYKz+jRWt3JYG7im70DCoyjJg@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt noted that the patch does not handle the case where a swap tier is removed while it still has active swap devices, which could lead to swap devices being left in an inconsistent state.\n\nReviewer Shakeel Butt expressed concerns that adding a memcg interface for swap tier functionality is premature, suggesting exploration of BPF as an alternative approach.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "suggested alternative"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi Youngjun,\n\nOn Mon, Jan 26, 2026 at 03:52:37PM +0900, Youngjun Park wrote:\n\n---\n\nOne of the LPC feedback you missed is to not add memcg interface for\nthis functionality and explore BPF way instead.\n\nWe are normally very conservative to add new interfaces to cgroup.\nHowever I am not even convinced that memcg interface is the right way to\nexpose this functionality. Swap is currently global and the idea to\nlimit or assign specific swap devices to specific cgroups makes sense\nbut that is the decision for the job orchestator or node controller.\nAllowing workloads to pick and choose swap devices do not make sense to\nme.\n\nShakeel",
              "reply_to": "Youngjun Park",
              "message_date": "2026-02-12",
              "message_id": "aY4bQFvpPRWgnOTM@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Chris Li's feedback about breaking the patch series into smaller, mergeable steps by proposing a modified roadmap to ensure immediate value is demonstrated in Step 1.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed",
                "proposed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hi Chris,\n\nThank you for the direction.\n\nI agree that breaking the series into smaller, mergeable steps is the\nright approach. However, since introducing the definitions alone might\nlack immediate usage, I propose a slightly\nmodified roadmap to ensure Step 1 demonstrates some value.\n\nHere is the plan I have in mind.\n\n1. Swap Tier Definition & Addition\n   - Introduce the concept, grouping logic, and the 'add' interface.\n   - Value: Enables basic exception handling within the swap device\n     itself using tiers.\n\n2. Advanced Control (Delete/Modify)\n   - Implement logic to remove or update tiers.\n   - Value: Enhances the usability and management of the tiers\n     established in Step 1.\n\n3. External Integration (memcg, bpf etc ... )\n   - Apply swap tiers for broader swap control.\n   - Value: Connects swap tiers to other subsystems like memcg.\n\nDoes this roadmap look reasonable to you? I will proceed with preparing\nthe real patch series based on this structure.\n\nBest regards,\nYoungjun",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "message_id": "aY6FiohercUYKyd6@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledged the need to limit swapfile size by adding a CONFIG option, agreeing to address this concern in the patch.\n\nAuthor addressed Chris Li's feedback about handling mixed operations in the swap tier interface, proposing to restrict it to single operations and considering alternative approaches such as global clone tiers.\n\nAuthor acknowledged that limiting contention to objects within the same tier is beneficial and agreed with the approach, but did not explicitly state if they plan to address this issue in a future patch.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged",
                "agreed",
                "acknowledged a fix is needed",
                "proposed an alternative approach",
                "think"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Sounds good. I will add a CONFIG option for it and ensure it doesn't exceed\nMAX_SWAPFILE.\n\n---\n\nI have given this a lot of thought.\n\nSince the current interface allows mixing add (+), remove (-), and modify\noperations, we must either restore from a saved state or reverse the\nsuccessful individual operations upon failure.\n\nI implemented both approaches and concluded that reversing individual\noperations is error-prone. Also, it could be slow if there are many\noperations.\n\nAnother approach could be using a \"global clone tier\" strategy.\n(Because operation globally synchronized)\n\nTherefore, I would like to propose restricting the interface to handle a\nsingle operation at a time. What do you think?\n\n---\n\nI agree. With the tier structure, we can limit contention to objects within\nthe same tier.\n\nI also think swap_avail_list could be optimized in a similar way in the\nfuture.\n\nYoungjun",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "message_id": "aY6J3Yky6yfcIf36@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledges that stripping out the remove/modify parts from the patch is a viable direction, indicating a willingness to revise the patch based on reviewer feedback.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "willingness to revise"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Oops, I replied to your previous email before seeing this one.\n\nStripping out the remove/modify parts is also feasible. Do you agree with\nthat direction?\n\nYoungjun",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "message_id": "aY6Ly/0OcWFJEQ1M@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "The author clarified the meaning of '+' in the swap tiers configuration, explaining it switches to an exclusive mode where only that specific tier is used, and changing the model to a subtraction-based one where all tiers are selected by default and users use '-' to exclude specific ones.\n\nAuthor acknowledged Nhat Pham's feedback and agreed to restructure the swapoff path in v2 to drop the per-vswap spinlock before calling try_to_unmap().",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation",
                "acknowledged",
                "agreed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "To clarify, previously, the default state used all tiers. Using \"+\"              \nswitched to \"an exclusive mode\"  where only that specific tier was used.         \n                                                                                 \nI am changing this to a subtraction-based model. By default, all tiers           \nare selected, and users use \"-\" to exclude specific ones.                        \n(Then not \"removed\" but \"changed\" is more proper?)                               \n                                                                                 \nIn this context, I intended \"+\" to be used to restore a tier that was            \npreviously excluded by \"-\".\n\n---\n\nYes, that's right :)\n\nThanks \nYoungjun Park.",
              "reply_to": "Nhat Pham",
              "message_date": "2026-02-13",
              "message_id": "aY6P2ULxocDT7HV/@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author addressed Shakeel Butt's concern about using the BPF approach for swap control, agreeing it would provide flexibility but expressing concerns about logical contradictions and potential conflicts with cgroup hierarchy semantics. The author suggests implementing a native interface instead of relying on constrained BPF hooks.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "expressed concerns"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Apologies for overlooking the feedback regarding the BPF approach. Thank you\nfor the suggestion.\n\nI agree that using BPF would provide greater flexibility, allowing control not\njust at the memcg level, but also per-process or for complex workloads.\n(As like orchestrator and node controller)\n\nHowever, I am concerned that this level of freedom might introduce logical\ncontradictions, particularly regarding cgroup hierarchy semantics.\n\nFor example, BPF might allow a topology that violates hierarchical constraints\n(a concern that was also touched upon during LPC)\n\n  - Group A (Parent): Assigned to SSD1\n  - Group B (Child of A): Assigned to SSD2\n\nIf Group A has a `memory.swap.max` limit, and Group B swaps out to SSD2, it\ncreates a consistency issue. Group B consumes Group A's swap quota, but it is\nutilizing a device (SSD2) that is distinct from the Parent's assignment. This\ncould lead to situations where the Parent's limit is exhausted by usage on a\ndevice it effectively doesn't \"own\" or shouldn't be using.\n\nOne might suggest restricting BPF to strictly adhere to these hierarchical\nconstraints. However, doing so would effectively eliminate the primary\nadvantage of using BPF\\u2014its flexibility. If we are to enforce standard cgroup\nsemantics anyway, a native interface seems more appropriate than a constrained\nBPF hook.\n\nBeyond this specific example, I suspect that delegating this logic to BPF\nmight introduce other unforeseen edge cases regarding hierarchy enforcement.\nIn my view, the BPF approach seems more like a \"next step.\"\n\nSince you acknowledged that the idea of assigning swap devices to cgroups\n\"makes sense,\" I believe implementing this within the standard, strictly\nconstrained \"cgroup land\" is preferable. \n\nA strict cgroup interface ensures\nthat hierarchy and accounting rules are consistently enforced, avoiding the\npotential conflicts that the unrestricted freedom of BPF might create.\n\nUltimately, I hope this swap tier mechanism can serve as a foundation to be\nleveraged by other subsystems, such as BPF and DAMON. I view this proposal as\nthe necessary first step toward that future.\n\nYoungjun Park",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-13",
              "message_id": "aY6hcPNxiolf5jj6@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "The author acknowledged that existing swapfiles' tier is immutable once assigned and explained how they ensured this invariant by removing tier reference, using operation validation instead of reference counting.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a technical issue",
                "provided an explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I missed one comment. \n\nThe tier of existing swapfiles is immutable once assigned at swapon.\nI removed tier reference.\nInstead of reference counting, each operation validates the tier\nrange at operation time to guarantee this invariant.\n\n- add:    Does not change existing swapfiles' tier. New tier may\n          split priority range, but existing assignments stay.\n- remove: Rejected with -EBUSY if any swapfile is attached.\n- modify: Rejected if the change would cause any swapfile to\n          move to a different tier.\n\nSo swapfiles never jump between tiers at runtime.\n\nYoungjun Park",
              "reply_to": "Chris Li",
              "message_date": "2026-02-13",
              "message_id": "aY82PzT1GSfmznTv@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt requested that further discussion be concluded on the previous version of the patch series before a new version is sent.\n\nReviewer Shakeel Butt expressed concerns about introducing stable interfaces for swap tiers, requesting a BPF approach first and questioning the need for hierarchical control.\n\nReviewer Shakeel Butt noted that while BPF offers more power, its control is limited to administrators who can still make mistakes.\n\nReviewer Shakeel Butt requested additional information about the patch's use-case, specifically asking for details on workload ordering and partitioning of swap devices among sub-workloads.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "lack of technical feedback",
                "requested changes",
                "uncertainty",
                "no clear signal",
                "requested clarification",
                "wanted to brainstorm future use-cases"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Please don't send a new version of the series before concluding the discussion\non the previous one.\n\nOn Fri, Feb 13, 2026 at 12:58:40PM +0900, YoungJun Park wrote:\n\n---\n\nYes it provides the flexibility but that is not the main reason I am pushing for\nit. The reason I want you to first try the BPF approach without introducing any\nstable interfaces. Show how swap tiers will be used and configured in production\nenvironment and then we can talk if a stable interface is needed. I am still not\nconvinced that swap tiers need to be controlled hierarchically and the non-root\nshould be able to control it.\n\n---\n\nYes BPF provides more power but it is controlled by admin and admin can shoot\ntheir foot in multiple ways.\n\n---\n\nNo need to constraint anything.\n\nTaking a step back, can you describe your use-case a bit more and share\nrequirements?\n\nYou have multiple swap devices of different properties and you want to assign\nthose swap devices to different workloads. Now couple of questions:\n\n1. If more than one device is assign to a workload, do you want to have\n   some kind of ordering between them for the worklod or do you want option to\n   have round robin kind of policy?\n\n2. What's the reason to use 'tiers' in the name? Is it similar to memory tiers\n   and you want promotion/demotion among the tiers?\n\n3. If a workload has multiple swap devices assigned, can you describe the\n   scenario where such workloads need to partition/divide given devices to their\n   sub-workloads?\n\nLet's start with these questions. Please note that I want us to not just look at\nthe current use-case but brainstorm more future use-cases and then come up with\nthe solution which is more future proof.\n\nthanks,\nShakeel",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-20",
              "message_id": "aZjxP2sTavBRGC1l@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "Chris Li",
              "summary": "Reviewer Chris Li expressed concern that Shakeel Butt hadn't responded to YoungJun Park's previous response, potentially leading to confusion about whether the discussion was concluded.\n\nReviewer Chris Li suggested that instead of addressing the issue directly, a config option could be added to protect the problematic behavior and mark it as experimental, allowing for further testing and feedback.\n\nReviewer Chris Li confirmed that their company uses a different swap device at different cgroup levels, emphasizing the practical need for control at non-root levels.\n\nChris Li noted that the swap device control introduced in this patch is a basic need and generic, similar to zswap.writeback, but not as limited, and suggested that the interface can be improved in future iterations.\n\nChris Li mentioned that he couldn't recall a specific thread on linux-mm mailing list, but offered to share their usage requirement for cgroup swapfile control interface as an alternative solution.\n\nReviewer Chris Li noted that swap tier allocation is dependent on the number of devices within each tier, and suggested using a round-robin approach to distribute swap operations across devices within the same tier.\n\nReviewer Chris Li suggested alternative names for the tier concept, proposing 'swap.device_speed_classes' and acknowledging that the current name is inspired by memory tiers.\n\nReviewer Chris Li noted that their deployment uses multiple swap devices to reduce lock contention and requested consideration for tiering based on job configuration's tolerated swap speed.\n\nReviewer Chris Li expressed a neutral sentiment, suggesting that instead of designing for future-proofing, it's better to start from the current need and make incremental improvements. He used zswap.writeback as an example of a solution that worked for its specific requirement.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concern",
                "potential confusion",
                "requested changes",
                "suggested alternative solution",
                "acknowledgment of real-world use case",
                "NEEDS_WORK",
                "lack of technical detail",
                "offering alternative solution",
                "suggested alternative names",
                "acknowledged inspiration",
                "consideration_for_tiering",
                "incremental progress is better",
                "starting from the current need is a solid starting point"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "In this case I think it is fine.  You haven't responded to YoungJun's\nlast response in over a week. He might have mistaken that the\ndiscussion concluded.\nConsider it is one of the iterations. It is hard enough to contribute\nto the kernel. Relax.\nPlus, much of the discussion on the mailing list always has differing\nopinions. So, it's hard to determine what is truly concluded.\nDifferent people might have different interitations of the same text.\n\n---\n\nIs that your biggest concern? Many different ways exist to solve that\nproblem. e.g. We can put a config option protecting it and mark it as\nexperimental. This will unblock the development allow experiment. We\ncan have more people to try it out and give feedback.\n\n---\n\nYes, my company uses a different swap device at different cgroup\nlevel. I did ask my coworker to confirm that usage. Control at the non\nroot level is a real need.\n\n---\n\nI think this swap device control is a very basic need. All your\nobjections to swapping control in the group can equally apply to\nzswap.writeback. Unlike zswap.writeback, which only control from the\nzswap behavior. This is a more generic version control swap device\nother than zswap as well. BTW, I raised that concern about\nzswap.writeback was not generic enough as swap control was limited\nwhen zswap was proposed. We did hold back zswap.writeback. The\nconsensers is interface can be improved as later iterations. So here\nwe are.\n\n---\n\nThere is a very long thread on the linux-mm maillist. I'm too lazy to dig it up.\n\nI can share our usage requirement to refresh your memory. We\ninternally use a cgroup swapfile control interface that has not been\nupstreamed. With this we can remove the need of that internal\ninterface and go upstream instead.\n\n---\n\nIt depends on the number of devices in the tiers. Different tiers\nmaintain an order. Within the same tier round robin.\n\n---\n\nI propose the tier name. Guilty. Yes, in was inpired by memory tiers.\nIt just different class of swap speeds. I am not fixed on the name. We\ncan also call it swap.device_speed_classes. You can suggest\nalternatives.\n\nPromotion / demotion is possible in the future. The current state,\nwithout promotion or demotion, already provides value. Our current\ndeployment uses only one class of swap device at a time. However I do\nknow other companies use  more than one class of swap device.\n\n---\n\nIn our deployment, we always use more than one swap device to reduce\nswap device lock contention.\nThe job config can describe the swap speed it can tolerate. Some jobs\ncan tolerate slower speeds, while others cannot.\n\n---\n\nTake zswap.writeback as example. We have a solution that worked for\nthe requirement at that time. Incremental improvement is fine as well.\nUsually, incremental progress is better. At least currently there is a\nreal need to allow different cgroups to select different swap speeds.\nThere is a risk in being too future-proof: we might design things that\npeople in the future don't use as we envisioned. I see that happen too\noften as well.\n\nSo starting from the current need is a solid starting point. It's just\na different design philosophy. Each to their own.\n\nThat is the only usage case I know. YoungJun feel free to add yours\nusage as well.\n\nChris",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-20",
              "message_id": "CACePvbU=4f4gT5kHUBq0wD7COHN+quE5g4bPQqJYgJNx_9vuhg@mail.gmail.com",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author acknowledges that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agrees to restructure in v2\n\nAuthor acknowledged a concern about committing to a stable interface too early and proposed reducing this risk by adding a build-time config option or marking it as experimental, but also expressed uncertainty about the need for a memcg interface if BPF becomes primary.\n\nAuthor acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), but did not explicitly state a plan for addressing this issue in v2.\n\nAuthor suggests that adding build-time config or runtime constraints would improve predictability of swap tier feature\n\nThe author is addressing Shakeel Butt's concern about the patch's ability to handle complex use cases beyond their own simple use case. The author explains that they initially suggested per-cgroup swap device priorities but later pivoted to the 'swap tier' mechanism proposed by Chris Li, which only handles their specific use case of assigning different swap devices to different workloads (cgroups).\n\nAuthor addressed Shakeel Butt's concern about how swap devices are ordered when they have the same priority within a tier, explaining that round-robin ordering is used in this case.\n\nAuthor acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, but did not provide a clear resolution signal.\n\nAuthor acknowledged the need to reduce lock contention in swap device allocation, suggesting a possible solution of partitioning devices between parent and child cgroups.\n\nAuthor addressed Shakeel Butt's concern about the patch's long-term maintainability by suggesting that it is premature to design a stable interface for future use cases, and instead proposes guarding the memcg & tier behind a CONFIG option to allow for flexibility in the future.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure",
                "uncertainty",
                "concerns",
                "acknowledged a concern",
                "did not promise a fix",
                "clarification",
                "explaining",
                "explanation",
                "acknowledged",
                "agreed",
                "possible scenario",
                "acknowledges need for further work",
                "proposes temporary solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Understood. Let's continue the discussion. :D\n\nChris has already provided a thorough response, but I would like to\nadd my perspective as well.\n\n---\n\nI understand your concern about committing to a stable interface too\nearly. As Chris suggested, we could reduce this concern by guarding\nthe interface behind a build-time config option or marking it as\nexperimental, which I will also touch on further below.\n\nOn that note, if BPF were to become the primary control mechanism,\nI am not sure a memcg interface would still be needed at all, since\nBPF already provides a high degree of freedom. However, that level\nof freedom is also what concerns me -- BPF-driven swap device\nassignments could subtly conflict with memcg hierarchy semantics in\nways that are hard to predict or debug. A more constrained memcg-based\napproach might actually be safer in that regard.\n\n---\n\nI think this concern is closely tied to your question #3 below about\nconcrete use cases for partitioning devices across sub-workloads.\nI hope my answer there helps clarify this.\n\n---\n\nAs I mentioned above, I think guarding the feature behind a build-time\nconfig or runtime constraints could keep the usage well-defined and\npredictable, while still being useful.\n\n---\n\nOur use case is simple at now. \nWe have two swap devices with different performance\ncharacteristics and want to assign different swap devices to different\nworkloads (cgroups).\n\nFor some background, when I initially proposed this, I suggested allowing\nper-cgroup swap device priorities so that it could also accommodate the\nbroader scenarios you mentioned. However, since even our own use case\ndoes not require reversing swap priorities within a cgroup, we pivoted\nto the \"swap tier\" mechanism that Chris proposed.\n\n---\n\nBoth. If devices are in the same tier with the same priority, round robin.\nIf they are in the same tier with different priorities, or in different\ntiers, ordering applies. The current tier structure should be able to\nsatisfy either preference.\n\n---\n\nThis was originally Chris's idea. I think he explained the rationale\nwell in his reply.\n\n---\n\nOne possible scenario is reducing lock contention by partitioning swap\ndevices between parent and child cgroups.\n\n---\n\nWe have clear production use cases from both us and Chris, and I also\npresented a deployment example in the cover letter.\n\nI think it is hard to design concretely for future use cases at this\npoint. When those needs become clearer, BPF with its flexibility\nwould be a better fit then. I see BPF as a natural extension path\nrather than a starting point.\n\nFor now, guarding the memcg & tier behind a CONFIG option would\nlet us move forward without committing to a stable interface, and\nwe can always pivot to BPF later if needed\n\nThanks,\nYoungJun Park",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-21",
              "message_id": "aZnBo+P3ifskts9J@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer Shakeel Butt expressed concerns that the patch does not address the primary use-case of controlling/partitioning swap devices among sub-workloads and requested further exploration before adding a stable API.\n\nReviewer Shakeel Butt questioned whether the patch's concept of swap tiers is a new innovation or simply reusing an existing interface, specifically referencing Google's prodkernel team's past work on per-cgroup swapfiles and zswap.\n\nReviewer Shakeel Butt expressed skepticism about the introduction of a new swap tier interface without a clear use case, indicating that it motivates him to push back harder on the patch.\n\nReviewer Shakeel Butt questioned the practicality of hierarchical swap device control, specifically asking for a real-world use case to justify this feature.\n\nReviewer Shakeel Butt noted that having multiple swap devices reduces lock contention, but this does not address hierarchical control of swap devices among sub-workloads.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "concerns",
                "questioning",
                "uncertainty",
                "skepticism",
                "pushback",
                "lacking concrete evidence or justification"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "No, that is secondary because I am not seeing the real use-case of\ncontrolling/partitioning swap devices among sub-workloads. Until that is\nfigured out, adding a stable API is not good.\n\n---\n\nI am assuming you meant Google and particularly Prodkernel team and not\nAndroid or ChromeOS. Google's prodkernel used to have per-cgroup\nswapfiles exposed through memory.swapfiles (if I remember correctly\nSuleiman implemented this along with ghost swapfiles). Later this was\ndeprecated (by Yu Zhao) and global (ghost) swapfiles were being used.\nThe memory.swapfiles interface instead of supporting real swapfiles\nstarted having select options among default, ghost/zswap and real\n(something like that). However such interface was used to just disable\nor enable zswap for a workload and never about hierarchically\ncontrolling the swap devices (Google prodkernel only have zswap). Has\nsomething changed?\n\n---\n\nThis just motivates me to pushback even harder on adding a new interface\nwithout a clear use-case.\n\n---\n\nI already asked above but let me say it again. What's the actual real\nworld use-case to control/allow/disallow swap devices hierarchically?\n\n---\n\nHaving more than one swap devices to reduce lock contention is unrelated\nto hierarchically control swap devices among sub-workloads.",
              "reply_to": "Chris Li",
              "message_date": "2026-02-21",
              "message_id": "20260221163043.GA35350@shakeel.butt@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "YoungJun Park (author)",
              "summary": "Author is addressing concerns about the BPF-first approach, specifically questioning its feasibility in an embedded environment and asking for clarification on precedents of BPF prototypes graduating to stable interfaces.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "author is seeking clarification",
                "author is expressing uncertainty"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "....\n\nAfter reading the reply and re-think more of it.\n\nI have a few questions regarding the BPF-first approach you\nsuggested, if you don't mind. Some of them I am re-asking\nbecause I feel they have not been clearly addressed yet.\n\n- We are in an embedded environment where enabling additional\n  kernel compile options is costly. BPF is disabled by\n  default in some of our production configurations. From a\n  trade-off perspective, does it make sense to enable BPF\n  just for swap device control?\n\n- You suggest starting with BPF and discussing a stable\n  interface later. I am genuinely curious, are there actual\n  precedents where a BPF prototype graduated into a stable\n  kernel interface? \n\n- You raised that stable interfaces are hard to remove. Would\n  gating it behind a CONFIG option or marking it experimental\n  be an acceptable compromise?\n\n- You already acknowledged the use-case for assigning\n  different swap devices to different workloads. Your\n  objection is specifically about hierarchical parent-child\n  partitioning. If the interface enforced uniform policy\n  within a subtree, would that be acceptable?\n\n- We already run a modified kernel with internal swap control\n  in production and have real feedback from it. Requiring BPF\n  as a prerequisite to gather production experience seems\n  unnecessary when we are already doing that.\n\nTo be honest, I am having trouble understanding the motivation\nbehind the BPF-first validation approach. If the real point is\nthat BPF enables more flexible swap-out policies than any fixed\ninterface can, that would make much more sense to me. I would\nappreciate it if you could share more on this.\n\nThanks,\nYoungjun Park",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-22",
              "message_id": "aZpY1FIjYLtLdu5F@yjaykim-PowerEdge-T330",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that the patch does not handle the case where a child cgroup re-enables a tier excluded by its parent, and requested that the effective tier list be limited to the intersection of the parent's allowed subset.\n\nReviewer Shakeel Butt requested additional information about the cgroup hierarchy structure of the reviewer's deployment, specifically asking if they use cgroup v1 or v2 in their production environment.\n\nReviewer Shakeel Butt questioned whether the proposed 'Swap Tiers' concept is identical to existing swap priority behavior and requested clarification on this point.\n\nReviewer Shakeel Butt requested that the patch authors gather all options and their pros/cons before making an informed decision, indicating a need for further discussion.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "lack of technical disagreement",
                "request for clarification",
                "questioning",
                "further discussion"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hi YoungJun,\n\nI see you have sent a separate email on BPF specific questions to which I will\nrespond separately, here I will respond to other questions/comments.\n\nOn Sat, Feb 21, 2026 at 11:30:59PM +0900, YoungJun Park wrote:\n\n---\n\nIf you don't mind, can you share a bit more about the cgroup hierarchy structure\nof your deployment. Do you use cgroup v1 or v2 on your production environment?\n\n---\n\nI assume this is the same swap priorities as of today, right? You want similar\npriority behavior within a tier.\n\n---\n\nI think your use-case is very clear. Before committing to any options, I want us\nto brainstorm all options and gather pros/cons and then make an informed\ndecision. Anyways I will respond to your other email (in a day or two).\n\nShakeel",
              "reply_to": "YoungJun Park",
              "message_date": "2026-02-22",
              "message_id": "aZvX0HZy1PDylL8A@linux.dev",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Usama Arif",
      "primary_email": "usama.arif@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    }
  ]
}